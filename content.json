{"meta":{"title":"二十","subtitle":"卷就完了","description":"欢迎来卷","author":"二十","url":"https://yinhuidong.github.io","root":"/"},"pages":[{"title":"","date":"2022-01-10T12:46:21.415Z","updated":"2021-12-27T12:21:34.000Z","comments":true,"path":"css/custom.css","permalink":"https://yinhuidong.github.io/css/custom.css","excerpt":"","text":"/* 文章页H1-H6图标样式效果 */ h1::before, h2::before, h3::before, h4::before, h5::before, h6::before { -webkit-animation: ccc 1.6s linear infinite ; animation: ccc 1.6s linear infinite ; } @-webkit-keyframes ccc { 0% { -webkit-transform: rotate(0deg); transform: rotate(0deg) } to { -webkit-transform: rotate(-1turn); transform: rotate(-1turn) } } @keyframes ccc { 0% { -webkit-transform: rotate(0deg); transform: rotate(0deg) } to { -webkit-transform: rotate(-1turn); transform: rotate(-1turn) } } #content-inner.layout h1::before { color: #ef50a8 ; margin-left: -1.55rem; font-size: 1.3rem; margin-top: -0.23rem; } #content-inner.layout h2::before { color: #fb7061 ; margin-left: -1.35rem; font-size: 1.1rem; margin-top: -0.12rem; } #content-inner.layout h3::before { color: #ffbf00 ; margin-left: -1.22rem; font-size: 0.95rem; margin-top: -0.09rem; } #content-inner.layout h4::before { color: #a9e000 ; margin-left: -1.05rem; font-size: 0.8rem; margin-top: -0.09rem; } #content-inner.layout h5::before { color: #57c850 ; margin-left: -0.9rem; font-size: 0.7rem; margin-top: 0.0rem; } #content-inner.layout h6::before { color: #5ec1e0 ; margin-left: -0.9rem; font-size: 0.66rem; margin-top: 0.0rem; } #content-inner.layout h1:hover, #content-inner.layout h2:hover, #content-inner.layout h3:hover, #content-inner.layout h4:hover, #content-inner.layout h5:hover, #content-inner.layout h6:hover { color: #49b1f5 ; } #content-inner.layout h1:hover::before, #content-inner.layout h2:hover::before, #content-inner.layout h3:hover::before, #content-inner.layout h4:hover::before, #content-inner.layout h5:hover::before, #content-inner.layout h6:hover::before { color: #49b1f5 ; -webkit-animation: ccc 3.2s linear infinite ; animation: ccc 3.2s linear infinite ; } /* 页面设置icon转动速度调整 */ #rightside_config i.fas.fa-cog.fa-spin { animation: fa-spin 5s linear infinite ; } /*--------更换字体------------*/ @font-face { font-family: 'tzy'; /* 字体名自定义即可 */ src: url('https://cdn.jsdelivr.net/gh/tzy13755126023/BLOG_SOURCE/font/ZhuZiAWan.woff2'); /* 字体文件路径 */ font-display: swap; } body, .gitcalendar { font-family: tzy !important; } .categoryBar-list { max-height: 400px; } .clock-row { overflow: hidden; text-overflow: ellipsis; } /*3s为加载动画的时间，1为加载动画的次数，ease-in-out为动画效果*/ #page-header, #web_bg { -webkit-animation: imgblur 2s 1 ease-in-out; animation: imgblur 2s 1 ease-in-out; } @keyframes imgblur { 0% { filter: blur(5px); } 100% { filter: blur(0px); } } /*适配使用-webkit内核的浏览器 */ @-webkit-keyframes imgblur { 0% { -webkit-filter: blur(5px); } 100% { -webkit-filter: blur(0px); } } .table-wrap img { margin: .6rem auto .1rem !important; } /* 标签外挂 网站卡片 start */ .site-card-group img { margin: 0 auto .1rem !important; } .site-card-group .info a img { margin-right: 10px !important; } [data-theme='dark'] .site-card-group .site-card .info .title { color: #f0f0f0 !important; } [data-theme='dark'] .site-card-group .site-card .info .desc { color: rgba(255, 255, 255, .7) !important; } .site-card-group .info .desc { margin-top: 4px !important; } /* 代码块颜色 */ figure.highlight pre .addition { color: #00bf03 !important; }"},{"title":"分类","date":"2022-01-10T12:46:21.418Z","updated":"2021-12-27T12:21:34.000Z","comments":true,"path":"categories/index.html","permalink":"https://yinhuidong.github.io/categories/index.html","excerpt":"","text":""},{"title":"","date":"2022-01-10T12:46:21.418Z","updated":"2021-12-27T12:21:35.000Z","comments":true,"path":"js/chocolate.js","permalink":"https://yinhuidong.github.io/js/chocolate.js","excerpt":"","text":"/* * @Author: tzy1997 * @Date: 2020-12-15 20:55:25 * @LastEditors: tzy1997 * @LastEditTime: 2021-01-12 19:02:25 */ // 友情链接页面 头像找不到时 替换图片 if (location.href.indexOf(\"link\") !== -1) { var imgObj = document.getElementsByTagName(\"img\"); for (i = 0; i < imgObj.length; i++) { imgObj[i].onerror = function() { this.src = \"https://cdn.jsdelivr.net/gh/tzy13755126023/BLOG_SOURCE/theme_f/friend_404.gif\" } } } $(function() { // 气泡 function bubble() { $('#page-header').circleMagic({ radius: 10, density: .2, color: 'rgba(255,255,255,.4)', clearOffset: 0.99 }); }! function(p) { p.fn.circleMagic = function(t) { var o, a, n, r, e = !0, i = [], d = p.extend({ color: \"rgba(255,0,0,.5)\", radius: 10, density: .3, clearOffset: .2 }, t), l = this[0]; function c() { e = !(document.body.scrollTop > a) } function s() { o = l.clientWidth, a = l.clientHeight, l.height = a + \"px\", n.width = o, n.height = a } function h() { if (e) for (var t in r.clearRect(0, 0, o, a), i) i[t].draw(); requestAnimationFrame(h) } function f() { var t = this; function e() { t.pos.x = Math.random() * o, t.pos.y = a + 100 * Math.random(), t.alpha = .1 + Math.random() * d.clearOffset, t.scale = .1 + .3 * Math.random(), t.speed = Math.random(), \"random\" === d.color ? t.color = \"rgba(\" + Math.floor(255 * Math.random()) + \", \" + Math.floor(0 * Math.random()) + \", \" + Math.floor(0 * Math.random()) + \", \" + Math.random().toPrecision(2) + \")\" : t.color = d.color } t.pos = {}, e(), this.draw = function() { t.alpha"},{"title":"留言板","date":"2022-01-10T12:46:21.418Z","updated":"2021-12-27T12:21:35.000Z","comments":true,"path":"message/index.html","permalink":"https://yinhuidong.github.io/message/index.html","excerpt":"","text":"本页面还在开发中……"},{"title":"标签","date":"2022-01-10T12:46:21.418Z","updated":"2021-12-27T12:21:35.000Z","comments":true,"path":"tags/index.html","permalink":"https://yinhuidong.github.io/tags/index.html","excerpt":"","text":""},{"title":"关于我","date":"2022-01-11T16:11:50.439Z","updated":"2022-01-11T16:11:50.439Z","comments":true,"path":"关于我/index.html","permalink":"https://yinhuidong.github.io/%E5%85%B3%E4%BA%8E%E6%88%91/index.html","excerpt":"","text":"基本信息 类别 信息 出生年月 1998年11月 现居地 北京市 籍贯 大庆市 邮箱 &#x31;&#57;&#55;&#50;&#x30;&#51;&#x39;&#55;&#55;&#51;&#x40;&#x71;&#x71;&#46;&#x63;&#111;&#109; 教育经历 时间 学校 专业 备注 2017.09~2021.07 齐齐哈尔大学 计算机科学与技术 统招本科 工作经历 时间 公司 职位 2020.11~至今 小米 Java 开发工程师 专业技能Java 熟练掌握java基础 了解常用的设计模式，领域驱动设计 读过spring，springmvc，mybatis 源码 深入理解并发编程，读过Java并发编程源码 读过tomcat源码 读过springboot，dubbo，zookeeper，springcloud（eureka，nacos，sentinel，feign，ribbon，gateway），Quartz任务调度源码 掌握jvm内存模型与垃圾回收，掌握jvm调优 读过RocketMQ源码，熟练掌握rabbitMQ 消息队列，了解kafka消息队列 读过redis底层数据结构及部分源码，掌握核心原理，分布式集群 深入理解MySQL底层原理 读过ElasticSearch源码 读过Netty源码 熟悉分布式微服务架构与远程通信协议原理，分布式事务 了解HTTP与HTTPS协议原理，序列化与反序列化原理，RPC通信原理，nginx 了解docker容器 GoLang 了解GoLang基本语法，携程 掌握beego，grpc框架 Python 了解Python基本语法 掌握Python爬虫 大数据 了解 Hadoop Spark Flume Flink 项目经验自我评价菜的像个阿里P7。"},{"title":"技术笔记","date":"2022-01-10T12:46:21.418Z","updated":"2021-12-27T12:21:35.000Z","comments":true,"path":"技术笔记/index.html","permalink":"https://yinhuidong.github.io/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/index.html","excerpt":"","text":"我是技术笔记"}],"posts":[{"title":"MySQL[二十一]LIMIT","slug":"MySQL/MySQL[二十一]LIMIT","date":"2022-01-20T16:00:00.000Z","updated":"2022-01-13T06:18:50.323Z","comments":true,"path":"2022/01/21/MySQL/MySQL[二十一]LIMIT/","link":"","permalink":"https://yinhuidong.github.io/2022/01/21/MySQL/MySQL[%E4%BA%8C%E5%8D%81%E4%B8%80]LIMIT/","excerpt":"","text":"1.问题先建立一张表： 1234567891011121314151617181920212223242526# 创建表CREATE TABLE t( id INT UNSIGNED NOT NULL AUTO_INCREMENT, key1 VARCHAR(100), common_field VARCHAR(100), PRIMARY KEY (id), KEY idx_key1 (key1)) Engine = InnoDB CHARSET = utf8;# 定义变量SELECT @i := 1;# 整个存储过程往表中插入10000数据DELIMITER $CREATE PROCEDURE my_b()BEGIN while @i &lt;= 10000 do insert into t(key1, common_field) VALUES (uuid(), uuid()); select @i := @i + 1; # 自增 end while;END;CALL my_b(); 表t包含3个列，id列是主键，key1列是二级索引列。表中包含1万条记录。 当我们执行下边这个语句的时候，是使用二级索引idx_key1的： 这个很好理解，因为在二级索引idx_key1中，key1列是有序的。而查询是要取按照key1列排序的第1条记录，那MySQL只需要从idx_key1中获取到第一条二级索引记录，然后直接回表取得完整的记录即可。 但是如果我们把上边语句的LIMIT 1换成LIMIT 5000, 1，则却需要进行全表扫描，并进行filesort，执行计划如下： LIMIT 5000, 1也可以使用二级索引idx_key1呀，我们可以先扫描到第5001条二级索引记录，对第5001条二级索引记录进行回表操作不就好了么，这样的代价肯定比全表扫描+filesort强呀。但是由于MySQL实现上的缺陷，不会出现上述的理想情况，它只会去执行全表扫描+filesort，下边我们来看一下。 2.LIMITMySQL是在server层准备向客户端发送记录的时候才会去处理LIMIT子句中的内容。拿下边这个语句举例子： 1SELECT * FROM t ORDER BY key1 LIMIT 5000, 1; 如果使用idx_key1执行上述查询，那么MySQL会这样处理： server层向InnoDB要第1条记录，InnoDB从idx_key1中获取到第一条二级索引记录，然后进行回表操作得到完整的聚簇索引记录，然后返回给server层。server层准备将其发送给客户端，此时发现还有个LIMIT 5000, 1的要求，意味着符合条件的记录中的第5001条才可以真正发送给客户端，所以在这里先做个统计，我们假设server层维护了一个称作limit_count的变量用于统计已经跳过了多少条记录，此时就应该将limit_count设置为1。 server层再向InnoDB要下一条记录，InnoDB再根据二级索引记录的next_record属性找到下一条二级索引记录，再次进行回表得到完整的聚簇索引记录返回给server层。server层在将其发送给客户端的时候发现limit_count才是1，所以就放弃发送到客户端的操作，将limit_count加1，此时limit_count变为了2。 … 重复上述操作 直到limit_count等于5000的时候，server层才会真正的将InnoDB返回的完整聚簇索引记录发送给客户端。 从上述过程中可以看到，由于MySQL中是在实际向客户端发送记录前才会去判断LIMIT子句是否符合要求，所以如果使用二级索引执行上述查询的话，意味着要进行5001次回表操作。server层在进行执行计划分析的时候会觉得执行这么多次回表的成本太大了，还不如直接全表扫描+filesort快呢，所以就选择了后者执行查询。 3.优化手段由于MySQL实现LIMIT子句的局限性，在处理诸如LIMIT 5000, 1这样的语句时就无法通过使用二级索引来加快查询速度了么？其实也不是，只要把上述语句改写成： 1SELECT * FROM t, (SELECT id FROM t ORDER BY key1 LIMIT 5000, 1) AS d WHERE t.id = d.id; 这样，SELECT id FROM t ORDER BY key1 LIMIT 5000, 1作为一个子查询单独存在，由于该子查询的查询列表只有一个id列，MySQL可以通过仅扫描二级索引idx_key1执行该子查询，然后再根据子查询中获得到的主键值去表t中进行查找。 这样就省去了前5000条记录的回表操作，从而大大提升了查询效率！","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/tags/MySQL/"}]},{"title":"MySQL[二十]海量数据处理","slug":"MySQL/MySQL[二十]海量数据处理总结","date":"2022-01-19T16:00:00.000Z","updated":"2022-01-12T00:43:24.939Z","comments":true,"path":"2022/01/20/MySQL/MySQL[二十]海量数据处理总结/","link":"","permalink":"https://yinhuidong.github.io/2022/01/20/MySQL/MySQL[%E4%BA%8C%E5%8D%81]%E6%B5%B7%E9%87%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%80%BB%E7%BB%93/","excerpt":"","text":"海量数据处理总结何谓海量数据处理？所谓海量数据处理，无非就是基于海量数据上的存储、处理、操作。何谓海量，就是数据量太大，所以导致要么是无法在较短时间内迅速解决，要么是数据太大，导致无法一次性装入内存。 那解决办法呢?针对时间，我们可以采用巧妙的算法搭配合适的数据结构，如Bloom filter/Hash/bit-map/堆/数据库或倒排索引/trie树，针对空间，无非就一个办法：大而化小，分而治之（hash映射），你不是说规模太大嘛，那简单啊，就把规模大化为规模小的，各个击破不就完了嘛。 至于所谓的单机及集群问题，通俗点来讲，单机就是处理装载数据的机器有限(只要考虑cpu，内存，硬盘的数据交互)，而集群，机器有多辆，适合分布式处理，并行计算(更多考虑节点和节点间的数据交互)。 海量数据处理的方法处理海量数据问题，无非就是 6 种方法： 1.分而治之/hash映射 + hash统计 + 堆/快速/归并排序； 分而治之/hash映射：针对数据太大，内存受限，只能把大文件化成(取模映射)小文件 hash_map统计：当大文件转化了小文件，那么我们便可以采用常规的hash_map(key，value)来进行频率统计。 堆/快速排序：统计完了之后，便进行排序(可采取堆排序)，得到次数最多的key。 2.多层划分多层划分—-其实本质上还是分而治之的思想，重在“分”的技巧上！适用范围：第k大，中位数，不重复或重复的数字基本原理及要点：因为元素范围很大，不能利用直接寻址表，所以通过多次划分，逐步确定范围，然后最后在一个可以接受的范围内进行。 3.Bloom filter/BitmapBloom filter适用范围：可以用来实现数据字典，进行数据的判重，或者集合求交集基本原理：当一个元素被加入集合时，通过K个Hash函数将这个元素映射成一个位阵列（Bit array）中的K个点，把它们置为1。检索时，我们只要看看这些点是不是都是1就（大约）知道集合中有没有它了：如果这些点有任何一个0，则被检索元素一定不在；如果都是1，则被检索元素很可能在。 Bloom Filter的这种高效是有一定代价的：在判断一个元素是否属于某个集合时，有可能会把不属于这个集合的元素误认为属于这个集合（false positive）。因此，Bloom Filter不适合那些“零错误”的应用场合。而在能容忍低错误率的应用场合下，Bloom Filter通过极少的错误换取了存储空间的极大节省。 BitmapBit-map就是用一个bit位来标记某个元素对应的Value， 而Key即是该元素。由于采用了Bit为单位来表示某个元素是否存在，因此在存储空间方面，可以大大节省。 Bitmap排序方法第一步，将所有的位都置为0，从而将集合初始化为空。第二步，通过读入文件中的每个整数来建立集合，将每个对应的位都置为1。第三步，检验每一位，如果该位为1，就输出对应的整数。 Bloom filter可以看做是对bit-map的扩展 4.Trie树/数据库/倒排索引Trie树适用范围：数据量大，重复多，但是数据种类小可以放入内存基本原理及要点：实现方式，节点孩子的表示方式扩展：压缩实现。 数据库索引适用范围：大数据量的增删改查基本原理及要点：利用数据的设计实现方法，对海量数据的增删改查进行处理。 倒排索引(Inverted index)适用范围：搜索引擎，关键字查询基本原理及要点：一种索引方法，被用来存储在全文搜索下某个单词在一个文档或者一组文档中的存储位置的映射。 5.外排序适用范围：大数据的排序，去重基本原理及要点：外排序的归并方法，置换选择败者树原理，最优归并树 6.分布式处理之Hadoop/MapreduceMapReduce是一种计算模型，简单的说就是将大批量的工作（数据）分解（MAP）执行，然后再将结果合并成最终结果（REDUCE）。这样做的好处是可以在任务被分解后，可以通过大量机器进行并行计算，减少整个操作的时间。适用范围：数据量大，但是数据种类小可以放入内存基本原理及要点：将数据交给不同的机器去处理，数据划分，结果归约。 海量数据处理题目1.海量日志数据，提取出某日访问次数最多的那个IP 分而治之/hash映射：针对数据太大，内存受限，只能是把大文件化成(取模映射)小文件；按照IP地址的Hash(IP)%1000值，把海量IP日志分别存储到1000个小文件中。这样，每个小文件最多包含4MB个IP地址 hash_map统计：当大文件转化了小文件，那么我们便可以采用常规的hash_map(ip，value)来进行频率统计。 堆/快速排序：统计完了之后，可以得到1024个小文件中的出现次数最多的IP，再依据常规的排序算法得到总体上出现次数最多的IP； Hash取模是一种等价映射，不会存在同一个元素分散到不同小文件中的情况，即这里采用的是mod1000算法，那么相同的IP在hash取模后，只可能落在同一个文件中，不可能被分散的。因为如果两个IP相等，那么经过Hash(IP)之后的哈希值是相同的，将此哈希值取模（如模1000），必定仍然相等。 2.寻找热门查询，300万个查询字符串中统计最热门的10个查询原题：搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来，每个查询串的长度为1-255字节。假设目前有一千万个记录（这些查询串的重复度比较高，虽然总数是1千万，但如果除去重复后，不超过3百万个。一个查询串的重复度越高，说明查询它的用户越多，也就是越热门），请你统计最热门的10个查询串，要求使用的内存不能超过1G。 数据规模大，一次处理不了，我们就需要将数据通过hash映射切分；而本题的情况属于数据量可以一次放入内存(300万个字符串假设没有重复，都是最大长度，那么最多占用内存3M*1K/4=0.75G。所以可以将所有字符串都存放在内存中进行处理)，所以只是需要一个合适的数据结构 所以我们在此直接读数据进行hash统计，统计后的数据只有0.75G，可以直接进行排序，而对这种TopK问题，一般是采用堆来解决。 hash_map统计：先对这批海量数据预处理。具体方法是：维护一个Key为Query字串，Value为该Query出现次数的HashMap，即hashmap(Query，Value)，每次读取一个Query，如果该字串不在HashMap中，那么加入该字串，并且将Value值设为1；如果该字串在HashMap中，那么将该字串的计数加一即可。最终我们在O(N)的时间复杂度内用Hash表完成了统计； 堆排序：借助堆这个数据结构，找出Top K，时间复杂度为O(NlogK)。因此，维护一个K(该题目中是10)大小的小根堆，然后遍历300万的Query，分别和根元素进行对比。所以，我们最终的时间复杂度是：O（N） + N’ * O（logK），（N为1000万，N’为300万）。 方案2：采用trie树，关键字域存该查询串出现的次数，没有出现为0。最后用10个元素的最小推来对出现频率进行排序。 3.有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词。 分而治之/hash映射：顺序读文件中，对于每个词x，取hash(x)%5000，然后按照该值存到5000个小文件（记为x0,x1,…x4999）中。这样每个文件大概是200k左右。如果其中的有的文件超过了1M大小，还可以按照类似的方法继续往下分，直到分解得到的小文件的大小都不超过1M。 hash_map统计：对每个小文件，采用trie树/hash_map等统计每个文件中出现的词以及相应的频率。 堆/归并排序：取出出现频率最大的100个词（可以用含100个结点的最小堆）后，再把100个词及相应的频率存入文件，这样又得到了5000个文件。最后就是把这5000个文件进行归并（类似于归并排序）的过程了。 4.海量数据分布在100台电脑中，想个办法高效统计出这批数据的TOP10。如果每个数据元素只出现一次，而且只出现在某一台机器中，那么可以采取以下步骤统计出现次数TOP10的数据元素： 如果每个数据元素只出现一次，而且只出现在某一台机器中，那么可以采取以下步骤统计出现次数TOP10的数据元素： 求出每台电脑上的TOP10后，然后把这100台电脑上的TOP10组合起来，共1000个数据，再利用上面类似的方法求出TOP10就可以了。 但如果同一个元素重复出现在不同的电脑中,则有两种方法： 遍历一遍所有数据，重新hash取摸，如此使得同一个元素只出现在单独的一台电脑中，然后采用上面所说的方法，统计每台电脑中各个元素的出现次数找出TOP10，继而组合100台电脑上的TOP10，找出最终的TOP10。 暴力求解：直接统计统计每台电脑中各个元素的出现次数，然后把同一个元素在不同机器中的出现次数相加，最终从所有数据中找出TOP10。 5.有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排序。方案1： hash映射：顺序读取10个文件，按照hash(query)%10的结果将query写入到另外10个文件（记为a0,a1,..a9）中。这样新生成的文件每个的大小大约也1G（假设hash函数是随机的）。 hash_map统计：找一台内存在2G左右的机器，依次对用hash_map(query, query_count)来统计每个query出现的次数。注：hash_map(query,query_count)是用来统计每个query的出现次数，不是存储他们的值，出现一次，则count+1。 堆/快速/归并排序：利用快速/堆/归并排序按照出现次数进行排序，将排序好的query和对应的query_cout输出到文件中，这样得到了10个排好序的文件（记为）。最后，对这10个文件进行归并排序（内排序与外排序相结合） 方案2：一般query的总量是有限的，只是重复的次数比较多而已，可能对于所有的query，一次性就可以加入到内存了。这样，我们就可以采用trie树/hash_map等直接来统计每个query出现的次数，然后按出现次数做快速/堆/归并排序就可以了。 方案3：与方案1类似，但在做完hash，分成多个文件后，可以交给多个文件来处理，采用分布式的架构来处理（比如MapReduce），最后再进行合并。 6.给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？可以估计每个文件安的大小为5G×64=320G，远远大于内存限制的4G。所以不可能将其完全加载到内存中处理。考虑采取分而治之的方法。 方案1： 分而治之/hash映射：遍历文件a，对每个url求取，然后根据所取得的值将url分别存储到1000个小文件（记为a0,a1,…,a999）中。这样每个小文件的大约为300M。遍历文件b，采取和a相同的方式将url分别存储到1000小文件中（记为b0,b1,…,b999）。这样处理后，所有可能相同的url都在对应的小文件（a0vsb0,a1vsb1,…,a999vsb999）中，不对应的小文件不可能有相同的url。然后我们只要求出1000对小文件中相同的url即可。 hash_set统计：求每对小文件中相同的url时，可以把其中一个小文件的url存储到hash_set中。然后遍历另一个小文件的每个url，看其是否在刚才构建的hash_set中，如果是，那么就是共同的url，存到文件里面就可以了。 方案2：如果允许有一定的错误率，可以使用Bloom filter，4G内存大概可以表示340亿bit。将其中一个文件中的url使用Bloom filter映射为这340亿bit，然后挨个读取另外一个文件的url，检查是否与Bloom filter，如果是，那么该url应该是共同的url（注意会有一定的错误率）。 7.在2.5亿个整数中找出不重复的整数，注，内存不足以容纳这2.5亿个整数。方案1： 分而治之/hash映射 hashmap统计 找出所有value为1的key值 方案2：采用2-Bitmap（每个数分配2bit，00表示不存在，01表示出现一次，10表示多次，11无意义）进行，共需内存2^32 * 2 bit=1 GB内存，还可以接受。然后扫描这2.5亿个整数，查看Bitmap中相对应位，如果是00变01，01变10，10保持不变。所描完事后，查看bitmap，把对应位是01的整数输出即可。 8.怎么在海量数据中找出重复次数最多的一个？ 先做hash，然后求模映射为小文件， 通过hashmap求出每个小文件中重复次数最多的一个，并记录重复次数。 然后比较所有小文件中出现最多的数，最大的就是重复次数最多 9.上千万或上亿数据（有重复），统计其中出现次数最多的前N个数据。上千万或上亿的数据，现在的机器的内存应该能存下。所以考虑采用hash_map来进行统计次数。然后利用堆取出前N个出现次数最多的数据。 10.一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前10个词，请给出思想，给出时间复杂度分析。方案1：如果文件比较大，无法一次性读入内存，可以采用hash取模的方法，将大文件分解为多个小文件，对于单个小文件利用hash_map统计出每个小文件中10个最常出现的词，然后再进行归并处理，找出最终的10个最常出现的词。 方案2：通过hash取模将大文件分解为多个小文件后，除了可以用hash_map统计出每个小文件中10个最常出现的词，也可以用trie树统计每个词出现的次数，时间复杂度是O(n*le)（le表示单词的平准长度），最终同样找出出现最频繁的前10个词（可用堆来实现），时间复杂度是O(nlog10)。 11.1000万字符串，其中有些是重复的，需要把重复的全部去掉，保留没有重复的字符串。请怎么设计和实现？方案1：这题用trie树比较合适，hash_map也行。 方案2：分而治之/hash映射 + hashmap 12.一个文本文件，找出前10个经常出现的词，但这次文件比较长，说是上亿行或十亿行，总之无法一次读入内存，问最优解。 首先根据用hash并求模，将文件分解为多个小文件， 对于单个文件利用hashmap求出每个文件件中10个最常出现的词。 然后再进行归并处理，找出最终的10个最常出现的词。 13.给40亿个不重复的unsigned int的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中？方案1：申请512M的内存，一个bit位代表一个unsigned int值。读入40亿个数，设置相应的bit位，读入要查询的数，查看相应bit位是否为1，为1表示存在，为0表示不存在。 14.10亿个QQ号，找出一个QQ号是不是在其中，时间复杂度要求O(1)用bitmap来做这个问题。首先对数据进行预处理。定义10亿bit位个int.在32位计算机下，一个int是32位，10亿位的话，就需要10亿除以32个int整数。大概有很多个。第一个int标记0-31这个数字范围的QQ号存不存在，比如说0000001这个QQ号，我就把第一个int的第1位置1。第二个int能够标记32-63这个范围的QQ存不存在，以此类推。把这10亿个QQ号预处理一遍。然后计算你给我的这个QQ号，它是在哪个int里面，然后找到相应的数据位，看是1还是0，就能在O(1)的时间里找到 15.对已经发放的所有139段的号码进行统计排序，已经发放的139号码段的文件都存放在一个文本文件中（原题是放在两个文件中），一个号码一行，现在需要将文件里的所有号码进行排序，并写入到一个新的文件中；号码可能会有很多，最多可能有一亿个不同的号码（所有的139段号码），存入文本文件中大概要占1.2G的空间；jvm最大的内存在300以内，程序要考虑程序的可执行性及效率；只能使用Java标准库，不得使用第三方工具。方案1： 顺序读取存放号码文件的中所有号码，并取139之后的八位转换为int类型；每读取号码数满一百万个（这个数据可配置）将已经读取的号码排序并存入新建的临时文件。 将所有生成的号码有序的临时文件合并存入结果文件。 这个算法虽然解决了空间问题，但是运行效率极低，由于IO读写操作太多，加上步骤1中的排序的算法（快速排序）本来效率就不高 方案2：bitmap一个号码占一个bit，一共需要 99999999 bit ，一个int32位，所以需要312.5万个int值，即1250万Byte = 12.5M，算法如下 初始化bits[capacity]； 顺序所有读入电话号码，并转换为int类型，修改位向量值bits[phoneNum]=1； 遍历bits数组，如果bits[index]=1，转换index为电话号码输出。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/tags/MySQL/"}]},{"title":"MySQL[十八]锁分析","slug":"MySQL/MySQL[十八]锁分析","date":"2022-01-17T16:00:00.000Z","updated":"2022-01-13T01:58:52.473Z","comments":true,"path":"2022/01/18/MySQL/MySQL[十八]锁分析/","link":"","permalink":"https://yinhuidong.github.io/2022/01/18/MySQL/MySQL[%E5%8D%81%E5%85%AB]%E9%94%81%E5%88%86%E6%9E%90/","excerpt":"","text":"一，语句加锁分析先给上一篇的hero表建立一个二级索引。 1alert table hero add index idx_name(name); 现在hero表就有了两个索引，主键聚簇索引&amp;普通二级索引。​ 1.普通的select语句在不同的隔离级别下，普通的select语句具有不同的表现。​ 在读未提交隔离级别下，不加锁，直接读取记录的最新版本；可能出现脏读，不可重复读，幻读。 在读已提交隔离级别下，不加锁，每次执行普通的select都会生成一个readview，避免脏读，但是没有避免幻读和不可重复读。 在可重复度隔离级别下，不加锁，只有在第一次执行普通的select语句时生成一个readview，这样就把脏读，不可重复读，幻读都避免了。 ​ 注意： 123456789101112131415# 事务T1，在可重复读隔离级别下：begin;select * from hero where number =30;# empty set# 此时事务T2执行了：insert into hero values(30,&#x27;g关羽&#x27;,&#x27;魏&#x27;);# 语句，并提交update hero set country = &#x27;蜀&#x27; where number =30;select * from hero where number =30;# 查到了一条记录 在可重复读隔离级别下，T1第一次执行普通的select语句时生成了一个readview，之后T2向hero表中插入一条新记录并提交。readview并不能阻止T1执行update或者delete语句来改动这个新插入的记录（由于t2已经提交，因此改动该记录并不会造成阻塞），但是这样一来，这条新记录的事务id隐藏列的值就变成了T1的事务id。之后T1在使用普通的select语句去查询这条记录就可以看到这条记录了，也就可以把这条记录返回给客户端。因为这个特殊现象的存在，我们可以认为MVCC并不能完全禁止幻读。 在串行化隔离级别下，需要分两种情况看： 在禁止自动提交的时候，普通的select语句会被转化为 select…lock in share mode ,也就是读取记录前需要先获得记录的S锁。 在允许自动提交的时候，普通的select语句并不会加锁，只是利用MVCC生成一个readview来读取记录。为啥不加锁呢？因为启用自动提交意味着一个事物中只包含一条语句，而只执行一条语句也就不会出现不可重复读，幻读这样的现象了。 2.锁定读的语句我们把下边四种语句放到一起讨论：​ 语句一：SELECT … LOCK IN SHARE MODE; 语句二：SELECT … FOR UPDATE; 语句三：UPDATE … 语句四：DELETE … ​ 语句一和语句二是MySQL中规定的两种锁定读的语法格式，而语句三和语句四由于在执行过程需要首先定位到被改动的记录并给记录加锁，也可以被认为是一种锁定读。​ 2.1 READ UNCOMMITTED/READ COMMITTED隔离级别下​ 在READ UNCOMMITTED下语句的加锁方式和READ COMMITTED隔离级别下语句的加锁方式基本一致，所以就放到一块儿说了。值得注意的是，采用加锁方式解决并发事务带来的问题时，其实脏读和不可重复读在任何一个隔离级别下都不会发生（因为读-写操作需要排队进行）。​ 1）使用主键进行等值查询 使用SELECT … LOCK IN SHARE MODE来为记录加锁，比方说：1SELECT * FROM hero WHERE number = 8 LOCK IN SHARE MODE; 这个语句执行时只需要访问一下聚簇索引中number值为8的记录，所以只需要给它加一个S型正经记录锁就好了，如图所示： 使用SELECT … FOR UPDATE来为记录加锁，比方说：1SELECT * FROM hero WHERE number = 8 FOR UPDATE; 这个语句执行时只需要访问一下聚簇索引中number值为8的记录，所以只需要给它加一个X型正经记录锁就好了，如图所示：​ 使用UPDATE …来为记录加锁，比方说：1UPDATE hero SET country = &#x27;汉&#x27; WHERE number = 8; 这条UPDATE语句并没有更新二级索引列，加锁方式和上边所说的SELECT … FOR UPDATE语句一致。如果UPDATE语句中更新了二级索引列，比方说：1UPDATE hero SET name = &#x27;cao曹操&#x27; WHERE number = 8; ​ 该语句的实际执行步骤是首先更新对应的number值为8的聚簇索引记录，再更新对应的二级索引记录，所以加锁的步骤就是：​ 为number值为8的聚簇索引记录加上X型正经记录锁（该记录对应的）。 为该聚簇索引记录对应的idx_name二级索引记录（也就是name值为’c曹操’，number值为8的那条二级索引记录）加上X型正经记录锁。 用带圆圈的数字来表示为各条记录加锁的顺序。 ​ 使用DELETE …来为记录加锁，比方说：1DELETE FROM hero WHERE number = 8; “DELETE表中的一条记录”其实意味着对聚簇索引和所有的二级索引中对应的记录做DELETE操作，本例子中就是要先把number值为8的聚簇索引记录执行DELETE操作，然后把对应的idx_name二级索引记录删除，所以加锁的步骤和上边更新带有二级索引列的UPDATE语句一致。 2）使用主键进行范围查询 使用SELECT … LOCK IN SHARE MODE来为记录加锁，比方说：SELECT * FROM hero WHERE number &lt;= 8 LOCK IN SHARE MODE; 这个语句看起来十分简单，但它的执行过程还是有点复杂的：​ 先到聚簇索引中定位到满足number &lt;= 8的第一条记录，也就是number值为1的记录，然后为其加锁。 判断一下该记录是否符合索引条件下推中的条件。不过需要注意的是，索引条件下推只是为了减少回表次数，也就是减少读取完整的聚簇索引记录的次数，从而减少IO操作。而对于聚簇索引而言不需要回表，它本身就包含着全部的列，也起不到减少IO操作的作用，所以InnoDB规定这个索引条件下推特性只适用于二级索引。也就是说在本例中与被使用索引有关的条件是：number &lt;= 8，而number列又是聚簇索引列，所以本例中并没有符合索引条件下推的查询条件，自然也就不需要判断该记录是否符合索引条件下推中的条件。 判断一下该记录是否符合范围查询的边界条件因为在本例中是利用主键number进行范围查询，InnoDB规定每从聚簇索引中取出一条记录时都要判断一下该记录是否符合范围查询的边界条件，也就是number &lt;= 8这个条件。如果符合的话将其返回给server层继续处理，否则的话需要释放掉在该记录上加的锁，并给server层返回一个查询完毕的信息。对于number值为1的记录是符合这个条件的，所以会将其返回到server层继续处理。 将该记录返回到server层继续判断。server层如果收到存储引擎层提供的查询完毕的信息，就结束查询，否则继续判断那些没有进行索引条件下推的条件，在本例中就是继续判断number &lt;= 8这个条件是否成立。不是在第3步中已经判断过了么，怎么在这又判断一回？InnoDB把凡是没有经过索引条件下推的条件都需要放到server层再判断一遍。如果该记录符合剩余的条件（没有进行索引条件下推的条件），那么就把它发送给客户端，不然的话需要释放掉在该记录上加的锁。 然后刚刚查询得到的这条记录（也就是number值为1的记录）组成的单向链表继续向后查找，得到了number值为3的记录，然后重复第2，3，4、5这几个步骤。 ​ 但是这个过程有个问题，就是当找到number值为8的那条记录的时候，还得向后找一条记录（也就是number值为15的记录），在存储引擎读取这条记录的时候，也就是上述的第1步中，就得为这条记录加锁，然后在第3步时，判断该记录不符合number &lt;= 8这个条件，又要释放掉这条记录的锁，这个过程导致number值为15的记录先被加锁，然后把锁释放掉，过程就是这样：​ 如果先在事务T1中执行： 123# 事务T1BEGIN;SELECT * FROM hero WHERE number &lt;= 8 LOCK IN SHARE MODE; 然后再到事务T2中执行： 123# 事务T2BEGIN;SELECT * FROM hero WHERE number = 15 FOR UPDATE; ​ 是没有问题的，因为在T2执行时，事务T1已经释放掉了number值为15的记录的锁，但是如果你先执行T2，再执行T1，由于T2已经持有了number值为15的记录的锁，事务T1将因为获取不到这个锁而等待。​ 我们再看一个使用主键进行范围查询的例子： 1SELECT * FROM hero WHERE number &gt;= 8 LOCK IN SHARE MODE; ​ 这个语句的执行过程其实和我们举的上一个例子类似。也是先到聚簇索引中定位到满足number &gt;= 8这个条件的第一条记录，也就是number值为8的记录，然后就可以沿着由记录组成的单向链表一路向后找，每找到一条记录，就会为其加上锁，然后判断该记录符不符合范围查询的边界条件，不过这里的边界条件比较特殊：number &gt;= 8，只要记录不小于8就算符合边界条件，所以判断和没判断是一样一样的。最后把这条记录返回给server层，server层再判断number &gt;= 8这个条件是否成立，如果成立的话就发送给客户端，否则的话就结束查询。不过InnoDB存储引擎找到索引中的最后一条记录，也就是Supremum伪记录之后，在存储引擎内部就可以立即判断这是一条伪记录，不必要返回给server层处理，也没必要给它也加上锁（也就是说在第1步中就压根儿没给这条记录加锁）。整个过程会给number值为8、15、20这三条记录加上S型正经记录锁，画个图表示一下就是这样： 使用SELECT … FOR UPDATE语句来为记录加锁：和SELECT … FOR UPDATE语句类似，只不过加的是X型正经记录锁。 使用UPDATE …来为记录加锁，比方说：UPDATE hero SET country = &#39;汉&#39; WHERE number &gt;= 8;这条UPDATE语句并没有更新二级索引列，加锁方式和上边所说的SELECT … FOR UPDATE语句一致。如果UPDATE语句中更新了二级索引列，比方说：UPDATE hero SET name = &#39;cao曹操&#39; WHERE number &gt;= 8; 这时候会首先更新聚簇索引记录，再更新对应的二级索引记录，所以加锁的步骤就是： 为number值为8的聚簇索引记录加上X型正经记录锁。 然后为上一步中的记录索引记录对应的idx_name二级索引记录加上X型正经记录锁。 为number值为15的聚簇索引记录加上X型正经记录锁。 然后为上一步中的记录索引记录对应的idx_name二级索引记录加上X型正经记录锁。 为number值为20的聚簇索引记录加上X型正经记录锁。 然后为上一步中的记录索引记录对应的idx_name二级索引记录加上X型正经记录锁。 ​ 如果是下边这个语句： 1UPDATE hero SET name = &#x27;汉&#x27; WHERE number &lt;= 8; 则会对number值为1、3、8聚簇索引记录以及它们对应的二级索引记录加X型正经记录锁，加锁顺序和上边语句中的加锁顺序类似，都是先对一条聚簇索引记录加锁后，再给对应的二级索引记录加锁。之后会继续对number值为15的聚簇索引记录加锁，但是随后InnoDB存储引擎判断它不符合边界条件，随即会释放掉该聚簇索引记录上的锁（注意这个过程中没有对number值为15的聚簇索引记录对应的二级索引记录加锁）。​ 使用DELETE …来为记录加锁，比方说：DELETE FROM hero WHERE number &gt;= 8; 和DELETE FROM hero WHERE number &lt;= 8;这两个语句的加锁情况和更新带有二级索引列的UPDATE语句一致。 ​ 3）使用二级索引进行等值查询 在READ UNCOMMITTED和READ COMMITTED隔离级别下，使用普通的二级索引和唯一二级索引进行加锁的过程是一样的。 ​ 使用SELECT … LOCK IN SHARE MODE来为记录加锁，比方说： 1SELECT * FROM hero WHERE name = &#x27;c曹操&#x27; LOCK IN SHARE MODE; 这个语句的执行过程是先通过二级索引idx_name定位到满足name = ‘c曹操’条件的二级索引记录，然后进行回表操作。所以先要对二级索引记录加S型正经记录锁，然后再给对应的聚簇索引记录加S型正经记录锁，示意图如下：这里需要再次强调一下这个语句的加锁顺序： 先对name列为’c曹操’二级索引记录进行加锁。 再对相应的聚簇索引记录进行加锁 idx_name是一个普通的二级索引，到idx_name索引中定位到满足name= ‘c曹操’这个条件的第一条记录后，就可以沿着这条记录一路向后找。可是从上边的描述中可以看出来，并没有对下一条二级索引记录进行加锁，这是为什么呢？因为InnoDB对等值匹配的条件有特殊处理，他们规定在InnoDB存储引擎层查找到当前记录的下一条记录时，在对其加锁前就直接判断该记录是否满足等值匹配的条件，如果不满足直接返回（也就是不加锁了），否则的话需要将其加锁后再返回给server层。所以这里也就不需要对下一条二级索引记录进行加锁了。 ​ 我们假设上边这个语句在事务T1中运行，然后事务T2中运行下边一个我们之前介绍过的语句： 1UPDATE hero SET name = &#x27;曹操&#x27; WHERE number = 8; ​ 这两个语句都是要对number值为8的聚簇索引记录和对应的二级索引记录加锁，但是不同点是加锁的顺序不一样。这个UPDATE语句是先对聚簇索引记录进行加锁，后对二级索引记录进行加锁，如果在不同事务中运行上述两个语句:​ 事务T2持有了聚簇索引记录的锁，事务T1持有了二级索引记录的锁。 事务T2在等待获取二级索引记录上的锁，事务T1在等待获取聚簇索引记录上的锁。 ​ 两个事务都分别持有一个锁，而且都在等待对方已经持有的那个锁，这种情况就是所谓的死锁，两个事务都无法运行下去，必须选择一个进行回滚，对性能影响比较大。 使用SELECT … FOR UPDATE语句时，比如： 1SELECT * FROM hero WHERE name = &#x27;c曹操&#x27; FOR UPDATE; 这种情况下与SELECT … LOCK IN SHARE MODE语句的加锁情况类似，都是给访问到的二级索引记录和对应的聚簇索引记录加锁，只不过加的是X型正经记录锁罢了。 使用UPDATE …来为记录加锁，比方说：与更新二级索引记录的SELECT … FOR UPDATE的加锁情况类似，不过如果被更新的列中还有别的二级索引列的话，对应的二级索引记录也会被加锁。 使用DELETE …来为记录加锁，比方说：与SELECT … FOR UPDATE的加锁情况类似，不过如果表中还有别的二级索引列的话，对应的二级索引记录也会被加锁。 ​ 4）使用二级索引进行范围查询 使用SELECT … LOCK IN SHARE MODE来为记录加锁，比方说：1SELECT * FROM hero FORCE INDEX(idx_name) WHERE name &gt;= &#x27;c曹操&#x27; LOCK IN SHARE MODE; 因为优化器会计算使用二级索引进行查询的成本，在成本较大时可能选择以全表扫描的方式来执行查询，所以我们这里使用FORCE INDEX(idx_name)来强制使用二级索引idx_name来执行查询。这个语句的执行过程其实是先到二级索引中定位到满足name &gt;= ‘c曹操’的第一条记录，也就是name值为c曹操的记录，然后就可以沿着这条记录的链表一路向后找，从二级索引idx_name的示意图中可以看出，所有的用户记录都满足name &gt;= ‘c曹操’的这个条件，所以所有的二级索引记录都会被加S型正经记录锁，它们对应的聚簇索引记录也会被加S型正经记录锁。不过需要注意一下加锁顺序，对一条二级索引记录加锁完后，会接着对它相应的聚簇索引记录加锁，完后才会对下一条二级索引记录进行加锁，以此类推。 再来看下边这个语句： 1SELECT * FROM hero FORCE INDEX(idx_name) WHERE name &lt;= &#x27;c曹操&#x27; LOCK IN SHARE MODE; 在使用number &lt;= 8这个条件的语句中，需要把number值为15的记录也加一个锁，之后又判断它不符合边界条件而把锁释放掉。而对于查询条件name &lt;= ‘c曹操’的语句来说，执行该语句需要使用到二级索引，而与二级索引相关的条件是可以使用索引条件下推这个特性的。InnoDB规定，如果一条记录不符合索引条件下推中的条件的话，直接跳到下一条记录（这个过程根本不将其返回到server层），如果这已经是最后一条记录，那么直接向server层报告查询完毕。​ 但是这里头有个问题：先对一条记录加了锁，然后再判断该记录是不是符合索引条件下推的条件，如果不符合直接跳到下一条记录或者直接向server层报告查询完毕，这个过程中并没有把那条被加锁的记录上的锁释放掉！！！。本例中使用的查询条件是name &lt;= ‘c曹操’，在为name值为’c曹操’的二级索引记录以及它对应的聚簇索引加锁之后，会接着二级索引中的下一条记录，也就是name值为’l刘备’的那条二级索引记录，由于该记录不符合索引条件下推的条件，而且是范围查询的最后一条记录，会直接向server层报告查询完毕，重点是这个过程中并不会释放name值为’l刘备’的二级索引记录上的锁，也就导致了语句执行完毕时的加锁情况如下所示：这样子会造成一个尴尬情况，假如T1执行了上述语句并且尚未提交，T2再执行这个语句： 1SELECT * FROM hero WHERE name = &#x27;l刘备&#x27; FOR UPDATE; T2中的语句需要获取name值为l刘备的二级索引记录上的X型正经记录锁，而T1中仍然持有name值为l刘备的二级索引记录上的S型正经记录锁，这就造成了T2获取不到锁而进入等待状态。​ 为啥不能释放不符合索引条件下推中的条件的二级索引记录上的锁呢？我也不知道，人家就是这么规定的。 ​ 使用SELECT … FOR UPDATE语句时：和SELECT … FOR UPDATE语句类似，只不过加的是X型正经记录锁。 使用UPDATE …来为记录加锁，比方说：1UPDATE hero SET country = &#x27;汉&#x27; WHERE name &gt;= &#x27;c曹操&#x27;; ​ FORCE INDEX只对SELECT语句起作用，UPDATE语句虽然支持该语法，但实质上不起作用，DELETE语句压根儿不支持该语法。​ 假设该语句执行时使用了idx_name二级索引来进行锁定读，那么它的加锁方式和上边所说的SELECT … FOR UPDATE语句一致。如果有其他二级索引列也被更新，那么也会为对应的二级索引记录进行加锁。​ 还有一个情况： 1UPDATE hero SET country = &#x27;汉&#x27; WHERE name &lt;= &#x27;c曹操&#x27;; 索引条件下推这个特性只适用于SELECT语句，也就是说UPDATE语句中无法使用，那么这个语句就会为name值为’c曹操’和’l刘备’的二级索引记录以及它们对应的聚簇索引进行加锁，之后在判断边界条件时发现name值为’l刘备’的二级索引记录不符合name &lt;= ‘c曹操’条件，再把该二级索引记录和对应的聚簇索引记录上的锁释放掉。这个过程如下图所示： 使用DELETE …来为记录加锁，比方说：123DELETE FROM hero WHERE name &gt;= &#x27;c曹操&#x27;;DELETE FROM hero WHERE name &lt;= &#x27;c曹操&#x27;; 如果这两个语句采用二级索引来进行锁定读，那么它们的加锁情况和更新带有二级索引列的UPDATE语句一致。 5）全表扫描比方说： 1SELECT * FROM hero WHERE country = &#x27;魏&#x27; LOCK IN SHARE MODE; ​ 由于country列上未建索引，所以只能采用全表扫描的方式来执行这条查询语句，存储引擎每读取一条聚簇索引记录，就会为这条记录加锁一个S型正常记录锁，然后返回给server层，如果server层判断country = ‘魏’这个条件是否成立，如果成立则将其发送给客户端，否则会释放掉该记录上的锁，画个图就像这样： 使用SELECT … FOR UPDATE进行加锁的情况与上边类似，只不过加的是X型正经记录锁。​ 对于UPDATE …和DELETE …的语句来说，在遍历聚簇索引中的记录，都会为该聚簇索引记录加上X型正经记录锁，然后：​ 如果该聚簇索引记录不满足条件，直接把该记录上的锁释放掉。 如果该聚簇索引记录满足条件，则会对相应的二级索引记录加上X型正经记录锁（DELETE语句会对所有二级索引列加锁，UPDATE语句只会为更新的二级索引列对应的二级索引记录加锁）。2.2 REPEATABLE READ隔离级别下​ 采用加锁的方式解决并发事务产生的问题时，REPEATABLE READ隔离级别与READ UNCOMMITTED和READ COMMITTED这两个隔离级别相比，最主要的就是要解决幻读问题，幻读问题的解决还得靠gap锁。 1）使用主键进行等值查询 使用SELECT … LOCK IN SHARE MODE来为记录加锁，比方说：1SELECT * FROM hero WHERE number = 8 LOCK IN SHARE MODE; 主键具有唯一性，如果在一个事务中第一次执行上述语句时将得到的结果集中包含一条记录，第二次执行上述语句前肯定不会有别的事务插入多条number值为8的记录（主键具有唯一性），也就是说一个事务中两次执行上述语句并不会发生幻读，这种情况下和READ UNCOMMITTED／READ COMMITTED隔离级别下一样，我们只需要为这条number值为8的记录加一个S型正经记录锁就好了，如图所示：但是如果我们要查询主键值不存在的记录，比方说：1SELECT * FROM hero WHERE number = 7 LOCK IN SHARE MODE; 由于number值为7的记录不存在，为了禁止幻读现象（也就是避免在同一事务中下一次执行相同语句时得到的结果集中包含number值为7的记录），在当前事务提交前我们需要预防别的事务插入number值为7的新记录，所以需要在number值为8的记录上加一个gap锁，也就是不允许别的事务插入number值在(3, 8)这个区间的新记录。如果在READ UNCOMMITTED／READ COMMITTED隔离级别下一样查询了一条主键值不存在的记录，那么什么锁也不需要加，因为在READ UNCOMMITTED／READ COMMITTED隔离级别下，并不需要禁止幻读问题。 其余语句使用主键进行等值查询的情况与READ UNCOMMITTED／READ COMMITTED隔离级别下的情况类似。 2）使用主键进行范围查询 使用SELECT … LOCK IN SHARE MODE语句来为记录加锁，比方说：1SELECT * FROM hero WHERE number &gt;= 8 LOCK IN SHARE MODE; 因为要解决幻读问题，所以需要禁止别的事务插入number值符合number &gt;= 8的记录，又因为主键本身就是唯一的，所以不用担心在number值为8的前边有新记录插入，只需要保证不要让新记录插入到number值为8的后边就好了，所以：​ 为number值为8的聚簇索引记录加一个S型正经记录锁。 为number值大于8的所有聚簇索引记录都加一个S型next-key锁（包括Supremum伪记录）。 为什么不给Supremum记录加gap锁，而要加next-key锁呢？其实InnoDB在处理Supremum记录上加的next-key锁时就是当作gap锁看待的，为了节省锁结构（锁的类型不一样的话不能被放到一个锁结构中）。 ​ 与READ UNCOMMITTED/READ COMMITTED隔离级别类似，在REPEATABLE READ隔离级别下，下边这个范围查询也是有点特殊： 1SELECT * FROM hero WHERE number &lt;= 8 LOCK IN SHARE MODE; ​ 这个语句在READ UNCOMMITTED/READ COMMITTED隔离级别下，这个语句会为number值为1、3、8、15这4条记录都加上S型正经记录锁，然后由于number值为15的记录不满足边界条件number &lt;= 8，随后便把这条记录的锁释放掉。在REPEATABLE READ隔离级别下的加锁过程与之类似，不过会为1、3、8、15这4条记录都加上S型next-key锁，但是注意：REPEATABLE READ隔离级别下，在判断number值为15的记录不满足边界条件 number &lt;= 8 后，并不会去释放加在该记录上的锁！！！ 所以在REPEATABLE READ隔离级别下，该语句的加锁示意图就如下所示： 这样如果别的事务想要插入的新记录的number值在(-∞, 1)、(1, 3)、(3, 8)、(8, 15)之间的话，是会进入等待状态的。​ 很显然这么粗暴的做法导致的一个后果就是别的事务竟然不允许插入number值在(8, 15)这个区间中的新记录，甚至不允许别的事务再获取number值为15的记录上的锁，而理论上只需要禁止别的事务插入number值在(-∞, 8)之间的新记录就好。 ​ 使用SELECT … FOR UPDATE语句来为记录加锁：和SELECT … LOCK IN SHARE MODE语句类似，只不过需要将上边提到的S型next-key锁替换成X型next-key锁。 使用UPDATE …来为记录加锁：如果UPDATE语句未更新二级索引列，比方说：1UPDATE hero SET country = &#x27;汉&#x27; WHERE number &gt;= 8; 这条UPDATE语句并没有更新二级索引列，加锁方式和上边所说的SELECT … FOR UPDATE语句一致。如果UPDATE语句中更新了二级索引列，比方说：1UPDATE hero SET name = &#x27;cao曹操&#x27; WHERE number &gt;= 8; 对聚簇索引记录加锁的情况和SELECT … FOR UPDATE语句一致，也就是对number值为8的聚簇索引记录加X型正经记录锁，对number值15、20的聚簇索引记录以及Supremum记录加X型next-key锁。但是因为也要更新二级索引idx_name，所以也会对number值为8、15、20的聚簇索引记录对应的idx_name二级索引记录加X型正经记录锁，画个图表示一下：如果是下边这个语句： 1UPDATE hero SET name = &#x27;cao曹操&#x27; WHERE number &lt;= 8; 则会对number值为1、3、8、15的聚簇索引记录加X型next-key，其中number值为15的聚簇索引记录不满足number &lt;= 8的边界条件，虽然在REPEATABLE READ隔离级别下不会将它的锁释放掉，但是也并不会对这条聚簇索引记录对应的二级索引记录加锁，也就是说只会为number值为1、3、8的聚簇索引记录对应的idx_name二级索引记录加X型正经记录锁，加锁示意图如下所示： 使用DELETE …来为记录加锁，比方说：123DELETE FROM hero WHERE number &gt;= 8;DELETE FROM hero WHERE number &lt;= 8; 这两个语句的加锁情况和更新带有二级索引列的UPDATE语句一致。​ 3）使用唯一二级索引进行等值查询​ 由于hero表并没有唯一二级索引，我们把原先的idx_name修改为一个唯一二级索引uk_name： 1ALTER TABLE hero DROP INDEX idx_name, ADD UNIQUE KEY uk_name (name); 使用SELECT … LOCK IN SHARE MODE语句来为记录加锁，比方说：1SELECT * FROM hero WHERE name = &#x27;c曹操&#x27; LOCK IN SHARE MODE; 由于唯一二级索引具有唯一性，如果在一个事务中第一次执行上述语句时将得到一条记录，第二次执行上述语句前肯定不会有别的事务插入多条name值为’c曹操’的记录（二级索引具有唯一性），也就是说一个事务中两次执行上述语句并不会发生幻读，这种情况下和READ UNCOMMITTED／READ COMMITTED隔离级别下一样，我们只需要为这条name值为’c曹操’的二级索引记录加一个S型正经记录锁，然后再为它对应的聚簇索引记录加一个S型正经记录锁就好了。注意加锁顺序，是先对二级索引记录加锁，再对聚簇索引加锁。如果对唯一二级索引列进行等值查询的记录并不存在，比如：1SELECT * FROM hero WHERE name = &#x27;g关羽&#x27; LOCK IN SHARE MODE; 为了禁止幻读，所以需要保证别的事务不能再插入name值为’g关羽’的新记录。在唯一二级索引uk_name中，键值比’g关羽’大的第一条记录的键值为l刘备，所以需要在这条二级索引记录上加一个gap锁，如图所示：注意，这里只对二级索引记录进行加锁，并不会对聚簇索引记录进行加锁。 使用SELECT … FOR UPDATE语句来为记录加锁，比如：和SELECT … LOCK IN SHARE MODE语句类似，只不过加的是X型正经记录锁。 使用UPDATE …来为记录加锁，比方说：与SELECT … FOR UPDATE的加锁情况类似，不过如果被更新的列中还有别的二级索引列的话，这些对应的二级索引记录也会被加X型正经记录锁。 使用DELETE …来为记录加锁，比方说：与SELECT … FOR UPDATE的加锁情况类似，不过如果表中还有别的二级索引列的话，这些对应的二级索引记录也会被加X型正经记录锁。4）使用唯一二级索引进行范围查询 使用SELECT … LOCK IN SHARE MODE语句来为记录加锁，比方说：1SELECT * FROM hero FORCE INDEX(uk_name) WHERE name &gt;= &#x27;c曹操&#x27; LOCK IN SHARE MODE; 这个语句的执行过程其实是先到二级索引中定位到满足name &gt;= ‘c曹操’的第一条记录，也就是name值为c曹操的记录，然后就可以沿着由记录组成的单向链表一路向后找。从二级索引idx_name的示意图中可以看出，所有的用户记录都满足name &gt;= ‘c曹操’的这个条件，所以所有的二级索引记录都会被加S型next-key锁，它们对应的聚簇索引记录也会被加S型正经记录锁，二级索引的最后一条Supremum记录也会被加S型next-key锁。不过需要注意一下加锁顺序，对一条二级索引记录加锁完后，会接着对它响应的聚簇索引记录加锁，完后才会对下一条二级索引记录进行加锁，以此类推～ 画个图表示一下就是这样：唯一二级索引本身就能保证其自身的值是唯一的，那为啥还要给name值为’c曹操’的记录加上S型next-key锁，而不是S型正经记录锁呢？其实我也不知道，按理说只需要给这条二级索引记录加S型正经记录锁就好了，我也没想明白InnoDB是怎么想的。​ 再来看下边这个语句： 1SELECT * FROM hero WHERE name &lt;= &#x27;c曹操&#x27; LOCK IN SHARE MODE; 这个语句先会为name值为’c曹操’的二级索引记录加S型next-key锁以及它对应的聚簇索引记录加S型正经记录锁。然后还要给name值为’l刘备’的二级索引记录加S型next-key锁，name值为’l刘备’的二级索引记录不满足索引条件下推的name &lt;= ‘c曹操’条件，压根儿不会释放掉该记录的锁就直接报告server层查询完毕了。这样可以禁止其他事务插入name值在(‘c曹操’, ‘l刘备’)之间的新记录，从而防止幻读产生。所以这个过程的加锁示意图如下：InnoDB在这里给name值为’l刘备’的二级索引记录加的是S型next-key锁，而不是简单的gap锁。 使用SELECT … FOR UPDATE语句来为记录加锁：和SELECT … LOCK IN SHARE MODE语句类似，只不过加的是X型正经记录锁。 使用UPDATE …来为记录加锁，比方说：1UPDATE hero SET country = &#x27;汉&#x27; WHERE name &gt;= &#x27;c曹操&#x27;; 假设该语句执行时使用了uk_name二级索引来进行锁定读（如果二级索引扫描的记录太多，也可能因为成本过大直接使用全表扫描的方式进行锁定读），而这条UPDATE语句并没有更新二级索引列，那么它的加锁方式和上边所说的SELECT … FOR UPDATE语句一致。如果有其他二级索引列也被更新，那么也会为这些二级索引记录进行加锁，就不赘述了。不过还需要强调一种情况，比方说：1UPDATE hero SET country = &#x27;汉&#x27; WHERE name &lt;= &#x27;c曹操&#x27;; 索引条件下推这个特性只适用于SELECT语句，也就是说UPDATE语句中无法使用，无法使用索引条件下推这个特性时需要先进行回表操作，那么这个语句就会为name值为’c曹操’和’l刘备’的二级索引记录加X型next-key锁，对它们对应的聚簇索引记录进行加X型正经记录锁。不过之后在判断边界条件时，虽然name值为’l刘备’的二级索引记录不符合name &lt;= ‘c曹操’的边界条件，但是在REPEATABLE READ隔离级别下并不会释放该记录上加的锁，整个过程的加锁示意图就是： 使用DELETE …来为记录加锁，比方说：123DELETE FROM hero WHERE name &gt;= &#x27;c曹操&#x27;;DELETE FROM hero WHERE name &lt;= &#x27;c曹操&#x27;; 如果这两个语句采用二级索引来进行锁定读，那么它们的加锁情况和更新带有二级索引列的UPDATE语句一致。 5）使用普通二级索引进行等值查询​ 我们再把上边的唯一二级索引uk_name改回普通二级索引idx_name： 1ALTER TABLE hero DROP INDEX uk_name, ADD INDEX idx_name (name); 使用SELECT … LOCK IN SHARE MODE语句来为记录加锁，比方说：1SELECT * FROM hero WHERE name = &#x27;c曹操&#x27; LOCK IN SHARE MODE; 由于普通的二级索引没有唯一性，所以一个事务在执行上述语句之后，要阻止别的事务插入name值为’c曹操’的新记录，InnoDB采用下边的方式对上述语句进行加锁：​ 对所有name值为’c曹操’的二级索引记录加S型next-key锁，它们对应的聚簇索引记录加S型正经就锁。 对最后一个name值为’c曹操’的二级索引记录的下一条二级索引记录加gap锁。 ​ 所以整个加锁示意图就如下所示： 如果对普通二级索引等值查询的值并不存在，比如： 1SELECT * FROM hero WHERE name = &#x27;g关羽&#x27; LOCK IN SHARE MODE; 加锁方式和我们上边介绍过的唯一二级索引的情况是一样的。 使用SELECT … FOR UPDATE语句来为记录加锁，比如：和SELECT … LOCK IN SHARE MODE语句类似，只不过加的是X型正经记录锁。 使用UPDATE …来为记录加锁，比方说：与SELECT … FOR UPDATE的加锁情况类似，不过如果被更新的列中还有别的二级索引列的话，这些对应的二级索引记录也会被加锁。 使用DELETE …来为记录加锁，比方说：与SELECT … FOR UPDATE的加锁情况类似，不过如果表中还有别的二级索引列的话，这些对应的二级索引记录也会被加锁。 ​ 6）使用普通二级索引进行范围查询​ 与唯一二级索引的加锁情况类似。​ 7）全表扫描比方说： 1SELECT * FROM hero WHERE country = &#x27;魏&#x27; LOCK IN SHARE MODE; ​ 由于country列上未建索引，所以只能采用全表扫描的方式来执行这条查询语句，存储引擎每读取一条聚簇索引记录，就会为这条记录加锁一个S型next-key锁，然后返回给server层，如果server层判断country = ‘魏’这个条件是否成立，如果成立则将其发送给客户端，否则会向InnoDB存储引擎发送释放掉该记录上的锁的消息，不过在REPEATABLE READ隔离级别下，InnoDB存储引擎并不会真正的释放掉锁，所以聚簇索引的全部记录都会被加锁，并且在事务提交前不释放。 全部记录都被加了next-key锁！此时别的事务别说想向表中插入啥新记录了，就是对某条记录加X锁都不可以，这种情况下会极大影响访问该表的并发事务处理能力，所以如果可能的话，尽可能为表建立合适的索引。​ 使用SELECT … FOR UPDATE进行加锁的情况与上边类似，只不过加的是X型正经记录锁，就不赘述了。对于UPDATE …语句来说，加锁情况与SELECT … FOR UPDATE类似，不过如果被更新的列中还有别的二级索引列的话，这些对应的二级索引记录也会被加X型正经记录锁。和DELETE …的语句来说，加锁情况与SELECT … FOR UPDATE类似，不过如果表中还有别的二级索引列的话，这些对应的二级索引记录也会被加X型正经记录锁。 3.半一致读的语句​ 半一致性读是一种夹在一致性读和锁定读之间的读取方式。当隔离级别不大于读已提交且执行update语句时将使用半一致性读。所谓半一致性读，就是当update语句读取到已经被其他记录加了X锁的记录的时候，InnoDB会将该记录的最新提交版本读出来，然后判断该版本是否与update语句中的搜索条件相匹配。如果不匹配，则不对该记录加锁，从而跳到下一条记录；如果匹配，则再次读取该记录并对其进行加锁。这样处理只是为了让update语句尽量少被别的语句阻塞。​ 假如事务T1的隔离级别为读已提交，T1执行了下面这条语句：​ 1select * from hero where number =8 for update; 该语句在执行时对number值为8的聚簇索引记录加了X型记录锁，此时隔离级别也为读已提交的事务T2执行了如下语句：​ 1update hero set name = &#x27;cao曹操&#x27; where number &gt;=8 and number &lt;20 and country !=&#x27;魏&#x27;； 该语句在执行时需要依次获取number值为8，15，20的聚簇索引记录的X型记录锁。由于T1已经获取了number值为8的聚簇索引记录的X型锁，按理说此时事务T2应该由于获取不到number值为8的聚簇索引的X型记录锁而阻塞。但是由于进行的是半一致读，所以存储引擎会先获取number=8的聚簇索引记录最新提交的版本并返回server层。该版本的country =’魏’,很显然不符合country!=’魏’的条件，所以server层决定放弃获取number=8的聚簇索引记录上的X锁，转而让存储引擎读取下一条记录。 4.insert语句​ insert语句在一般情况下不需要在内存中生成锁结构，并单纯依靠隐式锁保护插入的记录。不过当事务在插入一条记录前，需要先定位到该记录在B+树中的位置。如果该位置的下一条记录已经被加了意向锁，那么当前事务会为该记录加上一种类型为插入意向锁的锁，并且事务进入等待状态。​ 分析下在执行insert语句的时候，在内存中生成锁结构的两种特殊情况。​ 4.1 遇到重复键在插入一条新记录的时候，首先要做的就是确定这条记录应该插入到B+树的哪个位置。如果在确定位置时发现现有记录的主键或者唯一二级索引与待插入记录的主键或者唯一二级索引列相同，此时会报错。​ 当然在生成报错信息前，其实会对聚簇索引中number值为20的记录加S锁。不过加的锁的具体类型在不同隔离级别下是不一样的：​ 当隔离级别&lt;=读未提交时，加的是S记录锁 当隔离级别&gt;=可重复读时，加的是S型 next-key锁 ​ 如果是唯一索引列的值重复，也会报错。不过在报错之前会为已经存在的那条记录的二级索引记录加一个S锁。不论是什么隔离级别，如果再插入新记录的时候遇到唯一二级索引列重复，都会对已经在B+树中的那条唯一二级索引记录加next-key锁。 按理来说，在读未提交&amp;读已提交隔离级别下，不应该出现next-key锁，这主要是考虑到如果只加记录锁，可能出现多条记录的唯一二级索引列值都相同的情况。 另外，在使用insert...on duplicate key 这样的语法来插入记录的时候，如果遇到的主键或者唯一二级索引列的值重复，会对B+树中已经存在的相同键值的记录加X锁，而不是S锁。​ 4.2 外键检查 待插入记录的外键值在主表能找到 在插入成功之前，无论当前事务的隔离级别是什么，只需要直接给主表外键值对应的记录加一个S型记录锁即可。 待插入记录的外键值在主表找不到 此时会插入失败，但是在这个过程中需要根据隔离级别对主表中外键的值的聚簇索引记录进行加锁： 当隔离级别&lt;=读已提交的时候，并不对记录加锁 当隔离级别&gt;=可重复读的时候，加的是意向锁。 二，查看事务加锁情况1.获取锁信息-information_schema在 information_schema数据库中，有几个与事务和锁紧密相关的表，具体如下。​ INNODB_TRX:该表存储了InnoDB 存储引擎当前正在执行的事务信息，包括事务id (如果没有为该事务分配唯一的事务id 则会输出该事务对应的内存结构的指针)、事务状态(比如事务是正在运行还是在等拿待获取某个锁、事务正在执行的语句、事务是何时开启的)等。 ​ 比如我们可以在一个会话中执行事务T1: 1234# 事务T1begin;select * from hero where number =8 for update; number name country 8 c曹操 魏 然后再到另一个会话中查询INNODB_TRX表： 1select * from information_schema.INNODB_TRX\\G 从执行结果可以看到，当前系统中有一个事务id = 46671的事务，他的状态为正在运行，隔离级别为可重复读。​ innodb_locks :该表记录了一些锁信息，主要包括下面两个方面的锁信息； 如果一个事务想要获得某个锁但是没获取到，则记录该锁的信息 如果一个事务获取到了某个锁，但是这个锁阻塞了别的事务，则记录该锁信息 ​ 刚好刚才在事务T1中执行了一个加锁语句，现在来查询下innodb_locks： 1select * from information_schema.innodb_locks; 结果什么都没有！只有当系统中发生了某个事务因为获取不到锁而被阻塞的情况时，该表中才会有记录。​ 再到另一个会话中开启事务T2，然后执行： 123begin;select * from hero where number =8 for update; # 进入阻塞状态 此时再次查询innodb_locks，可以看到trx_id=46672&amp;46671的两个事务被显示出来了，但是我们无法仅凭上述内容区分到底是谁获取到了其他事物需要的锁，以及谁因为没有获取到锁而阻塞。可以到innodb_lock_waits表查看更多信息。​ innodb_lock_waits：表明每一个阻塞的事务是因为获取不到哪个事务持有的锁而阻塞。接着上面T2因为获取不到T1的锁而阻塞的例子，查询一下innodb_lock_waits表：1select * from information_schema.innodb_lock_waits; requesting_trx_id requested_lock_id blocking_trx_id blocking_lock_id 46672 46672:202:3:4 46671 46671:202:3:4 requesting_trx_id:因为获取不到锁而被阻塞的事务的事务idblocking_trx_id：表示因为获取不到别的事务需要的锁而导致其被阻塞的事务的事务id​ 在本例中：requesting_trx_id = T2，blocking_trx_id=T1。 ​ 这两张表在MySQL8.0被干掉了。 2.获取锁信息-show eninge innodb status现在假设前文使用的T1，T2事务都提交了，我们在新开启几个事务： 12345# 可重复读隔离级别begin;select * from hero force index(idx_name) where name &gt; &#x27;c曹操&#x27; and name &lt;=&#x27;x荀彧&#x27; and country != &#x27;吴&#x27; order by name desc for update; number name country 15 x荀彧 魏 1 l刘备 蜀 ​ 可以直接通过show eninge innodb status 语句获取当前系统中各个事务的加锁情况。​ 上述命令输出信息过多，可以使用 set global innodb_status_output_locks = on;​ 如果某个事务没有被分配唯一的事务id，则执行SHOW ENGINE INNODB STATUS 语句时并不会显示该事务在执行过程中持有的锁。比如，事务T4只执行了SELECT*FROM hero WHERE number=1 LOCK IN SHARE MODE 语句，那么事务T4所持有的锁是不显示 的。另外，SHOW ENGINE INNODB STATUS 不显示隐式锁。​ 另外，我们在SHOW ENGINE INNODB TATUS的输出中可以看到，hero表的 number列的值都是“800000XX”的形式，这是 因为number列是存储有符号数的(也就是既可以存储负数，也可以存储非负数)，InnoDB规定在储存有符号数的时候需要将首位置为1。 三，死锁假设我们开启了两个事物T1&amp;T2，他们的具体执行流程如下：​ 发生时间编号 T1 T2 ① begin ② begin ③ select * from hero where number=1 for update; ④ select * from hero where number=3 for update; ⑤ select * from hero where number=3 for update;(此操作阻塞) ⑥ select * from hero where number=1 for update;（死锁发生，记录日志，服务器回滚一个事务） ​ innodb有一个死锁检测机制，当它检测到死锁发生时，会选择一个较小的事务进行回滚，并向客户端发送一条消息： 1尝试获取锁的时候发生死锁，重启事务。 从上面的例子可以看出：当不同的事务以不同的顺序获取某些记录的锁时，可能会发生死锁。当死锁发生时，InnoDB会回滚一个事务以释放掉该事务所获取的锁。​ 我们有必要找出那些发生死锁的语句，通过优化语句来改变加锁顺序，或者建立合适的索引以改变加锁过程，从而避免死锁问题。不过，在实际应用中我们可能压根儿不知道哪些语句发生了死锁，因此需要根据死锁发生时的死锁日志来逆向定位产生死锁的语句，然后再优化业务。​ show engine innodb status 只会显示最近一次发生的死锁信息。如果死锁频繁出现，可以将全局系统变量innodb_print_all_deadlocks设置为ON,这样可以将每个死锁发生时的信息都记录在MySQL的错误日志中，然后就可以通过查看错误日志来分析更多的死锁情况了。 四，总结​ MVCC 和加锁是解决并发事务带来的一致性问题的两种方式。​ 共享锁简称为S锁，独占锁简称为X锁。S锁与S锁兼容;锁与S锁不兼容，与X锁也不兼容。​ 事务利用 MVCC 进行的读取操作称为一致性读，在读取记录前加锁的读取操作称为锁定读。InnoDB 规定下面两种语法来进行锁定读:​ SELECT..LOCK IN SHARE MODE 语句为读取的记录加S锁; SELECT…FOR UPDATE 语句为读取的记录加X锁。 ​ INSERT语句一般情况下不需要在内存中生成锁结构，并单纯依靠隐式锁保护插入的记录。UPDATE和DELETE语句在执行过程中，在B+树中定位到待改动记录并给该记录加锁的过程也算是一个锁定读。​ IS、IX锁是表级锁，它们的提出仅仅为了在之后加表级别的S锁和X锁时，可以快速判断表中的记录是否被上锁，以避免用遍历的方式来查看表中有没有上锁的记录。​ InnoDB 中的行级锁类型有下面这些。 Record Lock:被我们称为正经记录锁，只对记录本身加锁。 Gap Lock:锁住记录前的间隙，防止别的事务向该间隙插入新记录。 Next-Key Lock: Record Lock 和Gap Lock的结合体，既保护记录本身，也防止别的事务向该间隙插入新记录。 Insert Intention Lock:很“鸡肋”的锁，仅仅是为了解决“在当前事务插入记录时因碰到别的事务加的gap锁而进入等待状态时，也生成一个锁结构”而提出的。某个事务获取一条记录的该类型的锁后，不会阻止别的事务继续获取该记录上任何类型的锁。 隐式锁:依靠记录的trx_id属性来保护不被别的事务改动该记录。 ​ InnoDB存储引擎的锁都在内存中对应着一个锁结构。有时为了节省锁结构，会把符合下面条件的锁放到同一个锁结构中: 在同一个事务中进行加锁操作 被加锁的记录在同一个页面中 加锁的类型是一样的 等待状态是一样的 ​ 语句加锁的情况受到所在事务的隔离级别、语句执行时使用的索引类型、是否是精确匹配、是否是唯一性搜索、具体执行的语句类型等情况的制约，需要具体情况具体分析。​ 可以通过information_schema 数据库下的 INNODB_TRX、INNODB LOCKS、INNODB LOCK WAITS 表来查看事务和锁的相关信息，也可以通过 SHOW ENGINE INNODB STATUS语句查看事务和锁的相关信息。​ 不同事务由于互相持有对方需要的锁而导致事务都无法继续执行的情况称为死锁。死锁发生时，InnoDB会选择一个较小的事务进行回滚。可以通过查看死锁日志来分析死锁发生过程。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/tags/MySQL/"}]},{"title":"MySQL[十七]锁概述","slug":"MySQL/MySQL[十七]锁概述","date":"2022-01-16T16:00:00.000Z","updated":"2022-01-13T01:58:03.600Z","comments":true,"path":"2022/01/17/MySQL/MySQL[十七]锁概述/","link":"","permalink":"https://yinhuidong.github.io/2022/01/17/MySQL/MySQL[%E5%8D%81%E4%B8%83]%E9%94%81%E6%A6%82%E8%BF%B0/","excerpt":"","text":"一，并发事务带来的问题并发事务访问相同记录的情况大致可以划分为3种： 读&amp;读：并发事务相继读取相同的记录。读取操作本身不会对记录有任何影响，不会引起什么问题，所以允许这种情况发生 写&amp;写：并发事务相继对相同的记录进行改动 读&amp;写/写&amp;读：一个事务进行读取操作，另一个事务进行改动操作 1.写&amp;写情况一个事务修改了另一个事务未提交的数据就是脏写。所有的隔离级别都不允许这种情况。具体的处理方式其实就是通过加锁排队实现的。锁本质上是内存中的一个结构，在事务执行之前本身是没有锁的，当一个事务对记录进行改动的时候，首先会判断内存中有没有与这条记录关联的锁结构：没有就生成一个锁结构与记录关联。 假如现在事务T1要对一条记录进行写操作，就需要生成一个锁结构与之关联。 暂时先看锁结构里面的两个重要信息： trx信息：表示这个锁结构是与哪个事务关联 is_waiting:表示当前事务是否在等待 当事务T1改动了这条记录后，就生成了一个锁结构与该记录关联，因为之前没有别的事务为这条记录加锁，所以is_waiting属性就是false，这个场景就称之为获取锁成功，或者加锁成功，然后就可以继续执行操作了。 在事务T1提交之前，另一个事务T2也想对该记录做改动，那么先去看看有没有锁结构与这条记录关联，发现有一个锁结构与之关联后，然后也生成了一个锁结构与这条记录关联，不过锁结构的is_waiting属性值为true，表示当前事务需要等待，这个场景就是获取锁失败，或者加锁失败，或者没有成功的获取到锁： 事务T1提交之后，就会把该事务生成的锁结构释放掉，然后看看还有没有别的事务在等待获取锁，发现了事务T2还在等待获取锁，所以把事务T2对应的锁结构的is_waiting属性设置为false，然后把该事务对应的线程唤醒，让它继续执行，此时事务T2就算获取到锁了。 2.读&amp;写情况读-写或写-读情况：也就是一个事务进行读取操作，另一个进行改动操作。这种情况下可能发生脏读、不可重复读、幻读的问题。 幻读问题的产生是因为某个事务读了一个范围的记录，之后别的事务在该范围内插入了新记录，该事务再次读取该范围的记录时，可以读到新插入的记录，所以幻读问题准确的说并不是因为读取和写入一条相同记录而产生的。 怎么解决脏读、不可重复读、幻读这些问题呢？其实有两种可选的解决方案： 读操作利用多版本并发控制（MVCC），写操作进行加锁。 读、写操作都采用加锁的方式。 2.1 方案一MVCC就是通过生成一个ReadView，然后通过ReadView找到符合条件的记录版本（历史版本是由undo日志构建的），其实就像是在生成ReadView的那个时刻做了一次时间静止（就像用相机拍了一个快照），查询语句只能读到在生成ReadView之前已提交事务所做的更改，在生成ReadView之前未提交的事务或者之后才开启的事务所做的更改是看不到的。而写操作肯定针对的是最新版本的记录，读记录的历史版本和改动记录的最新版本本身并不冲突，也就是采用MVCC时，读-写操作并不冲突。 普通的SELECT语句在READ COMMITTED和REPEATABLE READ隔离级别下会使用到MVCC读取记录。在READ COMMITTED隔离级别下，一个事务在执行过程中每次执行SELECT操作时都会生成一个ReadView，ReadView的存在本身就保证了事务不可以读取到未提交的事务所做的更改，也就是避免了脏读现象；REPEATABLE READ隔离级别下，一个事务在执行过程中只有第一次执行SELECT操作才会生成一个ReadView，之后的SELECT操作都复用这个ReadView，这样也就避免了不可重复读和幻读的问题。 2.2 方案二如果一些业务场景不允许读取记录的旧版本，而是每次都必须去读取记录的最新版本，比方在银行存款的事务中，需要先把账户的余额读出来，然后将其加上本次存款的数额，最后再写到数据库中。在将账户余额读取出来后，就不想让别的事务再访问该余额，直到本次存款事务执行完成，其他事务才可以访问账户的余额。这样在读取记录的时候也就需要对其进行加锁操作，这样也就意味着读操作和写操作也像写-写操作那样排队执行。 脏读的产生是因为当前事务读取了另一个未提交事务写的一条记录，如果另一个事务在写记录的时候就给这条记录加锁，那么当前事务就无法继续读取该记录了，所以也就不会有脏读问题的产生了。 不可重复读的产生是因为当前事务先读取一条记录，另外一个事务对该记录做了改动之后并提交之后，当前事务再次读取时会获得不同的值，如果在当前事务读取记录时就给该记录加锁，那么另一个事务就无法修改该记录，自然也不会发生不可重复读了。 幻读问题的产生是因为当前事务读取了一个范围的记录，然后另外的事务向该范围内插入了新记录，当前事务再次读取该范围的记录时发现了新插入的新记录，新插入的那些记录就像是幻影记录。采用加锁的方式解决幻读问题比较麻烦，因为当前事务在第一次读取记录时那些幻影记录并不存在，所以读取的时候加锁就有点尴尬，因为你并不知道给谁加锁。 采用MVCC方式的话，读-写操作彼此并不冲突，性能更高，采用加锁方式的话，读-写操作彼此需要排队执行，影响性能。一般情况下我们当然愿意采用MVCC来解决读-写操作并发执行的问题，但是业务在某些特殊情况下，要求必须采用加锁的方式执行。 3.一致性读事务利用MVCC进行的读取操作称之为一致性读，或者一致性非锁读。所有普通的SELECT语句（plain SELECT）在READ COMMITTED、REPEATABLE READ隔离级别下都算是一致性读，比方说： 12SELECT * FROM t;SELECT * FROM t1 INNER JOIN t2 ON t1.col1 = t2.col2 一致性读并不会对表中的任何记录做加锁操作，其他事务可以自由的对表中的记录做改动。 4.锁定读4.1 共享锁&amp;独占锁 共享锁，英文名：Shared Locks，简称S锁。在事务要读取一条记录时，需要先获取该记录的S锁。 独占锁，也常称排他锁，英文名：Exclusive Locks，简称X锁。在事务要改动一条记录时，需要先获取该记录的X锁。 假如事务T1首先获取了一条记录的S锁之后，事务T2接着也要访问这条记录： 如果事务T2想要再获取一个记录的S锁，那么事务T2也会获得该锁，也就意味着事务T1和T2在该记录上同时持有S锁。 如果事务T2想要再获取一个记录的X锁，那么此操作会被阻塞，直到事务T1提交之后将S锁释放掉。 如果事务T1首先获取了一条记录的X锁之后，那么不管事务T2接着想获取该记录的S锁还是X锁都会被阻塞，直到事务T1提交。 所以说S锁和S锁是兼容的，S锁和X锁是不兼容的，X锁和X锁也是不兼容的： 兼容性 X S X 不兼容 不兼容 S 不兼容 兼容 4.2 锁定读的语句在采用加锁方式解决脏读、不可重复读、幻读这些问题时，读取一条记录时需要获取一下该记录的S锁，其实这是不严谨的，有时候想在读取记录时就获取记录的X锁，来禁止别的事务读写该记录，为此MySQL提出了两种比较特殊的SELECT语句格式： 对读取的记录加S锁：也就是在普通的SELECT语句后边加LOCK IN SHARE MODE，如果当前事务执行了该语句，那么它会为读取到的记录加S锁，这样允许别的事务继续获取这些记录的S锁（比方说别的事务也使用SELECT ... LOCK IN SHARE MODE语句来读取这些记录），但是不能获取这些记录的X锁（比方说使用SELECT ... FOR UPDATE语句来读取这些记录，或者直接修改这些记录）。如果别的事务想要获取这些记录的X锁，那么它们会阻塞，直到当前事务提交之后将这些记录上的S锁释放掉。 1SELECT ... LOCK IN SHARE MODE; 对读取的记录加X锁：也就是在普通的SELECT语句后边加FOR UPDATE，如果当前事务执行了该语句，那么它会为读取到的记录加X锁，这样既不允许别的事务获取这些记录的S锁（比方说别的事务使用SELECT ... LOCK IN SHARE MODE语句来读取这些记录），也不允许获取这些记录的X锁（比如说使用SELECT ... FOR UPDATE语句来读取这些记录，或者直接修改这些记录）。如果别的事务想要获取这些记录的S锁或者X锁，那么它们会阻塞，直到当前事务提交之后将这些记录上的X锁释放掉。 1SELECT ... FOR UPDATE; 5.写操作平常所用到的写操作无非是DELETE、UPDATE、INSERT这三种： DELETE：对一条记录做DELETE操作的过程其实是先在B+树中定位到这条记录的位置，然后获取一下这条记录的X锁，然后再执行delete mark操作。这个定位待删除记录在B+树中位置的过程看成是一个获取X锁的锁定读。 UPDATE：在对一条记录做UPDATE操作时分为三种情况： 如果未修改该记录的键值并且被更新的列占用的存储空间在修改前后未发生变化，则先在B+树中定位到这条记录的位置，然后再获取一下记录的X锁，最后在原记录的位置进行修改操作。这个定位待修改记录在B+树中位置的过程是一个获取X锁的锁定读。 如果未修改该记录的键值并且至少有一个被更新的列占用的存储空间在修改前后发生变化，则先在B+树中定位到这条记录的位置，然后获取一下记录的X锁，将该记录彻底删除掉（就是把记录彻底移入垃圾链表），最后再插入一条新记录。这个定位待修改记录在B+树中位置的过程看成是一个获取X锁的锁定读，新插入的记录由INSERT操作提供的隐式锁进行保护。 如果修改了该记录的键值，则相当于在原记录上做DELETE操作之后再来一次INSERT操作，加锁操作就需要按照DELETE和INSERT的规则进行了。 INSERT：一般情况下，新插入一条记录的操作并不加锁，InnoDB通过隐式锁来保护这条新插入的记录在本事务提交前不被别的事务访问。 在一个事务中加的锁一般在事务提交或终止时才会释放，当然也有一些特殊情况。 二，多粒度锁上面介绍的锁都是针对记录的，也可以被称之为行级锁或者行锁，对一条记录加锁影响的也只是这条记录而已，我们就说这个锁的粒度比较细；其实一个事务也可以在表级别进行加锁，自然就被称之为表级锁或者表锁，对一个表加锁影响整个表中的记录，我们就说这个锁的粒度比较粗。给表加的锁也可以分为共享锁（S锁）和独占锁（X锁）： 给表加S锁：如果一个事务给表加了S锁，那么： 别的事务可以继续获得该表的S锁 别的事务可以继续获得该表中的某些记录的S锁 别的事务不可以继续获得该表的X锁 别的事务不可以继续获得该表中的某些记录的X锁 给表加X锁：如果一个事务给表加了X锁（意味着该事务要独占这个表），那么： 别的事务不可以继续获得该表的S锁 别的事务不可以继续获得该表中的某些记录的S锁 别的事务不可以继续获得该表的X锁 别的事务不可以继续获得该表中的某些记录的X锁 思考一下： 如果我们想对表上S锁，首先需要确保表中的没有记录加了锁，如果有加锁记录，需要等到锁释放才可以对表上S锁。 如果想对表整体上X锁，首先需要确保表中没有加行锁，如果有加行锁的记录，需要等到全部行锁释放才可以对表上X锁。 在上锁（表锁）时，怎么知道表已经被上锁（行锁）了呢？InnoDB提出了意向锁： 意向共享锁，英文名：Intention Shared Lock，简称IS锁。当事务准备在某条记录上加S锁时，需要先在表级别加一个IS锁。 意向独占锁，英文名：Intention Exclusive Lock，简称IX锁。当事务准备在某条记录上加X锁时，需要先在表级别加一个IX锁。 总结一下：IS、IX锁是表级锁，它们的提出仅仅为了在之后加表级别的S锁和X锁时可以快速判断表中的记录是否被上锁，以避免用遍历的方式来查看表中有没有上锁的记录，也就是说其实IS锁和IX锁是兼容的，IX锁和IX锁是兼容的。 兼容性 X IX S IS X 不兼容 不兼容 不兼容 不兼容 IX 不兼容 兼容 不兼容 兼容 S 不兼容 不兼容 兼容 兼容 IS 不兼容 兼容 兼容 兼容 三，MySQL中的行锁&amp;表锁MySQL支持多种存储引擎，不同存储引擎对锁的支持也是不一样的。我们重点还是讨论InnoDB存储引擎中的锁。 1.InnoDB存储引擎中的锁1.1 InnoDB中的表级锁 表级别的S锁、X锁在对某个表执行SELECT、INSERT、DELETE、UPDATE语句时，InnoDB存储引擎是不会为这个表添加表级别的S锁或者X锁的。另外，在对某个表执行一些诸如ALTER TABLE、DROP TABLE这类的DDL语句时，其他事务对这个表并发执行诸如SELECT、INSERT、DELETE、UPDATE的语句会发生阻塞，同理，某个事务中对某个表执行SELECT、INSERT、DELETE、UPDATE语句时，在其他会话中对这个表执行DDL语句也会发生阻塞。这个过程其实是通过在server层使用元数据锁（英文名：Metadata Locks，简称MDL）来实现，一般情况下也不会使用InnoDB存储引擎自己提供的表级别的S锁和X锁。其实这个InnoDB存储引擎提供的表级S锁或者X锁是相当鸡肋，只会在一些特殊情况下，比方说崩溃恢复过程中用到。不过我们还是可以手动获取一下的，比方说在系统变量autocommit=0，innodb_table_locks = 1时，手动获取InnoDB存储引擎提供的表t的S锁或者X锁可以这么写： DDL语句执行时会隐式的提交当前会话中的事务，这主要是DDL语句的执行一般都会在若干个特殊事务中完成，在开启这些特殊事务前，需要将当前会话中的事务提交掉。 LOCK TABLES t READ：InnoDB存储引擎会对表t加表级别的S锁。 LOCK TABLES t WRITE：InnoDB存储引擎会对表t加表级别的X锁。 不过请尽量避免在使用InnoDB存储引擎的表上使用LOCK TABLES这样的手动锁表语句，它们并不会提供什么额外的保护，只是会降低并发能力而已。InnoDB的厉害之处还是实现了更细粒度的行锁。 表级别的IS锁、IX锁当我们在对使用InnoDB存储引擎的表的某些记录加S锁之前，那就需要先在表级别加一个IS锁，当我们在对使用InnoDB存储引擎的表的某些记录加X锁之前，那就需要先在表级别加一个IX锁。IS锁和IX锁的使命只是为了后续在加表级别的S锁和X锁时判断表中是否有已经被加锁的记录，以避免用遍历的方式来查看表中有没有上锁的记录。 表级别的AUTO-INC锁在使用MySQL过程中，我们可以为表的某个列添加AUTO_INCREMENT属性，之后在插入记录时，可以不指定该列的值，系统会自动为它赋上递增的值，比方说我们有一个表：由于这个表的id字段声明了AUTO_INCREMENT，也就意味着在书写插入语句时不需要为其赋值，比方说这样：上边的插入语句并没有为id列显式赋值，所以系统会自动为它赋上递增的值，效果就是这样：系统实现这种自动给AUTO_INCREMENT修饰的列递增赋值的原理主要是两个： 123456CREATE TABLE t ( id INT NOT NULL AUTO_INCREMENT, c VARCHAR(100), PRIMARY KEY (id)) Engine=InnoDB CHARSET=utf8;INSERT INTO t(c) VALUES(&#x27;aa&#x27;), (&#x27;bb&#x27;); mysql&gt; SELECT * FROM t;+—-+——+| id | c |+—-+——+| 1 | aa || 2 | bb |+—-+——+2 rows in set (0.00 sec) 采用AUTO-INC锁，也就是在执行插入语句时就在表级别加一个AUTO-INC锁，然后为每条待插入记录的AUTO_INCREMENT修饰的列分配递增的值，在该语句执行结束后，再把AUTO-INC锁释放掉。这样一个事务在持有AUTO-INC锁的过程中，其他事务的插入语句都要被阻塞，可以保证一个语句中分配的递增值是连续的。如果我们的插入语句在执行前不可以确定具体要插入多少条记录（无法预计即将插入记录的数量），比方说使用INSERT ... SELECT、REPLACE ... SELECT或者LOAD DATA这种插入语句，一般是使用AUTO-INC锁为AUTO_INCREMENT修饰的列生成对应的值。 这个AUTO-INC锁的作用范围只是单个插入语句，插入语句执行完成后，这个锁就被释放了，跟之前说的锁在事务结束时释放是不一样的。 采用一个轻量级的锁，在为插入语句生成AUTO_INCREMENT修饰的列的值时获取一下这个轻量级锁，然后生成本次插入语句需要用到的AUTO_INCREMENT列的值之后，就把该轻量级锁释放掉，并不需要等到整个插入语句执行完才释放锁。如果插入语句在执行前就可以确定具体要插入多少条记录，比方说我们上边的例子，在语句执行前就可以确定要插入2条记录，那么一般采用轻量级锁的方式对AUTO_INCREMENT修饰的列进行赋值。这种方式可以避免锁定表，可以提升插入性能。 InnoDB提供了一个称之为innodb_autoinc_lock_mode的系统变量来控制到底使用上述两种方式中的哪种来为AUTO_INCREMENT修饰的列进行赋值，当innodb_autoinc_lock_mode值为0时，一律采用AUTO-INC锁；当innodb_autoinc_lock_mode值为2时，一律采用轻量级锁；当innodb_autoinc_lock_mode值为1时，两种方式混着来（也就是在插入记录数量确定时采用轻量级锁，不确定时使用AUTO-INC锁）。不过当innodb_autoinc_lock_mode值为2时，可能会造成不同事务中的插入语句为AUTO_INCREMENT修饰的列生成的值是交叉的，在有主从复制的场景中是不安全的。 1.2 InnoDB中的行级锁行锁，也称为记录锁，有很多种类型。即使对同一条记录加行锁，如果类型不同，起到的功效也是不同的。 123456CREATE TABLE hero ( number INT, name VARCHAR(100), country varchar(100), PRIMARY KEY (number)) Engine=InnoDB CHARSET=utf8; 我们主要是想用这个表存储三国时的英雄，然后向这个表里插入几条记录： 123456INSERT INTO hero VALUES (1, &#x27;l刘备&#x27;, &#x27;蜀&#x27;), (3, &#x27;z诸葛亮&#x27;, &#x27;蜀&#x27;), (8, &#x27;c曹操&#x27;, &#x27;魏&#x27;), (15, &#x27;x荀彧&#x27;, &#x27;魏&#x27;), (20, &#x27;s孙权&#x27;, &#x27;吴&#x27;); 现在表里的数据就是这样的： 1234567891011mysql&gt; SELECT * FROM hero;+--------+------------+---------+| number | name | country |+--------+------------+---------+| 1 | l刘备 | 蜀 || 3 | z诸葛亮 | 蜀 || 8 | c曹操 | 魏 || 15 | x荀彧 | 魏 || 20 | s孙权 | 吴 |+--------+------------+---------+5 rows in set (0.01 sec) 为啥要在’刘备’、’曹操’、’孙权’前边加上’l’、’c’、’s’这几个字母呀？这个主要是因为我们采用utf8字符集，该字符集并没有对应的按照汉语拼音进行排序的比较规则，也就是说’刘备’、’曹操’、’孙权’这几个字符串的排序并不是按照它们汉语拼音进行排序的，在汉字前边加上了汉字对应的拼音的第一个字母，这样在排序时就是按照汉语拼音进行排序。 把hero表中的聚簇索引的示意图画一下： 现在准备工作做完了，下边看看都有哪些常用的行锁类型。 ①Record Locks（普通锁）前边提到的记录锁就是这种类型，也就是仅仅把一条记录锁上，正经记录锁。官方的类型名称为：LOCK_REC_NOT_GAP。比方说我们把number值为8的那条记录加一个正经记录锁的示意图如下： 正经记录锁是有S锁和X锁之分的，当一个事务获取了一条记录的S型正经记录锁后，其他事务也可以继续获取该记录的S型正经记录锁，但不可以继续获取X型正经记录锁；当一个事务获取了一条记录的X型正经记录锁后，其他事务既不可以继续获取该记录的S型正经记录锁，也不可以继续获取X型正经记录锁； ②Gap Locks（间歇锁）MySQL在REPEATABLE READ隔离级别下是可以解决幻读问题的，解决方案有两种，可以使用MVCC方案解决，也可以采用加锁方案解决。但是在使用加锁方案解决时有个大问题，就是事务在第一次执行读取操作时，那些幻影记录尚不存在，我们无法给这些幻影记录加上正经记录锁。不InnoDB提出了一种称之为Gap Locks的锁，官方的类型名称为：LOCK_GAP，简称为gap锁。 如图中为number值为8的记录加了gap锁，意味着不允许别的事务在number值为8的记录前边的间隙插入新记录，其实就是number列的值(3, 8)这个区间的新记录是不允许立即插入的。比方说有另外一个事务再想插入一条number值为4的新记录，它定位到该条新记录的下一条记录的number值为8，而这条记录上又有一个gap锁，所以就会阻塞插入操作，直到拥有这个gap锁的事务提交了之后，number列的值在区间(3, 8)中的新记录才可以被插入。 这个gap锁的提出仅仅是为了防止插入幻影记录而提出的，虽然有共享gap锁和独占gap锁这样的说法，但是它们起到的作用都是相同的。而且如果对一条记录加了gap锁（不论是共享gap锁还是独占gap锁），并不会限制其他事务对这条记录加正经记录锁或者继续加gap锁，**gap锁**的作用仅仅是为了防止插入幻影记录的而已。 给一条记录加了gap锁只是不允许其他事务往这条记录前边的间隙插入新记录，那对于最后一条记录之后的间隙，也就是hero表中number值为20的记录之后的间隙该咋办呢？也就是说给哪条记录加gap锁才能阻止其他事务插入number值在(20, +∞)这个区间的新记录呢？这时候应该想起我们在前边唠叨数据页时介绍的两条伪记录了： Infimum记录，表示该页面中最小的记录。 Supremum记录，表示该页面中最大的记录。 为了实现阻止其他事务插入number值在(20, +∞)这个区间的新记录，我们可以给索引中的最后一条记录，也就是number值为20的那条记录所在页面的Supremum记录加上一个gap锁，画个图就是这样： 这样就可以阻止其他事务插入number值在(20, +∞)这个区间的新记录。 ③Next-Key Locks（普通&amp;间歇）有时候我们既想锁住某条记录，又想阻止其他事务在该记录前边的间隙插入新记录，所以InnoDB就提出了一种称之为Next-Key Locks的锁，官方的类型名称为：LOCK_ORDINARY，我们也可以简称为next-key锁。比方说我们把number值为8的那条记录加一个next-key锁的示意图如下： next-key锁的本质就是一个正经记录锁和一个gap锁的合体，它既能保护该条记录，又能阻止别的事务将新记录插入被保护记录前边的间隙。 ④Insert Intention Locks（插入意向锁）一个事务在插入一条记录时需要判断一下插入位置是不是被别的事务加了所谓的gap锁（next-key锁也包含gap锁），如果有的话，插入操作需要等待，直到拥有gap锁的那个事务提交。但是InnoDB规定事务在等待的时候也需要在内存中生成一个锁结构，表明有事务想在某个间隙中插入新记录，但是现在在等待。InnoDB就把这种类型的锁命名为Insert Intention Locks，官方的类型名称为：LOCK_INSERT_INTENTION，我们也可以称为插入意向锁。 比方说我们把number值为8的那条记录加一个插入意向锁的示意图如下： 比方说现在T1为number值为8的记录加了一个gap锁，然后T2和T3分别想向hero表中插入number值分别为4、5的两条记录，所以现在为number值为8的记录加的锁的示意图就如下所示： 从图中可以看到，由于T1持有gap锁，所以T2和T3需要生成一个插入意向锁的锁结构并且处于等待状态。当T1提交后会把它获取到的锁都释放掉，这样T2和T3就能获取到对应的插入意向锁了（本质上就是把插入意向锁对应锁结构的is_waiting属性改为false），T2和T3之间也并不会相互阻塞，它们可以同时获取到number值为8的插入意向锁，然后执行插入操作。事实上插入意向锁并不会阻止别的事务继续获取该记录上任何类型的锁。 ⑤隐式锁一个事务在执行INSERT操作时，如果即将插入的间隙已经被其他事务加了gap锁，那么本次INSERT操作会阻塞，并且当前事务会在该间隙上加一个插入意向锁，否则一般情况下INSERT操作是不加锁的。那如果一个事务首先插入了一条记录（此时并没有与该记录关联的锁结构），然后另一个事务： 立即使用SELECT ... LOCK IN SHARE MODE语句读取这条记录，也就是在要获取这条记录的S锁，或者使用SELECT ... FOR UPDATE语句读取这条记录，也就是要获取这条记录的X锁，该咋办？如果允许这种情况的发生，那么可能产生脏读问题。 立即修改这条记录，也就是要获取这条记录的X锁，该咋办？如果允许这种情况的发生，那么可能产生脏写问题。 这时候事务id`又要起作用了。我们把聚簇索引和二级索引中的记录分开看一下： 对于聚簇索引记录来说，有一个trx_id隐藏列，该隐藏列记录着最后改动该记录的事务id。那么如果在当前事务中新插入一条聚簇索引记录后，该记录的trx_id隐藏列代表的的就是当前事务的事务id，如果其他事务此时想对该记录添加S锁或者X锁时，首先会看一下该记录的trx_id隐藏列代表的事务是否是当前的活跃事务，如果是的话，那么就帮助当前事务创建一个X锁（也就是为当前事务创建一个锁结构，is_waiting属性是false），然后自己进入等待状态（也就是为自己也创建一个锁结构，is_waiting属性是true）。 对于二级索引记录来说，本身并没有trx_id隐藏列，但是在二级索引页面的Page Header部分有一个PAGE_MAX_TRX_ID属性，该属性代表对该页面做改动的最大的事务id，如果PAGE_MAX_TRX_ID属性值小于当前最小的活跃事务id，那么说明对该页面做修改的事务都已经提交了，否则就需要在页面中定位到对应的二级索引记录，然后回表找到它对应的聚簇索引记录，然后再重复情景一的做法。 一个事务对新插入的记录可以不显式的加锁（生成一个锁结构），但是由于事务id的存在，相当于加了一个隐式锁。别的事务在对这条记录加S锁或者X锁时，由于隐式锁的存在，会先帮助当前事务生成一个锁结构，然后自己再生成一个锁结构后进入等待状态。 小贴士： 除了插入意向锁，在一些特殊情况下INSERT还会获取一些锁。 2.InnoDB锁的内存结构对一条记录加锁的本质就是在内存中创建一个锁结构与之关联，那么是不是一个事务对多条记录加锁，就要创建多个锁结构呢？比方说事务T1要执行下边这个语句： 12# 事务T1SELECT * FROM hero LOCK IN SHARE MODE; 很显然这条语句需要为hero表中的所有记录进行加锁，那是不是需要为每条记录都生成一个锁结构呢？其实理论上创建多个锁结构没问题，但是InnoDB决定在对不同记录加锁时，如果符合下边这些条件： 在同一个事务中进行加锁操作 被加锁的记录在同一个页面中 加锁的类型是一样的 等待状态是一样的 那么这些记录的锁就可以被放到一个锁结构中。InnoDB存储引擎中的锁结构： 锁所在的事务信息：不论是表锁还是行锁，都是在事务执行过程中生成的，哪个事务生成了这个锁结构，这里就记载着这个事务的信息。 实际上这个所谓的锁所在的事务信息在内存结构中只是一个指针而已，所以不会占用多大内存空间，通过指针可以找到内存中关于该事务的更多信息，比方说事务id是什么。下边介绍的所谓的索引信息其实也是一个指针。 索引信息：对于行锁来说，需要记录一下加锁的记录是属于哪个索引的。 表锁／行锁信息：表锁结构和行锁结构在这个位置的内容是不同的： 表锁：记载着这是对哪个表加的锁，还有其他的一些信息。 行锁：记载了三个重要的信息： Space ID：记录所在表空间。 Page Number：记录所在页号。 n_bits：对于行锁来说，一条记录就对应着一个比特位，一个页面中包含很多记录，用不同的比特位来区分到底是哪一条记录加了锁。为此在行锁结构的末尾放置了一堆比特位，这个n_bits属性代表使用了多少比特位。 并不是该页面中有多少记录，n_bits属性的值就是多少。为了让之后在页面中插入了新记录后也不至于重新分配锁结构，所以n_bits的值一般都比页面中记录条数多一些。 type_mode：这是一个32位的数，被分成了lock_mode、lock_type和rec_lock_type三个部分，如图所示： 锁的模式（lock_mode），占用低4位，可选的值如下： LOCK_IS（十进制的0）：表示共享意向锁，也就是IS锁。 LOCK_IX（十进制的1）：表示独占意向锁，也就是IX锁。 LOCK_S（十进制的2）：表示共享锁，也就是S锁。 LOCK_X（十进制的3）：表示独占锁，也就是X锁。 LOCK_AUTO_INC（十进制的4）：表示AUTO-INC锁。 在InnoDB存储引擎中，LOCK_IS，LOCK_IX，LOCK_AUTO_INC都算是表级锁的模式，LOCK_S和LOCK_X既可以算是表级锁的模式，也可以是行级锁的模式。 锁的类型（lock_type），占用第5～8位，不过现阶段只有第5位和第6位被使用： LOCK_TABLE（十进制的16），也就是当第5个比特位置为1时，表示表级锁。 LOCK_REC（十进制的32），也就是当第6个比特位置为1时，表示行级锁。 行锁的具体类型（rec_lock_type），使用其余的位来表示。只有在lock_type的值为LOCK_REC时，也就是只有在该锁为行级锁时，才会被细分为更多的类型： LOCK_ORDINARY（十进制的0）：表示next-key锁。 LOCK_GAP（十进制的512）：也就是当第10个比特位置为1时，表示gap锁。 LOCK_REC_NOT_GAP（十进制的1024）：也就是当第11个比特位置为1时，表示正经记录锁。 LOCK_INSERT_INTENTION（十进制的2048）：也就是当第12个比特位置为1时，表示插入意向锁。 其他的类型：还有一些不常用的类型我们就不多说了。 is_waiting属性也被放到了type_mode这个32位的数字中： - `LOCK_WAIT`（十进制的`256`） ：也就是当第9个比特位置为`1`时，表示`is_waiting`为`true`，也就是当前事务尚未获取到锁，处在等待状态；当这个比特位为`0`时，表示`is_waiting`为`false`，也就是当前事务获取锁成功。 其他信息：为了更好的管理系统运行过程中生成的各种锁结构而设计了各种哈希表和链表。 一堆比特位：如果是行锁结构的话，在该结构末尾还放置了一堆比特位，比特位的数量是由上边提到的n_bits属性表示的。页面中的每条记录在记录头信息中都包含一个heap_no属性，伪记录Infimum的heap_no值为0，Supremum的heap_no值为1，之后每插入一条记录，heap_no值就增1。锁结构最后的一堆比特位就对应着一个页面中的记录，一个比特位映射一个heap_no，不过为了编码方便，映射方式有点怪： 比方说现在有两个事务T1和T2想对hero表中的记录进行加锁，hero表中记录比较少，假设这些记录都存储在所在的表空间号为67，页号为3的页面上，那么如果： T1想对number值为15的这条记录加S型正常记录锁，在对记录加行锁之前，需要先加表级别的IS锁，也就是会生成一个表级锁的内存结构，不过我们这里不关心表级锁， 接下来分析一下生成行锁结构的过程： 事务T1要进行加锁，所以锁结构的锁所在事务信息指的就是T1。 直接对聚簇索引进行加锁，所以索引信息指的其实就是PRIMARY索引。 由于是行锁，所以接下来需要记录的是三个重要信息： Space ID：表空间号为67。 Page Number：页号为3。 n_bits：我们的hero表中现在只插入了5条用户记录，但是在初始分配比特位时会多分配一些，这主要是为了在之后新增记录时不用频繁分配比特位。其实计算n_bits有一个公式：其中n_recs指的是当前页面中一共有多少条记录（算上伪记录和在垃圾链表中的记录），比方说现在hero表一共有7条记录（5条用户记录和2条伪记录），所以n_recs的值就是7，LOCK_PAGE_BITMAP_MARGIN是一个固定的值64，所以本次加锁的n_bits值就是： 1n_bits = (1 + ((n_recs + LOCK_PAGE_BITMAP_MARGIN) / 8)) * 8 1n_bits = (1 + ((7 + 64) / 8)) * 8 = 72 type_mode是由三部分组成的： lock_mode，这是对记录加S锁，它的值为LOCK_S。 lock_type，这是对记录进行加锁，也就是行锁，所以它的值为LOCK_REC。 rec_lock_type，这是对记录加正经记录锁，也就是类型为LOCK_REC_NOT_GAP的锁。另外，由于当前没有其他事务对该记录加锁，所以应当获取到锁，也就是LOCK_WAIT代表的二进制位应该是0。 综上所属，此次加锁的type_mode的值应该是： 123type_mode = LOCK_S | LOCK_REC | LOCK_REC_NOT_GAP也就是：type_mode = 2 | 32 | 1024 = 1058 其他信息 一堆比特位因为number值为15的记录heap_no值为5，根据上边列举的比特位和heap_no的映射图来看，应该是第一个字节从低位往高位数第6个比特位被置为1，就像这样： 综上所述，事务T1为number值为5的记录加锁生成的锁结构就如下图所示： T2想对number值为3、8、15的这三条记录加X型的next-key锁，在对记录加行锁之前，需要先加表级别的IX锁，也就是会生成一个表级锁的内存结构，不过我们这里不关心表级锁。现在T2要为3条记录加锁，number为3、8的两条记录由于没有其他事务加锁，所以可以成功获取这条记录的X型next-key锁，也就是生成的锁结构的is_waiting属性为false；但是number为15的记录已经被T1加了S型正经记录锁，T2是不能获取到该记录的X型next-key锁的，也就是生成的锁结构的is_waiting属性为true。因为等待状态不相同，所以这时候会生成两个锁结构。这两个锁结构中相同的属性如下： 事务T2要进行加锁，所以锁结构的锁所在事务信息指的就是T2。 直接对聚簇索引进行加锁，所以索引信息指的其实就是PRIMARY索引。 由于是行锁，所以接下来需要记录是三个重要信息： Space ID：表空间号为67。 Page Number：页号为3。 n_bits：此属性生成策略同T1中一样，该属性的值为72。 type_mode是由三部分组成的： lock_mode，这是对记录加X锁，它的值为LOCK_X。 lock_type，这是对记录进行加锁，也就是行锁，所以它的值为LOCK_REC。 rec_lock_type，这是对记录加next-key锁，也就是类型为LOCK_ORDINARY的锁。 其他信息 不同的属性如下： 为number为3、8的记录生成的锁结构： type_mode值。由于可以获取到锁，所以is_waiting属性为false，也就是LOCK_WAIT代表的二进制位被置0。所以： 123type_mode = LOCK_X | LOCK_REC |LOCK_ORDINARY也就是type_mode = 3 | 32 | 0 = 35 一堆比特位因为number值为3、8的记录heap_no值分别为3、4，根据上边列举的比特位和heap_no的映射图来看，应该是第一个字节从低位往高位数第4、5个比特位被置为1，就像这样： 综上所述，事务T2为number值为3、8两条记录加锁生成的锁结构就如下图所示： 为number为15的记录生成的锁结构： type_mode值。由于不可以获取到锁，所以is_waiting属性为true，也就是LOCK_WAIT代表的二进制位被置1。所以： 123type_mode = LOCK_X | LOCK_REC |LOCK_ORDINARY | LOCK_WAIT也就是type_mode = 3 | 32 | 0 | 256 = 291 一堆比特位因为number值为15的记录heap_no值为5，根据上边列举的比特位和heap_no的映射图来看，应该是第一个字节从低位往高位数第6个比特位被置为1，就像这样： 综上所述，事务T2为number值为15的记录加锁生成的锁结构就如下图所示： 综上所述，事务T1先获取number值为15的S型正经记录锁，然后事务T2获取number值为3、8、15的X型正经记录锁共需要生成3个锁结构。 事务T2在对number值分别为3、8、15这三条记录加锁的情景中，是按照先对number值为3的记录加锁、再对number值为8的记录加锁，最后对number值为15的记录加锁的顺序进行的，如果我们一开始就对number值为15的记录加锁，那么该事务在为number值为15的记录生成一个锁结构后，直接就进入等待状态，就不为number值为3、8的两条记录生成锁结构了。在事务T1提交后会把在number值为15的记录上获取的锁释放掉，然后事务T2就可以获取该记录上的锁，这时再对number值为3、8的两条记录加锁时，就可以复用之前为number值为15的记录加锁时生成的锁结构了。 3.总结","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/tags/MySQL/"}]},{"title":"MySQL[十六]并发版本控制-MVCC","slug":"MySQL/MySQL[十六]并发版本控制-MVCC","date":"2022-01-15T16:00:00.000Z","updated":"2022-01-12T00:43:13.399Z","comments":true,"path":"2022/01/16/MySQL/MySQL[十六]并发版本控制-MVCC/","link":"","permalink":"https://yinhuidong.github.io/2022/01/16/MySQL/MySQL[%E5%8D%81%E5%85%AD]%E5%B9%B6%E5%8F%91%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6-MVCC/","excerpt":"","text":"一，事务隔离级别1.事务并发执行的一致性问题 脏写：一个事务修改了另一个未提交事务修改过的数据 脏读：一个事物读到了另一个未提交事务修改过的数据 不可重复读：一个事物修改了另一个未提交事务读取的数据 幻读：一个事物先根据某些搜索条件查询出一些记录，在该事务未提交时，另一个事务写入了一些符合搜索条件的记录 2.SQL标准中的隔离级别舍弃一部分隔离性来换取一部分性能在这里就体现在：设立一些隔离级别，隔离级别越低，越严重的问题就越可能发生。​ SQL标准中规定，针对不同的隔离级别，并发事务可以发生不同严重程度的问题，具体情况如下： 隔离级别 脏读 不可重复读 幻读 READ UNCOMMITTED Possible Possible Possible READ COMMITTED Not Possible Possible Possible REPEATABLE READ Not Possible Not Possible Possible SERIALIZABLE Not Possible Not Possible Not Possible 也就是说： READ UNCOMMITTED隔离级别下，可能发生脏读、不可重复读和幻读问题。 READ COMMITTED隔离级别下，可能发生不可重复读和幻读问题，但是不可以发生脏读问题。 REPEATABLE READ隔离级别下，可能发生幻读问题，但是不可以发生脏读和不可重复读的问题。 SERIALIZABLE隔离级别下，各种问题都不可以发生。二，一致性非锁读1.概念 MVCC是一种并发控制的方法，mysql的innodb就是基于MVCC实现对数据库的并发访问。在innodb引擎中指的就是在读已提交和可重复读这两种隔离级别下的事务对于select操作会访问版本链中的记录过程。别的事务可以修改这条记录，修改会记录在版本链中，select的时候可以直接去版本链中拿记录，这就可以实现读写的同时执行，提高效率。 2.版本链在Innodb的聚簇索引也就是主键索引中有两个隐藏列： trx_id:存储每次对某条聚簇索引记录进行修改的时候的事务id。roll_pointer：每次对哪条聚簇索引记录有修改的时候，都会把老版本写入到undo日志中。这个roll_pointer就是一个指针，他指向这条聚簇索引记录的上一个版本的位置，通过他来获取上一个版本的记录（insert操作在undo日志中没有这个属性，因为他没有老版本）。 3.ReadView读已提交和可重复读的区别就在于他们生成ReadView的策略不同。ReadView中主要就是有个列表来存储我们系统中当前活跃着的读写事务，也就是begin了还未提交的事务。通过这个列表来判断记录的某个版本是否对当前事务可见。假设当前事务列表里面的事务id为[80,100]. 如果你要访问的记录版本的事务id为50，比80还小，那说明这个事务在之前就已经提交了，所以对于当前事务是可以访问的。 如果你要访问的记录版本的事务id为90，发现此事务在列表id的最大值和最小值之间，那就判断一下是否在列表内，如果在那就说明此事务还未提交，所以版本不能被访问。如果不再，说明事务已经提交，所以版本可以被访问。 如果你要访问的记录版本的事务id为110，那比事务列表最大id100都大，那说明这个版本是在ReadView生成之后才发生的，所以不能被访问。 这些记录都是去版本链里面找的，先找最近记录，如果最近这一条事务id不符合条件，不可见的话，再去找上一个版本在比较当前事务的id和这个版本的事务id看能不能访问，以此类推直到返回可见的版本或者结束。 case：在读已提交隔离级别下：比如此时有一个事务id为100的事务，修改了name，使得name等于小明2，但是事务还没提交。则此时的版本链是： 那此时另一个事务发起了select 语句要查询id为1的记录，那此时生成的ReadView 列表只有[100]。那就去版本链去找了，首先肯定找最近的一条，发现trx_id是100,也就是name为小明2的那条记录，发现在列表内，所以不能访问。这时候就通过指针继续找下一条，name为小明1的记录，发现trx_id是60，小于列表中的最小id,所以可以访问，直接访问结果为小明1。那这时候我们把事务id为100的事务提交了，并且新建了一个事务id为110也修改id为1的记录，并且不提交事务。这时候的版本链是： 这时候之前那个select事务又执行了一次查询,要查询id为1的记录。这个时候关键的地方来了如果你是已提交读隔离级别，这时候你会重新生成一个ReadView，那你的活动事务列表中的值就变了，变成了[110]。按照上的说法，你去版本链通过trx_id对比查找到合适的结果就是小明2。如果你是可重复读隔离级别，这时候你的ReadView还是第一次select时候生成的ReadView,也就是列表的值还是[100]。所以select的结果是小明1。所以第二次select结果和第一次一样，所以叫可重复读！也就是说已提交读隔离级别下的事务在每次查询的开始都会生成一个独立的ReadView,而可重复读隔离级别则在第一次读的时候生成一个ReadView，之后的读都复用之前的ReadView。这就是Mysql的MVCC,通过版本链，实现多版本，可并发读-写，写-读。通过ReadView生成策略的不同实现不同的隔离级别。 三，purge insert undo 日志在事务提交之后就可以释放掉了，而update undo 日志由于还需要支持MVCC，因此不能立即删除。 为了支持MVCC，delete mark 操作仅仅是在记录上打一个删除标记，并没有真正将记录删除。 ​ 为了节约存储空间，我们应该在合适的时机把 update undo log 以及仅仅标记为delete mark的记录彻底删除，这个删除操作就成为purge。关键在于：这个合适的时机。​ update undo日志和被标记为删除的记录只是为了支持MCC而存在的，只要系统中最早产生的那个ReadView不再访问它们，它们的使命就结束了，就可以丢进历史的垃圾堆里了。一个 ReadView在什么时候才肯定不会访问某个事务执行过程中产生的undo日志呢?其实，只要我们能保证生成ReadView时某个事务已经提交，那么该 ReadView肯定就不需要访问该事务运行过程中产生的undo日志了(因为该事务所改动的记录的最新版本均对该ReadView可见)。​ 在一个事务提交时，会为这个事务生成一个名为事务no的值，该值用来表示事物提交的顺序，先提交的事务的事务no值小，后提交的大。 一个ReadView结构其实还包含一个事务no的属性。在生成一个ReadView的时候，会把比当前系统中最大的事务no值还大1的值赋值给这个属性。 ​ InnoDB还把当前系统中所有的ReadView按照创建时间连成了一个链表。当执行purge操作时(这个purge操作是在专门的后台线程中执行的)，就把系统中最早生成的ReadView给取出来。如果当前系统中不存在ReadView，就现场创建一个(新创建的这个ReadView的事务no值肯定比当前已经提交的事务的事务 no值大)。然后从各个回滚段的History 链表中取出事务no值较小的各组undo日志。如果一组undo日志的事务 no 值小于当前系统最早生成的ReadView的事务no属性值，就意味着该组undo日志没有用了，就会从History链表中移除，并且释放掉它们占用的存储空间。 如果该组undo日志包含因 delete mark 操作而产生的undo日志(TRX_UNDO_DEL_MARKS 属性值为1)，那么也需要将对应的标记为删除的记录给彻底删除。​ 注意：当前系统中最早生成的ReadView决定了purge操作中可以清理哪些update undo log以及 delete mark log。如果某个事物使用可重复读的隔离级别，那么这个事务会一直复用最初产生的ReadView。假如这个事务运行了很久，一直没有提交，那么最早生成的ReadView一直不释放，系统中的update undo log以及 delete mark log会越来越多，表空间文件越来越大，一条记录的版本链越来越长，从而影响系统性能。 四，一致性非锁读的演示会话1：​ 1234567891011121314151617#前置准备create database es_lock;use es_lock;create table parent (id int ,name varchar(20) not null);insert into parent(id,name)values (1,&#x27;aaa&#x27;),(2,&#x27;bbb&#x27;),(3,&#x27;ccc&#x27;),(4,&#x27;ddd&#x27;),(5,&#x27;eee&#x27;),(6,&#x27;fff&#x27;),(7,&#x27;ggg&#x27;);# 开始begin;select * from parent where id =1;# 1select * from parent where id =1;# 3 不同隔离级别下的结果：select * from parent where id =1;# 读已提交：null# 可重复读：1条记录commit ; 会话2：​ 1234567use es_lock;# 开始begin ;update parent set id =3 where id =1;# 2commit ;","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/tags/MySQL/"}]},{"title":"MySQL[十五]undo日志","slug":"MySQL/MySQL[十五]undo日志","date":"2022-01-14T16:00:00.000Z","updated":"2022-01-12T00:43:02.433Z","comments":true,"path":"2022/01/15/MySQL/MySQL[十五]undo日志/","link":"","permalink":"https://yinhuidong.github.io/2022/01/15/MySQL/MySQL[%E5%8D%81%E4%BA%94]undo%E6%97%A5%E5%BF%97/","excerpt":"","text":"1.事务回滚的需求我们说过事务需要保证原子性，也就是事务中的操作要么全部完成，要么什么也不做。但是偏偏有时候事务执行到一半会出现一些情况，比如： 情况一：事务执行过程中可能遇到各种错误，比如服务器本身的错误，操作系统错误，甚至是突然断电导致的错误。 情况二：程序员可以在事务执行过程中手动输入ROLLBACK语句结束当前的事务的执行。 这两种情况都会导致事务执行到一半就结束，但是事务执行过程中可能已经修改了很多东西，为了保证事务的原子性，我们需要把东西改回原先的样子，这个过程就称之为回滚（英文名：rollback），这样就可以造成一个假象：这个事务看起来什么都没做，所以符合原子性要求。 每当我们要对一条记录做改动时（这里的改动可以指INSERT、DELETE、UPDATE），都需要留一手 —— 把回滚时所需的东西都给记下来。比方说： 你插入一条记录时，至少要把这条记录的主键值记下来，之后回滚的时候只需要把这个主键值对应的记录删掉就好了。 你删除了一条记录，至少要把这条记录中的内容都记下来，这样之后回滚时再把由这些内容组成的记录插入到表中就好了。 你修改了一条记录，至少要把修改这条记录前的旧值都记录下来，这样之后回滚时再把这条记录更新为旧值就好了。 数据库把这些为了回滚而记录的这些东西称之为撤销日志，英文名为undo log，我们也可以土洋结合，称之为undo日志。这里需要注意的一点是，由于查询操作（SELECT）并不会修改任何用户记录，所以在查询操作执行时，并不需要记录相应的undo日志。在真实的InnoDB中，undo日志其实并不像我们上边所说的那么简单，不同类型的操作产生的undo日志的格式也是不同的，不过先暂时把这些具体细节放一放，我们先回过头来看看事务id。 2.事务id2.1给事务分配id的时机一个事务可以是一个只读事务，或者是一个读写事务： 我们可以通过START TRANSACTION READ ONLY语句开启一个只读事务。在只读事务中不可以对普通的表（其他事务也能访问到的表）进行增、删、改操作，但可以对临时表做增、删、改操作。 我们可以通过START TRANSACTION READ WRITE语句开启一个读写事务，或者使用BEGIN、START TRANSACTION语句开启的事务默认也算是读写事务。在读写事务中可以对表执行增删改查操作。 如果某个事务执行过程中对某个表执行了增、删、改操作，那么InnoDB存储引擎就会给它分配一个独一无二的事务id，分配方式如下： 对于只读事务来说，只有在它第一次对某个用户创建的临时表执行增、删、改操作时才会为这个事务分配一个事务id，否则的话是不分配事务id的。 对某个查询语句执行EXPLAIN分析它的查询计划时，有时候在Extra列会看到Using temporary的提示，这个表明在执行该查询语句时会用到内部临时表。这个所谓的内部临时表和我们手动用CREATE TEMPORARY TABLE创建的用户临时表并不一样，在事务回滚时并不需要把执行SELECT语句过程中用到的内部临时表也回滚，在执行SELECT语句用到内部临时表时并不会为它分配事务id。 对于读写事务来说，只有在它第一次对某个表（包括用户创建的临时表）执行增、删、改操作时才会为这个事务分配一个事务id，否则的话也是不分配事务id的。有的时候虽然我们开启了一个读写事务，但是在这个事务中全是查询语句，并没有执行增、删、改的语句，那也就意味着这个事务并不会被分配一个事务id。 只有在事务对表中的记录做改动时才会为这个事务分配一个唯一的**事务id**。 上边描述的事务id分配策略是针对MySQL 5.7来说的，前边的版本的分配方式可能不同。 2.2事务id是怎么生成的这个事务id本质上就是一个数字，它的分配策略和我们前边提到的对隐藏列row_id（当用户没有为表创建主键和UNIQUE键时InnoDB自动创建的列）的分配策略大抵相同，具体策略如下： 服务器会在内存中维护一个全局变量，每当需要为某个事务分配一个事务id时，就会把该变量的值当作事务id分配给该事务，并且把该变量自增1。 每当这个变量的值为256的倍数时，就会将该变量的值刷新到系统表空间的页号为5的页面中一个称之为Max Trx ID的属性处，这个属性占用8个字节的存储空间。 当系统下一次重新启动时，会将上边提到的Max Trx ID属性加载到内存中，将该值加上256之后赋值给我们前边提到的全局变量（因为在上次关机时该全局变量的值可能大于Max Trx ID属性值）。 这样就可以保证整个系统中分配的事务id值是一个递增的数字。先被分配id的事务得到的是较小的事务id，后被分配id的事务得到的是较大的事务id。 2.3trx_id隐藏列聚簇索引的记录除了会保存完整的用户数据以外，而且还会自动添加名为trx_id、roll_pointer的隐藏列，如果用户没有在表中定义主键以及UNIQUE键，还会自动添加一个名为row_id的隐藏列。所以一条记录在页面中的真实结构看起来就是这样的： 其中的trx_id列其实还蛮好理解的，就是某个对这个聚簇索引记录做改动的语句所在的事务对应的事务id而已（此处的改动可以是INSERT、DELETE、UPDATE操作）。至于roll_pointer隐藏列我们后边分析。 3.undo日志的格式为了实现事务的原子性，InnoDB存储引擎在实际进行增、删、改一条记录时，都需要先把对应的undo日志记下来。一般每对一条记录做一次改动，就对应着一条undo日志，但在某些更新记录的操作中，也可能会对应着2条undo日志。一个事务在执行过程中可能新增、删除、更新若干条记录，也就是说需要记录很多条对应的undo日志，这些undo日志会被从0开始编号，也就是说根据生成的顺序分别被称为第0号undo日志、第1号undo日志、…、第n号undo日志等，这个编号也被称之为undo no。 这些undo日志是被记录到类型为FIL_PAGE_UNDO_LOG的页面中。这些页面可以从系统表空间中分配，也可以从一种专门存放undo日志的表空间，也就是所谓的undo tablespace中分配。先来看看不同操作都会产生什么样子的undo日志吧～我们先来创建一个名为undo_demo的表： 1234567CREATE TABLE undo_demo ( id INT NOT NULL, key1 VARCHAR(100), col VARCHAR(100), PRIMARY KEY (id), KEY idx_key1 (key1))Engine=InnoDB CHARSET=utf8; 这个表中有3个列，其中id列是主键，我们为key1列建立了一个二级索引，col列是一个普通的列。每个表都会被分配一个唯一的table id，我们可以通过系统数据库information_schema中的innodb_sys_tables表来查看某个表对应的table id是什么，现在我们查看一下undo_demo对应的table id是多少： 1234567mysql&gt; SELECT * FROM information_schema.innodb_sys_tables WHERE name = &#x27;yhd/undo_demo&#x27;;+----------+---------------------+------+--------+-------+-------------+------------+---------------+------------+| TABLE_ID | NAME | FLAG | N_COLS | SPACE | FILE_FORMAT | ROW_FORMAT | ZIP_PAGE_SIZE | SPACE_TYPE |+----------+---------------------+------+--------+-------+-------------+------------+---------------+------------+| 138 | yhd/undo_demo | 33 | 6 | 482 | Barracuda | Dynamic | 0 | Single |+----------+---------------------+------+--------+-------+-------------+------------+---------------+------------+1 row in set (0.01 sec) 从查询结果可以看出，undo_demo表对应的table id为138。 3.1INSERT操作对应的undo日志当我们向表中插入一条记录时会有乐观插入和悲观插入的区分，但是不管怎么插入，最终导致的结果就是这条记录被放到了一个数据页中。如果希望回滚这个插入操作，那么把这条记录删除就好了，也就是说在写对应的undo日志时，主要是把这条记录的主键信息记上。所以InnoDB设计了一个类型为TRX_UNDO_INSERT_REC的undo日志，它的完整结构如下图所示： 根据示意图我们强调几点： undo no在一个事务中是从0开始递增的，也就是说只要事务没提交，每生成一条undo日志，那么该条日志的undo no就增1。 如果记录中的主键只包含一个列，那么在类型为TRX_UNDO_INSERT_REC的undo日志中只需要把该列占用的存储空间大小和真实值记录下来，如果记录中的主键包含多个列，那么每个列占用的存储空间大小和对应的真实值都需要记录下来（图中的len就代表列占用的存储空间大小，value就代表列的真实值）。 当我们向某个表中插入一条记录时，实际上需要向聚簇索引和所有的二级索引都插入一条记录。不过记录undo日志时，我们只需要考虑向聚簇索引插入记录时的情况就好了，因为其实聚簇索引记录和二级索引记录是一一对应的，我们在回滚插入操作时，只需要知道这条记录的主键信息，然后根据主键信息做对应的删除操作，做删除操作时就会顺带着把所有二级索引中相应的记录也删除掉。后边说到的DELETE操作和UPDATE操作对应的undo日志也都是针对聚簇索引记录而言的。 现在我们向undo_demo中插入两条记录： 12345BEGIN; # 显式开启一个事务，假设该事务的id为100# 插入两条记录INSERT INTO undo_demo(id, key1, col) VALUES (1, &#x27;AWM&#x27;, &#x27;狙击枪&#x27;), (2, &#x27;M416&#x27;, &#x27;步枪&#x27;); 因为记录的主键只包含一个id列，所以我们在对应的undo日志中只需要将待插入记录的id列占用的存储空间长度（id列的类型为INT，INT类型占用的存储空间长度为4个字节）和真实值记录下来。本例中插入了两条记录，所以会产生两条类型为TRX_UNDO_INSERT_REC的undo日志: 第一条undo日志的undo no为0，记录主键占用的存储空间长度为4，真实值为1。画一个示意图就是这样： 第二条undo日志的undo no为1，记录主键占用的存储空间长度为4，真实值为2。画一个示意图就是这样（与第一条undo日志对比，undo no和主键各列信息有不同）： 为了最大限度的节省undo日志占用的存储空间，和我们前边说过的redo日志类似，InnoDB会给undo日志中的某些属性进行压缩处理。 ①roll_pointer隐藏列的含义roll_pointer本质上就是一个指向记录对应的undo日志的一个指针。比方说我们上边向undo_demo表里插入了2条记录，每条记录都有与其对应的一条undo日志。记录被存储到了类型为FIL_PAGE_INDEX的页面中（就是我们前边一直所说的数据页），undo日志被存放到了类型为FIL_PAGE_UNDO_LOG的页面中。效果如图所示： **roll_pointer**本质就是一个指针，指向记录对应的undo日志。 3.2 DELETE操作对应的undo日志插入到页面中的记录会根据记录头信息中的next_record属性组成一个单向链表，我们把这个链表称之为正常记录链表；被删除的记录其实也会根据记录头信息中的next_record属性组成一个链表，只不过这个链表中的记录占用的存储空间可以被重新利用，所以也称这个链表为垃圾链表。Page Header部分有一个称之为PAGE_FREE的属性，它指向由被删除记录组成的垃圾链表中的头节点。我们先画一个图，假设此刻某个页面中的记录分布情况是这样的（这个不是undo_demo表中的记录，只是我们随便举的一个例子）： 为了突出主题，在这个简化版的示意图中，我们只把记录的delete_mask标志位展示了出来。从图中可以看出，正常记录链表中包含了3条正常记录，垃圾链表里包含了2条已删除记录，在垃圾链表中的这些记录占用的存储空间可以被重新利用。页面的Page Header部分的PAGE_FREE属性的值代表指向垃圾链表头节点的指针。假设现在我们准备使用DELETE语句把正常记录链表中的最后一条记录给删除掉，其实这个删除的过程需要经历两个阶段： 阶段一：仅仅将记录的delete_mask标识位设置为1，其他的不做修改（其实会修改记录的trx_id、roll_pointer这些隐藏列的值）。InnoDB把这个阶段称之为delete mark。把这个过程画下来就是这样：可以看到，正常记录链表中的最后一条记录的delete_mask值被设置为1，但是并没有被加入到垃圾链表。也就是此时记录处于一个中间状态。在删除语句所在的事务提交之前，被删除的记录一直都处于这种所谓的中间状态。 为啥会有这种奇怪的中间状态呢？其实主要是为了实现一个称之为MVCC的功能。 阶段二：当该删除语句所在的事务提交之后，会有专门的线程后来真正的把记录删除掉。所谓真正的删除就是把该记录从正常记录链表中移除，并且加入到垃圾链表中，然后还要调整一些页面的其他信息，比如页面中的用户记录数量PAGE_N_RECS、上次插入记录的位置PAGE_LAST_INSERT、垃圾链表头节点的指针PAGE_FREE、页面中可重用的字节数量PAGE_GARBAGE、还有页目录的一些信息等等。InnoDB把这个阶段称之为purge。把阶段二执行完了，这条记录就算是真正的被删除掉了。这条已删除记录占用的存储空间也可以被重新利用了。画下来就是这样：将被删除记录加入到垃圾链表时，实际上加入到链表的头节点处，会跟着修改PAGE_FREE属性的值。 页面的Page Header部分有一个PAGE_GARBAGE属性，该属性记录着当前页面中可重用存储空间占用的总字节数。每当有已删除记录被加入到垃圾链表后，都会把这个PAGE_GARBAGE属性的值加上该已删除记录占用的存储空间大小。PAGE_FREE指向垃圾链表的头节点，之后每当新插入记录时，首先判断PAGE_FREE指向的头节点代表的已删除记录占用的存储空间是否足够容纳这条新插入的记录，如果不可以容纳，就直接向页面中申请新的空间来存储这条记录（并不会尝试遍历整个垃圾链表，找到一个可以容纳新记录的节点）。如果可以容纳，那么直接重用这条已删除记录的存储空间，并且把PAGE_FREE指向垃圾链表中的下一条已删除记录。但是这里有一个问题，如果新插入的那条记录占用的存储空间大小小于垃圾链表的头节点占用的存储空间大小，那就意味头节点对应的记录占用的存储空间里有一部分空间用不到，这部分空间就被称之为碎片空间。那这些碎片空间岂不是永远都用不到了么？其实也不是，这些碎片空间占用的存储空间大小会被统计到PAGE_GARBAGE属性中，这些碎片空间在整个页面快使用完前并不会被重新利用，不过当页面快满时，如果再插入一条记录，此时页面中并不能分配一条完整记录的空间，这时候会首先看一看PAGE_GARBAGE的空间和剩余可利用的空间加起来是不是可以容纳下这条记录，如果可以的话，InnoDB会尝试重新组织页内的记录，重新组织的过程就是先开辟一个临时页面，把页面内的记录依次插入一遍，因为依次插入时并不会产生碎片，之后再把临时页面的内容复制到本页面，这样就可以把那些碎片空间都解放出来（很显然重新组织页面内的记录比较耗费性能）。 从上边的描述中我们也可以看出来，在删除语句所在的事务提交之前，只会经历阶段一，也就是delete mark阶段（提交之后我们就不用回滚了，所以只需考虑对删除操作的阶段一做的影响进行回滚）。InnoDB为此设计了一种称之为TRX_UNDO_DEL_MARK_REC类型的undo日志，它的完整结构如下图所示： 在对一条记录进行delete mark操作前，需要把该记录的旧的trx_id和roll_pointer隐藏列的值都给记到对应的undo日志中来，就是我们图中显示的old trx_id和old roll_pointer属性。这样有一个好处，那就是可以通过undo日志的old roll_pointer找到记录在修改之前对应的undo日志。比方说在一个事务中，我们先插入了一条记录，然后又执行对该记录的删除操作，这个过程的示意图就是这样：从图中可以看出来，执行完delete mark操作后，它对应的undo日志和INSERT操作对应的undo日志就串成了一个链表。这个链表就称之为版本链。 与类型为TRX_UNDO_INSERT_REC的undo日志不同，类型为TRX_UNDO_DEL_MARK_REC的undo日志还多了一个索引列各列信息的内容，也就是说如果某个列被包含在某个索引中，那么它的相关信息就应该被记录到这个索引列各列信息部分，所谓的相关信息包括该列在记录中的位置（用pos表示），该列占用的存储空间大小（用len表示），该列实际值（用value表示）。所以索引列各列信息存储的内容实质上就是&lt;pos, len, value&gt;的一个列表。这部分信息主要是用在事务提交后，对该中间状态记录做真正删除的阶段二，也就是purge阶段中使用的。 现在继续在上边那个事务id为100的事务中删除一条记录，比如我们把id为1的那条记录删除掉： 12345678BEGIN; # 显式开启一个事务，假设该事务的id为100# 插入两条记录INSERT INTO undo_demo(id, key1, col) VALUES (1, &#x27;AWM&#x27;, &#x27;狙击枪&#x27;), (2, &#x27;M416&#x27;, &#x27;步枪&#x27;); # 删除一条记录 DELETE FROM undo_demo WHERE id = 1; 这个delete mark操作对应的undo日志的结构就是这样： 对照着这个图，我们得注意下边几点： 因为这条undo日志是id为100的事务中产生的第3条undo日志，所以它对应的undo no就是2。 在对记录做delete mark操作时，记录的trx_id隐藏列的值是100（也就是说对该记录最近的一次修改就发生在本事务中），所以把100填入old trx_id属性中。然后把记录的roll_pointer隐藏列的值取出来，填入old roll_pointer属性中，这样就可以通过old roll_pointer属性值找到最近一次对该记录做改动时产生的undo日志。 由于undo_demo表中有2个索引：一个是聚簇索引，一个是二级索引idx_key1。只要是包含在索引中的列，那么这个列在记录中的位置（pos），占用存储空间大小（len）和实际值（value）就需要存储到undo日志中。 对于主键来说，只包含一个id列，存储到undo日志中的相关信息分别是： pos：id列是主键，也就是在记录的第一个列，它对应的pos值为0。pos占用1个字节来存储。 len：id列的类型为INT，占用4个字节，所以len的值为4。len占用1个字节来存储。 value：在被删除的记录中id列的值为1，也就是value的值为1。value占用4个字节来存储。 画一个图演示一下就是这样：所以对于id列来说，最终存储的结果就是&lt;0, 4, 1&gt;，存储这些信息占用的存储空间大小为1 + 1 + 4 = 6个字节。 对于idx_key1来说，只包含一个key1列，存储到undo日志中的相关信息分别是： pos：key1列是排在id列、trx_id列、roll_pointer列之后的，它对应的pos值为3。pos占用1个字节来存储。 len：key1列的类型为VARCHAR(100)，使用utf8字符集，被删除的记录实际存储的内容是AWM，所以一共占用3个字节，也就是所以len的值为3。len占用1个字节来存储。 value：在被删除的记录中key1列的值为AWM，也就是value的值为AWM。value占用3个字节来存储。 画一个图演示一下就是这样：所以对于key1列来说，最终存储的结果就是&lt;3, 3, &#39;AWM&#39;&gt;，存储这些信息占用的存储空间大小为1 + 1 + 3 = 5个字节。从上边的叙述中可以看到，&lt;0, 4, 1&gt;和&lt;3, 3, &#39;AWM&#39;&gt;共占用11个字节。然后index_col_info len本身占用2个字节，所以加起来一共占用13个字节，把数字13就填到了index_col_info len的属性中。 3.3 UPDATE操作对应的undo日志在执行UPDATE语句时，InnoDB对更新主键和不更新主键这两种情况有截然不同的处理方案。 ①不更新主键的情况在不更新主键的情况下，又可以细分为被更新的列占用的存储空间不发生变化和发生变化的情况。 就地更新（in-place update）更新记录时，对于被更新的每个列来说，如果更新后的列和更新前的列占用的存储空间都一样大，那么就可以进行就地更新，也就是直接在原记录的基础上修改对应列的值。 先删除掉旧记录，再插入新记录在不更新主键的情况下，如果有任何一个被更新的列更新前和更新后占用的存储空间大小不一致，那么就需要先把这条旧的记录从聚簇索引页面中删除掉，然后再根据更新后列的值创建一条新的记录插入到页面中。注意，这里所说的删除并不是delete mark操作，而是真正的删除掉，也就是把这条记录从正常记录链表中移除并加入到垃圾链表中，并且修改页面中相应的统计信息（比如PAGE_FREE、PAGE_GARBAGE等这些信息）。不过这里做真正删除操作的线程并不是在DELETE语句中做purge操作时使用的另外专门的线程，而是由用户线程同步执行真正的删除操作，真正删除之后紧接着就要根据各个列更新后的值创建的新记录插入。这里如果新创建的记录占用的存储空间大小不超过旧记录占用的空间，那么可以直接重用被加入到垃圾链表中的旧记录所占用的存储空间，否则的话需要在页面中新申请一段空间以供新记录使用，如果本页面内已经没有可用的空间的话，那就需要进行页面分裂操作，然后再插入新记录。 针对UPDATE不更新主键的情况（包括上边所说的就地更新和先删除旧记录再插入新记录），InnoDB设计了一种类型为TRX_UNDO_UPD_EXIST_REC的undo日志`，它的完整结构如下： 其实大部分属性和我们介绍过的TRX_UNDO_DEL_MARK_REC类型的undo日志是类似的，不过还是要注意这么几点： n_updated属性表示本条UPDATE语句执行后将有几个列被更新，后边跟着的&lt;pos, old_len, old_value&gt;分别表示被更新列在记录中的位置、更新前该列占用的存储空间大小、更新前该列的真实值。 如果在UPDATE语句中更新的列包含索引列，那么也会添加索引列各列信息这个部分，否则的话是不会添加这个部分的。 现在继续在上边那个事务id为100的事务中更新一条记录，比如我们把id为2的那条记录更新一下： 12345678910111213BEGIN; # 显式开启一个事务，假设该事务的id为100# 插入两条记录INSERT INTO undo_demo(id, key1, col) VALUES (1, &#x27;AWM&#x27;, &#x27;狙击枪&#x27;), (2, &#x27;M416&#x27;, &#x27;步枪&#x27;); # 删除一条记录 DELETE FROM undo_demo WHERE id = 1; # 更新一条记录UPDATE undo_demo SET key1 = &#x27;M249&#x27;, col = &#x27;机枪&#x27; WHERE id = 2; 这个UPDATE语句更新的列大小都没有改动，所以可以采用就地更新的方式来执行，在真正改动页面记录时，会先记录一条类型为TRX_UNDO_UPD_EXIST_REC的undo日志，长这样： 对照着这个图我们注意一下这几个地方： 因为这条undo日志是id为100的事务中产生的第4条undo日志，所以它对应的undo no就是3。 这条日志的roll_pointer指向undo no为1的那条日志，也就是插入主键值为2的记录时产生的那条undo日志，也就是最近一次对该记录做改动时产生的undo日志。 由于本条UPDATE语句中更新了索引列key1的值，所以需要记录一下索引列各列信息部分，也就是把主键和key1列更新前的信息填入。 ②更新主键的情况在聚簇索引中，记录是按照主键值的大小连成了一个单向链表的，如果我们更新了某条记录的主键值，意味着这条记录在聚簇索引中的位置将会发生改变，比如你将记录的主键值从1更新为10000，如果还有非常多的记录的主键值分布在1 ~ 10000之间的话，那么这两条记录在聚簇索引中就有可能离得非常远，甚至中间隔了好多个页面。针对UPDATE语句中更新了记录主键值的这种情况，InnoDB在聚簇索引中分了两步处理： 将旧记录进行delete mark操作注意：这里是delete mark操作！也就是说在UPDATE语句所在的事务提交前，对旧记录只做一个delete mark操作，在事务提交后才由专门的线程做purge操作，把它加入到垃圾链表中。 之所以只对旧记录做delete mark操作，是因为别的事务同时也可能访问这条记录，如果把它真正的删除加入到垃圾链表后，别的事务就访问不到了。这个功能就是所谓的MVCC。 根据更新后各列的值创建一条新记录，并将其插入到聚簇索引中（需重新定位插入的位置）。由于更新后的记录主键值发生了改变，所以需要重新从聚簇索引中定位这条记录所在的位置，然后把它插进去。 针对UPDATE语句更新记录主键值的这种情况，在对该记录进行delete mark操作前，会记录一条类型为TRX_UNDO_DEL_MARK_REC的undo日志；之后插入新记录时，会记录一条类型为TRX_UNDO_INSERT_REC的undo日志，也就是说每对一条记录的主键值做改动时，会记录2条undo日志。 4.通用链表结构在写入undo日志的过程中会使用到多个链表，很多链表都有同样的节点结构，如图所示： 在某个表空间内，我们可以通过一个页的页号和在页内的偏移量来唯一定位一个节点的位置，这两个信息也就相当于指向这个节点的一个指针。所以： Pre Node Page Number和Pre Node Offset的组合就是指向前一个节点的指针 Next Node Page Number和Next Node Offset的组合就是指向后一个节点的指针。 整个List Node占用12个字节的存储空间。 为了更好的管理链表，InnoDB还提出了一个基节点的结构，里边存储了这个链表的头节点、尾节点以及链表长度信息，基节点的结构示意图如下： 其中： List Length表明该链表一共有多少节点。 First Node Page Number和First Node Offset的组合就是指向链表头节点的指针。 Last Node Page Number和Last Node Offset的组合就是指向链表尾节点的指针。 整个List Base Node占用16个字节的存储空间。 所以使用List Base Node和List Node这两个结构组成的链表的示意图就是这样： 5.FIL_PAGE_UNDO_LOG页面表空间其实是由许许多多的页面构成的，页面默认大小为16KB。这些页面有不同的类型，比如类型为FIL_PAGE_INDEX的页面用于存储聚簇索引以及二级索引，类型为FIL_PAGE_TYPE_FSP_HDR的页面用于存储表空间头部信息的，还有其他各种类型的页面，其中有一种称之为FIL_PAGE_UNDO_LOG类型的页面是专门用来存储undo日志的，这种类型的页面的通用结构如下图所示（以默认的16KB大小为例）： 我们就简称为Undo页面，上图中的File Header和File Trailer是各种页面都有的通用结构。Undo Page Header是Undo页面所特有的，我们来看一下它的结构： 其中各个属性的意思如下： TRX_UNDO_PAGE_TYPE：本页面准备存储什么种类的undo日志。前边介绍了好几种类型的undo日志，它们可以被分为两个大类： TRX_UNDO_INSERT（使用十进制1表示）：类型为TRX_UNDO_INSERT_REC的undo日志属于此大类，一般由INSERT语句产生，或者在UPDATE语句中有更新主键的情况也会产生此类型的undo日志。 TRX_UNDO_UPDATE（使用十进制2表示），除了类型为TRX_UNDO_INSERT_REC的undo日志，其他类型的undo日志都属于这个大类，比如我们前边说的TRX_UNDO_DEL_MARK_REC、TRX_UNDO_UPD_EXIST_REC啥的，一般由DELETE、UPDATE语句产生的undo日志属于这个大类。 这个TRX_UNDO_PAGE_TYPE属性可选的值就是上边的两个，用来标记本页面用于存储哪个大类的undo日志，不同大类的undo日志不能混着存储，比如一个Undo页面的TRX_UNDO_PAGE_TYPE属性值为TRX_UNDO_INSERT，那么这个页面就只能存储类型为TRX_UNDO_INSERT_REC的undo日志，其他类型的undo日志就不能放到这个页面中了。 之所以把undo日志分成两个大类，是因为类型为TRX_UNDO_INSERT_REC的undo日志在事务提交后可以直接删除掉，而其他类型的undo日志还需要为所谓的MVCC服务，不能直接删除掉，对它们的处理需要区别对待。 TRX_UNDO_PAGE_START：表示在当前页面中是从什么位置开始存储undo日志的，或者说表示第一条undo日志在本页面中的起始偏移量。 TRX_UNDO_PAGE_FREE：与上边的TRX_UNDO_PAGE_START对应，表示当前页面中存储的最后一条undo日志结束时的偏移量，或者说从这个位置开始，可以继续写入新的undo日志。假设现在向页面中写入了3条undo日志，那么TRX_UNDO_PAGE_START和TRX_UNDO_PAGE_FREE的示意图就是这样：当然，在最初一条undo日志也没写入的情况下，TRX_UNDO_PAGE_START和TRX_UNDO_PAGE_FREE的值是相同的。 TRX_UNDO_PAGE_NODE：代表一个List Node结构（链表的普通节点，我们上边刚说的）。 6.Undo页面链表6.1单个事务中的Undo页面链表因为一个事务可能包含多个语句，而且一个语句可能对若干条记录进行改动，而对每条记录进行改动前，都需要记录1条或2条的undo日志，所以在一个事务执行过程中可能产生很多undo日志，这些日志可能一个页面放不下，需要放到多个页面中，这些页面就通过我们上边介绍的TRX_UNDO_PAGE_NODE属性连成了链表： 链表中的第一个Undo页面称它为first undo page，其余的Undo页面称之为normal undo page，这是因为在first undo page中除了记录Undo Page Header之外，还会记录其他的一些管理信息。 在一个事务执行过程中，可能混着执行INSERT、DELETE、UPDATE语句，也就意味着会产生不同类型的undo日志。但是，同一个Undo页面要么只存储TRX_UNDO_INSERT大类的undo日志，要么只存储TRX_UNDO_UPDATE大类的undo日志，反正不能混着存，所以在一个事务执行过程中就可能需要2个Undo页面的链表，一个称之为insert undo链表，另一个称之为update undo链表，画个示意图就是这样： 另外，InnoDB规定对普通表和临时表的记录改动时产生的undo日志要分别记录（我们稍后阐释为啥这么做），所以在一个事务中最多有4个以Undo页面为节点组成的链表： 当然，并不是在事务一开始就会为这个事务分配这4个链表，具体分配策略如下： 刚刚开启事务时，一个Undo页面链表也不分配。 当事务执行过程中向普通表中插入记录或者执行更新记录主键的操作之后，就会为其分配一个普通表的insert undo链表。 当事务执行过程中删除或者更新了普通表中的记录之后，就会为其分配一个普通表的update undo链表。 当事务执行过程中向临时表中插入记录或者执行更新记录主键的操作之后，就会为其分配一个临时表的insert undo链表。 当事务执行过程中删除或者更新了临时表中的记录之后，就会为其分配一个临时表的update undo链表。 总结一句就是：按需分配，啥时候需要啥时候再分配，不需要就不分配。 6.2多个事务中的Undo页面链表为了尽可能提高undo日志的写入效率，不同事务执行过程中产生的undo日志需要被写入到不同的Undo页面链表中。比方说现在有事务id分别为1、2的两个事务，我们分别称之为trx 1和trx 2，假设在这两个事务执行过程中： trx 1对普通表做了DELETE操作，对临时表做了INSERT和UPDATE操作。InnoDB会为trx 1分配3个链表，分别是： 针对普通表的update undo链表 针对临时表的insert undo链表 针对临时表的update undo链表。 trx 2对普通表做了INSERT、UPDATE和DELETE操作，没有对临时表做改动。InnoDB会为trx 2分配2个链表，分别是： 针对普通表的insert undo链表 针对普通表的update undo链表。 综上所述，在trx 1和trx 2执行过程中，InnoDB共需为这两个事务分配5个Undo页面链表，画个图就是这样： 如果有更多的事务，那就意味着可能会产生更多的Undo页面链表。 7.undo日志具体写入过程7.1段（Segment）的概念段是一个逻辑上的概念，本质上是由若干个零散页面和若干个完整的区组成的。比如一个B+树索引被划分成两个段，一个叶子节点段，一个非叶子节点段，这样叶子节点就可以被尽可能的存到一起，非叶子节点被尽可能的存到一起。每一个段对应一个INODE Entry结构，这个INODE Entry结构描述了这个段的各种信息，比如段的ID，段内的各种链表基节点，零散页面的页号有哪些等信息。为了定位一个INODE Entry，InnoDB设计了一个Segment Header的结构，整个Segment Header占用10个字节大小，各个属性的意思如下： Space ID of the INODE Entry：INODE Entry结构所在的表空间ID。 Page Number of the INODE Entry：INODE Entry结构所在的页面页号。 Byte Offset of the INODE Ent：INODE Entry结构在该页面中的偏移量 知道了表空间ID、页号、页内偏移量，就可以唯一定位一个INODE Entry的地址。 7.2 Undo Log Segment HeaderInnoDB规定，每一个Undo页面链表都对应着一个段，称之为Undo Log Segment。也就是说链表中的页面都是从这个段里边申请的，所以他们在Undo页面链表的第一个页面，也就是上边提到的first undo page中设计了一个称之为Undo Log Segment Header的部分，这个部分中包含了该链表对应的段的segment header信息以及其他的一些关于这个段的信息，所以Undo页面链表的第一个页面其实长这样： 可以看到这个Undo链表的第一个页面比普通页面多了个Undo Log Segment Header，我们来看一下它的结构： TRX_UNDO_STATE：本Undo页面链表处在什么状态。一个Undo Log Segment可能处在的状态包括： TRX_UNDO_ACTIVE：活跃状态，也就是一个活跃的事务正在往这个段里边写入undo日志。 TRX_UNDO_CACHED：被缓存的状态。处在该状态的Undo页面链表等待着之后被其他事务重用。 TRX_UNDO_TO_FREE：对于insert undo链表来说，如果在它对应的事务提交之后，该链表不能被重用，那么就会处于这种状态。 TRX_UNDO_TO_PURGE：对于update undo链表来说，如果在它对应的事务提交之后，该链表不能被重用，那么就会处于这种状态。 TRX_UNDO_PREPARED：包含处于PREPARE阶段的事务产生的undo日志。 TRX_UNDO_LAST_LOG：本Undo页面链表中最后一个Undo Log Header的位置。 TRX_UNDO_FSEG_HEADER：本Undo页面链表对应的段的Segment Header信息。 TRX_UNDO_PAGE_LIST：Undo页面链表的基节点。 Undo页面的Undo Page Header部分有一个12字节大小的TRX_UNDO_PAGE_NODE属性，这个属性代表一个List Node结构。每一个Undo页面都包含Undo Page Header结构，这些页面就可以通过这个属性连成一个链表。这个TRX_UNDO_PAGE_LIST属性代表着这个链表的基节点，当然这个基节点只存在于Undo页面链表的第一个页面，也就是first undo page中。 7.3Undo Log Header一个事务在向Undo页面中写入undo日志时的方式是十分简单暴力的，就是直接往里怼，写完一条紧接着写另一条，各条undo日志之间是亲密无间的。写完一个Undo页面后，再从段里申请一个新页面，然后把这个页面插入到Undo页面链表中，继续往这个新申请的页面中写。InnoDB认为同一个事务向一个Undo页面链表中写入的undo日志算是一个组，比方说我们上边介绍的trx 1由于会分配3个Undo页面链表，也就会写入3个组的undo日志；trx 2由于会分配2个Undo页面链表，也就会写入2个组的undo日志。在每写入一组undo日志时，都会在这组undo日志前先记录一下关于这个组的一些属性，InnoDB把存储这些属性的地方称之为Undo Log Header。所以Undo页面链表的第一个页面在真正写入undo日志前，其实都会被填充Undo Page Header、Undo Log Segment Header、Undo Log Header这3个部分，如图所示： 这个Undo Log Header具体的结构如下： TRX_UNDO_TRX_ID：生成本组undo日志的事务id。 TRX_UNDO_TRX_NO：事务提交后生成的一个需要序号，使用此序号来标记事务的提交顺序（先提交的此序号小，后提交的此序号大）。 TRX_UNDO_DEL_MARKS：标记本组undo日志中是否包含由于Delete mark操作产生的undo日志。 TRX_UNDO_LOG_START：表示本组undo日志中第一条undo日志的在页面中的偏移量。 TRX_UNDO_XID_EXISTS：本组undo日志是否包含XID信息。 TRX_UNDO_DICT_TRANS：标记本组undo日志是不是由DDL语句产生的。 TRX_UNDO_TABLE_ID：如果TRX_UNDO_DICT_TRANS为真，那么本属性表示DDL语句操作的表的table id。 TRX_UNDO_NEXT_LOG：下一组的undo日志在页面中开始的偏移量。 TRX_UNDO_PREV_LOG：上一组的undo日志在页面中开始的偏移量。 一般来说一个Undo页面链表只存储一个事务执行过程中产生的一组undo日志，但是在某些情况下，可能会在一个事务提交之后，之后开启的事务重复利用这个Undo页面链表，这样就会导致一个Undo页面中可能存放多组Undo日志，TRX_UNDO_NEXT_LOG和TRX_UNDO_PREV_LOG就是用来标记下一组和上一组undo日志在页面中的偏移量的。 TRX_UNDO_HISTORY_NODE：一个12字节的List Node结构，代表一个称之为History链表的节点。 7.4 小结对于没有被重用的Undo页面链表来说，链表的第一个页面，也就是first undo page在真正写入undo日志前，会填充Undo Page Header、Undo Log Segment Header、Undo Log Header这3个部分，之后才开始正式写入undo日志。对于其他的页面来说，也就是normal undo page在真正写入undo日志前，只会填充Undo Page Header。链表的List Base Node存放到first undo page的Undo Log Segment Header部分，List Node信息存放到每一个Undo页面的undo Page Header部分，所以画一个Undo页面链表的示意图就是这样： 8.重用Undo页面为了能提高并发执行的多个事务写入undo日志的性能，InnoDB决定为每个事务单独分配相应的Undo页面链表（最多可能单独分配4个链表）。但是这样也造成了一些问题，比如其实大部分事务执行过程中可能只修改了一条或几条记录，针对某个Undo页面链表只产生了非常少的undo日志，这些undo日志可能只占用一点存储空间，每开启一个事务就新创建一个Undo页面链表（虽然这个链表中只有一个页面）来存储这么一点undo日志岂不是太浪费了么？的确是挺浪费，于是InnoDB决定在事务提交后在某些情况下重用该事务的Undo页面链表。一个Undo页面链表是否可以被重用的条件很简单： 该链表中只包含一个Undo页面。如果一个事务执行过程中产生了非常多的undo日志，那么它可能申请非常多的页面加入到Undo页面链表中。在该事物提交后，如果将整个链表中的页面都重用，那就意味着即使新的事务并没有向该Undo页面链表中写入很多undo日志，那该链表中也得维护非常多的页面，那些用不到的页面也不能被别的事务所使用，这样就造成了另一种浪费。所以InnoDB规定，只有在Undo页面链表中只包含一个Undo页面时，该链表才可以被下一个事务所重用。 该Undo页面已经使用的空间小于整个页面空间的3/4。 Undo页面链表按照存储的undo日志所属的大类可以被分为insert undo链表和update undo链表两种，这两种链表在被重用时的策略也是不同的，我们分别看一下： insert undo链表insert undo链表中只存储类型为TRX_UNDO_INSERT_REC的undo日志，这种类型的undo日志在事务提交之后就没用了，就可以被清除掉。所以在某个事务提交后，重用这个事务的insert undo链表（这个链表中只有一个页面）时，可以直接把之前事务写入的一组undo日志覆盖掉，从头开始写入新事务的一组undo日志。 假设有一个事务使用的insert undo链表，到事务提交时，只向insert undo链表中插入了3条undo日志，这个insert undo链表只申请了一个Undo页面。假设此刻该页面已使用的空间小于整个页面大小的3/4，那么下一个事务就可以重用这个insert undo链表（链表中只有一个页面）。假设此时有一个新事务重用了该insert undo链表，那么可以直接把旧的一组undo日志覆盖掉，写入一组新的undo日志。 在重用Undo页面链表写入新的一组undo日志时，不仅会写入新的Undo Log Header，还会适当调整Undo Page Header、Undo Log Segment Header、Undo Log Header中的一些属性，比如TRX_UNDO_PAGE_START、TRX_UNDO_PAGE_FREE等等。 update undo链表在一个事务提交后，它的update undo链表中的undo日志也不能立即删除掉（这些日志用于MVCC）。所以如果之后的事务想重用update undo链表时，就不能覆盖之前事务写入的undo日志。这样就相当于在同一个Undo页面中写入了多组的undo日志。 9.回滚段9.1回滚段的概念一个事务在执行过程中最多可以分配4个Undo页面链表，在同一时刻不同事务拥有的Undo页面链表是不一样的，所以在同一时刻系统里其实可以有许许多多个Undo页面链表存在。为了更好的管理这些链表，InnoDB又设计了一个称之为Rollback Segment Header的页面，在这个页面中存放了各个Undo页面链表的frist undo page的页号，这些页号称之为undo slot。可以这样理解，每个Undo页面链表都相当于是一个班，这个链表的first undo page就相当于这个班的班长，找到了这个班的班长，就可以找到班里的其他同学（其他同学相当于normal undo page）。有时候学校需要向这些班级传达一下精神，就需要把班长都召集在会议室，这个Rollback Segment Header就相当于是一个会议室。 InnoDB规定，每一个Rollback Segment Header页面都对应着一个段，这个段就称为Rollback Segment，翻译过来就是回滚段。与之前介绍的各种段不同的是，这个Rollback Segment里其实只有一个页面。 了解了Rollback Segment的含义之后，我们再来看看这个称之为Rollback Segment Header的页面的各个部分的含义都是啥意思： TRX_RSEG_MAX_SIZE：本Rollback Segment中管理的所有Undo页面链表中的Undo页面数量之和的最大值。换句话说，本Rollback Segment中所有Undo页面链表中的Undo页面数量之和不能超过TRX_RSEG_MAX_SIZE代表的值。该属性的值默认为无限大，也就是我们想写多少Undo页面都可以。 无限大其实也只是个夸张的说法，4个字节能表示最大的数也就是0xFFFFFFFF，但是0xFFFFFFFF这个数有特殊用途，所以实际上TRX_RSEG_MAX_SIZE的值为0xFFFFFFFE。 TRX_RSEG_HISTORY_SIZE：History链表占用的页面数量。 TRX_RSEG_HISTORY：History链表的基节点。 TRX_RSEG_FSEG_HEADER：本Rollback Segment对应的10字节大小的Segment Header结构，通过它可以找到本段对应的INODE Entry。 TRX_RSEG_UNDO_SLOTS：各个Undo页面链表的first undo page的页号集合，也就是undo slot集合。一个页号占用4个字节，对于16KB大小的页面来说，这个TRX_RSEG_UNDO_SLOTS部分共存储了1024个undo slot，所以共需1024 × 4 = 4096个字节。 9.2 从回滚段中申请Undo页面链表初始情况下，由于未向任何事务分配任何Undo页面链表，所以对于一个Rollback Segment Header页面来说，它的各个undo slot都被设置成了一个特殊的值：FIL_NULL（对应的十六进制就是0xFFFFFFFF），表示该undo slot不指向任何页面。 随着时间的流逝，开始有事务需要分配Undo页面链表了，就从回滚段的第一个undo slot开始，看看该undo slot的值是不是FIL_NULL： 如果是FIL_NULL，那么在表空间中新创建一个段（也就是Undo Log Segment），然后从段里申请一个页面作为Undo页面链表的first undo page，然后把该undo slot的值设置为刚刚申请的这个页面的页号，这样也就意味着这个undo slot被分配给了这个事务。 如果不是FIL_NULL，说明该undo slot已经指向了一个undo链表，也就是说这个undo slot已经被别的事务占用了，那就跳到下一个undo slot，判断该undo slot的值是不是FIL_NULL，重复上边的步骤。 一个Rollback Segment Header页面中包含1024个undo slot，如果这1024个undo slot的值都不为FIL_NULL，这就意味着这1024个undo slot都已经被分配给了某个事务，此时由于新事务无法再获得新的Undo页面链表，就会回滚这个事务并且给用户报错： 1Too many active concurrent transactions 用户看到这个错误，可以选择重新执行这个事务（可能重新执行时有别的事务提交了，该事务就可以被分配Undo页面链表了）。 当一个事务提交时，它所占用的undo slot有两种命运： 如果该undo slot指向的Undo页面链表符合被重用的条件（就是我们上边说的Undo页面链表只占用一个页面并且已使用空间小于整个页面的3/4）。该undo slot就处于被缓存的状态，InnoDB规定这时该Undo页面链表的TRX_UNDO_STATE属性（该属性在first undo page的Undo Log Segment Header部分）会被设置为TRX_UNDO_CACHED。被缓存的undo slot都会被加入到一个链表，根据对应的Undo页面链表的类型不同，也会被加入到不同的链表： 如果对应的Undo页面链表是insert undo链表，则该undo slot会被加入insert undo cached链表。 如果对应的Undo页面链表是update undo链表，则该undo slot会被加入update undo cached链表。 ​ 一个回滚段就对应着上述两个cached链表，如果有新事务要分配undo slot时，先从对应的cached链表中找。如果没有被缓存的undo slot，才会到回滚段的Rollback Segment Header页面中再去找。​ 如果该undo slot指向的Undo页面链表不符合被重用的条件，那么针对该undo slot对应的Undo页面链表类型不同，也会有不同的处理： 如果对应的Undo页面链表是insert undo链表，则该Undo页面链表的TRX_UNDO_STATE属性会被设置为TRX_UNDO_TO_FREE，之后该Undo页面链表对应的段会被释放掉（也就意味着段中的页面可以被挪作他用），然后把该undo slot的值设置为FIL_NULL。 如果对应的Undo页面链表是update undo链表，则该Undo页面链表的TRX_UNDO_STATE属性会被设置为TRX_UNDO_TO_PRUGE，则会将该undo slot的值设置为FIL_NULL，然后将本次事务写入的一组undo日志放到所谓的History链表中（需要注意的是，这里并不会将Undo页面链表对应的段给释放掉，因为这些undo日志还有用）。 9.3 多个回滚段一个事务执行过程中最多分配4个Undo页面链表，而一个回滚段里只有1024个undo slot，很显然undo slot的数量有点少。即使假设一个读写事务执行过程中只分配1个Undo页面链表，那1024个undo slot也只能支持1024个读写事务同时执行，再多了就崩溃了。 在InnoDB的早期只有一个回滚段，但后来定义了128个回滚段，也就相当于有了128 × 1024 = 131072个undo slot。假设一个读写事务执行过程中只分配1个Undo页面链表，那么就可以同时支持131072个读写事务并发执行。 只读事务并不需要分配Undo页面链表，MySQL 5.7中所有刚开启的事务默认都是只读事务，只有在事务执行过程中对记录做了某些改动时才会被升级为读写事务。 每个回滚段都对应着一个Rollback Segment Header页面，有128个回滚段，自然就要有128个Rollback Segment Header页面，这些页面的地址需要找个地方存一下！于是InnoDB在系统表空间的第5号页面的某个区域包含了128个8字节大小的格子： 每个8字节的格子的构造就像这样： 如果所示，每个8字节的格子其实由两部分组成： 4字节大小的Space ID，代表一个表空间的ID。 4字节大小的Page number，代表一个页号。 也就是说每个8字节大小的格子相当于一个指针，指向某个表空间中的某个页面，这些页面就是Rollback Segment Header。这里需要注意的一点事，要定位一个Rollback Segment Header还需要知道对应的表空间ID，这也就意味着不同的回滚段可能分布在不同的表空间中。 所以通过上边的叙述我们可以大致清楚，在系统表空间的第5号页面中存储了128个Rollback Segment Header页面地址，每个Rollback Segment Header就相当于一个回滚段。在Rollback Segment Header页面中，又包含1024个undo slot，每个undo slot都对应一个Undo页面链表。我们画个示意图： 9.4回滚段的分类把这128个回滚段给编一下号，最开始的回滚段称之为第0号回滚段，之后依次递增，最后一个回滚段就称之为第127号回滚段。这128个回滚段可以被分成两大类： 第0号、第33～127号回滚段属于一类。其中第0号回滚段必须在系统表空间中（就是说第0号回滚段对应的Rollback Segment Header页面必须在系统表空间中），第33～127号回滚段既可以在系统表空间中，也可以在自己配置的undo表空间中。如果一个事务在执行过程中由于对普通表的记录做了改动需要分配Undo页面链表时，必须从这一类的段中分配相应的undo slot。 第1～32号回滚段属于一类。这些回滚段必须在临时表空间（对应着数据目录中的ibtmp1文件）中。如果一个事务在执行过程中由于对临时表的记录做了改动需要分配Undo页面链表时，必须从这一类的段中分配相应的undo slot。 也就是说如果一个事务在执行过程中既对普通表的记录做了改动，又对临时表的记录做了改动，那么需要为这个记录分配2个回滚段，再分别到这两个回滚段中分配对应的undo slot。 为啥要把针对普通表和临时表来划分不同种类的回滚段呢？这个还得从Undo页面本身说起，我们说Undo页面其实是类型为FIL_PAGE_UNDO_LOG的页面的简称，说到底它也是一个普通的页面。在修改页面之前一定要先把对应的redo日志写上，这样在系统奔溃重启时才能恢复到奔溃前的状态。我们向Undo页面写入undo日志本身也是一个写页面的过程，InnoDB为此还设计了许多种redo日志的类型，也就是说我们对Undo页面做的任何改动都会记录相应类型的redo日志。但是对于临时表来说，因为修改临时表而产生的undo日志只需要在系统运行过程中有效，如果系统奔溃了，那么在重启时也不需要恢复这些undo日志所在的页面，所以在写针对临时表的Undo页面时，并不需要记录相应的redo日志。总结一下针对普通表和临时表划分不同种类的回滚段的原因：在修改针对普通表的回滚段中的Undo页面时，需要记录对应的redo日志，而修改针对临时表的回滚段中的Undo页面时，不需要记录对应的redo日志。 实际上在MySQL 5.7.21这个版本中，如果我们仅仅对普通表的记录做了改动，那么只会为该事务分配针对普通表的回滚段，不分配针对临时表的回滚段。但是如果我们仅仅对临时表的记录做了改动，那么既会为该事务分配针对普通表的回滚段，又会为其分配针对临时表的回滚段（不过分配了回滚段并不会立即分配undo slot，只有在真正需要Undo页面链表时才会去分配回滚段中的undo slot）。 9.5 为事务分配Undo页面链表详细过程接下来以事务对普通表的记录做改动为例，梳理一下事务执行过程中分配Undo页面链表时的完整过程： 事务在执行过程中对普通表的记录首次做改动之前，首先会到系统表空间的第5号页面中分配一个回滚段（其实就是获取一个Rollback Segment Header页面的地址）。一旦某个回滚段被分配给了这个事务，那么之后该事务中再对普通表的记录做改动时，就不会重复分配了。使用round-robin（循环使用）方式来分配回滚段。比如当前事务分配了第0号回滚段，那么下一个事务就要分配第33号回滚段，下下个事务就要分配第34号回滚段，简单一点的说就是这些回滚段被轮着分配给不同的事务。 ​ 在分配到回滚段后，首先看一下这个回滚段的两个cached链表有没有已经缓存了的undo slot，比如如果事务做的是INSERT操作，就去回滚段对应的insert undo cached链表中看看有没有缓存的undo slot；如果事务做的是DELETE操作，就去回滚段对应的update undo cached链表中看看有没有缓存的undo slot。如果有缓存的undo slot，那么就把这个缓存的undo slot分配给该事务。 ​ 如果没有缓存的undo slot可供分配，那么就要到Rollback Segment Header页面中找一个可用的undo slot分配给当前事务。从Rollback Segment Header页面中分配可用的undo slot的方式我们上边也说过了，就是从第0个undo slot开始，如果该undo slot的值为FIL_NULL，意味着这个undo slot是空闲的，就把这个undo slot分配给当前事务，否则查看第1个undo slot是否满足条件，依次类推，直到最后一个undo slot。如果这1024个undo slot都没有值为FIL_NULL的情况，就直接报错（一般不会出现这种情况）。 ​ 找到可用的undo slot后，如果该undo slot是从cached链表中获取的，那么它对应的Undo Log Segment已经分配了，否则的话需要重新分配一个Undo Log Segment，然后从该Undo Log Segment中申请一个页面作为Undo页面链表的first undo page。 ​ 然后事务就可以把undo日志写入到上边申请的Undo页面链表了！ 对临时表的记录做改动的步骤和上述的一样。不过需要再次强调一次，如果一个事务在执行过程中既对普通表的记录做了改动，又对临时表的记录做了改动，那么需要为这个记录分配2个回滚段。并发执行的不同事务其实也可以被分配相同的回滚段，只要分配不同的undo slot就可以了。 10.回滚段相关配置10.1配置回滚段数量系统中一共有128个回滚段，其实这只是默认值，我们可以通过启动参数innodb_rollback_segments来配置回滚段的数量，可配置的范围是1~128。但是这个参数并不会影响针对临时表的回滚段数量，针对临时表的回滚段数量一直是32，也就是说： 如果我们把innodb_rollback_segments的值设置为1，那么只会有1个针对普通表的可用回滚段，但是仍然有32个针对临时表的可用回滚段。 如果我们把innodb_rollback_segments的值设置为2～33之间的数，效果和将其设置为1是一样的。 如果我们把innodb_rollback_segments设置为大于33的数，那么针对普通表的可用回滚段数量就是该值减去32。 10.2 配置undo表空间默认情况下，针对普通表设立的回滚段（第0号以及第33~127号回滚段）都是被分配到系统表空间的。其中的第0号回滚段是一直在系统表空间的，但是第33~127号回滚段可以通过配置放到自定义的undo表空间中。但是这种配置只能在系统初始化（创建数据目录时）的时候使用，一旦初始化完成，之后就不能再次更改了。我们看一下相关启动参数： 通过innodb_undo_directory指定undo表空间所在的目录，如果没有指定该参数，则默认undo表空间所在的目录就是数据目录。 通过innodb_undo_tablespaces定义undo表空间的数量。该参数的默认值为0，表明不创建任何undo表空间。第33~127号回滚段可以平均分布到不同的undo表空间中。 如果我们在系统初始化的时候指定了创建了undo表空间，那么系统表空间中的第0号回滚段将处于不可用状态。 比如我们在系统初始化时指定的innodb_rollback_segments为35，innodb_undo_tablespaces为2，这样就会将第33、34号回滚段分别分布到一个undo表空间中。 设立undo表空间的一个好处就是在undo表空间中的文件大到一定程度时，可以自动的将该undo表空间截断（truncate）成一个小文件。而系统表空间的大小只能不断的增大，却不能截断。 11.总结为了保证事务的原子性设计，InnoDB引入了undo日志。undo日志记载了回滚一个段所需的必要内容。 在事务对表中的记录进行改动的时候，才会为这个事务分配一个唯一的ID。事务ID值是一个递增的数字。先被分配ID的事务得到的是较小的事务ID，后被分配ID的事务得到的是较大的事务ID。未被分配事务ID的事务ID默认是0。聚簇索引记录中有一个trx_id隐藏列，他代表对这个聚簇索引隐藏记录进行改动的语句所在的事务对应的事务ID。 InnoDB针对不同的场景设计了不同类型的undo日志。 类型为FIL_PAGE_UNDO_LOG的页面是专门用来存储undo日志的，简称为undo页面。 在一个事务执行过程中，最多分配四个undo页面链表： 针对普通表的insert undo链表 针对普通表的update undo链表 针对临时表的insert undo链表 针对临时表的update undo链表 只有在真正用到这些链表的时候才会去创建他们。 每个undo页面链表都对应一个undo log segment。undo页面链表的第一个页面中有一个名为undo log segment header 的部分，专门用来存储关于这个段的一些信息。 同一个事务向一个undo页面链表中写入的undo日志算是一个组，每个组都以一个undo log header部分开头。 一个undo页面链表如果可以被重用，需要符合两个条件： 该链表只包含一个undo页面 该undo页面已经使用的空间小于整个页面空间的3/4 每一个Rollback segmrnt header 页面都对应一个回滚段，每个回滚段包含1024个undo slot，一个undo slot代表一个undo页面链表的第一个页面的页号。目前，InnoDB最多支持128个回滚段，其中第0号，第33127号回滚段是针对普通表设计的，第132号回滚段是针对临时表设计的。 我们可以选择将undo日志记录到专门的undo表空间中，在undo表空间中的文件大到一定程度时，可以自动将该undo表空间截断为小文件。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/tags/MySQL/"}]},{"title":"MySQL[十四]redo日志","slug":"MySQL/MySQL[十四]redo日志","date":"2022-01-13T16:00:00.000Z","updated":"2022-01-12T00:42:54.375Z","comments":true,"path":"2022/01/14/MySQL/MySQL[十四]redo日志/","link":"","permalink":"https://yinhuidong.github.io/2022/01/14/MySQL/MySQL[%E5%8D%81%E5%9B%9B]redo%E6%97%A5%E5%BF%97/","excerpt":"","text":"一，什么是redo日志InnoDB存储引擎是以页为单位来管理存储空间的，我们的CRUD操作其实都是在访问页面。在真正访问页面之前，需要把磁盘中的页加载到内存的BufferPool，之后才能访问，但是因为事务要保持持久性，如果我们仅仅在内存的缓冲池修改了页面，假设事务提交后突然发生故障，导致内存的数据都消失了，那么这个已经提交的事务在数据库做的更改就丢失了。 如何保证持久性呢？可以在事务提交完成之前，把事务修改的所有页面都刷新到磁盘。不过这样做存在一些问题： 刷新一个完整的数据页过于浪费 随机IO效率比较低 事实上仅仅是为了保证事务的持久性，没有必要每次提交事务的时候就把该事务在内存修改过的全部页面刷新到磁盘，只需要把修改的内容记录一下就好，这样在事务提交的时候，就会把这个记录刷新到磁盘。即使系统因为崩溃而重启只需要按照记录的内容重新更新数据页即可恢复数据，上述记录修改的内容就叫做重做日志（redo log）。 相比于在事务提交的时候将所有修改过的内存中的页面刷新到磁盘，重做日志有以下好处： redo日志占用空间小：在存储表空间ID，页号，偏移量以及需要更新的值时，需要的存储空间很小。 redo日志是顺序写入磁盘的：在执行事务过程中，每执行一条语句，就可能产生若干条redo日志，这些日志是按照产生的顺序写入磁盘的，也就是使用顺序IO。 二，redo日志格式重做日志本质上仅仅是记录了一下事务对数据库进行了哪些修改。针对事务对数据库的不同修改场景MySQL定义了很多种重做日志，但是大部分类型的重做日志都有以下的通用结构。 type 重做日志的类型 space ID 表空间ID page number 页号 Data 日志的具体内容 1. 简单的redo日志类型行格式里面有一个隐藏列叫做row_id。为row_id进行赋值的方式如下： 服务器会在内存中维护一个全局变量，每当像某个包含row_id隐藏列的表插入一条记录的时候，就会把这个全局变量的值当做新记录row_id的值，并且把这个全局变量自增1。 每当这个全局变量的值是256的整数倍的时候，就会把这个变量的值刷新到系统表空间页号为7的页面中一个叫做Max Row ID的属性中。 当系统启动的时候，会将Max Row ID属性加载到内存，并把这个值加上256之后赋值给前面提到的全局变量。 这个Max Row ID占用的存储空间是8字节。当某个事务向某个包含row_id的表插入一条记录并且该记录分配的row_id值为256的整数倍的时候，就会像系统表空间页号为7的页面的相应偏移量处写入8字节的值。但是这个写入操作实际上是在内存缓冲区完成的，我们需要把这次修改以redo日志的形式记录下来，这样在事务提交之后，即使系统崩溃，也可以将该页面恢复成崩溃前的状态。在这种对页面的修改特别简单的时候，重做日志仅仅需要记录一下在某个页面的某个偏移量处修改了几个字节的值，具体修改后的内容是什么就可以了。这也叫做物理日志。 offset表示页面中的偏移量。如果写入的是字节序列类型的重做日志，还需要有一个len属性记录实际写入的长度。 2.复杂的redo日志类型有时候执行一条语句会修改非常多的页面，包括系统数据页面和用户数据页面（用户数据指的就是聚簇索引和二级索引对应的B+树）。 这时我们如果使用简单的物理redo日志来记录这些修改时，可以有两种解决方案： 方案一：在每个修改的地方都记录一条redo日志。也就是有多少个修改的记录，就写多少条物理redo日志。这样子记录redo日志的缺点是显而易见的，因为被修改的地方是在太多了，可能记录的redo日志占用的空间都比整个页面占用的空间都多。 方案二：将整个页面的第一个被修改的字节到最后一个修改的字节之间所有的数据当成是一条物理redo日志中的具体数据。第一个被修改的字节到最后一个修改的字节之间仍然有许多没有修改过的数据，我们把这些没有修改的数据也加入到redo日志中太浪费了。 正因为上述两种使用物理redo日志的方式来记录某个页面中做了哪些修改比较浪费，InnoDB提出了一些新的redo日志类型。 这些类型的redo日志既包含物理层面的意思，也包含逻辑层面的意思，具体指： 物理层面看，这些日志都指明了对哪个表空间的哪个页进行了修改。 逻辑层面看，在系统崩溃重启时，并不能直接根据这些日志里的记载，将页面内的某个偏移量处恢复成某个数据，而是需要调用一些事先准备好的函数，执行完这些函数后才可以将页面恢复成系统崩溃前的样子。 这个类型为MLOG_COMP_REC_INSERT的redo日志并没有记录PAGE_N_DIR_SLOTS的值修改为了什么，PAGE_HEAP_TOP的值修改为了什么，PAGE_N_HEAP的值修改为了什么等等这些信息，而只是把在本页面中插入一条记录所有必备的要素记了下来，之后系统崩溃重启时，服务器会调用相关向某个页面插入一条记录的那个函数，而redo日志中的那些数据就可以被当成是调用这个函数所需的参数，在调用完该函数后，页面中的PAGE_N_DIR_SLOTS、PAGE_HEAP_TOP、PAGE_N_HEAP等等的值也就都被恢复到系统崩溃前的样子了。这就是所谓的逻辑日志的意思。 日志格式说了一堆核心其实就是：重做日志会把事务执行过程中对数据库所做的所有修改都记录下来，在之后系统因为崩溃而重启后可以把事务所做的任何修改都恢复过来。 为了节省重做日志占用的空间大小，InnoDB还对重做日志中的某些数据进行了压缩处理，比如表空间ID&amp;page number 一般占用4字节来存储，但是经过压缩之后占用的空间就更小了。 三，Mini-Transcation1.以组的形式写入redo日志语句在执行过程中可能修改若干个页面。比如我们前边说的一条INSERT语句可能修改系统表空间页号为7的页面的Max Row ID属性（当然也可能更新别的系统页面，只不过我们没有都列举出来而已），还会更新聚簇索引和二级索引对应B+树中的页面。由于对这些页面的更改都发生在Buffer Pool中，所以在修改完页面之后，需要记录一下相应的redo日志。在执行语句的过程中产生的redo日志被InnoDB人为的划分成了若干个不可分割的组，比如： 更新Max Row ID属性时产生的redo日志是不可分割的。 向聚簇索引对应B+树的页面中插入一条记录时产生的redo日志是不可分割的。 向某个二级索引对应B+树的页面中插入一条记录时产生的redo日志是不可分割的。 还有其他的一些对页面的访问操作时产生的redo日志是不可分割的。。。 怎么理解这个不可分割的意思呢？我们以向某个索引对应的B+树插入一条记录为例，在向B+树中插入这条记录之前，需要先定位到这条记录应该被插入到哪个叶子节点代表的数据页中，定位到具体的数据页之后，有两种可能的情况： 情况一：该数据页的剩余的空闲空间充足，足够容纳这一条待插入记录，那么事情很简单，直接把记录插入到这个数据页中，记录一条类型为MLOG_COMP_REC_INSERT的redo日志就好了，我们把这种情况称之为乐观插入。假如某个索引对应的B+树长这样：现在我们要插入一条键值为10的记录，很显然需要被插入到页b中，由于页b现在有足够的空间容纳一条记录，所以直接将该记录插入到页b中就好了，就像这样： 情况二：该数据页剩余的空闲空间不足，那么事情就悲剧了，我们前边说过，遇到这种情况要进行所谓的页分裂操作，也就是新建一个叶子节点，然后把原先数据页中的一部分记录复制到这个新的数据页中，然后再把记录插入进去，把这个叶子节点插入到叶子节点链表中，最后还要在内节点中添加一条目录项记录指向这个新创建的页面。很显然，这个过程要对多个页面进行修改，也就意味着会产生多条redo日志，我们把这种情况称之为悲观插入。假如某个索引对应的B+树长这样：现在我们要插入一条键值为10的记录，很显然需要被插入到页b中，但是从图中也可以看出来，此时页b已经塞满了记录，没有更多的空闲空间来容纳这条新记录了，所以我们需要进行页面的分裂操作，就像这样：如果作为内节点的页a的剩余空闲空间也不足以容纳增加一条目录项记录，那需要继续做内节点页a的分裂操作，也就意味着会修改更多的页面，从而产生更多的redo日志。另外，对于悲观插入来说，由于需要新申请数据页，还需要改动一些系统页面，比方说要修改各种段、区的统计信息信息，各种链表的统计信息（比如什么FREE链表、FSP_FREE_FRAG链表等，我们在介绍表空间那一篇中介绍过的各种东西），反正总共需要记录的redo日志有二、三十条。 其实不光是悲观插入一条记录会生成许多条redo日志，InnoDB为了其他的一些功能，在乐观插入时也可能产生多条redo日志。 InnoDB认为向某个索引对应的B+树中插入一条记录的这个过程必须是原子的，不能说插了一半之后就停止了。比方说在悲观插入过程中，新的页面已经分配好了，数据也复制过去了，新的记录也插入到页面中了，可是没有向内节点中插入一条目录项记录，这个插入过程就是不完整的，这样会形成一棵不正确的B+树。我们知道redo日志是为了在系统崩溃重启时恢复崩溃前的状态，如果在悲观插入的过程中只记录了一部分redo日志，那么在系统崩溃重启时会将索引对应的B+树恢复成一种不正确的状态，这是InnoDB所不能忍受的。所以他们规定在执行这些需要保证原子性的操作时必须以组的形式来记录的redo日志，在进行系统崩溃重启恢复时，针对某个组中的redo日志，要么把全部的日志都恢复掉，要么一条也不恢复。怎么做到的呢？这得分情况讨论： 有的需要保证原子性的操作会生成多条redo日志，比如向某个索引对应的B+树中进行一次悲观插入就需要生成许多条redo日志。如何把这些redo日志划分到一个组里边儿呢？InnoDB做了一个很简单的操作，就是在该组中的最后一条redo日志后边加上一条特殊类型的redo日志，该类型名称为MLOG_MULTI_REC_END，type字段对应的十进制数字为31，该类型的redo日志结构很简单，只有一个type字段：所以某个需要保证原子性的操作产生的一系列redo日志必须要以一个类型为MLOG_MULTI_REC_END结尾，就像这样：这样在系统崩溃重启进行恢复时，只有当解析到类型为MLOG_MULTI_REC_END的redo日志，才认为解析到了一组完整的redo日志，才会进行恢复。否则的话直接放弃前边解析到的redo日志。 有的需要保证原子性的操作只生成一条redo日志，比如更新Max Row ID属性的操作就只会生成一条redo日志。其实在一条日志后边跟一个类型为MLOG_MULTI_REC_END的redo日志也是可以的，InnoDB不想浪费一个比特位。虽然redo日志的类型比较多，但撑死了也就是几十种，是小于127这个数字的，也就是说我们用7个比特位就足以包括所有的redo日志类型，而type字段其实是占用1个字节的，也就是说我们可以省出来一个比特位用来表示该需要保证原子性的操作只产生单一的一条redo日志，示意图如下：如果type字段的第一个比特位为1，代表该需要保证原子性的操作只产生了单一的一条redo日志，否则表示该需要保证原子性的操作产生了一系列的redo日志。 2.Mini-TransactionMySQL把对底层页面中的一次原子访问的过程称之为一个Mini-Transaction，简称mtr，比如上边所说的修改一次Max Row ID的值算是一个Mini-Transaction，向某个索引对应的B+树中插入一条记录的过程也算是一个Mini-Transaction。一个所谓的mtr可以包含一组redo日志，在进行崩溃恢复时这一组redo日志作为一个不可分割的整体。 一个事务可以包含若干条语句，每一条语句其实是由若干个mtr组成，每一个mtr又可以包含若干条redo日志，画个图表示它们的关系就是这样： 四，redo日志的写入过程1.redo log blockInnoDB为了更好的进行系统崩溃恢复，他们把通过mtr生成的redo日志都放在了大小为512字节的页中。为了和表空间中的页做区别，我们这里把用来存储redo日志的页称为block。一个redo log block的示意图如下： 真正的redo日志都是存储到占用496字节大小的log block body中，图中的log block header和log block trailer存储的是一些管理信息。 其中log block header的几个属性的意思分别如下： LOG_BLOCK_HDR_NO：每一个block都有一个大于0的唯一标号，本属性就表示该标号值。 LOG_BLOCK_HDR_DATA_LEN：表示block中已经使用了多少字节，初始值为12（因为log block body从第12个字节处开始）。随着往block中写入的redo日志越来也多，本属性值也跟着增长。如果log block body已经被全部写满，那么本属性的值被设置为512。 LOG_BLOCK_FIRST_REC_GROUP：一条redo日志也可以称之为一条redo日志记录（redo log record），一个mtr会生产多条redo日志记录，这些redo日志记录被称之为一个redo日志记录组（redo log record group）。LOG_BLOCK_FIRST_REC_GROUP就代表该block中第一个mtr生成的redo日志记录组的偏移量（其实也就是这个block里第一个mtr生成的第一条redo日志的偏移量）。 LOG_BLOCK_CHECKPOINT_NO：表示所谓的checkpoint的序号，checkpoint是我们后续内容的重点，现在先不用清楚它的意思，稍安勿躁。 log block trailer中属性的意思如下： LOG_BLOCK_CHECKSUM：表示block的校验值，用于正确性校验，我们暂时不关心它。 2.redo 日志缓冲区InnoDB为了解决磁盘速度过慢的问题而引入了Buffer Pool。同理，写入redo日志时也不能直接直接写到磁盘上，实际上在服务器启动时就向操作系统申请了一大片称之为redo log buffer的连续内存空间，翻译成中文就是redo日志缓冲区，也可以简称为log buffer。这片内存空间被划分成若干个连续的redo log block，就像这样： 我们可以通过启动参数innodb_log_buffer_size来指定log buffer的大小，在MySQL 5.7.21这个版本中，该启动参数的默认值为16MB。 3.redo log 日志写入log buffer向log buffer中写入redo日志的过程是顺序的，也就是先往前边的block中写，当该block的空闲空间用完之后再往下一个block中写。当我们想往log buffer中写入redo日志时，第一个遇到的问题就是应该写在哪个block的哪个偏移量处，所以InnoDB特意提供了一个称之为buf_free的全局变量，该变量指明后续写入的redo日志应该写入到log buffer中的哪个位置，如图所示： 一个mtr执行过程中可能产生若干条redo日志，这些redo日志是一个不可分割的组，所以其实并不是每生成一条redo日志，就将其插入到log buffer中，而是每个mtr运行过程中产生的日志先暂时存到一个地方，当该mtr结束的时候，将过程中产生的一组redo日志再全部复制到log buffer中。我们现在假设有两个名为T1、T2的事务，每个事务都包含2个mtr，我们给这几个mtr命名一下： 事务T1的两个mtr分别称为mtr_T1_1和mtr_T1_2。 事务T2的两个mtr分别称为mtr_T2_1和mtr_T2_2。 每个mtr都会产生一组redo日志，不同的事务可能是并发执行的，所以T1、T2之间的mtr可能是交替执行的。每当一个mtr执行完成时，伴随该mtr生成的一组redo日志就需要被复制到log buffer中，也就是说不同事务的mtr可能是交替写入log buffer的，我们画个示意图（为了美观，我们把一个mtr中产生的所有的redo日志当作一个整体来画）： 从示意图中我们可以看出来，不同的mtr产生的一组redo日志占用的存储空间可能不一样，有的mtr产生的redo日志量很少，比如mtr_t1_1、mtr_t2_1就被放到同一个block中存储，有的mtr产生的redo日志量非常大，比如mtr_t1_2产生的redo日志甚至占用了3个block来存储。 五，redo 日志文件1.redo日志刷盘时机mtr运行过程中产生的一组redo日志在mtr结束时会被复制到log buffer中，在一些情况下它们会被刷新到磁盘里，比如： log buffer空间不足时log buffer的大小是有限的（通过系统变量innodb_log_buffer_size指定），如果不停的往这个有限大小的log buffer里塞入日志，很快它就会被填满。InnoDB认为如果当前写入log buffer的redo日志量已经占满了log buffer总容量的大约一半左右，就需要把这些日志刷新到磁盘上。 事务提交时之所以使用redo日志主要是因为它占用的空间少，还是顺序写，在事务提交时可以不把修改过的Buffer Pool页面刷新到磁盘，但是为了保证持久性，必须要把修改这些页面对应的redo日志刷新到磁盘。 将某个脏页刷新到磁盘前，会保证先将该脏页对应的 redo 日志刷新到磁盘中（再一次 强调，redo 日志是顺序刷新的，所以在将某个脏页对应的 redo 日志从 redo log buffer 刷新到磁盘时，也会保证将在其之前产生的 redo 日志也刷新到磁盘）。 后台线程不停的刷后台有一个线程，大约每秒都会刷新一次log buffer中的redo日志到磁盘。 正常关闭服务器时 做所谓的checkpoint时 其他的一些情况… 2.redo日志文件组MySQL的数据目录（使用SHOW VARIABLES LIKE &#39;datadir&#39;查看）下默认有两个名为ib_logfile0和ib_logfile1的文件，log buffer中的日志默认情况下就是刷新到这两个磁盘文件中。如果我们对默认的redo日志文件不满意，可以通过下边几个启动参数来调节： innodb_log_group_home_dir该参数指定了redo日志文件所在的目录，默认值就是当前的数据目录。 innodb_log_file_size该参数指定了每个redo日志文件的大小，在MySQL 5.7.21这个版本中的默认值为48MB， innodb_log_files_in_group该参数指定redo日志文件的个数，默认值为2，最大值为100。 磁盘上的redo日志文件不只一个，而是以一个日志文件组的形式出现的。这些文件以ib_logfile[数字]（数字可以是0、1、2…）的形式进行命名。在将redo日志写入日志文件组时，是从ib_logfile0开始写，如果ib_logfile0写满了，就接着ib_logfile1写，同理，ib_logfile1写满了就去写ib_logfile2，依此类推。如果写到最后一个文件该咋办？那就重新转到ib_logfile0继续写，所以整个过程如下图所示： 总共的redo日志文件大小其实就是：innodb_log_file_size × innodb_log_files_in_group。 如果采用循环使用的方式向redo日志文件组里写数据的话，那岂不是要追尾，也就是后写入的redo日志覆盖掉前边写的redo日志？当然可能了！所以InnoDB提出了checkpoint的概念。 3.redo日志文件格式log buffer本质上是一片连续的内存空间，被划分成了若干个512字节大小的block。将log buffer中的redo日志刷新到磁盘的本质就是把block的镜像写入日志文件中，所以redo日志文件其实也是由若干个512字节大小的block组成。 redo日志文件组中的每个文件大小都一样，格式也一样，都是由两部分组成： 前2048个字节，也就是前4个block是用来存储一些管理信息的。 从第2048字节往后是用来存储log buffer中的block镜像的。 所以我们前边所说的循环使用redo日志文件，其实是从每个日志文件的第2048个字节开始算，画个示意图就是这样： 普通block的格式我们在了解log buffer的时候都说过了，就是log block header、log block body、log block trialer这三个部分。这里需要介绍一下每个redo日志文件前2048个字节，也就是前4个特殊block的格式都是什么作用。 从图中可以看出来，这4个block分别是： log file header：描述该redo日志文件的一些整体属性各个属性的具体释义如下： 属性名 长度（单位：字节） 描述 LOG_HEADER_FORMAT 4 redo日志的版本，在MySQL 5.7.21中该值永远为1 LOG_HEADER_PAD1 4 做字节填充用的，没什么实际意义，忽略～ LOG_HEADER_START_LSN 8 标记本redo日志文件开始的LSN值，也就是文件偏移量为2048字节初对应的LSN值。 LOG_HEADER_CREATOR 32 一个字符串，标记本redo日志文件的创建者是谁。正常运行时该值为MySQL的版本号，比如：&quot;MySQL 5.7.21&quot;，使用mysqlbackup命令创建的redo日志文件的该值为&quot;ibbackup&quot;和创建时间。 LOG_BLOCK_CHECKSUM 4 本block的校验值，所有block都有，我们不关心 checkpoint1：记录关于checkpoint的一些属性，看一下它的结构：各个属性的具体释义如下： 属性名 长度（单位：字节） 描述 LOG_CHECKPOINT_NO 8 服务器做checkpoint的编号，每做一次checkpoint，该值就加1。 LOG_CHECKPOINT_LSN 8 服务器做checkpoint结束时对应的LSN值，系统崩溃恢复时将从该值开始。 LOG_CHECKPOINT_OFFSET 8 上个属性中的LSN值在redo日志文件组中的偏移量 LOG_CHECKPOINT_LOG_BUF_SIZE 8 服务器在做checkpoint操作时对应的log buffer的大小 LOG_BLOCK_CHECKSUM 4 本block的校验值，所有block都有，我们不关心 第三个block未使用，忽略 checkpoint2：结构和checkpoint1一样。 六，Log Sequence Number自系统开始运行，就不断的在修改页面，也就意味着会不断的生成redo日志。redo日志的量在不断的递增。InnoDB为记录已经写入的redo日志量，设计了一个称之为Log Sequence Number的全局变量，翻译过来就是：日志序列号，简称lsn。InnoDB规定初始的lsn值为8704（也就是一条redo日志也没写入时，lsn的值为8704）。 在向log buffer中写入redo日志时不是一条一条写入的，而是以一个mtr生成的一组redo日志为单位进行写入的。而且实际上是把日志内容写在了log block body处。但是在统计lsn的增长量时，是按照实际写入的日志量加上占用的log block header和log block trailer来计算的。我们来看一个例子： 系统第一次启动后初始化log buffer时，buf_free（就是标记下一条redo日志应该写入到log buffer的位置的变量）就会指向第一个block的偏移量为12字节（log block header的大小）的地方，那么lsn值也会跟着增加12： 如果某个mtr产生的一组redo日志占用的存储空间比较小，也就是待插入的block剩余空闲空间能容纳这个mtr提交的日志时，lsn增长的量就是该mtr生成的redo日志占用的字节数，就像这样：我们假设上图中mtr_1产生的redo日志量为200字节，那么lsn就要在8716的基础上增加200，变为8916。 如果某个mtr产生的一组redo日志占用的存储空间比较大，也就是待插入的block剩余空闲空间不足以容纳这个mtr提交的日志时，lsn增长的量就是该mtr生成的redo日志占用的字节数加上额外占用的log block header和log block trailer的字节数，就像这样：我们假设上图中mtr_2产生的redo日志量为1000字节，为了将mtr_2产生的redo日志写入log buffer，我们不得不额外多分配两个block，所以lsn的值需要在8916的基础上增加1000 + 12×2 + 4 × 2 = 1032。 从上边的描述中可以看出来，每一组由mtr生成的redo日志都有一个唯一的LSN值与其对应，LSN值越小，说明redo日志产生的越早。 1.flushed_to_disk_lsnredo日志是首先写到log buffer中，之后才会被刷新到磁盘上的redo日志文件。所以InnoDB提出了一个称之为buf_next_to_write的全局变量，标记当前log buffer中已经有哪些日志被刷新到磁盘中了。画个图表示就是这样： lsn是表示当前系统中写入的redo日志量，这包括了写到log buffer而没有刷新到磁盘的日志，相应的，InnoDB提出了一个表示刷新到磁盘中的redo日志量的全局变量，称之为flushed_to_disk_lsn。系统第一次启动时，该变量的值和初始的lsn值是相同的，都是8704。随着系统的运行，redo日志被不断写入log buffer，但是并不会立即刷新到磁盘，lsn的值就和flushed_to_disk_lsn的值拉开了差距。我们推理一下： 系统第一次启动后，向log buffer中写入了mtr_1、mtr_2、mtr_3这三个mtr产生的redo日志，假设这三个mtr开始和结束时对应的lsn值分别是： mtr_1：8716 ～ 8916 mtr_2：8916 ～ 9948 mtr_3：9948 ～ 10000 此时的lsn已经增长到了10000，但是由于没有刷新操作，所以此时flushed_to_disk_lsn的值仍为8704，如图： 随后进行将log buffer中的block刷新到redo日志文件的操作，假设将mtr_1和mtr_2的日志刷新到磁盘，那么flushed_to_disk_lsn就应该增长mtr_1和mtr_2写入的日志量，所以flushed_to_disk_lsn的值增长到了9948，如图： 综上所述，当有新的redo日志写入到log buffer时，首先lsn的值会增长，但flushed_to_disk_lsn不变，随后随着不断有log buffer中的日志被刷新到磁盘上，flushed_to_disk_lsn的值也跟着增长。如果两者的值相同时，说明log buffer中的所有redo日志都已经刷新到磁盘中了。 应用程序向磁盘写入文件时其实是先写到操作系统的缓冲区中去，如果某个写入操作要等到操作系统确认已经写到磁盘时才返回，那需要调用一下操作系统提供的fsync函数。其实只有当系统执行了fsync函数后，flushed_to_disk_lsn的值才会跟着增长，当仅仅把log buffer中的日志写入到操作系统缓冲区却没有显式的刷新到磁盘时，另外的一个称之为write_lsn的值跟着增长。 2.lsn值和redo日志文件偏移量的对应关系因为lsn的值是代表系统写入的redo日志量的一个总和，一个mtr中产生多少日志，lsn的值就增加多少（当然有时候要加上log block header和log block trailer的大小），这样mtr产生的日志写到磁盘中时，很容易计算某一个lsn值在redo日志文件组中的偏移量，如图： 初始时的LSN值是8704，对应文件偏移量2048，之后每个mtr向磁盘中写入多少字节日志，lsn的值就增长多少。 3.flush链表中的LSN一个mtr代表一次对底层页面的原子访问，在访问过程中可能会产生一组不可分割的redo日志，在mtr结束时，会把这一组redo日志写入到log buffer中。除此之外，在mtr结束时还有一件非常重要的事情要做，就是把在mtr执行过程中可能修改过的页面加入到Buffer Pool的flush链表。 当第一次修改某个缓存在Buffer Pool中的页面时，就会把这个页面对应的控制块插入到flush链表的头部，之后再修改该页面时由于它已经在flush链表中了，就不再次插入了。也就是说flush链表中的脏页是按照页面的第一次修改时间从大到小进行排序的。在这个过程中会在缓存页对应的控制块中记录两个关于页面何时修改的属性： oldest_modification：如果某个页面被加载到Buffer Pool后进行第一次修改，那么就将修改该页面的mtr开始时对应的lsn值写入这个属性。 newest_modification：每修改一次页面，都会将修改该页面的mtr结束时对应的lsn值写入这个属性。也就是说该属性表示页面最近一次修改后对应的系统lsn值。 接着上边flushed_to_disk_lsn的例子看一下： 假设mtr_1执行过程中修改了页a，那么在mtr_1执行结束时，就会将页a对应的控制块加入到flush链表的头部。并且将mtr_1开始时对应的lsn，也就是8716写入页a对应的控制块的oldest_modification属性中，把mtr_1结束时对应的lsn，也就是8916写入页a对应的控制块的newest_modification属性中。画个图表示一下（oldest_modification缩写成了o_m，newest_modification缩写成了n_m）： 接着假设mtr_2执行过程中又修改了页b和页c两个页面，那么在mtr_2执行结束时，就会将页b和页c对应的控制块都加入到flush链表的头部。并且将mtr_2开始时对应的lsn，也就是8916写入页b和页c对应的控制块的oldest_modification属性中，把mtr_2结束时对应的lsn，也就是9948写入页b和页c对应的控制块的newest_modification属性中。画个图表示一下：从图中可以看出来，每次新插入到flush链表中的节点都是被放在了头部，也就是说flush链表中前边的脏页修改的时间比较晚，后边的脏页修改时间比较早。 接着假设mtr_3执行过程中修改了页b和页d，不过页b之前已经被修改过了，所以它对应的控制块已经被插入到了flush链表，所以在mtr_3执行结束时，只需要将页d对应的控制块都加入到flush链表的头部即可。所以需要将mtr_3开始时对应的lsn，也就是9948写入页d对应的控制块的oldest_modification属性中，把mtr_3结束时对应的lsn，也就是10000写入页d对应的控制块的newest_modification属性中。另外，由于页b在mtr_3执行过程中又发生了一次修改，所以需要更新页b对应的控制块中newest_modification的值为10000。画个图表示一下： 总结一下上边说的，就是：flush链表中的脏页按照修改发生的时间顺序进行排序，也就是按照oldest_modification代表的LSN值进行排序，被多次更新的页面不会重复插入到flush链表中，但是会更新newest_modification属性的值。 七，checkpointredo日志文件组容量是有限的，我们不得不选择循环使用redo日志文件组中的文件，但是这会造成最后写的redo日志与最开始写的redo日志追尾，这时应该想到：redo日志只是为了系统崩溃后恢复脏页用的，如果对应的脏页已经刷新到了磁盘，也就是说即使现在系统崩溃，那么在重启后也用不着使用redo日志恢复该页面了，所以该redo日志也就没有存在的必要了，那么它占用的磁盘空间就可以被后续的redo日志所重用。也就是说：判断某些redo日志占用的磁盘空间是否可以覆盖的依据就是它对应的脏页是否已经刷新到磁盘里。我们看一下前边的那个例子： 如图，虽然mtr_1和mtr_2生成的redo日志都已经被写到了磁盘上，但是它们修改的脏页仍然留在Buffer Pool中，所以它们生成的redo日志在磁盘上的空间是不可以被覆盖的。之后随着系统的运行，如果页a被刷新到了磁盘，那么它对应的控制块就会从flush链表中移除，就像这样子： 这样mtr_1生成的redo日志就没有用了，它们占用的磁盘空间就可以被覆盖掉了。InnoDB提出了一个全局变量checkpoint_lsn来代表当前系统中可以被覆盖的redo日志总量是多少，这个变量初始值也是8704。 比方说现在页a被刷新到了磁盘，mtr_1生成的redo日志就可以被覆盖了，所以我们可以进行一个增加checkpoint_lsn的操作，我们把这个过程称之为做一次checkpoint。做一次checkpoint其实可以分为两个步骤： 步骤一：计算一下当前系统中可以被覆盖的redo日志对应的lsn值最大是多少。redo日志可以被覆盖，意味着它对应的脏页被刷到了磁盘，只要我们计算出当前系统中被最早修改的脏页对应的oldest_modification值，那凡是在系统lsn值小于该节点的oldest_modification值时产生的redo日志都是可以被覆盖掉的，我们就把该脏页的oldest_modification赋值给checkpoint_lsn。比方说当前系统中页a已经被刷新到磁盘，那么flush链表的尾节点就是页c，该节点就是当前系统中最早修改的脏页了，它的oldest_modification值为8916，我们就把8916赋值给checkpoint_lsn（也就是说在redo日志对应的lsn值小于8916时就可以被覆盖掉）。 步骤二：将checkpoint_lsn和对应的redo日志文件组偏移量以及此次checkpint的编号写到日志文件的管理信息（就是checkpoint1或者checkpoint2）中。InnoDB维护了一个目前系统做了多少次checkpoint的变量checkpoint_no，每做一次checkpoint，该变量的值就加1。我们前边说过计算一个lsn值对应的redo日志文件组偏移量是很容易的，所以可以计算得到该checkpoint_lsn在redo日志文件组中对应的偏移量checkpoint_offset，然后把这三个值都写到redo日志文件组的管理信息中。我们说过，每一个redo日志文件都有2048个字节的管理信息，但是上述关于checkpoint的信息只会被写到日志文件组的第一个日志文件的管理信息中。不过我们是存储到checkpoint1中还是checkpoint2中呢？InnoDB规定，当checkpoint_no的值是偶数时，就写到checkpoint1中，是奇数时，就写到checkpoint2中。 记录完checkpoint的信息之后，redo日志文件组中各个lsn值的关系就像这样： 1.批量从flush链表中刷出脏页一般情况下都是后台的线程在对LRU链表和flush链表进行刷脏操作，这主要因为刷脏操作比较慢，不想影响用户线程处理请求。但是如果当前系统修改页面的操作十分频繁，这样就导致写日志操作十分频繁，系统lsn值增长过快。如果后台的刷脏操作不能将脏页刷出，那么系统无法及时做checkpoint，可能就需要用户线程同步的从flush链表中把那些最早修改的脏页（oldest_modification最小的脏页）刷新到磁盘，这样这些脏页对应的redo日志就没用了，然后就可以去做checkpoint了。 2.查看系统中的各种LSN值我们可以使用SHOW ENGINE INNODB STATUS命令查看当前InnoDB存储引擎中的各种LSN值的情况，比如： 12345678910111213mysql&gt; SHOW ENGINE INNODB STATUS\\G(...省略前边的许多状态)LOG---Log sequence number 124476971Log flushed up to 124099769Pages flushed up to 124052503Last checkpoint at 1240524940 pending log flushes, 0 pending chkp writes24 log i/o&#x27;s done, 2.00 log i/o&#x27;s/second----------------------(...省略后边的许多状态) 其中： Log sequence number：代表系统中的lsn值，也就是当前系统已经写入的redo日志量，包括写入log buffer中的日志。 Log flushed up to：代表flushed_to_disk_lsn的值，也就是当前系统已经写入磁盘的redo日志量。 Pages flushed up to：代表flush链表中被最早修改的那个页面对应的oldest_modification属性值。 Last checkpoint at：当前系统的checkpoint_lsn值。 3.innodb_flush_log_at_trx_commit的用法为了保证事务的持久性，用户线程在事务提交时需要将该事务执行过程中产生的所有redo日志都刷新到磁盘上。这一条要求太狠了，会很明显的降低数据库性能。如果对事务的持久性要求不是那么强烈的话，可以选择修改一个称为innodb_flush_log_at_trx_commit的系统变量的值，该变量有3个可选的值： 0：当该系统变量值为0时，表示在事务提交时不立即向磁盘中同步redo日志，这个任务是交给后台线程做的。这样很明显会加快请求处理速度，但是如果事务提交后服务器挂了，后台线程没有及时将redo日志刷新到磁盘，那么该事务对页面的修改会丢失。 1：当该系统变量值为1时，表示在事务提交时需要将redo日志同步到磁盘，可以保证事务的持久性。1也是innodb_flush_log_at_trx_commit的默认值。 2：当该系统变量值为2时，表示在事务提交时需要将redo日志写到操作系统的缓冲区中，但并不需要保证将日志真正的刷新到磁盘。这种情况下如果数据库挂了，操作系统没挂的话，事务的持久性还是可以保证的，但是操作系统也挂了的话，那就不能保证持久性了。 八，崩溃恢复在服务器不挂的情况下，redo日志不仅没用，反而让性能变得更差。但是万一数据库挂了，我们就可以在重启时根据redo日志中的记录就可以将页面恢复到系统崩溃前的状态。我们接下来大致看一下恢复过程。 1.确定恢复的起点checkpoint_lsn之前的redo日志都可以被覆盖，也就是说这些redo日志对应的脏页都已经被刷新到磁盘中了，既然它们已经被刷盘，我们就没必要恢复它们了。对于checkpoint_lsn之后的redo日志，它们对应的脏页可能没被刷盘，也可能被刷盘了，我们不能确定，所以需要从checkpoint_lsn开始读取redo日志来恢复页面。 当然，redo日志文件组的第一个文件的管理信息中有两个block都存储了checkpoint_lsn的信息，我们当然是要选取最近发生的那次checkpoint的信息。衡量checkpoint发生时间早晚的信息就是所谓的checkpoint_no，只要把checkpoint1和checkpoint2这两个block中的checkpoint_no值读出来比一下大小，哪个的checkpoint_no值更大，说明哪个block存储的就是最近的一次checkpoint信息。这样我们就能拿到最近发生的checkpoint对应的checkpoint_lsn值以及它在redo日志文件组中的偏移量checkpoint_offset。 2.确定恢复的终点redo日志恢复的起点确定了，那终点是哪个呢？这个还得从block的结构说起。在写redo日志的时候都是顺序写的，写满了一个block之后会再往下一个block中写。 普通block的log block header部分有一个称之为LOG_BLOCK_HDR_DATA_LEN的属性，该属性值记录了当前block里使用了多少字节的空间。对于被填满的block来说，该值永远为512。如果该属性的值不为512，那么就是它了，它就是此次崩溃恢复中需要扫描的最后一个block。 3.怎么恢复确定了需要扫描哪些redo日志进行崩溃恢复之后，接下来就是怎么进行恢复了。假设现在的redo日志文件中有5条redo日志，如图： 由于redo 0在checkpoint_lsn后前边，恢复时可以不管它。现在可以按照redo日志的顺序依次扫描checkpoint_lsn之后的各条redo日志，按照日志中记载的内容将对应的页面恢复出来。这样没什么问题，不过InnoDB还是想了一些办法加快这个恢复的过程： 使用哈希表根据redo日志的space ID和page number属性计算出散列值，把space ID和page number相同的redo日志放到哈希表的同一个槽里，如果有多个space ID和page number都相同的redo日志，那么它们之间使用链表连接起来，按照生成的先后顺序链接起来的，如图所示：之后就可以遍历哈希表，因为对同一个页面进行修改的redo日志都放在了一个槽里，所以可以一次性将一个页面修复好（避免了很多读取页面的随机IO），这样可以加快恢复速度。另外需要注意一点的是，同一个页面的redo日志是按照生成时间顺序进行排序的，所以恢复的时候也是按照这个顺序进行恢复，如果不按照生成时间顺序进行排序的话，那么可能出现错误。比如原先的修改操作是先插入一条记录，再删除该条记录，如果恢复时不按照这个顺序来，就可能变成先删除一条记录，再插入一条记录，这显然是错误的。 跳过已经刷新到磁盘的页面checkpoint_lsn之前的redo日志对应的脏页确定都已经刷到磁盘了，但是checkpoint_lsn之后的redo日志我们不能确定是否已经刷到磁盘，主要是因为在最近做的一次checkpoint后，可能后台线程又不断的从LRU链表和flush链表中将一些脏页刷出Buffer Pool。这些在checkpoint_lsn之后的redo日志，如果它们对应的脏页在崩溃发生时已经刷新到磁盘，那在恢复时也就没有必要根据redo日志的内容修改该页面了。那在恢复时怎么知道某个redo日志对应的脏页是否在崩溃发生时已经刷新到磁盘了呢？这还得从页面的结构说起，每个页面都有一个称之为File Header的部分，在File Header里有一个称之为FIL_PAGE_LSN的属性，该属性记载了最近一次修改页面时对应的lsn值（其实就是页面控制块中的newest_modification值）。如果在做了某次checkpoint之后有脏页被刷新到磁盘中，那么该页对应的FIL_PAGE_LSN代表的lsn值肯定大于checkpoint_lsn的值，凡是符合这种情况的页面就不需要重复执行lsn值小于FIL_PAGE_LSN的redo日志了，所以更进一步提升了崩溃恢复的速度。 九，LOG_BLOCK_HDR_NO是如何计算的对于实际存储redo日志的普通的log block来说，在log block header处有一个称之为LOG_BLOCK_HDR_NO的属性，我们说这个属性代表一个唯一的标号。这个属性是初次使用该block时分配的，跟当时的系统lsn值有关。使用下边的公式计算该block的LOG_BLOCK_HDR_NO值： 1((lsn / 512) &amp; 0x3FFFFFFFUL) + 1 从图中可以看出，0x3FFFFFFFUL对应的二进制数的前2位为0，后30位的值都为1。一个二进制位与0做与运算（&amp;）的结果肯定是0，一个二进制位与1做与运算（&amp;）的结果就是原值。让一个数和0x3FFFFFFFUL做与运算的意思就是要将该值的前2个比特位的值置为0，这样该值就肯定小于或等于0x3FFFFFFFUL了。这也就说明了，不论lsn多大，((lsn / 512) &amp; 0x3FFFFFFFUL)的值肯定在0``~~0x3FFFFFFFUL~~之间，再加1的话肯定在~~1~~``0x40000000UL之间。而0x40000000UL这个值就代表着1GB。也就是说系统最多能产生不重复的LOG_BLOCK_HDR_NO值只有1GB个。InnoDB规定redo日志文件组中包含的所有文件大小总和不得超过512GB，一个block大小是512字节，也就是说redo日志文件组中包含的block块最多为1GB个，所以有1GB个不重复的编号值也就够用了。 另外，LOG_BLOCK_HDR_NO值的第一个比特位比较特殊，称之为flush bit，如果该值为1，代表着本block是在某次将log buffer中的block刷新到磁盘的操作中的第一个被刷入的block。 十，double write​ 1.脏页刷盘风险​ 关于IO的最小单位： 数据库IO的最小单位是16K（MySQL默认，oracle是8K） 文件系统IO的最小单位是4K（也有1K的） 磁盘IO的最小单位是512字节 因此，存在IO写入导致page损坏的风险：​ ​​ 2.doublewrite：两次写提高innodb的可靠性，用来解决部分写失败(partial page write页断裂)。​ 2.1 Double write解决了什么问题​ 一个数据页的大小是16K，假设在把内存中的脏页写到数据库的时候，写了2K突然掉电，也就是说前2K数据是新的，后14K是旧的，那么磁盘数据库这个数据页就是不完整的，是一个坏掉的数据页。redo只能加上旧、校检完整的数据页恢复一个脏块，不能修复坏掉的数据页，所以这个数据就丢失了，可能会造成数据不一致，所以需要double write。​ 2.2使用情景​ 当数据库正在从内存想磁盘写一个数据页是，数据库宕机，从而导致这个页只写了部分数据，这就是部分写失效，它会导致数据丢失。这时是无法通过重做日志恢复的，因为重做日志记录的是对页的物理修改，如果页本身已经损坏，重做日志也无能为力。​ 2.3 double write工作流程 doublewrite由两部分组成，一部分为内存中的doublewrite buffer，其大小为2MB，另一部分是磁盘上共享表空间(ibdata x)中连续的128个页，即2个区(extent)，大小也是2M。 当一系列机制触发数据缓冲池中的脏页刷新时，并不直接写入磁盘数据文件中，而是先拷贝至内存中的doublewrite buffer中； 接着从两次写缓冲区分两次写入磁盘共享表空间中(连续存储，顺序写，性能很高)，每次写1MB； 待第二步完成后，再将doublewrite buffer中的脏页数据写入实际的各个表空间文件(离散写)；(脏页数据固化后，即进行标记对应doublewrite数据可覆盖) 2.4 doublewrite的崩溃恢复​ 如果操作系统在将页写入磁盘的过程中发生崩溃，在恢复过程中，innodb存储引擎可以从共享表空间的doublewrite中找到该页的一个最近的副本，将其复制到表空间文件，再应用redo log，就完成了恢复过程。因为有副本所以也不担心表空间中数据页是否损坏。 Q：为什么_log write_不需要_doublewrite_的支持？A：因为_redolog_写入的单位就是512字节，也就是磁盘IO的最小单位，所以无所谓数据损坏。 ​ 3.doublewrite的副作用3.1 double write带来的写负载 double write是一个buffer, 但其实它是开在物理文件上的一个buffer, 其实也就是file, 所以它会导致系统有更多的fsync操作, 而硬盘的fsync性能是很慢的, 所以它会降低mysql的整体性能。 但是，doublewrite buffer写入磁盘共享表空间这个过程是连续存储，是顺序写，性能非常高，(约占写的10%)，牺牲一点写性能来保证数据页的完整还是很有必要的。3.2 监控double write工作负载 12345678mysql&gt; show global status like &#x27;%dblwr%&#x27;;+----------------------------+-------+| Variable_name | Value |+----------------------------+-------+| Innodb_dblwr_pages_written | 7 || Innodb_dblwr_writes | 3 |+----------------------------+-------+2 rows in set (0.00 sec) 关注点：Innodb_dblwr_pages_written / Innodb_dblwr_writes​ 开启doublewrite后，每次脏页刷新必须要先写doublewrite，而doublewrite存在于磁盘上的是两个连续的区，每个区由连续的页组成，一般情况下一个区最多有64个页，所以一次IO写入应该可以最多写64个页。​ 而根据以上系统Innodb_dblwr_pages_written与Innodb_dblwr_writes的比例来看，大概在3左右，远远还没到64(如果约等于64，那么说明系统的写压力非常大，有大量的脏页要往磁盘上写)，所以从这个角度也可以看出，系统写入压力并不高。​ 3.3 关闭double write适合的场景 海量DML 不惧怕数据损坏和丢失 系统写负载成为主要负载1234567mysql&gt; show variables like &#x27;%double%&#x27;;+--------------------+-------+| Variable_name | Value |+--------------------+-------+| innodb_doublewrite | ON |+--------------------+-------+1 row in set (0.04 sec) 作为InnoDB的一个关键特性，doublewrite功能默认是开启的，但是在上述特殊的一些场景也可以视情况关闭，来提高数据库写性能。静态参数，配置文件修改，重启数据库。3.4 为什么没有把double write里面的数据写到data page里面呢？ double write里面的数据是连续的，如果直接写到data page里面，而data page的页又是离散的，写入会很慢。 double write里面的数据没有办法被及时的覆盖掉，导致double write的压力很大；短时间内可能会出现double write溢出的情况。十一，总结 redo日志记录了事务执行过程中都修改了哪些内容。 事务提交时只将执行过程中产生的redo日志刷新到磁盘，而不是将所有修改过的页面都刷新到磁盘。这样做有两个好处： redo日志占用的空间非常小 redo日志是顺序写入磁盘的 一条redo日志由下面几部分组成。 type：这条redo日志的类型 space ID:表空间ID page number :页号 data：这条redo日志的具体内容 redo日志的类型有简单和复杂之分。简单类型的redo日志是纯粹的物理日志，复杂类型的redo日志兼有物理日志和逻辑日志的特性。 一个MTR可以包含一组redo日志。在进行崩溃恢复时，这一组redo日志作为一个不可分割的整体来处理。 redo日志存放在大小为512字节的block中。每一个block被分为3部分： log block header log block body log block trailer redo日志缓冲区是一片连续的内存空间，由若干个block组成；可以通过启动选项innodb_log_buffer_size 来调整他的大小。 redo日志文件组由若干个日志文件组成，这些redo日志文件是被循环使用的。redo日志文件组中每个文件的大小都一样，格式也一样，都是由两部分组成的： 前2048字节用来存储一些管理信息 从第2048字节往后的字节用来存储log buffer中的block镜像 lsn指已经写入的redo日志量，flushed_to_disk_lsn指刷新到磁盘中的redo日志量，flush链表中的脏页按照修改发生的时间顺序进行排序，也就是按照oldest_modification代表的lsn值进行排序。被多次更新的页面不会重复插入到flush链表，但是会更新newest_modification属性的值。checkpoint_lsn表示当前系统中可以被覆盖的redo日志总量是多少。 redo日志占用的磁盘空间在他对应的脏页已经被刷新到磁盘后即可被覆盖。执行一次checkpoint的意思就是增加checkpoint_lsn的值，然后把相关信息放到日志文件的管理信息中。 innodb_flush_log_at_trx_commit系统变量控制着在事务提交时是否将该事务运行过程中产生的redo刷新到磁盘。 在崩溃恢复过程中，从redo日志文件组第一个文件的管理信息中取出最近发生的那次checkpoint信息，然后从checkpoint_lsn在日志文件组中对应的偏移量开始，一直扫描日志文件中的block，直到某个block的LOG_BLOCK_HDR_DATA_LEN值不等于512为止。再恢复过程中，使用hash表可加快恢复过程，并且会跳过已经刷新到磁盘的页面。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/tags/MySQL/"}]},{"title":"MySQL[十三]事务","slug":"MySQL/MySQL[十三]事务","date":"2022-01-12T16:00:00.000Z","updated":"2022-01-12T00:42:46.192Z","comments":true,"path":"2022/01/13/MySQL/MySQL[十三]事务/","link":"","permalink":"https://yinhuidong.github.io/2022/01/13/MySQL/MySQL[%E5%8D%81%E4%B8%89]%E4%BA%8B%E5%8A%A1/","excerpt":"","text":"一，事务的起源1.原子性（Atomicity）现实世界中转账操作是一个不可分割的操作，也就是说要么压根儿就没转，要么转账成功，不能存在中间的状态，也就是转了一半的这种情况。设计数据库的大叔们把这种要么全做，要么全不做的规则称之为原子性。但是在现实世界中的一个不可分割的操作却可能对应着数据库世界若干条不同的操作，数据库中的一条操作也可能被分解成若干个步骤（比如先修改缓存页，之后再刷新到磁盘等），最要命的是在任何一个可能的时间都可能发生意想不到的错误（可能是数据库本身的错误，或者是操作系统错误，甚至是直接断电之类的）而使操作执行不下去。为了保证在数据库世界中某些操作的原子性，MySQL需要保证如果在执行操作的过程中发生了错误，把已经做了的操作恢复成没执行之前的样子。 2.隔离性（Isolation）现实世界中的两次状态转换应该是互不影响的，所以对于现实世界中状态转换对应的某些数据库操作来说，不仅要保证这些操作以原子性的方式执行完成，而且要保证其它的状态转换不会影响到本次状态转换，这个规则被称之为隔离性。这时MySQL就需要采取一些措施来让访问相同数据的不同状态转换对应的数据库操作的执行顺序有一定规律。 3.一致性（Consistency）我们生活的这个世界存在着形形色色的约束，比如身份证号不能重复，性别只能是男或者女，高考的分数只能在0～750之间，人民币面值最大只能是100（现在是2019年），红绿灯只有3种颜色，房价不能为负的。 只有符合这些约束的数据才是有效的。数据库世界只是现实世界的一个映射，现实世界中存在的约束当然也要在数据库世界中有所体现。如果数据库中的数据全部符合现实世界中的约束（all defined rules），我们说这些数据就是一致的，或者说符合一致性的。 如何保证数据库中数据的一致性（就是符合所有现实世界的约束）呢？这其实靠两方面的努力： 数据库本身能为我们保证一部分一致性需求（就是数据库自身可以保证一部分现实世界的约束永远有效）。我们知道MySQL数据库可以为表建立主键、唯一索引、外键、声明某个列为NOT NULL来拒绝NULL值的插入。比如说当我们对某个列建立唯一索引时，如果插入某条记录时该列的值重复了，那么MySQL就会报错并且拒绝插入。除了这些我们已经非常熟悉的保证一致性的功能，MySQL还支持CHECK语法来自定义约束，比如这样：上述例子中的CHECK语句本意是想规定balance列不能存储小于0的数字，对应的现实世界的意思就是银行账户余额不能小于0。但是很遗憾，MySQL仅仅支持CHECK语法，但实际上并没有一点卵用，也就是说即使我们使用上述带有CHECK子句的建表语句来创建account表，那么在后续插入或更新记录时，MySQL并不会去检查CHECK子句中的约束是否成立。虽然CHECK子句对一致性检查没什么卵用，但是我们还是可以通过定义触发器的方式来自定义一些约束条件以保证数据库中数据的一致性。 1234567CREATE TABLE account ( id INT NOT NULL AUTO_INCREMENT COMMENT &#x27;自增id&#x27;, name VARCHAR(100) COMMENT &#x27;客户名称&#x27;, balance INT COMMENT &#x27;余额&#x27;, PRIMARY KEY (id), CHECK (balance &gt;= 0) ); 其它的一些数据库，比如SQL Server或者Oracle支持的CHECK语法是有实实在在的作用的，每次进行插入或更新记录之前都会检查一下数据是否符合CHECK子句中指定的约束条件是否成立，如果不成立的话就会拒绝插入或更新。 更多的一致性需求需要靠写业务代码的程序员自己保证。为建立现实世界和数据库世界的对应关系，理论上应该把现实世界中的所有约束都反应到数据库世界中，但是在更改数据库数据时进行一致性检查是一个耗费性能的工作，比方说我们为account表建立了一个触发器，每当插入或者更新记录时都会校验一下balance列的值是不是大于0，这就会影响到插入或更新的速度。仅仅是校验一行记录符不符合一致性需求倒也不是什么大问题，有的一致性需求简直变态，比方说银行会建立一张代表账单的表，里边儿记录了每个账户的每笔交易，每一笔交易完成后，都需要保证整个系统的余额等于所有账户的收入减去所有账户的支出。如果在数据库层面实现这个一致性需求的话，每次发生交易时，都需要将所有的收入加起来减去所有的支出，再将所有的账户余额加起来，看看两个值相不相等。如果账单表里有几亿条记录，光是这个校验的过程可能就要跑好几个小时，这样的性能代价是完全承受不起的。现实生活中复杂的一致性需求比比皆是，而由于性能问题把一致性需求交给数据库去解决这是不现实的，所以这个锅就甩给了业务端程序员。比方说我们的account表，我们也可以不建立触发器，只要编写业务的程序员在自己的业务代码里判断一下，当某个操作会将balance列的值更新为小于0的值时，就不执行该操作就好了！ 原子性和隔离性都会对一致性产生影响，比如我们现实世界中转账操作完成后，有一个一致性需求就是参与转账的账户的总的余额是不变的。如果数据库不遵循原子性要求，也就是转了一半就不转了，那最后就是不符合一致性需求的；类似的，如果数据库不遵循隔离性要求，也就是说可能不符合一致性需求了。所以说，数据库某些操作的原子性和隔离性都是保证一致性的一种手段，在操作执行完成后保证符合所有既定的约束则是一种结果。那满足原子性和隔离性的操作一定就满足一致性么？那倒也不一定，那不满足原子性和隔离性的操作就一定不满足一致性么？这也不一定，只要最后的结果符合所有现实世界中的约束，那么就是符合一致性的。 4.持久性（Durability）当现实世界的一个状态转换完成后，这个转换的结果将永久的保留，这个规则被MySQL称为持久性。当把现实世界的状态转换映射到数据库世界时，持久性意味着该转换对应的数据库操作所修改的数据都应该在磁盘上保留下来，不论之后发生了什么事故，本次转换造成的影响都不应该被丢失掉。 二，事务的概念我们把原子性（Atomicity）、隔离性（Isolation）、一致性（Consistency）和持久性（Durability）这四个词对应的英文单词首字母提取出来就是A、I、C、D，稍微变换一下顺序可以组成一个完整的英文单词：ACID。MySQL叔为了方便起见，把需要保证原子性、隔离性、一致性和持久性的一个或多个数据库操作称之为一个事务（英文名是：transaction）。 我们现在知道事务是一个抽象的概念，它其实对应着一个或多个数据库操作，MySQL根据这些操作所执行的不同阶段把事务大致上划分成了这么几个状态： 活动的（active）事务对应的数据库操作正在执行过程中时，我们就说该事务处在活动的状态。 部分提交的（partially committed）当事务中的最后一个操作执行完成，但由于操作都在内存中执行，所造成的影响并没有刷新到磁盘时，我们就说该事务处在部分提交的状态。 失败的（failed）当事务处在活动的或者部分提交的状态时，可能遇到了某些错误（数据库自身的错误、操作系统错误或者直接断电等）而无法继续执行，或者人为的停止当前事务的执行，我们就说该事务处在失败的状态。 中止的（aborted）如果事务执行了半截而变为失败的状态，那么就需要把已经修改的数据调整为未修改之前的数据，换句话说，就是要撤销失败事务对当前数据库造成的影响。书面一点的话，我们把这个撤销的过程称之为回滚。当回滚操作执行完毕时，也就是数据库恢复到了执行事务之前的状态，我们就说该事务处在了中止的状态。 提交的（committed）当一个处在部分提交的状态的事务将修改过的数据都同步到磁盘上之后，我们就可以说该事务处在了提交的状态。 随着事务对应的数据库操作执行到不同阶段，事务的状态也在不断变化，一个基本的状态转换图如下所示： 从图中大家也可以看出了，只有当事务处于提交的或者中止的状态时，一个事务的生命周期才算是结束了。对于已经提交的事务来说，该事务对数据库所做的修改将永久生效，对于处于中止状态的事务，该事务对数据库所做的所有修改都会被回滚到没执行该事务之前的状态。 三，MySQL中事务的语法我们说事务的本质其实只是一系列数据库操作，只不过这些数据库操作符合ACID特性而已，那么MySQL中如何将某些操作放到一个事务里去执行的呢？ 1.开启事务我们可以使用下边两种语句之一来开启一个事务： BEGIN [WORK];BEGIN语句代表开启一个事务，后边的单词WORK可有可无。开启事务后，就可以继续写若干条语句，这些语句都属于刚刚开启的这个事务。 1234mysql&gt; BEGIN;Query OK, 0 rows affected (0.00 sec)mysql&gt; 加入事务的语句... START TRANSACTION;START TRANSACTION语句和BEGIN语句有着相同的功效，都标志着开启一个事务，比如这样：不过比BEGIN语句牛逼一点儿的是，可以在START TRANSACTION语句后边跟随几个修饰符，就是它们几个： 1234mysql&gt; START TRANSACTION;Query OK, 0 rows affected (0.00 sec)mysql&gt; 加入事务的语句... READ ONLY：标识当前事务是一个只读事务，也就是属于该事务的数据库操作只能读取数据，而不能修改数据。 其实只读事务中只是不允许修改那些其他事务也能访问到的表中的数据，对于临时表来说（我们使用CREATE TMEPORARY TABLE创建的表），由于它们只能在当前会话中可见，所以只读事务其实也是可以对临时表进行增、删、改操作的。 READ WRITE：标识当前事务是一个读写事务，也就是属于该事务的数据库操作既可以读取数据，也可以修改数据。 WITH CONSISTENT SNAPSHOT：启动一致性读。 比如我们想开启一个只读事务的话，直接把READ ONLY这个修饰符加在START TRANSACTION语句后边就好，比如这样： 1START TRANSACTION READ ONLY; 如果我们想在START TRANSACTION后边跟随多个修饰符的话，可以使用逗号将修饰符分开，比如开启一个只读事务和一致性读，就可以这样写： 1START TRANSACTION READ ONLY, WITH CONSISTENT SNAPSHOT; 或者开启一个读写事务和一致性读，就可以这样写： 1START TRANSACTION READ WRITE, WITH CONSISTENT SNAPSHOT 不过这里需要注意的一点是，READ ONLY和READ WRITE是用来设置所谓的事务访问模式的，就是以只读还是读写的方式来访问数据库中的数据，一个事务的访问模式不能同时既设置为只读的也设置为读写的，所以我们不能同时把READ ONLY和READ WRITE放到START TRANSACTION语句后边。另外，如果我们不显式指定事务的访问模式，那么该事务的访问模式就是读写模式。 2.提交事务开启事务之后就可以继续写需要放到该事务中的语句了，当最后一条语句写完了之后，我们就可以提交该事务了，提交的语句也很简单： 1COMMIT [WORK] COMMIT语句就代表提交一个事务，后边的WORK可有可无。转账举例： 12345678910111213mysql&gt; BEGIN;Query OK, 0 rows affected (0.00 sec)mysql&gt; UPDATE account SET balance = balance - 10 WHERE id = 1;Query OK, 1 row affected (0.02 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; UPDATE account SET balance = balance + 10 WHERE id = 2;Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; COMMIT;Query OK, 0 rows affected (0.00 sec) 3.手动中止事务如果我们写了几条语句之后发现上边的某条语句写错了，我们可以手动的使用下边这个语句来将数据库恢复到事务执行之前的样子： 1ROLLBACK [WORK] ROLLBACK语句就代表中止并回滚一个事务，后边的WORK可有可无类似的。转账举例： 12345678910111213mysql&gt; BEGIN;Query OK, 0 rows affected (0.00 sec)mysql&gt; UPDATE account SET balance = balance - 10 WHERE id = 1;Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; UPDATE account SET balance = balance + 1 WHERE id = 2;Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; ROLLBACK;Query OK, 0 rows affected (0.00 sec) 这里需要强调一下，ROLLBACK语句是我们程序员手动的去回滚事务时才去使用的，如果事务在执行过程中遇到了某些错误而无法继续执行的话，事务自身会自动的回滚。 我们这里所说的开启、提交、中止事务的语法只是针对使用黑框框时通过mysql客户端程序与服务器进行交互时控制事务的语法，如果大家使用的是别的客户端程序，比如JDBC之类的，那需要参考相应的文档来看看如何控制事务。 4.支持事务的存储引擎MySQL中并不是所有存储引擎都支持事务的功能，目前只有InnoDB和NDB存储引擎支持（NDB存储引擎不是我们的重点），如果某个事务中包含了修改使用不支持事务的存储引擎的表，那么对该使用不支持事务的存储引擎的表所做的修改将无法进行回滚。比方说我们有两个表，tbl1使用支持事务的存储引擎InnoDB，tbl2使用不支持事务的存储引擎MyISAM，它们的建表语句如下所示： 1234567CREATE TABLE tbl1 ( i int) engine=InnoDB;CREATE TABLE tbl2 ( i int) ENGINE=MyISAM; 我们看看先开启一个事务，写一条插入语句后再回滚该事务，tbl1和tbl2的表现有什么不同： 1234567891011121314mysql&gt; SELECT * FROM tbl1;Empty set (0.00 sec)mysql&gt; BEGIN;Query OK, 0 rows affected (0.00 sec)mysql&gt; INSERT INTO tbl1 VALUES(1);Query OK, 1 row affected (0.00 sec)mysql&gt; ROLLBACK;Query OK, 0 rows affected (0.00 sec)mysql&gt; SELECT * FROM tbl1;Empty set (0.00 sec) 可以看到，对于使用支持事务的存储引擎的tbl1表来说，我们在插入一条记录再回滚后，tbl1就恢复到没有插入记录时的状态了。再看看tbl2表的表现： 12345678910111213141516171819mysql&gt; SELECT * FROM tbl2;Empty set (0.00 sec)mysql&gt; BEGIN;Query OK, 0 rows affected (0.00 sec)mysql&gt; INSERT INTO tbl2 VALUES(1);Query OK, 1 row affected (0.00 sec)mysql&gt; ROLLBACK;Query OK, 0 rows affected, 1 warning (0.01 sec)mysql&gt; SELECT * FROM tbl2;+------+| i |+------+| 1 |+------+1 row in set (0.00 sec) 可以看到，虽然我们使用了ROLLBACK语句来回滚事务，但是插入的那条记录还是留在了tbl2表中。 5.自动提交MySQL中有一个系统变量autocommit： 1234567mysql&gt; SHOW VARIABLES LIKE &#x27;autocommit&#x27;;+---------------+-------+| Variable_name | Value |+---------------+-------+| autocommit | ON |+---------------+-------+1 row in set (0.01 sec) 可以看到它的默认值为ON，也就是说默认情况下，如果我们不显式的使用START TRANSACTION或者BEGIN语句开启一个事务，那么每一条语句都算是一个独立的事务，这种特性称之为事务的自动提交。假如我们在转账时不以START TRANSACTION或者BEGIN语句显式的开启一个事务，那么下边这两条语句就相当于放到两个独立的事务中去执行： 12UPDATE account SET balance = balance - 10 WHERE id = 1;UPDATE account SET balance = balance + 10 WHERE id = 2; 当然，如果我们想关闭这种自动提交的功能，可以使用下边两种方法之一： 显式的的使用START TRANSACTION或者BEGIN语句开启一个事务。这样在本次事务提交或者回滚前会暂时关闭掉自动提交的功能。 把系统变量autocommit的值设置为OFF，就像这样：这样的话，我们写入的多条语句就算是属于同一个事务了，直到我们显式的写出COMMIT语句来把这个事务提交掉，或者显式的写出ROLLBACK语句来把这个事务回滚掉。 1SET autocommit = OFF; 6.隐式提交当我们使用START TRANSACTION或者BEGIN语句开启了一个事务，或者把系统变量autocommit的值设置为OFF时，事务就不会进行自动提交，但是如果我们输入了某些语句之后就会悄悄的提交掉，就像我们输入了COMMIT语句了一样，这种因为某些特殊的语句而导致事务提交的情况称为隐式提交，这些会导致事务隐式提交的语句包括： 定义或修改数据库对象的数据定义语言（Data definition language，缩写为：DDL）。所谓的数据库对象，指的就是数据库、表、视图、存储过程等等这些东西。当我们使用CREATE、ALTER、DROP等语句去修改这些所谓的数据库对象时，就会隐式的提交前边语句所属于的事务，就像这样： 1234567BEGIN;SELECT ... # 事务中的一条语句UPDATE ... # 事务中的一条语句... # 事务中的其它语句CREATE TABLE ... # 此语句会隐式的提交前边语句所属于的事务 隐式使用或修改mysql数据库中的表当我们使用ALTER USER、CREATE USER、DROP USER、GRANT、RENAME USER、REVOKE、SET PASSWORD等语句时也会隐式的提交前边语句所属于的事务。 事务控制或关于锁定的语句当我们在一个事务还没提交或者回滚时就又使用START TRANSACTION或者BEGIN语句开启了另一个事务时，会隐式的提交上一个事务，比如这样：或者当前的autocommit系统变量的值为OFF，我们手动把它调为ON时，也会隐式的提交前边语句所属的事务。或者使用LOCK TABLES、UNLOCK TABLES等关于锁定的语句也会隐式的提交前边语句所属的事务。 1234567BEGIN;SELECT ... # 事务中的一条语句UPDATE ... # 事务中的一条语句... # 事务中的其它语句BEGIN; # 此语句会隐式的提交前边语句所属于的事务 加载数据的语句比如我们使用LOAD DATA语句来批量往数据库中导入数据时，也会隐式的提交前边语句所属的事务。 关于MySQL复制的一些语句使用START SLAVE、STOP SLAVE、RESET SLAVE、CHANGE MASTER TO等语句时也会隐式的提交前边语句所属的事务。 其它的一些语句使用ANALYZE TABLE、CACHE INDEX、CHECK TABLE、FLUSH、 LOAD INDEX INTO CACHE、OPTIMIZE TABLE、REPAIR TABLE、RESET等语句也会隐式的提交前边语句所属的事务。 7.保存点如果你开启了一个事务，并且已经敲了很多语句，忽然发现上一条语句有点问题，你只好使用ROLLBACK语句来让数据库状态恢复到事务执行之前的样子，然后一切从头再来，总有一种一夜回到解放前的感觉。所以MySQL提出了一个保存点（英文：savepoint）的概念，就是在事务对应的数据库语句中打几个点，我们在调用ROLLBACK语句时可以指定会滚到哪个点，而不是回到最初的原点。定义保存点的语法如下： 1SAVEPOINT 保存点名称; 当我们想回滚到某个保存点时，可以使用下边这个语句（下边语句中的单词WORK和SAVEPOINT是可有可无的）： 1ROLLBACK [WORK] TO [SAVEPOINT] 保存点名称; 不过如果ROLLBACK语句后边不跟随保存点名称的话，会直接回滚到事务执行之前的状态。 如果我们想删除某个保存点，可以使用这个语句： 1RELEASE SAVEPOINT 保存点名称; 下边还是以转账的例子展示一下保存点的用法，在执行完扣除第一个账户的钱10元的语句之后打一个保存点： 12345678910111213141516171819202122232425262728293031323334353637383940414243mysql&gt; SELECT * FROM account;+----+--------+---------+| id | name | balance |+----+--------+---------+| 1 | 狗哥 | 11 || 2 | 猫爷 | 2 |+----+--------+---------+2 rows in set (0.00 sec)mysql&gt; BEGIN;Query OK, 0 rows affected (0.00 sec)mysql&gt; UPDATE account SET balance = balance - 10 WHERE id = 1;Query OK, 1 row affected (0.01 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; SAVEPOINT s1; # 一个保存点Query OK, 0 rows affected (0.00 sec)mysql&gt; SELECT * FROM account;+----+--------+---------+| id | name | balance |+----+--------+---------+| 1 | 狗哥 | 1 || 2 | 猫爷 | 2 |+----+--------+---------+2 rows in set (0.00 sec)mysql&gt; UPDATE account SET balance = balance + 1 WHERE id = 2; # 更新错了Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; ROLLBACK TO s1; # 回滚到保存点s1处Query OK, 0 rows affected (0.00 sec)mysql&gt; SELECT * FROM account;+----+--------+---------+| id | name | balance |+----+--------+---------+| 1 | 狗哥 | 1 || 2 | 猫爷 | 2 |+----+--------+---------+2 rows in set (0.00 sec)","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/tags/MySQL/"}]},{"title":"DDD之领域层设计规范","slug":"领域驱动设计/DDD之领域层设计规范","date":"2022-01-12T00:19:55.254Z","updated":"2022-01-12T00:19:55.254Z","comments":true,"path":"2022/01/12/领域驱动设计/DDD之领域层设计规范/","link":"","permalink":"https://yinhuidong.github.io/2022/01/12/%E9%A2%86%E5%9F%9F%E9%A9%B1%E5%8A%A8%E8%AE%BE%E8%AE%A1/DDD%E4%B9%8B%E9%A2%86%E5%9F%9F%E5%B1%82%E8%AE%BE%E8%AE%A1%E8%A7%84%E8%8C%83/","excerpt":"","text":"在一个DDD架构设计中，领域层的设计合理性会直接影响整个架构的代码结构以及应用层、基础设施层的设计。但是领域层设计又是有挑战的任务，特别是在一个业务逻辑相对复杂应用中，每一个业务规则是应该放在Entity、ValueObject 还是 DomainService是值得用心思考的，既要避免未来的扩展性差，又要确保不会过度设计导致复杂性。 一，需求背景用代码实现一个龙与魔法的游戏世界的（极简）规则？ 基础配置如下 玩家（Player）可以是战士（Fighter）、法师（Mage）、龙骑（Dragoon） 怪物（Monster）可以是兽人（Orc）、精灵（Elf）、龙（Dragon），怪物有血量 武器（Weapon）可以是剑（Sword）、法杖（Staff），武器有攻击力 玩家可以装备一个武器，武器攻击可以是物理类型（0），火（1），冰（2）等，武器类型决定伤害类型 攻击规则如下 兽人对物理攻击伤害减半 精灵对魔法攻击伤害减半 龙对物理和魔法攻击免疫，除非玩家是龙骑，则伤害加倍 二，OOP实现对于熟悉Object-Oriented Programming的同学，一个比较简单的实现是通过类的继承关系（此处省略部分非核心代码）： 12345678910111213141516171819202122public abstract class Player &#123; Weapon weapon&#125;public class Fighter extends Player &#123;&#125;public class Mage extends Player &#123;&#125;public class Dragoon extends Player &#123;&#125; public abstract class Monster &#123; Long health;&#125;public Orc extends Monster &#123;&#125;public Elf extends Monster &#123;&#125;public Dragoon extends Monster &#123;&#125; public abstract class Weapon &#123; int damage; int damageType; // 0 - physical, 1 - fire, 2 - ice etc.&#125;public Sword extends Weapon &#123;&#125;public Staff extends Weapon &#123;&#125; 而实现规则代码如下： 1234567891011121314151617181920212223242526272829303132333435public class Player &#123; public void attack(Monster monster) &#123; monster.receiveDamageBy(weapon, this); &#125;&#125; public class Monster &#123; public void receiveDamageBy(Weapon weapon, Player player) &#123; this.health -= weapon.getDamage(); // 基础规则 &#125;&#125; public class Orc extends Monster &#123; @Override public void receiveDamageBy(Weapon weapon, Player player) &#123; if (weapon.getDamageType() == 0) &#123; this.setHealth(this.getHealth() - weapon.getDamage() / 2); // Orc的物理防御规则 &#125; else &#123; super.receiveDamageBy(weapon, player); &#125; &#125;&#125; public class Dragon extends Monster &#123; @Override public void receiveDamageBy(Weapon weapon, Player player) &#123; if (player instanceof Dragoon) &#123; this.setHealth(this.getHealth() - weapon.getDamage() * 2); // 龙骑伤害规则 &#125; // else no damage, 龙免疫力规则 &#125;&#125; 然后跑几个单元测试： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778public class BattleTest &#123; @Test @DisplayName(&quot;Dragon is immune to attacks&quot;) public void testDragonImmunity() &#123; // Given Fighter fighter = new Fighter(&quot;Hero&quot;); Sword sword = new Sword(&quot;Excalibur&quot;, 10); fighter.setWeapon(sword); Dragon dragon = new Dragon(&quot;Dragon&quot;, 100L); // When fighter.attack(dragon); // Then assertThat(dragon.getHealth()).isEqualTo(100); &#125; @Test @DisplayName(&quot;Dragoon attack dragon doubles damage&quot;) public void testDragoonSpecial() &#123; // Given Dragoon dragoon = new Dragoon(&quot;Dragoon&quot;); Sword sword = new Sword(&quot;Excalibur&quot;, 10); dragoon.setWeapon(sword); Dragon dragon = new Dragon(&quot;Dragon&quot;, 100L); // When dragoon.attack(dragon); // Then assertThat(dragon.getHealth()).isEqualTo(100 - 10 * 2); &#125; @Test @DisplayName(&quot;Orc should receive half damage from physical weapons&quot;) public void testFighterOrc() &#123; // Given Fighter fighter = new Fighter(&quot;Hero&quot;); Sword sword = new Sword(&quot;Excalibur&quot;, 10); fighter.setWeapon(sword); Orc orc = new Orc(&quot;Orc&quot;, 100L); // When fighter.attack(orc); // Then assertThat(orc.getHealth()).isEqualTo(100 - 10 / 2); &#125; @Test @DisplayName(&quot;Orc receive full damage from magic attacks&quot;) public void testMageOrc() &#123; // Given Mage mage = new Mage(&quot;Mage&quot;); Staff staff = new Staff(&quot;Fire Staff&quot;, 10); mage.setWeapon(staff); Orc orc = new Orc(&quot;Orc&quot;, 100L); // When mage.attack(orc); // Then assertThat(orc.getHealth()).isEqualTo(100 - 10); &#125;&#125; 三，分析OOP代码设计的缺陷1.编程语言的强类型无法承载业务规则以上的OOP代码可以跑得通，直到我们加一个限制条件： 战士只能装备剑 法师只能装备法杖 这个规则在Java语言里无法通过强类型来实现，虽然Java有Variable Hiding（或者C#的new class variable），但实际上只是在子类上加了一个新变量，所以会导致以下的问题： 123456789101112131415161718192021@Datapublic class Fighter extends Player &#123; private Sword weapon;&#125; @Testpublic void testEquip() &#123; Fighter fighter = new Fighter(&quot;Hero&quot;); Sword sword = new Sword(&quot;Sword&quot;, 10); fighter.setWeapon(sword); Staff staff = new Staff(&quot;Staff&quot;, 10); fighter.setWeapon(staff); assertThat(fighter.getWeapon()).isInstanceOf(Staff.class); // 错误了&#125; 在最后，虽然代码感觉是setWeapon(Staff)，但实际上只修改了父类的变量，并没有修改子类的变量，所以实际不生效，也不抛异常，但结果是错的。 当然，可以在父类限制setter为protected，但这样就限制了父类的API，极大的降低了灵活性，同时也违背了Liskov substitution principle，即一个父类必须要cast成子类才能使用： 1234567891011121314151617181920@Datapublic abstract class Player &#123; @Setter(AccessLevel.PROTECTED) private Weapon weapon;&#125; @Testpublic void testCastEquip() &#123; Fighter fighter = new Fighter(&quot;Hero&quot;); Sword sword = new Sword(&quot;Sword&quot;, 10); fighter.setWeapon(sword); Player player = fighter; Staff staff = new Staff(&quot;Staff&quot;, 10); player.setWeapon(staff); // 编译不过，但从API层面上应该开放可用&#125; 最后，如果规则增加一条： 战士和法师都能装备匕首（dagger） 好家伙，之前写的强类型代码都废了，需要重构。 2.对象继承导致代码强依赖父类逻辑，违反开闭原则开闭原则（OCP）规定“对象应该对于扩展开放，对于修改封闭“，继承虽然可以通过子类扩展新的行为，但因为子类可能直接依赖父类的实现，导致一个变更可能会影响所有对象。在这个例子里，如果增加任意一种类型的玩家、怪物或武器，或增加一种规则，都有可能需要修改从父类到子类的所有方法。 比如，如果要增加一个武器类型：狙击枪，能够无视所有防御一击必杀，需要修改的代码包括： Weapon Player和所有的子类（是否能装备某个武器的判断） Monster和所有的子类（伤害计算逻辑） 123456789101112131415161718public class Monster &#123; public void receiveDamageBy(Weapon weapon, Player player) &#123; this.health -= weapon.getDamage(); // 老的基础规则 if (Weapon instanceof Gun) &#123; // 新的逻辑 this.setHealth(0); &#125; &#125;&#125; public class Dragon extends Monster &#123; public void receiveDamageBy(Weapon weapon, Player player) &#123; if (Weapon instanceof Gun) &#123; // 新的逻辑 super.receiveDamageBy(weapon, player); &#125; // 老的逻辑省略 &#125;&#125; 在一个复杂的软件中为什么会建议“尽量”不要违背OCP？最核心的原因就是一个现有逻辑的变更可能会影响一些原有的代码，导致一些无法预见的影响。这个风险只能通过完整的单元测试覆盖来保障，但在实际开发中很难保障单测的覆盖率。OCP的原则能尽可能的规避这种风险，当新的行为只能通过新的字段/方法来实现时，老代码的行为自然不会变。 继承虽然能Open for extension，但很难做到Closed for modification。所以今天解决OCP的主要方法是通过Composition-over-inheritance，即通过组合来做到扩展性，而不是通过继承。 3.Player.attack(monster) 还是 Monster.receiveDamage(Weapon, Player)？在这个例子里，其实业务规则的逻辑到底应该写在哪里是有异议的：当我们去看一个对象和另一个对象之间的交互时，到底是Player去攻击Monster，还是Monster被Player攻击？目前的代码主要将逻辑写在Monster的类中，主要考虑是Monster会受伤降低Health，但如果是Player拿着一把双刃剑会同时伤害自己呢？是不是发现写在Monster类里也有问题？代码写在哪里的原则是什么？ 4.多对象行为类似，导致代码重复当我们有不同的对象，但又有相同或类似的行为时，OOP会不可避免的导致代码的重复。在这个例子里，如果我们去增加一个“可移动”的行为，需要在Player和Monster类中都增加类似的逻辑： 12345678910111213141516public abstract class Player &#123; int x; int y; void move(int targetX, int targetY) &#123; // logic &#125;&#125; public abstract class Monster &#123; int x; int y; void move(int targetX, int targetY) &#123; // logic &#125;&#125; 一个可能的解法是有个通用的父类： 1234567891011public abstract class Movable &#123; int x; int y; void move(int targetX, int targetY) &#123; // logic &#125;&#125; public abstract class Player extends Movable;public abstract class Monster extends Movable; 但如果再增加一个跳跃能力Jumpable呢？一个跑步能力Runnable呢？如果Player可以Move和Jump，Monster可以Move和Run，怎么处理继承关系？要知道Java（以及绝大部分语言）是不支持多父类继承的，所以只能通过重复代码来实现。 5.总结在这个案例里虽然从直觉来看OOP的逻辑很简单，但如果你的业务比较复杂，未来会有大量的业务规则变更时，简单的OOP代码会在后期变成复杂的一团浆糊，逻辑分散在各地，缺少全局视角，各种规则的叠加会触发bug。有没有感觉似曾相识？对的，电商体系里的优惠、交易等链路经常会碰到类似的坑。而这类问题的核心本质在于： 业务规则的归属到底是对象的“行为”还是独立的”规则对象“？ 业务规则之间的关系如何处理？ 通用“行为”应该如何复用和维护？ 四，基于DDD的解法1.领域对象回到我们原来的问题域上面，我们从领域层拆分一下各种对象： 实体类 在DDD里，实体类包含ID和内部状态，在这个案例里实体类包含Player、Monster和Weapon。Weapon被设计成实体类是因为两把同名的Weapon应该可以同时存在，所以必须要有ID来区分，同时未来也可以预期Weapon会包含一些状态，比如升级、临时的buff、耐久等。 1234567891011121314151617181920212223242526public class Player implements Movable &#123; private PlayerId id; private String name; private PlayerClass playerClass; // enum private WeaponId weaponId; // （Note 1） private Transform position = Transform.ORIGIN; private Vector velocity = Vector.ZERO;&#125; public class Monster implements Movable &#123; private MonsterId id; private MonsterClass monsterClass; // enum private Health health; private Transform position = Transform.ORIGIN; private Vector velocity = Vector.ZERO;&#125; public class Weapon &#123; private WeaponId id; private String name; private WeaponType weaponType; // enum private int damage; private int damageType; // 0 - physical, 1 - fire, 2 - ice&#125; 在这个简单的案例里，我们可以利用enum的PlayerClass、MonsterClass来代替继承关系，后续也可以利用Type Object设计模式来做到数据驱动。 Note 1: 因为 Weapon 是实体类，但是Weapon能独立存在，Player不是聚合根，所以Player只能保存WeaponId，而不能直接指向Weapon。 值对象的组件化 可以通过接口的方式对领域对象做组件化处理： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public interface Movable &#123; // 相当于组件 Transform getPosition(); Vector getVelocity(); // 行为 void moveTo(long x, long y); void startMove(long velX, long velY); void stopMove(); boolean isMoving();&#125; // 具体实现public class Player implements Movable &#123; public void moveTo(long x, long y) &#123; this.position = new Transform(x, y); &#125; public void startMove(long velocityX, long velocityY) &#123; this.velocity = new Vector(velocityX, velocityY); &#125; public void stopMove() &#123; this.velocity = Vector.ZERO; &#125; @Override public boolean isMoving() &#123; return this.velocity.getX() != 0 || this.velocity.getY() != 0; &#125;&#125; @Valuepublic class Transform &#123; public static final Transform ORIGIN = new Transform(0, 0); long x; long y;&#125; @Valuepublic class Vector &#123; public static final Vector ZERO = new Vector(0, 0); long x; long y;&#125; 注意两点： Moveable的接口没有Setter。一个Entity的规则是不能直接变更其属性，必须通过Entity的方法去对内部状态做变更。这样能保证数据的一致性。 抽象Movable的好处是如同ECS一样，一些特别通用的行为（如在大地图里移动）可以通过统一的System代码去处理，避免了重复劳动。 2.装备行为因为我们已经不会用Player的子类来决定什么样的Weapon可以装备，所以这段逻辑应该被拆分到一个单独的类里。这种类在DDD里被叫做领域服务（Domain Service）。 123public interface EquipmentService &#123; boolean canEquip(Player player, Weapon weapon);&#125; 在DDD里，一个Entity不应该直接参考另一个Entity或服务，也就是说以下的代码是错误的： 123456789public class Player &#123; @Autowired EquipmentService equipmentService; // BAD: 不可以直接依赖 public void equip(Weapon weapon) &#123; // ... &#125;&#125; 这里的问题是Entity只能保留自己的状态（或非聚合根的对象）。任何其他的对象，无论是否通过依赖注入的方式弄进来，都会破坏Entity的Invariance，并且还难以单测。 正确的引用方式是通过方法参数引入（Double Dispatch）： 1234567891011public class Player &#123; public void equip(Weapon weapon, EquipmentService equipmentService) &#123; if (equipmentService.canEquip(this, weapon)) &#123; this.weaponId = weapon.getId(); &#125; else &#123; throw new IllegalArgumentException(&quot;Cannot Equip: &quot; + weapon); &#125; &#125;&#125; 在这里，无论是Weapon还是EquipmentService都是通过方法参数传入，确保不会污染Player的自有状态。 Double Dispatch是一个使用Domain Service经常会用到的方法，类似于调用反转。 然后在EquipmentService里实现相关的逻辑判断，这里我们用了另一个常用的Strategy（或者叫Policy）设计模式： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class EquipmentServiceImpl implements EquipmentService &#123; private EquipmentManager equipmentManager; @Override public boolean canEquip(Player player, Weapon weapon) &#123; return equipmentManager.canEquip(player, weapon); &#125;&#125; // 策略优先级管理public class EquipmentManager &#123; private static final List&lt;EquipmentPolicy&gt; POLICIES = new ArrayList&lt;&gt;(); static &#123; POLICIES.add(new FighterEquipmentPolicy()); POLICIES.add(new MageEquipmentPolicy()); POLICIES.add(new DragoonEquipmentPolicy()); POLICIES.add(new DefaultEquipmentPolicy()); &#125; public boolean canEquip(Player player, Weapon weapon) &#123; for (EquipmentPolicy policy : POLICIES) &#123; if (!policy.canApply(player, weapon)) &#123; continue; &#125; return policy.canEquip(player, weapon); &#125; return false; &#125;&#125; // 策略案例public class FighterEquipmentPolicy implements EquipmentPolicy &#123; @Override public boolean canApply(Player player, Weapon weapon) &#123; return player.getPlayerClass() == PlayerClass.Fighter; &#125; /** * Fighter能装备Sword和Dagger */ @Override public boolean canEquip(Player player, Weapon weapon) &#123; return weapon.getWeaponType() == WeaponType.Sword || weapon.getWeaponType() == WeaponType.Dagger; &#125;&#125; // 其他策略省略，见源码 这样设计的最大好处是未来的规则增加只需要添加新的Policy类，而不需要去改变原有的类。 3.攻击行为在上文中曾经有提起过，到底应该是Player.attack(Monster)还是Monster.receiveDamage(Weapon, Player)？在DDD里，因为这个行为可能会影响到Player、Monster和Weapon，所以属于跨实体的业务逻辑。在这种情况下需要通过一个第三方的领域服务（Domain Service）来完成。 1234567891011121314151617181920public interface CombatService &#123; void performAttack(Player player, Monster monster);&#125; public class CombatServiceImpl implements CombatService &#123; private WeaponRepository weaponRepository; private DamageManager damageManager; @Override public void performAttack(Player player, Monster monster) &#123; Weapon weapon = weaponRepository.find(player.getWeaponId()); int damage = damageManager.calculateDamage(player, weapon, monster); if (damage &gt; 0) &#123; monster.takeDamage(damage); // （Note 1）在领域服务里变更Monster &#125; // 省略掉Player和Weapon可能受到的影响 &#125;&#125; 同样的在这个案例里，可以通过Strategy设计模式来解决damage的计算问题： 123456789101112131415161718192021222324252627282930313233343536// 策略优先级管理public class DamageManager &#123; private static final List&lt;DamagePolicy&gt; POLICIES = new ArrayList&lt;&gt;(); static &#123; POLICIES.add(new DragoonPolicy()); POLICIES.add(new DragonImmunityPolicy()); POLICIES.add(new OrcResistancePolicy()); POLICIES.add(new ElfResistancePolicy()); POLICIES.add(new PhysicalDamagePolicy()); POLICIES.add(new DefaultDamagePolicy()); &#125; public int calculateDamage(Player player, Weapon weapon, Monster monster) &#123; for (DamagePolicy policy : POLICIES) &#123; if (!policy.canApply(player, weapon, monster)) &#123; continue; &#125; return policy.calculateDamage(player, weapon, monster); &#125; return 0; &#125;&#125; // 策略案例public class DragoonPolicy implements DamagePolicy &#123; public int calculateDamage(Player player, Weapon weapon, Monster monster) &#123; return weapon.getDamage() * 2; &#125; @Override public boolean canApply(Player player, Weapon weapon, Monster monster) &#123; return player.getPlayerClass() == PlayerClass.Dragoon &amp;&amp; monster.getMonsterClass() == MonsterClass.Dragon; &#125;&#125; 4.单元测试123456789101112131415161718192021222324252627282930313233343536373839@Test@DisplayName(&quot;Dragoon attack dragon doubles damage&quot;)public void testDragoonSpecial() &#123; // Given Player dragoon = playerFactory.createPlayer(PlayerClass.Dragoon, &quot;Dart&quot;); Weapon sword = weaponFactory.createWeaponFromPrototype(swordProto, &quot;Soul Eater&quot;, 60); ((WeaponRepositoryMock)weaponRepository).cache(sword); dragoon.equip(sword, equipmentService); Monster dragon = monsterFactory.createMonster(MonsterClass.Dragon, 100); // When combatService.performAttack(dragoon, dragon); // Then assertThat(dragon.getHealth()).isEqualTo(Health.ZERO); assertThat(dragon.isAlive()).isFalse();&#125; @Test@DisplayName(&quot;Orc should receive half damage from physical weapons&quot;)public void testFighterOrc() &#123; // Given Player fighter = playerFactory.createPlayer(PlayerClass.Fighter, &quot;MyFighter&quot;); Weapon sword = weaponFactory.createWeaponFromPrototype(swordProto, &quot;My Sword&quot;); ((WeaponRepositoryMock)weaponRepository).cache(sword); fighter.equip(sword, equipmentService); Monster orc = monsterFactory.createMonster(MonsterClass.Orc, 100); // When combatService.performAttack(fighter, orc); // Then assertThat(orc.getHealth()).isEqualTo(Health.of(100 - 10 / 2));&#125; 具体的代码比较简单，解释省略 5.移动系统最后还有一种Domain Service，通过组件化，我们其实可以实现ECS一样的System，来降低一些重复性的代码： 1234567891011121314151617181920212223242526272829303132public class MovementSystem &#123; private static final long X_FENCE_MIN = -100; private static final long X_FENCE_MAX = 100; private static final long Y_FENCE_MIN = -100; private static final long Y_FENCE_MAX = 100; private List&lt;Movable&gt; entities = new ArrayList&lt;&gt;(); public void register(Movable movable) &#123; entities.add(movable); &#125; public void update() &#123; for (Movable entity : entities) &#123; if (!entity.isMoving()) &#123; continue; &#125; Transform old = entity.getPosition(); Vector vel = entity.getVelocity(); long newX = Math.max(Math.min(old.getX() + vel.getX(), X_FENCE_MAX), X_FENCE_MIN); long newY = Math.max(Math.min(old.getY() + vel.getY(), Y_FENCE_MAX), Y_FENCE_MIN); entity.moveTo(newX, newY); &#125; &#125;&#125; 单元测试 1234567891011121314151617181920212223242526@Test@DisplayName(&quot;Moving player and monster at the same time&quot;)public void testMovement() &#123; // Given Player fighter = playerFactory.createPlayer(PlayerClass.Fighter, &quot;MyFighter&quot;); fighter.moveTo(2, 5); fighter.startMove(1, 0); Monster orc = monsterFactory.createMonster(MonsterClass.Orc, 100); orc.moveTo(10, 5); orc.startMove(-1, 0); movementSystem.register(fighter); movementSystem.register(orc); // When movementSystem.update(); // Then assertThat(fighter.getPosition().getX()).isEqualTo(2 + 1); assertThat(orc.getPosition().getX()).isEqualTo(10 - 1);&#125; 在这里MovementSystem就是一个相对独立的Domain Service，通过对Movable的组件化，实现了类似代码的集中化、以及一些通用依赖/配置的中心化（如X、Y边界等）。 五，DDD领域层的一些设计规范 基于继承关系的OOP代码：OOP的代码最好写，也最容易理解，所有的规则代码都写在对象里，但是当领域规则变得越来越复杂时，其结构会限制它的发展。新的规则有可能会导致代码的整体重构。 基于领域对象 + 领域服务的DDD架构：DDD的规则其实最复杂，同时要考虑到实体类的内聚和保证不变性（Invariants），也要考虑跨对象规则代码的归属，甚至要考虑到具体领域服务的调用方式，理解成本比较高。 1.实体类大多数DDD架构的核心都是实体类，实体类包含了一个领域里的状态、以及对状态的直接操作。Entity最重要的设计原则是保证实体的不变性（Invariants），也就是说要确保无论外部怎么操作，一个实体内部的属性都不能出现相互冲突，状态不一致的情况。所以几个设计原则如下： 1）创建即一致在贫血模型里，通常见到的代码是一个模型通过手动new出来之后，由调用方一个参数一个参数的赋值，这就很容易产生遗漏，导致实体状态不一致。所以DDD里实体创建的方法有两种： constructor参数要包含所有必要属性，或者在constructor里有合理的默认值。 比如，账号的创建： 123456789101112public class Account &#123; private String accountNumber; private Long amount;&#125; @Testpublic void test() &#123; Account account = new Account(); account.setAmount(100L); TransferService.transfer(account); // 报错了，因为Account缺少必要的AccountNumber&#125; 如果缺少一个强校验的constructor，就无法保障创建的实体的一致性。所以需要增加一个强校验的constructor： 1234567891011121314public class Account &#123; public Account(String accountNumber, Long amount) &#123; assert StringUtils.isNotBlank(accountNumber); assert amount &gt;= 0; this.accountNumber = accountNumber; this.amount = amount; &#125;&#125; @Testpublic void test() &#123; Account account = new Account(&quot;123&quot;, 100L); // 确保对象的有效性&#125; 2)使用Factory模式来降低调用方复杂度另一种方法是通过Factory模式来创建对象，降低一些重复性的入参。比如： 123456public class WeaponFactory &#123; public Weapon createWeaponFromPrototype(WeaponPrototype proto, String newName) &#123; Weapon weapon = new Weapon(null, newName, proto.getWeaponType(), proto.getDamage(), proto.getDamageType()); return weapon; &#125;&#125; 通过传入一个已经存在的Prototype，可以快速的创建新的实体。还有一些其他的如Builder等设计模式就不一一指出了。 3)尽量避免public setter一个最容易导致不一致性的原因是实体暴露了public的setter方法，特别是set单一参数会导致状态不一致的情况。比如，一个订单可能包含订单状态（下单、已支付、已发货、已收货）、支付单、物流单等子实体，如果一个调用方能随意去set订单状态，就有可能导致订单状态和子实体匹配不上，导致业务流程走不通的情况。所以在实体里，需要通过行为方法来修改内部状态： 123456789101112131415161718192021222324@Data @Setter(AccessLevel.PRIVATE) // 确保不生成public setterpublic class Order &#123; private int status; // 0 - 创建，1 - 支付，2 - 发货，3 - 收货 private Payment payment; // 支付单 private Shipping shipping; // 物流单 public void pay(Long userId, Long amount) &#123; if (status != 0) &#123; throw new IllegalStateException(); &#125; this.status = 1; this.payment = new Payment(userId, amount); &#125; public void ship(String trackingNumber) &#123; if (status != 1) &#123; throw new IllegalStateException(); &#125; this.status = 2; this.shipping = new Shipping(trackingNumber); &#125;&#125; 4)通过聚合根保证主子实体的一致性在稍微复杂一点的领域里，通常主实体会包含子实体，这时候主实体就需要起到聚合根的作用，即： 子实体不能单独存在，只能通过聚合根的方法获取到。任何外部的对象都不能直接保留子实体的引用 子实体没有独立的Repository，不可以单独保存和取出，必须要通过聚合根的Repository实例化 子实体可以单独修改自身状态，但是多个子实体之间的状态一致性需要聚合根来保障 常见的电商域中聚合的案例如主子订单模型、商品/SKU模型、跨子订单优惠、跨店优惠模型等。很多聚合根和Repository的设计规范在我前面一篇关于Repository的文章中已经详细解释过，可以拿来参考。 5)不可以强依赖其他聚合根实体或领域服务一个实体的原则是高内聚、低耦合，即一个实体类不能直接在内部直接依赖一个外部的实体或服务。这个原则和绝大多数ORM框架都有比较严重的冲突，所以是一个在开发过程中需要特别注意的。这个原则的必要原因包括：对外部对象的依赖性会直接导致实体无法被单测；以及一个实体无法保证外部实体变更后不会影响本实体的一致性和正确性。 所以，正确的对外部依赖的方法有两种： 只保存外部实体的ID：这里我再次强烈建议使用强类型的ID对象，而不是Long型ID。强类型的ID对象不单单能自我包含验证代码，保证ID值的正确性，同时还能确保各种入参不会因为参数顺序变化而出bug。具体可以参考我的Domain Primitive文章。 针对于“无副作用”的外部依赖，通过方法入参的方式传入。比如上文中的equip(Weapon，EquipmentService）方法。 如果方法对外部依赖有副作用，不能通过方法入参的方式，只能通过Domain Service解决，见下文。 6)任何实体的行为只能直接影响到本实体（和其子实体）这个原则更多是一个确保代码可读性、可理解的原则，即任何实体的行为不能有“直接”的”副作用“，即直接修改其他的实体类。这么做的好处是代码读下来不会产生意外。 另一个遵守的原因是可以降低未知的变更的风险。在一个系统里一个实体对象的所有变更操作应该都是预期内的，如果一个实体能随意被外部直接修改的话，会增加代码bug的风险。 2.领域服务在上文讲到，领域服务其实也分很多种，在这里根据上文总结出来三种常见的： 1)单对象策略型这种领域对象主要面向的是单个实体对象的变更，但涉及到多个领域对象或外部依赖的一些规则。在上文中，EquipmentService即为此类： 变更的对象是Player的参数 读取的是Player和Weapon的数据，可能还包括从外部读取一些数据 在这种类型下，实体应该通过方法入参的方式传入这种领域服务，然后通过Double Dispatch来反转调用领域服务的方法，比如： 123Player.equip(Weapon, EquipmentService) &#123; EquipmentService.canEquip(this, Weapon);&#125; 为什么这种情况下不能先调用领域服务，再调用实体对象的方法，从而减少实体对领域服务的入参型依赖呢？比如，下面这个方法是错误的： 1234boolean canEquip = EquipmentService.canEquip(Player, Weapon);if (canEquip) &#123; Player.equip(Weapon); // ❌，这种方法不可行，因为这个方法有不一致的可能性&#125; 其错误的主要原因是缺少了领域服务入参会导致方法有可能产生不一致的情况。 2)跨对象事务型当一个行为会直接修改多个实体时，不能再通过单一实体的方法作处理，而必须直接使用领域服务的方法来做操作。在这里，领域服务更多的起到了跨对象事务的作用，确保多个实体的变更之间是有一致性的。 在上文里，虽然以下的代码虽然可以跑到通，但是是不建议的： 12345public class Player &#123; void attack(Monster, CombatService) &#123; CombatService.performAttack(this, Monster); // ❌，不要这么写，会导致副作用 &#125;&#125; 而我们真实调用应该直接调用CombatService的方法： 1234public void test() &#123; //... combatService.performAttack(mage, orc);&#125; Player.attack会直接影响到Monster，但这个调用Monster又没有感知。 3)通用组件型这种类型的领域服务更像ECS里的System，提供了组件化的行为，但本身又不直接绑死在一种实体类上。具体案例可以参考上文中的MovementSystem实现。 3.策略对象Policy或者Strategy设计模式是一个通用的设计模式，但是在DDD架构中会经常出现，其核心就是封装领域规则。 一个Policy是一个无状态的单例对象，通常需要至少2个方法：canApply 和 一个业务方法。其中，canApply方法用来判断一个Policy是否适用于当前的上下文，如果适用则调用方会去触发业务方法。通常，为了降低一个Policy的可测试性和复杂度，Policy不应该直接操作对象，而是通过返回计算后的值，在Domain Service里对对象进行操作。 在上文案例里，DamagePolicy只负责计算应该受到的伤害，而不是直接对Monster造成伤害。这样除了可测试外，还为未来的多Policy叠加计算做了准备。 除了本文里静态注入多个Policy以及手动排优先级之外，在日常开发中经常能见到通过Java的SPI机制或类SPI机制注册Policy，以及通过不同的Priority方案对Policy进行排序，在这里就不作太多的展开了。 六，领域事件一般的副作用发生在核心领域模型状态变更后，同步或者异步对另一个对象的影响或行为。在这个案例里，我们可以增加一个副作用规则： 当Monster的生命值降为0后，给Player奖励经验值 这种问题有很多种解法，比如直接把副作用写在CombatService里： 123456789public class CombatService &#123; public void performAttack(Player player, Monster monster) &#123; // ... monster.takeDamage(damage); if (!monster.isAlive()) &#123; player.receiveExp(10); // 收到经验 &#125; &#125;&#125; 但是这样写的问题是：很快CombatService的代码就会变得很复杂，比如我们再加一个副作用： 当Player的exp达到100时，升一级 这时我们的代码就会变成： 123456789101112public class CombatService &#123; public void performAttack(Player player, Monster monster) &#123; // ... monster.takeDamage(damage); if (!monster.isAlive()) &#123; player.receiveExp(10); // 收到经验 if (player.canLevelUp()) &#123; player.levelUp(); // 升级 &#125; &#125; &#125;&#125; 如果再加上“升级后奖励XXX”呢？“更新XXX排行”呢？依此类推，后续这种代码将无法维护。所以我们需要介绍一下领域层最后一个概念：领域事件（Domain Event）。 1.领域事件介绍领域事件是一个在领域里发生了某些事后，希望领域里其他对象能够感知到的通知机制。在上面的案例里，代码之所以会越来越复杂，其根本的原因是反应代码（比如升级）直接和上面的事件触发条件（比如收到经验）直接耦合，而且这种耦合性是隐性的。领域事件的好处就是将这种隐性的副作用“显性化”，通过一个显性的事件，将事件触发和事件处理解耦，最终起到代码更清晰、扩展性更好的目的。 所以，领域事件是在DDD里，比较推荐使用的跨实体“副作用”传播机制。 2.领域事件实现和消息队列中间件不同的是，领域事件通常是立即执行的、在同一个进程内、可能是同步或异步。我们可以通过一个EventBus来实现进程内的通知机制，简单实现如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758// 实现者：瑜进 2019/11/28public class EventBus &#123; // 注册器 @Getter private final EventRegistry invokerRegistry = new EventRegistry(this); // 事件分发器 private final EventDispatcher dispatcher = new EventDispatcher(ExecutorFactory.getDirectExecutor()); // 异步事件分发器 private final EventDispatcher asyncDispatcher = new EventDispatcher(ExecutorFactory.getThreadPoolExecutor()); // 事件分发 public boolean dispatch(Event event) &#123; return dispatch(event, dispatcher); &#125; // 异步事件分发 public boolean dispatchAsync(Event event) &#123; return dispatch(event, asyncDispatcher); &#125; // 内部事件分发 private boolean dispatch(Event event, EventDispatcher dispatcher) &#123; checkEvent(event); // 1.获取事件数组 Set&lt;Invoker&gt; invokers = invokerRegistry.getInvokers(event); // 2.一个事件可以被监听N次，不关心调用结果 dispatcher.dispatch(event, invokers); return true; &#125; // 事件总线注册 public void register(Object listener) &#123; if (listener == null) &#123; throw new IllegalArgumentException(&quot;listener can not be null!&quot;); &#125; invokerRegistry.register(listener); &#125; private void checkEvent(Event event) &#123; if (event == null) &#123; throw new IllegalArgumentException(&quot;event&quot;); &#125; if (!(event instanceof Event)) &#123; throw new IllegalArgumentException(&quot;Event type must by &quot; + Event.class); &#125; &#125;&#125; 调用方式 123456789101112131415161718192021222324252627public class LevelUpEvent implements Event &#123; private Player player;&#125; public class LevelUpHandler &#123; public void handle(Player player);&#125; public class Player &#123; public void receiveExp(int value) &#123; this.exp += value; if (this.exp &gt;= 100) &#123; LevelUpEvent event = new LevelUpEvent(this); EventBus.dispatch(event); this.exp = 0; &#125; &#125;&#125;@Testpublic void test() &#123; EventBus.register(new LevelUpHandler()); player.setLevel(1); player.receiveExp(100); assertThat(player.getLevel()).equals(2);&#125; 3.目前领域事件的缺陷和展望从上面代码可以看出来，领域事件的很好的实施依赖EventBus、Dispatcher、Invoker这些属于框架级别的支持。同时另一个问题是因为Entity不能直接依赖外部对象，所以EventBus目前只能是一个全局的Singleton，而大家都应该知道全局Singleton对象很难被单测。这就容易导致Entity对象无法被很容易的被完整单测覆盖全。 另一种解法是侵入Entity，对每个Entity增加一个List: 1234567891011121314151617181920212223242526272829public class Player &#123; List&lt;Event&gt; events; public void receiveExp(int value) &#123; this.exp += value; if (this.exp &gt;= 100) &#123; LevelUpEvent event = new LevelUpEvent(this); events.add(event); // 把event加进去 this.exp = 0; &#125; &#125;&#125; @Testpublic void test() &#123; EventBus.register(new LevelUpHandler()); player.setLevel(1); player.receiveExp(100); for(Event event: player.getEvents()) &#123; // 在这里显性的dispatch事件 EventBus.dispatch(event); &#125; assertThat(player.getLevel()).equals(2);&#125; 但是能看出来这种解法不但会侵入实体本身，同时也需要比较啰嗦的显性在调用方dispatch事件，也不是一个好的解决方案。 也许未来会有一个框架能让我们既不依赖全局Singleton，也不需要显性去处理事件，但目前的方案基本都有或多或少的缺陷，大家在使用中可以注意。 七，总结在真实的业务逻辑里，我们的领域模型或多或少的都有一定的“特殊性”，如果100%的要符合DDD规范可能会比较累，所以最主要的是梳理一个对象行为的影响面，然后作出设计决策，即： 是仅影响单一对象还是多个对象， 规则未来的拓展性、灵活性， 性能要求， 副作用的处理，等等 当然，很多时候一个好的设计是多种因素的取舍，需要大家有一定的积累，真正理解每个架构背后的逻辑和优缺点。一个好的架构师不是有一个正确答案，而是能从多个方案中选出一个最平衡的方案。","categories":[{"name":"领域驱动设计","slug":"领域驱动设计","permalink":"https://yinhuidong.github.io/categories/%E9%A2%86%E5%9F%9F%E9%A9%B1%E5%8A%A8%E8%AE%BE%E8%AE%A1/"}],"tags":[{"name":"DDD","slug":"DDD","permalink":"https://yinhuidong.github.io/tags/DDD/"}]},{"title":"浅谈领域模型设计","slug":"领域驱动设计/浅谈领域模型设计","date":"2022-01-12T00:19:55.254Z","updated":"2022-01-12T00:19:55.254Z","comments":true,"path":"2022/01/12/领域驱动设计/浅谈领域模型设计/","link":"","permalink":"https://yinhuidong.github.io/2022/01/12/%E9%A2%86%E5%9F%9F%E9%A9%B1%E5%8A%A8%E8%AE%BE%E8%AE%A1/%E6%B5%85%E8%B0%88%E9%A2%86%E5%9F%9F%E6%A8%A1%E5%9E%8B%E8%AE%BE%E8%AE%A1/","excerpt":"","text":"一，前言-我对DDD的一点小看法什么也不说，从一个小需求开始，事实胜于雄辩。 二，实际需求1.需求一期，用户注册假如现在需要很多业务员推广一个银行app，让人注册，当用户注册的时候，根据电话号的区号讲用户划分给对应区的业务员，方便后面算业务员的绩效。 1）传统模式下的代码12345678910111213141516171819202122232425262728293031323334353637383940414243public class User&#123; Long UserId; String name; String phone; String address; Long repId;&#125;public class UserService&#123; private SalesRepRepository salesRepRepository; private User Repository; public User register(String name ,String phone , String address)&#123; //检验逻辑 if(name == null || name.length == 0)&#123; throw new Exception(&quot;注册用户名不能为空！&quot;) &#125; //此处省略校验电话号，地址逻辑 //取电话号里面的区号，然后通过区号找到区域内的SalesRep String areaCode=null; String[] areas = new String[]&#123;&quot;0571&quot;, &quot;021&quot;, &quot;010&quot;&#125;; for (int i = 0; i &amp;lt; phone.length(); i++) &#123; String prefix = phone.substring(0, i); if (Arrays.asList(areas).contains(prefix)) &#123; areaCode = prefix; break; &#125; &#125; SalesRep rep = salesRepRepository.findRep(areaCode); //最后创建用户，落盘，然后返回 User user = new User(); user.name=name; user.phone=phone; user.address=address; if(rep!=null)&#123; user.repId = rep.repId; return userRepo.save(user); &#125;&#125; 2）领域模块设计后的代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public class PhoneNumber&#123; private final String number; public String getNumber()&#123; return number; &#125; public PhoneNumber(String number)&#123; if(number == null)&#123; throw new Exception(&quot;number is empty!&quot;); &#125;else if(isValid(number))&#123; throw new Exception(&quot;number format is error&quot;) &#125; &#125; public String getAreaCode() &#123; for (int i = 0; i &amp;lt; number.length(); i++) &#123; String prefix = number.substring(0, i); if (isAreaCode(prefix)) &#123; return prefix; &#125; &#125; return null; &#125; private static boolean isAreaCode(String prefix) &#123; String[] areas = new String[]&#123;&quot;0571&quot;, &quot;021&quot;, &quot;010&quot;&#125;; return Arrays.asList(areas).contains(prefix); &#125; public static boolean isValid(String number) &#123; String pattern = &quot;^0?[1-9]&#123;2,3&#125;-?\\\\d&#123;8&#125;$&quot;; return number.matches(pattern); &#125;&#125;public class User&#123; UserId userId; Name name; PhoneNumber phone; Address address; RepId repId;&#125;public User register(Name name,PhoneNumber phone,Address address)&#123; //根据手机号查找业务员 SalesRep rep=salesRepRepository.findRep(phone.getAreaCode); User user=new User(); user.name=name; user.phone=phone; user.address = address; if(rep!=null)&#123; user.repId=rep.repId; &#125; return userRepo.saveUser(user);&#125; 3）对比①接口清晰度传统代码传参三个String类型的参数，如果顺序错了，这种问题，code review就能发现么？ ②业务逻辑清晰度参数校验和错误处理全部写在service，这样的做法，是不是会让业务逻辑看起来并不明确？ 假如我现在增加一个字段，是不是还得继续加参数校验逻辑，假如我有十个地方需要传这四个参数，是不是十个地方都要改？代码冗余，可能忘记修改某一个地方，业务逻辑不清晰。 ③单元测试很多时候，我们接口的传参都是允许不传或者有默认值的，或者我现在需求改动，又加了或者较少了一个参数，单元测试的覆盖率怎么样？能否保证所有情况全部被覆盖？ 2.需求二期，国内转账业务员很给力，推广了很多用户都来注册，接下来要开始做真正的业务需求了。 1）传统方式开发123public void pay(BigDecimal money, Long recipientId) &#123; BankService.transfer(money, &quot;CNY&quot;, recipientId);&#125; 2)领域模型开发12345678910111213public class Money &#123; private BigDecimal amount; private Currency currency; public Money(BigDecimal amount, Currency currency) &#123; this.amount = amount; this.currency = currency; &#125;&#125;public void pay(Money money,Long recipientId)&#123; BankService.transfer(money,recipientId);&#125; 3）网络键盘侠此时好像领域模型开发代码量更大，项目结构更复杂，垃圾！ 真的是这样么？走着瞧。 3.需求三期，支持跨国转账，手动计算汇率一期的时候，一切顺利进行，到了二期，产品说：小开发呀，我们的业务越做越大了，已经扩展到海外了，现在需要考虑跨国转账了，还得计算汇率。作为开发，你虽然心里把产品骂开了花，但是不得不跟产品说，行，没问题，我们定个排期。（你个xxxxx，我xxxxx） 1）传统方式开发12345678910public void pay(Money money, Currency targetCurrency, Long recipientId) &#123; if (money.getCurrency().equals(targetCurrency)) &#123; BankService.transfer(money, recipientId); &#125; else &#123; BigDecimal rate = ExchangeService.getRate(money.getCurrency(), targetCurrency); BigDecimal targetAmount = money.getAmount().multiply(new BigDecimal(rate)); Money targetMoney = new Money(targetAmount, targetCurrency); BankService.transfer(targetMoney, recipientId); &#125;&#125; 2）领域模型开发12345678910111213141516171819202122232425@Value //ExchangeRate 汇率对象，通过封装金额计算逻辑以及各种校验逻辑，让原始代码变得极其简单：public class ExchangeRate &#123; private BigDecimal rate; private Currency from; private Currency to; public ExchangeRate(BigDecimal rate, Currency from, Currency to) &#123; this.rate = rate; this.from = from; this.to = to; &#125; public Money exchange(Money fromMoney) &#123; notNull(fromMoney); isTrue(this.from.equals(fromMoney.getCurrency())); BigDecimal targetAmount = fromMoney.getAmount().multiply(rate); return new Money(targetAmount, to); &#125;&#125;public void pay(Money money, Currency targetCurrency, Long recipientId) &#123; ExchangeRate rate = ExchangeService.getRate(money.getCurrency(), targetCurrency); Money targetMoney = rate.exchange(money); BankService.transfer(targetMoney, recipientId);&#125; 3)网络键盘侠到了此时，传统模式开发的代码已经需求重构了，业务逻辑也开始增加，不再清晰。 4.需求四期，需要保留转账存根，调用第三方接口计算汇率1）传统方式开发123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public class TransferController &#123; private TransferService transferService; public Result&lt;Boolean&gt; transfer(String targetAccountNumber, BigDecimal amount, HttpSession session) &#123; Long userId = (Long) session.getAttribute(&quot;userId&quot;); return transferService.transfer(userId, targetAccountNumber, amount, &quot;CNY&quot;); &#125;&#125; public class TransferServiceImpl implements TransferService &#123; private static final String TOPIC_AUDIT_LOG = &quot;TOPIC_AUDIT_LOG&quot;; private AccountMapper accountDAO; private KafkaTemplate&lt;String, String&gt; kafkaTemplate; private YahooForexService yahooForex; @Override public Result&lt;Boolean&gt; transfer(Long sourceUserId, String targetAccountNumber, BigDecimal targetAmount, String targetCurrency) &#123; // 1. 从数据库读取数据，忽略所有校验逻辑如账号是否存在等 AccountDO sourceAccountDO = accountDAO.selectByUserId(sourceUserId); AccountDO targetAccountDO = accountDAO.selectByAccountNumber(targetAccountNumber); // 2. 业务参数校验 if (!targetAccountDO.getCurrency().equals(targetCurrency)) &#123; throw new InvalidCurrencyException(); &#125; // 3. 获取外部数据，并且包含一定的业务逻辑 // exchange rate = 1 source currency = X target currency BigDecimal exchangeRate = BigDecimal.ONE; if (sourceAccountDO.getCurrency().equals(targetCurrency)) &#123; exchangeRate = yahooForex.getExchangeRate(sourceAccountDO.getCurrency(), targetCurrency); &#125; BigDecimal sourceAmount = targetAmount.divide(exchangeRate, RoundingMode.DOWN); // 4. 业务参数校验 if (sourceAccountDO.getAvailable().compareTo(sourceAmount) &lt; 0) &#123; throw new InsufficientFundsException(); &#125; if (sourceAccountDO.getDailyLimit().compareTo(sourceAmount) &lt; 0) &#123; throw new DailyLimitExceededException(); &#125; // 5. 计算新值，并且更新字段 BigDecimal newSource = sourceAccountDO.getAvailable().subtract(sourceAmount); BigDecimal newTarget = targetAccountDO.getAvailable().add(targetAmount); sourceAccountDO.setAvailable(newSource); targetAccountDO.setAvailable(newTarget); // 6. 更新到数据库 accountDAO.update(sourceAccountDO); accountDAO.update(targetAccountDO); // 7. 发送审计消息 String message = sourceUserId + &quot;,&quot; + targetAccountNumber + &quot;,&quot; + targetAmount + &quot;,&quot; + targetCurrency; kafkaTemplate.send(TOPIC_AUDIT_LOG, message); return Result.success(true); &#125; &#125; 2) 领域模型开发①抽象数据存储层1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162//抽象数据存储层@Datapublic class Account &#123; private AccountId id; private AccountNumber accountNumber; private UserId userId; private Money available; private Money dailyLimit; public void withdraw(Money money) &#123; // 转出 &#125; public void deposit(Money money) &#123; // 转入 &#125;&#125;public interface AccountRepository &#123; Account find(AccountId id); Account find(AccountNumber accountNumber); Account find(UserId userId); Account save(Account account);&#125; public class AccountRepositoryImpl implements AccountRepository &#123; @Autowired private AccountMapper accountDAO; @Autowired private AccountBuilder accountBuilder; @Override public Account find(AccountId id) &#123; AccountDO accountDO = accountDAO.selectById(id.getValue()); return accountBuilder.toAccount(accountDO); &#125; @Override public Account find(AccountNumber accountNumber) &#123; AccountDO accountDO = accountDAO.selectByAccountNumber(accountNumber.getValue()); return accountBuilder.toAccount(accountDO); &#125; @Override public Account find(UserId userId) &#123; AccountDO accountDO = accountDAO.selectByUserId(userId.getId()); return accountBuilder.toAccount(accountDO); &#125; @Override public Account save(Account account) &#123; AccountDO accountDO = accountBuilder.fromAccount(account); if (accountDO.getId() == null) &#123; accountDAO.insert(accountDO); &#125; else &#123; accountDAO.update(accountDO); &#125; return accountBuilder.toAccount(accountDO); &#125; &#125; ②抽象第三方服务1234567891011121314151617public interface ExchangeRateService &#123; ExchangeRate getExchangeRate(Currency source, Currency target);&#125; public class ExchangeRateServiceImpl implements ExchangeRateService &#123; @Autowired private YahooForexService yahooForexService; @Override public ExchangeRate getExchangeRate(Currency source, Currency target) &#123; if (source.equals(target)) &#123; return new ExchangeRate(BigDecimal.ONE, source, target); &#125; BigDecimal forex = yahooForexService.getExchangeRate(source.getValue(), target.getValue()); return new ExchangeRate(forex, source, target); &#125; ③抽象中间件1234567891011121314151617181920212223242526272829303132333435363738@Value@AllArgsConstructorpublic class AuditMessage &#123; private UserId userId; private AccountNumber source; private AccountNumber target; private Money money; private Date date; public String serialize() &#123; return userId + &quot;,&quot; + source + &quot;,&quot; + target + &quot;,&quot; + money + &quot;,&quot; + date; &#125; public static AuditMessage deserialize(String value) &#123; // todo return null; &#125;&#125; public interface AuditMessageProducer &#123; SendResult send(AuditMessage message);&#125; public class AuditMessageProducerImpl implements AuditMessageProducer &#123; private static final String TOPIC_AUDIT_LOG = &quot;TOPIC_AUDIT_LOG&quot;; @Autowired private KafkaTemplate&lt;String, String&gt; kafkaTemplate; @Override public SendResult send(AuditMessage message) &#123; String messageBody = message.serialize(); kafkaTemplate.send(TOPIC_AUDIT_LOG, messageBody); return SendResult.success(); &#125;&#125; ④封装业务逻辑123//封装业务逻辑ExchangeRate exchangeRate = exchangeRateService.getExchangeRate(sourceAccount.getCurrency(), targetMoney.getCurrency());Money sourceMoney = exchangeRate.exchangeTo(targetMoney); 1234567891011121314151617181920212223242526272829303132@Data//封装转账方法public class Account &#123; private AccountId id; private AccountNumber accountNumber; private UserId userId; private Money available; private Money dailyLimit; public Currency getCurrency() &#123; return this.available.getCurrency(); &#125; // 转入 public void deposit(Money money) &#123; if (!this.getCurrency().equals(money.getCurrency())) &#123; throw new InvalidCurrencyException(); &#125; this.available = this.available.add(money); &#125; // 转出 public void withdraw(Money money) &#123; if (this.available.compareTo(money) &lt; 0) &#123; throw new InsufficientFundsException(); &#125; if (this.dailyLimit.compareTo(money) &lt; 0) &#123; throw new DailyLimitExceededException(); &#125; this.available = this.available.subtract(money); &#125;&#125; 1234567891011121314public interface AccountTransferService &#123; void transfer(Account sourceAccount, Account targetAccount, Money targetMoney, ExchangeRate exchangeRate);&#125; public class AccountTransferServiceImpl implements AccountTransferService &#123; private ExchangeRateService exchangeRateService; @Override public void transfer(Account sourceAccount, Account targetAccount, Money targetMoney, ExchangeRate exchangeRate) &#123; Money sourceMoney = exchangeRate.exchangeTo(targetMoney); sourceAccount.deposit(sourceMoney); targetAccount.withdraw(targetMoney); &#125;&#125; ⑤最终业务逻辑12345678910111213141516171819202122232425262728293031public class TransferServiceImplNew implements TransferService &#123; private AccountRepository accountRepository; private AuditMessageProducer auditMessageProducer; private ExchangeRateService exchangeRateService; private AccountTransferService accountTransferService; @Override public Result&lt;Boolean&gt; transfer(Long sourceUserId, String targetAccountNumber, BigDecimal targetAmount, String targetCurrency) &#123; // 参数校验 Money targetMoney = new Money(targetAmount, new Currency(targetCurrency)); // 读数据 Account sourceAccount = accountRepository.find(new UserId(sourceUserId)); Account targetAccount = accountRepository.find(new AccountNumber(targetAccountNumber)); ExchangeRate exchangeRate = exchangeRateService.getExchangeRate(sourceAccount.getCurrency(), targetMoney.getCurrency()); // 业务逻辑 accountTransferService.transfer(sourceAccount, targetAccount, targetMoney, exchangeRate); // 保存数据 accountRepository.save(sourceAccount); accountRepository.save(targetAccount); // 发送审计消息 AuditMessage message = new AuditMessage(sourceAccount, targetAccount, targetMoney); auditMessageProducer.send(message); return Result.success(true); &#125;&#125; 3)网络键盘侠传统开发： 一段业务代码里经常包含了参数校验、数据读取存储、业务计算、调用外部服务、发送消息等多种逻辑。在这个案例里虽然是写在了同一个方法里，在真实代码中经常会被拆分成多个子方法，但实际效果是一样的，而在我们日常的工作中，绝大部分代码都或多或少的接近于此类结构。在Martin Fowler的 P of EAA书中，这种很常见的代码样式被叫做Transaction Script（事务脚本）。虽然这种类似于脚本的写法在功能上没有什么问题，但是长久来看，他有以下几个很大的问题：可维护性差、可扩展性差、可测试性差。 领域模型： 业务逻辑清晰，数据存储和业务逻辑完全分隔。 Entity、Domain Primitive、Domain Service都是独立的对象，没有任何外部依赖，但是却包含了所有核心业务逻辑，可以单独完整测试。 原有的TransferService不再包括任何计算逻辑，仅仅作为组件编排，所有逻辑均delegate到其他组件。这种仅包含Orchestration（编排）的服务叫做Application Service（应用服务）。 最底层不再是数据库，而是Entity、Domain Primitive和Domain Service。这些对象不依赖任何外部服务和框架，而是纯内存中的数据和操作。这些对象我们打包为Domain Layer（领域层）。领域层没有任何外部依赖关系。 再其次的是负责组件编排的Application Service，但是这些服务仅仅依赖了一些抽象出来的ACL类和Repository类，而其具体实现类是通过依赖注入注进来的。Application Service、Repository、ACL等我们统称为Application Layer（应用层）。应用层 依赖 领域层，但不依赖具体实现。 最后是ACL，Repository等的具体实现，这些实现通常依赖外部具体的技术实现和框架，所以统称为Infrastructure Layer（基础设施层）。Web框架里的对象如Controller之类的通常也属于基础设施层。 三，感想写这段代码，考虑到最终的依赖关系，我们可能先写Domain层的业务逻辑，然后再写Application层的组件编排，最后才写每个外部依赖的具体实现。这种架构思路和代码组织结构就叫做Domain-Driven Design（领域驱动设计，或DDD）。 DDD不是一个什么特殊的架构，而是任何传统代码经过合理的重构之后最终一定会抵达的终点。DDD的架构能够有效的解决传统架构中的问题： 高可维护性：当外部依赖变更时，内部代码只用变更跟外部对接的模块，其他业务逻辑不变。 高可扩展性：做新功能时，绝大部分的代码都能复用，仅需要增加核心业务逻辑即可。 高可测试性：每个拆分出来的模块都符合单一性原则，绝大部分不依赖框架，可以快速的单元测试，做到100%覆盖。 代码结构清晰：通过POM module可以解决模块间的依赖关系， 所有外接模块都可以单独独立成Jar包被复用。当团队形成规范后，可以快速的定位到相关代码。 传统的面向对象设计，对象里面只定义了属性，不包含行为方法，在领域模型里，对象里面应该包含着属性还有行为方法，通过对不同种类对象的划分，在业务层进行组合完成功能，就像映射到一个实体的人类，人代表一个类，之前我们只标记了他有名字，照片，年龄，现在我想把行为也定义进来，喝水，吃饭。走路，抽烟。我在完成一件事的时候，实际上可能是很多人，很多动作的组合。 领域模型设计，可以理解成就是将业务拆分成小单元（基本行为），划分给每一个对象，业务层只需要关心如何组装对象，让开发人员做到简洁开发。说的简单一点，我现在有一个箱子，里面有很多积木，有圆的，长方形的，正方形的，我现在需要把他们组装在一起，变成一个变形金刚或者房子，汽车。 依个人浅薄意见，由于业务场景的不同，模型的定义和功能（行为）就不同，领域的边界划分是重点，只有边界明确了。才能更好的实施，表面上看，项目结构好像更加复杂化了，实际上，如果经历了长期的迭代，需求变更，他只会带来轻松，灵活，易扩展，很少情况下会重构代码。 这个理念是否要落地成一个框架？框架怎么设计？比如Dubbo的SPI机制，是不是也是一种领域模型设计，因为要完成的功能不同，如果设计成框架，很难进行统一，是不是可以只提供核心规范，定义一些标准的格式，剩下的都通过扩展点留给开发者根据需求扩展。 其实不应该设计成框架的，他应该是一种理念或者规范，我们的一切目的都是为了简化开发，所以应该是模型更正确，我们定义好一组模型，按照这个模型规范进行开发，针对不同的需求，架构师划分领域边界，规定好入参出参，开发人员负责开发具体代码，在service层进行简单组装，清晰明了，如果领域划分的明确，开发应该变得很简单。 传统的应用开发，很大程度上，业务驱动技术和架构，也就是业务驱动模型，在DDD里，我们换了一个角度，以模型驱动业务，通过不同模型里面方法的拼装，完成业务功能。就比如我们为什么要定义VO,DTO,PO，还不是为了适应业务需求，可能单纯映射数据库的实体类，并不能满足业务需求，需要扩展，再比如，数据库里，订单是一张表，每一条记录都是独立的，但是实际上，会涉及到拆单，拼单，但是加入我们提前定义好了模型，模型领域划分明确，是不是在业务层只需要调用order的方法就可以了？ 展望未来，如果DDD可以大行其道，是不是以后会多很多模型jar包，里面封装这各种各样的模型，开发人员根据业务需求引入各种各样的模型jar，只需要在service层简单拼装，就可以完成很多复杂的需求。","categories":[{"name":"领域驱动设计","slug":"领域驱动设计","permalink":"https://yinhuidong.github.io/categories/%E9%A2%86%E5%9F%9F%E9%A9%B1%E5%8A%A8%E8%AE%BE%E8%AE%A1/"}],"tags":[{"name":"DDD","slug":"DDD","permalink":"https://yinhuidong.github.io/tags/DDD/"}]},{"title":"DDD之DP","slug":"领域驱动设计/DDD之DP","date":"2022-01-12T00:19:55.253Z","updated":"2022-01-12T00:19:55.253Z","comments":true,"path":"2022/01/12/领域驱动设计/DDD之DP/","link":"","permalink":"https://yinhuidong.github.io/2022/01/12/%E9%A2%86%E5%9F%9F%E9%A9%B1%E5%8A%A8%E8%AE%BE%E8%AE%A1/DDD%E4%B9%8BDP/","excerpt":"","text":"一，还在用传统业务模型么？1.从一个小需求开始首先提一个小需求，我用传统的编码方式完成它。 做一个用户注册系统，同时希望在用户注册后能够通过用户电话（先假设仅限座机）的地域（区号）对业务员发奖金。 12345678910111213141516171819202122232425262728293031323334353637383940414243public class User&#123; Long UserId; String name; String phone; String address; Long repId;&#125;public class UserService&#123; private SalesRepRepository salesRepRepository; private User Repository; public User register(String name ,String phone , String address)&#123; //检验逻辑 if(name == null || name.length == 0)&#123; throw new Exception(&quot;注册用户名不能为空！&quot;) &#125; //此处省略校验电话号，地址逻辑 //取电话号里面的区号，然后通过区号找到区域内的SalesRep String areaCode=null; String[] areas = new String[]&#123;&quot;0571&quot;, &quot;021&quot;, &quot;010&quot;&#125;; for (int i = 0; i &amp;lt; phone.length(); i++) &#123; String prefix = phone.substring(0, i); if (Arrays.asList(areas).contains(prefix)) &#123; areaCode = prefix; break; &#125; &#125; SalesRep rep = salesRepRepository.findRep(areaCode); //最后创建用户，落盘，然后返回 User user = new User(); user.name=name; user.phone=phone; user.address=address; if(rep!=null)&#123; user.repId = rep.repId; return userRepo.save(user); &#125;&#125; 咋一看这段代码毫无问题，参数进行了检验，业务逻辑还算合理，最终落盘完成了需求。真的是这样么？转备好，接下来，我要开始挑刺儿了。 2.需求分析我们日常大部分业务代码和模型其实都是跟这个是类似的，貌似我从接触代码的第一天起，就觉得代码应该这样写，直到我看了阿里技术专家团队的文章，只是感觉我的传统思想被打破了，为我打开了一扇新的大门。 从以下四个维度去分析这个代码： 1）接口清晰度在Java代码中，对于一个方法来说所有的参数名在编译时丢失，留下的是一个参数类型的列表，所以，在运行时： 1User register(String,String,String); 所以以下的代码是一段编译器完全不会报错的，很难通过看代码就能发现的 bug ： 1service.register(&quot;尹会东&quot;, &quot;北京市昌平区天通苑本五区&quot;, &quot;0571-12345678&quot;); 这段代码的问题，就算是普通的code review也很难发现错误。 还有一种情况： 123User findByName(String name);User findByPhone(String phone);User findByNameAndPhone(String name, String phone); 这里参数顺序错了只会返回null，并不会报错。 2)数据验证和错误处理作为服务端，任何来源的参数对我们而讲都是不可信的，但是业务代码里面充满大量的参数校验，而且甚至每一个接口都会出现，，甚至可能还会重复，或者未来我们加一个字段，如果有很多地方都需要加这个字段，但是有一个地方我们忘了加，那将是毁灭性的影响。 可能大部分人会想到，是不是可以通过注解的方式校验，但是注解你就能保证，很多地方同时加，你不会有遗漏么？而且复杂的逻辑校验，是不是还是需要我们手动硬编码？ 3）业务代码的清晰度从一些入参里抽取一部分数据，然后调用一个外部依赖获取更多的数据，然后通常从新的数据中再抽取部分数据用作其他的作用。这种代码通常被称作“胶水代码”，其本质是由于外部依赖的服务的入参并不符合我们原始的入参导致的。 当然你可能会提出是否可以抽取出来一个静态工具类，但是这里要思考的是，静态工具类是否是最好的实现方式呢？当你的项目里充斥着大量的静态工具类，业务代码散在多个文件当中时，你是否还能找到核心的业务逻辑呢？ 4）可测试性单元测试的时候，如果我们突然加了一个入参，整个测试是不是会变化很大？需要多额外测试很多种情况，作为一个写完需求首先习惯本地测试的程序员，你应该了解，添加一个参数相当于多了很多种情况。 好，对上面的代码喷了那么久，我又想到了，或者知道了什么妙计呢？ 3.引入DP1）隐性的概念显性化电话号仅仅是用户的一个参数，属于隐形概念，但实际上电话号的区号才是真正的业务逻辑，而我们需要将电话号的概念显性化，通过写一个Value Object： 1234567891011121314151617181920212223242526272829303132333435public class PhoneNumber&#123; private final String number; public String getNumber()&#123; return number; &#125; public PhoneNumber(String number)&#123; if(number == null)&#123; throw new Exception(&quot;number is empty!&quot;); &#125;else if(isValid(number))&#123; throw new Exception(&quot;number format is error&quot;) &#125; &#125; public String getAreaCode() &#123; for (int i = 0; i &amp;lt; number.length(); i++) &#123; String prefix = number.substring(0, i); if (isAreaCode(prefix)) &#123; return prefix; &#125; &#125; return null; &#125; private static boolean isAreaCode(String prefix) &#123; String[] areas = new String[]&#123;&quot;0571&quot;, &quot;021&quot;, &quot;010&quot;&#125;; return Arrays.asList(areas).contains(prefix); &#125; public static boolean isValid(String number) &#123; String pattern = &quot;^0?[1-9]&#123;2,3&#125;-?\\\\d&#123;8&#125;$&quot;; return number.matches(pattern); &#125;&#125; 这里面很重要的几点： 通过private final String number 确保PhoneNumber是一个 Value Object； 校验逻辑都放在了constructor里面，确保只要是phoneNumber类被创建出来，一定是校验通过的。 之前的 findAreaCode 方法变成了 PhoneNumber 类里的 getAreaCode ，突出了 areaCode 是 PhoneNumber 的一个计算属性。 Type 指我们在今后的代码里可以通过 PhoneNumber 去显性的标识电话号这个概念 。 Class 指我们可以把所有跟电话号相关的逻辑完整的收集到一个文件里。 2）使用DP之后的效果123456789101112131415161718192021public class User&#123; UserId userId; Name name; PhoneNumber phone; Address address; RepId repId;&#125;public User register(Name name,PhoneNumber phone,Address address)&#123; //根据手机号查找业务员 SalesRep rep=salesRepRepository.findRep(phone.getAreaCode); User user=new User(); user.name=name; user.phone=phone; user.address = address; if(rep!=null)&#123; user.repId=rep.repId; &#125; return userRepo.saveUser(user);&#125; 在使用了 DP 之后，所有的数据验证逻辑和非业务流程的逻辑都消失了，剩下都是核心业务逻辑，可以一目了然。 3）让隐性的上下文显性化需求：转账 A给B转账x元。 123public void pay(BigDecimal money, Long recipientId) &#123; BankService.transfer(money, &quot;CNY&quot;, recipientId);&#125; 如果是国内转账，此处毫无问题，但是如果是跨境呢？ 此处抽象一个Money，来做支付功能。 12345678910111213public class Money &#123; private BigDecimal amount; private Currency currency; public Money(BigDecimal amount, Currency currency) &#123; this.amount = amount; this.currency = currency; &#125;&#125;public void pay(Money money,Long recipientId)&#123; BankService.transfer(money,recipientId);&#125; 通过将默认货币这个隐性的上下文概念显性化，并且和金额合并为 Money ，我们可以避免很多当前看不出来，但未来可能会暴雷的bug。 4)封装多对象行为需求：跨境转账 12345678910public void pay(Money money, Currency targetCurrency, Long recipientId) &#123; if (money.getCurrency().equals(targetCurrency)) &#123; BankService.transfer(money, recipientId); &#125; else &#123; BigDecimal rate = ExchangeService.getRate(money.getCurrency(), targetCurrency); BigDecimal targetAmount = money.getAmount().multiply(new BigDecimal(rate)); Money targetMoney = new Money(targetAmount, targetCurrency); BankService.transfer(targetMoney, recipientId); &#125;&#125; 在这个case里，由于 targetCurrency 不一定和 money 的 Curreny 一致，需要调用一个服务去取汇率，然后做计算。最后用计算后的结果做转账。 这个case最大的问题在于，金额的计算被包含在了支付的服务中，涉及到的对象也有2个 Currency ，2 个 Money ，1 个 BigDecimal ，总共 5 个对象。这种涉及到多个对象的业务逻辑，需要用 DP 包装掉. 在这个 case 里，可以将转换汇率的功能，封装到一个叫做 ExchangeRate 的 DP 里： 12345678910111213141516171819@Valuepublic class ExchangeRate &#123; private BigDecimal rate; private Currency from; private Currency to; public ExchangeRate(BigDecimal rate, Currency from, Currency to) &#123; this.rate = rate; this.from = from; this.to = to; &#125; public Money exchange(Money fromMoney) &#123; notNull(fromMoney); isTrue(this.from.equals(fromMoney.getCurrency())); BigDecimal targetAmount = fromMoney.getAmount().multiply(rate); return new Money(targetAmount, to); &#125;&#125; ExchangeRate 汇率对象，通过封装金额计算逻辑以及各种校验逻辑，让原始代码变得极其简单： 12345public void pay(Money money, Currency targetCurrency, Long recipientId) &#123; ExchangeRate rate = ExchangeService.getRate(money.getCurrency(), targetCurrency); Money targetMoney = rate.exchange(money); BankService.transfer(targetMoney, recipientId);&#125; 在DDD里面，DP可以说是一切模型，方法，架构的基础。 总结：Domain Primitive 是一个在特定领域里，拥有精准定义的、可自我验证的、拥有行为的 Value Object 。 特点： 隐性的概念显性化 让隐性的上下文显性化 封装多对象行为 二，GET到了，想操作一下？老应用重构流程 在新应用中使用 DP 是比较简单的，但在老应用中使用 DP 是可以遵循以下流程按部就班的升级。 1.创建DP，收集所有DP行为在真实的项目中，以前散落在各个服务或工具类里面的代码，可以都抽出来放在 DP 里，成为 DP 自己的行为或属性。这里面的原则是：所有抽离出来的方法要做到无状态，比如原来是 static 的方法。如果原来的方法有状态变更，需要将改变状态的部分和不改状态的部分分离，然后将无状态的部分融入 DP 。因为 DP 本身不能带状态，所以一切需要改变状态的代码都不属于 DP 的范畴。 2.替换数据校验和无状态逻辑为了保障现有方法的兼容性，在第二步不会去修改接口的签名，而是通过代码替换原有的校验逻辑和根 DP 相关的业务逻辑。 12345678910public User register(String name, String phone, String address) throws ValidationException &#123; Name _name = new Name(name); PhoneNumber _phone = new PhoneNumber(phone); Address _address = new Address(address); SalesRep rep = salesRepRepo.findRep(_phone.getAreaCode()); // 其他代码...&#125; 3.创建新接口创建新接口，将DP的代码提升到接口参数层： 123public User register(Name name, PhoneNumber phone, Address address) &#123; SalesRep rep = salesRepRepo.findRep(phone.getAreaCode());&#125; 4.修改外部调用1service.register(new Name(&quot;尹会东&quot;), new PhoneNumber(&quot;0571-12345678&quot;), new Address(&quot;北京市昌平区天通苑本五区&quot;));","categories":[{"name":"领域驱动设计","slug":"领域驱动设计","permalink":"https://yinhuidong.github.io/categories/%E9%A2%86%E5%9F%9F%E9%A9%B1%E5%8A%A8%E8%AE%BE%E8%AE%A1/"}],"tags":[{"name":"DDD","slug":"DDD","permalink":"https://yinhuidong.github.io/tags/DDD/"}]},{"title":"DDD之Repository模式","slug":"领域驱动设计/DDD之Repository模式","date":"2022-01-12T00:19:55.253Z","updated":"2022-01-12T00:19:55.253Z","comments":true,"path":"2022/01/12/领域驱动设计/DDD之Repository模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/12/%E9%A2%86%E5%9F%9F%E9%A9%B1%E5%8A%A8%E8%AE%BE%E8%AE%A1/DDD%E4%B9%8BRepository%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"一，传统架构下的实体模型传统的应用架构，几乎就是根据需求设计数据库的表，根据表建立实体，对应着实体的就是DAO，Service，Controller，也就是传统的MVC三层架构。 回顾下我们平时写的代码，里面有着很多的xxxUtils工具类，很多的参数校验逻辑与业务逻辑混杂在一起，很多的实体类直接与数据库进行一对一映射。 好处很明显，在业务初期，开发起来很容易，相对比较简单，流水线式编码，但是，一旦后期需求变更，业务改造，数据库表发生变化，可能给我们带来毁灭性的负担。所谓牵一发而动全身，前面欠下了技术债，后面很难受，想补救工作量巨大，不补救，系统难以升级，难以扩展，灵活性急剧下降。 对于第三方（包括但不限于数据库）的强依赖，导致我们在做业务扩展的时候，顾虑重重，缺少了一往无前的动力。 但是截至目前，包括我所参与开发的项目，依然是采用这种模式，为什么？ 数据库思维：从有了数据库的那一天起，开发人员的思考方式就逐渐从“写业务逻辑“转变为了”写数据库逻辑”，也就是我们经常说的在写CRUD代码。 所谓简单：贫血模型的优势在于“简单”，仅仅是对数据库表的字段映射，所以可以从前到后用统一格式串通。这里简单打了引号，是因为它只是表面上的简单，实际上当未来有模型变更时，你会发现其实并不简单，每次变更都是非常复杂的事情。 脚本思维：很多常见的代码都属于“脚本”或“胶水代码”，也就是流程式代码。脚本代码的好处就是比较容易理解，但长久来看缺乏健壮性，维护成本会越来越高。 两个概念，你是否明确？ 数据模型：也就是和数据库一一映射的类 业务模型/领域模型：业务逻辑中，相关连的数据如何联动 真实代码结构中，Data Model和 Domain Model实际上会分别在不同的层里，Data Model只存在于数据层，而Domain Model在领域层，而链接了这两层的关键对象，就是Repository。 二，Repository的作用在传统的MVC三层架构中，我们操作数据库的层，一般叫做DAO，或者Mapper层。 由于他与数据库直接耦合，导致了强依赖性。更可怕的是，由于我们都是在Service层直接注入Mapper层，导致了这种强依赖的传递，也就是整个应用体系开始变得越加依赖数据库DB。 举一个例子 1234567891011121314public Interface UserDao&#123; public List&lt;User&gt; selectUserByIds(List&lt;Integer&gt; ids);&#125;public class UserService&#123; @Resource private UserDao userDao; public List&lt;User&gt; getUserList(List&lt;Integer&gt; ids)&#123; return userDao.selectUserByIds(ids); &#125; &#125; 这个代码，咋一看，简单明了，没有任何问题。但是假如现在由于数据量的增长和访问数量的增加，我需要引入缓存的逻辑，假如有十个地方调用了这个DAO中的方法，我需要在这十个地方都修改成： 1234567891011121314151617181920public class UserService&#123; @Resource private UserDao userDao; @Resource private RedisTemplate redisTemplate; public List&lt;User&gt; getUserList(List&lt;Integer&gt; ids)&#123; List&lt;User&gt; users=redisTemplate.opsForValue().get(key); if(users!=null)&#123; return users; &#125;else&#123; List&lt;User&gt; userList=userDao.selectUserByIds(ids); redisTemplate.opsForValue().set(key,userList); return userList; &#125; &#125;&#125; 所以，需要一个逻辑，能够隔离业务逻辑与DB之间的传递强耦合关系，让我们的应用更加灵活，健壮，这个就是Repository的价值。 三，模型对象代码规范1.什么是DO,DTO,Entity？ Data Object：DO的字段类型和名称应该和数据库物理表格的字段类型和名称一一对应，这样我们不需要去跑到数据库上去查一个字段的类型和名称。 Entity：实体对象是我们正常业务应该用的业务模型，它的字段和方法应该和业务语言保持一致，和持久化方式无关。也就是说，Entity和DO很可能有着完全不一样的字段命名和字段类型，甚至嵌套关系。Entity的生命周期应该仅存在于内存中，不需要可序列化和可持久化。 DTO：主要作为Application层的入参和出参，比如CQRS里的Command、Query、Event，以及Request、Response等都属于DTO的范畴。DTO的价值在于适配不同的业务场景的入参和出参，避免让业务对象变成一个万能大对象。 2.对象之间的关系在实际开发中DO、Entity和DTO不一定是1:1:1的关系。一些常见的非1:1关系如下： 复杂的实体拆分成多张数据库的表：常见的原因，字段多，查询性能差，需要将非检索、大字段等单独存为一张表，提升基础信息表的检索效率。 当然，除了一些数据查询频繁，聚合性非常强的表。 拆分的实体：我接触过的，订单，商品，购物车。 3.模型所在模块和转化器由于现在从一个对象变为3+个对象，对象间需要通过转化器（Converter/Mapper）来互相转化。而这三种对象在代码中所在的位置也不一样，简单总结如下： DTO Assembler：在Application层，Entity到DTO的转化器有一个标准的名称叫DTO Assembler。Martin Fowler在P of EAA一书里对于DTO 和 Assembler的描述：Data Transfer Object。DTO Assembler的核心作用就是将1个或多个相关联的Entity转化为1个或多个DTO。 Data Converter：在Infrastructure层，Entity到DO的转化器没有一个标准名称，但是为了区分Data Mapper，我们叫这种转化器Data Converter。这里要注意Data Mapper通常情况下指的是DAO，比如Mybatis的Mapper。Data Mapper的出处也在P of EAA一书里：Data Mapper 如果是手写一个Assembler，通常我们会去实现2种类型的方法，如下；Data Converter的逻辑和此类似，略过。 12345678910111213141516171819public class DtoAssembler &#123; // 通过各种实体，生成DTO public OrderDTO toDTO(Order order, Item item) &#123; OrderDTO dto = new OrderDTO(); dto.setId(order.getId()); dto.setItemTitle(item.getTitle()); // 从多个对象里取值，且字段名称不一样 dto.setDetailAddress(order.getAddress.getDetail()); // 可以读取复杂嵌套字段 // 省略N行 return dto; &#125; // 通过DTO，生成实体 public Item toEntity(ItemDTO itemDTO) &#123; Item entity = new Item(); entity.setId(itemDTO.getId()); // 省略N行 return entity; &#125;&#125; 我们能看出来通过抽象出一个Assembler/Converter对象，我们能把复杂的转化逻辑都收敛到一个对象中，并且可以很好的单元测试。这个也很好的收敛了常见代码里的转化逻辑。 在调用方使用时是非常方便的: 123456789101112public class Application &#123; private DtoAssembler assembler; private OrderRepository orderRepository; private ItemRepository itemRepository; public OrderDTO getOrderDetail(Long orderId) &#123; Order order = orderRepository.find(orderId); Item item = itemRepository.find(order.getItemId()); return assembler.toDTO(order, item); // 原来的很多复杂转化逻辑都收敛到一行代码了 &#125;&#125; 4.模型规范总结 从使用复杂度角度来看，区分了DO、Entity、DTO带来了代码量的膨胀（从1个变成了3+2+N个）。但是在实际复杂业务场景下，通过功能来区分模型带来的价值是功能性的单一和可测试、可预期，最终反而是逻辑复杂性的降低。 四，Repository代码规范1.接口规范 接口名称不应该使用底层实现的语法：我们常见的insert、select、update、delete都属于SQL语法，使用这几个词相当于和DB底层实现做了绑定。相反，我们应该把 Repository 当成一个中性的类 似Collection 的接口，使用语法如 find、save、remove。在这里特别需要指出的是区分 insert/add 和 update 本身也是一种和底层强绑定的逻辑，一些储存如缓存实际上不存在insert和update的差异，在这个 case 里，使用中性的 save 接口，然后在具体实现上根据情况调用 DAO 的 insert 或 update 接口。 出参入参不应该使用底层数据格式：需要记得的是 Repository 操作的是 Entity 对象（实际上应该是Aggregate Root），而不应该直接操作底层的 DO 。更近一步，Repository 接口实际上应该存在于Domain层，根本看不到 DO 的实现。这个也是为了避免底层实现逻辑渗透到业务代码中的强保障。 应该避免所谓的“通用”Repository模式：很多 ORM 框架都提供一个“通用”的Repository接口，然后框架通过注解自动实现接口，比较典型的例子是Spring Data、Entity Framework等，这种框架的好处是在简单场景下很容易通过配置实现，但是坏处是基本上无扩展的可能性（比如加定制缓存逻辑），在未来有可能还是会被推翻重做。当然，这里避免通用不代表不能有基础接口和通用的帮助类。 先定义一个基础的 Repository 基础接口类，以及一些Marker接口类： 123456789101112131415161718192021222324252627282930313233343536373839public interface Repository&lt;T extends Aggregate&lt;ID&gt;, ID extends Identifier&gt; &#123; /** * 将一个Aggregate附属到一个Repository，让它变为可追踪。 * Change-Tracking在下文会讲，非必须 */ void attach(@NotNull T aggregate); /** * 解除一个Aggregate的追踪 * Change-Tracking在下文会讲，非必须 */ void detach(@NotNull T aggregate); /** * 通过ID寻找Aggregate。 * 找到的Aggregate自动是可追踪的 */ T find(@NotNull ID id); /** * 将一个Aggregate从Repository移除 * 操作后的aggregate对象自动取消追踪 */ void remove(@NotNull T aggregate); /** * 保存一个Aggregate * 保存后自动重置追踪条件 */ void save(@NotNull T aggregate);&#125;// 聚合根的Marker接口public interface Aggregate&lt;ID extends Identifier&gt; extends Entity&lt;ID&gt; &#123;&#125;// 实体类的Marker接口public interface Entity&lt;ID extends Identifier&gt; extends Identifiable&lt;ID&gt; &#123;&#125;public interface Identifiable&lt;ID extends Identifier&gt; &#123; ID getId();&#125;// ID类型DP的Marker接口public interface Identifier extends Serializable &#123;&#125; 业务自己的接口只需要在基础接口上进行扩展，举个订单的例子： 123456789// 代码在Domain层public interface OrderRepository extends Repository&lt;Order, OrderId&gt; &#123; // 自定义Count接口，在这里OrderQuery是一个自定义的DTO Long count(OrderQuery query); // 自定义分页查询接口 Page&lt;Order&gt; query(OrderQuery query); // 自定义有多个条件的查询接口 Order findInStore(OrderId id, StoreId storeId);&#125; 每个业务需要根据自己的业务场景来定义各种查询逻辑。 这里需要再次强调的是Repository的接口是在Domain层，但是实现类是在Infrastructure层。 2.Repository基础实现先举个Repository的最简单实现的例子。注意OrderRepositoryImpl在Infrastructure层： 12345678910111213141516171819202122232425262728293031323334353637383940414243// 代码在Infrastructure层@Repository // Spring的注解public class OrderRepositoryImpl implements OrderRepository &#123; private final OrderDAO dao; // 具体的DAO接口 private final OrderDataConverter converter; // 转化器 public OrderRepositoryImpl(OrderDAO dao) &#123; this.dao = dao; this.converter = OrderDataConverter.INSTANCE; &#125; @Override public Order find(OrderId orderId) &#123; OrderDO orderDO = dao.findById(orderId.getValue()); return converter.fromData(orderDO); &#125; @Override public void remove(Order aggregate) &#123; OrderDO orderDO = converter.toData(aggregate); dao.delete(orderDO); &#125; @Override public void save(Order aggregate) &#123; if (aggregate.getId() != null &amp;&amp; aggregate.getId().getValue() &gt; 0) &#123; // update OrderDO orderDO = converter.toData(aggregate); dao.update(orderDO); &#125; else &#123; // insert OrderDO orderDO = converter.toData(aggregate); dao.insert(orderDO); aggregate.setId(converter.fromData(orderDO).getId()); &#125; &#125; @Override public Page&lt;Order&gt; query(OrderQuery query) &#123; List&lt;OrderDO&gt; orderDOS = dao.queryPaged(query); long count = dao.count(query); List&lt;Order&gt; result = orderDOS.stream().map(converter::fromData).collect(Collectors.toList()); return Page.with(result, query, count); &#125; @Override public Order findInStore(OrderId id, StoreId storeId) &#123; OrderDO orderDO = dao.findInStore(id.getValue(), storeId.getValue()); return converter.fromData(orderDO); &#125;&#125; 从上面的实现能看出来一些套路：所有的Entity/Aggregate会被转化为DO，然后根据业务场景，调用相应的DAO方法进行操作，事后如果需要则把DO转换回Entity。代码基本很简单，唯一需要注意的是save方法，需要根据Aggregate的ID是否存在且大于0来判断一个Aggregate是否需要更新还是插入。 3.Repository复杂实现针对单一Entity的Repository实现一般比较简单，但是当涉及到多Entity的Aggregate Root时，就会比较麻烦，最主要的原因是在一次操作中，并不是所有Aggregate里的Entity都需要变更，但是如果用简单的写法，会导致大量的无用DB操作。 举一个常见的例子，在主子订单的场景下，一个主订单Order会包含多个子订单LineItem，假设有个改某个子订单价格的操作，会同时改变主订单价格，但是对其他子订单无影响： 如果用一个非常naive的实现来完成，会导致多出来两个无用的更新操作，如下： 1234567891011121314151617181920212223242526272829public class OrderRepositoryImpl extends implements OrderRepository &#123; private OrderDAO orderDAO; private LineItemDAO lineItemDAO; private OrderDataConverter orderConverter; private LineItemDataConverter lineItemConverter; // 其他逻辑省略 @Override public void save(Order aggregate) &#123; if (aggregate.getId() != null &amp;&amp; aggregate.getId().getValue() &gt; 0) &#123; // 每次都将Order和所有LineItem全量更新 OrderDO orderDO = orderConverter.toData(aggregate); orderDAO.update(orderDO); for (LineItem lineItem: aggregate.getLineItems()) &#123; save(lineItem); &#125; &#125; else &#123; // 插入逻辑省略 &#125; &#125; private void save(LineItem lineItem) &#123; if (lineItem.getId() != null &amp;&amp; lineItem.getId().getValue() &gt; 0) &#123; LineItemDO lineItemDO = lineItemConverter.toData(lineItem); lineItemDAO.update(lineItemDO); &#125; else &#123; LineItemDO lineItemDO = lineItemConverter.toData(lineItem); lineItemDAO.insert(lineItemDO); lineItem.setId(lineItemConverter.fromData(lineItemDO).getId()); &#125; &#125;&#125; 在这个情况下，会导致4个UPDATE操作，但实际上只需要2个。在绝大部分情况下，这个成本不高，可以接受，但是在极端情况下（当非Aggregate Root的Entity非常多时），会导致大量的无用写操作。 五，Repository迁移路径在我们日常的代码中，使用Repository模式是一个很简单，但是又能得到很多收益的事情。最大的收益就是可以彻底和底层实现解耦，让上层业务可以快速自发展。 我们假设现有的传统代码包含了以下几个类（还是用订单举例）： OrderDO OrderDAO 可以通过以下几个步骤逐渐的实现Repository模式： 生成Order实体类，初期字段可以和OrderDO保持一致 生成OrderDataConverter，通过MapStruct基本上2行代码就能完成 写单元测试，确保Order和OrderDO之间的转化100%正确 生成OrderRepository接口和实现，通过单测确保OrderRepository的正确性 将原有代码里使用了OrderDO的地方改为Order 将原有代码里使用了OrderDAO的地方都改为用OrderRepository 通过单测确保业务逻辑的一致性。","categories":[{"name":"领域驱动设计","slug":"领域驱动设计","permalink":"https://yinhuidong.github.io/categories/%E9%A2%86%E5%9F%9F%E9%A9%B1%E5%8A%A8%E8%AE%BE%E8%AE%A1/"}],"tags":[{"name":"DDD","slug":"DDD","permalink":"https://yinhuidong.github.io/tags/DDD/"}]},{"title":"DDD之应用架构","slug":"领域驱动设计/DDD之应用架构","date":"2022-01-12T00:19:55.253Z","updated":"2022-01-12T00:19:55.254Z","comments":true,"path":"2022/01/12/领域驱动设计/DDD之应用架构/","link":"","permalink":"https://yinhuidong.github.io/2022/01/12/%E9%A2%86%E5%9F%9F%E9%A9%B1%E5%8A%A8%E8%AE%BE%E8%AE%A1/DDD%E4%B9%8B%E5%BA%94%E7%94%A8%E6%9E%B6%E6%9E%84/","excerpt":"","text":"一，前言之所以写这些文章，很大程度上，是因为阅读了阿里技术专家的文章，读完之后，对我内心触动很大，文章多出引用了阿里技术文章的内容，仅作为个人学习用途，也是作为技术人，希望技术可以被更多人学习到。 作为一个实习生，谈架构，未免让人感觉，好高骛远，以下陈述只属于个人浅薄意见。 依个人浅薄意见，架构的本质就是应用的拆分和聚合。 拆分也就是应用微服务化，聚合，并不是指将多个微服务聚合成一个应用，而是指聚合多个微服务的功能，让他完成一个大的功能。这样做的灵活性，可扩展性就会更高。 所谓应用架构，个人理解，更可以指的是应用中固定不变的代码结构，设计模式，规范和组件间的通信。 一个好的应用架构，通过规定一套规范，可以让团队内能力参差不齐的开发人员更好的共同开发，降低开发成本，提升开发效率和代码质量。 要求，或者原则？ 独立于框架：架构不应该依赖某个外部的库或框架，不应该被框架的结构所束缚。 独立于前端：前台展示的样式可能会随时发生变化（今天可能是网页、明天可能变成console、后天是独立app），但是底层架构不应该随之而变化。 独立于底层数据源：无论今天你用MySQL、Oracle还是MongoDB、CouchDB，甚至使用文件系统，软件架构不应该因为不同的底层数据储存方式而产生巨大改变。 独立于外部依赖：无论外部依赖如何变更、升级，业务的核心逻辑不应该随之而大幅变化。 测试覆盖率：无论外部依赖了什么数据库、硬件、UI或者服务，业务的逻辑应该都能够快速被验证正确性。 通过一个需求，来论证。 用户可以通过银行网页转账给另一个账号，支持跨币种转账。同时因为监管和对账需求，需要记录本次转账活动。 二，传统互联网架构下的架构设计1、从MySql数据库中找到转出和转入的账户，选择用 MyBatis 的 mapper 实现 DAO； 2、从 Yahoo（或其他渠道）提供的汇率服务获取转账的汇率信息（底层是 http 开放接口）； 3、计算需要转出的金额，确保账户有足够余额，并且没超出每日转账上限； 4、实现转入和转出操作，扣除手续费，保存数据库； 5、发送 Kafka 审计消息，以便审计和对账用； 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public class TransferController &#123; private TransferService transferService; public Result&lt;Boolean&gt; transfer(String targetAccountNumber, BigDecimal amount, HttpSession session) &#123; Long userId = (Long) session.getAttribute(&quot;userId&quot;); return transferService.transfer(userId, targetAccountNumber, amount, &quot;CNY&quot;); &#125;&#125; public class TransferServiceImpl implements TransferService &#123; private static final String TOPIC_AUDIT_LOG = &quot;TOPIC_AUDIT_LOG&quot;; private AccountMapper accountDAO; private KafkaTemplate&lt;String, String&gt; kafkaTemplate; private YahooForexService yahooForex; @Override public Result&lt;Boolean&gt; transfer(Long sourceUserId, String targetAccountNumber, BigDecimal targetAmount, String targetCurrency) &#123; // 1. 从数据库读取数据，忽略所有校验逻辑如账号是否存在等 AccountDO sourceAccountDO = accountDAO.selectByUserId(sourceUserId); AccountDO targetAccountDO = accountDAO.selectByAccountNumber(targetAccountNumber); // 2. 业务参数校验 if (!targetAccountDO.getCurrency().equals(targetCurrency)) &#123; throw new InvalidCurrencyException(); &#125; // 3. 获取外部数据，并且包含一定的业务逻辑 // exchange rate = 1 source currency = X target currency BigDecimal exchangeRate = BigDecimal.ONE; if (sourceAccountDO.getCurrency().equals(targetCurrency)) &#123; exchangeRate = yahooForex.getExchangeRate(sourceAccountDO.getCurrency(), targetCurrency); &#125; BigDecimal sourceAmount = targetAmount.divide(exchangeRate, RoundingMode.DOWN); // 4. 业务参数校验 if (sourceAccountDO.getAvailable().compareTo(sourceAmount) &lt; 0) &#123; throw new InsufficientFundsException(); &#125; if (sourceAccountDO.getDailyLimit().compareTo(sourceAmount) &lt; 0) &#123; throw new DailyLimitExceededException(); &#125; // 5. 计算新值，并且更新字段 BigDecimal newSource = sourceAccountDO.getAvailable().subtract(sourceAmount); BigDecimal newTarget = targetAccountDO.getAvailable().add(targetAmount); sourceAccountDO.setAvailable(newSource); targetAccountDO.setAvailable(newTarget); // 6. 更新到数据库 accountDAO.update(sourceAccountDO); accountDAO.update(targetAccountDO); // 7. 发送审计消息 String message = sourceUserId + &quot;,&quot; + targetAccountNumber + &quot;,&quot; + targetAmount + &quot;,&quot; + targetCurrency; kafkaTemplate.send(TOPIC_AUDIT_LOG, message); return Result.success(true); &#125; &#125; 一段业务代码里经常包含了参数校验、数据读取存储、业务计算、调用外部服务、发送消息等多种逻辑。在这个案例里虽然是写在了同一个方法里，在真实代码中经常会被拆分成多个子方法，但实际效果是一样的，而在我们日常的工作中，绝大部分代码都或多或少的接近于此类结构。在Martin Fowler的 P of EAA书中，这种很常见的代码样式被叫做Transaction Script（事务脚本）。虽然这种类似于脚本的写法在功能上没有什么问题，但是长久来看，他有以下几个很大的问题：可维护性差、可扩展性差、可测试性差。 1.可维护性差一个应用最大的成本一般都不是来自于开发阶段，而是应用整个生命周期的总维护成本，所以代码的可维护性代表了最终成本。 可维护性 = 当依赖变化时，有多少代码需要随之改变。 数据结构的不稳定性：AccountDO类是一个纯数据结构，映射了数据库中的一个表。这里的问题是数据库的表结构和设计是应用的外部依赖，长远来看都有可能会改变，比如数据库要做Sharding，或者换一个表设计，或者改变字段名。 依赖库的升级：AccountMapper依赖MyBatis的实现，如果MyBatis未来升级版本，可能会造成用法的不同（可以参考iBatis升级到基于注解的MyBatis的迁移成本）。同样的，如果未来换一个ORM体系，迁移成本也是巨大的。 第三方服务依赖的不确定性：第三方服务，比如Yahoo的汇率服务未来很有可能会有变化：轻则API签名变化，重则服务不可用需要寻找其他可替代的服务。在这些情况下改造和迁移成本都是巨大的。同时，外部依赖的兜底、限流、熔断等方案都需要随之改变。 第三方服务API的接口变化：YahooForexService.getExchangeRate返回的结果是小数点还是百分比？入参是（source, target）还是（target, source）？谁能保证未来接口不会改变？如果改变了，核心的金额计算逻辑必须跟着改，否则会造成资损。 中间件更换：今天我们用Kafka发消息，明天如果要上阿里云用RocketMQ该怎么办？后天如果消息的序列化方式从String改为Binary该怎么办？如果需要消息分片该怎么改？ 案例里的代码对于任何外部依赖的改变都会有比较大的影响。如果你的应用里有大量的此类代码，你每一天的时间基本上会被各种库升级、依赖服务升级、中间件升级、jar包冲突占满，最终这个应用变成了一个不敢升级、不敢部署、不敢写新功能、并且随时会爆发的炸弹，终有一天会给你带来惊喜。 2.可扩展性差事务脚本式代码的第二大缺陷是：虽然写单个用例的代码非常高效简单，但是当用例多起来时，其扩展性会变得越来越差。 可扩展性 = 做新需求或改逻辑时，需要新增/修改多少代码。 如果今天需要增加一个跨行转账的能力，你会发现基本上需要重新开发，基本上没有任何的可复用性。 数据来源被固定、数据格式不兼容：原有的AccountDO是从本地获取的，而跨行转账的数据可能需要从一个第三方服务获取，而服务之间数据格式不太可能是兼容的，导致从数据校验、数据读写、到异常处理、金额计算等逻辑都要重写。 业务逻辑无法复用：数据格式不兼容的问题会导致核心业务逻辑无法复用。每个用例都是特殊逻辑的后果是最终会造成大量的if-else语句，而这种分支多的逻辑会让分析代码非常困难，容易错过边界情况，造成bug。 逻辑和数据存储的相互依赖：当业务逻辑增加变得越来越复杂时，新加入的逻辑很有可能需要对数据库schema或消息格式做变更。而变更了数据格式后会导致原有的其他逻辑需要一起跟着动。在最极端的场景下，一个新功能的增加会导致所有原有功能的重构，成本巨大。 在事务脚本式的架构下，一般做第一个需求都非常的快，但是做第N个需求时需要的时间很有可能是呈指数级上升的，绝大部分时间花费在老功能的重构和兼容上，最终你的创新速度会跌为0，促使老应用被推翻重构。 3.可测试性能差除了部分工具类、框架类和中间件类的代码有比较高的测试覆盖之外，我们在日常工作中很难看到业务代码有比较好的测试覆盖，而绝大部分的上线前的测试属于人肉的“集成测试”。低测试率导致我们对代码质量很难有把控，容易错过边界条件，异常case只有线上爆发了才被动发现。而低测试覆盖率的主要原因是业务代码的可测试性比较差。 可测试性 = 运行每个测试用例所花费的时间 * 每个需求所需要增加的测试用例数量 设施搭建困难：当代码中强依赖了数据库、第三方服务、中间件等外部依赖之后，想要完整跑通一个测试用例需要确保所有依赖都能跑起来，这个在项目早期是及其困难的。在项目后期也会由于各种系统的不稳定性而导致测试无法通过。 运行耗时长：大多数的外部依赖调用都是I/O密集型，如跨网络调用、磁盘调用等，而这种I/O调用在测试时需要耗时很久。另一个经常依赖的是笨重的框架如Spring，启动Spring容器通常需要很久。当一个测试用例需要花超过10秒钟才能跑通时，绝大部分开发都不会很频繁的测试。 耦合度高：假如一段脚本中有A、B、C三个子步骤，而每个步骤有N个可能的状态，当多个子步骤耦合度高时，为了完整覆盖所有用例，最多需要有N _ N _ N个测试用例。当耦合的子步骤越多时，需要的测试用例呈指数级增长。 在事务脚本模式下，当测试用例复杂度远大于真实代码复杂度，当运行测试用例的耗时超出人肉测试时，绝大部分人会选择不写完整的测试覆盖，而这种情况通常就是bug很难被早点发现的原因。 4.总结以上的代码违背了至少以下几个软件设计的原则： 单一性原则（Single Responsibility Principle）：单一性原则要求一个对象/类应该只有一个变更的原因。但是在这个案例里，代码可能会因为任意一个外部依赖或计算逻辑的改变而改变。 依赖反转原则（Dependency Inversion Principle）：依赖反转原则要求在代码中依赖抽象，而不是具体的实现。在这个案例里外部依赖都是具体的实现，比如YahooForexService虽然是一个接口类，但是它对应的是依赖了Yahoo提供的具体服务，所以也算是依赖了实现。同样的KafkaTemplate、MyBatis的DAO实现都属于具体实现。 开放封闭原则（Open Closed Principle）：开放封闭原则指开放扩展，但是封闭修改。在这个案例里的金额计算属于可能会被修改的代码，这个时候该逻辑应该需要被包装成为不可修改的计算类，新功能通过计算类的拓展实现。 三，基于DDD重构 这是一个传统的三层分层结构：UI层、业务层、和基础设施层。上层对于下层有直接的依赖关系，导致耦合度过高。在业务层中对于下层的基础设施有强依赖，耦合度高。我们需要对这张图上的每个节点做抽象和整理，来降低对外部依赖的耦合度。 1.抽象数据存储层将Data Access层做抽象，降低系统对数据库的直接依赖。 新建Account实体对象：一个实体（Entity）是拥有ID的域对象，除了拥有数据之外，同时拥有行为。Entity和数据库储存格式无关，在设计中要以该领域的通用严谨语言（Ubiquitous Language）为依据。 新建对象储存接口类AccountRepository：Repository只负责Entity对象的存储和读取，而Repository的实现类完成数据库存储的细节。通过加入Repository接口，底层的数据库连接可以通过不同的实现类而替换。 12345678910111213141516@Datapublic class Account &#123; private AccountId id; private AccountNumber accountNumber; private UserId userId; private Money available; private Money dailyLimit; public void withdraw(Money money) &#123; // 转出 &#125; public void deposit(Money money) &#123; // 转入 &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445public interface AccountRepository &#123; Account find(AccountId id); Account find(AccountNumber accountNumber); Account find(UserId userId); Account save(Account account);&#125; public class AccountRepositoryImpl implements AccountRepository &#123; @Autowired private AccountMapper accountDAO; @Autowired private AccountBuilder accountBuilder; @Override public Account find(AccountId id) &#123; AccountDO accountDO = accountDAO.selectById(id.getValue()); return accountBuilder.toAccount(accountDO); &#125; @Override public Account find(AccountNumber accountNumber) &#123; AccountDO accountDO = accountDAO.selectByAccountNumber(accountNumber.getValue()); return accountBuilder.toAccount(accountDO); &#125; @Override public Account find(UserId userId) &#123; AccountDO accountDO = accountDAO.selectByUserId(userId.getId()); return accountBuilder.toAccount(accountDO); &#125; @Override public Account save(Account account) &#123; AccountDO accountDO = accountBuilder.fromAccount(account); if (accountDO.getId() == null) &#123; accountDAO.insert(accountDO); &#125; else &#123; accountDAO.update(accountDO); &#125; return accountBuilder.toAccount(accountDO); &#125; &#125; Account实体类和AccountDO数据类的对比如下： Data Object数据类：AccountDO是单纯的和数据库表的映射关系，每个字段对应数据库表的一个column，这种对象叫Data Object。DO只有数据，没有行为。AccountDO的作用是对数据库做快速映射，避免直接在代码里写SQL。无论你用的是MyBatis还是Hibernate这种ORM，从数据库来的都应该先直接映射到DO上，但是代码里应该完全避免直接操作 DO。 Entity实体类：Account 是基于领域逻辑的实体类，它的字段和数据库储存不需要有必然的联系。Entity包含数据，同时也应该包含行为。在 Account 里，字段也不仅仅是String等基础类型，而应该尽可能用上一讲的 Domain Primitive 代替，可以避免大量的校验代码。 DAO 和 Repository 类的对比如下： DAO对应的是一个特定的数据库类型的操作，相当于SQL的封装。所有操作的对象都是DO类，所有接口都可以根据数据库实现的不同而改变。比如，insert 和 update 属于数据库专属的操作。 Repository对应的是Entity对象读取储存的抽象，在接口层面做统一，不关注底层实现。比如，通过 save 保存一个Entity对象，但至于具体是 insert 还是 update 并不关心。Repository的具体实现类通过调用DAO来实现各种操作，通过Builder/Factory对象实现AccountDO 到 Account之间的转化 Repository和Entity 通过Account对象，避免了其他业务逻辑代码和数据库的直接耦合，避免了当数据库字段变化时，大量业务逻辑也跟着变的问题。 通过Repository，改变业务代码的思维方式，让业务逻辑不再面向数据库编程，而是面向领域模型编程。 Account属于一个完整的内存中对象，可以比较容易的做完整的测试覆盖，包含其行为。 Repository作为一个接口类，可以比较容易的实现Mock或Stub，可以很容易测试。 AccountRepositoryImpl实现类，由于其职责被单一出来，只需要关注Account到AccountDO的映射关系和Repository方法到DAO方法之间的映射关系，相对于来说更容易测试。 2.抽象第三方服务类似对于数据库的抽象，所有第三方服务也需要通过抽象解决第三方服务不可控，入参出参强耦合的问题。在这个例子里我们抽象出 ExchangeRateService 的服务，和一个ExchangeRate的Domain Primitive类： 1234567891011121314151617public interface ExchangeRateService &#123; ExchangeRate getExchangeRate(Currency source, Currency target);&#125; public class ExchangeRateServiceImpl implements ExchangeRateService &#123; @Autowired private YahooForexService yahooForexService; @Override public ExchangeRate getExchangeRate(Currency source, Currency target) &#123; if (source.equals(target)) &#123; return new ExchangeRate(BigDecimal.ONE, source, target); &#125; BigDecimal forex = yahooForexService.getExchangeRate(source.getValue(), target.getValue()); return new ExchangeRate(forex, source, target); &#125; 3.ACL防腐层很多时候我们的系统会去依赖其他的系统，而被依赖的系统可能包含不合理的数据结构、API、协议或技术实现，如果对外部系统强依赖，会导致我们的系统被”腐蚀“。这个时候，通过在系统间加入一个防腐层，能够有效的隔离外部依赖和内部逻辑，无论外部如何变更，内部代码可以尽可能的保持不变。 ACL 不仅仅只是多了一层调用，在实际开发中ACL能够提供更多强大的功能： 适配器：很多时候外部依赖的数据、接口和协议并不符合内部规范，通过适配器模式，可以将数据转化逻辑封装到ACL内部，降低对业务代码的侵入。在这个案例里，我们通过封装了ExchangeRate和Currency对象，转化了对方的入参和出参，让入参出参更符合我们的标准。 缓存：对于频繁调用且数据变更不频繁的外部依赖，通过在ACL里嵌入缓存逻辑，能够有效的降低对于外部依赖的请求压力。同时，很多时候缓存逻辑是写在业务代码里的，通过将缓存逻辑嵌入ACL，能够降低业务代码的复杂度。 兜底：如果外部依赖的稳定性较差，一个能够有效提升我们系统稳定性的策略是通过ACL起到兜底的作用，比如当外部依赖出问题后，返回最近一次成功的缓存或业务兜底数据。这种兜底逻辑一般都比较复杂，如果散落在核心业务代码中会很难维护，通过集中在ACL中，更加容易被测试和修改。 易于测试：类似于之前的Repository，ACL的接口类能够很容易的实现Mock或Stub，以便于单元测试。 功能开关：有些时候我们希望能在某些场景下开放或关闭某个接口的功能，或者让某个接口返回一个特定的值，我们可以在ACL配置功能开关来实现，而不会对真实业务代码造成影响。同时，使用功能开关也能让我们容易的实现Monkey测试，而不需要真正物理性的关闭外部依赖。 4.抽象中间件对各种中间件的抽象的目的是让业务代码不再依赖中间件的实现逻辑。因为中间件通常需要有通用型，中间件的接口通常是String或Byte[] 类型的，导致序列化/反序列化逻辑通常和业务逻辑混杂在一起，造成胶水代码。通过中间件的ACL抽象，减少重复胶水代码。 1234567891011121314151617181920212223242526272829303132333435363738@Value@AllArgsConstructorpublic class AuditMessage &#123; private UserId userId; private AccountNumber source; private AccountNumber target; private Money money; private Date date; public String serialize() &#123; return userId + &quot;,&quot; + source + &quot;,&quot; + target + &quot;,&quot; + money + &quot;,&quot; + date; &#125; public static AuditMessage deserialize(String value) &#123; // todo return null; &#125;&#125; public interface AuditMessageProducer &#123; SendResult send(AuditMessage message);&#125; public class AuditMessageProducerImpl implements AuditMessageProducer &#123; private static final String TOPIC_AUDIT_LOG = &quot;TOPIC_AUDIT_LOG&quot;; @Autowired private KafkaTemplate&lt;String, String&gt; kafkaTemplate; @Override public SendResult send(AuditMessage message) &#123; String messageBody = message.serialize(); kafkaTemplate.send(TOPIC_AUDIT_LOG, messageBody); return SendResult.success(); &#125;&#125; 5.封装业务逻辑在这个案例里，有很多业务逻辑是跟外部依赖的代码混合的，包括金额计算、账户余额的校验、转账限制、金额增减等。这种逻辑混淆导致了核心计算逻辑无法被有效的测试和复用。在这里，我们的解法是通过Entity、Domain Primitive和Domain Service封装所有的业务逻辑： 1）用DP封装跟实体无关的无状态计算逻辑使用ExchangeRate来封装汇率计算逻辑： 12345BigDecimal exchangeRate = BigDecimal.ONE;if (sourceAccountDO.getCurrency().equals(targetCurrency)) &#123; exchangeRate = yahooForex.getExchangeRate(sourceAccountDO.getCurrency(), targetCurrency);&#125;BigDecimal sourceAmount = targetAmount.divide(exchangeRate, RoundingMode.DOWN); 变为: 12ExchangeRate exchangeRate = exchangeRateService.getExchangeRate(sourceAccount.getCurrency(), targetMoney.getCurrency());Money sourceMoney = exchangeRate.exchangeTo(targetMoney); 2)用Entity封装单对象的有状态的行为，包括业务校验用Account实体类封装所有Account的行为，包括业务校验如下： 1234567891011121314151617181920212223242526272829303132@Datapublic class Account &#123; private AccountId id; private AccountNumber accountNumber; private UserId userId; private Money available; private Money dailyLimit; public Currency getCurrency() &#123; return this.available.getCurrency(); &#125; // 转入 public void deposit(Money money) &#123; if (!this.getCurrency().equals(money.getCurrency())) &#123; throw new InvalidCurrencyException(); &#125; this.available = this.available.add(money); &#125; // 转出 public void withdraw(Money money) &#123; if (this.available.compareTo(money) &lt; 0) &#123; throw new InsufficientFundsException(); &#125; if (this.dailyLimit.compareTo(money) &lt; 0) &#123; throw new DailyLimitExceededException(); &#125; this.available = this.available.subtract(money); &#125;&#125; 原有的业务代码则可以简化为： 12sourceAccount.deposit(sourceMoney);targetAccount.withdraw(targetMoney); 3)用Domain Service封装多对象逻辑在这个案例里，我们发现这两个账号的转出和转入实际上是一体的，也就是说这种行为应该被封装到一个对象中去。特别是考虑到未来这个逻辑可能会产生变化：比如增加一个扣手续费的逻辑。这个时候在原有的TransferService中做并不合适，在任何一个Entity或者Domain Primitive里也不合适，需要有一个新的类去包含跨域对象的行为。这种对象叫做Domain Service。 1234567891011121314public interface AccountTransferService &#123; void transfer(Account sourceAccount, Account targetAccount, Money targetMoney, ExchangeRate exchangeRate);&#125; public class AccountTransferServiceImpl implements AccountTransferService &#123; private ExchangeRateService exchangeRateService; @Override public void transfer(Account sourceAccount, Account targetAccount, Money targetMoney, ExchangeRate exchangeRate) &#123; Money sourceMoney = exchangeRate.exchangeTo(targetMoney); sourceAccount.deposit(sourceMoney); targetAccount.withdraw(targetMoney); &#125;&#125; 原始代码则简化为一行： 1accountTransferService.transfer(sourceAccount, targetAccount, targetMoney, exchangeRate); 6.重构后效果12345678910111213141516171819202122232425262728293031public class TransferServiceImplNew implements TransferService &#123; private AccountRepository accountRepository; private AuditMessageProducer auditMessageProducer; private ExchangeRateService exchangeRateService; private AccountTransferService accountTransferService; @Override public Result&lt;Boolean&gt; transfer(Long sourceUserId, String targetAccountNumber, BigDecimal targetAmount, String targetCurrency) &#123; // 参数校验 Money targetMoney = new Money(targetAmount, new Currency(targetCurrency)); // 读数据 Account sourceAccount = accountRepository.find(new UserId(sourceUserId)); Account targetAccount = accountRepository.find(new AccountNumber(targetAccountNumber)); ExchangeRate exchangeRate = exchangeRateService.getExchangeRate(sourceAccount.getCurrency(), targetMoney.getCurrency()); // 业务逻辑 accountTransferService.transfer(sourceAccount, targetAccount, targetMoney, exchangeRate); // 保存数据 accountRepository.save(sourceAccount); accountRepository.save(targetAccount); // 发送审计消息 AuditMessage message = new AuditMessage(sourceAccount, targetAccount, targetMoney); auditMessageProducer.send(message); return Result.success(true); &#125;&#125; 业务逻辑清晰，数据存储和业务逻辑完全分隔。 Entity、Domain Primitive、Domain Service都是独立的对象，没有任何外部依赖，但是却包含了所有核心业务逻辑，可以单独完整测试。 原有的TransferService不再包括任何计算逻辑，仅仅作为组件编排，所有逻辑均delegate到其他组件。这种仅包含Orchestration（编排）的服务叫做Application Service（应用服务）。 通过对外部依赖的抽象和内部逻辑的封装重构，应用整体的依赖关系变了： 最底层不再是数据库，而是Entity、Domain Primitive和Domain Service。这些对象不依赖任何外部服务和框架，而是纯内存中的数据和操作。这些对象我们打包为Domain Layer（领域层）。领域层没有任何外部依赖关系。 再其次的是负责组件编排的Application Service，但是这些服务仅仅依赖了一些抽象出来的ACL类和Repository类，而其具体实现类是通过依赖注入注进来的。Application Service、Repository、ACL等我们统称为Application Layer（应用层）。应用层 依赖 领域层，但不依赖具体实现。 最后是ACL，Repository等的具体实现，这些实现通常依赖外部具体的技术实现和框架，所以统称为Infrastructure Layer（基础设施层）。Web框架里的对象如Controller之类的通常也属于基础设施层。 写这段代码，考虑到最终的依赖关系，我们可能先写Domain层的业务逻辑，然后再写Application层的组件编排，最后才写每个外部依赖的具体实现。这种架构思路和代码组织结构就叫做Domain-Driven Design（领域驱动设计，或DDD）。所以DDD不是一个特殊的架构设计，而是所有Transction Script代码经过合理重构后一定会抵达的终点。 四，总结DDD不是一个什么特殊的架构，而是任何传统代码经过合理的重构之后最终一定会抵达的终点。DDD的架构能够有效的解决传统架构中的问题： 高可维护性：当外部依赖变更时，内部代码只用变更跟外部对接的模块，其他业务逻辑不变。 高可扩展性：做新功能时，绝大部分的代码都能复用，仅需要增加核心业务逻辑即可。 高可测试性：每个拆分出来的模块都符合单一性原则，绝大部分不依赖框架，可以快速的单元测试，做到100%覆盖。 代码结构清晰：通过POM module可以解决模块间的依赖关系， 所有外接模块都可以单独独立成Jar包被复用。当团队形成规范后，可以快速的定位到相关代码。 用传统的白话讲： DDD就是你写一个东西，规定好入参和出参的格式，里面加上各种各样的强规范，拓展就是实现你的接口，开发成本边低，总之，你写的业务逻辑在烂，也只会影响你自己的，不会影响别人的，系统的设计，要根据业务的需求，都是业务驱动技术，paas化就是为了更好的扩展业务的复杂度，只需要规定一系列规范，开展一个新的业务线，只是一个新接口的问题，甚至不用，只要垂直领域规划好了，领域边界明确了，只要增加扩展点也就是切点就可以了。paas化的难点就是边界的明确。","categories":[{"name":"领域驱动设计","slug":"领域驱动设计","permalink":"https://yinhuidong.github.io/categories/%E9%A2%86%E5%9F%9F%E9%A9%B1%E5%8A%A8%E8%AE%BE%E8%AE%A1/"}],"tags":[{"name":"DDD","slug":"DDD","permalink":"https://yinhuidong.github.io/tags/DDD/"}]},{"title":"Kubernetes基础入门","slug":"云计算/Kubernetes基础入门","date":"2022-01-12T00:19:55.252Z","updated":"2022-01-12T00:19:55.252Z","comments":true,"path":"2022/01/12/云计算/Kubernetes基础入门/","link":"","permalink":"https://yinhuidong.github.io/2022/01/12/%E4%BA%91%E8%AE%A1%E7%AE%97/Kubernetes%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8/","excerpt":"","text":"一，Kubernetes基础入门1.基础知识 以上展示了一个master（主节点）和6个worker（工作节点）的k8s集群 docker是每一个worker节点的运行时环境。 kubelet负责控制所有容器的启动停止，保证节点工作正常，已经帮助节点交互master。 2.部署一个应用123456789101112131415# kubectl create 帮我们创建k8s集群中的一些对象kubectl create --help#Create a deployment named my-nginx that runs the nginx imagekubectl create deployment my-nginx --image=nginx# Create a deployment with commandkubectl create deployment my-nginx --image=nginx -- date# Create a deployment named my-nginx that runs the nginx image with 3 replicas.kubectl create deployment my-nginx --image=nginx --replicas=3# Create a deployment named my-nginx that runs the nginx image and expose port 80.kubectl create deployment my-nginx --image=nginx --port=80 Deployment（部署） 在k8s中，通过发布 Deployment，可以创建应用程序 (docker image) 的实例 (docker container)，这个实例会被包含在称为 Pod 的概念中，Pod 是 k8s 中最小可管理单元。 在 k8s 集群中发布 Deployment 后，Deployment 将指示 k8s 如何创建和更新应用程序的实例，master 节点将应用程序实例调度到集群中的具体的节点上。 创建应用程序实例后，Kubernetes Deployment Controller 会持续监控这些实例。如果运行实例的 worker 节点关机或被删除，则 Kubernetes Deployment Controller 将在群集中资源最优的另一个 worker 节点上重新创建一个新的实例。这提供了一种自我修复机制来解决机器故障或维护问题。 在容器编排之前的时代，各种安装脚本通常用于启动应用程序，但是不能够使应用程序从机器故障中恢复。通过创建应用程序实例并确保它们在集群节点中的运行实例个数，Kubernetes Deployment 提供了一种完全不同的方式来管理应用程序。 Deployment 处于 master 节点上，通过发布 Deployment，master 节点会选择合适的 worker 节点创建 Container（即图中的正方体），Container 会被包含在 Pod （即蓝色圆圈）里。 3.应用程序探索 了解Kubernetes Pods（容器组） 了解Kubernetes Nodes（节点） 排查故障 创建 Deployment 后，k8s创建了一个 Pod（容器组） 来放置应用程序实例（container 容器）。 3.1、了解PodPod （容器组） 是一个k8s中一个抽象的概念，用于存放一组 container（可包含一个或多个 container 容器，即图上正方体)，以及这些 container （容器）的一些共享资源。这些资源包括： 共享存储，称为卷(Volumes)，即图上紫色圆柱 网络，每个 Pod（容器组）在集群中有个唯一的 IP，pod（容器组）中的 container（容器）共享该IP地址 container（容器）的基本信息，例如容器的镜像版本，对外暴露的端口等 Pod（容器组）是 k8s 集群上的最基本的单元。当我们在 k8s 上创建 Deployment 时，会在**集群上创建包含容器的 Pod (而不是直接创建容器)**。每个Pod都与运行它的 worker 节点（Node）绑定，并保持在那里直到终止或被删除。如果节点（Node）发生故障，则会在群集中的其他可用节点（Node）上运行相同的 Pod（从同样的镜像创建 Container，使用同样的配置，IP 地址不同，Pod 名字不同）。 TIP重要： Pod 是一组容器（可包含一个或多个应用程序容器），以及共享存储（卷 Volumes）、IP 地址和有关如何运行容器的信息。 如果多个容器紧密耦合并且需要共享磁盘等资源，则他们应该被部署在同一个Pod（容器组）中。 3.2、了解NodePod（容器组）总是在 Node（节点） 上运行。Node（节点）是 kubernetes 集群中的计算机，可以是虚拟机或物理机。每个 Node（节点）都由 master 管理。一个 Node（节点）可以有多个Pod（容器组），kubernetes master 会根据每个 Node（节点）上可用资源的情况，自动调度 Pod（容器组）到最佳的 Node（节点）上。 每个 Kubernetes Node（节点）至少运行： Kubelet，负责 master 节点和 worker 节点之间通信的进程；管理 Pod（容器组）和 Pod（容器组）内运行的 Container（容器）。 kube-proxy，负责进行流量转发 容器运行环境（如Docker）负责下载镜像、创建和运行容器等。 3.3、故障排除 kubectl get - 显示资源列表 12345678910# kubectl get 资源类型#获取类型为Deployment的资源列表kubectl get deployments#获取类型为Pod的资源列表kubectl get pods#获取类型为Node的资源列表kubectl get nodes 12345# 查看所有名称空间的 Deploymentkubectl get deployments -Akubectl get deployments --all-namespaces# 查看 kube-system 名称空间的 Deploymentkubectl get deployments -n kube-system 1234567#####并不是所有的对象都在名称空间中# 在名称空间里kubectl api-resources --namespaced=true# 不在名称空间里kubectl api-resources --namespaced=false kubectl describe - 显示有关资源的详细信息 1234567# kubectl describe 资源类型 资源名称#查看名称为nginx-XXXXXX的Pod的信息kubectl describe pod nginx-XXXXXX #查看名称为nginx的Deployment的信息kubectl describe deployment my-nginx kubectl logs - 查看pod中的容器的打印日志（和命令docker logs 类似） 12345# kubectl logs Pod名称#查看名称为nginx-pod-XXXXXXX的Pod内的容器打印的日志#本案例中的 nginx-pod 没有输出日志，所以您看到的结果是空的kubectl logs -f nginx-pod-XXXXXXX kubectl exec - 在pod中的容器环境内执行命令(和命令docker exec 类似) 123456# kubectl exec Pod名称 操作命令# 在名称为nginx-pod-xxxxxx的Pod中运行bashkubectl exec -it nginx-pod-xxxxxx /bin/bash### 注意：新版1.21.0 提示这个命令会过期 3.4、kubectl run也可以独立跑一个Pod 12## kubectl run --helpkubectl run nginx --image=nginx 4、应用外部可见4.1、目标 了解 Kubernetes 中的 Service 了解 标签(Label) 和 标签选择器(Label Selector) 对象如何与 Service 关联 在 Kubernetes 集群外用 Service 暴露应用 4.2、Kubernetes Service 总览 Kubernetes Pod 是转瞬即逝的。 Pod 实际上拥有 生命周期。 当一个工作 Node 挂掉后, 在 Node 上运行的 Pod 也会消亡。 ReplicaSet 会自动地通过创建新的 Pod 驱动集群回到目标状态，以保证应用程序正常运行。 Kubernetes 的 Service 是一个抽象层，它定义了一组 Pod 的逻辑集，并为这些 Pod 支持外部流量暴露、负载平衡和服务发现。 Service 使从属 Pod 之间的松耦合成为可能。 和其他 Kubernetes 对象一样, Service 用 YAML (更推荐) 或者 JSON 来定义. Service 下的一组 Pod 通常由 LabelSelector (请参阅下面的说明为什么您可能想要一个 spec 中不包含selector的服务)来标记。 尽管每个 Pod 都有一个唯一的 IP 地址，但是如果没有 Service ，这些 IP 不会暴露在群集外部。Service 允许您的应用程序接收流量。Service 也可以用在 ServiceSpec 标记type的方式暴露 ClusterIP (默认) - 在集群的内部 IP 上公开 Service 。这种类型使得 Service 只能从集群内访问。 NodePort - 使用 NAT 在集群中每个选定 Node 的相同端口上公开 Service 。使用&lt;NodeIP&gt;:&lt;NodePort&gt; 从集群外部访问 Service。是 ClusterIP 的超集。 LoadBalancer - 在当前云中创建一个外部负载均衡器(如果支持的话)，并为 Service 分配一个固定的外部IP。是 NodePort 的超集。 ExternalName - 通过返回带有该名称的 CNAME 记录，使用任意名称(由 spec 中的externalName指定)公开 Service。不使用代理。这种类型需要kube-dns的v1.7或更高版本。 4.3、Service 和 Label Service 通过一组 Pod 路由通信。Service 是一种抽象，它允许 Pod 死亡并在 Kubernetes 中复制，而不会影响应用程序。在依赖的 Pod (如应用程序中的前端和后端组件)之间进行发现和路由是由Kubernetes Service 处理的。 Service 匹配一组 Pod 是使用 标签(Label)和选择器(Selector), 它们是允许对 Kubernetes 中的对象进行逻辑操作的一种分组原语。标签(Label)是附加在对象上的键/值对，可以以多种方式使用: 指定用于开发，测试和生产的对象 嵌入版本标签 使用 Label 将对象进行分类 4.4、kubectl expose123456789101112kubectl expose deployment tomcat6 --port=8912 --target-port=8080 --type=NodePort## --port：集群内访问service的端口 8912## --target-port： pod容器的端口 8080## --nodePort： 每个机器开发的端口 30403## 进行验证kubectl get svc curl ip:port## kubectl exec 进去pod修改，并测试负载均衡 5、伸缩应用程序-扩缩容目标 用 kubectl 扩缩应用程序 扩缩一个 Deployment 我们创建了一个 Deployment ，然后通过 服务提供访问 Pod 的方式。我们发布的 Deployment 只创建了一个 Pod 来运行我们的应用程序。当流量增加时，我们需要对应用程序进行伸缩操作以满足系统性能需求。 12345## 扩展kubectl scale --replicas=3 deployment tomcat6#持续观测效果watch kubectl get pods -o wide 6、执行滚动升级目标 使用 kubectl 执行滚动更新 滚动更新允许通过使用新的实例逐步更新 Pod 实例从而实现 Deployments 更新，停机时间为零。 与应用程序扩展类似，如果暴露了 Deployment，服务（Service）将在更新期间仅对可用的 pod 进行负载均衡。可用 Pod 是应用程序用户可用的实例。 滚动更新允许以下操作： 将应用程序从一个环境提升到另一个环境（通过容器镜像更新） 回滚到以前的版本 持续集成和持续交付应用程序，无需停机 123456789101112#应用升级: tomcat:alpine、tomcat:jre8-alpinekubectl set image deployment.apps/tomcat6 tomcat=tomcat:jre8-alpine #可以携带--record参数，记录变更##回滚升级### 查看历史记录kubectl rollout history deployment.apps/tomcat6kubectl rollout history deploy tomcat6### 回滚到指定版本kubectl rollout undo deployment.apps/tomcat6 --to-revision=1kubectl rollout undo deploy tomcat6 --to-revision=1 7、以上用配置文件方式1、部署一个应用12345678910111213141516171819apiVersion: apps/v1 #与k8s集群版本有关，使用 kubectl api-versions 即可查看当前集群支持的版本kind: Deployment #该配置的类型，我们使用的是 Deploymentmetadata: #译名为元数据，即 Deployment 的一些基本属性和信息 name: nginx-deployment #Deployment 的名称 labels: #标签，可以灵活定位一个或多个资源，其中key和value均可自定义，可以定义多组，目前不需要理解 app: nginx #为该Deployment设置key为app，value为nginx的标签spec: #这是关于该Deployment的描述，可以理解为你期待该Deployment在k8s中如何使用 replicas: 1 #使用该Deployment创建一个应用程序实例 selector: #标签选择器，与上面的标签共同作用，目前不需要理解 matchLabels: #选择包含标签app:nginx的资源 app: nginx template: #这是选择或创建的Pod的模板 metadata: #Pod的元数据 labels: #Pod的标签，上面的selector即选择包含标签app:nginx的Pod app: nginx spec: #期望Pod实现的功能（即在pod中部署） containers: #生成container，与docker中的container是同一种 - name: nginx #container的名称 image: nginx:1.7.9 #使用镜像nginx:1.7.9创建container，该container默认80端口可访问 kubectl apply -f xxx.yaml 2、暴露应用12345678910111213141516apiVersion: v1kind: Servicemetadata: name: nginx-service #Service 的名称 labels: #Service 自己的标签 app: nginx #为该 Service 设置 key 为 app，value 为 nginx 的标签spec: #这是关于该 Service 的定义，描述了 Service 如何选择 Pod，如何被访问 selector: #标签选择器 app: nginx #选择包含标签 app:nginx 的 Pod ports: - name: nginx-port #端口的名字 protocol: TCP #协议类型 TCP/UDP port: 80 #集群内的其他容器组可通过 80 端口访问 Service nodePort: 32600 #通过任意节点的 32600 端口访问 Service targetPort: 80 #将请求转发到匹配 Pod 的 80 端口 type: NodePort #Serive的类型，ClusterIP/NodePort/LoaderBalancer 3、扩缩容修改deployment.yaml 中的 replicas 属性即可 完成后运行 kubectl apply -f xxx.yaml 4、滚动升级修改deployment.yaml 中的 imageName 属性等 完成后运行 kubectl apply -f xxx.yaml 以上都可以直接 kubectl edit deploy/service 等，修改完成后自动生效 二、其他1、查看Kubernetes适配的docker版本[https://github.com/kubernetes/kubernetes/releases](https://github.com/kubernetes/kubernetes/releases) 查看他的changelog，搜索适配的docker版本即可。 2、弃用dockershim的问题[https://kubernetes.io/zh/blog/2020/12/02/dockershim-faq/](https://kubernetes.io/zh/blog/2020/12/02/dockershim-faq/) 使用containerd： ``[https://kubernetes.io/zh/docs/setup/production-environment/container-runtimes/#containerd](https://kubernetes.io/zh/docs/setup/production-environment/container-runtimes/#containerd) 配置docker：[https://kubernetes.io/zh/docs/setup/production-environment/container-runtimes/#docker](https://kubernetes.io/zh/docs/setup/production-environment/container-runtimes/#docker) 3、部署dashboard[https://github.com/kubernetes/dashboard](https://github.com/kubernetes/dashboard)type: NodePort#访问测试每次访问都需要令牌kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk &#39;&#123;print $1&#125;&#39;) 4、master初始化的日志1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980[root@i-iqrlgkwc ~]# kubeadm init \\&gt; --apiserver-advertise-address=10.170.11.8 \\&gt; --image-repository registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images \\&gt; --kubernetes-version v1.21.0 \\&gt; --service-cidr=10.96.0.0/16 \\&gt; --pod-network-cidr=192.168.0.0/16[init] Using Kubernetes version: v1.21.0[preflight] Running pre-flight checks [WARNING IsDockerSystemdCheck]: detected &quot;cgroupfs&quot; as the Docker cgroup driver. The recommended driver is &quot;systemd&quot;. Please follow the guide at https://kubernetes.io/docs/setup/cri/[preflight] Pulling images required for setting up a Kubernetes cluster[preflight] This might take a minute or two, depending on the speed of your internet connection[preflight] You can also perform this action in beforehand using &#x27;kubeadm config images pull&#x27;[certs] Using certificateDir folder &quot;/etc/kubernetes/pki&quot;[certs] Generating &quot;ca&quot; certificate and key[certs] Generating &quot;apiserver&quot; certificate and key[certs] apiserver serving cert is signed for DNS names [k8s-01 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 10.170.11.8][certs] Generating &quot;apiserver-kubelet-client&quot; certificate and key[certs] Generating &quot;front-proxy-ca&quot; certificate and key[certs] Generating &quot;front-proxy-client&quot; certificate and key[certs] Generating &quot;etcd/ca&quot; certificate and key[certs] Generating &quot;etcd/server&quot; certificate and key[certs] etcd/server serving cert is signed for DNS names [k8s-01 localhost] and IPs [10.170.11.8 127.0.0.1 ::1][certs] Generating &quot;etcd/peer&quot; certificate and key[certs] etcd/peer serving cert is signed for DNS names [k8s-01 localhost] and IPs [10.170.11.8 127.0.0.1 ::1][certs] Generating &quot;etcd/healthcheck-client&quot; certificate and key[certs] Generating &quot;apiserver-etcd-client&quot; certificate and key[certs] Generating &quot;sa&quot; key and public key[kubeconfig] Using kubeconfig folder &quot;/etc/kubernetes&quot;[kubeconfig] Writing &quot;admin.conf&quot; kubeconfig file[kubeconfig] Writing &quot;kubelet.conf&quot; kubeconfig file[kubeconfig] Writing &quot;controller-manager.conf&quot; kubeconfig file[kubeconfig] Writing &quot;scheduler.conf&quot; kubeconfig file[kubelet-start] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;[kubelet-start] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;[kubelet-start] Starting the kubelet[control-plane] Using manifest folder &quot;/etc/kubernetes/manifests&quot;[control-plane] Creating static Pod manifest for &quot;kube-apiserver&quot;[control-plane] Creating static Pod manifest for &quot;kube-controller-manager&quot;[control-plane] Creating static Pod manifest for &quot;kube-scheduler&quot;[etcd] Creating static Pod manifest for local etcd in &quot;/etc/kubernetes/manifests&quot;[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory &quot;/etc/kubernetes/manifests&quot;. This can take up to 4m0s[kubelet-check] Initial timeout of 40s passed.[apiclient] All control plane components are healthy after 66.504822 seconds[upload-config] Storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace[kubelet] Creating a ConfigMap &quot;kubelet-config-1.21&quot; in namespace kube-system with the configuration for the kubelets in the cluster[upload-certs] Skipping phase. Please see --upload-certs[mark-control-plane] Marking the node k8s-01 as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers][mark-control-plane] Marking the node k8s-01 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule][bootstrap-token] Using token: os234q.tqr5fxmvapgu0b71[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster[bootstrap-token] Creating the &quot;cluster-info&quot; ConfigMap in the &quot;kube-public&quot; namespace[kubelet-finalize] Updating &quot;/etc/kubernetes/kubelet.conf&quot; to point to a rotatable kubelet client certificate and key[addons] Applied essential addon: CoreDNS[addons] Applied essential addon: kube-proxyYour Kubernetes control-plane has initialized successfully!To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/configAlternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.confYou should now deploy a pod network to the cluster.Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/Then you can join any number of worker nodes by running the following on each as root:kubeadm join 10.170.11.8:6443 --token os234q.tqr5fxmvapgu0b71 \\ --discovery-token-ca-cert-hash sha256:68251032e1f77a7356e784bdeb8e1f7f728cb0fb31c258dc7b44befc9f516f85","categories":[{"name":"云计算","slug":"云计算","permalink":"https://yinhuidong.github.io/categories/%E4%BA%91%E8%AE%A1%E7%AE%97/"}],"tags":[{"name":"云计算","slug":"云计算","permalink":"https://yinhuidong.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"}]},{"title":"Kubernetes安装","slug":"云计算/Kubernetes安装","date":"2022-01-12T00:19:55.252Z","updated":"2022-01-12T00:19:55.252Z","comments":true,"path":"2022/01/12/云计算/Kubernetes安装/","link":"","permalink":"https://yinhuidong.github.io/2022/01/12/%E4%BA%91%E8%AE%A1%E7%AE%97/Kubernetes%E5%AE%89%E8%A3%85/","excerpt":"","text":"一，Kubernetes简介1.部署方式的变迁1.1.背景 传统部署时代： 在物理服务器上运行应用程序 无法为应用程序定义资源边界 导致资源分配问题 例如，如果在物理服务器上运行多个应用程序，则可能会出现一个应用程序占用大部分资源的情况， 结果可能导致其他应用程序的性能下降。 一种解决方案是在不同的物理服务器上运行每个应用程序，但是由于资源利用不足而无法扩展， 并且维护许多物理服务器的成本很高。 虚拟化部署时代： 作为解决方案，引入了虚拟化 虚拟化技术允许你在单个物理服务器的 CPU 上运行多个虚拟机（VM） 虚拟化允许应用程序在 VM 之间隔离，并提供一定程度的安全 一个应用程序的信息 不能被另一应用程序随意访问。 虚拟化技术能够更好地利用物理服务器上的资源 因为可轻松地添加或更新应用程序 ，所以可以实现更好的可伸缩性，降低硬件成本等等。 每个 VM 是一台完整的计算机，在虚拟化硬件之上运行所有组件，包括其自己的操作系统。 缺点：虚拟层冗余导致的资源浪费与性能下降 容器部署时代： 容器类似于 VM，但可以在应用程序之间共享操作系统（OS）。 容器被认为是轻量级的。 容器与 VM 类似，具有自己的文件系统、CPU、内存、进程空间等。 由于它们与基础架构分离，因此可以跨云和 OS 发行版本进行移植。 参照【Docker隔离原理- namespace 6项隔离（资源隔离）与 cgroups 8项资源限制（资源限制）】 裸金属：真正的物理服务器 容器优势： 敏捷性：敏捷应用程序的创建和部署：与使用 VM 镜像相比，提高了容器镜像创建的简便性和效率。 及时性：持续开发、集成和部署：通过快速简单的回滚（由于镜像不可变性），支持可靠且频繁的 容器镜像构建和部署。 解耦性：关注开发与运维的分离：在构建/发布时创建应用程序容器镜像，而不是在部署时。 从而将应用程序与基础架构分离。 可观测性：可观察性不仅可以显示操作系统级别的信息和指标，还可以显示应用程序的运行状况和其他指标信号。 跨平台：跨开发、测试和生产的环境一致性：在便携式计算机上与在云中相同地运行。 可移植：跨云和操作系统发行版本的可移植性：可在 Ubuntu、RHEL、CoreOS、本地、 Google Kubernetes Engine 和其他任何地方运行。 简易性：以应用程序为中心的管理：提高抽象级别，从在虚拟硬件上运行 OS 到使用逻辑资源在 OS 上运行应用程序。 大分布式：松散耦合、分布式、弹性、解放的微服务：应用程序被分解成较小的独立部分， 并且可以动态部署和管理 - 而不是在一台大型单机上整体运行。 隔离性：资源隔离：可预测的应用程序性能。 高效性：资源利用：高效率和高密度 K8S之前：10台服务器：25+15中间件K8S之后：10台服务器：上百个应用了。k8s管理10几台服务器。资源规划。 1.2 容器化问题 弹性的容器化应用管理 强大的故障转移能力 高性能的负载均衡访问机制 便捷的扩展 自动化的资源监测 …… docker swarm：大规模进行容器编排mesos：apacheKubernetes : google；竞品： Kubernetes 胜利 1.3 为什么用kubernrtes 容器是打包和运行应用程序的好方式。在生产环境中，你需要管理运行应用程序的容器，并确保不会停机。 例如，如果一个容器发生故障，则需要启动另一个容器。如果系统处理此行为，会不会更容易？ 这就是 Kubernetes 来解决这些问题的方法！ Kubernetes 为你提供了一个可弹性运行分布式系统的框架。linux之上的一个服务编排框架； Kubernetes 会满足你的扩展要求、故障转移、部署模式等。 例如，Kubernetes 可以轻松管理系统的 Canary 部署。 Kubernetes 为你提供： 服务发现和负载均衡Kubernetes 可以使用 DNS 名称或自己的 IP 地址公开容器，如果进入容器的流量很大， Kubernetes 可以负载均衡并分配网络流量，从而使部署稳定。 存储编排Kubernetes 允许你自动挂载你选择的存储系统，例如本地存储、公共云提供商等。 自动部署和回滚你可以使用 Kubernetes 描述已部署容器的所需状态，它可以以受控的速率将实际状态 更改为期望状态。例如，你可以自动化 Kubernetes 来为你的部署创建新容器， 删除现有容器并将它们的所有资源用于新容器。 自动完成装箱计算Kubernetes 允许你指定每个容器所需 CPU 和内存（RAM）。 当容器指定了资源请求时，Kubernetes 可以做出更好的决策来管理容器的资源。 自我修复Kubernetes 重新启动失败的容器、替换容器、杀死不响应用户定义的 运行状况检查的容器，并且在准备好服务之前不将其通告给客户端。 密钥与配置管理Kubernetes 允许你存储和管理敏感信息，例如密码、OAuth 令牌和 ssh 密钥。 你可以在不重建容器镜像的情况下部署和更新密钥和应用程序配置，也无需在堆栈配置中暴露密钥 ……. **为了生产环境的容器化大规模应用编排，必须有一个自动化的框架。系统** 2.简介 Kubernetes 是一个可移植的、可扩展的开源平台，用于管理容器化的工作负载和服务，可促进声明式配置和自动化。 Kubernetes 拥有一个庞大且快速增长的生态系统。Kubernetes 的服务、支持和工具广泛可用。 名称 Kubernetes 源于希腊语，意为“舵手”或“飞行员”。Google 在 2014 年开源了 Kubernetes 项目。 Kubernetes 建立在 Google 在大规模运行生产工作负载方面拥有十几年的经验 的基础上，结合了社区中最好的想法和实践。 2.1 Kubernetes 不是什么 Kubernetes 不是传统的、包罗万象的 PaaS（平台即服务）系统。 Kubernetes 在容器级别而不是在硬件级别运行 它提供了 PaaS 产品共有的一些普遍适用的功能， 例如部署、扩展、负载均衡、日志记录和监视。 但是，Kubernetes 不是单体系统，默认解决方案都是可选和可插拔的。 Kubernetes 提供了构建开发人员平台的基础，但是在重要的地方保留了用户的选择和灵活性。 Kubernetes： 不限制支持的应用程序类型。 Kubernetes 旨在支持极其多种多样的工作负载，包括无状态、有状态和数据处理工作负载。 如果应用程序可以在容器中运行，那么它应该可以在 Kubernetes 上很好地运行。 不部署源代码，也不构建你的应用程序。 持续集成(CI)、交付和部署（CI/CD）工作流取决于组织的文化和偏好以及技术要求。 不提供应用程序级别的服务作为内置服务，例如中间件（例如，消息中间件）、 数据处理框架（例如，Spark）、数据库（例如，mysql）、缓存、集群存储系统 （例如，Ceph）。这样的组件可以在 Kubernetes 上运行，并且/或者可以由运行在 Kubernetes 上的应用程序通过可移植机制（例如， 开放服务代理）来访问。 不要求日志记录、监视或警报解决方案。 它提供了一些集成作为概念证明，并提供了收集和导出指标的机制。 不提供或不要求配置语言/系统（例如 jsonnet），它提供了声明性 API， 该声明性 API 可以由任意形式的声明性规范所构成。RESTful；写yaml文件 不提供也不采用任何全面的机器配置、维护、管理或自我修复系统。 此外，Kubernetes 不仅仅是一个编排系统，实际上它消除了编排的需要。 编排的技术定义是执行已定义的工作流程：首先执行 A，然后执行 B，再执行 C。 相比之下，Kubernetes 包含一组独立的、可组合的控制过程， 这些过程连续地将当前状态驱动到所提供的所需状态。 如何从 A 到 C 的方式无关紧要，也不需要集中控制，这使得系统更易于使用 且功能更强大、系统更健壮、更为弹性和可扩展。 容器管家： 安装了很多应用。 ————————- qq电脑管家。（自动杀垃圾，自动卸载没用东西….） 机器上有很多容器。 ————————– kubernete容器的管家。（容器的启动停止、故障转义、负载均衡等） 二，Kubernetes安装1.集群原理 集群：主从： 主从同步/复制 ;mysql 主 – mysql 从 主管理从 v 分片（数据集群）： 大家都一样 每个人存一部分东西 1.1 master-node 架构 11000台机器 地主+奴隶 地（机器） 奴隶（在机器上干活） master：主节点（地主）。可能有很多（多人控股公司） node：work节点（工作节点）。 很多。真正干应用的活 master 和 worker怎么交互 master决定worker里面都有什么 worker只是和master （API） 通信； 每一个节点自己干自己的活 程序员使用UI或者CLI操作k8s集群的master，就可以知道整个集群的状况。 1.2 工作原理 master节点（Control Plane【控制面板】）：master节点控制整个集群 master节点上有一些核心组件： Controller Manager：控制管理器 etcd：键值数据库（redis）【记账本，记事本】 scheduler：调度器 api server：api网关（所有的控制都需要通过api-server） node节点（worker工作节点）： kubelet（监工）：每一个node节点上必须安装的组件。 kube-proxy：代理。代理网络 部署一个应用？ 程序员：调用CLI告诉master，我们现在要部署一个tomcat应用 程序员的所有调用都先去master节点的网关api-server。这是matser的唯一入口（mvc模式中的c层） 收到的请求先交给master的api-server。由api-server交给controller-mannager进行控制 controller-mannager 进行 应用部署 controller-mannager 会生成一次部署信息。 tomcat –image:tomcat6 –port 8080 ,真正不部署应用 部署信息被记录在etcd中 scheduler调度器从etcd数据库中，拿到要部署的应用，开始调度。看哪个节点合适， scheduler把算出来的调度信息再放到etcd中 每一个node节点的监控kubelet，随时和master保持联系的（给api-server发送请求不断获取最新数据），所有节点的kubelet就会从master 假设node2的kubelet最终收到了命令，要部署。 kubelet就自己run一个应用在当前机器上，随时给master汇报当前应用的状态信息，分配ip node和master是通过master的api-server联系的 每一个机器上的kube-proxy能知道集群的所有网络。只要node访问别人或者别人访问node，node上的kube-proxy网络代理自动计算进行流量转发 下图和上图一样 无论访问哪个机器，都可以访问到真正的应用（Service【服务】） 1.3 原理分解1.主节点（master） 快速介绍： master也要装kubelet和kubeproxy 前端访问（UI\\CLI）： kube-apiserver： scheduler: controller manager: etcd kubelet+kubeproxy每一个节点的必备+docker（容器运行时环境） 2.工作节点（node） 快速介绍： Pod： docker run 启动的是一个container（容器），容器是docker的基本单位，一个应用是一个容器 kubelet run 启动的一个应用称为一个Pod；Pod是k8s的基本单位。 - Pod是容器的一个再封装 - atguigu(永远不变) ==slf4j= log4j(类) - 应用 ===== Pod ======= docker的容器 - 一个容器往往代表不了一个基本应用。博客（php+mysql合起来完成） - 准备一个Pod 可以包含多个 container；一个Pod代表一个基本的应用。 - IPod（看电影、听音乐、玩游戏）【一个基本产品，原子】； - Pod（music container、movie container）【一个基本产品，原子的】 Kubelet：监工，负责交互master的api-server以及当前机器的应用启停等，在master机器就是master的小助手。每一台机器真正干活的都是这个 Kubelet Kube-proxy： 其他： 2.组件交互原理 想让k8s部署一个tomcat？ 开机默认所有节点的kubelet、master节点的scheduler（调度器）、controller-manager（控制管理器）一直监听master的api-server发来的事件变化（for ::） 程序员使用命令行工具： kubectl ； kubectl create deploy tomcat –image=tomcat8（告诉master让集群使用tomcat8镜像，部署一个tomcat应用） kubectl命令行内容发给api-server，api-server保存此次创建信息到etcd etcd给api-server上报事件，说刚才有人给我里面保存一个信息。（部署Tomcat[deploy]） controller-manager监听到api-server的事件，是 （部署Tomcat[deploy]） controller-manager 处理这个 （部署Tomcat[deploy]）的事件。controller-manager会生成Pod的部署信息【pod信息】 controller-manager 把Pod的信息交给api-server，再保存到etcd etcd上报事件【pod信息】给api-server。 scheduler专门监听 【pod信息】 ，拿到 【pod信息】的内容，计算，看哪个节点合适部署这个Pod【pod调度过后的信息（node: node-02）】， scheduler把 【pod调度过后的信息（node: node-02）】交给api-server保存给etcd etcd上报事件【pod调度过后的信息（node: node-02）】，给api-server 其他节点的kubelet专门监听 【pod调度过后的信息（node: node-02）】 事件，集群所有节点kubelet从api-server就拿到了 【pod调度过后的信息（node: node-02）】 事件 每个节点的kubelet判断是否属于自己的事情；node-02的kubelet发现是他的事情 node-02的kubelet启动这个pod。汇报给master当前启动好的所有信息 3.安装3.1 理解安装方式 二进制方式（建议生产环境使用） MiniKube….. kubeadm引导方式（官方推荐） GA 大致流程 准备N台服务器，内网互通， 安装Docker容器化环境【k8s放弃dockershim】 安装Kubernetes 三台机器安装核心组件（kubeadm(创建集群的引导工具), kubelet，kubectl（程序员用的命令行） ） kubelet可以直接通过容器化的方式创建出之前的核心组件（api-server）【官方把核心组件做成镜像】 由kubeadm引导创建集群 3.2执行1.准备机器 开通三台机器，内网互通，配置公网ip。centos7.8/7.9，基础实验2c4g三台也可以 每台机器的hostname不要用localhost，可用k8s-01，k8s-02，k8s-03之类的【不包含下划线、小数点、大写字母】（这个后续步骤也可以做） 2.安装前置环境基础环境** 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556##########################################################################关闭防火墙： 如果是云服务器，需要设置安全组策略放行端口# https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#check-required-portssystemctl stop firewalldsystemctl disable firewalld# 修改 hostnamehostnamectl set-hostname k8s-01# 查看修改结果hostnamectl status# 设置 hostname 解析echo &quot;127.0.0.1 $(hostname)&quot; &gt;&gt; /etc/hosts#关闭 selinux： sed -i &#x27;s/enforcing/disabled/&#x27; /etc/selinux/configsetenforce 0#关闭 swap：swapoff -a sed -ri &#x27;s/.*swap.*/#&amp;/&#x27; /etc/fstab #允许 iptables 检查桥接流量#https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#%E5%85%81%E8%AE%B8-iptables-%E6%A3%80%E6%9F%A5%E6%A1%A5%E6%8E%A5%E6%B5%81%E9%87%8F## 开启br_netfilter## sudo modprobe br_netfilter## 确认下## lsmod | grep br_netfilter## 修改配置#####这里用这个，不要用课堂上的配置。。。。。。。。。#将桥接的 IPv4 流量传递到 iptables 的链：# 修改 /etc/sysctl.conf# 如果有配置，则修改sed -i &quot;s#^net.ipv4.ip_forward.*#net.ipv4.ip_forward=1#g&quot; /etc/sysctl.confsed -i &quot;s#^net.bridge.bridge-nf-call-ip6tables.*#net.bridge.bridge-nf-call-ip6tables=1#g&quot; /etc/sysctl.confsed -i &quot;s#^net.bridge.bridge-nf-call-iptables.*#net.bridge.bridge-nf-call-iptables=1#g&quot; /etc/sysctl.confsed -i &quot;s#^net.ipv6.conf.all.disable_ipv6.*#net.ipv6.conf.all.disable_ipv6=1#g&quot; /etc/sysctl.confsed -i &quot;s#^net.ipv6.conf.default.disable_ipv6.*#net.ipv6.conf.default.disable_ipv6=1#g&quot; /etc/sysctl.confsed -i &quot;s#^net.ipv6.conf.lo.disable_ipv6.*#net.ipv6.conf.lo.disable_ipv6=1#g&quot; /etc/sysctl.confsed -i &quot;s#^net.ipv6.conf.all.forwarding.*#net.ipv6.conf.all.forwarding=1#g&quot; /etc/sysctl.conf# 可能没有，追加echo &quot;net.ipv4.ip_forward = 1&quot; &gt;&gt; /etc/sysctl.confecho &quot;net.bridge.bridge-nf-call-ip6tables = 1&quot; &gt;&gt; /etc/sysctl.confecho &quot;net.bridge.bridge-nf-call-iptables = 1&quot; &gt;&gt; /etc/sysctl.confecho &quot;net.ipv6.conf.all.disable_ipv6 = 1&quot; &gt;&gt; /etc/sysctl.confecho &quot;net.ipv6.conf.default.disable_ipv6 = 1&quot; &gt;&gt; /etc/sysctl.confecho &quot;net.ipv6.conf.lo.disable_ipv6 = 1&quot; &gt;&gt; /etc/sysctl.confecho &quot;net.ipv6.conf.all.forwarding = 1&quot; &gt;&gt; /etc/sysctl.conf# 执行命令以应用sysctl -p################################################################# docker环境** 1234567891011121314151617181920212223sudo yum remove docker*sudo yum install -y yum-utils#配置docker yum 源sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo#安装docker 19.03.9yum install -y docker-ce-3:19.03.9-3.el7.x86_64 docker-ce-cli-3:19.03.9-3.el7.x86_64 containerd.io#安装docker 19.03.9 docker-ce 19.03.9yum install -y docker-ce-19.03.9-3 docker-ce-cli-19.03.9 containerd.io#启动服务systemctl start dockersystemctl enable docker#配置加速sudo mkdir -p /etc/dockersudo tee /etc/docker/daemon.json &lt;&lt;-&#x27;EOF&#x27;&#123; &quot;registry-mirrors&quot;: [&quot;https://82m9ar63.mirror.aliyuncs.com&quot;]&#125;EOFsudo systemctl daemon-reloadsudo systemctl restart docker 安装k8s核心** 1234567891011121314151617181920212223# 配置K8S的yum源cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=0repo_gpgcheck=0gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF# 卸载旧版本yum remove -y kubelet kubeadm kubectl# 查看可以安装的版本yum list kubelet --showduplicates | sort -r# 安装kubelet、kubeadm、kubectl 指定版本yum install -y kubelet-1.21.0 kubeadm-1.21.0 kubectl-1.21.0# 开机启动kubeletsystemctl enable kubelet &amp;&amp; systemctl start kubelet 初始化master节点** 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172############下载核心镜像 kubeadm config images list：查看需要哪些镜像###############封装成images.sh文件#!/bin/bashimages=( kube-apiserver:v1.21.0 kube-proxy:v1.21.0 kube-controller-manager:v1.21.0 kube-scheduler:v1.21.0 coredns:v1.8.0 etcd:3.4.13-0 pause:3.4.1)for imageName in $&#123;images[@]&#125; ; do docker pull registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images/$imageNamedone#####封装结束chmod +x images.sh &amp;&amp; ./images.sh# registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images/coredns:v1.8.0##注意1.21.0版本的k8s coredns镜像比较特殊，结合阿里云需要特殊处理，重新打标签docker tag registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images/coredns:v1.8.0 registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images/coredns/coredns:v1.8.0########kubeadm init 一个master################################kubeadm join 其他worker########################kubeadm init \\--apiserver-advertise-address=10.170.11.8 \\--image-repository registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images \\--kubernetes-version v1.21.0 \\--service-cidr=10.96.0.0/16 \\--pod-network-cidr=192.168.0.0/16## 注意：pod-cidr与service-cidr# cidr 无类别域间路由（Classless Inter-Domain Routing、CIDR）# 指定一个网络可达范围 pod的子网范围+service负载均衡网络的子网范围+本机ip的子网范围不能有重复域######按照提示继续######## init完成后第一步：复制相关文件夹To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config## 导出环境变量Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf### 部署一个pod网络You should now deploy a pod network to the cluster.Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ ##############如下：安装calico#####################kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml### 命令检查kubectl get pod -A ##获取集群中所有部署好的应用Podkubectl get nodes ##查看集群所有机器的状态 Then you can join any number of worker nodes by running the following on each as root:kubeadm join 172.24.80.222:6443 --token nz9azl.9bl27pyr4exy2wz4 \\ --discovery-token-ca-cert-hash sha256:4bdc81a83b80f6bdd30bb56225f9013006a45ed423f131ac256ffe16bae73a20 初始化worker节点** 1234567891011## 用master生成的命令即可kubeadm join 172.24.80.222:6443 --token nz9azl.9bl27pyr4exy2wz4 \\ --discovery-token-ca-cert-hash sha256:4bdc81a83b80f6bdd30bb56225f9013006a45ed423f131ac256ffe16bae73a20 ##过期怎么办kubeadm token create --print-join-commandkubeadm token create --ttl 0 --print-join-commandkubeadm join --token y1eyw5.ylg568kvohfdsfco --discovery-token-ca-cert-hash sha256: 6c35e4f73f72afd89bf1c8c303ee55677d2cdb1342d67bb23c852aba2efc7c73 验证集群** 123456789101112#获取所有节点kubectl get nodes#给节点打标签## k8s中万物皆对象。node:机器 Pod：应用容器###加标签 《h1》kubectl label node k8s-02 node-role.kubernetes.io/worker=&#x27;&#x27;###去标签kubectl label node k8s-02 node-role.kubernetes.io/worker-## k8s集群，机器重启了会自动再加入集群，master重启了会自动再加入集群控制中心 设置ipvs模式**k8s整个集群为了访问通；默认是用iptables,性能低下（kube-proxy在集群之间同步iptables的内容） 123456789101112131415161718#1、查看默认kube-proxy 使用的模式kubectl logs -n kube-system kube-proxy-28xv4#2、需要修改 kube-proxy 的配置文件,修改mode 为ipvs。默认iptables，但是集群大了以后就很慢kubectl edit cm kube-proxy -n kube-system修改如下 ipvs: excludeCIDRs: null minSyncPeriod: 0s scheduler: &quot;&quot; strictARP: false syncPeriod: 30s kind: KubeProxyConfiguration metricsBindAddress: 127.0.0.1:10249 mode: &quot;ipvs&quot; ###修改了kube-proxy的配置，为了让重新生效，需要杀掉以前的Kube-proxy kubectl get pod -A|grep kube-proxy kubectl delete pod kube-proxy-pqgnt -n kube-system### 修改完成后可以重启kube-proxy以生效","categories":[{"name":"云计算","slug":"云计算","permalink":"https://yinhuidong.github.io/categories/%E4%BA%91%E8%AE%A1%E7%AE%97/"}],"tags":[{"name":"云计算","slug":"云计算","permalink":"https://yinhuidong.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"}]},{"title":"云计算概述","slug":"云计算/初识云计算","date":"2022-01-12T00:19:55.252Z","updated":"2022-01-12T00:19:55.252Z","comments":true,"path":"2022/01/12/云计算/初识云计算/","link":"","permalink":"https://yinhuidong.github.io/2022/01/12/%E4%BA%91%E8%AE%A1%E7%AE%97/%E5%88%9D%E8%AF%86%E4%BA%91%E8%AE%A1%E7%AE%97/","excerpt":"","text":"1.云计算的技术革命1）互联网时代的历程 1994年4月，中国与国际互联网实现全功能连接 1998，各个门户网站兴起，真正意义上进入网络时代 1997~2000，各大互联网巨头成立 2002~2012，IDC/数据中心的硬件时代（Internet DataCenter） 2005 Amazon云计算平台AWS（Amazon Web Service） 2）云计算是什么 云计算不是新技术，是一种新的互联网模式，通过使用公有云或者私有云资源，便捷、快速的为我们提供服务。 在虚拟化、分布式、自动化平台上的更深层解决方案 私有云、公有云、混合云，4c8g 70-80%,裸金属 “云”中的资源在使用者看来是可以无限扩展的，并且可以随时获取，按需使用，随时扩展，按使用付费。这种特性经常被称为像水电一样使用IT基础设施。3）云计算的历程4）相关概念 IaaS：Infrastructure-as-a-Service 基础设施即服务（提供服务器） PaaS：Platform-as-a-Service 平台即服务（提供带环境的服务器） SaaS：Software-as-a-Service 软件即服务（提供做好的软件） CaaS：Container-as-a-Service 容器即服务 5）云平台的优缺点优势 稳定性：云平台大量资源，分布式集群部署，保障服务永不宕机，几个9；0.999999,1 弹性扩展：按需索取，一键秒级开通需要的资源 安全性：云上平台生产级可用的完善权限系统 成本：初期计算资源成本极低，后期更是大量降低运维成本 易用性：各大云商都有Web管理控制台，可视化，智能化便捷操作 缺点 公有云，服务资源被第三方管理，不符合特殊级别的安全场景 私有云，搭建、维护、升级成本大 2.云计算技术架构演进过程1）体系变革 2）架构的变革 单体架构阶段 集群架构阶段 分布式架构阶段 微服务架构阶段 网格架构阶段 分布式和集群的区别？** 分布式：把一个大型应用，拆分出很多功能模块，各个功能部署在不同服务器，所有这些服务器合起来提供完整服务。 集群：一个功能模块复制很多份部署 3.云上的挑战 云机器资源编排 云存储方案 云负载均衡方案 云缓存方案 云持久化 云运维 云监控 云容器技术 云DevOps 云安全防护 4.云原生的生态系统1）完整云原生平台基础研究量 Docker、Docker Compose：容器化技术 Kubernetes：大规模容器编排 Helm：云原生应用商店 Rancher：易用的容器管理平台 KubeSphere：一站式容器云平台 OpenTracing：云原生链路追踪标准 Jaeger：云原生链路追踪实现产品 Istio：ServiceMesh下的服务流量治理 Jenkins、JenkinsX、Jenkins-BlueOcean：老牌的CI/CD平台 Gitlab/hub-CICD：Gitlab/hub自带的CICD Argo：kubernetes声明式持续集成 Nexus：Maven私库 Habor：Docker私库 Prometheus+Grafana：监控与可视化方案 ElasticSearch+Fluentd+Kibana：日志与可视化方案 Serverless：无服务器上云方案 SpringCloud Kubernetes：微服务上云方案5.云原生的术语 1）云原生的定义 https://github.com/cncf/toc/blob/master/DEFINITION.md 云原生技术有利于各组织在公有云、私有云和混合云等新型动态环境中，构建和运行 可弹性扩展的应用。 云原生的代表技术包括容器、服务网格、微服务、不可变基础设施和声明式 API( ( 配置文件、请求、可视化操作 ….) 。 云上配置文件 这些技术能够构建容错性好、易于管理和便于观察的 松耦合系统。结合可靠的 自动化手段，云原生技术使工程师能够轻松地对系统作出频繁和可预测的重大变更。 云原生计算基金会（CNCF）致力于培育和维护一个厂商中立的开源生态系统，来推广云原生技术。我们通过将最前沿的模式民主化，让这些创新为大众所用。 kubesphere国内厂商","categories":[{"name":"云计算","slug":"云计算","permalink":"https://yinhuidong.github.io/categories/%E4%BA%91%E8%AE%A1%E7%AE%97/"}],"tags":[{"name":"云计算","slug":"云计算","permalink":"https://yinhuidong.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"}]},{"title":"深入理解DockerFile","slug":"云计算/深入理解DockerFile","date":"2022-01-12T00:19:55.252Z","updated":"2022-01-12T00:19:55.253Z","comments":true,"path":"2022/01/12/云计算/深入理解DockerFile/","link":"","permalink":"https://yinhuidong.github.io/2022/01/12/%E4%BA%91%E8%AE%A1%E7%AE%97/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3DockerFile/","excerpt":"","text":"1.是什么Dockerfile是用来构建Docker镜像的构建文件，是由一系列命令和参数构成的脚本。 2.构建三步骤 编写Dockerfile文件 docker build 生成镜像 docker run 镜像 3.Dockerfile构建过程解析1）Dockerfile内容基础知识 每条保留字指令都必须为大写字母且后面要跟随至少一个参数 指令按照从上到下，顺序执行 #表示注释 每条指令都会创建一个新的镜像层，并对镜像进行提交 2）docker执行dockerfile大致流程 docker从基础镜像运行一个容器 执行一条指令并对容器作出修改 执行类似docker commit 的操作提交一个新的镜像层 docker在基于刚提交的镜像运行一个容器 执行Dockerfile的下一条指令直到所有指令都执行完成 3）抽象 Dockerfile —-&gt; .class Docker镜像 ——&gt; jar Docker容器 ——-&gt; 运行的jar 4.Dockerfile保留字指令123456789101112131415161718FROM：基础镜像，当前新的镜像是基于哪个镜像的MAINTAINER：镜像维护者的名字和邮箱RUN：容器构建时需要运行的命令EXPOSE：当前容器对外暴露出的端口WORKDIR：指定在创建容器后，终端默认登录进来的工作目录，一个落脚点ENV:用来在构建镜像过程中设置环境变量 这个环境变量可以再后续任何RUN指令中使用，这就如同在命令前面指定了环境变量前缀一样 也可以在其他指令中直接使用 ENV MY_PATH/usr/local WORKDIR $MY_PATHADD:拷贝并解压COPY:拷贝VOLUME:容器数据卷，用于数据保存和持久化工作CMD:指定一个容器启动时要运行的命令 Dockerfile中可以有多个CMD命令，但是只有最后一个生效，CMD会被docker run之后的参数替换ENTRYPOINT:指定一个容器启动时要运行的命令 ENTRYPOINT的目的和CMD一样，都是在指定容器启动程序及参数ONBUILD:当构建一个被继承的Dockerfile时运行命令，父镜像再被子镜像继承后，父镜像的onbuild被触 5.自定义镜像mycentosBase镜像：类似java的Object，99%的镜像都是通过在base镜像中安装和配置需要的软件构建出来的。 1.编写12345678910111213141516171819vim Dockerfile-------------------------------------------from centos#基础系统是centosmaintainer yhd&lt;1972039773@qq.com&gt;#作者邮件env mypath /tmp#我的路径workdir /tmp#开始工作目录 $mypathrun yum -y install vim#安装vimrun yum -y install net-toolsexpose 80#暴露的端口cmd echo $mypathcmd echo &quot;success---ok!&quot;cmd /bin/bash-------------------------------------------- 2.构建1docker build -f /root/Dockerfile -t mycentos:1.3 3.运行12docker images 查看所有镜像，发现已经有了docker run -it mycentos:1.3 4.列出镜像的变更历史1docker history mycentos:1.3 6.cmd-entrypoint命令案例1）cmd ls123docker build -f /root/Dockerfile -t mil:1.1 .docker run -it mil -al 会报错因为命令行追加的参数会覆盖原有的cmd命令ls 2）entrypoint ls123docker build -f /root/Dockerfile -t mi2:1.1docker run -it mi2 -al 会执行ls -al 因为命令行追加的参数会与上一条命令合在一起 ls -al 7.onbuild命令子镜像继承了父镜像，子镜像运行的时候，父镜像onbuild的命令就会执行 类似父类构造器写的代码，会在子类实例化以后执行 8.自定义tomcat91234567891011121314151617181920212223242526272829303132333435363738mkdir yhdcd yhdtouch c.txtcp /opt/apache-tomcat-9.0.8.tar.gz /root/yhd/cp /opt/jdk-1.8.tar.gz /root/yhd/vim Dockerfile----------------------from centosmaintainer yhd&lt;1972039773@qq.com&gt;#吧c.txt拷贝到/usr/local/cincontainer.txtcopy c.txt /usr/local/cincontainer.txt#吧java和tomcat添加到容器中add jdk -1.8.tar.gz /usr/local/add apache-tomcat-9.0.8.tar.gz /usr/local/#安装vim编辑器run yum -y install vim#设置家目录env mypath /usr/localworkdir #mypath#配置java和tomcat环境变量env java_home /usr/local/jdk-1.8env classpath $java_home/lib/dt.jar:$java_home/lib/tools.jarenv catalina_home /usr/local/apache-tomcat-9.0.8env catalina_base /usr/local/apache-tomcat-9.0.8env path $path:$java_home/bin:$catalina_home/lib:$catalina_home/bin#容器运行时监听的端口expose 8080#启动时运行tomcat#entrypoint [&quot;/usr/local/apache-tomcat-9.0.8/binstartup.sh&quot;]#cmd [&quot;/usr/local/apache-tomcat-9.0.8/bin/catalina.sh&quot;,&quot;run&quot;]cmd /usr/local/apache-tomcat-9.0.8/bin/startup.sh&amp;&amp; tail -F /usr/local/apache-tomcat-9.0.8/bin/logs/catalina.out----------------------#构建docker build -t yhdTomcat9:1.1 .#如果dockerfile文件在当前目录下且就叫Dockerfile，可以省略 -f /root/yhd/Dockerfile#运行docker run -d -p 9080:8080 --name myt9 -v /root/yhd/tomcat9/test:/usr/local/apache-tomcat-9.0.8/webapps/test --privileged=trueyhdTomcat9:1.1 9.总结使用本地docker实例运行dockerfile docker build -&gt;image镜像 docker run -&gt;docker 容器 容器-&gt;commit -&gt;镜像-&gt;push &lt;-pull","categories":[{"name":"云计算","slug":"云计算","permalink":"https://yinhuidong.github.io/categories/%E4%BA%91%E8%AE%A1%E7%AE%97/"}],"tags":[{"name":"云计算","slug":"云计算","permalink":"https://yinhuidong.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"}]},{"title":"Docker实战","slug":"云计算/Docker实战","date":"2022-01-12T00:19:55.251Z","updated":"2022-01-12T00:19:55.251Z","comments":true,"path":"2022/01/12/云计算/Docker实战/","link":"","permalink":"https://yinhuidong.github.io/2022/01/12/%E4%BA%91%E8%AE%A1%E7%AE%97/Docker%E5%AE%9E%E6%88%98/","excerpt":"","text":"一，Windows下Docker安装文档1.前期准备工作 首先需要明确，在Windows下Vmware虚拟机和docker只能二选一，不能共存。 关于Vmware的卸载和注册表的清理 卸载：通过控制面板 注册表清理：进入注册表编辑面板，ctrl+F搜索vmware，相关的东西全干掉，一个不留 确保虚拟化的开启，开启需要进入BIOS 进入控制面板-程序-启用或关闭windows功能，把Hyper-v勾选上，启用后电脑会重启，完成后即可下载安装docker。 2.安装 下载链接 1https://docs.docker.com/docker-for-windows/install/#download-docker-for-windows 如果提示是启动失败，（小米电脑直接成功了，但是华硕失败了），大概原因是因为虚拟化的原因，失败了也不要慌。按照给出的错误提示链接点击进去下载相应的虚拟化软件包安装即可。 后续会在文档末尾提供相关的软件包，我的电脑不是基于ARM架构的，所以需要ARM架构安装包的同学根据错误提示链接自行下载安装即可。 3.干一波 拉个nginx试试 看着没啥毛病 over！ 4.配置源不开vpn拉镜像慢？小意思，操作一波 12345678910111213141516171819&#123; &quot;registry-mirrors&quot;: [ &quot;https://pb5bklzr.mirror.aliyuncs.com&quot;, &quot;https://hub-mirror.c.163.com&quot;, &quot;https://docker.mirrors.ustc.edu.cn&quot; ], &quot;insecure-registries&quot;: [], &quot;debug&quot;: false, &quot;experimental&quot;: false, &quot;features&quot;: &#123; &quot;buildkit&quot;: true &#125;, &quot;builder&quot;: &#123; &quot;gc&quot;: &#123; &quot;enabled&quot;: true, &quot;defaultKeepStorage&quot;: &quot;20GB&quot; &#125; &#125;&#125; 二，Linux下安装Docker文档1.安装软件包1yum install -y yum-utils device-mapper-persistent-data lvm2 2.设置镜像仓库1yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 3.更新yum索引包1yum makecache fast 4.安装docker CE（社区版） EE 旗舰版=收费1yum -y install docker-ce 5.启动docker1systemctl start docker 6.配置阿里云加速器123456789mkdir -p /etc/dockervim /etc/docker/daemon.json ---------------------------&#123;&quot;registry-mirrors&quot;: [&quot;https://dx4hocp4.mirror.aliyuncs.com&quot;]&#125;--------------------------systemctl daemon-reloadsystemctl restart docker 7.docker可视化界面Portainer**https://documentation.portainer.io/** 123456# 服务端部署docker run -d -p 8000:8000 -p 9000:9000 --name=portainer --restart=always -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer-ce# 访问 9000 端口即可#agent端部署docker run -d -p 9001:9001 --name portainer_agent --restart=always -v /var/run/docker.sock:/var/run/docker.sock -v /var/lib/docker/volumes:/var/lib/docker/volumes portainer/agent 三，使用docker快速搭建环境1.docker安装MySQL123456789101112131415161718192021222324252627282930313233343536# 从远程拉取MySQL的镜像docker pull mysql:5.7# 运行拉下来的镜像docker run -p 3306:3306 --name ershi-mysql -e MYSQL_ROOT_PASSWORD=root -d mysql:5.7# 已交互式的方式进入运行中的容器docker exec -it mysql的容器ID bin/bash#一般docker容器都是基于ubuntu系统的，因为更加轻量级，所以进入以后记得先更新源apt update# 安装vim，不安装也无所谓，个人强迫症apt install vim #编辑MySQL的配置文件vim /etc/mysql/my.cnf加入如下代码块[client]default_character_set=utf8[mysqld]collation_server = utf8_general_cicharacter_set_server = utf8:wq!# 退出容器ctrl + P + Q# 重启MySQL容器docker restart mysql的容器ID连接上mysql# 查看字符编码SHOW VARIABLES LIKE &#x27;character%&#x27;;# 给用户设置权限和密码GRANT ALL PRIVILEGES ON *.* TO root@&#x27;%&#x27; IDENTIFIED BY &quot;root&quot;;# 刷新FLUSH PRIVILEGES;over！ 2.docker安装rabbitmq并安装死信队列插件1234567891011121314151617181920#拉取镜像docker pull rabbitmq#展示镜像docker images# 后台运行mq镜像生成实例docker run -d -p 5672:5672 -p 15672:15672 --name myrabbitmq 镜像ID# 查看运行中的镜像列表docker ps# 交互模式进入docker容器docker exec -it 容器ID bin/bash# 开启rabbitmq控制台插件命令rabbitmq-plugins enable rabbitmq_management# 进入插件目录下cd plugins# 下载插件到当前目录下wget https://github.com/rabbitmq/rabbitmq-delayed-message-exchange/releases/download/v3.8.0/rabbitmq_delayed_message_exchange-3.8.0.ez# 开启插件rabbitmq-plugins enable rabbitmq_delayed_message_exchange# 退出容器ctrl + P + Q 3.docker安装redis1234567891011# 拉取镜像docker pull redis# 查看镜像docker images# 本地搞一个redis的配置文件http://download.redis.io/redis-stable/redis.conf# 比如说这个文件在本地的路径为 c://root/software/redis/docker/conf# 将镜像作为容器运行,具体解释# -v 挂载目录，规则与端口映射相同# redis-server /etc/redis/redis.conf 以配置文件启动redis，加载容器内的conf文件，最终找到的是挂载的目录/usr/local/docker/redis.confdocker run -itd -p 6379:6379 --name ershi-redis -v c://root/software/redis/docker/conf/redis.conf:/etc/redis/redis.conf redis redis-server /etc/redis/redis.conf 4.docker安装ElasticSearch123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172# 安装es#拉取镜像docker pull elasticsearch:7.7.0#启动镜像docker run --name elasticsearch -d -e ES_JAVA_OPTS=&quot;-Xms512m -Xmx512m&quot; -e &quot;discovery.type=single-node&quot; -p 9200:9200 -p 9300:9300 elasticsearch:7.7.0访问localhost:9200 出现json串表示安装成功# 安装elasticsearch-head#拉取镜像docker pull mobz/elasticsearch-head:5#创建容器docker create --name elasticsearch-head -p 9100:9100 mobz/elasticsearch-head:5#启动容器docker start elasticsearch-headordocker start 容器id （docker ps -a 查看容器id ）# 访问localhost:9100 出现插件管理控制台 说明安装成功尝试连接easticsearch会发现无法连接上，由于是前后端分离开发，所以会存在跨域问题，需要在服务端做CORS的配置。# 解决办法：修改docker中elasticsearch的elasticsearch.yml文件docker exec -it elasticsearch /bin/bash （进不去使用容器id进入）vi config/elasticsearch.yml# 在最下面添加2行http.cors.enabled: true http.cors.allow-origin: &quot;*&quot;# 退出并重启exitdocker restart 容器id# 访问localhost:9100 可以连接成功# bug修复 ElasticSearch-head 操作时不修改配置，默认会报 406错误码# 进入es-haed 插件容器docker exec -it elasticsearch-head /bin/bash# 编辑他的前端交互js文件vim /usr/src/app/_site/vendor.js第6886行：把 application/x-www-form-urlencoded 改成 application/json;charset=UTF-8第7574行：把 application/x-www-form-urlencoded 改成 application/json;charset=UTF-8#保存退出，重启容器，成功查询到es数据# 安装ik分词器docker exec -it elasticsearch /bin/bash （进不去使用容器id进入）cd /usr/share/elasticsearch/pluginsmkdir ikcd ik wget https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.7.0/elasticsearch-analysis-ik-7.7.0.zip#解压unzip elasticsearch-analysis-ik-7.7.0.zip#删除压缩包rm -rf elasticsearch-analysis-ik-7.7.0.zip 5.docker安装nginx12345docker pull nginxdocker imagesdocker run -itd -p 80:80 --name ershi-nginx imageID 6.docker搭建RocketMQ和控制台123456789101112131415161718#创建RocketMQ使用的共有网络，便于相互访问docker network create rocketmq_network #foxiswho/rocketmq 4.7.0以后不再分别创建broker及nameserver的镜像，统一使用rocketmq镜像，只是在启动命令上区分docker pull foxiswho/rocketmq:4.8.0#rocketmq控制台2.0.0版本，源码来自于官方仓库https://github.com/apache/rocketmq-externals#rocketmq-consoledocker pull 56553655/rocketmq-console-ng:2.0.0 #启动rocketmq nameserverdocker run -d --network rocketmq_network --network-alias rmqnamesrv --name rmqnamesrv -e &quot;JAVA_OPT_EXT=-Xms512M -Xmx512M -Xmn128m&quot; -p 9876:9876 foxiswho/rocketmq:4.8.0 sh mqnamesrv #启动rocketmq brokerdocker run -d --network rocketmq_network --network-alias rmqbroker --name rmqbroker -e &quot;NAMESRV_ADDR=rmqnamesrv:9876&quot; -e &quot;JAVA_OPT_EXT=-Xms512M -Xmx512M -Xmn128m&quot; -p 10911:10911 -p 10912:10912 -p 10909:10909 foxiswho/rocketmq:4.8.0 sh mqbroker #启动rocketmq-console-ngdocker run -d --network rocketmq_network --network-alias rocketmq-console-ng -p 8080:8080 --name rocketmq-console-ng 56553655/rocketmq-console-ng:2.0.0# 访问localhost:8080 成功出现控制台 over！ 7.docker安装xxl-job1234567docker pull xuxueli/xxl-job-admin:2.2.0docker stop xxl-job-admin &amp;&amp; docker rm xxl-job-admin## 执行数据库脚本，脚本在下面的代码块docker run -e PARAMS=&quot;--spring.datasource.url=jdbc:mysql://localhost:3316/xxl_job?Unicode=true&amp;characterEncoding=UTF-8 --spring.datasource.username=root --spring.datasource.password=root&quot; -p 8000:8080 --name xxl-job-admin -d xuxueli/xxl-job-admin:2.2.0#访问 localhost:8000/xxl-job-admin/ admin 123456 数据库脚本 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117CREATE DATABASE IF NOT EXISTS `xxl_job` DEFAULT CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;USE `xxl_job`;SET NAMES utf8mb4;CREATE TABLE `xxl_job_info` ( `id` INT(11) NOT NULL AUTO_INCREMENT, `job_group` INT(11) NOT NULL COMMENT &#x27;执行器主键ID&#x27;, `job_desc` VARCHAR(255) NOT NULL, `add_time` DATETIME DEFAULT NULL, `update_time` DATETIME DEFAULT NULL, `author` VARCHAR(64) DEFAULT NULL COMMENT &#x27;作者&#x27;, `alarm_email` VARCHAR(255) DEFAULT NULL COMMENT &#x27;报警邮件&#x27;, `schedule_type` VARCHAR(50) NOT NULL DEFAULT &#x27;NONE&#x27; COMMENT &#x27;调度类型&#x27;, `schedule_conf` VARCHAR(128) DEFAULT NULL COMMENT &#x27;调度配置，值含义取决于调度类型&#x27;, `misfire_strategy` VARCHAR(50) NOT NULL DEFAULT &#x27;DO_NOTHING&#x27; COMMENT &#x27;调度过期策略&#x27;, `executor_route_strategy` VARCHAR(50) DEFAULT NULL COMMENT &#x27;执行器路由策略&#x27;, `executor_handler` VARCHAR(255) DEFAULT NULL COMMENT &#x27;执行器任务handler&#x27;, `executor_param` VARCHAR(512) DEFAULT NULL COMMENT &#x27;执行器任务参数&#x27;, `executor_block_strategy` VARCHAR(50) DEFAULT NULL COMMENT &#x27;阻塞处理策略&#x27;, `executor_timeout` INT(11) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;任务执行超时时间，单位秒&#x27;, `executor_fail_retry_count` INT(11) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;失败重试次数&#x27;, `glue_type` VARCHAR(50) NOT NULL COMMENT &#x27;GLUE类型&#x27;, `glue_source` MEDIUMTEXT COMMENT &#x27;GLUE源代码&#x27;, `glue_remark` VARCHAR(128) DEFAULT NULL COMMENT &#x27;GLUE备注&#x27;, `glue_updatetime` DATETIME DEFAULT NULL COMMENT &#x27;GLUE更新时间&#x27;, `child_jobid` VARCHAR(255) DEFAULT NULL COMMENT &#x27;子任务ID，多个逗号分隔&#x27;, `trigger_status` TINYINT(4) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;调度状态：0-停止，1-运行&#x27;, `trigger_last_time` BIGINT(13) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;上次调度时间&#x27;, `trigger_next_time` BIGINT(13) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;下次调度时间&#x27;, PRIMARY KEY (`id`)) ENGINE=INNODB DEFAULT CHARSET=utf8mb4;CREATE TABLE `xxl_job_log` ( `id` BIGINT(20) NOT NULL AUTO_INCREMENT, `job_group` INT(11) NOT NULL COMMENT &#x27;执行器主键ID&#x27;, `job_id` INT(11) NOT NULL COMMENT &#x27;任务，主键ID&#x27;, `executor_address` VARCHAR(255) DEFAULT NULL COMMENT &#x27;执行器地址，本次执行的地址&#x27;, `executor_handler` VARCHAR(255) DEFAULT NULL COMMENT &#x27;执行器任务handler&#x27;, `executor_param` VARCHAR(512) DEFAULT NULL COMMENT &#x27;执行器任务参数&#x27;, `executor_sharding_param` VARCHAR(20) DEFAULT NULL COMMENT &#x27;执行器任务分片参数，格式如 1/2&#x27;, `executor_fail_retry_count` INT(11) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;失败重试次数&#x27;, `trigger_time` DATETIME DEFAULT NULL COMMENT &#x27;调度-时间&#x27;, `trigger_code` INT(11) NOT NULL COMMENT &#x27;调度-结果&#x27;, `trigger_msg` TEXT COMMENT &#x27;调度-日志&#x27;, `handle_time` DATETIME DEFAULT NULL COMMENT &#x27;执行-时间&#x27;, `handle_code` INT(11) NOT NULL COMMENT &#x27;执行-状态&#x27;, `handle_msg` TEXT COMMENT &#x27;执行-日志&#x27;, `alarm_status` TINYINT(4) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;告警状态：0-默认、1-无需告警、2-告警成功、3-告警失败&#x27;, PRIMARY KEY (`id`), KEY `I_trigger_time` (`trigger_time`), KEY `I_handle_code` (`handle_code`)) ENGINE=INNODB DEFAULT CHARSET=utf8mb4;CREATE TABLE `xxl_job_log_report` ( `id` INT(11) NOT NULL AUTO_INCREMENT, `trigger_day` DATETIME DEFAULT NULL COMMENT &#x27;调度-时间&#x27;, `running_count` INT(11) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;运行中-日志数量&#x27;, `suc_count` INT(11) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;执行成功-日志数量&#x27;, `fail_count` INT(11) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;执行失败-日志数量&#x27;, `update_time` DATETIME DEFAULT NULL, PRIMARY KEY (`id`), UNIQUE KEY `i_trigger_day` (`trigger_day`) USING BTREE) ENGINE=INNODB DEFAULT CHARSET=utf8mb4;CREATE TABLE `xxl_job_logglue` ( `id` INT(11) NOT NULL AUTO_INCREMENT, `job_id` INT(11) NOT NULL COMMENT &#x27;任务，主键ID&#x27;, `glue_type` VARCHAR(50) DEFAULT NULL COMMENT &#x27;GLUE类型&#x27;, `glue_source` MEDIUMTEXT COMMENT &#x27;GLUE源代码&#x27;, `glue_remark` VARCHAR(128) NOT NULL COMMENT &#x27;GLUE备注&#x27;, `add_time` DATETIME DEFAULT NULL, `update_time` DATETIME DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=INNODB DEFAULT CHARSET=utf8mb4;CREATE TABLE `xxl_job_registry` ( `id` INT(11) NOT NULL AUTO_INCREMENT, `registry_group` VARCHAR(50) NOT NULL, `registry_key` VARCHAR(255) NOT NULL, `registry_value` VARCHAR(255) NOT NULL, `update_time` DATETIME DEFAULT NULL, PRIMARY KEY (`id`), KEY `i_g_k_v` (`registry_group`,`registry_key`,`registry_value`)) ENGINE=INNODB DEFAULT CHARSET=utf8mb4;CREATE TABLE `xxl_job_group` ( `id` INT(11) NOT NULL AUTO_INCREMENT, `app_name` VARCHAR(64) NOT NULL COMMENT &#x27;执行器AppName&#x27;, `title` VARCHAR(12) NOT NULL COMMENT &#x27;执行器名称&#x27;, `address_type` TINYINT(4) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;执行器地址类型：0=自动注册、1=手动录入&#x27;, `address_list` TEXT COMMENT &#x27;执行器地址列表，多地址逗号分隔&#x27;, `update_time` DATETIME DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=INNODB DEFAULT CHARSET=utf8mb4;CREATE TABLE `xxl_job_user` ( `id` INT(11) NOT NULL AUTO_INCREMENT, `username` VARCHAR(50) NOT NULL COMMENT &#x27;账号&#x27;, `password` VARCHAR(50) NOT NULL COMMENT &#x27;密码&#x27;, `role` TINYINT(4) NOT NULL COMMENT &#x27;角色：0-普通用户、1-管理员&#x27;, `permission` VARCHAR(255) DEFAULT NULL COMMENT &#x27;权限：执行器ID列表，多个逗号分割&#x27;, PRIMARY KEY (`id`), UNIQUE KEY `i_username` (`username`) USING BTREE) ENGINE=INNODB DEFAULT CHARSET=utf8mb4;CREATE TABLE `xxl_job_lock` ( `lock_name` VARCHAR(50) NOT NULL COMMENT &#x27;锁名称&#x27;, PRIMARY KEY (`lock_name`)) ENGINE=INNODB DEFAULT CHARSET=utf8mb4;INSERT INTO `xxl_job_group`(`id`, `app_name`, `title`, `address_type`, `address_list`, `update_time`) VALUES (1, &#x27;xxl-job-executor-sample&#x27;, &#x27;示例执行器&#x27;, 0, NULL, &#x27;2018-11-03 22:21:31&#x27; );INSERT INTO `xxl_job_info`(`id`, `job_group`, `job_desc`, `add_time`, `update_time`, `author`, `alarm_email`, `schedule_type`, `schedule_conf`, `misfire_strategy`, `executor_route_strategy`, `executor_handler`, `executor_param`, `executor_block_strategy`, `executor_timeout`, `executor_fail_retry_count`, `glue_type`, `glue_source`, `glue_remark`, `glue_updatetime`, `child_jobid`) VALUES (1, 1, &#x27;测试任务1&#x27;, &#x27;2018-11-03 22:21:31&#x27;, &#x27;2018-11-03 22:21:31&#x27;, &#x27;XXL&#x27;, &#x27;&#x27;, &#x27;CRON&#x27;, &#x27;0 0 0 * * ? *&#x27;, &#x27;DO_NOTHING&#x27;, &#x27;FIRST&#x27;, &#x27;demoJobHandler&#x27;, &#x27;&#x27;, &#x27;SERIAL_EXECUTION&#x27;, 0, 0, &#x27;BEAN&#x27;, &#x27;&#x27;, &#x27;GLUE代码初始化&#x27;, &#x27;2018-11-03 22:21:31&#x27;, &#x27;&#x27;);INSERT INTO `xxl_job_user`(`id`, `username`, `password`, `role`, `permission`) VALUES (1, &#x27;admin&#x27;, &#x27;e10adc3949ba59abbe56e057f20f883e&#x27;, 1, NULL);INSERT INTO `xxl_job_lock` ( `lock_name`) VALUES ( &#x27;schedule_lock&#x27;);COMMIT; 8.docker安装nacos123456789docker pull nacos/nacos-server # 往数据库导入nacos脚本# 运行容器docker run -d -e MODE=standalone -e SPRING_DATASOURCE_PLATFORM=mysql -e MYSQL_SERVICE_HOST=192.168.1.6 -e MYSQL_SERVICE_PORT=3316 -e MYSQL_SERVICE_USER=root -e MYSQL_SERVICE_PASSWORD=root -e MYSQL_SERVICE_DB_NAME=nacos_test -p 8848:8848 --restart=always --name nacos nacos/nacos-server 数据库脚本 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225create database nacos_test;use nacos_test;CREATE TABLE `config_info` ( `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT &#x27;id&#x27;, `data_id` varchar(255) COLLATE utf8_bin NOT NULL COMMENT &#x27;data_id&#x27;, `group_id` varchar(255) COLLATE utf8_bin DEFAULT NULL, `content` longtext COLLATE utf8_bin NOT NULL COMMENT &#x27;content&#x27;, `md5` varchar(32) COLLATE utf8_bin DEFAULT NULL COMMENT &#x27;md5&#x27;, `gmt_create` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &#x27;创建时间&#x27;, `gmt_modified` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &#x27;修改时间&#x27;, `src_user` text COLLATE utf8_bin COMMENT &#x27;source user&#x27;, `src_ip` varchar(20) COLLATE utf8_bin DEFAULT NULL COMMENT &#x27;source ip&#x27;, `app_name` varchar(128) COLLATE utf8_bin DEFAULT NULL, `tenant_id` varchar(128) COLLATE utf8_bin DEFAULT &#x27;&#x27; COMMENT &#x27;租户字段&#x27;, `c_desc` varchar(256) COLLATE utf8_bin DEFAULT NULL, `c_use` varchar(64) COLLATE utf8_bin DEFAULT NULL, `effect` varchar(64) COLLATE utf8_bin DEFAULT NULL, `type` varchar(64) COLLATE utf8_bin DEFAULT NULL, `c_schema` text COLLATE utf8_bin, PRIMARY KEY (`id`), UNIQUE KEY `uk_configinfo_datagrouptenant` (`data_id`,`group_id`,`tenant_id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT=&#x27;config_info&#x27;;# Dump of table config_info_aggr# ------------------------------------------------------------CREATE TABLE `config_info_aggr` ( `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT &#x27;id&#x27;, `data_id` varchar(255) COLLATE utf8_bin NOT NULL COMMENT &#x27;data_id&#x27;, `group_id` varchar(255) COLLATE utf8_bin NOT NULL COMMENT &#x27;group_id&#x27;, `datum_id` varchar(255) COLLATE utf8_bin NOT NULL COMMENT &#x27;datum_id&#x27;, `content` longtext COLLATE utf8_bin NOT NULL COMMENT &#x27;内容&#x27;, `gmt_modified` datetime NOT NULL COMMENT &#x27;修改时间&#x27;, `app_name` varchar(128) COLLATE utf8_bin DEFAULT NULL, `tenant_id` varchar(128) COLLATE utf8_bin DEFAULT &#x27;&#x27; COMMENT &#x27;租户字段&#x27;, PRIMARY KEY (`id`), UNIQUE KEY `uk_configinfoaggr_datagrouptenantdatum` (`data_id`,`group_id`,`tenant_id`,`datum_id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT=&#x27;增加租户字段&#x27;;# Dump of table config_info_beta# ------------------------------------------------------------CREATE TABLE `config_info_beta` ( `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT &#x27;id&#x27;, `data_id` varchar(255) COLLATE utf8_bin NOT NULL COMMENT &#x27;data_id&#x27;, `group_id` varchar(128) COLLATE utf8_bin NOT NULL COMMENT &#x27;group_id&#x27;, `app_name` varchar(128) COLLATE utf8_bin DEFAULT NULL COMMENT &#x27;app_name&#x27;, `content` longtext COLLATE utf8_bin NOT NULL COMMENT &#x27;content&#x27;, `beta_ips` varchar(1024) COLLATE utf8_bin DEFAULT NULL COMMENT &#x27;betaIps&#x27;, `md5` varchar(32) COLLATE utf8_bin DEFAULT NULL COMMENT &#x27;md5&#x27;, `gmt_create` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &#x27;创建时间&#x27;, `gmt_modified` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &#x27;修改时间&#x27;, `src_user` text COLLATE utf8_bin COMMENT &#x27;source user&#x27;, `src_ip` varchar(20) COLLATE utf8_bin DEFAULT NULL COMMENT &#x27;source ip&#x27;, `tenant_id` varchar(128) COLLATE utf8_bin DEFAULT &#x27;&#x27; COMMENT &#x27;租户字段&#x27;, PRIMARY KEY (`id`), UNIQUE KEY `uk_configinfobeta_datagrouptenant` (`data_id`,`group_id`,`tenant_id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT=&#x27;config_info_beta&#x27;;# Dump of table config_info_tag# ------------------------------------------------------------CREATE TABLE `config_info_tag` ( `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT &#x27;id&#x27;, `data_id` varchar(255) COLLATE utf8_bin NOT NULL COMMENT &#x27;data_id&#x27;, `group_id` varchar(128) COLLATE utf8_bin NOT NULL COMMENT &#x27;group_id&#x27;, `tenant_id` varchar(128) COLLATE utf8_bin DEFAULT &#x27;&#x27; COMMENT &#x27;tenant_id&#x27;, `tag_id` varchar(128) COLLATE utf8_bin NOT NULL COMMENT &#x27;tag_id&#x27;, `app_name` varchar(128) COLLATE utf8_bin DEFAULT NULL COMMENT &#x27;app_name&#x27;, `content` longtext COLLATE utf8_bin NOT NULL COMMENT &#x27;content&#x27;, `md5` varchar(32) COLLATE utf8_bin DEFAULT NULL COMMENT &#x27;md5&#x27;, `gmt_create` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &#x27;创建时间&#x27;, `gmt_modified` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &#x27;修改时间&#x27;, `src_user` text COLLATE utf8_bin COMMENT &#x27;source user&#x27;, `src_ip` varchar(20) COLLATE utf8_bin DEFAULT NULL COMMENT &#x27;source ip&#x27;, PRIMARY KEY (`id`), UNIQUE KEY `uk_configinfotag_datagrouptenanttag` (`data_id`,`group_id`,`tenant_id`,`tag_id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT=&#x27;config_info_tag&#x27;;# Dump of table config_tags_relation# ------------------------------------------------------------CREATE TABLE `config_tags_relation` ( `id` bigint(20) NOT NULL COMMENT &#x27;id&#x27;, `tag_name` varchar(128) COLLATE utf8_bin NOT NULL COMMENT &#x27;tag_name&#x27;, `tag_type` varchar(64) COLLATE utf8_bin DEFAULT NULL COMMENT &#x27;tag_type&#x27;, `data_id` varchar(255) COLLATE utf8_bin NOT NULL COMMENT &#x27;data_id&#x27;, `group_id` varchar(128) COLLATE utf8_bin NOT NULL COMMENT &#x27;group_id&#x27;, `tenant_id` varchar(128) COLLATE utf8_bin DEFAULT &#x27;&#x27; COMMENT &#x27;tenant_id&#x27;, `nid` bigint(20) NOT NULL AUTO_INCREMENT, PRIMARY KEY (`nid`), UNIQUE KEY `uk_configtagrelation_configidtag` (`id`,`tag_name`,`tag_type`), KEY `idx_tenant_id` (`tenant_id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT=&#x27;config_tag_relation&#x27;;# Dump of table group_capacity# ------------------------------------------------------------CREATE TABLE `group_capacity` ( `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT COMMENT &#x27;主键ID&#x27;, `group_id` varchar(128) COLLATE utf8_bin NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;Group ID，空字符表示整个集群&#x27;, `quota` int(10) unsigned NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;配额，0表示使用默认值&#x27;, `usage` int(10) unsigned NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;使用量&#x27;, `max_size` int(10) unsigned NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;单个配置大小上限，单位为字节，0表示使用默认值&#x27;, `max_aggr_count` int(10) unsigned NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;聚合子配置最大个数，，0表示使用默认值&#x27;, `max_aggr_size` int(10) unsigned NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;单个聚合数据的子配置大小上限，单位为字节，0表示使用默认值&#x27;, `max_history_count` int(10) unsigned NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;最大变更历史数量&#x27;, `gmt_create` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &#x27;创建时间&#x27;, `gmt_modified` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &#x27;修改时间&#x27;, PRIMARY KEY (`id`), UNIQUE KEY `uk_group_id` (`group_id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT=&#x27;集群、各Group容量信息表&#x27;;# Dump of table his_config_info# ------------------------------------------------------------CREATE TABLE `his_config_info` ( `id` bigint(64) unsigned NOT NULL, `nid` bigint(20) unsigned NOT NULL AUTO_INCREMENT, `data_id` varchar(255) COLLATE utf8_bin NOT NULL, `group_id` varchar(128) COLLATE utf8_bin NOT NULL, `app_name` varchar(128) COLLATE utf8_bin DEFAULT NULL COMMENT &#x27;app_name&#x27;, `content` longtext COLLATE utf8_bin NOT NULL, `md5` varchar(32) COLLATE utf8_bin DEFAULT NULL, `gmt_create` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP, `gmt_modified` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP, `src_user` text COLLATE utf8_bin, `src_ip` varchar(20) COLLATE utf8_bin DEFAULT NULL, `op_type` char(10) COLLATE utf8_bin DEFAULT NULL, `tenant_id` varchar(128) COLLATE utf8_bin DEFAULT &#x27;&#x27; COMMENT &#x27;租户字段&#x27;, PRIMARY KEY (`nid`), KEY `idx_gmt_create` (`gmt_create`), KEY `idx_gmt_modified` (`gmt_modified`), KEY `idx_did` (`data_id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT=&#x27;多租户改造&#x27;;# Dump of table permissions# ------------------------------------------------------------CREATE TABLE `permissions` ( `role` varchar(50) NOT NULL, `resource` varchar(255) NOT NULL, `action` varchar(8) NOT NULL, UNIQUE KEY `uk_role_permission` (`role`,`resource`,`action`) USING BTREE) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;# Dump of table roles# ------------------------------------------------------------CREATE TABLE `roles` ( `username` varchar(50) NOT NULL, `role` varchar(50) NOT NULL, UNIQUE KEY `idx_user_role` (`username`,`role`) USING BTREE) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;# Dump of table tenant_capacity# ------------------------------------------------------------CREATE TABLE `tenant_capacity` ( `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT COMMENT &#x27;主键ID&#x27;, `tenant_id` varchar(128) COLLATE utf8_bin NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;Tenant ID&#x27;, `quota` int(10) unsigned NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;配额，0表示使用默认值&#x27;, `usage` int(10) unsigned NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;使用量&#x27;, `max_size` int(10) unsigned NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;单个配置大小上限，单位为字节，0表示使用默认值&#x27;, `max_aggr_count` int(10) unsigned NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;聚合子配置最大个数&#x27;, `max_aggr_size` int(10) unsigned NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;单个聚合数据的子配置大小上限，单位为字节，0表示使用默认值&#x27;, `max_history_count` int(10) unsigned NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;最大变更历史数量&#x27;, `gmt_create` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &#x27;创建时间&#x27;, `gmt_modified` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &#x27;修改时间&#x27;, PRIMARY KEY (`id`), UNIQUE KEY `uk_tenant_id` (`tenant_id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT=&#x27;租户容量信息表&#x27;;# Dump of table tenant_info# ------------------------------------------------------------CREATE TABLE `tenant_info` ( `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT &#x27;id&#x27;, `kp` varchar(128) COLLATE utf8_bin NOT NULL COMMENT &#x27;kp&#x27;, `tenant_id` varchar(128) COLLATE utf8_bin DEFAULT &#x27;&#x27; COMMENT &#x27;tenant_id&#x27;, `tenant_name` varchar(128) COLLATE utf8_bin DEFAULT &#x27;&#x27; COMMENT &#x27;tenant_name&#x27;, `tenant_desc` varchar(256) COLLATE utf8_bin DEFAULT NULL COMMENT &#x27;tenant_desc&#x27;, `create_source` varchar(32) COLLATE utf8_bin DEFAULT NULL COMMENT &#x27;create_source&#x27;, `gmt_create` bigint(20) NOT NULL COMMENT &#x27;创建时间&#x27;, `gmt_modified` bigint(20) NOT NULL COMMENT &#x27;修改时间&#x27;, PRIMARY KEY (`id`), UNIQUE KEY `uk_tenant_info_kptenantid` (`kp`,`tenant_id`), KEY `idx_tenant_id` (`tenant_id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT=&#x27;tenant_info&#x27;;# Dump of table users# ------------------------------------------------------------CREATE TABLE `users` ( `username` varchar(50) NOT NULL, `password` varchar(500) NOT NULL, `enabled` tinyint(1) NOT NULL, PRIMARY KEY (`username`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;INSERT INTO `users` (`username`, `password`, `enabled`)VALUES (&#x27;nacos&#x27;, &#x27;$2a$10$EuWPZHzz32dJN7jexM34MOeYirDdFAZm2kuWj7VEOJhhZkDrxfvUu&#x27;, 1); 9.docker安装sentinel123docker pull bladex/sentinel-dashboarddocker run --name sentinel -d -p 8858:8858 -d bladex/sentinel-dashboard 10.docker安装zipkin123docker pull openzipkin/zipkindocker run -d --restart always -p 9411:9411 --name zipkin openzipkin/zipkin 11.docker安装mongo123docker pull mongo:latestdocker run -itd --name mongo -p 27017:27017 mongo 12.docker安装zookeeper123docker pull zookeeperdocker run --privileged=true -d --name zookeeper --publish 2181:2181 -d zookeeper:latest 13.docker安装seata123456789101112131415docker pull seata docker run -d --name seata-server -p 8091:8091 seataio/seata-server:1.2.0docker exec -it seataio/seata-server:1.2.0 sh# 创建数据库vim file.conf 修改模式为db 修改数据库连接信息 vim registry.conf 将seata 注册进nacos中docker restart seataio/seata-server:1.2.0 shdocker logs -f seataio/seata-server:1.2.0 sh 数据库脚本 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556create database seata;use seata;CREATE TABLE IF NOT EXISTS `global_table`( `xid` VARCHAR(128) NOT NULL, `transaction_id` BIGINT, `status` TINYINT NOT NULL, `application_id` VARCHAR(32), `transaction_service_group` VARCHAR(32), `transaction_name` VARCHAR(128), `timeout` INT, `begin_time` BIGINT, `application_data` VARCHAR(2000), `gmt_create` DATETIME, `gmt_modified` DATETIME, PRIMARY KEY (`xid`), KEY `idx_gmt_modified_status` (`gmt_modified`, `status`), KEY `idx_transaction_id` (`transaction_id`)) ENGINE = InnoDB DEFAULT CHARSET = utf8;-- the table to store BranchSession dataCREATE TABLE IF NOT EXISTS `branch_table`( `branch_id` BIGINT NOT NULL, `xid` VARCHAR(128) NOT NULL, `transaction_id` BIGINT, `resource_group_id` VARCHAR(32), `resource_id` VARCHAR(256), `branch_type` VARCHAR(8), `status` TINYINT, `client_id` VARCHAR(64), `application_data` VARCHAR(2000), `gmt_create` DATETIME(6), `gmt_modified` DATETIME(6), PRIMARY KEY (`branch_id`), KEY `idx_xid` (`xid`)) ENGINE = InnoDB DEFAULT CHARSET = utf8;-- the table to store lock dataCREATE TABLE IF NOT EXISTS `lock_table`( `row_key` VARCHAR(128) NOT NULL, `xid` VARCHAR(96), `transaction_id` BIGINT, `branch_id` BIGINT NOT NULL, `resource_id` VARCHAR(256), `table_name` VARCHAR(32), `pk` VARCHAR(36), `gmt_create` DATETIME, `gmt_modified` DATETIME, PRIMARY KEY (`row_key`), KEY `idx_branch_id` (`branch_id`)) ENGINE = InnoDB DEFAULT CHARSET = utf8; 14.docker安装Apache-abTest1234docker run --rm russmckendrick/ab -v -k -n 10000 -c 30 http://10.225.20.87:9006/order-server/order/add/1/1请求数 和 并发数 ab -n1000 -c100 http://10.225.20.87:9006/order-server/order/add/1/1","categories":[{"name":"云计算","slug":"云计算","permalink":"https://yinhuidong.github.io/categories/%E4%BA%91%E8%AE%A1%E7%AE%97/"}],"tags":[{"name":"云计算","slug":"云计算","permalink":"https://yinhuidong.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"}]},{"title":"Docker概念与命令","slug":"云计算/Docker概念与命令","date":"2022-01-12T00:19:55.251Z","updated":"2022-01-12T00:19:55.251Z","comments":true,"path":"2022/01/12/云计算/Docker概念与命令/","link":"","permalink":"https://yinhuidong.github.io/2022/01/12/%E4%BA%91%E8%AE%A1%E7%AE%97/Docker%E6%A6%82%E5%BF%B5%E4%B8%8E%E5%91%BD%E4%BB%A4/","excerpt":"","text":"一，docker 三要素1.镜像/容器12Person person = new Person();Person person2 = new Person(); Docker镜像就是一个只读的模板，镜像可以创建Docker容器，一个镜像可以创建很多容器。 镜像：java类 ||| 容器：java对象 Docker是利用容器运行的一个或者一组应用。容器是使用镜像创建的运行实例。 他可以被启动，开始，停止，删除。每个容器都是相互隔离的，保障安全的平台。 可以把容器看作是一个简易版的linux环境和运行在其中的应用程序。 2.仓库集中存放镜像文件的场所。 仓库和仓库注册服务器是有区别的。仓库注册服务器上往往存放着多个仓库，每个仓库有很多的镜像。 每个镜像有不同的标签。 仓库分为公开库和私有库。 最大的公开库是docker Hub。 国内公开库包括阿里云，网易云。 3.总结Docker本身是一个容器运行载体或称为管理引擎。把应用程序和配置依赖打包好形成一个可以交付的运行环境，这个打包好 的运行环境就是一个image镜像文件。只有通过这个镜像文件才能生成Docker容器，image文件可以看做是容器模板，Docker根据image文件生成容器的实例。同一个image文件，可以生成多个同时运行的容器实例。 一个容器运行一种服务，需要的时候，就可以通过docker客户端创建一个对应的运行实例，也就是容器。 至于仓储，也就是存了一堆镜像的地方，可以把镜像发布到仓储中，需要的时候从仓储中拉下来就可以了。 二，docker-hello1.查看docker 运行状态12ps -ef |grep dockerdocker run helloworld 2.docker-helloworld执行流程 先在本机寻找该镜像 如果有，以镜像为模板生产容器实例运行 如果本机没有，去阿里云上查找该镜像 如果找到了，下载这个镜像，以这个镜像为模板生产容器实例运行 如果没找到，返回失败错误，查找不到该镜像 3.docker 运行的底层原理docker是怎么工作的？docker 是一个cs架构的系统，docker守护进程运行在主机上，然后通过socket连接从客户端访问，守护进程从客户端接受命令并管理运行在主机上的容器。容器是一个运行时环境，就是logo的集装箱。 docker 为什么比vm快？1）docker有着比虚拟机更少的抽象层。由于docker不需要硬件资源虚拟化，运行在docker容器上的程序实际上使用的都是实际物理机的硬件资源。因此在cpu，内存利用率上docker将会在效率上有明显优势。 2）docker利用的是宿主机的内核，因此当新建一个容器的时候，docker不需要像虚拟机一样，重新加载一个操作系统内核。docker直接利用宿主机的操作系统，省略了整个过程。 三，docker 帮助命令123docker infodocker --helpdocker [option] COMMAND [args] 四，docker 镜像命令123456789101112docker images 列出本地镜像模板option： -a：列出本地所有镜像（镜像分层概念） -q：只显示镜像ID --digests：显示镜像的摘要信息 --no-trunc：显示完整的镜像信息docker search tomcat |offcial：官方版docker search -s 30 tomcat (点赞数超过30的tomcat)docker pull tomcat ==docker pull tomcat:latestdocker rmi -f centos 强制删除某个镜像docker rmi -f centos nginx强制删除多个镜像docker rmi -f $(docker images -qa) 删除所有镜像 五，docker容器命令12345678910111213141516171819202122232425262728293031323334353637383940新建并启动容器 docker run [options] images [command] -i:交互启动 -t:重新分配一个伪输入终端 -d:后台运行 --name：制定一个名字 docker run -it 831691599b88 执行完直接进入了容器，不信pwd列出当前所有正在运行的容器 docker ps -a：所有 -q：只显示容器编号退出容器 exit 退出并关闭 ctrl+P+Q 容器不停止退出启动容器 docker start 容器名重启容器 docker restart 容器id停止容器 docker stop强制停止容器 docker kill删除已经停止的容器 docker rm 容器id启动守护式容器 docker run -d centos查看容器日志 docker logs -f -t --tail 3 容器id -t 假如时间戳 -f 跟随最新的日志打印 -tail 显示最后多少条查看容器内运行的进程 docker top 30f00b23e1c5查看容器内部细节 docker inspect 容器ID进入正在运行的容器并以命令行交互 docker attach ID docker exec -t ID 命令（不进入容器内部直接执行命令,相当于启动新的进程）从容器内拷贝文件到主机上 docker cp 容器ID：容器内路径 目的主机路径 docker cp xxxxx:/tmp/yum.log /root 六，docker镜像原理1.docker 的本质union 联合文件系统：一种分层，轻量级并且高性能的文件系统，他支持对文件系统的修改作为一次提交 来一层层叠加，union文件系统是docker镜像的基础，镜像可以通过分层来继承，基于基础镜像，可以制作各种具体的应用镜像。 特性：一次同时加载多个文件系统，但是从外面看，只能看见一个文件系统，联合加载会把各层文件系统叠加起来，这样最终的文件系统会包含所有底层的文件和目录。 2.docker 镜像加载原理 docker镜像实际上由一层一层的文件系统组成，这种层级的文件系统UnionFS。 bootfs（boot file system）主要包含bootloader和kerner，bootloader加载引导kerner，linux刚启动会加载bootfs，在docker镜像最底层就是bootfs。这一层与我们典型的Linux系统是一样的，包含boot加载器和内核。当boot加载完成之后整个内核就都在内存中了，此时内存的使用权由bootfs交给内核，系统卸载bootfs。 rootfs在bootfs之上，rootfs就是linux各种不同操作系统的发行版。 对于一个精简的OS，rootfs很小，只需要包含最基本的命令，工具和程序库，底层直接用了宿主机的。 3.为什么这么设置docker最大的一个好处就是资源共享。 比如：有多个镜像都从相同的镜像构建而来，那么宿主机只需要在磁盘上保存一份base镜像就可以，同时内存也需要加载一份，就可以为所有容器服务，而且镜像的每一层都可以被共享。 七，docker镜像commit1234docker commit 提交容器副本让他成为一个新的镜像。docker commit - m=&quot;提交的信息&quot; -a=&quot;作者&quot; 容器ID 要创建的目标镜像名:[标签名]docker run -it -p 8888:8080 tomcat (主机端口:容器端口)docker run -d -p 8888:8080 tomcat 后台运行 本地镜像推送到阿里云 12345678910111213141516171819202122登录阿里云docker管理控制台：https://cr.console.aliyun.com/ 创建命名空间（其实就是仓库）1. 登录阿里云Docker Registry$ docker login --username=尹会东yhd registry.cn-hangzhou.aliyuncs.com用于登录的用户名为阿里云账号全名，密码为开通服务时设置的密码。2. 从Registry中拉取镜像$ docker pull registry.cn-hangzhou.aliyuncs.com/ershi-dev-tools-repository/ershi_rabbitmq:1.03. 将镜像推送到Registry# 登陆阿里云账户docker login --username=尹会东yhd registry.cn-hangzhou.aliyuncs.com# 将某个指定的镜像ID重命名为 阿里云网址/命名空间名/镜像的本地仓库名:tagdocker tag ee70f97dd7fc registry.cn-hangzhou.aliyuncs.com/ershi-dev-tools-repository/ershi_rabbitmq:1.0# 推送 阿里云网址/命名空间名/镜像的本地仓库名:tag 到阿里云docker push registry.cn-hangzhou.aliyuncs.com/ershi-dev-tools-repository/ershi_rabbitmq:1.0 八，数据卷介绍1.docker理念将代码与运行环境打包形成容器运行，运行可以伴随着容器，但是我们对数据的要求是希望持久化 容器之间共享数据 docker容器产生的数据，如果不通过docker commit 生成新的镜像，使得数据作为镜像的一部分保存下来，那么当容器删除后，数据自然也就没有了，为了能保存数据，docker使用卷。 2.数据卷是什么卷就是目录或者文件，存在于一个或者多个容器之中，由docker挂载到容器，但不属于联合文件系统，因此能够绕过UnionFileSystem提供一些用于持续存储或共享数据的特性。 卷的设计目的就是数据的持久化，完全独立于容器的生命周期，因此docker不会再容器删除时删除其挂载的数据卷。 3.特点 数据卷可在容器间共享或重用数据 卷中的更改可以直接生效 数据卷中的更改不会包含在镜像的更新中 数据卷的生命周期一直持续到没有容器使用它为止 4.容器数据卷用V命令添加docker run -it -v /宿主机绝对路径目录 ：/容器内目录 镜像名 主机和容器在卷目录下数据共享 容器停止退出后，主机修改后，数据仍然同步 可以设置权限使容器只能读，不能修改或者新建 5.容器数据卷用dockerFile添加1）是什么JavaEE Hello.java —-&gt;Hello.class Docker images —-&gt;DockerFile 镜像模板的描述文件 2）怎么做 根目录下新建mydocker文件夹并进入 12mkdir mydockercd /mydocker 创建文件dockerFile 1234567vim Dockerfile---------------------FROM centosVOLUME [&quot;/dataVolumeContainer1&quot;,&quot;/dataVolumeContainer1&quot;]CMD echo &quot;fineshed,success!&quot;CMD /bin/bash---------------------- 说明：出于可移植性和分享的考虑，用-v主机目录：容器目录这种方法不能够直接在Dokcerfile中实现。由于宿主机目录是依赖于特定宿主机的，并不能够保证在所有的宿主机上都存在这样的特定目录。 build 生成镜像文件 1docker build -f /mydocker/Dockerfile -t zzyy/centos 启动容器 1docker run zzyy/centos 主机对应目录地址 1docker inspect 容器ID既可以查看 3）容器数据卷命名的容器挂载数据卷，其他容器通过挂载这个容器实现数据共享，挂载数据卷的容器，称之为数据卷容器。","categories":[{"name":"云计算","slug":"云计算","permalink":"https://yinhuidong.github.io/categories/%E4%BA%91%E8%AE%A1%E7%AE%97/"}],"tags":[{"name":"云计算","slug":"云计算","permalink":"https://yinhuidong.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"}]},{"title":"Docker网络模式及容器间网络通信","slug":"云计算/Docker网络模式及容器间网络通信","date":"2022-01-12T00:19:55.251Z","updated":"2022-01-12T00:19:55.251Z","comments":true,"path":"2022/01/12/云计算/Docker网络模式及容器间网络通信/","link":"","permalink":"https://yinhuidong.github.io/2022/01/12/%E4%BA%91%E8%AE%A1%E7%AE%97/Docker%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%BC%8F%E5%8F%8A%E5%AE%B9%E5%99%A8%E9%97%B4%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1/","excerpt":"","text":"1.默认网络安装 Docker 以后，会默认创建三种网络，可以通过 docker network ls 查看。 12345[root@localhost ~]# docker network lsNETWORK ID NAME DRIVER SCOPE688d1970f72e bridge bridge local885da101da7d host host localf4f1b3cf1b7f none null local 在学习 Docker 网络之前，有必要先来了解一下这几种网络模式都是什么意思。 网络模式 简介 bridge 为每一个容器分配、设置 IP 等，并将容器连接到一个 docker0 虚拟网桥，默认为该模式。 host 容器将不会虚拟出自己的网卡，配置自己的 IP 等，而是使用宿主机的 IP 和端口。 none 容器有独立的 Network namespace，但并没有对其进行任何网络设置，如分配 veth pair 和网桥连接，IP 等。 container 新创建的容器不会创建自己的网卡和配置自己的 IP，而是和一个指定的容器共享 IP、端口范围等。 1)bridge网络协议在该模式中，Docker 守护进程创建了一个虚拟以太网桥 docker0，新建的容器会自动桥接到这个接口，附加在其上的任何网卡之间都能自动转发数据包。 默认情况下，守护进程会创建一对对等虚拟设备接口 veth pair，将其中一个接口设置为容器的 eth0 接口（容器的网卡），另一个接口放置在宿主机的命名空间中，以类似 vethxxx 这样的名字命名，从而将宿主机上的所有容器都连接到这个内部网络上。 比如运行一个基于 busybox 镜像构建的容器 bbox01，查看 ip addr： busybox 被称为嵌入式 Linux 的瑞士军刀，整合了很多小的 unix 下的通用功能到一个小的可执行文件中。 然后宿主机通过 ip addr 查看信息如下： 通过以上的比较可以发现，证实了之前所说的：守护进程会创建一对对等虚拟设备接口 veth pair，将其中一个接口设置为容器的 eth0 接口（容器的网卡），另一个接口放置在宿主机的命名空间中，以类似 vethxxx 这样的名字命名。 同时，守护进程还会从网桥 docker0 的私有地址空间中分配一个 IP 地址和子网给该容器，并设置 docker0 的 IP 地址为容器的默认网关。也可以安装 yum install -y bridge-utils 以后，通过 brctl show 命令查看网桥信息。 对于每个容器的 IP 地址和 Gateway 信息，可以通过 docker inspect 容器名称|ID 进行查看，在 NetworkSettings 节点中可以看到详细信息。 可以通过 docker network inspect bridge 查看所有 bridge 网络模式下的容器，在 Containers 节点中可以看到容器名称。 关于 bridge 网络模式的使用，只需要在创建容器时通过参数 --net bridge 或者 --network bridge 指定即可，当然这也是创建容器默认使用的网络模式，也就是说这个参数是可以省略的。 Bridge 桥接模式的实现步骤主要如下： Docker Daemon 利用 veth pair 技术，在宿主机上创建一对对等虚拟网络接口设备，假设为 veth0 和 veth1。而veth pair 技术的特性可以保证无论哪一个 veth 接收到网络报文，都会将报文传输给另一方。 Docker Daemon 将 veth0 附加到 Docker Daemon 创建的 docker0 网桥上。保证宿主机的网络报文可以发往 veth0； Docker Daemon 将 veth1 添加到 Docker Container 所属的 namespace 下，并被改名为 eth0。如此一来，宿主机的网络报文若发往 veth0，则立即会被 Container 的 eth0 接收，实现宿主机到 Docker Container 网络的联通性；同时，也保证 Docker Container 单独使用 eth0，实现容器网络环境的隔离性。 2)Host网络 host 网络模式需要在创建容器时通过参数 --net host 或者 --network host 指定； 采用 host 网络模式的 Docker Container，可以直接使用宿主机的 IP 地址与外界进行通信，若宿主机的 eth0 是一个公有 IP，那么容器也拥有这个公有 IP。同时容器内服务的端口也可以使用宿主机的端口，无需额外进行 NAT 转换； host 网络模式可以让容器共享宿主机网络栈，这样的好处是外部主机与容器直接通信，但是容器的网络缺少隔离性。 比如基于 host 网络模式创建了一个基于 busybox 镜像构建的容器 bbox02，查看 ip addr： 然后宿主机通过 ip addr 查看信息如下： 返回信息一模一样，可以通过 docker network inspect host 查看所有 host 网络模式下的容器，在 Containers 节点中可以看到容器名称。 3)none网络模式 none 网络模式是指禁用网络功能，只有 lo 接口 local 的简写，代表 127.0.0.1，即 localhost 本地环回接口。在创建容器时通过参数 --net none 或者 --network none 指定； none 网络模式即不为 Docker Container 创建任何的网络环境，容器内部就只能使用 loopback 网络设备，不会再有其他的网络资源。可以说 none 模式为 Docke Container 做了极少的网络设定，但是俗话说得好“少即是多”，在没有网络配置的情况下，作为 Docker 开发者，才能在这基础做其他无限多可能的网络定制开发。这也恰巧体现了 Docker 设计理念的开放。 比如基于 none 网络模式创建了一个基于 busybox 镜像构建的容器 bbox03，查看 ip addr： 可以通过 docker network inspect none 查看所有 none 网络模式下的容器，在 Containers 节点中可以看到容器名称。 4)container网络模式 Container 网络模式是 Docker 中一种较为特别的网络的模式。在创建容器时通过参数 --net container:已运行的容器名称|ID 或者 --network container:已运行的容器名称|ID 指定； 处于这个模式下的 Docker 容器会共享一个网络栈，这样两个容器之间可以使用 localhost 高效快速通信。 Container 网络模式即新创建的容器不会创建自己的网卡，配置自己的 IP，而是和一个指定的容器共享 IP、端口范围等。同样两个容器除了网络方面相同之外，其他的如文件系统、进程列表等还是隔离的。 比如基于容器 bbox01 创建了 container 网络模式的容器 bbox04，查看 ip addr： 容器 bbox01 的 ip addr 信息如下： 宿主机的 ip addr 信息如下： 通过以上测试可以发现，Docker 守护进程只创建了一对对等虚拟设备接口用于连接 bbox01 容器和宿主机，而 bbox04 容器则直接使用了 bbox01 容器的网卡信息。 这个时候如果将 bbox01 容器停止，会发现 bbox04 容器就只剩下 lo 接口了。 然后 bbox01 容器重启以后，bbox04 容器也重启一下，就又可以获取到网卡信息了。 2.自定义网络虽然 Docker 提供的默认网络使用比较简单，但是为了保证各容器中应用的安全性，在实际开发中更推荐使用自定义的网络进行容器管理，以及启用容器名称到 IP 地址的自动 DNS 解析。 从 Docker 1.10 版本开始，docker daemon 实现了一个内嵌的 DNS server，使容器可以直接通过容器名称通信。方法很简单，只要在创建容器时使用 --name 为容器命名即可。但是使用 Docker DNS 有个限制：只能在 user-defined 网络中使用。也就是说，默认的 bridge 网络是无法使用 DNS 的，所以就需要自定义网络。 1)创建网络通过 docker network create 命令可以创建自定义网络模式，命令提示如下： 进一步查看 docker network create 命令使用详情，发现可以通过 --driver 指定网络模式且默认是 bridge 网络模式，提示如下： 创建一个基于 bridge 网络模式的自定义网络模式 custom_network，完整命令如下： 1docker network create custom_network 通过 docker network ls 查看网络模式： 123456[root@localhost ~]# docker network lsNETWORK ID NAME DRIVER SCOPEb3634bbd8943 bridge bridge local062082493d3a custom_network bridge local885da101da7d host host localf4f1b3cf1b7f none null local 通过自定义网络模式 custom_network 创建容器： 1docker run -di --name bbox05 --net custom_network busybox 通过 docker inspect 容器名称|ID 查看容器的网络信息，在 NetworkSettings 节点中可以看到详细信息。 2)连接网络通过 docker network connect 网络名称 容器名称 为容器连接新的网络模式。 1docker network connect bridge bbox05 通过 docker inspect 容器名称|ID 再次查看容器的网络信息，多增加了默认的 bridge。 3)断开网络通过 docker network disconnect 网络名称 容器名称 命令断开网络。 1docker network disconnect custom_network bbox05 通过 docker inspect 容器名称|ID 再次查看容器的网络信息，发现只剩下默认的 bridge。 4)移除网络可以通过 docker network rm 网络名称 命令移除自定义网络模式，网络模式移除成功会返回网络模式名称。 1docker network rm custom_network 注意：如果通过某个自定义网络模式创建了容器，则该网络模式无法删除。 3.容器间网络通信接下来通过所学的知识实现容器间的网络通信。首先明确一点，容器之间要互相通信，必须要有属于同一个网络的网卡。 先创建两个基于默认的 bridge 网络模式的容器。 12docker run -di --name default_bbox01 busyboxdocker run -di --name default_bbox02 busybox 通过 docker network inspect bridge 查看两容器的具体 IP 信息。 然后测试两容器间是否可以进行网络通信。 经过测试，从结果得知两个属于同一个网络的容器是可以进行网络通信的，但是 IP 地址可能是不固定的，有被更改的情况发生，那容器内所有通信的 IP 地址也需要进行更改，能否使用容器名称进行网络通信？继续测试。 经过测试，从结果得知使用容器进行网络通信是不行的，那怎么实现这个功能呢？ 从 Docker 1.10 版本开始，docker daemon 实现了一个内嵌的 DNS server，使容器可以直接通过容器名称通信。方法很简单，只要在创建容器时使用 --name 为容器命名即可。 但是使用 Docker DNS 有个限制：只能在 user-defined 网络中使用。也就是说，默认的 bridge 网络是无法使用 DNS 的，所以就需要自定义网络。 先基于 bridge 网络模式创建自定义网络 custom_network，然后创建两个基于自定义网络模式的容器。 12docker run -di --name custom_bbox01 --net custom_network busyboxdocker run -di --name custom_bbox02 --net custom_network busybox 通过 docker network inspect custom_network 查看两容器的具体 IP 信息。 然后测试两容器间是否可以进行网络通信，分别使用具体 IP 和容器名称进行网络通信。 经过测试，从结果得知两个属于同一个自定义网络的容器是可以进行网络通信的，并且可以使用容器名称进行网络通信。 那如果此时希望 bridge 网络下的容器可以和 custom_network 网络下的容器进行网络又该如何操作？其实答案也非常简单：让 bridge 网络下的容器连接至新的 custom_network 网络即可。 1docker network connect custom_network default_bbox01 4.关于docker重启后ip变化的痛点如上图这个依赖关系。在搭建集群的过程中遇到了一个痛点，比如nacos依赖mysql做持久化配置，seata直接依赖于mysql和nacos两个docker容器，一旦重启，ip变化，导致三个容器之间无法像之前一样通信。 解决方案： 自定义网络** 12345678910111213141516171819202122232425# --subnet 指定的是网段，随意指定，别占用宿主机就可，否则连不上外网（坑已经踩了，环境已经白搭过了）docker network create --subnet=192.18.0.1/16 ershi-network# 按照依赖关系，先安装MySQL ，指定上ip和连接的网络 ，宿主机访问直接通过localhost ，其他该自定义网络下的容器访问直接通过容器名（--name ershi-mysql）即可docker run -p 3316:3306 --name ershi-mysql -e MYSQL_ROOT_PASSWORD=root -d --net ershi-network --ip 192.18.0.2 mysql:5.7# 接下来安装nacos 并利用mysql 做持久化配置docker run -d -e MODE=standalone -e SPRING_DATASOURCE_PLATFORM=mysql -e MYSQL_SERVICE_HOST=ershi-mysql -e MYSQL_SERVICE_PORT=3306 -e MYSQL_SERVICE_USER=root -e MYSQL_SERVICE_PASSWORD=root -e MYSQL_SERVICE_DB_NAME=nacos_test -p 8848:8848 --restart=always --name nacos --net ershi-network --ip 192.18.0.3 nacos/nacos-server# 最后安装seatadocker run -d --name seata-server -p 8091:8091 --net ershi-network --ip 192.18.0.4 seataio/seata-server:1.2.0#安装完之后进入容器docker exec -it 容器ID shcd resources vi file.conf 选择 db的方式，然后配置url ： jdbc:mysql://ershi-mysql:3306/seata:wq!vi registry.conf选择nacos的方式然后nacos的address 改成 nacos (对应前面两个容器的名字):wq!ctrl + p +qdocker restart 容器IDdocker logs -f 容器ID#看日志没得问题访问localhost:8848/nacos# 服务列表里面 seata已经成功注册上了","categories":[{"name":"云计算","slug":"云计算","permalink":"https://yinhuidong.github.io/categories/%E4%BA%91%E8%AE%A1%E7%AE%97/"}],"tags":[{"name":"云计算","slug":"云计算","permalink":"https://yinhuidong.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"}]},{"title":"Jenkins","slug":"云计算/Jenkins","date":"2022-01-12T00:19:55.251Z","updated":"2022-01-12T00:19:55.252Z","comments":true,"path":"2022/01/12/云计算/Jenkins/","link":"","permalink":"https://yinhuidong.github.io/2022/01/12/%E4%BA%91%E8%AE%A1%E7%AE%97/Jenkins/","excerpt":"","text":"1234/var/jenkins_home jenkins的家目录包含了jenkins的所有配置。以后要注意备份 /var/jenkins_home （以文件的方式固化的） Jenkins镜像用 https://hub.docker.com/r/jenkinsci/jenkins/ 驱动我们整个CICD过程的很多工具 1.Jenkins安装https://www.jenkins.io/zh/doc/book/installing/ 1234567891011121314151617181920212223242526docker run \\ -u root \\ -d \\ -p 8080:8080 \\ -p 50000:50000 \\ -v jenkins-data:/var/jenkins_home \\ -v /etc/localtime:/etc/localtime:ro \\ -v /var/run/docker.sock:/var/run/docker.sock \\ --restart=always \\jenkinsci/blueocean#自己构建镜像 RUN的时候就把时区设置好 #如果是别人的镜像，docker hub，UTC； 容器运行时 ， -v/etc/localtime:/etc/localtime:rojenkinsci/jenkins 是没有 blueocean插件的，得自己装jenkinsci/blueocean：带了的 #/var/run/docker.sock 表示Docker守护程序通过其监听的基于Unix的套接字。 该映射允许jenkinsci/blueocean 容器与Docker守护进程通信， 如果 jenkinsci/blueocean 容器需要实例化其他Docker容器，则该守护进程是必需的。 如果运行声明式管道，其语法包含agent部分用 docker；例如， agent &#123; docker &#123; ... &#125; &#125; 此选项是必需的。 #如果你的jenkins 安装插件装不上。使用这个镜像【 registry.cn-qingdao.aliyuncs.com/lfy/jenkins:plugins-blueocean 】默认访问账号/密码是【admin/admin】 安装插件，并配置用户 2.Windows 下 Docker 安装 Jenkins1docker run -d --name jenkins -p 8081:8080 -v C:\\Users\\root\\software\\jenkins:/var/jenkins_home jenkins 手动下载cloudbees-folder.hpi放入jinkins目录** PS： jenkins安装插件过程中提示这个下载不下来，所以为了后面方便，这里将这步提前操作cloudbees-folder.hpi 下载地址： http://ftp.icm.edu.pl/packages/jenkins/plugins/cloudbees-folder/下载完成后将cloudbees-folder.hpi放入目录：C:\\Users\\root\\software\\jenkins\\war\\WEB-INF\\detached-pluginsPS： 这里C:\\Users\\root\\software\\jenkins是我的Windows挂载目录，相对应的找自己的地方, 后面用到的目录一样换成自己的 修改如下文件内容：** 位置：C:\\Users\\root\\software\\jenkins\\hudson.model.UpdateCenter.xml将上述文件中默认url换成： https://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.jsonPS：初始的url连接不上，换为国内清华大学的（网上找的链接） 重启Jinkins的容器服务**浏览器访问： http://localhost:8081/** PS： 这里我自己用Doker启动时绑定的端口是8081，需要换成自己的映射端口号 过程中会输入初始密码，如图： 这个密码的位置： C:\\Users\\root\\software\\jenkins\\secrets\\initialAdminPassword 插件安装可能会出错，问题不大，直接跳过**设置完用户密码后就进入如下页面了**对于之前安装失败的插件处理** Jenkins -&gt; 系统管理 -&gt; 管理插件","categories":[{"name":"云计算","slug":"云计算","permalink":"https://yinhuidong.github.io/categories/%E4%BA%91%E8%AE%A1%E7%AE%97/"}],"tags":[{"name":"云计算","slug":"云计算","permalink":"https://yinhuidong.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"}]},{"title":"MongoDB集群篇","slug":"MongoDB/Mongo集群","date":"2022-01-12T00:19:55.250Z","updated":"2022-01-12T00:19:55.250Z","comments":true,"path":"2022/01/12/MongoDB/Mongo集群/","link":"","permalink":"https://yinhuidong.github.io/2022/01/12/MongoDB/Mongo%E9%9B%86%E7%BE%A4/","excerpt":"","text":"一，MongoDB集群之复制集1.简介一组Mongodb复制集,就是一组mongod进程,这些进程维护同一个数据集合。复制集提供了数据冗余和高等级的可靠性,这是生产部署的基础。目的 保证数据在生产部署时的冗余和可靠性,通过在不同的机器上保存副本来保证数据不会因为单点损坏而丢失。能够随时应对数据丢失、机器损坏带来的风险。 还能提高读取能力,用户的读取服务器和写入服务器在不同的地方,而且,由不同的服务器为不同的用户提供服务,提高整个系统的负载。 ​ 机制​ 一组复制集就是一组mongod实例掌管同一个数据集,实例可以在不同的机器上面。实例中包含一个主导(Primary),接受客户端所有的写入操作,其他都是副本实例(Secondary),从主服务器上获得数据并保持 同步。 主服务器很重要,包含了所有的改变操作(写)的日志。但是副本服务器集群包含有所有的主服务器数据,因此当主服务器挂掉了,就会在副本服务器上重新选取一个成为主服务器。 每个复制集还有一个仲裁者(Arbiter),仲裁者不存储数据,只是负责通过心跳包来确认集群中集合的数量,并在主服务器选举的时候作为仲裁决定结果。2.架构 基本的架构由3台服务器组成,一个三成员的复制集,由三个有数据,或者两个有数据,一个作为仲裁者。 1）三个存储数据的复制集一个主,两个从库组成,主库宕机时,这两个从库都可以被选为主库。当主库宕机后,两个从库都会进行竞选,其中一个变为主库,当原主库恢复后,作为从库加入当前的复制集群即可。 2）存在arbiter节点的复制集一个主库,一个从库,可以在选举中成为主库,一个arbiter节点,在选举中,只进行投票,不能成为主库。 说明:由于arbiter节点没有复制数据,因此这个架构中仅提供一个完整的数据副本。arbiter节点只需要更少的资源,代价是更有限的冗余和容错。 当主库宕机时,将会选择从库成为主,主库修复后,将其加入到现有的复制集群中即可。 3.Primary选举复制集通过replSetInitiate命令(或mongo shell的rs.initiate())进行初始化,初始化后各个成员间开始发送心跳消息,并发起Priamry选举操作,获得『大多数』成员投票支持的节点,会成为Primary,其余节点成为Secondary。『大多数』的定义假设复制集内投票成员数量为N,则大多数为 N/2 + 1,当复制集内存活成员数量不足大多数时,整个复制集将无法选举出Primary,复制集将无法提供写服务,处于只读状态。 4.成员说明 成员 说明 Primary Priamry的作用是接收用户的写入操作,将自己的数据同步给其他的Secondary。 Secondary 正常情况下,复制集的Seconary会参与Primary选举(自身也可能会被选为Primary),并从Primary同步最新写入的数据,以保证与Primary存储相同的数据。Secondary可以提供读服 务,增加Secondary节点可以提供复制集的读服务能力,同时提升复制集的可用性。另外, Mongodb支持对复制集的Secondary节点进行灵活的配置,以适应多种场景的需求。 ​ Arbiter Arbiter节点只参与投票,不能被选为Primary,并且不从Primary同步数据。比如你部署了一个2个节点的复制集,1个Primary,1个Secondary,任一节点宕机,复制集将不能提供服务了 (无法选出Primary),这时可以给复制集添加一个Arbiter节点,即使有节点宕机,仍能选出 Primary。Arbiter本身不存储数据,是非常轻量级的服务,当复制集成员为偶数时,最好加入 一个Arbiter节点,以提升复制集可用性。 ​ Priority0 Priority0节点的选举优先级为0,不会被选举为Primary。比如你跨机房A、B部署了一个复制集,并且想指定Primary必须在A机房,这时可以将B机房的复制集成员Priority设置为0,这样 Primary就一定会是A机房的成员。(注意:如果这样部署,最好将『大多数』节点部署在A机 房,否则网络分区时可能无法选出Primary) ​ Vote0 Mongodb 3.0里,复制集成员最多50个,参与Primary选举投票的成员最多7个,其他成员(Vote0)的vote属性必须设置为0,即不参与投票。 ​ Hidden Hidden节点不能被选为主(Priority为0),并且对Driver不可见。因Hidden节点不会接受Driver的请求,可使用Hidden节点做一些数据备份、离线计算的任务,不会影响复制集的服 务。 ​ Delayed Delayed节点必须是Hidden节点,并且其数据落后与Primary一段时间(可配置,比如1个小时)。因Delayed节点的数据比Primary落后一段时间,当错误或者无效的数据写入Primary 时,可通过Delayed节点的数据来恢复到之前的时间点。 ​ 5.故障转移从节点宕机：集群依然可以正常使用,可以读写操作。主节点宕机：选举出新的主节点继续提供服务。停止集群中的两个节点：当前集群无法选举出Priamry,无法提供写操作,只能进行读操作。 6.增加arbiter节点当集群中的节点数为偶数时,如一主一从情况下,任意一节点宕机都无法选举出Priamry,无法提供写操作,加入arbiter节点后即可解决该问题。 123456789101112131415161718192021docker create --name mongo04 -p 27020:27017 -v mongo-data-04:/data/db mongo:4.0.3 -- replSet &quot;rs0&quot; --bind_ip_alldocker start mongo04 #在主节点执行rs0:PRIMARY&gt; rs.addArb(&quot;172.16.55.185:27020&quot;)&#123; &quot;ok&quot; : 1, &quot;operationTime&quot; : Timestamp(1551627454, 1), &quot;$clusterTime&quot; :&#123; &quot;clusterTime&quot; : Timestamp(1551627454, 1), &quot;signature&quot; :&#123; &quot;hash&quot; : BinData(0,&quot;AAAAAAAAAAAAAAAAAAAAAAAAAAA=&quot;), &quot;keyId&quot; : NumberLong(0) &#125; &#125;&#125;#查询集群状态rs.status() 通过测试,添加arbiter节点后,如果集群节点数不满足N/2+1时,arbiter节点可作为“凑数”节点,可以选出主节点,继续提供服务。 二,MongoDB集群之分片集群分片(sharding)是MongoDB用来将大型集合分割到不同服务器(或者说一个集群)上所采用的方法。尽管分片起源于关系型数据库分区,但MongoDB分片完全又是另一回事。和MySQL分区方案相比,MongoDB的最大区别在于它几乎能自动完成所有事情,只要告诉MongoDB要分配数据,它就能自动维护数据在不同服务器之间的均衡。 1.简介高数据量和吞吐量的数据库应用会对单机的性能造成较大压力,大的查询量会将单机的CPU耗尽,大的数据量对单机的存储压力较大,最终会耗尽系统的内存而将压力转移到磁盘IO上。​ 为了解决这些问题,有两个基本的方法: 垂直扩展和水平扩展。 垂直扩展:增加更多的CPU和存储资源来扩展容量。 水平扩展:将数据集分布在多个服务器上。水平扩展即分片。 分片为应对高吞吐量与大数据量提供了方法。使用分片减少了每个分片需要处理的请求数,因此,通过水平扩展,集群可以提高自己的存储容量和吞吐量。举例来说,当插入一条数据时,应用只需要访问存储这条数据的分片.​ 使用分片减少了每个分片存储的数据。例如,如果数据库1tb的数据集,并有4个分片,然后每个分片可能仅持有256GB的数据。如果有40个分片,那么每个切分可能只有25GB的数据。 2.优势 对集群进行抽象,让集群“不可见” MongoDB自带了一个叫做mongos的专有路由进程。mongos就是掌握统一路口的路由器,其会将客户端发来的请求准确无误的路由到集群中的一个或者一组服务器上,同时会把接收到的响应拼装起来发回到客户端。 保证集群总是可读写 MongoDB通过多种途径来确保集群的可用性和可靠性。将MongoDB的分片和复制功能结合使用,在确保数据分片到多台服务器的同时,也确保了每分数据都有相应的备份,这样就可以确保有服务器换掉时,其他的从库可以立即接替坏掉的部分继续工作。 使集群易于扩展 当系统需要更多的空间和资源的时候,MongoDB使我们可以按需方便的扩充系统容量。 3.架构 组件 说明 ConfigServer 存储集群所有节点、分片数据路由信息。默认需要配置3个Config Server节点。 Mongos 提供对外应用访问,所有操作均通过mongos执行。一般有多个mongos节点。数据迁移和数据自动平衡。 Mongod 存储应用数据记录。一般有多个Mongod节点,达到数据分片目的。 Mongos本身并不持久化数据,Sharded cluster所有的元数据都会存储到Config Server,而用户的数据会分散存储到各个shard。Mongos启动后,会从配置服务器加载元数据,开始提供服务,将用户的请求正确路由到对应的分片。当数据写入时,MongoDB Cluster根据分片键设计写入数据。当外部语句发起数据查询时,MongoDB根据数据分布自动路由至指定节点返回数据。 4.集群中的数据分布在一个shard server内部,MongoDB会把数据分为chunks,每个chunk代表这个shard server内部一部分数据。chunk的产生,会有以下两个用途: Splitting:当一个chunk的大小超过配置中的chunk size时,MongoDB的后台进程会把这个chunk切分成更小的chunk,从而避免chunk过大的情况 Balancing:在MongoDB中,balancer是一个后台进程,负责chunk的迁移,从而均衡各个shard server的负载,系统初始1个chunk,chunk size默认值64M,生产库上选择适合业务的chunk size是最好的。mongoDB会自动拆分和迁移chunks。 ​ 1）chunk分裂及迁移随着数据的增长,其中的数据大小超过了配置的chunk长会让chunk分裂得越来越多。这时候,各个shard 上的chunk数量就会不平衡。mongos中的一个组件balancer 就会执行自动平衡。把chunk从chunk数量最多的shard节点挪动到数量最少的节点。 2）chunksizechunk的分裂和迁移非常消耗IO资源;chunk分裂的时机:在插入和更新,读数据不会分裂。 小的chunksize:数据均衡是迁移速度快,数据分布更均匀。数据分裂频繁,路由节点消耗更多资源。 大的chunksize:数据分裂少。数据块移动集中消耗IO资源。 适合业务的chunksize是最好的。chunkSize 对分裂及迁移的影响 MongoDB 默认的 chunkSize 为64MB,如无特殊需求,建议保持默认值;chunkSize 会直接影响到 chunk 分裂、迁移的行为。 chunkSize 越小,chunk 分裂及迁移越多,数据分布越均衡;反之,chunkSize 越大,chunk 分裂及迁移会更少,但可能导致数据分布不均。 chunk 自动分裂只会在数据写入时触发,所以如果将 chunkSize 改小,系统需要一定的时间来将 chunk 分裂到指定的大小。 chunk 只会分裂,不会合并,所以即使将 chunkSize 改大,现有的 chunk 数量不会减少,但 chunk 大小会随着写入不断增长,直到达到目标大小。 ​ ​ ​ ​","categories":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://yinhuidong.github.io/categories/MongoDB/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://yinhuidong.github.io/tags/MongoDB/"}]},{"title":"DevOps","slug":"云计算/DevOps","date":"2022-01-12T00:19:55.250Z","updated":"2022-01-12T00:19:55.250Z","comments":true,"path":"2022/01/12/云计算/DevOps/","link":"","permalink":"https://yinhuidong.github.io/2022/01/12/%E4%BA%91%E8%AE%A1%E7%AE%97/DevOps/","excerpt":"","text":"1.DevOps是什么Development和Operations的组合词； DevOps 看作开发（软件工程）、技术运营和质量保障（QA）三者的交集。 突出重视软件开发人员和运维人员的沟通合作，通过自动化流程来使得软件构建、测试、 发布更加快捷、频繁和可靠。 DevOps 希望做到的是软件产品交付过程中 IT 工具链的打通，使得各个团队减少时间损 耗，更加高效地协同工作。专家们总结出了下面这个 DevOps 能力图，良好的闭环可以大大 增加整体的产出。 2.CICD是什么持续集成 持续部署 2.1 基本理念 ①持续集成持续集成是指软件个人研发的部分向软件整体部分交付，频繁进行集成以便更快地发现 其中的错误。“持续集成”源自于极限编程（XP），是 XP 最初的 12 种实践之一。 CI 需要具备这些： 全面的自动化测试。这是实践持续集成&amp;持续部署的基础，同时，选择合适的 自动化测试工具也极其重要； 灵活的基础设施。容器，虚拟机的存在让开发人员和 QA 人员不必再大费周 折； 版本控制工具。如 Git，CVS，SVN 等； 自动化的构建和软件发布流程的工具，如 Jenkins，flow.ci； 反馈机制。如构建/测试的失败，可以快速地反馈到相关负责人，以尽快解决达到一个更稳定的版本。 ②持续交付 持续交付在持续集成的基础上，将集成后的代码部署到更贴近真实运行环境的「类生产环境」（production-like environments）中。持续交付优先于整个产品生命周期的软件部署，建立 在高水平自动化持续集成之上。 灰度发布。 持续交付和持续集成的优点非常相似： 快速发布。能够应对业务需求，并更快地实现软件价值。 编码-&gt;测试-&gt;上线-&gt;交付的频繁迭代周期缩短，同时获得迅速反馈； 高质量的软件发布标准。整个交付过程标准化、可重复、可靠， 整个交付过程进度可视化，方便团队人员了解项目成熟度； 更先进的团队协作方式。从需求分析、产品的用户体验到交互 设计、开发、测试、运维等角色密切协作，相比于传统的瀑布式软件团队，更少浪费。 ③持续部署持续部署是指当交付的代码通过评审之后，自动部署到生产环境中。持续部署是持续交付的最高阶段。这意味着，所有通过了一系列的自动化测试的改动都将自动部署到生产环境。它也可以被称为“Continuous Release”。 “开发人员提交代码，持续集成服务器获取代码，执行单元测试，根据测试结果决定是否部署到预演环境，如果成功部署到预演环境，进行整体验收测试，如果测试通过，自动部署到产品环境，全程自动化高效运转。 持续部署主要好处是，可以相对独立地部署新的功能，并能快速地收集真实用户的反馈。 “You build it, you run it”，这是 Amazon 一年可以完成 5000 万次部署， 平均每个工程师每天部署超过 50 次的核心秘籍。 5000/365 = 15 万次 开发人员代码敲完。可以release的时候，提交代码， 剩下的全部一站式自动搞定. 2.2 最佳实践①内循环与外循环 内循环（开发要做的事情）： 编码、测试、运行、debug、提交 代码推送到代码仓库（svn，git）【代码回滚】 进行CI过程（持续集成），万物皆可容器化。打包成一个Docker镜像 镜像推送到镜像仓库 测试 持续部署流程（CD），拿到之前的镜像，进行CD。怎么放到各种环境。uat、test、prod 外循环（） 运行时监控 生产环境的管理 监控 线上反馈到开发 来到内循环 ②实践流程新功能，bug修复。 创建分支来做这个事情（开发功能） 提交分支的代码改变 进入持续集成流程 当前分支代码功能性自动化构建和测试 自动工具推送这次提交 自动化集成测试 可以看到效果 人工确认此次功能是否发布到生产环境 代码合并。 进入持续部署流程 构建、测试、发布…… 2.3 CICD LandSpace","categories":[{"name":"云计算","slug":"云计算","permalink":"https://yinhuidong.github.io/categories/%E4%BA%91%E8%AE%A1%E7%AE%97/"}],"tags":[{"name":"云计算","slug":"云计算","permalink":"https://yinhuidong.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"}]},{"title":"Docker存储原理","slug":"云计算/Docker存储原理","date":"2022-01-12T00:19:55.250Z","updated":"2022-01-12T00:19:55.250Z","comments":true,"path":"2022/01/12/云计算/Docker存储原理/","link":"","permalink":"https://yinhuidong.github.io/2022/01/12/%E4%BA%91%E8%AE%A1%E7%AE%97/Docker%E5%AD%98%E5%82%A8%E5%8E%9F%E7%90%86/","excerpt":"","text":"问题： 容器：某个软件完整的运行环境，包含了一个小型的Linux系统 宿主机里面同时4个nginx，一个nginx运行时完整环境有20M 4个nginx合起来占用多少磁盘空间？ 80M？ 软件装在docker上和宿主机上的对比 优点：docker的移植性，便捷性高于宿主机部署，进程隔离，很方便的资源限制 缺点：docker虚拟化技术，损失很少的性能 镜像、容器 镜像：固定不变的。一个镜像可以启动很多容器 容器：文件系统可能logs经常变化的，一个镜像可以启动很多容器 docker在底层使用自己的存储驱动。来组建文件内容 storage drivers。docker 基于 AUFS （联合文件系统）。 一，镜像如何存储？1.镜像探索 截取nginx 的分层 nginx这个镜像怎么存的？使用docker image inspect nginx 这里面指示了镜像是如何存储 LowerDir ：底层目录; diff（只是存储不同）；包含小型linux和装好的软件1234567891011/var/lib/docker/overlay2/67b3802c6bdb5bcdbcccbbe7aed20faa7227d584ab37668a03ff6952e631f7f2/diff：用户文件；/var/lib/docker/overlay2/f56920fac9c356227079df41c8f4b056118c210bf4c50bd9bb077bdb4c7524b4/diff： nginx的启动命令放在这里/var/lib/docker/overlay2/0e569a134838b8c2040339c4fdb1f3868a7118dd7f4907b40468f5fe60f055e5/diff： nginx的配置文件在这里/var/lib/docker/overlay2/2b51c82933078e19d78b74c248dec38164b90d80c1b42f0fdb1424953207166e/diff: 小linux系统 从下往上看 小linux系统（FROM apline） + Dockerfile的每一个命令可能都引起了系统的修改，所以和git一样，只记录变化。 我们进入到这个镜像启动的容器，容器的文件系统就是镜像的 docker ps -s 可以看到这个容器真正用到的文件大小 容器会自己建立层；如果想要改东西，把改的内容复制到容器层即可 docker inspect container MergedDir ：合并目录；容器最终的完整工作目录全内容都在合并层；数据卷在容器层产生；所有的增删改都在容器层； UpperDir ：上层目录 WorkDir ：工作目录（临时层），pid； LowerDir（底层）\\UpperDir（）\\MergedDir\\WorkDir(临时东西)docker底层的 storage driver完成了以上的目录组织结果； 什么东西适合运行容器？ docker启动一个MySQL默认什么都不做？ docker被干掉，MySQL数据就没了 文件挂载 docker commit 提交 mysql容器，也能提交 100G 100G 2.镜像和分层Docker映像由一系列层组成。 每层代表图像的Dockerfile中的一条指令。 除最后一层外的每一层都是只读的。 如以下Dockerfile： Dockerfile文件里面几句话，镜像就有几层12345FROM ubuntu:15.04COPY . /appRUN make /appCMD python /app/app.py# 每一个指令都可能会引起镜像改变，这些改变类似git的方式逐层叠加。 该Dockerfile包含四个命令，每个命令创建一个层。 FROM语句从ubuntu：15.04映像创建一个图层开始。 COPY命令从Docker客户端的当前目录添加一些文件。 RUN命令使用make命令构建您的应用程序。 最后，最后一层指定要在容器中运行的命令。 每一层只是与上一层不同的一组。 这些层彼此堆叠。 创建新容器时，可以在基础层之上添加一个新的可写层。 该层通常称为“容器层”。 对运行中的容器所做的所有更改（例如写入新文件，修改现有文件和删除文件）都将写入此薄可写容器层。 3.容器和分层 容器和镜像之间的主要区别是可写顶层。 在容器中添加新数据或修改现有数据的所有写操作都存储在此可写层中。 删除容器后，可写层也会被删除。 基础图像保持不变。 因为每个容器都有其自己的可写容器层，并且所有更改都存储在该容器层中，所以多个容器可以共享对同一基础映像的访问，但具有自己的数据状态。 共享同一Ubuntu 15.04映像的多个容器**** ** 4.磁盘容量预估123456789docker ps -ssize：用于每个容器的可写层的数据量（在磁盘上）。virtual size：容器使用的用于只读图像数据的数据量加上容器的可写图层大小。多个容器可以共享部分或全部只读图像数据。从同一图像开始的两个容器共享100％的只读数据，而具有不同图像的两个容器（具有相同的层）共享这些公共层。 因此，不能只对虚拟大小进行总计。这高估了总磁盘使用量，可能是一笔不小的数目。 5.镜像如何挑选1234567busybox：是一个集成了一百多个最常用Linux命令和工具的软件。linux工具里的瑞士军刀alpine：Alpine操作系统是一个面向安全的轻型Linux发行版经典最小镜像，基于busybox，功能比Busybox完善。slim：docker hub中有些镜像有slim标识，都是瘦身了的镜像。也要优先选择无论是制作镜像还是下载镜像，优先选择alpine类型. 6.写时复制 写时复制是一种共享和复制文件的策略，可最大程度地提高效率。 如果文件或目录位于映像的较低层中，而另一层（包括可写层）需要对其进行读取访问，则它仅使用现有文件。 另一层第一次需要修改文件时（在构建映像或运行容器时），将文件复制到该层并进行修改。 这样可以将I / O和每个后续层的大小最小化。 二，容器如何挂载？ 每一个容器里面的内容，支持三种挂载方式： docker自动在外部创建文件夹自动挂载容器内部指定的文件夹内容【Dockerfile VOLUME指令的作用】 自己在外部创建文件夹，手动挂载 可以把数据挂载到内存中。 –mount 挂载到 linux宿主机，手动挂载（不用了） -v 可以自动挂载，到linux’主机或者docker自动管理的这一部分区域 Volumes(卷) ：存储在主机文件系统的一部分中，该文件系统由Docker管理（在Linux上是“ / var /lib / docker / volumes /”）。 非Docker进程不应修改文件系统的这一部分。 卷是在Docker中持久存储数据的最佳方法。 Bind mounts(绑定挂载) 可以在任何地方 存储在主机系统上。 它们甚至可能是重要的系统文件或目录。 Docker主机或Docker容器上的非Docker进程可以随时对其进行修改。 tmpfs mounts(临时挂载) 仅存储在主机系统的内存中，并且永远不会写入主机系统的文件系统。 1.卷 匿名卷使用 12345docker run -dP -v :/etc/nginx nginx#docker将创建出匿名卷，并保存容器/etc/nginx下面的内容# -v 宿主机:容器里的目录 具名卷使用 123docker run -dP -v nginx:/etc/nginx nginx#docker将创建出名为nginx的卷，并保存容器/etc/nginx下面的内容 如果将空卷装入存在文件或目录的容器中的目录中，则容器中的内容（复制）到该卷中。 如果启动一个容器并指定一个尚不存在的卷，则会创建一个空卷。 -v 宿主机绝对路径:Docker容器内部绝对路径：叫挂载；这个有空挂载问题-v 不以/开头的路径:Docker容器内部绝对路径：叫绑定（docker会自动管理，docker不会把他当前目录，而把它当前卷） 用哪个比较好？ 如果自己开发测试，用 -v 绝对路径的方式 如果是生产环境建议用卷 除非特殊 /bin/docker 需要挂载主机路径的则操作 绝对路径挂载 nginx测试html挂载几种不同情况： 不挂载 效果：访问默认欢迎页 -v /root/html:/usr/share/nginx/html 效果：访问forbidden -v html:/usr/share/nginx/html:ro 效果：访问默认欢迎页 -v /usr/share/nginx/html 效果：匿名卷 （什么都不写也不要加冒号，直接写容器内的目录） 原因 -v html:/usr/share/nginx/html； docker inspect 容器的时候； docker自动管理的方式12345678910111213141516171819202122# -v不以绝对路径方式；### 1、先在docker底层创建一个你指定名字的卷（具名卷） html### 2、把这个卷和容器内部目录绑定### 3、容器启动以后，目录里面的内容就在卷里面存着；#####-v nginxhtml:/usr/share/nginx/html 也可以以下操作## 1、 docker create volume nginxhtml 如果给卷里面就行修改，容器内部的也就改了。## 2、 docker volume inspect nginxhtml## 3、docker run -d -P -v nginxhtml:/usr/share/nginx/html --name=nginx777 nginx# 可以看到&quot;Mounts&quot;: [ &#123; &quot;Type&quot;: &quot;volume&quot;, //这是个卷 &quot;Name&quot;: &quot;html&quot;, //名字是html &quot;Source&quot;: &quot;/var/lib/docker/volumes/html/_data&quot;, //宿主机的目录。容器里面的哪两个文件都在 &quot;Destination&quot;: &quot;/usr/share/nginx/html&quot;, //容器内部 &quot;Driver&quot;: &quot;local&quot;, &quot;Mode&quot;: &quot;z&quot;, &quot;RW&quot;: true, //读写模式 &quot;Propagation&quot;: &quot;&quot; &#125; ] 123#卷：就是为了保存数据docker volume #可以对docker自己管理的卷目录进行操作；/var/lib/docker/volumes(卷的根目录) 2.bind mount 如果将绑定安装或非空卷安装到存在某些文件或目录的容器中的目录中，则这些文件或目录会被安装遮盖，就像您将文件保存到Linux主机上的/ mnt中一样，然后 将USB驱动器安装到/ mnt中。在卸载USB驱动器之前，/ mnt的内容将被USB驱动器的内容遮盖。 被遮盖的文件不会被删除或更改，但是在安装绑定安装或卷时将无法访问。 总结：外部目录覆盖内部容器目录内容，但不是修改。所以谨慎，外部空文件夹挂载方式也会导致容器内部是空文件夹。 123docker run -dP -v /my/nginx:/etc/nginx:ro nginx# bind mount和 volumes 的方式写法区别在于 # 所有以/开始的都认为是 bind mount ，不以/开始的都认为是 volumes. 警惕bind mount 方式，文件挂载没有在外部准备好内容而导致的容器启动失败问题** 123456789# 一行命令启动nginx，并且配置文件和html页面。需要知道卷的位置才能改docker run -d -P -v nginxconf:/etc/nginx/ -v nginxpage:/usr/share/nginx/html nginx# 想要实现 docker run -d -P -v /root/nginxconf:/etc/nginx/ -v/root/nginxhtml:/usr/share/nginx/html --name=nginx999 nginx### 1、提前准备好东西 目录nginxconf，目录里面的配置we年都放里面，，再调用命令### 2、docker cp nginxdemo:/etc/nginx /root/nginxconf #注意/的使用### 3、docker run -d -P -v /root/nginxconf:/etc/nginx/ -v/root/nginxhtml:/usr/share/nginx/html --name=nginx999 nginx ** 3.管理卷1234docker volume create xxx：创建卷名docker volume inspect xxx：查询卷详情docker volume ls: 列出所有卷docker volume prune: 移除无用卷 4.关于docker cp小细节 docker cp [OPTIONS] CONTAINER:SRC_PATH DEST_PATH|- ：把容器里面的复制出来 docker cp [OPTIONS] SRC_PATH|- CONTAINER:DEST_PATH：把外部的复制进去 自动创建文件夹不会做递归。把父文件夹做好。****************************************************","categories":[{"name":"云计算","slug":"云计算","permalink":"https://yinhuidong.github.io/categories/%E4%BA%91%E8%AE%A1%E7%AE%97/"}],"tags":[{"name":"云计算","slug":"云计算","permalink":"https://yinhuidong.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"}]},{"title":"Mongo使用篇","slug":"MongoDB/Mongo基操","date":"2022-01-12T00:19:55.249Z","updated":"2022-01-12T00:19:55.250Z","comments":true,"path":"2022/01/12/MongoDB/Mongo基操/","link":"","permalink":"https://yinhuidong.github.io/2022/01/12/MongoDB/Mongo%E5%9F%BA%E6%93%8D/","excerpt":"","text":"一，Mongo简介文档数据库MongoDB中的记录是一个文档，它是由字段和值对组成的数据结构。MongoDB文档类似于JSON对象。字段的值可以包括其他文档，数组和文档数组。使用文档的优点是： 文档对应编程语言中的数据类型 嵌入式文档减少了对连接的需求 动态模式支持流畅的多态性 ​ 集合/视图/按需实例化视图mongo将文档存储在集合中。集合类似于关系数据库中的表。除集合外，Mongo还支持： 只读视图 按需实例化视图 主要特性高性能Mongo提供高性能的数据持久化。特别是， 对嵌入式数据模型的支持减少了数据库系统上的IO操作 索引支持更快的查询，并且可以包含来自嵌入式文档和数组的键 丰富的查询语言Mongo支持丰富的查询语言以支持读写操作以及： 数据聚合 文本搜索和地理空间查询 高可用Mongo的复制工具–副本集 提供： 自动故障转移 数据冗余 副本集是一组维护相同数据集合的mongod实例，提供了冗余和提高了数据可用性。水平扩展Mongo提供水平可伸缩性作为其核心功能的一部分： 分片将数据分布在集群的机器上 从3.4开始，mongo支持基于分片键创建数据区域。在平衡集群中，mongo仅将区域覆盖的读写定向到区域内的那些分片。 支持多种存储引擎Mongo支持多种存储引擎 WiredTiger存储引擎（包括对静态加密的支持） 内存存储引擎 另外，mongo提供课插拔的存储引擎API，允许自己来发存储引擎。 二，下载，安装，启动1.mongo安装12sudo docker search mongosudo docker pull mongo 2.mongo启动12sudo docker run -d -p 27017:27017 --name mongodb -e MONGO_INITDB_ROOT_USERNAME=root -e MONGO_INITDB_ROOT_PASSWORD=root mongosudo service mongod start 此时可以通过 /var/log/mongodb/mongod.log文件观察服务启动情况。 3.启动测试12&gt;mongo&gt;db 4.配置文件位置/etc/mongodb.conf 5.基础配置 bindIp: 127.0.0.1”一行注释掉 MongoDB默认情况下将数据文件及日志文件分别存放在/var/lib/mongodb及/var/log/mongodb下，我们可以通过配置文件更改默认路径： 将storage中的dbPath和systemLog中的path属性分别改成我们期望的路径 通过以下命令行启动MongoDB后台进程 mongod -f /etc/mongod.conf &amp; 默认端口 27017 中文文档 三，MongoDB基本操作1.基本概念 SQL术语/概念 Mongo术语/概念 解释/说明 database database 数据库 table collection 数据库表/集合 row document 数据记录行/文档 column field 数据字段/域 index index 索引 table joins 表连接,MongoDB不支持 primary key primary key 主键,MongoDB自动将_id字段设置为主键 2.数据库以及表的操作1234567891011121314151617181920212223242526272829#查看所有数据库&gt; show dbslocal 0.078125GB#通过use关键字切换数据库&gt; use adminswitched to db admin#创建数据库#说明:在MongoDB中,数据库是自动创建的,通过use切换到新数据库中,进行插入数据即可自动创建数据库&gt; db.user.insert(&#123;id:1,name:&#x27;zhangsan&#x27;&#125;)&gt; show dbsadmin 0.203125GBlocal 0.078125GB#查看表&gt; show tablessystem.indexesuser#删除集合(表)&gt; db.user.drop()true #如果成功删除选定集合,则 drop() 方法返回 true,否则返回 false。#删除数据库 &gt; use testdb #先切换到要删除的数据中 switched to db testdb &gt; db.dropDatabase() #删除数据库&#123; &quot;dropped&quot; : &quot;testdb&quot;, &quot;ok&quot; : 1 &#125; 3.新增数据在MongoDB中,存储的文档结构是一种类似于json的结构,称之为bson(全称为:BinaryJSON)。 12345678910 #语法:db.COLLECTION_NAME.insert(document) &gt; db.user.insert(&#123;id:1,username:&#x27;zhangsan&#x27;,age:20&#125;) WriteResult(&#123; &quot;nInserted&quot; : 1 &#125;) &gt; db.user.save(&#123;id:2,username:&#x27;lisi&#x27;,age:25&#125;) WriteResult(&#123; &quot;nInserted&quot; : 1 &#125;) &gt; db.user.find() #查询数据 &#123; &quot;_id&quot; : ObjectId(&quot;5c08c0024b318926e0c1f6dc&quot;), &quot;id&quot; : 1, &quot;username&quot; : &quot;zhangsan&quot;,&quot;age&quot; : 20 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5c08c0134b318926e0c1f6dd&quot;), &quot;id&quot; : 2, &quot;username&quot; : &quot;lisi&quot;,&quot;age&quot; : 25 &#125; 4.更新数据update() 方法用于更新已存在的文档。语法格式如下: 123456789db.collection.update( &lt;query&gt;, &lt;update&gt;, [ upsert: &lt;boolean&gt;, multi: &lt;boolean&gt;, writeConcern: &lt;document&gt; ]) 参数说明： query : update的查询条件,类似sql update查询内where后面的。 update : update的对象和一些更新的操作符(如 inc…)等,也可以理解为sql update查询内set后面的 upsert : 可选,这个参数的意思是,如果不存在update的记录,是否插入objNew,true为插入,默认是false,不插入。 multi : 可选,mongodb 默认是false,只更新找到的第一条记录,如果这个参数为true,就把按条件查出来多条记录全部更新。 writeConcern :可选,抛出异常的级别。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&gt; db.user.find()&#123; &quot;_id&quot; : ObjectId(&quot;5c08c0024b318926e0c1f6dc&quot;), &quot;id&quot; : 1, &quot;username&quot; : &quot;zhangsan&quot;,&quot;age&quot; : 20 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5c08c0134b318926e0c1f6dd&quot;), &quot;id&quot; : 2, &quot;username&quot; : &quot;lisi&quot;,&quot;age&quot; : 25 &#125;&gt; db.user.update(&#123;id:1&#125;,&#123;$set:&#123;age:22&#125;&#125;) #更新数据WriteResult(&#123; &quot;nMatched&quot; : 1, &quot;nUpserted&quot; : 0, &quot;nModified&quot; : 1 &#125;)&gt; db.user.find()&#123; &quot;_id&quot; : ObjectId(&quot;5c08c0024b318926e0c1f6dc&quot;), &quot;id&quot; : 1, &quot;username&quot; : &quot;zhangsan&quot;,&quot;age&quot; : 22 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5c08c0134b318926e0c1f6dd&quot;), &quot;id&quot; : 2, &quot;username&quot; : &quot;lisi&quot;,&quot;age&quot; : 25 &#125;#注意:如果这样写,会删除掉其他的字段&gt; db.user.update(&#123;id:1&#125;,&#123;age:25&#125;)WriteResult(&#123; &quot;nMatched&quot; : 1, &quot;nUpserted&quot; : 0, &quot;nModified&quot; : 1 &#125;)&gt; db.user.find()&#123; &quot;_id&quot; : ObjectId(&quot;5c08c0024b318926e0c1f6dc&quot;), &quot;age&quot; : 25 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5c08c0134b318926e0c1f6dd&quot;), &quot;id&quot; : 2, &quot;username&quot; : &quot;lisi&quot;,&quot;age&quot; : 25 &#125;#更新不存在的字段,会新增字段&gt; db.user.update(&#123;id:2&#125;,&#123;$set:&#123;sex:1&#125;&#125;) #更新数据&gt; db.user.find()&#123; &quot;_id&quot; : ObjectId(&quot;5c08c0024b318926e0c1f6dc&quot;), &quot;age&quot; : 25 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5c08c0134b318926e0c1f6dd&quot;), &quot;id&quot; : 2, &quot;username&quot; : &quot;lisi&quot;,&quot;age&quot; : 25, &quot;sex&quot; : 1 &#125;#更新不存在的数据,默认不会新增数据&gt; db.user.update(&#123;id:3&#125;,&#123;$set:&#123;sex:1&#125;&#125;)WriteResult(&#123; &quot;nMatched&quot; : 0, &quot;nUpserted&quot; : 0, &quot;nModified&quot; : 0 &#125;)&gt; db.user.find()&#123; &quot;_id&quot; : ObjectId(&quot;5c08c0024b318926e0c1f6dc&quot;), &quot;age&quot; : 25 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5c08c0134b318926e0c1f6dd&quot;), &quot;id&quot; : 2, &quot;username&quot; : &quot;lisi&quot;,&quot;age&quot; : 25, &quot;sex&quot; : 1 &#125;#如果设置第一个参数为true,就是新增数据&gt; db.user.update(&#123;id:3&#125;,&#123;$set:&#123;sex:1&#125;&#125;,true)WriteResult(&#123; &quot;nMatched&quot; : 0, &quot;nUpserted&quot; : 1, &quot;nModified&quot; : 0, &quot;_id&quot; : ObjectId(&quot;5c08cb281418d073246bc642&quot;)&#125;)&gt; db.user.find()&#123; &quot;_id&quot; : ObjectId(&quot;5c08c0024b318926e0c1f6dc&quot;), &quot;age&quot; : 25 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5c08c0134b318926e0c1f6dd&quot;), &quot;id&quot; : 2, &quot;username&quot; : &quot;lisi&quot;,&quot;age&quot; : 25, &quot;sex&quot; : 1 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5c08cb281418d073246bc642&quot;), &quot;id&quot; : 3, &quot;sex&quot; : 1 &#125; 5.删除数据通过remove()方法进行删除数据,语法如下:1234567db.collection.remove( &lt;query&gt;, &#123; justOne: &lt;boolean&gt;, writeConcern: &lt;document&gt; &#125;) 参数说明： query :(可选)删除的文档的条件。 justOne : (可选)如果设为 true 或 1,则只删除一个文档,如果不设置该参数,或使用默认值 false,则删除所有匹配条件的文档。 writeConcern :(可选)抛出异常的级别。 123456789101112131415161718&gt; db.user.remove(&#123;age:25&#125;)WriteResult(&#123; &quot;nRemoved&quot; : 2 &#125;) #删除了2条数据#插入4条测试数据db.user.insert(&#123;id:1,username:&#x27;zhangsan&#x27;,age:20&#125;)db.user.insert(&#123;id:2,username:&#x27;zhangsan&#x27;,age:21&#125;)db.user.insert(&#123;id:3,username:&#x27;zhangsan&#x27;,age:22&#125;)db.user.insert(&#123;id:4,username:&#x27;zhangsan&#x27;,age:23&#125;) &gt; db.user.remove(&#123;age:22&#125;,true) WriteResult(&#123; &quot;nRemoved&quot; : 1 &#125;) #删除了1条数据 #删除所有数据 &gt; db.user.remove(&#123;&#125;) #说明:为了简化操作,官方推荐使用deleteOne()与deleteMany()进行删除数据操作。db.user.deleteOne(&#123;id:1&#125;) db.user.deleteMany(&#123;&#125;) #删除所有数据 6.查询数据MongoDB 查询数据的语法格式如下:db.user.find([query],[fields]) query :可选,使用查询操作符指定查询条件 fields :可选,使用投影操作符指定返回的键。查询时返回文档中所有键值, 只需省略该参数即可(默认省略)。 如果你需要以易读的方式来读取数据,可以使用 pretty() 方法,语法格式如下:db.col.find().pretty()pretty() 方法以格式化的方式来显示所有文档。条件查询: 操作 格式 范例 RDBMS中的类似语句 等于 {: } db.col.find({“by”:”yhd”}).pretty() where by = ‘yhd 小于 {:{$lt:}} db.col.find({“likes”:小于{$lt:50}}).pretty() where likes &lt; 50 小于或等于 {:{$lte:}} db.col.find({“likes”:{$lte:50}}).pretty() where likes &lt;=50 大于 {:{$gt:}} db.col.find({“likes”:{$gt:50}}).pretty() where likes &gt; 50 大于或等于 {:{$gte:}} db.col.find({“likes”:{$gte:50}}).pretty() where likes &gt;=50 不等于 {:{$ne:}} db.col.find({“likes”:{$ne:50}}).pretty() where likes !=50 123456789101112131415161718#插入测试数据db.user.insert(&#123;id:1,username:&#x27;zhangsan&#x27;,age:20&#125;)db.user.insert(&#123;id:1,username:&#x27;zhangsan&#x27;,age:20&#125;)db.user.insert(&#123;id:1,username:&#x27;zhangsan&#x27;,age:20&#125;)db.user.insert(&#123;id:1,username:&#x27;zhangsan&#x27;,age:20&#125;)db.user.find() #查询全部数据db.user.find(&#123;&#125;,&#123;id:1,username:1&#125;) #只查询id与username字段db.user.find().count() #查询数据条数db.user.find(&#123;id:1&#125;) #查询id为1的数据db.user.find(&#123;age:&#123;$lte:21&#125;&#125;) #查询小于等于21的数据db.user.find(&#123;age:&#123;$lte:21&#125;, id:&#123;$gte:2&#125;&#125;) #and查询,age小于等于21并且id大于等于2db.user.find(&#123;$or:[&#123;id:1&#125;,&#123;id:2&#125;]&#125;) #查询id=1 or id=2#分页查询:Skip()跳过几条,limit()查询条数db.user.find().limit(2).skip(1) #跳过1条数据,查询2条数据db.user.find().sort(&#123;id:-1&#125;) #按照age倒序排序,-1为倒序,1为正序 四，索引索引通常能够极大的提高查询的效率,如果没有索引,MongoDB在读取数据时必须扫描集合中的每个文件并选取那些符合查询条件的记录。这种扫描全集合的查询效率是非常低的,特别在处理大量的数据时,查询可以要花费几十秒甚至几分钟,这对网站的性能是非常致命的。索引是特殊的数据结构,索引存储在一个易于遍历读取的数据集合中,索引是对数据库表中一列或多列的值进行排序的一种结构。 123456789101112131415161718192021222324252627282930313233#查看索引&gt; db.user.getIndexes()[ &#123; &quot;v&quot; : 2, &quot;key&quot; : &#123; &quot;_id&quot; : 1 &#125;, &quot;name&quot; : &quot;_id_&quot;, &quot;ns&quot; : &quot;testdb.user&quot; &#125;]#说明:1表示升序创建索引,-1表示降序创建索引。#创建索引&gt; db.user.createIndex(&#123;&#x27;age&#x27;:1&#125;)&#123; &quot;createdCollectionAutomatically&quot; : false, &quot;numIndexesBefore&quot; : 1, &quot;numIndexesAfter&quot; : 2, &quot;ok&quot; : 1&#125;#删除索引db.user.dropIndex(&quot;age_1&quot;)#或者,删除除了_id之外的索引db.user.dropIndexes()#创建联合索引db.user.createIndex(&#123;&#x27;age&#x27;:1,&#x27;id&#x27;:-1&#125;)#查看索引大小,单位:字节db.user.totalIndexSize() 五，执行计划MongoDB 查询分析可以确保我们建议的索引是否有效,是查询语句性能分析的重要工具。 123456789101112#插入1000条数据for(var i=1;i&lt;1000;i++)db.user.insert(&#123;id:100+i,username:&#x27;name_&#x27;+i,age:10+i&#125;)#查看执行计划db.user.find(&#123;age:&#123;$gt:100&#125;,id:&#123;$lt:200&#125;&#125;).explain() #查询方式,常见的有COLLSCAN/全表扫描、IXSCAN/索引扫描、FETCH/根据索引去检索文档、SHARD_MERGE/合并分片结果、IDHACK/针对_id进行查询#测试没有使用索引db.user.find(&#123;username:&#x27;zhangsan&#x27;&#125;).explain() 六，UI客户端Robo 3T是MongoDB的客户端工具,我们可以使用它来操作MongoDB。​ 七，java连接Mongo基本CRUD1.依赖jar包123456789101112&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;mongo-java-driver&lt;/artifactId&gt; &lt;version&gt;3.0.4&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 2.获取连接工具类1234567891011121314151617181920212223242526272829303132333435363738/** * mongodb 连接数据库工具类 */public class MongoDBUtil &#123; //不通过认证获取连接数据库对象 public static MongoDatabase getConnect()&#123; //连接到 mongodb 服务 MongoClient mongoClient = new MongoClient(&quot;localhost&quot;, 27017); //连接到数据库 MongoDatabase mongoDatabase = mongoClient.getDatabase(&quot;test&quot;); //返回连接数据库对象 return mongoDatabase; &#125; //需要密码认证方式连接 public static MongoDatabase getConnect2()&#123; List&lt;ServerAddress&gt; adds = new ArrayList&lt;&gt;(); //ServerAddress()两个参数分别为 服务器地址 和 端口 ServerAddress serverAddress = new ServerAddress(&quot;localhost&quot;, 27017); adds.add(serverAddress); List&lt;MongoCredential&gt; credentials = new ArrayList&lt;&gt;(); //MongoCredential.createScramSha1Credential()三个参数分别为 用户名 数据库名称 密码 MongoCredential mongoCredential = MongoCredential.createScramSha1Credential(&quot;username&quot;, &quot;databaseName&quot;, &quot;password&quot;.toCharArray()); credentials.add(mongoCredential); //通过连接认证获取MongoDB连接 MongoClient mongoClient = new MongoClient(adds, credentials); //连接到数据库 MongoDatabase mongoDatabase = mongoClient.getDatabase(&quot;test&quot;); //返回连接数据库对象 return mongoDatabase; &#125;&#125; 3.增删改查1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465/** * 插入数据 * mongoDB中的数据都是通过文档（对应于关系型数据库表中的一行）保存的，而文档又保存在集合（对应于关系型数据库的表）中。 */@Testpublic void insertTest() &#123; //1.获取集合 这里的yhd指的是集合的名字 如果指定的集合不存在 mongoDB会在你第一次插入文档时创建集合 MongoCollection&lt;Document&gt; collection = MongoDBUtil.getConnect().getCollection(&quot;yhd&quot;); //2.创建文档 要插入文档需要先创建文档对象 Document document = new Document(&quot;name&quot;, &quot;张三&quot;).append(&quot;age&quot;, &quot;20&quot;).append(&quot;sex&quot;, &quot;男&quot;); //3.插入文档 //3.1 插入一个文档 使用 MongoCollection 对象的 insertOne() 方法，该方法接收一个 Document 对象作为要插入的数据 collection.insertOne(document); //3.2 插入一堆文档 使用 MongoCollection 对象的 insertMany() 方法，该方法接收一个 数据类型为 Document 的 List 对象作为要插入的数据 collection.insertMany(list()); //4.查询文档 使用 MongoCollection 对象的 find() 方法，该方法有多个重载方法，可以使用不带参数的 find() 方法查询集合中的所有文档， // 也可以通过传递一个 Bson 类型的 过滤器查询符合条件的文档。这几个重载方法均返回一个 FindIterable 类型的对象， // 可通过该对象遍历出查询到的所有文档。 //4.1简单查询 FindIterable&lt;Document&gt; documents = collection.find(); MongoCursor&lt;Document&gt; iterator = documents.iterator(); while (iterator.hasNext())&#123; System.out.println(&quot;iterator.next() = &quot; + iterator.next()); &#125; //4.2带过滤器查询 Bson query = Filters.eq(&quot;name&quot;, &quot;张三&quot;); FindIterable&lt;Document&gt; result = collection.find(query); MongoCursor&lt;Document&gt; cursor = result.iterator(); while (cursor.hasNext())&#123; System.out.println(&quot;cursor.next() = &quot; + cursor.next()); &#125; //4.3 取出查询的第一个文档 Document first = result.first(); System.out.println(&quot;first = &quot; + first); //5.修改文档 //5.1修改单个 使用 MongoCollection 对象的 updateOne() 方法，该方法接收两个参数， // 第一个数据类型为 Bson 的过滤器筛选出需要修改的文档，第二个参数数据类型为 Bson 指定如何修改筛选出的文档。 // 然后修改过滤器筛选出的第一个文档。 Bson filter = Filters.eq(&quot;name&quot;, &quot;张三&quot;); Document doc = new Document(&quot;$set&quot;, new Document(&quot;age&quot;, 100)); collection.updateOne(filter, doc); //5.2修改一堆 使用 MongoCollection 对象的 updateMany() 方法，该方法接收两个参数， // 第一个数据类型为 Bson 的过滤器筛选出需要修改的文档，第二个参数数据类型为 Bson 指定如何修改筛选出的文档。 // 然后修改过滤器筛选出的所有文档。 collection.updateMany(filter,doc); //6.删除 删除与筛选器匹配的单个文档，使用 MongoCollection 对象的 deleteOne() 方法， // 该方法接收一个数据类型为 Bson 的的对象作为过滤器筛选出需要删除的文档。 // 然后删除第一个。为了便于创建过滤器对象，JDBC驱动程序提供了 Filters 类。 Bson delete = Filters.eq(&quot;age&quot;,18); collection.deleteOne(delete); //删除与筛选器匹配的所有文档，使用 MongoCollection 对象的 deleteMany() 方法， // 该方法接收一个数据类型为 Bson 的的对象作为过滤器筛选出需要删除的文档。然后删除所有筛选出的文档。 collection.deleteMany(delete);&#125;private List&lt;Document&gt; list()&#123; Document a = new Document(&quot;name&quot;, &quot;李四&quot;).append(&quot;age&quot;, &quot;21&quot;).append(&quot;sex&quot;, &quot;男&quot;); Document b = new Document(&quot;name&quot;, &quot;王五&quot;).append(&quot;age&quot;, &quot;22&quot;).append(&quot;sex&quot;, &quot;女&quot;); Document c = new Document(&quot;name&quot;, &quot;赵六&quot;).append(&quot;age&quot;, &quot;23&quot;).append(&quot;sex&quot;, &quot;女&quot;); List&lt;Document&gt; list = new ArrayList&lt;&gt;(); list.add(a); list.add(b); list.add(c); return list;&#125; 八，SpringBoot整合Mongospring-data对MongoDB做了支持,使用spring-data-mongodb可以简化MongoDB的操作。 1.dependency1234567891011121314151617&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-mongodb&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 2.config1234spring.application.name=spring-boot-mongospring.data.mongodb.uri=mongodb://root:root@172.17.0.3:27017spring.data.mongodb.database=yhd 3.Person实体类123456789101112131415@Data@Accessors(chain = true)@NoArgsConstructor@AllArgsConstructorpublic class Person implements Serializable &#123; private Integer id; private String name; private Integer age; private String gender;&#125; 4.PersonDao12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455@Componentpublic class PersonDao &#123; @Resource private MongoTemplate mongoTemplate; /** * 保存对象 * @param person */ public void savePerson(Person person)&#123; mongoTemplate.save(person); &#125; /** * 以名字为条件批量查询 * @param name * @return */ public List&lt;Person&gt; queryPersonListByName(String name)&#123; Query query = Query.query(Criteria.where(&quot;name&quot;).is(name)); return mongoTemplate.find(query,Person.class); &#125; /** * 分页查询 * @param page * @param size * @return */ public List&lt;Person&gt; queryPersonList(Integer page, Integer size)&#123; Query query = new Query().limit(size).skip((page - 1) * size); return this.mongoTemplate.find(query, Person.class); &#125; /** * 修改对象 * @param person * @return */ public UpdateResult update(Person person)&#123; Query query = Query.query(Criteria.where(&quot;id&quot;).is(person.getId())); Update update = Update.update(&quot;age&quot;, person.getAge()); return mongoTemplate.updateFirst(query,update,Person.class); &#125; /** * 删除对象 */ public DeleteResult deleteById(Integer id) &#123; Query query = Query.query(Criteria.where(&quot;id&quot;).is(id)); return this.mongoTemplate.remove(query, Person.class); &#125;&#125; 5.Test12345678910111213141516171819202122232425262728293031323334353637383940414243@SpringBootTestclass SpringbootMongoApplicationTests &#123; @Autowired private PersonDao personDao; @Test void testSave() &#123; Person person = new Person(1, &quot;张三&quot;, 20, &quot;男&quot;); this.personDao.savePerson(person); &#125; @Test void testQuery() &#123; List&lt;Person&gt; personList = this.personDao.queryPersonListByName(&quot;张三&quot;); for (Person person : personList) &#123; System.out.println(person); &#125; &#125; @Test void testQuery2() &#123; List&lt;Person&gt; personList = this.personDao.queryPersonList(2, 2); for (Person person : personList) &#123; System.out.println(person); &#125; &#125; @Test void testUpdate() &#123; Person person = new Person(); person.setId(1); person.setAge(30); this.personDao.update(person); &#125; @Test void testDelete() &#123; this.personDao.deleteById(1); &#125;&#125;","categories":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://yinhuidong.github.io/categories/MongoDB/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://yinhuidong.github.io/tags/MongoDB/"}]},{"title":"二十理解JVM","slug":"JVM/二十理解JVM","date":"2022-01-12T00:19:55.246Z","updated":"2022-01-12T00:19:55.249Z","comments":true,"path":"2022/01/12/JVM/二十理解JVM/","link":"","permalink":"https://yinhuidong.github.io/2022/01/12/JVM/%E4%BA%8C%E5%8D%81%E7%90%86%E8%A7%A3JVM/","excerpt":"","text":"一，JVM与Java体系结构1，虚拟机与java虚拟机概述1）虚拟机所谓虚拟机就是一台虚拟的计算机。它是一款软件，用来执行一系列虚拟计算机指令。大体上，虚拟机可以分为系统虚拟机和程序虚拟机。 Visual Box ，VMware就属于系统虚拟机，他们完全是对物理计算机的仿真，提供了一个可以运行完整操作系统的软件平台。 程序虚拟机的典型代表是Java虚拟机，他专门为了执行单个计算机程序而设计，在java虚拟机中执行的指令我们称为Java字节码指令。 无论是系统虚拟机还是程序虚拟机，在上面运行的软件都被限制于虚拟机提供的资源中。 2）Java虚拟机java虚拟机是一台执行java字节码的虚拟计算机，它拥有独立的运行机制，其运行的java字节码也未必由java语言编译而成。 JVM平台的各种语言可以共享Java虚拟机带来的跨平台性，优秀的垃圾回收器，以及可靠的即时编译器。 Java技术的核心就是Java虚拟机，因为所有的Java程序都运行在Java虚拟机内部。 作用 java虚拟机就是二进制字节码的运行环境，负责装载字节码到其内部，解释编译为对应平台的机器指令执行。每一条java指令，java虚拟机规范中都有详细定义，如怎么取操作数，怎么处理操作数，处理结果放在哪里。 特点 一次编译，到处运行 自动内存管理 自动垃圾回收功能 2，JVM的位置 jvm是运行在操作系统之上，他与硬件没有直接的交互。 3，JVM的整体结构 HotSpot VM是目前市面上高性能虚拟机代表作之一。 他采用解释器与即时编译器并存的架构。 在今天，Java程序的运行性能早已脱胎换骨，已经达到了可以和C/C++程序一较高下的地步。 内存当中，多线程共享方法区和堆空间，线程私有的是java栈，本地方放栈，程序计数器。 4，Java代码执行流程java程序编译成字节码文件，字节码执行后运行在不同的平台上。 .java文件-&gt;java编译器(词法分析-语法分析-语法、抽象语法树-语义分析-注解抽象语法树-字节码生成器)-&gt;.class文件-&gt;java虚拟机(类加载器-字节码校验器-翻译字节码(解析执行)，JIT编译器(编译执行))-操作系统 5，jvm的架构模型java编译器输入的指令流基本上是一种基于栈的指令集架构，另一种指令集架构则是基于寄存器的指令集架构。 这两种架构之间的区别： 基于栈的指令集架构 1.设计和实现更简单，适用于资源受限的系统 2.避开了寄存器的分配难题，使用0地址指令方式分配 3.指令流中的大部分指令是0地址指令，其执行过程依赖于操作栈，指令集更小 4.不需要硬件支持，可移植性更好，更好实现跨平台 基于寄存器的指令集架构 1.典型的应用是x86的二进制指令集；比如传统的PC以及安卓的Davlik虚拟机。 2.指令级架构则完全依赖于硬件，可移植性差 3.性能优秀和执行更高效 4.花费更少的指令去完成一项操作 5.在大部分情况下，基于寄存器的指令集架构往往都是一地址指令，二地址指令和三地址指令为主，而基于栈式的指令集架构是以0地址指令为主。 0地址指令是只操作栈顶元素，所以舍弃了地址，只保留一个操作数，但是一进制指令是一个地址对应一个操作数。 总结：由于java语言跨平台的设计，所以java的指令都是根据栈来设计的，不同平台的CPU架构不同，所以不能设计为基于寄存器的，优点是跨平台性，指令集小，编译容易实现，缺点是性能下降，实现同样的功能需要更多的指令。 时至今日，尽管嵌入式平台已经不是java程序的主流运行平台，那么为什么不讲java的指令集架构更换为基于寄存器的？ 因为没有必要，基于栈的指令级架构同样适用于非资源受限的操作系统。 栈：跨平台性，指令集小，指令多，执行性能比寄存器差 6，JVM的生命周期1）启动java虚拟机的启动是通过引导类加载器创建一个初始类来完成的，这个类是由虚拟机的具体实现指定的。 2）执行一个运行中的java虚拟机有一个任务：执行java程序。 程序开始执行时他才运行，程序结束时他就停止。 执行所谓的java程序的时候，真真正正执行的是一个叫做java虚拟机的进程。 3）销毁程序正常执行结束 程序执行过程中发生异常或者错误 操作系统出现错误导致虚拟机终止 调用System或Runtime类的exit方法或者runtime类里面的halt方法，并且java安全管理器也允许这次退出 JNI API加载或者卸载java虚拟机的时候 4）类的加载过程父类的静态代码块和静态属性 子类的静态代码块和静态属性 父类的普通代码块和普通属性 父类的构造器 子类的普通代码块和普通属性 子类的构造器 7，HotSpot虚拟机jdk3开始 HotSpot虚拟机成为默认的虚拟机 特点：热点探测技术 通过计数器找到最具有编译价值的代码，触发即时编译或者栈上替换 通过编译器与解释器协同工作，在最优化的响应时间与最佳执行性能中取得平衡 对象不一定在堆中创建，也可以栈上分配，便于垃圾回收 二，类加载子系统1.内存结构概述类加载器系统加载（加载，链接，初始化）字节码文件到运行时数据区 执行引擎（解释器，JIT即时编译器，GC垃圾回收器）根据字节码指令执行程序，本地方法接口 2.类加载器和类的加载过程类加载器子系统负责从文件系统或者网络中加载Class文件，class文件在文件开头有特定的文件标识。 类加载器只负责class文件的加载，至于他是否可以运行，由执行引擎决定。 加载类的信息存放在元空间，除了类的信息，元空间还有运行时常量池（常量池加载到内存中就叫做运行时常量池）。 可能还包括字符串字面量和数字常量（这部分常量信息是class文件中常量池部分的内存映射） 过程：加载-&gt;链接（验证，准备，解析）-&gt;初始化 ClassLoader：class文件加载到jvm，过程中需要一个运输工具（类加载器）。 3.类的加载详细过程1）加载通过全限定类名加载一个类的二进制字节流，将静态结构转化为方法区的运行时数据结构，在内存中生成一个代表这个类的Class对象 加载.class文件的方式： 从本地系统中直接加载 通过网络获取，典型场景：Web Applet 从zip压缩包中读取，成为日后jar ，war格式的基础 运行时计算生成，使用最多的是：动态代理技术 其他文件生成：典型场景JSP 从专有数据库提取.class文件，比较少见 从加密文件中获取，典型的防Class文件被反编译的保护措施 2）链接1.验证：确保class文件内容不会危害到当前虚拟机 2.准备：为类变量分配内存并设置初始值，不会为实例变量分配空间初始化，类变量分配在方法区，实例变量分配在堆空间。 3.解析：将常量池的符号引用转换为直接引用 3）初始化执行类构造器方法（完成静态属性和静态代码块变量的赋值操作）的过程，此方法不需要定义，是javac完成的。 clinit()不同于类的构造器,他只会加载一次。若该类具有父类，JVM会保证子类的clinit()执行前，父类的clinit()已经执行完毕。 虚拟机必须保证一个类的clinit()方法在多线程下被同步加锁。 任何一个类声明以后，内部至少存在一个类的构造器 4.类加载器分类1）引导类加载器启动类加载器 BootStrap ClassLoader 加载java核心类库，只加载java javax sun开头的类 2）系统类加载器extends ClassLoader java ClassLoader.getSystemClassLoader(); 加载用户自定义类，父类加载器为扩展类加载器 3）拓展类加载器extends ClassLoader java SystemClassLoader.getParent(); 4）用户自定义类加载器extends ClassLoader 1.为什么要用户自定义类加载器？ 隔离加载类 修改类加载的方式 扩展加载源 防止源码泄露 2.用户自定义类加载器步骤？ jdk1.2之前，继承ClassLoader重写loadClass().jdk1.2之后建议重写findClass() 也可以继承URLClassLoader,避免了自己编写findClass()以及获取自己码流的方式。 3.ClassLoader 它是一个抽象类，其后所有的类加载器都继承自ClassLoader(不包括启动类加载器) 1234getParent() 返回父类加载器loadClass(String name) 返回Class实例findClass(String name) 返回Class实例defineClass(String name,byte[] b ,int off,int len) 把字节数组b中的内容转换为一个java类，返回结果为java.lang.Class类的实例 5.双亲委派机制java虚拟机对class文件采用的是按需加载，而且加载某个类的class文件时，java虚拟机采用的是双亲委派机制，就是把请求交由父类加载器处理，它是一种任务委派模式 1）工作原理如果一个类加载器收到了类加载请求，他并不会自己先去加载，而是把这个请求委托给父类的加载器去执行。 如果父类加载器还存在其父类加载器，则进一步向上委托，依次递归，请求最终将达到顶层的启动类加载器。 如果父加载器可以完成类加载任务，就成功返回，倘若父加载器无法完成此类加载任务，子加载器才会尝试自己去加载。 2）举例调用JDBC接口，接口是引导类加载器加载的，但是实现类是系统类加载器加载的。 3）优点避免类的重复加载 保护程序安全，避免核心API被篡改 6.沙箱安全机制自定义的String类，但是在加载自定义String类的时候会率先使用引导类加载器加载，而引导类加载器在加载的过程中会先加载jdk自带的文件，报错信息说没有main方法，就是因为加载的是jdk的String，这样可以保证对Java核心API源码的保护，这就是沙箱安全机制。 7.其他1）同一个类？全限定类名 加载这个类的类加载器实例对象必须相同（在JVM中，即使这两个类对象来源于同一个Class文件，被同一个虚拟机所加载，只要加载他们的ClassLoader实例对象不同，那么这两个类对象也是不相等的） jvm必须知道一个类型是由启动类加载器加载的还是由用户类加载器加载的。如果一个类型是由用户类加载器加载的，那么jvm会将这个类加载器的一个引用作为类型信息的一部分保存在方法区中，当解析一个类型到另一个类型的引用的时候，JVM需要保证这两个类型的类加载器是相同的。 2）类的主动使用和被动使用区别在于会不会导致类的初始化 ①主动使用创建类的实例 访问某个类或接口的静态变量，或者对该静态变量赋值 调用类的静态方法 反射 初始化一个类的子类 java虚拟机启动时候被表明为启动类的类 java7提供的动态语言支持 某些类clinit方法的执行 ②被动使用除了主动使用的，都是被动使用 三，运行时数据区1.内部结构JVM内存布局规定了java在运行过程中内存申请，分配，管理的策略，保证了JVM高效稳定的运行。不同的JVM对于内存的划分方式和管理机制存在着部分差异。 2.结构线程私有的：java虚拟机栈 本地方法栈 程序计数器 线程间共享的：堆（0.95的垃圾回收） 方法区（0.05的垃圾回收） 3.线程线程是一个程序里面的运行单元，JVM允许一个应用有多个线程并行的执行。 在HotSpot虚拟机，每个线程都与操作系统的本地线程直接映射。 当一个Java线程准备好执行以后，此时操作系统的本地线程也同时创建。java线程执行终止后，本地线程也会回收。 操作系统负责所有线程的安排调度到任何一个可用的CPU上。一旦本地线程初始化成功，他就会调用Java线程中run(); 当run正常执行完，或者出现异常后有相应的异常处理机制，也都算正常执行完，这时候java线程和本地线程都会回收，资源得到释放。 如果run方法执行中，出现一些为捕获的异常，这时就会导致java线程被终止，java线程终止以后，本地线程来决定jvm到底要不要终止（jvm要不要终止取决于当前线程是不是最后一个用户线程，如果剩下的都是守护线程，虚拟机就会退出了） 四，程序计数器-PCJVM中的程序计数器，命名源于CPU的寄存器，寄存器存储相关的现场信息，CPU只有把数据装载到寄存器才能运行，这里，也不是指广义上的物理寄存器，或许将其翻译为PC计数器，会更贴切，并且也不容易引起一些不必要的误会，JVM中的PC寄存器是对物理PC寄存器的一种抽象。 程序计数器是用来存储指向下一条指令的地址，也即将要执行的指令代码，由执行引擎读取下一条指令。 运行速度最快的区域。线程私有的，生命周期与线程的生命周期一致。 任何时间一个线程都只有一个方法在执行，也就是所谓的当前方法，程序计数器会存储当前线程正在执行的Java方法的JVM指令地址，或者，如果在执行native方法，则为空。 引用自jvm规范： 1If the method currently being executed by the thread is native, the value of the Java Virtual Machine&#x27;s pc register is undefined 程序计数器存放的是Java字节码的地址，而native方法的方法体是非Java的，所以程序计数器的值才未定义。 那在native方法执行后，线程又如何确保下一次执行的位置？ 这是因为每个Java线程都直接映射到一个OS线程上执行。所以native方法就在本地线程上执行，无需理会JVM规范中的程序计数器的概念。仔细看一下JVM规范，如果一个线程执行Native方法，程序计数器的值未定义，可不是一定为空，任何值都可以。native方法执行后会退出(栈帧pop)，方法退出返回到被调用的地方继续执行程序。 循环，分支跳转，异常处理，线程恢复都是依赖于程序计数器 字节码解释器工作时，就是通过改变这个计数器的值来选取下一条需要执行的字节码指令 JVM规范中唯一一个没有内存溢出错误的区域 1.使用PC寄存器存储字节码指令地址有什么作用？ 因为cpu需要不停地切换各个线程，这时候切换回来以后，就得知道接着从哪里继续执行。JVM的字节码解释器就需要通过改变PC寄存器的值来明确下一条应该执行什么样的字节码指令。 2.pc寄存器为什么要设置成线程私有的？ 每一个线程都需要一个PC线程计数器来记录执行到了哪条字节码指令。 cpu时间片 CPU分配给各个程序的时间，每个线程被分配一个时间段，称作他的时间片。微观上，由于只有一个cpu，一次只能处理程序要求的一部分，如何处理公平，一种方法就是引入时间片，每个程序轮流执行。 并行vs串行 并行：几个线程同时执行 串行：几个线程一个一个执行 并发：垃圾回收线程和用户线程同时执行 五，虚拟机栈1，虚拟机栈出现的背景由于跨平台性的设计，java的指令集架构是基于栈的指令集架构。不同平台cpu架构不同，所以不能设计为基于寄存器的指令集架构。 优点是跨平台，指令集小，编译器容易实现，缺点是性能下降，实现同样的功能需要更多的指令。 2．内存中的栈与堆栈是运行时单位，堆是存储的单位。 栈解决的是程序的运行问题，既程序如何执行，或者说如何处理数据，堆解决的是数据存储的问题，既数据怎么放，放在哪里。 栈中可以存放方法内的基本类型变量和引用变量的地址。 3．虚拟机栈基本内容每个线程创建的时候都会创建一个ｊａｖａ虚拟机栈，其内部保存一个个的栈帧，对应着一次次方法的调用（一个栈帧对应着一个ｊａｖａ方法）。 栈顶的方法被称为当前方法。栈是线程私有的，生命周期和线程一致。 1）作用主管ｊａｖａ程序的运行，他保存方法的局部变量（８种数据类型，对象的引用地址），部分结果，并参与方法的调用和返回。 2）栈的特点栈是一种快速有效的分配存储方式，访问速度仅仅次于程序计数器 jvm堆栈的直接操作只有两个 ：方法执行，入栈；方法结束，出栈。 对于栈来说，不存在垃圾回收问题。 4.栈的常见异常和参数设置jvm规范允许栈的大小是动态的或者是固定不变的。 如果采用固定大小的java栈，那每一个线程的虚拟机容量可以在线程创建的时候独立选定，如果线程请求分配的栈容量超过java虚拟机允许的最大容量，jvm会抛出StackOverFlowError（递归没有出口）。 特别的，在hotspot虚拟机中，如果在为栈分配内存时，内存不足，抛出的异常并不是oom，而是StackOverFlowError。 如果java虚拟机栈可以动态扩容，并且在尝试扩展的时候无法申请到足够的内存，或者在创建新的线程是没有足够的内存去创建对应的虚拟机栈，那java虚拟机会抛出oom。 设置栈的大小 -Xss 设置线程的最大栈空间，栈的大小直接决定了函数调用的最大可达参数。 1-Xss256k 5．栈的存储结构和运行原理每个线程都有自己的栈，栈中的数据都是以栈帧的形式存在。 在这个线程上正在执行的每个方法都各自对应一个栈帧。 栈帧是一个内存区块，是一个数据集，维系着方法执行过程中各种数据信息。 １）栈的运行原理一个线程中，一个时间点上，只会有一个活动的栈帧，既只有当前正在执行的方法的栈帧是有效的，这个栈帧被称为当前栈帧。与这个栈帧对应的方法是当前方法，定义当前方法的类叫做当前类。 执行引擎运行的所有字节码指令只针对当前栈帧进行操作。 如果在该方法中调用了其他方法，对应的新的栈帧被创建出来，放在栈顶，成为新的当前栈帧。 不同线程中所包含的栈帧是不允许相互引用的，既不可能在一个栈帧之中引用另外一个线程的栈帧。 如果当前方法调用了其他方法，方法返回时，当前栈帧会传回此方法的执行结果给前一个栈帧，接着，虚拟机会丢弃当前栈帧，使得前一个栈帧成为当前栈帧。 Java方法有两种返回函数的方式，一种是正常的函数返回，使用return指令，另一种是抛出异常，不管使用哪种方式，栈帧都会弹出。 ２）栈帧的内部结构每个栈帧中存储着 局部变量表 操作数栈 动态链接（指向运行时常量池的方法引用） 方法返回地址 一些附加信息 ３）局部变量表本地变量表：定义为一个数字数组，主要用于存储方法参数和定义在方法体内的局部变量，这些数据类型包括各种基本数据类型，对象引用，以及返回地址类型。 由于局部变量表是建立在线程的栈上的，是线程私有数据，因此不存在数据安全问题。 局部变量表所需容量大小是在编译器确定下来的，并保存在方法的Ｃｏｄｅ属性的ｍａｘｉｍｕｎ ｌｏｃａｌ ｖａｒｉａｂｌｅｓ数据项，方法运行期间变量表的大小是不会改变的。 参数值的存放总是在局部变量数组的ｉｎｄｅｘ０开始，到数组长度－１的索引结束。 局部变量表，最基本的存储单元是变量槽。 局部变量表中存放编译期间可知的各种基本数据类型，引用类型，ｒｅｔｕｒｎＡｄｄｒｅｓｓ类型。 方法嵌套的次数由栈的大小决定，局部变量表中的变量只在当前方法调用中有效，局部变量表随着栈帧销毁。 ４）关于ｓｌｏｔ的理解参数值的存放总是在局部变量表的ｉｎｄｅｘ０开始，到数组长度－１的索引结束。 局部变量表，最基本的存储单元是变量槽 ｓｌｏｔ 局部变量表中存放着编译期可知的各种基本数据类型，引用类型，ｒｅｔｕｒｎＡｄｄｒｅｓｓ的变量。 在局部变量表里，３２位以内的类型只占用一个ｓｏｒｔ，６４占用两个ｓｏｒｔ。 ｊｖｍ会为局部变量表中的每一个ｓｌｏｔ都分配一个访问索引，通过这个索引就可以成功访问到局部变量表中指定的局部变量值。 当一个实例方法被调用的时候，他的方法参数和方法体内部定义的局部变量将会按照顺序被复制到局部变量表中的每一个slot上。 如果需要访问一个局部变量表中64bit的局部变量值时，只需要使用一个索引即可。 如果当前帧是由构造方法或者实例方法(非static)创建的，那么该对象引用this将会存放在index为0的slot处，其余的参数按照参数表顺序继续排列。 为什么静态方法不能使用ｔｈｉｓ？ 静态方法的局部变量表没有ｔｈｉｓ ｓｌｏｔ的重复利用：栈帧中的局部变量表中的槽位是可以重复利用的，如果一个局部变量过了其作用域，那么再其作用域之后声明的新的局部变量就很有可能会复用过期的局部变量的槽位，从而达到节省资源的目的。 面试题：为什么if建议写大括号？ 为了栈帧局部变量表里的slot槽能够最大可能的重复利用 6．静态变量与局部变量的对比变量按照数据类型：基本数据类型和引用数据类型 变量按照类中声明位置：成员变量（类变量 ｓｔａｔｉｃ ，局部变量） 局部变量 成员变量 在使用前都经过默认赋值，ｉｎｔ显式赋值。 实例变量 随着对象创建会在堆空间中分配实例变量空间，并进行默认赋值。 局部变量显式赋值，否则没法使用。 在栈帧中，与性能调优最为密切的就是局部变量表，在方法执行时，虚拟机使用局部变量表完成方法的传递。 在局部变量表中的变量也是重要的垃圾回收根节点，只要被局部变量表中直接引用或间接引用的对象都不会被回收。 7．操作数栈栈的结构可以是数组或者单向链表，操作数栈是数组实现的。 操作数栈，在方法执行过程中，根据字节码指令，往栈中写入数据或者提取数据。 某些字节码指令将数值压入操作数栈，其余的字节码指令将操作数取出栈，使用他们后再把结果压入栈。 执行复制，操作，求和。 如果被调用的方法带有返回值的话，其返回值将会被压入当前栈帧的操作数栈中，并更新ＰＣ寄存器中下一条需要执行的字节码指令。 操作数栈中元素的数据类型必须与字节码指令的序列严格匹配，这由编译器在编译期间进行验证，同时在类加载过程中的类检验阶段的数据流分析阶段要再次验证。 操作数栈，主要用于保存计算过程的中间结果，同时作为计算过程中变量的临时存储空间。 每一个操作数栈都会拥有一个明确的栈深度用于存储数值，其所需的最大深度在编译期间就定义好了，保存在方法的ｃｏｄｅ属性，为ｍａｘ＿ｓｔａｃｋ的值。 栈中的任何元素都是可以任意的ｊａｖａ数据类型。 ｊａｖａ虚拟机的解释引擎是基于栈的执行引擎，其中的栈指的就是操作数栈。 涉及操作数栈的字节码指令执行分析 8．栈顶缓存技术由于操作数是存储在内存中，因此频繁的执行内存读写必然会影响执行速度。为了解决这个问题，HotSpot JVM的设计者们提出了栈顶缓存技术，将栈顶元素全部缓存在物理CPU的寄存器中，以此降低对内存的读写次数，提升执行引擎的执行效率。 9．动态链接指向运行时常量池的方法引用。 每一个栈帧的内部都包含一个指向运行时常量池中该栈帧所属方法的引用。包含这个引用的目的就是为了支持当前方法的代码能够实现动态链接。 在ｊａｖａ源文件在编译到字节码文件中时，所有的变量和方法引用都作为符号引用保存在ｃｌａｓｓ文件的常量池里。比如描述一个方法调用了另外的其他方法时，就是通过常量池中指向方法的符号引用来表示的，那么动态链接的作用就是为了将这些符号引用转换为调用方法的直接引用。 为什么需要常量池？为了提供一些符号和常量，便于指令识别。 10.方法的调用在ｊｖｍ中，将符号引用转换为调用方法的直接引用与方法的绑定机制有关。 1）动态链接与静态链接静态链接：当一个字节码文件被装载到JVM内部时，如果被调用的 目标方法在编译期可知 ，且运行期保持不变时，这种情况下调用方法的符号引用转换为直接引用的过程称之为静态链接。 动态链接：如果 被调用的方法在编译期无法被确定下来 ，也就是说，只能够在程序运行期将调用方法的符号引用转换为直接引用，由于这种引用转换过程具备动态性，因此也就被称之为动态链接。 2）对应方法的绑定机制早期绑定(Early Binding)和晚期绑定(Late Binding)。 绑定是一个字段(属性)、方法或者类在符号引用被替换为直接引用的过程，这仅仅发生一次。 1.早期绑定就是指被调用的 目标方法如果在编译期可知，且运行期保持不变时 ，即可将这个方法与所属的类型进行绑定，这样一来，由于明确了被调用的目标方法究竟是哪一个，因此也就可以使用静态链接的方式将符号引用转换为直接引用。 2.如果 被调用的方法在编译期无法被确定下来，只能够在程序运行期根据实际的类型绑定相关的方法 ，这种绑定方式也就被称之为晚期绑定。 Java中任何一个普通的方法其实都具备虚函数的特征，它们相当于C语言中的虚函数(C中则需要使用关键字virtual 来显示定义)。 如果在Java程序中不希望某个方法拥有虚函数的特征时，则可以使用关键字final来标记这个方法。 3）4种方法调用指令区分非虚方法与虚方法虚方法与非虚方法 如果方法在编译期就确定了具体的调用版本，这个版本再运行时是不可变的。这样的方法称为非虚方法。 静态方法、私有方法、final方法、实例构造器、父类方法都是非虚方法。其他方法称为虚方法。 (子类对象的多态性的使用前提：①类的继承关系 ②方法的重写) 动态类型语言和静态类型语言 1、动态类型语言和静态类型语言两者的区别就在于对类型的检查是在编译期还是在运行期，满足前者就是静态类型语言，反之是动态类型语言。 2、静态类型语言是判断变量自身的类型信息 3、动态类型语言是判断变量值的类型信息。变量没有类型信息，变量值才有类型信息，这是动态语言的一个重要特征。 4）方法重写的本质与虚方法表的使用方法重写的本质 找到操作数栈顶的第一个元素所执行的对象的实际类型，记做Ｃ。 如果在类型Ｃ中找到与常量中的描述符和简单名称都相符的方法，则进行访问权限校验，如果通过则返回这个方法的直接引用，查找过程结束；如果不通过则返回java.lang.IllegalAccessError。 否则，按照继承关系从下往上依次对C的各个父类进行第二步的搜索和验证过程。 如果始终没有找到合适的方法，则抛出java.lang.AbstractMethodError异常。 java.lang.IllegalAccessError 程序试图访问或者修改一个属性或调用一个方法，这个属性或者方法，你没有权限访问，一般的，这个会引起编译期异常，这个错误 如果发生在运行时，就说明一个类发生了不兼容的改变。 补充：多态的本质就是指向同一个虚方法表的引用。 11.方法返回地址存放着调用该方法的pc寄存器的值 一个方法结束，有两种方式： 1.正常执行完成 2.出现未处理的异常，非正常退出 无论通过哪种方式退出，在方法退出后都要返回到该方法被调用的位置，方法正常退出时，调用者的PC寄存器的值作为返回地址；即调用该方法的指令的下一条指令的地址，而通过异常退出的，返回的地址需要通过异常表来确定，栈帧中一般不会保存这部分信息。 当一个方法开始执行后，只有两种方式可以退出方法： 1.当执行引擎遇到任意一个方法返回的字节码指令，会有返回值传递给上层的方法调用者，简称正常完成出口。 一个方法在正常执行完成之后究竟需要使用哪一个返回指令还需要根据方法返回值的实际参数类型而定。 在字节码指令中，返回指令包含ireturn（int），lreturn（long），freturn（float），dreturn（double），areturn是引用类型，另外还有一个return指令供声明为void的方法，实例初始化方法，类和接口初始化方法使用。 在方法执行过程中遇到了异常，并且这个异常没有在方法内进行处理，也就是只要在本地方法的异常表中没有搜索到匹配的异常处理器，就会导致方法退出。简称异常完成出口。 方法执行过程中抛出异常时的异常处理，存储在一个异常处理表，方便在发生异常的时候找到处理异常的代码 总结 本质上，方法的退出就是当前栈帧出栈的过程，此时，需要回复上层方法的局部变量表，操作栈数，将返回值压入调用者栈帧的操作数栈，设置PC寄存器值等，让调用者方法能够继续执行下去。 正常完成出口和异常完成出口的区别在于：通过异常完成出口退出的不会给他的上层调用者产生任何返回值。 12.栈帧中的一些附加信息栈帧中还允许携带与java虚拟机实现相关的一些附加信息。例如：对程序调试提供支持的信息。 13.虚拟机的五道面试题1.栈溢出的情况？ 递归没有出口，方法无限循环调用 主要从两个方面考虑，申请固定内存但是内存不足，动态扩容内存不足。 2.调整栈的大小，就能保证栈不溢出嘛？ 不是，比如递归没有出口 3.分配的栈内存越大越好么？ 不是 4.垃圾回收是否涉及到虚拟机栈？ 不是，但是局部变量表的引用属于垃圾回收的根节点。 5.方法中定义的局部变量是否是线程安全的？ 不一定，只有内部生内部死的变量才是线程安全的，不是内部产生的（方法参数），内部产生又返回到外面的（返回值），是线程不安全的。 六，本地方法接口和本地方法栈 什么是本地方法？1、简单讲：一个Native Method 就是一个Java调用非Java代码的接口。一个 Native Method是这样的一个Java方法：该方法的实现由非Java语言实现，比如 C 。这个特征并非Java所特有，很多其它的编程语言都有这一机制，比如在C中，可以用 extern “C” 告知C 编译器去调用一个C 的函数。 2、“A native method is a Java method whose implementation is provided by non-java code.” 3、在定义一个native method 时，并不提供实现体(有些像定义一个Java interface )，因为其实现体是由非Java语言在外面实现的。 4、本地接口的作用是融合不同的编程语言为Java 所用，它的初衷是融合 C/C++程序。 5、标识符native可以与所有其他的Java标识符连用(除了abstract)。 为什么使用本地方法？1.java虽然使用方便，但是有一些层次的任务用java实现并不容易，或者我们对程序的效率很在意时，问题会出现。 2.目前该方法的使用越来越少了，除非是与硬件有关的应用，比如通过java程序驱动打印机或者java系统管理生产设备，在企业级应用中已经少见，因为现在的异构领域间的通信很发达，比如可以使用Socket通信，也可以使用WebService等。 本地方法栈的理解java虚拟机栈用于管理java方法的使用，而本地方法栈用于管理本地方法的调用，本地方法栈也是线程私有的，它允许被实现成固定或者可动态扩展的内存大小（在内存溢出方面是相通的）。本地方法主要是使用c语言来实现的，它的具体做法是在本地方法栈中登记本地方法，在执行引擎执行的时候加载本地方法库。 本地方法栈，本地接口和本地方法库之间的联系 当某个线程调用一个本地方法时，他就进入了一个全新的并且不再受虚拟机限制的世界，他和虚拟机拥有同样的权限，本地方法可以通过本地方法接口来访问虚拟机内部的运行时数据区。他甚至可以直接使用本地处理器中的寄存器，直接从本地内存的堆中分配任意数量的内存。 并不是所有的jvm都支持本地方法，因为Java虚拟机规范并没有明确要求本地方法栈的使用语言、具体实现方式、数据结构等。如果 JVM 产品不打算支持 native 方法，也可以无需实现本地方法栈。 在hotspot虚拟机中直接将本地方法栈和java虚拟机栈合二为一。 七，堆 1.堆的核心概念一个jvm实例只存在一个堆空间，他是java内存管理的核心区域。他在jvm启动时创建，空间大小也就确定了，是jvm管理的最大一块内存空间，堆内存的大小是可以调节的，jvm规范规定，堆可以处于物理上内存不连续的空间，但是在逻辑上他应该被视为连续的。所有线程共享java堆，在这里还可以划分线程私有的缓冲区（TLAB）。 jvm规范中对java堆的描述是：几乎所有的对象实例以及数组都应当在运行时分配在堆上。数组和对象可能永远不会存储在栈上，因为栈帧中保存引用，这个引用指向对象或者数组在堆中的位置。在方法结束后，堆中对象不会马上移除，仅仅在垃圾收集的时候才会被移除。堆是GC垃圾回收的重点区域。 2.堆的内存细分 现代垃圾收集器大部分基于分代收集理论，堆空间细分为：新生代（伊甸园区，幸存者0和幸存者1），老年代，永久代（jdk8改名叫元空间） 3.设置堆内存大小与OOM 1）堆空间的大小设置java堆用于存储java实例对象，那么堆的大小在jvm启动的时候就已经设定好了，可以通过-Xms和-Xmx来设置。 1234-Xms用于表示堆的起始内存-Xmx用于表示堆区的最大内存-X是jvm的运行参数ms是memory start 一旦堆区中的内存大小超过-Xms所指定的最大内存时，将会抛出OutOfMemoryError异常。通常将起始内存和最大内存设置相同的值，其目的是为了能够在垃圾回收机制清理完堆空间后不需要重新分隔计算堆空间的大小，从而提高性能。 默认情况下，初始内存：物理内存/64，最大内存大小物理内存/4，查看设置的参数JPS 或者 jstat -gc 进程id -XX:+PrintGCDetails 打印垃圾回收细节 123456//返回java虚拟机中的堆内存总量long initMemory=Runtime.getRuntime().totalMemory()/1024/1024;//返回java虚拟机试图使用的最大堆内存量long maxMemory=Runtime.getRuntime().maxMemory()/1024/1024;System.out.println(&quot;-Xms:&quot;+initMemory);System.out.println(&quot;-Xmx:&quot;+maxMemory); 2）OutOfMemoryError集合或者数组创建过大，导致堆空间超出内存。 3.年轻代和老年代存储在jvm中的java对象可以被划分为两类 生命周期比较短的和生命周期比较长的。 java堆区进一步细分的话，可以划分为年轻代和老年代（1:2），其中年轻代又可以划分为Eden，幸存者0，幸存者1（8:1:1）。默认自适应，其实并不是8:1:1。 12-NewRatio=2 设置新生代与老年代的比例，默认是2-XX：SurvivorRatio=8 设置新生代中Eden区与Survivor区的比例 几乎所有的对象都是在Eden区被new出来的。绝大多数java对象的销毁都是在新生代进行的，80%的对象都是朝生夕死，可以使用-Xmn设置新生代最大内存，一般使用默认。 4.对象分配过程1.new的对象先放在伊甸园区，此区有大小限制。 2.当伊甸园的空间填满时，程序有需要创建对象，JVM的垃圾回收器将对伊甸园区进行垃圾回收，将伊甸园区的不在被其他对象所引用的对象销毁，在加载新的对象放在伊甸园区。 3.然后将伊甸园区剩余的对象移动到幸存者0. 4.如果再次触发垃圾回收，此时上次幸存下来的放到幸存者0区的，接着再去幸存者1区。 5.啥时候去养老区呢？可以设置次数，默认是15次。-XX:MaxTenuringThreshold= (伊甸园区满了会触发ygc，将幸存者区域和伊甸园区都回收一下，但是幸存者区满了不会触发垃圾回收) 总结：针对幸存者S0，S1区的总结：复制之后有交换，谁空谁是to。关于垃圾回收，频繁在新生区收集，很少在养老区收集，几乎不在永久区或者元空间收集。 5.常用调优工具1.JDK命令行 2.Jconsole 3.VisualVM 4.Jprofiler 5.Java Flight Recorder 6.GCViewer 7.GC Easy 6.Minor GC,Major GC,fULL GCjvm在进行GC时，并非每次都对上面三个内存区域一起回收的，大部分回收都是指新生代。 针对hotSpotVM的实现，它里面的GC按照回收区域分为两大类型：一种是部分收集，一种是FullGC 1.部分收集：不是完整收集整个Java堆的垃圾回收，又分为： 1）新生代收集Minor GC：只是新生代的垃圾收集 2）老年代收集Major GC：只是老年代的垃圾收集 目前只有CMSGC拥有单独收集老年代的行为，很多时候Minor GC会和FullGC混淆使用，需要具体分辨老年代回收还是整堆回收。 3）混合收集：收集整个新生代以及部分老年代的垃圾 2.整堆收集：收集整个java堆和方法区的垃圾 7.分代式GC策略触发条件1）年轻代触发机制1.当年轻代空间不足时，就会出发minorGC，这里的年轻代指的是Eden满，Survivor满不会触发GC 2.因为java对象大多数存活时间比较短暂，所以MinorGC非常频繁，一般回收速度也比较快。 3.MinorGC会引发STW，暂停其他用户的线程，等垃圾回收结束，用户线程才恢复运行。 2）老年代GC触发机制1.指的是发生在老年代的GC，对象从老年代消失时，我们说的MajorGC和FullGC发生了。 2.出现了MajorGC，经常会伴随至少一次的MinorGC，也就是在老年代空间不足的时候，会先尝试触发minorGC，如果之后空间还是不足，则会触发MajorGC。 3.MajorGC的速度一般会比MinorGC慢10倍以上，STW时间更长。 4.如果MajorGC后，内存还是不足，就会OOM。 3）FullGC触发条件1.System.gc() 不是一定执行的。 2.老年代空间不足 3.方法区空间不足 4.通过Minor GC后进入老年代的平均大小大于老年代的可用内存。 5.Eden区，survivor0 区向survivor1区复制时，对象大小大于To Space可用区域，则把该对象转存到老年代，且老年代的可用内存小于该对象的内存就会触发GC。 Full GC是开发或者调优中尽量要避免的，这样暂停时间会短一些。 8.堆空间分代思想研究表明，堆空间百分之七十以上的对象是临时对象，其实不分代完全可以，分代的唯一理由就是优化GC性能，如果没有分代，所有对象都在一块，GC的时候要找到哪些对象没用，这样就会造成整堆扫描，而很多对象都是朝生夕死的，如果分代的话，把新创建的对象放到某一块区域，当GC时先把这块区域存储朝生夕死对象的区域进行回收，这样就会腾出很大空间。 9.对象分配原则针对不同年龄段对象分配原则如下 优先分配到伊甸园区，大对象直接分配到老年代，尽量避免程序中出现过多的大对象，长期存活的对象分配到老年代，动态对象年龄判断，如果幸存者区中相同年龄的所有对象大小的总和大于幸存者空间的一半，年龄大于或等于该年龄的对象可以直接进入老年代，无需等到达到16. 空间分配担保： -XX：HandlePromotionFailure 10.TLAB堆空间为每个线程分配的TLAB 堆空间是线程共享区域，任何线程都可以访问到堆区中的共享资源。由于对象实例的创建在JVM中非常频繁，因此在并发环境下从堆区中划分内存空间是线程不安全的。为了避免多个线程操作同一地址，需要使用加锁等机制，进而影响分配速度。 从内存模型而不是垃圾收集的角度，对Eden区域继续进行划分，JVM为每一个线程分配了一个线程私有的缓存区域，它包含在伊甸园区内。 多线程同时分配内存时，使用TLAB可以避免一系列的非线程安全问题，同时还能提升内存分配的吞吐量，因此我们可以将这种内存分配的方式称为快速分配策略 尽管不是所有的对象实例都能够在TLAB中成功分配内存，但是JVM确实是将TLAB作为内存分配的首选。 在程序中，可以通过-XX:UseTLAB设置是否开启TLAB空间。 默认情况下，TLAB空间的内存非常小，仅仅占整个Eden空间的百分之一，当然我们可以通过选项-XX：TLABWasteTargetPercent设置Tlab空间所占用Eden空间的百分比大小。 一旦对象在TLAB空间分配内存失败时，JVM会尝试通过加锁机制确保数据操作的原子性，从而直接在Eden空间中分配内存。 11.堆空间的参数设置1234567891011-XX:+PrintFlagsInitial 查看所有的参数的默认值-XX:+PrintFlagsFinal 查看所有的参数的最终值-Xms: 初始化堆空间大小-Xmx: 最大堆空间内存-Xmn: 设置新生代的大小-XX:NewRatio 配置新生代与老年代在堆结构的占比-XX:SurvivorRatio:设置新生代中Eden和s0/s1空间的比例-XX:MaxTenuringThreshold 设置新生代垃圾的最大年龄-XX:+PrintGCDetails 输出详细的GC处理日志打印gc简要信息：XX:+PrintGC -verbose:gc-XX:HandlePromotionFailure 是否设置空间分配担保 在发生MinorGC之前，虚拟机会检查老年代最大可用的连续空间是否大于新生代所有对象的总空间，如果大于，则此次GC是安全的。如果小于，虚拟机会查看空间分配担保的策略参数是否为true，如果为true，那么会继续检查老年代最大可用连续空间是否大于历次晋升到老年代的对象的平均大小。如果大于，则尝试进行老年代GC，但是这次GC依然是有风险的；如果小于，则改为进行FullGC。如果空间分配担保的策略参数为false，直接FullGC。 12.从逃逸分析角度分析对象内存分配1）堆是分配对象存储的唯一选择嘛？在java虚拟机中，对象在java堆中分配内存的，这是一个普遍的常识。但是有一个特殊的情况，那就是如果经过逃逸分析后发现，一个对象并没有逃逸出方法的话，那么就可能优化成栈上分配。这样就无需再堆上分配内存，也无须进行垃圾回收了。这是最常见的堆外存储技术。 2）逃逸分析1.如果将堆上的对象分配到栈，需要使用逃逸分析手段。 2.这是一种可以有效减少java程序中同步负载和内存堆分配压力的跨函数全局数据流分析算法。 3.通过逃逸分析，hotspot编译器能够分析出一个新的对象的引用的适用范围从而决定是否要将这个对象分配到堆上。 4.逃逸分析的基本行为就是分析对象动态作用域： 当一个对象在方法中被定义后，对象只在方法内部使用，则认为没有发生逃逸。 当一个对象在方法中被定义后，它被外部方法所引用，则认为发生逃逸。例如作为调用参数传递到其他地方中。 5.没有发生逃逸的对象，则可以分配到栈上，随着方法的执行结束，栈空间就被移除，栈空间指向堆空间对象的引用就没了，等到年轻代GC，对象就会被回收。 3）补充其实主要就是解决了循环引用，没有逃逸分析，这个对象可能一直不被回收，但是有了逃逸分析，栈帧销毁，这个对象就是垃圾了。 13.使用逃逸分析堆代码进行优化1）栈上分配将堆分配转换为栈分配，如果一个对象在子程序中被分配，要使指向该对象的指针永远不会逃逸，对象可能是栈分配的候选，而不是堆分配。 JIT编译器在编译期间根据逃逸分析的结果，发现如果一个对象并没有逃逸出方法的话，就可能被优化成栈上分配。分配完成后，继续在调用栈内执行，最后线程结束后，栈空间被回收，局部变量对象也被回收。这样就无需进行垃圾回收了。 常见的栈上分配场景：在逃逸分析中，已经说明了。分别是给成员变量赋值，方法返回值，实例引用传递。 1-XX:+DoEscapeAnalysis //默认开启 2）同步省略如果一个对象被发现只能从一个线程被访问到，那么这个对象的操作可以不考虑同步。线程同步的代价是相当高的，同步的后果是降低并发和性能。在动态编译同步代码块的时候，jit编译器可以借助逃逸分析来判断同步块所使用的锁的对象是否只能被一个线程访问而没有被发布到其他线程，如果没有，那么JIT编译器在编译这个同步块的时候就会取消对这部分代码的同步。这样就能大大提高并发性能，这个取消的性能就叫做同步省略，也叫锁消除。 3）分离对象或标量替换有的对象可能不需要作为一个连续的内存结构存在也可以被访问到，那么对象的部分可以不存储在内存，而是存储在CPU寄存器中 1.标量是指一个无法在分解成更小的数据的数据，Java中的原始数据类型就是标量。相对的，那些还可以分解的数据叫做聚合量，java中的对象就是聚合量，因为他可以分解成其他聚合量和标量。在JIT阶段，如果经过逃逸分析，发现一个对象不会被外界访问的话，那么经过JIT优化，就会把这个对象拆解成若干个其中包含的若干个成员变量来代替。这个过程就是标量替换。 2.标量替换的参数设置 参数-XX：+EliminateAllocations开启了标量替换，允许将对象打散分配在栈上。 3.逃逸分析技术并不成熟，其根本原因就是无法保证逃逸分析的性能消耗一定高于他的消耗。虽然经过逃逸分析可以做标量替换，栈上分配和锁消除。但是逃逸分析自身也是需要进行一系列复杂的分析的，这其实也是一个相对耗时的过程，虽然并不成熟，但是他也是即时编译器优化技术中一个十分重要的手段。 通过逃逸分析，jvm会在栈上分配那些不会逃逸的对象，这在理论上是可行的，但是取决于jvm设计者的选择。HotSpot虚拟机中并未这么做，所以可以明确所有的对象实力都是创建在堆上。 intern字符串的缓存和静态变量曾经都被分配到永久带上，而永久代已经被元数据区取代。但是，intern字符串缓存和静态变量并不是被转移到元数据区，而是直接在堆上分配 ，所以这一点同样符合前面的一点结论：对象实例都是分配在堆上。 八，方法区1.栈，堆，方法区的交互关系 1234Person person=new Person();Person这个运行时的大Class实例放在方法区person这个在java虚拟机栈的某个栈帧的局部变量表new Person();结构在堆中。 2.方法区的理解尽管所有方法区在逻辑上是属于堆的一部分，但一些简单的实现可能不会选择去进行垃圾回收或者压缩，对于hotspot而言，方法区还有一个别名叫非堆，目的就是要和堆分开。所以方法区可以看做是一块独立于java堆的内存空间。方法区与堆空间一样，是多线程共享的，方法区在jvm启动的时候被创建，并且他的实际物理内存空间中和java堆区一样都可以是物理上不连续的。方法区的大小和堆一样可以选择固定大小和扩展。方法区的大小决定了系统可以保存多少个类，如果系统定义了太多的类，导致方法区溢出，虚拟机同样会OOM。关闭jvm就会释放这个区域的内存。 加载大量第三方jar tomcat部署的工程太多 大量动态的生成反射类 关闭jvm就会释放这个区域的内存 3.方法区的演进过程jdk7以前，习惯上把方法区成为永久代，jdk8开始，使用元空间取代了永久代。 方法区等同于永久代仅仅针对hotspot而言。 永久代更容易OOM 永久代使用的是jvm的内存，元空间使用的是本地内存 两者不光名字不同，内部结构也发生了变化 根据《java虚拟机规范》的规定，如果方法去无法满足新的内存的分配需求时，将抛出OOM。 4.设置方法区大小的参数方法区的大小不必是固定的，jvm可以根据应用需要动态调整。 1.元数据区大小可以使用参数-XX:MetaspaceSize和-XX:MaxMetaspaceSize指定，替代上述原有的两个参数。 2.默认值依赖于平台，win下，元空间默认大小是21M，最大值是-1，代表没有限制。 3.与永久代不同，默认情况下，如果指定大小，虚拟机会耗尽所有的可用系统内存。如果元数据去发生溢出，虚拟机一样会OOM。 4.-XX:MetaspaceSize：设置初始元空间大小。这就是一个水平线，达到这个线就会触发FullGC，如果设置小了就会频繁GC，所以尽量设置大点，每次GC，会卸载没用的类(即对应的类加载器不在存活)，然后这个水平线将会被重置。新的水位线取决于GC释放了多少空间。如果释放空间不足，那么在不超过最大值时，他会适当调高，如果释放空间过多，则适当降低该值。 5.如何解决OOM1.要解决OOM异常或heap space的异常，一般的手段是首先通过内存映像分析工具堆dump出来的堆转储快照进行分析，重点是确认内存中的对象是否是必要的，也就是要先分清楚到底是出现了内存泄漏还是内存溢出。 2.如果是内存泄漏，可以进一步通过工具查看泄露对象到GC ROOTS的引用链，于是就能找到泄露对象到底是通过怎么样的路径与GC ROOTS相关联导致垃圾收集器无法自动回收他们的。掌握了泄露对象的类型信息，以及GC ROOTS引用链的信息，就可以比较准确的定位出泄露代码的位置。 3.如果不存在内存泄漏，换句话说就是内存中的对象却是还必须存活着，那就应当检查虚拟机堆参数，与机器物理内存对比看是否还可以调大，从代码上检查是否存在某些生命周期过长，持有状态时间过长的情况，尝试减少程序运行期的内存消耗。 6.方法区的内部结构 他用于存储已经被虚拟机加载的类型信息，常量，静态变量，即时编译器编译后的代码缓存等。 1）类型信息对每个加载的类型，jv必须在方法区存储一下类型信息。 全限定类名，直接父类的全限定类名，这个类型的修饰符，这个类型直接接口的一个有序列表 2）域信息jvm必须在方法区中保存类型的所有域的信息以及域的声明顺序。 域的相关信息包括：域名称，域类型，域修饰符。 3）方法信息jvm必须保存所有方法的以下信息，同域信息一样包括声明顺序 方法名称，返回类型，参数的数量和类型，顺序，方法的修饰符，方法的字节码，操作数栈，局部变量表以及大小，异常表 每个异常处理的开始位置，结束位置，代码处理在程序计数器中的偏移地址，被捕获的异常类的常量池索引。 12345678910111213141516171819202122232425262728/** * @author yinhuidong * @createTime 2020-08-20-12:09 * 1.静态变量和类关联在一起，随着类的加载而加载，他们成为类数据在逻辑上的一部分。 * 2.类变量被类的所有实例共享，即使没有实例时你也可以放问他。 * 3.全局常量 static final * 被声明为final的类变量的处理方法则不同，每个全局常量在编译的时候就会被分配了。 *图：final-static * javap -v Order.class * javap -v -p DemoE.class &gt; DemoE.txt */public class DemoD &#123; public static void main(String[] args) &#123; Order order=null; order.hello(); &#125;&#125;class Order&#123; public static int a=1; public static final int b=2; static &#123; a=3; &#125; public final static void hello()&#123; System.out.println(&quot;a = &quot; + a+&quot;----&quot;+&quot;b = &quot; + b); System.out.println(&quot;hello&quot;); &#125;&#125; 4）class文件中常量池的理解方法区内部包含了运行时常量池，字节码文件内部包含了常量池。 为什么需要常量池？ java中的字节码需要数据支持，通常这种数据会很大以至于不能直接存到字节码里，换另一种方式，可以存到常量池，这个字节码包含了指向常量池的引用，在动态链接的时候就会用到运行时常量池。 一个有效的字节码文件中除了包含类的版本信息，字段，方法以及接口等描述信息外，还包含一项信息那就是常量池表，包含各种字面量和对应类型，域和方法的符号引用。 总结：常量池，可以看做是一张表，虚拟机指令根据这张常量表找到要执行的类名，方法名，参数类型，字面量等类型。 5）运行时常量池方法区的一部分，class文件的常量池被类加载器加载到运行时数据区就会在方法区生成对应的运行时常量池，JVM为每一个已经加载的类或接口都维护了一个常量池。池子中的数据就像数组一样，都是通过索引访问的，运行时常量池包括多种不同的变量，包括编译期就已经明确的数值字面量，也包括到运行期解析后才能获得的方法或者字段引用。此时不再是常量池中的符号地址了，这里换位真实地址。 运行时常量池动态性，当创建类或者接口的运行时常量池时，如果构造运行时常量池所需的内存空间超过了方法区所能提供的最大值，则会OOM。 7.图解常量池操作 123456789public class DemoE &#123; public static void main(String[] args) &#123; int x=500; int y=100; int a=x/y; int b=50; System.out.println(a+b); &#125;&#125; 1）运行时常量池运行时常量池是方法区的一部分。 常量池表用于存放编译期生成的各种字面量与符号引用，这部分内容将在类加载后存放到方法区的运行时常量池。 运行时常量池相对于Class文件常量池的另外一个重要特征是具备动态性，Java语言并不要求常量一定只有编译期才能产生，也就是说，并非预置入Class文件中常量池的内容才能进入方法区运行时常量池，运行期间也可以将新的常量放入池中，这种特性被开发人员利用得比较多的便是String类的intern()方法。 8.方法区的演进 只有hotspot才有永久代。hotspot中方法区的变化： jdk1.6 有永久代，静态变量存放在永久代 jdk1.7 有永久代，但已经逐步‘去永久带’，字符串常量池，静态变量移除，保存在堆 jdk1.8 无永久代，类型信息，字段，方法，常量保存在本地内存的元空间，但是字符串常量池，静态变量仍在堆。 永久代为什么要被元空间替换？ 以前使用虚拟机内存，现在使用本地内存 ①为永久代设置空间大小很难确定 ②对永久代调优困难 方法区的垃圾收集主要分两部分 常量池中废弃的常量和不再使用的类型 9.StringTable为什么调整位置jdk7将StringTable放到了堆空间。因为永久代的垃圾回收效率低，在full gc的时候才会触发。而full gc是老年代空间不足，永久代不足的时候才会触发。这就导致了StringTable的回收效率不高。而我们在开发的时候会有大量的字符串被创建，回收效率低，导致永久代内存不足。放到堆里，能及时回收内存。 10.静态变量放在哪里12345678910111213141516171819202122/** * @author yinhuidong * @createTime 2020-08-20-15:38 * 分析： * 1.new出来的结构，也就是对象实例都在堆空间 * 2.1.6：obj1随着DemoG的类型信息放在方法区；1.7：放在堆空间 * 3.obj2是实例变量，在堆空间 * 4.obj3在foo方法对应的栈帧的局部变量表 */public class DemoG &#123; static class Test&#123; static Order obj1=new Order(); //jdk7以前放在永久代，7开始放在堆空间 Order obj2=new Order(); //实例变量 堆空间 void foo()&#123; Order obj3=new Order(); //方法内部局部变量 栈帧里面的局部变量表 &#125; &#125; static class Order&#123; &#125;&#125; 11.方法区的垃圾回收1.一般来说，方法区的回收效果不好，特别是类型的卸载，但是有时候回收又是必要的 2.方法区的垃圾回收主要是两部分：废弃的常量和不再使用的类型 3.方法区常量池主要存放：字面量和符号引用 符号引用包括：1.类和接口的全限定类名2.字段的名称和描述符3.方法的名称和描述符 4.只要常量池中的常量没有被任何地方引用，就可以被回收 5.判断一个类是否可以被回收 1.该类所属的实例都已经被回收 2.加载该类的类加载器已经被回收 3.该类对应的Class对象没有在任何地方被引用，无法在任何地方通过反射访问该类的对象 九，对象的实例化，内存布局与访问定位1.创建对象的方式1）newnew（直接new ，工厂模式，构建者模式） 2）Class.forName().newInstance()反射的方式，只能调用空参构造器，权限是public 3）Constructor.newInstance(xxx)反射的方式，可以调用空参，带参的构造器，权限没要求 4）使用clone（）不调用任何构造器，当前需要实现Cloneable接口，实现clone（）；分为深克隆和浅克隆 对象嵌套 5）使用反序列化从文件，网络中获取一个对象的二进制流 6）第三方库Objenesis2.创建对象的步骤1）首先判断对应的类是否已经加载，链接，初始化2）为对象分配内存内存规整-指针碰撞 所有用过的内存在一边，空闲的内存在另一边，中间放着一个指针作为分界点的指示器，分配内存就仅仅是吧指针想空闲那边挪动一段与对象大小相等的距离罢了，如果垃圾收集器选择的是基于压缩算法的，虚拟机采用这种分配方式。一般使用带有整理过程的收集器时，使用指针碰撞。 内存不规整-空闲列表分配 如果内存不是规整的，已使用的内存和未使用的内存相互交错，那么虚拟机将采用的是空闲列表法来为对象分配内存。虚拟机维护一个列表，记录哪块内存可用，有多大，在分配的时候找到一块内存足够大的空间划分给对象实例，并更新列表上的内容。这种分配方式称为空闲列表。选择哪种分配方式由java堆是否规整决定，而java堆是否规整又由所采用的垃圾收集器是否带有压缩整理功能决定。 3）处理并发安全问题采用cas配上失败重试保证更新的原子性，每个线程预先分配一块tlab 4）初始化分配到空间所有属性设置默认值，保证对象实例字段在不赋值时可以直接使用 给对象属性赋值的步骤：显式初始化/代码块初始化 ，构造器初始化 5）设置对象头 6）执行init方法初始化属性的显示初始化，代码块初始化，构造器初始化 3.对象访问定位JVM是如何通过栈帧中的对象引用访问到其内部的对象实例呢？ 定位，通过栈上reference访问。创建对象的目的是为了使用它。对象访问方式主要有两种： 1）句柄访问 优点：引用中存储稳定句柄地址，对象被移动时只会改变句柄中实例数据指针即可，引用本身不需要被修改。 缺点：需要额外维护一个句柄，效率低。 2）直接指针(HotSpot采用) 优点：效率高 十，直接内存不是虚拟机运行时数据区的一部分，也不是jvm规范中定义的内存区域。 直接内存是在java堆外，直接向系统申请的内存空间。 来源于NIO，通过存在堆中的DirectByteBuffer操作Native内存，通常，访问直接内存的速度会优于java堆，即读写性能高，因此出于性能考虑，读写频繁的场合可能会考虑使用直接内存，java的NIO库允许java程序使用直接内存，用于数据缓冲区。 12345678910public class DemoH &#123; private static Integer BUFFER=10*1024*1024; public static void main(String[] args) &#123; Scanner sc = new Scanner(System.in); ByteBuffer byteBuffer=ByteBuffer.allocate(BUFFER); sc.next(); byteBuffer=null; System.gc(); &#125;&#125; 也可能导致OOM异常，由于直接内存在java堆外，因此它的大小不会受限于-Xmx指定的最大堆的大小，但是系统内存是有限的，java堆和直接内存的总和依然受限于操作系统能给出的最大内存。 缺点：分配回收成本高，不受JVM内存回收管理，直接内存大小可以通过MaxDirectMemorySize设置，如果不指定，默认与堆的最大值-Xmx参数值一致，简单理解java进程内存=java堆+本地内存。 十一，执行引擎1.虚拟机是一个相对于物理机的概念，这两种机器都有代码执行能力，其区别是物理机的执行引擎是直接建立在处理器，缓存，指令集和操作系统层面的，而虚拟机的执行引擎是由软件自行实现的，因此可以不受物理条件制约的定制指令集与执行引擎的结构体系，能够执行那些不被硬件直接支持的指令集格式。 2.jvm的主要作用负责装在字节码到其内部，但是字节码不能直接运行在操作系统之上，执行引擎就是将字节码指令编译为对应平台上的本地机器指令。 3.外观上看：jvm的执行引擎输入输出都是一致的，输入的是字节码二进制流，处理过程是字节码解析执行的过程，输出的是执行结果。 1，java代码的编译和执行过程 大部分的程序代码转换为物理机的目标代码或虚拟机能执行的指令集之前，都需要经过上图的步骤。 为什么java称为半解释型半编译型语言？ 因为jvm的执行引擎是解释器和jit即时编译器交互工作的。 什么是解释器？什么是JIT编译器？ 解释器：将字节码指令逐行翻译成机器指令并执行 编译器：将源代码直接编译成对应的机器指令。 2，机器码，指令，汇编语言 1.机器码：二进制编码方式表示的机器指令 2.指令，指令集：将机器中特定的0和1简化成对应的指令。每个平台所支持的指令就叫做指令集 3.汇编语言：用助记符代替机器指令的操作码，所以汇编编写的程序还需要翻译成机器指令码 4.高级语言：需要把程序解释和编译成机器的指令码 5.字节码：一种中间状态的二进制文件，实现特定软件运行和软件环境，与硬件环境无关 3，解释器 解释器其实就是一个运行时翻译者，将字节码文件中的内容翻译为对应平台的本地机器指令执行。当一条字节码指令被解释执行完成后，接着再根据PC寄存器中记录的下一条需要被执行的字节码指令执行解释操作。 解释器分类 1.字节码解释器：纯软件代码模拟字节码的执行 2.模板解释器：将每一条字节码指令和一个模板函数相关联，模板函数中直接产生这条字节码执行时的机器码 基于解释器执行已经沦落为低效的代名词，JIT编译的目的是为了避免函数被解释执行，而是将整个函数体编译成为机器码，每次函数执行，只执行编译后的机器码即可。 4，JIT即时编译器HotSpot采用解释器与即时编译器共存的架构。由JVM决定何时使用哪种方式执行。 解释器可以边解释边执行，这样程序启动时间就会变快，及时编译器是都编译好了在执行，程序启动时间慢。但是一旦jit编译器把越来越多的代码编译成本地代码，执行效率立马起飞。 1）热点代码以及探测方式判断是否启动jit编译器将字节码直接编译为对应平台的本地机器指令，需要根据执行的频率而定。热点代码就是需要被编译为本地代码的字节码，jit在运行时会针对频繁调用的热点代码直接编译为对应平台的本地机器指令，提升性能。 2）OSR编译一个方法被多次调用，或者一个方法体内部循环次数较多的循环体都可以被称为热点代码，因此都可以通过jit编译器编译为本地机器指令，也就是栈上替换.hotspot采用的热点探测方式是基于计数器的热点探测。 为每个方法建立2个不同类型的计数器，分别为方法调用计数器和回边计数器 1.方法调用计数器：统计方法调用次数，默认client模式下1500，server模式下100002.回边计数器：统计循环执行次数 当一个方法被调用时，先判断有没有jit编译过，有的话直接使用jit编译后的机器指令，没有的话计数器+1，然后判断两个计数器之和是否超过方法调用计数器的阈值。如果超过，就会向jit发出即时编译申请。 热度衰减是在虚拟机进行垃圾回收的时候顺便进行的，也就是在一段时间方法一直没有执行，计数器的计数就会减半。 可以自己手动设置虚拟机采用哪种编译模式 3）JDK9引入了AOT编译器aot编译器在程序执行之前，就将字节码转换为机器码过程。 好处：可以直接运行，不必预热。 坏处：由于提前编译成了机器指令，无法实现java一次编译到处运行。 4）JDK10的Graal编译器全新的即时编译器，实验阶段，需要手动开启，前景大好 十二，StringTable1，String的基本特征String字符串使用一对引号引起来表示 1234String s1=“yhd”;String s2=new String(“yhd”);jdk8:private final char value[];jdk9:private final byte value[]; String声明为final，不可被继承。 String类实现了序列化接口，可以跨jvm进行传输，实现了Conparable接口，支持比较大小。 String代表不可变的字符序列，简称不可变性。 1）当对字符串重新赋值，需要重新指定内存区域赋值，不能使用原有的value值。 2）当对现有的字符串进行连接操作时，也需要重新指定内存区域赋值，不能使用原有value赋值。 3）当调用String的replace方法修改指定的字符或者字符串时，也需要重新指定内存区域赋值。 一方面数组的长度一旦确定了，就不能再改变，一方面存储在字符串常量池的数据不可发生变化。 通过字面量的方式给一个字符串赋值，此时的字符串值声明在字符串常量池。 字符串常量池不会存储两个相同内容的字符串。 String的底层是一个固定大小的hashtable，默认长度是1009，如果放入StringPool的String特别多，就会造成Hash冲突严重，从而导致链表会很长，而链表长了会影响String.intern的性能。 使用-XX:StringTableSize设置StringTable的长度。 jdk6 StringTable长度固定为1009，字符串多就会造成性能下降。 jdk7 StringTable的长度默认值是60013,jdk8开始1009是可设置的最小值。 2，String内存的分配常量池就类似一个Java系统级别提供的缓存。8种基本数据类型的常量池都是系统协调的，String类型的常量池比较特殊。它的主要使用方法有两种： 1）直接使用双引号声明出来的String对象会直接存储在常量池中 2）如果不是用双引号声明的String对象，可以使用String提供的intern()方法。 所有的字符串都保存在堆中，这样调优的时候仅仅需要调整堆的大小就可以了。 StringTable为什么要调整？ 1）permSize默认比较小 2）永久代垃圾回收频率低 3，String的基本操作java语言规范里要求完全相同的字符串字面量，应该包含同样的Unicode字符串序列，并且必须是指向同一个String类实例。 4，字符串拼接1）常量与常量的拼接结果在常量池，原理是编译期优化 2）.常量池中不会存在相同内容的常量 3）只要其中有一个是变量，结果就在堆中。变量拼接的原理是StringBuilder 4）如果拼接的结果调用intern()方法，则主动将常量池中还没有的字符串对象放入池中，并返回此对象的地址。 5）字符串拼接底层原理分析 1234567891011121314public static void main(String[] args) &#123; String s1=&quot;a&quot;; String s2=&quot;b&quot;; String s3=&quot;ab&quot;; String s4=s1+s2; System.out.println(s3==s4); /** * 如下的s1+s2的执行细节： * ① StringBuilder s= new StringBuilder(); * ② s.append(&quot;a&quot;); * ③ s.append(&quot;b&quot;); * ④ s.toString(); //类似于new String(&quot;ab&quot;); */ &#125; 12345678910111213141516public class DemoA &#123; /** * 1.字符串拼接操作不一定使用的是StringBuilder! * 如果拼接符号左右两边都是字符串常量或常量引用。则仍然使用编译器优化，即非 * StringBuilder的方式。 * 2.针对于final修饰类，方法，基本数据类型，引用数据类型的结构时，能使用上final * 的时候建议使用上。 * @param args */ public static void main(String[] args) &#123; final String s1=&quot;a&quot;; final String s2=&quot;b&quot;; String s3=&quot;ab&quot;; String s4=s1+s2; System.out.println(s3==s4);//true &#125; 12345拼接操作和append操作的效率对比StringBuilder s= new StringBuilder();//每次直接编译器优化s.append(&quot;a&quot;);src+=&quot;a&quot;;//每次都会创建一个新的StringBuilder 而且还会new String()总结：实际开发，尽量少用空参的list和StringBuilder，防止不断扩容。 5，intern（）的使用intern()方法就是确保字符串在内存中只有一份，这样可以节约内存空间，加快字符串操作任务的执行速度，这个值会被存放在字符串内部池。 如何保证变量s指向的是字符串常量池的数据呢？ 12345方式1：String s=“shkstart”;//字面量定义的方式方式2：String s=new String(“shkstart”).intern();String s=new StringBuilder(“shkstart”).toString().intern(); 1String a=new String(“a”);//底层创建了两个对象，一个在堆空间，一个在字符串常量池 1234567891011121314new String(“a”)+new String(“b”);//创建了几个对象/* 对象1：new StringBuilder(); 对象2：new String(“a”); 对象3：常量池中的a 对象4：new String(“b”); 对象5：常量池中的b 对象6：new Sring(“ab”);*/ toString()的调用，在字符串常量池中，没有生成”ab”。 1234567891011121314151617181920212223void contextLoads() &#123; String str=new String(&quot;1&quot;); //str指向堆空间的引用地址 String intern = str.intern(); //intern 指向字符串常量池的引用地址 String str2=&quot;1&quot;; //指向常量池的地址 System.out.println(str==str2);//false System.out.println(intern==str2);//true String str3=new String(&quot;1&quot;)+new String(&quot;1&quot;); //str3指向堆空间 //new StringBuilder().append().append().toString(); String intern1 = str3.intern();//intern1指向字符串常量池 String str4=&quot;11&quot;; // 常量池 System.out.println(str3==str4);//false System.out.println(intern1==str4);//true String str5=new String(&quot;a&quot;)+new String(&quot;a&quot;);//str5指向堆空间 String str6=&quot;aa&quot;; //str6常量池 String intern2 = str5.intern(); //intern2指向常量池的副本地址 System.out.println(str5==str6);//false System.out.println(str6==intern2); //true &#125; 总结String的intern()的使用： jdk1.6中，将这个字符串对象尝试放入字符串常量池。 1）如果字符串常量池有，则并不会放入。返回已经有的字符串常量池中的对象的地址。 2）如果没有，会把此对象复制一份，放入串池，并返回串池中的对象地址。 jdk1.7起，将这个字符串对象尝试放入字符串常量池。 1）如果字符串常量池有，则并不会放入。返回已经有的字符串常量池中的对象的地址。 2）如果没有，则会把对象的引用地址复制一份，放入串池，并返回字符串常量池中的引用地址，所以相当于指向的是对象的引用。 练习 123456789@Testpublic void test1()&#123; String s=new String(&quot;a&quot;)+new String(&quot;b&quot;);//堆 //执行完上一行代码，字符串常量池中并没有 ab String s2=s.intern();//6：字符串常量池 8：没有创建字符串，而是创建一个引用指向堆中的ab //ab 常量池 System.out.println(s2==&quot;ab&quot;); //8：true 6：true System.out.println(s==&quot;ab&quot;); //8：true 6：false&#125; 12345678910@Testpublic void test1()&#123; String x=&quot;ab&quot;; //常量池 String s=new String(&quot;a&quot;)+new String(&quot;b&quot;);//堆 String s2=s.intern();//6：字符串常量池 8：指向常量池ab的引用 //ab 常量池 System.out.println(s2==&quot;ab&quot;); //8：true 6：true System.out.println(s==&quot;ab&quot;); //8：false 6：false&#125; 12345678@Testpublic void test2()&#123; String s1=new String(&quot;ab&quot;); //会在字符串常量池生成 ab，但是str指向的是堆 String s=new String(&quot;a&quot;)+new String(&quot;b&quot;); //不会再字符串常量池生成 ab s1.intern(); String s2=&quot;ab&quot;; //常量池 System.out.println(s1==s2); //false&#125; 开发中推荐使用intern（），节省内存空间。 6，StringTable的垃圾回收1234//-Xms15m -Xmx15m -XX:+PrintStringTableStatistics -XX:+PrintGCDetailspublic static void main(String[] args) &#123; IntStream.range(0, 100).forEachOrdered(i -&gt; String.valueOf(i).intern());&#125; 7，G1中的String去重操作12String s1=new String(&quot;a&quot;);String s2=new String(&quot;a&quot;); 此时s1和s2在堆空间new了两个一模一样的对象，两个对象在同时指向字符串常量池的同一个字符串a，此时G1就会删除一个堆空间的对象，让s1和s2都指向同一个堆空间的对象。 实现 1）当垃圾收集器工作的时候，会访问堆上存活的对象。对每一个访问的对象都会检查是否是候选的要去重的String对象。 2）如果是，把这个对象的一个引用插入到队列中等待后续的处理。一个去重的线程在后台运行，处理这个队列。处理队列的一个元素意味着从队列删除这个元素，然后尝试去重它引用的String对象。 3）使用一个hashtable来记录所有的被String对象使用的不重复的char数组。当去重的时候，会检查这个hashtable，来看堆上是否已经存在一个一模一样的char数组。 4）如果存在，String对象会被调整引用那个数组，释放对原来的数组的引用，最终会被垃圾收集器回收掉。 5）如果查找失败，char数组会被插入到hashtable，这样以后的时候就可以共享这个数组了。 命令行选项 123UseStringDeduplication //开启String去重，默认不开启PrintStringDeduplicationStatistics //打印详细的去重统计信息StringDeduplicationAgeThreshold //打到这个年龄的String对象被认为是去重的候选对象 十三，垃圾判断算法1．什么是垃圾？在ｊｖｍ进行垃圾回收之前，会先判断哪些对象是垃圾，也就是说，要判断哪些对象可以被销毁了，其占有的空间是可以被回收的。根据ｊｖｍ的架构划分，ｊａｖａ中几乎所有的对象实例都在堆空间中存放，所以垃圾回收也主要是针对堆空间进行垃圾回收。 在ｊｖｍ眼中，垃圾就是指那些在堆空间中存在的，已经死亡的对象，而对于死亡的定义，我们可以简单的将他理解为不可能再被任何途径使用的对象。那怎么才能确定一个对象是存活还是死亡呢？这就涉及到了垃圾判断算法，其主要包括引用计数法和可达性分析算法。 2，为什么需要GC？对于高级语言来讲，如果不进行GC，内存迟早会消耗殆尽，除了释放没用的对象，垃圾回收也可以清理内存里的记录碎片。碎片整理将所占用的堆内存移动到堆的一端，以便JVM将整理出的内存分配给新的对象，没有GC不能保证程序的正常运行。 3，早期的垃圾回收new：申请内存 delete：释放内存 优点：灵活控制内存释放的时间 缺点：频繁申请和释放内存的管理负担，一旦忘记释放，可能会引发内存泄漏从而导致内存溢出，使程序崩溃。 4，Java垃圾回收机制1）自动内存管理，降低内存泄漏和内存溢出的风险 2）使程序员更专注与业务代码的开发 5．垃圾判断算法１）引用计数法在这种算法中，假设堆中每个对象都有一个引用计数器。当一个对象被创建并且初始化赋值以后，对象的计数器就会设置为１，每当有一个地方引用他，计数器的值就会＋１，例如将对象Ｂ赋值给对象Ａ，那么Ｂ被引用，Ｂ的引用计数器就会＋１. 反之，当引用失效的时候，比如一个对象的某个引用被设置了新的值，则之前被引用的对象的计数器就会－１.而那些引用计数为０的对象，就可以称之为垃圾，可以被收集。 特别的，当一个对象被当做垃圾收集时，他引用的任何对象的计数器的值都－１. 优点：实现简单，对程序不被长时间打断的实时环境比较有利 缺点：需要额外的空间来存储计数器，难以检测对象之间的循环依赖 java并没有选择引用计数，是因为其存在一个基本难题，也就是很难处理循环引用关系。 Python如何解决循环引用？手动解决（在合适的时机，接触引用关系）使用弱引用weakref（weakref是Python提供的标准库，为了解决循环依赖） ２）可达性分析算法可达性是指，如果一个对象会被至少一个在程序中的变量通过直接或间接的方式被其他可达的对象引用，则称该对象就是可达的。 对象成为可达对象的两个条件： 对象属于跟集中的对象 对象被一个可达的对象引用 在java语言中，GC ROOTS 包括以下几类元素： 虚拟机栈中引用的对象（各个线程被调用的方法中使用到的参数，局部变量等） 本地方法栈内JNI（通常说的本地方法）引用的对象 方法区中类静态属性引用的对象（java类的引用类型静态变量） 方法区中常量引用的对象（字符串常量池（String Table）里的引用） 所有被同步锁synchronized持有的对象 java虚拟机内部的引用（基本数据类型对应的class对象，一些常驻的异常对象，系统类加载器） 本地代码缓存 临时的（分代收集，局部回收） 如何判断一个root 由于root采用栈方式存放变量和指针，所以如果一个指针，他保存了堆内存里面的对象，但是自己又不存放在堆内存里面，那他就是一个root。 如果要使用可达性分析算法来判断内存是否可回收，那么分析工作必须在一个能保障一致性的快照中进行。这点不满足的话分析结果的准确性就无法保证。这点也是导致GC进行时必须Stop the World的一个重要原因。 优点：可以解决循环引用的问题，不需要占用额外的空间 缺点：多线程场景下，其他线程可能会更新已经访问过的对象的引用。 6.对象的finalization机制java语言提供了对象终止机制来允许开发人员提供对象被销毁之前的自定义处理逻辑。当垃圾回收器发现没有引用指向一个对象，即：垃圾回收此对象之前，总会先调用这个对象的finalize方法。finalize方法允许在子类中被重写，用于在对象被回收时进行资源释放。通常在这个方法中进行一些资源释放和清理的工作。 应该交给垃圾回收机制调用（永远不要主动调用某个对象的finalize方法） 1.在finalize时可能会导致对象复活 2.finalize（）方法的执行时间是没有保障的，他完全由GC线程决定，极端情况下，若不发生GC，则finalize（）将没有执行的机会。 3.一个糟糕的finalize（）会严重影响GC的性能。 从功能上来说，finalize（）与c中的析构函数比较相似，但是java采用的是基于垃圾回收期的自动内存管理机制，所以finalize（）方法在本质上不同于c中西沟函数。 由于finalize（）方法的存在，虚拟机中的对象一般处于三种可能的状态。 可触及的：从根节点开始，可以到达这个对象。 可复活的：对象的所有引用都被释放，但是对象有可能在finalize（）中复活。 不可触及的：对象的finalize（）方法被调用，并且没有复活，那么就会进入不可触及状态。不可触及的对象不可能被复活，因为finalize（）只会被调用一次。 判定一个对象是否可回收，至少需要经历两次标记过程 1）如果该对象到GC Roots没有引用链，则进行第一次标记。 2）进行筛选，判断该对象是否有必要执行finalize() ①如果对象没有重写该方法，或者该方法已经被虚拟机调用过，则虚拟机认为没有必要执行，该对象为不可触及 ②如果该对象重写了该方法，且没有执行过，那么对象会被插入到一个队列中，由一个虚拟机自动创建的，低优先级的Finalizer线程触发finalize（）执行。 ③finalize()是对象逃脱死亡的最后机会，GC线程会对队列中的对象进行第二次标记，如果对象在finalize()中与引用链上的任何一个对象建立了联系，那么在第二次标记时，对象会被移出即将回收集合。之后，对象会再次出现没有引用存在的情况，在这个情况下，finalize()不会被再次调用，对象会直接变成不可触及的状态，也就是说，一个对象的finalize()只会被调用一次。 7，使用MAT查看GC RootMAT是一款功能强大的java堆内存分析器，用于查找内存泄漏以及内存消耗情况。 1）获取dump文件①命令行使用jmap12345678910C:\\Users\\ASUS\\Ideaproject\\project03\\gc&gt;jps16912 Launcher5056 RemoteMavenServer3613636 Jps14876 GetDump15132C:\\Users\\ASUS\\Ideaproject\\project03\\gc&gt;jmap -dump:format=b,live,file=test1.bin 14876Dumping heap to C:\\Users\\ASUS\\Ideaproject\\project03\\gc\\test1.bin ...Heap dump file created ②JvisualVM 2）使用MAT分析dump文件①查看GC Roots 8，使用Jprofiler进行GCRoots淑源9，使用Jprofiler分析OOM1-XX:+HeapDumpOnOutOfMemoryError //发生OOM时生成dump文件日志 十四，垃圾回收算法当成功区分出内存中存活对象和死亡对象后，GC接下来的任务是执行垃圾回收，释放掉无用对象所占用的内存空间，以便有足够的可用内存空间为新对象分配内存。目前在JVM中比较常见的三种垃圾收集算法是标记-清除算法，复制算法，标记-压缩算法。 １．标记清除算法当堆中的有效内存空间被耗尽时，就会停止整个程序，然后进行两项工作，第一项是标记，第二项则是清除。 标记：从引用跟节点开始遍历，标记所有被引用的对象。一般是在对象的Header中记录为可达对象。 清除：对堆内存从头到尾进行线性遍历，如果发现某个对象在其header中没有标记为可达对象，则将其回收。 优点：不需要进行对象的移动，并且仅对不存活的对象进行处理，存活对象比较多的情况下极为高效。 缺点：标记和清除过程的效率都不高，这种方法需要使用一个空闲列表来记录所有的空闲区域以及大小，对空闲列表的管理会增加分配对象时的工作量；标记清除后会产生大量不连续的内存碎片，虽然空闲区域的大小是足够的，但却可能没有一个单一的区域能满足这次分配所需大小，分配还会失败，不得不触发再一次的垃圾回收。 何为清除：所谓清除，并不是并不是真的置空，而是把需要清除的对象地址保存在空闲的地址列表里，下次有新的对象需要加载时，判断垃圾的位置空间是够够，够就存放。 ２．标记整理算法算法标记的过程与标记清除算法中的标记过程一样，但是对标记后出的垃圾对象的处理情况有所不同，他不是直接对可回收对象进行清理，而是让所有的对象都像一端移动，然后直接清理掉端边界以外的内存。在基于标记整理算法的收集齐实现中，一般增加句柄和句柄表。 优点：经过整理之后，新对象的分配只需要通过指针碰撞便能完成，比较简单；使用这种方法，空闲区域的位置是始终可知的，也不会再有碎片的问题了。 缺点：ＧＣ暂停的时间会增长，因为你需要将所有的对象都拷贝到一个新的地方，还得更新他们的引用地址。 ３．复制算法复制算法主要是为了克服句柄的开销和解决堆碎片的垃圾回收。它将内存按照容量分为大小相等的两块，每次只使用其中的一块，当这一块的内存用完了，就将还活着的对象复制到另一块内存上面（空闲面），然后再把已使用过的内存空间一次清理掉。 复制算法比较适合于新生代（短生存期的对象），在老年代（长生存期的对象）中，对象存活率比较高，如果执行较多的复制操作，效率将会变低。所以老年代一般会选用其他算法，如标记整理算法。一种典型的基于复制算法的垃圾回收是stop-and-copy算法，他将堆分为对象区和空闲区，在对象区与空闲区的切换过程中，程序暂停执行。 优点：标记阶段和复制阶段可以同时进行，每次只对一块内存进行回收，运行高效，只需要移动栈顶指针，按顺序分配内存即可，实现简单，内存回收时，不用考虑内存碎片的出现。 缺点：需要一块能容纳下所有存活对象的额外的内存空间。因此，可一次性分配的最大内存缩小了一半。 4.分代收集算法将堆内存划分为新生代，老年代和永久代。新生代又被进一步划分为伊甸园区和幸存者0，幸存者1区。所有通过new创建的对象的内存都在堆中分配，其大小可以通过-Xms和-Xmx来控制。分代收集，是基于这样一个事实：不同的对象的生命周期是不一样的。因此可以将不同生命周期的对象分代，不同的代采取不同的回收算法进行垃圾回收，以便提高回收效率。 新生代：几乎所有新生成的对象首先都是放在年轻代的。新生代内存按照 8:1:1 的比例分为一个 Eden 区和两个 Survivor（Survivor0，Survivor1）区。大部分对象在 Eden 区中生成。当新对象生成，Eden 空间申请失败（因为空间不足等），则会发起一次 GC（Scavenge GC）。回收时先将 Eden 区存活对象复制到一个 Survivor0 区，然后清空 Eden 区，当这个 Survivor0 区也存放满了时，则将 Eden 区和 Survivor0 区存活对象复制到另一个 Survivor1 区，然后清空 Eden 和这个 Survivor0 区，此时 Survivor0 区是空的，然后将 Survivor0 区和 Survivor1 区交换，即保持 Survivor1 区为空， 如此往复。当 Survivor1 区不足以存放 Eden 和 Survivor0 的存活对象时，就将存活对象直接存放到老年代。当对象在 Survivor 区躲过一次 GC 的话，其对象年龄便会加 1，默认情况下，如果对象年龄达到 15 岁，就会移动到老年代中。若是老年代也满了就会触发一次 Full GC，也就是新生代、老年代都进行回收。新生代大小可以由-Xmn来控制，也可以用-XX:SurvivorRatio来控制 Eden 和 Survivor 的比例。 老年代：在新生代中经历了 N 次垃圾回收后仍然存活的对象，就会被放到年老代中。因此，可以认为年老代中存放的都是一些生命周期较长的对象。内存比新生代也大很多（大概比例是 1:2），当老年代内存满时触发 Major GC 即 Full GC，Full GC 发生频率比较低，老年代对象存活时间比较长，存活率高。一般来说，大对象会被直接分配到老年代。所谓的大对象是指需要大量连续存储空间的对象，最常见的一种大对象就是大数组。当然分配的规则并不是百分之百固定的，这要取决于当前使用的是哪种垃圾收集器组合和 JVM 的相关参数。 永久代：用于存放静态文件（class类、方法）和常量等。永久代对垃圾回收没有显著影响，但是有些应用可能动态生成或者调用一些class，例如 Hibernate 等，在这种时候需要设置一个比较大的持久代空间来存放这些运行过程中新增的类。对永久代的回收主要回收两部分内容：废弃常量和无用的类。永久代在 Java SE8 特性中已经被移除了，取而代之的是元空间（MetaSpace），因此也不会再出现java.lang.OutOfMemoryError: PermGen error的错误了。 特别的，在分代收集算法中，对象的存储具有以下特点： 1.对象优先在伊甸园区分配 2.大对象直接进入老年代 3.长期存活的对象将进入老年代，默认为15岁 对于晋升老年代的年龄阈值，为什么是15岁？ 实际上，HotSpot虚拟机的对象头其中一部分用于存储对象自身的运行时数据，如哈希码，GC分代年龄，锁状态标志，线程持有的锁，偏向线程ID，偏向时间戳等，这部分数据的长度在32位和64位的虚拟机中分别为32bit和64bit，官方称为mark word。在 32 位的 HotSpot 虚拟机中，如果对象处于未被锁定的状态下，那么Mark Word的 32bit 空间中 25bit 用于存储对象哈希码，4bit 用于存储对象分代年龄，2bit 用于存储锁标志位，1bit 固定为 0，其中对象的分代年龄占 4 位，也就是从0000到1111，而其值最大为 15，所以分代年龄也就不可能超过 15 这个数值了。 GC的分类 新生代GC Minor GC ：发生在新生代的垃圾收集动作，因为java对象大多具有朝生夕灭的特性，因此MinorGC非常频繁，一般回收速度也比较快。在新生代中，每次垃圾收集时都会发现有大量对象死去，只有少量存活，因此可以选用复制算法。 老年代GC Major GC：发生在老年代的垃圾回收动作。Major GC 经常会伴随至少一次Minor GC。由于老年代中的对象的生命周期比较长，因此Major GC并不频繁，一般都是等待老年代满了之后才进行Full GC，而且其速度一般会比Minor GC慢10倍以上。另外，如果分配了Direct Memory，在老年代中进行 Full GC 时，会顺便清理掉 Direct Memory 中的废弃对象。而老年代中因为对象存活率高、没有额外空间对它进行分配担保，就必须使用“标记-清除”算法或“标记-整理”算法来进行回收。新生代采用空闲指针的方式来控制 GC 触发，指针保持最后一个分配的对象在新生代区间的位置，当有新的对象要分配内存时，用于检查空间是否足够，不够就触发 GC。当连续分配对象时，对象会逐渐从 Eden 到 Survivor，最后到老年代。 5.增量收集算法上述现有的算法，在垃圾回收过程中，应用软件将处于一种STW的状态，在该状态下，应用程序所有的线程都会挂起，暂停一切正常的工作，等待垃圾回收完成。如果垃圾回收时间过长，应用程序会被挂起很久，将严重影响用户体验或者系统稳定性。为了解决这个问题，即对实时垃圾收集算法的亚久直接导致增量收集算法的产生。 如果一次性将所有的垃圾进行处理，需要造成系统长时间的停顿，那么就可以让垃圾收集线程 和应用程序线程交替执行。每次，垃圾收集线程值收集一小片区域的内存空间，接着切换到应用程序线程，依次反复，一直到垃圾收集完成。 总的来说，增量收集算法的基础仍然是传统的标记-清除和复制算法。增量收集算法通过对线程间冲突的妥善处理，允许垃圾收集线程以分阶段的方式完成标记，清理或复制工作。 缺点：线程切换和上下文的转换消耗性能，会使得垃圾回收的总体成本上升，造成系统吞吐量的下降。 6.分区算法分代算法将按照对象的生命周期长短划分成两个部分，分区算法将整个堆空间划分成连续的不同的小空间。每一个小空间都独立使用，独立回收。这种算法的好处是可以控制一次回收多少个小区间。 十五，垃圾回收前置知识1.System.gc()的理解在默认情况下，通过system.gc()或者Runtime.getRuntime().gc()的调用，会显式触发Full GC，同时对老年代和新生代进行垃圾回收，尝试释放被丢弃对象占用的内存。然而，System.gc()无法保证一定对垃圾收集器的调用。 12345678910public class SystemGCTest &#123; public static void main(String[] args) &#123; new SystemGCTest(); //提醒jvm的垃圾回收期进行垃圾收集，但是不一定马上进行垃圾回收 // 与Runtime.getRuntime().gc();的作用是一样的。 System.gc(); //调用finalize()方法 System.runFinalization(); &#125;&#125; 2.内存溢出与内存泄漏1）内存溢出内存溢出是相对于内存泄漏来说的，尽管更容易被理解，但是同样的，内存溢出也是引发程序崩溃的罪魁祸首之一。大多数情况下，GC会进行各种年龄段的垃圾回收实在不行了就放大招，来一次Full GC操作，这时候会回收大量内存，供应用程序继续使用。javadoc对OOM的解释是：没有空闲内存，并且垃圾收集器也无法提供更多内存。 堆内存不够的原因有两个：java虚拟机的堆内存设置不够。代码中创建了大量的大对象，并且长时间不能被垃圾收集器收集（存在被引用） OOM之前，通常垃圾收集器会先进行GC，当然也不是任何情况下垃圾收集器都会被触发：比如我们创建一个超过堆空间大小的对象。 2）内存泄漏只有对象不在被程序用到了，但是GC又不能回收他们的情况，才叫内存泄漏。实际上一些导致对象生命周期变得很长甚至OOM的操作，也称为内存泄漏。 尽管内存泄漏并不会立刻引起程序崩溃，但是一旦发生内存泄漏，程序中的可用内存就会被逐步蚕食，直至耗尽所有内存，最终出现OOM，导致程序崩溃。 这里的存储空间并不是指物理内存，而是指虚拟内存大小，这个虚拟内存取决于磁盘交换区设定的大小。 Example： 1.单例模式，单例的生命周期默认和应用程序一样长，所以单例程序中，如果持有对外部对象的引用的话，那么这个外部对象是不能被回收的，否则会导致内存泄漏的产生。 2.一些提供close的资源未关闭导致内存泄漏（数据库连接，网络连接，io连接） 3.Stop The World指的是GC事件发生过程中，会产生应用程序的停顿。停顿产生时整个应用程序线程都会被停掉，没有任何响应。 STW事件和采用哪款垃圾收集器无关，所有GC都有这个事件。它是由JVM在后台自动发起和自动完成的。 4.垃圾回收的并行与并发1）并行当系统有一个以上cpu，当一个cpu执行一个进程时，另一个cpu可以执行另一个进程，两个进程互不抢占cpu资源，可以同时进行，我们称之为并行。 其实决定并行因素不是cpu数量，而是cpu的核心数，比如一个cpu多个核也可以并行 2）并发一个cpu核心在一个时间段同时处理多个任务，某一个时间点只处理一个任务。 两者对比 并发指的是多个事情在同一时间段同时发生了。 并行指的是多个事情在同一时间点上同时发生了。 并发的多个任务之间是互相抢占资源的。 并行的多个任务之间是不互相抢占资源的。 只有在多cpu或者一个cpu多核的情况下，才会发生并行。否则，看似同时发生的事情，其实都是并发执行的。 垃圾回收的并发与并行 并行：指多条垃圾收集线程并行工作，但此时用户线程仍处于等待状态 串行：相较于并行的概念，单线程执行。如果内存不够，则程序暂停，启动垃圾回收器进行垃圾收集。回收完，在启动程序的线程。 并发：指用户线程和垃圾回收线程同时执行。垃圾回收线程在执行时不会停顿用户程序的运行。 5.安全点与安全区域1）安全点程序执行过程中并不是在所有地方都能停顿下来开始GC，只有在特定的位置才能停顿下来开始GC，这些位置称为安全点。 安全点太少可能导致GC等待时间太长，太多可能导致程序运行时的性能问题。 方法调用，循环跳转，异常跳转。 如何在GC发生时，检查所有线程都跑到最近的安全点停下来呢？ 抢先式中断（目前没有虚拟机采用了）中断所有线程，哪个没到安全点就恢复线程，让线程跑到安全点。 主动式中断设置一个中断标志，各个线程运行到安全点的时候主动轮询这个标志，如果中断标志为真，则将自己进行中断挂起。 2）安全区域安全点机制保证了程序执行时，在不太长的时间内就会遇到可进入的GC安全点，但是假如程序处于sleep状态，这时候线程无法响应jvm的中断请求，走到安全点去中断挂起，jvm也不太可能的等待线程被唤醒。对于这种情况，就需要安全区域来解决。 安全区域是指在一段代码片段中，对象的引用关系不会发生变化，在这个区域中的任何位置开始GC都是安全的 实际执行时 当线程运行到安全区域的代码时，首先标识已经进入了安全区域，如果这段时间内发生GC，JVM会忽略标识为安全区域状态的线程。 当线程即将离开安全区域时，会检查jvm是否已经完成GC，如果完成了，则继续运行，否则线程必须等待直到收到可以安全离开安全区域的信号为止。 6.引用强引用：Object obj=new Object(); 只要强引用还存在，垃圾回收器就永远不会回收掉引用的对象。 软引用：可能还有用，但并非必须的对象，系统内存不足时，这类引用关联的对象将被回收。 弱引用：被弱引用关联的对象只能存活到下一次垃圾回收。 虚引用：就是为了在这个对象被垃圾回收的时候能够获得一个系统通知。 终结器引用：用来实现对象的finalize（）无需手动编码，内部配合引用队列使用。 在GC时，终结器引用入队，由Finalizer线程通过终结器引用找到被引用对象并调用他的finalize（），第二次GC时才能回收被引用对象。 十六，垃圾回收器1.垃圾回收分类按照垃圾回收线程分：串行垃圾回收器和并行垃圾回收器 串行回收是指在同一时间段内只允许有一个cpu执行垃圾回收操作，此时工作线程被暂停，直至垃圾回收结束。 并行则是允许运用多个cpu同时执行垃圾回收操作。 按照工作模式分：并发式垃圾回收器和独占式垃圾回收器 并发式垃圾回收器与应用程序线程交替，尽可能减少应用程序暂停时间。 独占式垃圾回收器一旦运行，就禁止应用程序中的所有用户线程，直到垃圾回收过程完全结束。 按照碎片处理方式分类，压缩垃圾回收器和非压缩垃圾回收器 压缩式垃圾回收器会在回收完成后，对存活对象进行压缩整理，消除回收后的碎片。 非压缩式的垃圾回收器不进行这步操作。 按照工作的内存区间分：又可分为年轻代的垃圾回收器和老年代的垃圾回收器。 2.评估GC的性能指标吞吐量：运行用户代码的时间占总运行时间的比例。（总运行时间=程序运行时间+垃圾回收时间） 暂停时间：执行垃圾回收时，程序的工作线程被暂停的时间。 收集频率：相对于应用程序的执行，收集发生的频率。 内存占用：java堆区所占内存大小。 三者总体表现会随着技术进步越来越好，主要抓住两点：吞吐量和暂停时间。 如果以吞吐量优先，那么必然需要降低内存回收的执行效率，但是这样会导致GC需要更长的时间来执行内存回收。如果选择低延迟优先，为了降低每次内存回收时的暂停时间，也只能频繁的执行内存回收，但又引起了年轻代内存的缩减和导致程序吞吐量的下降。 标准：在最大吞吐量优先的情况下，降低停顿时间。 3.不同的垃圾回收器概述1）垃圾收集器发展历史JDK1.3发布Serial GC 他是第一款GC。ParNew是Serial 的多线程版本。 JDK1.4发布Parallel GC 和 Concurrent Mark Sweep GC。 JDK6之后Parallel GC称为HotSpot默认垃圾回收器。 JDK1.7引入G1。 JDK9把G1变为默认垃圾收集器，替代CMS。 JDK10中G1垃圾收集器的并行完整垃圾回收，实现并行性来改善最坏情况下的延迟。 JDK11引入Epsilon GC。同时引入ZGC。 JDK12增强G1，自动返回未使用堆内存给操作系统，同时引入Shenandoah GC。 JDK13增强ZGC。 JDK14删除CMS，并拓展ZGC的平台兼容性。 2）垃圾收集器分类串行回收器：Serial，Serial Old 并行回收器：ParNew，Parallel Scavenge，Parallel Old 并发回收器：CMS,Gl 新生代收集器：Serial，ParNew，Parallel Scavenge 老年代收集器：Serial Old，Parallel Old，CMS 整堆收集器：G1 为什么要有很多收集器？ 因为java使用场景很多，移动端，服务端等。所以就需要针对不同的场景，提供不同的垃圾回收器，提高垃圾收集的性能。 对垃圾收集器进行比较只是对具体应用场景选择最合适的收集器。 如何查看默认的垃圾收集器？ 12345-XX:+PrintCommandLineFlags 查看命令行相关参数（包含垃圾收集器）使用命令行指令：jinfo -flag 相关垃圾回收器参数 进程IDParallel Scavenge 和 Parallel Old 4.Serial 回收器（串行回收器）jdk1.3之前回收新生代的唯一选择（HotSpot在Client模式下的默认新生代垃圾收集器） Serial 收集器采用复制算法，串行回收和STW机制的方式执行内存回收。 除了年轻代，Serial收集器还提供用于执行老年代垃圾回收的Serial Old收集器。Serial Old 收集器同样也采用了串行回收和STW机制，只不过内存回收算法使用的是标记-压缩算法。 Serial Old是运行在Client模式下默认的老年代垃圾回收器。 Serial Old在Server模式下主要有两个用途：1.与新生代的Parallel Scavenge配合使用 2.作为老年代CMS收集器的后备垃圾收集方案。 这个收集器是一个单线程的收集器，但他的单线程的意义并不仅仅说明它只会使用一个CPU或一条收集线程去完成垃圾收集工作，更重要的是在它进行垃圾收集时，必须暂停其他所有的工作线程，直到他收集结束。 优势：简单而高效，对于限定单个CPU的环境来讲，Serial收集器由于没有线程交互的开销，专心做垃圾收集自然可以获得最高的单线程收集效率。（运行在Client模式下的虚拟机是个不错的选择） 在HotSpot虚拟机中，使用-XX:UseSerialGC 参数可以指定年轻代和老年代都是用串行收集器。（等价于新生代使用Serial GC，老年代使用Serial Old GC），一般在java web程序中是不会使用这种垃圾回收器的。 5.ParNew回收器（并行回收）如果说Serial GC是年轻代中的单线程垃圾收集器，那么ParNew收集器则是Serial收集器的多线程版本。（Par是Parallel的缩写 New只能处理新生代） ParNew收集器除了采用并行回收方式执行内存回收外，两款垃圾收集器之间几乎没有差别，也是STW，也是复制算法。 ParNew是很多JVM运行在Server模式下新生代的默认垃圾收集器。 对于新生代，回收次数频繁，使用并行高效。对于老年代，回收次数少，使用串行方式节省资源。（CPU并行需要切换线程，串行可以省去切换线程的资源） 由于ParNew收集器是并行回收，那么是否可以断定在任何场景下他的回收效率都比Serial收集器更高效？ 1.多核系统下，充分利用系统资源，可以快速完成垃圾收集，提升程序吞吐量。 2.但是单个CPU下，他不一定有Serial收集器更高效。避免了线程切换。 除了Serial 外，目前只有ParNew 能与CMS收集器配合工作。 在程序中，开发人员可以通过选项-XX:UseParNewGC手动指定使用ParNew收集器执行内存回收任务 。他表示年轻代使用并行收集器并不影响老年代。 -XX:ParallelGCThreads限制线程数量，默认开启和CPU数相同的线程数。 6.Parallel Scavenge 回收器（吞吐量优先）1）概述HotSpot的年轻代除了拥有ParNew收集器是基于并行回收的以外，Parallel Scavenge收集器同样也采用了复制算法，并行回收和STW机制。 那么他的出现是否多此一举呢？ 1.和ParNew收集器不同，Parallel Scavenge收集器的目标则是达到一个可控制的吞吐量，他也被称为吞吐量优先的垃圾收集器。 2.自适应调节策略也是Parallel Scavenge与ParNew一个重要区别。 高吞吐量可以高效率的利用CPU时间，尽快完成程序的运算任务，主要适合在后台运算而不需要太多交互的任务。因此，常见在服务器环境中使用。例如：那些执行批量处理，订单处理，工资支付，科学计算的应用程序。 Parallel 收集器在jdk1.6的时候提供了用于执行老年代垃圾搜集的Parallel Old收集器，用来替代老年代的Serial Old收集器。Parallel Old收集器采用了标记-压缩算法，但是同样也是基于并行回收和STW机制。 在程序吞吐量优先的场景中，Parallel收集器和Parallel Old收集器的组合，在Server模式下的内存回收性能很不错。在Java8中，默认是此垃圾回收器。 2）参数配置-XX:UseParallelGC 手动指定年轻代使用Parallel并行收集器执行内存回收任务。 -XX:+UseParallelOldGC 手动指定老年代都是使用并行回收器。（分别适用于新生代和老年代。默认是JDK8开启的） -XX:ParallelGCThreads 设置年轻代并行收集器的线程数。一般的最好与CPU数量相等，避免线程数影响收集器性能。（默认情况下，CPU数小于8，ParallelGCThreads的值等于CPU数量，当cpu数大于8个的时候，ParallelGCThreads值=3+[5*cpu数]/8 ） -XX:MaxGCPauseMillis 设置垃圾收集器最大停顿时间（STW的时间，单位是ms） 为了尽可能把停顿时间控制在MaxGCPauseMills以内，收集器在工作时会调整Java堆大小或者其他一些参数。对于用户来讲，停顿时间越短，体验越好。但是在服务器端，我们注重高并发，整体的吞吐量，所以服务端适合Parallel，进行控制。（该参数谨慎使用） -XX:GCTimeRatio 垃圾收集时间占总时间比例（=1/(N+1)）用于衡量吞吐量大小。 -XX:UseAdaptiveSizePolicy 设置Parallel Scavenge收集器具有自适应调节策略 在这种模式下，年轻代的大小，Eden和幸存者的比例，晋升老年代的对象年龄等参数会被自动调整，已经达到在堆大小，吞吐量和停顿时间之间的平衡点。 在手动调优比较困难的场合，可以直接使用这种自适应的方式，仅仅指定虚拟机的最大堆，目标吞吐量和停顿时间，让虚拟机自己完成调优工作。 7.CMS回收器（低延迟）1）概述JDK1.5，这款收集器是HotSpot虚拟机第一款真正意义上的并发收集器，第一次实现了让垃圾收集线程与用户线程同时工作。 CMS的关注点是尽可能缩短垃圾收集时用户线程的停顿时间。停顿时间越短就越适合于用户交互的程序，良好的响应速度能提升用户体验。 目前很大一部分的java应用集中在互联网或者B/S架构系统的服务端上，这类应用尤其重视服务的响应速度，希望系统停段时间最短，以给用户较好的体验感。 CMS的垃圾收集算法采用标记-清除算法，并且也会STW。 不幸的是，CMS作为老年代的收集器，却无法与JDK1.4中已经存在的新生代收集器Parallel Scavenge 配合工作，所以在JDK1.5中使用CMS来收集老年代的时候，新生代只能选择ParNew或者Parial收集器中的一个。 在G1出现之前，CMS还是非常广泛的，一直到今天，仍然有许多系统使用CMS GC。 2）CMS工作原理初始标记：标记出GCROOTS能直接关联到的对象（速度快） 并发标记：从GCROOTS的直接关联对象开始遍历整个对象图的过程（耗时长，但是不需要停顿用户线程，可以与垃圾收集线程一起并发运行） 重新标记：修正并发标记期间，因为用户程序继续运作而导致标记产生变动的那一部分对象的标记记录（时间比初始标记稍微长） 并发清除：清理删除掉标记阶段判断的已经死亡的对象，释放内存空间。（由于不需要移动存活的对象，所以这个阶段也是可以与用户线程并发执行的） 初始化标记和再次标记仍然要STW机制，目前所有的来收集器都做不到完全不需要STW，只是尽可能的缩短暂停时间。由于最耗费时间的并发标记与并发清除阶段都不需要暂停工作，所以整体的回收是低停顿的。 在CMS回收过程中，还应该确保应用程序线程有足够的内存可用。因此，CMS收集器不能像其他收集器那样等到老年代几乎完全被填满了在进行收集，而是当堆内存使用率达到某一阈值时，便开始进行回收，以确保应用程序在CMS工作过程中依然有足够的空间支持应用程序在运行。要是CMS运行期间预留的内存无法满足程序需要，就会出现一次”Concurrent Mode Failure”失败，这时虚拟机将启动预备方案，临时启动Serial Old收集器来重新收集老年代的垃圾收集，这样停顿时间就很长了。 CMS的垃圾收集算法使用的是标记清除算法，不可避免的会产生内存碎片，无法使用指针碰撞，只能选择空闲列表。 标记清除算法会造成内存碎片，为什么不把算法换成标记整理算法呢？ 因为当并发清除时，用标记整理算法整理内存的话，原来的用户线程使用的内存无法继续使用，要保证用户线程还能继续执行，前提是他运行的资源不受影响。 优点：并发收集，低延迟 缺点：会产生内存碎片，对CPU资源敏感（并发阶段，占用了一部分线程导致应用程序变慢，总吞吐量降低），无法处理浮动垃圾（在并发阶段如果产生新的垃圾对象，CMS无法对这些垃圾对象进行标记，最终会导致这些新产生 的垃圾对象没有及时回收，只能在下一次GC的时候释放这些之前未被回收的内存空间） 3）参数设置-XX:+UseConcMarkSweepGC 手动指定使用CMS收集器执行内存回收任务 -XX:CMSlnitiatingOccupanyFraction 设置堆使用率的阈值，一旦达到阈值，便开始垃圾回收 -XX:+UseCMSCompactAtFullCollection 用于指定在执行完Full GC后堆内存空间进行压缩整理，以此避免内存碎片的产生。不过由于内存压缩整理过程无法并发执行，所带来的的问题就是停顿时间变得更长了。 -XX:+CMSFullGCsBeforeCompaction 设置在执行了多少次Full GC后对内存进行压缩整理。 -XX:ParallelCMSThreads 设置CMS线程数。CMS默认启动线程数是 （ParallelGCThreads+3）/4，ParallelGCThreads 是年轻代并行收集器的线程数。当CPU资源紧张时，受到CMS收集器线程的影响，应用程序的性能在垃圾回收节点可能会非常糟糕。 4）小技巧HotSpot有这么多的垃圾回收器，那么如果有人问，Serial GC，Parallel GC，Concurrent Mark Sweep GC这三个GC有什么不同呢？ 最小化的使用内存和并行开销 Serial GC 最大化应用程序的吞吐量 Parallel GC 最小化GC的中断或停顿时间 CMS GC 5）后续版本变化JDK9 声明CMS为过时，JDK14直接删掉了，如果使用不会报错，只是会给出警告，并使用默认的垃圾收集器。 8.G1回收器（区域分代化）1）概述G1是在java7引入的垃圾回收器，为了适应不断扩大的内存和不断增加的处理器数量，进一步降低暂停时间，同时兼顾良好的吞吐量。 为什么叫G1回收器？ 因为G1是一个并行回收器，他把堆内存分割成很多不相关的区域。G1有计划的避免在整个java堆中进行全区域的垃圾收集。G1跟踪各个区域里面的垃圾堆积的价值大小（回收所获得的空间大小以及回收所需时间的经验值），在后台维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的区域。 由于这种方式的侧重点在于回收垃圾最大量的区间，所以我们给G1一个名字，垃圾优先。 G1主要针对配备多核CPU以及大容量内存的机器，是JDK9以后的默认垃圾回收器，在JDK8还不是默认的垃圾回收器，需要使用-XX:UseGmentGC来启用。 与其他垃圾回收器相比，G1使用了全新的分区算法： 2）特点①并行与并发并行性：G1在回收期间，可以有多个GC线程同时工作，有效利用多核计算能力。此时用户线程STW。 并发性：G1拥有与应用程序交替执行的能力，部分工作可以和应用程序同时执行，因此，一般来说，不会再整个回收阶段发生完全阻塞应用程序的情况。 ②分代收集从分代上看，G1依然属于分代型垃圾回收器，他会区分年轻代和老年代，年轻代依然有Eden区和幸存者区。但从堆的结构上看，他不要求整个Eden区，年轻代或者老年代都是连续的，也不再坚持固定大小和固定数量。 将堆空间分为若干个区域，这些区域中包含了逻辑上的年轻代和老年代。 和之前的各类回收器不同，他同时兼顾年轻代和老年代。 ③空间整合CMS：标记清除算法，内存碎片，若干次GC后进行一次碎片整理。 G1将内存划分为一个个小区域，内存的回收是以一个个小区域为单位的。区域之间是复制算法，但是整体上可以看成标记压缩算法，两种算法都可以避免内存碎片。这种特性有利于程序长时间运行，分配大对象时不会因为无法找到连续内存空间而提前触发下一次GC。尤其是当java堆非常大的时候，G1的优势更加明显。 ④可预测的停顿时间每次根据允许的收集时间，优先回收价值最大的区域，保证了G1收集器在有限的时间内可以获取尽可能高的收集效率。 3）缺点在用户程序运行过程中，G1无论是为了垃圾收集产生的内存占用还是程序运行时的额外执行负载都要比CMS高。 经验上来讲：在小内存应用上CMS的表现大概率会优先于G1，而G1在大内存应用上则发挥其优势。平衡点在6-8G之间。 4）参数设置-XX:+UseG1GC 手动指定使用G1垃圾收集器执行内存回收任务 -XX:G1HeapRegionSize 设置每个区域的大小。值是2的幂，范围是1M-32M之间。 -XX:MaxGCPauseMillis 设置期望达到的最大GC停顿时间指标 -XX:ParallelGCThread 设置STW工作线程数，最多设置为8 -XX:ConcGCThreads 设置并发标记的线程数 -XX:InitiatingHeapOccupancyPercent 设置触发并发GC周期的java堆占用率阈值。超过此值，就会出发GC。 5）G1调优的步骤开启垃圾收集器 设置堆的最大内存 设置最大停顿时间 G1提供了三种垃圾收集模式：YoungGC，Mixed GC和Full GC，在不同的条件下被触发。 6）适用场景面向服务端应用，针对具有大内存，多处理器的机器 需要低GC延迟，并具有大堆的应用程序提供解决方案 用来替换掉jdk5的cms HotSpot垃圾收集器，除了G1以外，其他的垃圾收集器使用内置的JVM线程执行GC的多线程操作，而G1 GC可以采用应用线程承担后台运行的GC工作，即当JVM的GC线程处理速度慢时，系统会调用应用程序线程帮助加速垃圾收集过程。 所有的区域都大小相同，并且在JVM生命周期内不会被改变，虽然还保留新生代和老年代的概念，但是新生代和老年代不再是物理隔离了，他们都是一部分区域的集合。通过区域的动态分配的方式实现逻辑上的连续。 7）HumongousG1垃圾收集器还增加了一种新的内存区域，叫做Humongous，主要存储大对象，如果超过1.5个区域，就放到H。 设置H的原因：对于堆中的大对象，默认直接会被分配到老年代，但是如果他是一个短期的大对象，就会对垃圾收集器造成负面影响。为了解决这个问题，G1划分了一个H区，他专门用来存放大对象。如果一个H区装不下一个大对象，那么G1会寻找连续的H区来存储。为了能找到连续的H区，有时候不得不启动Full GC。G1的大多数行为都把H区作为老年代的一部分来看待。 8）回收过程G1 GC的垃圾回收过程主要包括如下三个环节 年轻代GC 老年代并发标记过程 混合回收 如果需要，单线程，独占式，高强度的Full GC还是继续存在的。他针对GC的评估失败提供了一种失败保护机制，即强力回收。 应用程序分配内存，当年轻代的Eden区用尽时开始年轻代回收过程；G1的年轻代收集阶段是一个并行的独占式收集器。在年轻代回收期，G1 GC暂停所有应用程序线程，启动多线程执行年轻代回收。然后从年轻代区间移动存活对象到Survivor区间或者老年代区间，也有可能是两个区间都会涉及。 当堆内存使用达到一定值的时候，开始老年代并发标记过程。 标记完成马上开始混合回收过程。对于一个混合回收期，G1 GC从老年区间移动存活对象到空闲区间，这些空闲区间也就成为了老年代的一部分。和年轻代不同，老年代的G1回收器和其他GC不同，G1的老年代回收器不需要整个老年代被回收，一次只需要扫描/回收一小部分老年代的区域就可以了。同时，这个老年代的区域是和新生代一起被回收的。 ①记忆集与写屏障一个对象被不同区域引用的问题 一个区域不可能是孤立的，一个区域中的对象可能被其他任意区域中的对象引用，判断对象存活时，是否需要扫描整个Java堆才能保证准确？ 在其他的分代收集器，也存在这样的问题 回收新生代也不得不同时扫描老年代？ 这样的话会降低Minor GC的效率 解决方法 无论G1还是其他垃圾收集器，Jvm都是使用Remembered Set 来避免全局扫描。 每个区域都有一个对应的Remembered Set ； 每次Reference类型数据写操作时，都会产生一个Write Barrier暂时中断操作； 然后检查将要写入的引用指向的对象是否和该Reference类型数据在不同的区域 如果不同，通过CardTable把相关引用信息记录到引用指向对象的所在区域对应的Remembered Set 中； 当进行垃圾收集时，在GC跟节点的枚举范围加入Remembered Set；就可以保证不进行全局扫描，也不会有遗漏。 ②年轻代GCJVM启动时，G1先准备好Eden区，程序在运行过程中不断创建对象到Eden区，当Eden空间耗尽时，G1会启动一次年轻代垃圾回收过程。 年轻代垃圾回收只会回收Eden区和幸存者区。 首先G1停止应用程序的执行，G1创建回收集，回收集是指需要被回收的内存分段的集合，年轻代回收过程的回收集包含年轻代Eden区和幸存者区所有的内存字分段。然后开始进行如下回收：扫描根 更新RSet 处理RSet 复制对象 处理引用 第一阶段，扫描根。 根是指static变量指向的对象，正在执行的方法调用链条上的局部变量等。根引用连同RSet记录的外部引用作为扫描存活对象的入口。 第二阶段，更新RSet。 处理dirty card queue( 见备注)中的card，更新RSet。 此阶段完成后，RSet可 以准确的反映老年代对所在的内存分段中对象的引用。 第三阶段，处理RSet. 识别被老年代对象指向的Eden中的对象，这些被指向的Eden中的对象被认为是存活的对象。第四阶段，复制对象。 此阶段，对象树被遍历，Eden区 内存段中存活的对象会被复制到Survivor区中空的内存分段，Survivor区内存段中存活的对象如果年龄未达阈值，年龄会加1，达到阀值会被会被复制到 01d区中空的内存分段。如果Survivor空间不够，Eden空 间的部分数据会直接晋升到老年代空间。 第五阶段，处理引用。 处理Soft，Weak，Phantom, Final, JNI Weak等引用。最终Eden空间的数据为空，GC停止工作，而目标内存中的对象都是连续存储的，没有碎片，所以复制过程可以达到内存整理的效果，减少碎片。 ②并发标记过程初始标记阶段 根区域扫描 并发标记 再次标记 独占标记 独占清理 并发清理阶段 1.初始标记阶段:标记从根节点直接可达的对象。这个阶段是STW的，并且会触发- - 次年轻代GC。 2.根区域扫描(Root Region Scanning) : G1 GC扫描Survivor区直接可达的老年代， 区域对象，并标记被引用的对象。这一-过程必须在young GC之前完成。 3.并发标记(Concurrent Marking): 在整个堆中进行并发标记(和应用程序并发执行)， 此过程可能被young GC中断。在并发标记阶段，若发现区域对象中的所有对象都是垃圾,那这个区域会被立即回收。同时，并发标记过程中，会计算每个区域的对象活性(区域中存活对象的比例)。 4.再次标记(Remark):由 于应用程序持续进行，需要修正上一- 次的标记结果。是STW 的。G1中采用了比CMS更快的初始快照算法:snapshot-at-the-beginning (SATB)。 5.独占清理(cleanup,STW):计算各个区域的存活对象和GC回收比例，并进行排序，识别可以混合回收的区域。为下阶段做铺垫。是STW的。 ➢这个阶段并不会实际上去做垃圾的收集。 6.并发清理阶段:识别并清理完全空闲的区域。 ③混合回收当越来越多的对象晋升到老年代，为了避免堆内存被耗尽，虚拟机会触发一个混合的垃圾收集器，即Mixed GC，该算法并不是一个Old GC，除了回收整个Young Region，还会回收一部分的Old Region。这里需要注意：是一部分老年代，而不是全部老年代。可以选择哪些Old Region进行收集。从而可以对垃圾回收的耗时时间进行控制。也要注意的是Mixed GC并不是Full GC。 并发标记结束以后，老年代中百分百为垃圾的内存分段被回收了，部分为垃圾的内存分段被计算了出来。默认情况下，这些老年代的内存分段会分8次(可以通过-XX: G1MixedGCCountTarget设置)被回收。 混合回收的回收集(Collection Set)包括八分之- -的老年代内存分段，Eden区 内存 分段，Survivor区 内存分段。混合回收的算法和年轻代回收的算法完全一样， 只是回收集多了老年代的内存分段。具体过程请参考上面的年轻代回收过程。 由于老年代中的内存分段默认分8次回收，G1会优先回收垃圾多的内存分段。垃圾占内存分段比例越高的，越会被先回收。并且有一个阈值会决定内存分段是否被回收，-XX:G1MixedGCLiveThresholdPercent，默认为65%，意思是垃圾占内存分段比例要达到65%才会被回收。如果垃圾占比太低，意味着存活的对象占比高，在复制的时候会花费更多的时间。 混合回收并不一定要进行8次。有一个阈值-XX:G1HeapWastePercent，默认值为10%，意思是允许整个堆内存中有10%的空间被浪费，意味着如果发现可以回收的垃圾占堆内存的比例低于10%，则不再进行混合回收。因为GC会花费很多的时间但是回收到的内存却很少。 ④Full GCG1的初衷就是要避免Full GC的出现，但是如果上述方式不能正常工作，G1会停止应用程序的执行，使用单线程的内存回收算法进行垃圾回收，性能会非常差，应用程序停顿时间会很长。 要避免Full GC的发生，一旦发生需要进行调整。什么时候会发生Full GC呢？比如堆内存太小，当G1在复制存活对象的时候没有空的内存分段可用，则会回退到Full GC，这种情况下可以通过增大内存解决。 导致G1Full GC的原因可能有两个 Evacuation的时候没有足够的to-space来存放晋升的对象 并发处理过程完成之前空间耗尽 ⑤G1回收器优化建议年轻代大小 避免使用-Xmn或者-XX:NewRatio等相关选项显式设置年轻代的大小 固定年轻代的大小会覆盖暂停时间目标 暂停时间目标不要太过严苛 G1 GC的吞吐量目标是90%的应用程序时间和10%的垃圾回收时间 评估G1 GC的吞吐量时，暂停时间目标不要太严苛。目标太严苛表示你愿意承受更多的垃圾回收开销，而这些会直接影响到吞吐量。 9.垃圾回收器总结1）对比 垃圾收集器 分类 作用位置 使用算法 特点 适用场景 Serial 串行 新生代 复制算法 响应速度优先 单CPU的client模式 ParNew 并行 新生代 复制算法 响应速度优先 多CPU的server模式与CMS配合使用 Parallel 并行 新生代 复制算法 吞吐量优先 后台运算而不需要太多交互的场景 Serial Old 串行 老年代 标记压缩 响应速度优先 单CPU的client模式 Parallel Old 并行 老年代 标记压缩 吞吐量优先 后台运算而不需要太多交互的场景 CMS 并发 老年代 标记清除 响应速度优先 互联网/BS业务 G1 并发，并行 新生代，老年代 标记压缩，复制 响应速度优先 服务端应用 GC发展阶段 Serial =&gt; Parallel(并行) =&gt;CMS(并发)=&gt;G1=&gt;ZGC 2）垃圾回收器的组合 3）怎么选择垃圾回收器？1.优先调整堆的大小让JVM自适应完成 2.如果内存小于100M，使用串行收集器 3.如果是单核，单机程序，并且没有停顿时间的要求，串行收集器 4.如果是多CPU，需要高吞吐量，允许停顿时间超过1s，选择并行或者JVM自己选择 5.如果是多CPU，追求低停顿时间，需快速响应，使用并发收集器 官方推荐G1，性能高。现在互联网的项目，基本都是使用G1. 没有最好的收集器，调优是针对特定场景，特定需求。 10，GC日志分析1）内存分配与垃圾回收的参数列表-XX: +PrintGC 输出Gc日志。类似: -verbose:gc -XX: +PrintGCDetails输出GC的详细日志 -XX: +PrintGCTimeStamps输出Gc的时间戳( 以基准时间的形式) -XX: +PrintGCDateStamps输出GC的时间戳(以日期的形式，如2013-05- 04T21 :53:59.234+0800) -XX: +PrintHeapAtGC在进行GC的前后打印出堆的信息 -Xloggc:../logs/gc.log日志文件的输出路径 2）GC日志中垃圾回收数据分析123456789101112131415/** * @author yhd * @createtime 2021/3/18 13:37 * vmoptions:-Xms10m -Xmx10m -XX:+PrintGCDetails */public class GetDump &#123; private int[] arr = new int[999999999]; public static void main(String[] args) &#123; IntStream.range(0, 1000000).forEach(i -&gt; new GetDump()); &#125;&#125; 12345678910111213141516171819202122[GC (Allocation Failure) [PSYoungGen: 2048K-&gt;496K(2560K)] 2048K-&gt;953K(9728K), 0.0008097 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] [GC (Allocation Failure) [PSYoungGen: 1918K-&gt;488K(2560K)] 2375K-&gt;1141K(9728K), 0.0006189 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] [GC (Allocation Failure) [PSYoungGen: 488K-&gt;488K(2560K)] 1141K-&gt;1181K(9728K), 0.0005018 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] [Full GC (Allocation Failure) [PSYoungGen: 488K-&gt;0K(2560K)] [ParOldGen: 693K-&gt;1041K(7168K)] 1181K-&gt;1041K(9728K), [Metaspace: 4058K-&gt;4056K(1056768K)], 0.0079747 secs] [Times: user=0.14 sys=0.00, real=0.01 secs] [GC (Allocation Failure) [PSYoungGen: 0K-&gt;0K(2560K)] 1041K-&gt;1041K(9728K), 0.0004530 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] [Full GC (Allocation Failure) [PSYoungGen: 0K-&gt;0K(2560K)] [ParOldGen: 1041K-&gt;987K(7168K)] 1041K-&gt;987K(9728K), [Metaspace: 4056K-&gt;4056K(1056768K)], 0.0072571 secs] [Times: user=0.00 sys=0.00, real=0.01 secs] Heap PSYoungGen total 2560K, used 51K [0x00000000ffd00000, 0x0000000100000000, 0x0000000100000000) eden space 2048K, 2% used [0x00000000ffd00000,0x00000000ffd0cc80,0x00000000fff00000) from space 512K, 0% used [0x00000000fff80000,0x00000000fff80000,0x0000000100000000) to space 512K, 0% used [0x00000000fff00000,0x00000000fff00000,0x00000000fff80000) ParOldGen total 7168K, used 987K [0x00000000ff600000, 0x00000000ffd00000, 0x00000000ffd00000) object space 7168K, 13% used [0x00000000ff600000,0x00000000ff6f6d18,0x00000000ffd00000) Metaspace used 4093K, capacity 4642K, committed 4864K, reserved 1056768K class space used 463K, capacity 497K, committed 512K, reserved 1048576KException in thread &quot;main&quot; java.lang.OutOfMemoryError: Java heap space 参数说明 &quot; [GC&quot;和&quot; [Full GC&quot; 说明了这次垃圾收集的停顿类型，如果有&quot;Full &quot;则说明GC发生了&quot;Stop The World&quot; 使用Serial收集器在新生代的名字是Default New Generation,因此显示的是&quot; [ De fNew&quot; 使用ParNew收集器在新生代的名字会变成&quot; [ParNew&quot;,意思是&quot;Parallel New Generation&quot; 使用Parallel Scavenge收 集器在新生代的名字是&quot; [PSYoungGen&quot; 老年代的收集和新生代道理一样， 名字也是收集器决定的 使用G1收集器的话，会显示为&quot;garbage- first heap&quot; Allocation Failure 表明本次引起GC的原因是因为在年轻代中没有足够的空间能够存储新的数据了。 [PSYoungGen: 5986K-&gt;696K(8704K) ] 5986K-&gt; 704K (9216K) 中括号内: GC回收前年轻代大小，回收后大小，(年轻代总 大小) 括号外: GC回收前年轻代和老年代大小，回收后大小，( 年轻代和老年代总大小) user代表用户态回收耗时，sys 内核态回收耗时，rea实际耗时。由于多核的原因，时间总和可能会超过real时间 11，日志分析工具的使用常用日志分析工具 GCViewer,GCEasy,GCHIsto,GCLogViewer,Hpjmeter,garbagecat 12,垃圾回收器的新发展1）Epsilon GC仅仅做内存分配，分配完之后程序直接结束 2）Shenandoah GC低停顿时间，由红帽开发。 暂停时间与堆大小无关，堆大小并不会影响垃圾收集的停顿时间。 高并发下吞吐量下降。 3）ZGC可伸缩，低延迟的垃圾回收器。 在尽可能对吞吐量影响不大的前提下，实现在任意堆内存大小下都可以把垃圾收集的停顿时间限制在10ms以内的低延迟。 ZGC收集器是一款基于区域内存布局的，不设置分代的，使用了读屏障，染色指针和内存多重映射等技术来实现可并发的标记-压缩算法的，以低延迟为首要目标的一款垃圾收集器。 工作过程分为四个阶段：并发标记-并发预备重分配-并发重分配-并发重映射。 除了初始标记是STW的，其余都可并发。 十七，类的加载过程详解1，概述在Java中数据类型分为基本数据类型和引用数据类型。基本数据类型由虚拟机预先定义，引用数据类型则需要进行类的加载。 (接口，注解，枚举类，类的加载都统称为类的加载) 按照Java虚拟机规范，从class文件到加载到内存中的类，到类卸载出内存为止，它的整个生命周期包括如下7个阶段: 其中，验证、准备、解析3个部分统称为链接（Linking） 从程序中类的使用过程看: 2，Loading1）加载完成的操作加载的理解 所谓加载，简而言之就是将 Java 类的字节码文件加载到机器内存中，并在内存中构建出 Java 类的原型——类模板对象。所谓类模板对象，其实就是 Java 类在 JVM 内存中的一个快照，JVM 将从字节码文件中解析出的常量池、类字段、类方法等信息存储到模板中，这样 JVM 在运行期便能通过类模板而获取 Java 类中的任意信息，能够对 Java 类的成员变量进行遍历，也能进行Java 方法的调用。 反射的机制即基于这一基础。如果 JVM 没有将 Java 类的声明信息存储起来，则 JVM 在运行期也无法反射。 加载完成的操作 加载阶段，简言之，查找并加载类的二进制数据，生成 Class 的实例。 在加载类时，Java 虚拟机必须完成以下3件事情： ①通过类的全名，获取类的二进制数据流 ②解析类的二进制数据流为方法区内的数据结构(Java 类模型) ③创建 java.lang.Class 类的实例，表示该类型。作为方法区这个类的各种数据的访问入口 2）二进制流的获取方式对于类的二进制数据流，虚拟机可以通过多种途径产生或获得。(只要所读取的字节码符合 JVM 规范即可) ①虚拟机可能通过文件系统读入一个 class 后缀的文件(最常见)。 ②读入 jar、zip 等归档数据包，提取类文件。 ③事先存放在数据库中的类的二进制数据。 ④使用类似于 HTTP 之类的协议通过网络进行加载。 ⑤在运行时生成一段 Class 的二进制信息等。 在获取到类的二进制信息后，Java 虚拟机就会处理这些数据，并最终转为一个 java.lang.Class 的实例。 如果输入数据不是 ClassFile 的结构，则会抛出 ClassFormatError。 3）类模型与class实例的位置①类模型的位置加载的类在 JVM 中创建相应的类结构，类结构会存储在方法区(JDK 1.8之前：永久代；JDK 1.8之后：元空间) ②Class 实例的位置类将 .class 文件加载至元空间后，会在堆中创建一个 java.lang.Class 对象，用来封装类位于方法区内的数据结构，该 Class 对象是在加载类的过程中创建的，每个类都对应有一个 Class 类型的对象。 ③图示 外部可以通过访问代表 Order 类的 Class 对象来获取 Order 的类数据结构。 ④说明Class 类的构造方法是私有的，只有 JVM 能够创建。 java.lang.Class 实例是访问类型元数据的接口，也是实现反射的关键数据、入口。通过 Class 类提供的接口，可以获得目标类所关联的 .class 文件中具体的数据结构：方法、字段等信息。 4）数组类的加载创建数组类的情况稍微有些特殊，因为数组类本身并不是由类加载器负责创建，而是由 JVM 在运行时根据需要而直接创建的，但数组的元素类型仍然需要依靠类加载器去创建。创建数组类(下述简称 A)的过程： 1.如果数组的元素类型是引用类型，那么就遵循定义的加载过程递归加载和创建数组 A 的元素类型。 2.JVM 使用指定的元素类型和数组维度来创建新的数组类。 如果数组的元素类型是引用类型，数组类的可访问性就由元素类型的可访问性决定。否则数组类的可访问性将被缺省定义为 public。 3，Linking1）Verification当类加载到系统后，就开始链接操作，验证是链接操作的第一步。 它的目的是保证加载的字节码是合法、合理并符合规范的。 验证的步骤比较复杂，实际要验证的项目也很繁多，大体上 Java 虚拟机需要做以下检查，如图所示： 整体说明 验证的内容则涵盖了类数据信息的格式验证、语义检查、字节码验证，以及符号引用验证等。 其中格式验证会和加载阶段一起执行。验证通过之后，类加载器才会成功将类的二进制数据信息加载到方法区中。 格式验证之外的验证操作将会在方法区中进行。 链接阶段的验证虽然拖慢了加载速度，但是它避免了在字节码运行时还需要进行各种检查。 具体说明 ①格式验证：是否以魔数 0xCAFEBABE 开头，主版本和副版本号是否在当前 Java 虚拟机的支持范围内，数据中每一个项是否都拥有正确的长度等 ②Java 虚拟机会进行字节码的语义检查，但凡在语义上不符合规范的，虚拟机也不会给予验证通过。比如： 是否所有的类都有父类的存在(在 Java 里，除了 Object 外，其他类都应该有父类) 是否一些被定义为 final 的方法或者类被重写或继承了 非抽象类是否实现了所有抽象方法或者接口方法 是否存在不兼容的方法(比如方法的签名除了返回值不同，其他都一样，这种方法会让虚拟机无从下手调度；absract 情况下的方法，就不能是final 的了) ③Java 虚拟机还会进行字节码验证，字节码验证也是验证过程中最为复杂的一个过程。它试图通过对字节码流的分析，判断字节码是否可以被正确地执行。比如： 在字节码的执行过程中，是否会跳转到一条不存在的指令 函数的调用是否传递了正确类型的参数 变量的赋值是不是给了正确的数据类型等 栈映射帧(StackMapTable)就是在这个阶段，用于检测在特定的字节码处，其局部变量表和操作数栈是否有着正确的数据类型。但遗憾的是，100%准确地判断一段字节码是否可以被安全执行是无法实现的，因此，该过程只是尽可能地检查出可以预知的明显的问题。如果在这个阶段无法通过检查，虚拟机也不会正确装载这个类。但是，如果通过了这个阶段的检查，也不能说明这个类是完全没有问题的 在前面3次检查中，已经排除了文件格式错误、语义错误以及字节码的不正确性。但是依然不能确保类是没有问题的 ④校验器还将进行符号引用的验证。Class 文件在其常量池会通过字符串记录自己将要使用的其他类或者方法。因此，在验证阶段，虚拟机就会检查这些类或者方法确实是存在的，并且当前类有权限访问这些数据，如果一个需要使用类无法在系统中找到，则会抛出 NoClassDefFoundError，如果一个方法无法被找到，则会抛出 NoSuchMethdError。 此阶段在解析环节才会执行 2）Preparation准备阶段(Preparation)，简言之，为类的静态变量分配内存，并将其初始化为默认值。 当一个类验证通过时，虚拟机就会进入准备阶段。在这个阶段，虚拟机就会为这个类分配相应的内存空间，并设置默认初始值。 Java 虚拟机为各类型变量默认的初始值如表所示： 注意：Java 并不支持 boolean 类型，对于 boolean 类型，内部实现是 int，由于 int 的默认值是0，故对应的，boolean 的默认值就是 false。 注意： ①这里不包含基本数据类型的字段用 static final 修饰的情况，因为 final 在编译的时候就会分配了，准备阶段会显式赋值。 ②注意这里不会为实例变量分配初始化，类变量会分配在方法区中，而实例变量是会随着对象一起分配到 Java 堆中。 ③在这个阶段不会像初始化阶段中那样会有初始化或者代码被执行。 3）Resolution在准备阶段完成后，就进入了解析阶段。 解析阶段(Resolution)，简言之，将类、接口、字段和方法的符号引用转为直接引用。 ①具体描述符号引用就是一些字面量的引用，和虚拟机的内部数据结构和内存分布无关。比较容理解的就是在 Class 类文件中，通过常量池进行了大量的符号引用。但是在程序实际运行时，只有符号引用是不够的，比如当如下 println() 方法被调用时，系统需要明确知道该方法的位置。 举例:输出操作System.out.println()对应的字节码: 1invokevirtual #24 &lt;java/io/PrintStream.println&gt; 以方法为例，Java 虚拟机为每个类都准备了一张方法表，将其所有的方法都列在表中，当需要调用一个类的方法的时候，只要知道这个方法在方法表中的偏移量就可以直接调用该方法。通过解析操作，符号引用就可以转变为目标方法在类中方法表中的位置，从而使得方法被成功调用。 ②小结所谓解析就是将符号引用转为直接引用，也就是得到类、字段、方法在内存中的指针或者偏移量。因此，可以说，如果直接引用存在，那么可以肯定系统中存在该类、方法或者字段。但只存在符号引用，不能确定系统中一定存在该结构。 不过 Java 虚拟机规范并没有明确要求解析阶段一定要按照顺序执行。在 HotSpot VM 中，加载、验证、准备和初始化会按照顺序有条不紊地执行，但链接阶段中的解析操作往往会伴随着 JVM 在执行完初始化之后再执行。 ③字符串的复习最后，再来看一下 CONSTANT_String 的解析。由于字符串在程序开发中有着重要的作用，因此，读者有必要了解一下 String 在 Java 虚拟机中的处理。当在 Java 代码中直接使用字符串常量时，就会在类中出现 CONSTANT_String，它表示字符串常量，并且会引用一个 CONSTANT_UTF8 的常量项。在 Java 虚拟机内部运行中的常量池，会维护一张字符串拘留表(intern)，它会保存所有出现过的字符串常量，并且没有重复项。只要以 CONSTANT_String 形式出现的字符串也都会在这张表中。使用 String.intern() 方法可以得到一个字符串在拘留表中的引用，因为该表中没有重复项，所以任何字面相同的字符串的 String.intern() 方法返回总是相等的。 4,Initialization初始化阶段，简言之，为类的静态变量赋予正确的初始值 ①具体描述类的初始化是类装载的最后一个阶段。如果前面的步骤都没有问题，那么表示类可以顺利装载到系统中。此时，类才会开始执行 Java 字节码。(即：到了初始化阶段，才真正开始执行类中定义的 Java 程序代码) 初始化阶段的重要工作是执行类的初始化方法：() 方法 该方法仅能由 Java 编译器生成并由 JVM 调用，程序开发者无法自定义一个同名的方法，更无法直接在 Java 程序中调用该方法，虽然该方法也是由字节码指令所组成 它是类静态成员的赋值语句以及 static 语句块合并产生的 ②说明在加载一个类之前，虚拟机总是会试图加载该类的父类，因此父类的 总是在子类 之前被调用，也就是说，父类的 static 块优先级高于子类 Java 编译器并不会为所有的类都产生() 初始化方法。哪些类在编译为字节码后，字节码文件中将不会包含 () 方法？ 一个类中并没有声明任何的类变量，也没有静态代码块时。 一个类中声明类变量，但是没有明确使用类变量的初始化语句以及静态代码块来执行初始化操作时。 一个类中包含 static final 修饰的基本数据类型的字段，这些类字段初始化语句采用编译时常量表达式。 1）static与final的搭配问题1234567891011121314151617181920212223242526272829/** * 说明：使用static + final修饰的字段的显式赋值的操作，到底是在哪个阶段进行的赋值？ * 情况1：在链接阶段的准备环节赋值 * 情况2：在初始化阶段&lt;clinit&gt;()中赋值 * * 结论： * 在链接阶段的准备环节赋值的情况： * 1. 对于基本数据类型的字段来说，如果使用static final修饰，则显式赋值(直接赋值常量，而非调用方法）通常是在链接阶段的准备环节进行 * 2. 对于String来说，如果使用字面量的方式赋值，使用static final修饰的话，则显式赋值通常是在链接阶段的准备环节进行 * * 在初始化阶段&lt;clinit&gt;()中赋值的情况： * 排除上述的在准备环节赋值的情况之外的情况。 * * 最终结论：使用static + final修饰，且显式赋值中不涉及到方法或构造器调用的基本数据类型或String类型的显式赋值，是在链接阶段的准备环节进行。 */public class InitializationTest2 &#123; public static int a = 1;//在初始化阶段&lt;clinit&gt;()中赋值 public static final int INT_CONSTANT = 10;//在链接阶段的准备环节赋值 public static final Integer INTEGER_CONSTANT1 = Integer.valueOf(100);//在初始化阶段&lt;clinit&gt;()中赋值 public static Integer INTEGER_CONSTANT2 = Integer.valueOf(1000);//在初始化阶段&lt;clinit&gt;()中赋值 public static final String s0 = &quot;helloworld0&quot;;//在链接阶段的准备环节赋值 public static final String s1 = new String(&quot;helloworld1&quot;);//在初始化阶段&lt;clinit&gt;()中赋值 public static String s2 = &quot;helloworld2&quot;; public static final int NUM1 = new Random().nextInt(10);//在初始化阶段&lt;clinit&gt;()中赋值&#125; 2）&lt;clinit&gt;()的线程安全性它是由类的静态变量和静态代码块组成。 对于&lt;clinit&gt;() 方法的调用，也就是类的初始化，虚拟机会在内部确保其多线程环境中的安全性 虚拟机会保证一个类的&lt;clinit&gt;()方法在多线程环境中被正确地加锁、同步，如果多个线程同时去初始化一个类，那么只会有一个线程去执行这个类的&lt;clinit&gt;()方法，其他线程都需要阻塞等待，直到活动线程执行&lt;clinit&gt;()方法完毕 正是因为函数&lt;clinit&gt;()带锁线程安全的，因此，如果一个在类的&lt;clinit&gt;()方法中有耗时很长的操作，就可能造成多个线程阻塞，引发死锁。并且这种死锁是很难发现的，因为看起来它们并没有可用的锁信息 如果之前的线程成功加载了类，则等在队列中的线程就没有机会再执行&lt;clinit&gt;()方法了。那么，当需要使用这个类时，虚拟机会直接返回给它已经准备好的信息 123456789101112131415161718192021222324252627282930313233343536/** * @author yhd * @createtime 2021/3/26 9:37 * 注意：不要再类的clinit()里面显式调用另一个类，容易造成死锁，并且JVM工具并不难呢过给出相应提示。 */public class ClinitTest &#123; public static void main(String[] args) &#123; new Thread(() -&gt; &#123; A a = new A(); &#125;).start(); B b = new B(); &#125;&#125;class A &#123; static &#123; try &#123; Class.forName(&quot;com.yhd.B&quot;); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; &#125;&#125;class B &#123; static &#123; try &#123; Class.forName(&quot;com.yhd.A&quot;); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 3）类的初始化情况：主动使用vs被动使用1-XX:TraceClassLoading //如果针对代码，设置参数 -XX:+TraceClassLoading，可以追踪类的加载信息并打印出来 Java 程序对类的使用分为两种：主动使用 和 被动使用 ①主动使用Class 只有在必须要首次使用的时候才会被装载，Java 虚拟机不会无条件地装载 Class 类型。Java 虚拟机规定，一个类或接口在初次使用前，必须要进行初始化。这里指的”使用”，是指主动使用，主动使用只有下列几种情况：(即：如果出现如下的情况，则会对类进行初始化操作。而初始化操作之前的加载、验证、准备已经完成) 1.当创建一个类的实例时，比如使用 new 关键字，或者通过反射、克隆、反序列化 2.当调用类的静态方法时，即当使用了字节码 invokestatic 指令 3.当使用类、接口的静态字段时(final 修饰特殊考虑)，比如，使用 getstatic 或者 putsttic 指令。(对应访问变量、赋值变量操作) 4.当使用 java.lang.reflect 包中的方法反射类的方法时。比如：Class.forname(&quot;com.atguigu.java.Test&quot;) 5.当初始化子类时，如果发现其父类还没有进行过初始化，则需要先触发其父类的初始化 6.如果一个接口定义了 default 方法，那么直接实现或者间接实现该接口的类的初始化，该接口要在其之前被初始化 7.当虚拟机启动时，用户需要指定一个要执行的主类(包含 main() 方法的那个类)，虚拟机会先初始化这个主类 8.当初次调用 MethodHandle 实例时，初始化该 MethodHandle 指向的方法所在的类。(涉及解析 REF_getStatic、REF_putStatic、REF_invokeStatic 方法句柄对应的类)（8了解，掌握以上7点即可） 针对5，说明 当 Java 虚拟机初始化一个类时，要求它的所有父类都已经被初始化，但是这条规则并不适用于接口 在初始化一个类时，并不会先初始化它所实现的接口 在初始化一个接口时，并不会先初始化它的父接口 因此，一个父接口并不会因为它的子接口或者实现类的初始化而初始化，只有当程序首次使用特定接口的静态字段时，才会导致该接口的初始化 针对7，说明 JVM 启动的时候通过引导类加载器加载一个初始类。这个类在调用 public static void main(String[]) 方法之前被链接和初始化。这个方法的执行将依次导致所需的类的加载、链接和初始化 ②被动使用除了以上的情况属于主动使用，其他的情况均属于被动使用。被动使用不会引起类的初始化 也就是说：并不是在代码中出现的类，就一定会被加载或者初始化。如果不符合主动使用的条件，类就不会初始化 1.当访问一个静态字段时，只有真正声明这个字段的类才会被初始化 当通过子类引用父类的静态变量，不会导致子类初始化 没有初始化并不代表子类没有加载 2.通过数组定义类引用，不会触发此类的初始化 12Parent[] parents = new Parent[10]; //不会触发Parent类的初始化，但会加载Parent类 parents[0] = new Parent(); //给数组元素赋值时，才开始加载Parent类 3.引用变量不会触发此类或接口的初始化。因为常量在链接阶段就已经被显式赋值了 4.调用 ClassLoader 类的 loadClass() 方法加载一个类，并不是对类的主动使用，不会导致类的初始化 5，Using任何一个类型在使用之前都必须经历过完整的加载、链接和初始化3个类加载步骤。一旦一个类型成功经历过这3个步骤之后，便“万事俱备，只欠东风”，就等着开发者使用了。 开发人员可以在程序中访问和调用它的静态类成员信息（比如:静态字段、静态方法)），或者使用new关键字为其创建对象实例。 6，Unloading1）类、类的加载器、类的实例之间的引用关系在类加载器的内部实现中，用一个 Java 集合来存放所加载类的引用。另一方面，一个 Class 对象总是会引用它的类加载器，调用 Class 对象的 getClassLoader() 方法，就能获得它的类加载器。由此可见，代表某个类的 Class 实例与其类的加载器之间为双向关联关系 一个类的实例总是引用代表这个类的 Class 对象。在 Object 类中定义了 getClass() 方法，这个方法返回代表对象所属类的 Class 对象的引用。此外，所有的 Java 类都有一个静态属性 Class，它引用代表这个类的 Class 对象 2）类的生命周期当 Sample 类被加载、链接和初始化后，它的生命周期就开始了。当代表 Sample 类的 Class 对象不再被引用，即不可触及时，Class 对象就会结束生命周期，Sample 类在方法区内的数据也会被卸载，从而结束 Sample 类的生命周期 一个类何时结束生命周期，取决于代表它的 Class 对象何时结束生命周期 3）具体例子 Loader1 变量和 obj 变量间接应用代表 Sample 类的 Class 对象，而 objClass 变量则直接引用它 如果程序运行过程中，将上图左侧三个引用变量都置为 null，此时 Sample 对象结束生命周期，MyClassLoader 对象结束生命周期，代表 Sample 类的 Class 对象也结束生命周期，Sample 类在方法区内的二进制数据被卸载 当再次有需要时，会检查 Sample 类的 Class 对象是否存在，如果存在会直接使用，不再重新加载；如果不存在 Sample 类会被重新加载，在 Java 虚拟机的堆区会生成一个新的代表 Sample 类的 Class 实例(可以通过哈希码查看是否是同一个实例) 4）类的卸载启动类加载器加载的类型在整个运行期间是不可能被卸载的(JVM 和 JSL 规范) 被系统类加载器和扩展类加载器加载的类型在运行期间不太可能被卸载，因为系统类加载器实例或者扩展类的实例基本上在整个运行期间总能直接或者间接的访问的到，其达到 unreachable 的可能性极小 被开发者自定义的类加载器实例加载的类型只有在很简单的上下文环境中才能被卸载，而且一般还要借助于强制调用虚拟机的垃圾收集功能才可以做到。可以预想，稍微复杂点的应用场景(比如：很多时候用户在开发自定义类的加载器实例的时候采用缓存的策略以提高系统性能)，被加载的类型在运行期间也是几乎不太可能被卸载的(至少卸载的时间是不确定的) 综合以上三点，一个已经加载的类型被卸载的几率很小至少被卸载的时间是不确定的。同时我们可以看的出来，开发者在开发代码时候，不应该以虚拟机的类型卸载为前提，去实现系统中的特定功能。 5）方法区的垃圾回收方法区的垃圾收集主要回收两部分内容：常量池中废弃的常量和不再使用的类型 HotSpot 虚拟机对常量池的回收策略是很明确的，只要常量池中的常量没有被任何地方引用，就可以被回收 判定一个常量是否”废弃”还是相对简单，而要判定一个类型是否属于”不再被使用的类”的条件就比较苛刻了。需要同时满足下面三个条件： ①该类所有的实例都已经被回收。也就是 Java 堆中不存在该类及其任何派生子类的实例 ②加载该类的类加载器已经被回收。这个条件除非是经过精心设计的可替换类加载器的场景，如 OSGI、JSP 的重加载等，否则通常是很难达成的 ③该类对应的 java.lang.Class 对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法 Java 虚拟机被允许对满足上述三个条件的无用类进行回收，这里说的仅仅是”被允许”，而并不是和对象一样，没有引用了就必然会回收 十八，深入类的加载器1，概述类加载器是 JVM 执行类加载机制的前提 ClassLoader 的作用： ClassLoader 是 Java 的核心组件，所有的 Class 都是由 ClassLoader 进行加载的，ClassLoader 负责通过各种方式将 Class 信息的二进制数据流读入 JVM 内部，转换为一个与目标类对应的 java.lang.Class 对象实例。然后交给 Java 虚拟机尽心链接、初始化等操作。因此，ClassLoader 在整个装载阶段，只能影响到类的加载，而无法通过 ClassLoader 去改变类的链接和初始化行为。至于它是否可以运行，则由 Execution Engine 决定。 类加载器最早出现在 Java 1.0 版本中，那个时候只是单纯地为了满足 Java Applet 应用而被研发出来，但如今类加载器却在 OSGI、字节码加解密领域大放异彩。 这主要归功于 Java 虚拟机的设计者们当初在设计类加载器的时候，并没有考虑将它绑定在 JVM 内部，这样做的好处就是能够更加灵活和动态地执行类加载操作。 1）类加载的分类类的加载分类：显式加载 vs 隐式加载 Class 文件的显式加载与隐式加载的方式是指 JVM 加载 Class 文件到内存的方式 显式加载指的是在代码中通过调用 ClassLoader 加载 Class 对象，如直接使用 Class.forName(name) 或 this.getClass().getClassLoader().loadClass() 加载 Class对象 隐式加载则是不直接在代码中调用 ClassLoader 的方法加载 Class 对象，而是通过虚拟机自动加载到内存中，如在加载某个类的 Class 文件时，该类的 Class 文件中引用了另外一个类的对象，此时额外引用的类将通过 JVM 自动加载到内存中。 在日常开发中以上两种方式一般会混合使用。 2）类加载的必要性一般情况下，Java 开发人员并不需要在程序中显式地使用类加载器，但是了解类加载器的加载机制却显得至关重要。从以下几个方面说： 避免在开发中遇到 java.lang.ClassNotFoundException 异常或 java.lang.NoClassDeFoundError 异常时手足无措。只有了解类加载器的加载机制才能够在出现异常的时候快速地根据错误异常日志定位问题和解决问题 需要支持类的动态加载或需要对编译后的字节码文件进行加解密操作时，就需要与类加载器打交道了 开发人员可以在程序中编写自定义类加载器来重新定义类的加载规则，以便实现一些自定义的处理逻辑 3）命名空间①何为类的唯一性？对于任意一个类，都需要由加载它的类加载器和这个类本身一同确认其在 Java 虚拟机中的唯一性。每一个类加载器，都拥有一个独立的类名称空间：比较两个类是否相等，只有在这两个类是由同一个类加载器加载的前提下才有意义。否则，即使这两个类源自同一个 Class 文件，被同一个虚拟机加载，只要加载他们的类加载器不同，那这两个类就必定不相等 ②命名空间 每个类加载器都有自己的命名空间，命名空间由该加载器以及所有的父加载器所加载的类组成 在同一命名空间中，不会出现类的完整名字(包括类的包名)相同的两个类 在不同的命名空间中，有可能会出现类的完整名字(包括类的包名)相同的两个类 在大型应用中，我们往往借助这一特性，来运行同一个类的不同版本。 4）类加载机制的基本特征 双亲委派模型。但不是所有类加载都遵守这个模型，有的时候，启动类加载器所加载的类型，是可能要加载用户代码的，比如 JDK 内部的 ServiceProvider/ServiceLoader 机制，用户可以在标准 API 框架上，提供自己的实现，JDK 也需要提供些默认的参考实现。例如，Java 中 JNDI、JDBC、文件系统、Cipher 等很多方面，都是利用的这种机制，这种情况就不会用双亲委派模型去加载，而是利用所谓的上下文加载器 可见性，子类加载器可以访问父加载器加载的类型，但是反过来是不允许的。不然，因为缺少必要的隔离，我们就没有办法利用类加载器去实现容器的逻辑 单一性，由于父加载器的类型对于子加载器是可见的，所以父加载器中加载过的类型，就不会在子加载器中重复加载。但是注意，类加载器”邻居”间，同一类型仍然可以被加载多次，因为相互并不可见 2,类加载器分类JVM 支持两种类型的类加载器，分别为引导类加载器(Bootstrap ClassLoader)和自定义类加载器(User-Defined ClassLoader) 从概念上来讲，自定义类加载器一般指的是程序中由开发人员自定义的一类类加载器，但是 Java 虚拟机规范却没有这么定义，而是将所有派生于抽象类 ClassLoader 的类加载器都划分为自定义类加载器。无论类加载器的类型如何划分，在程序中我们最常见的类加载器结构主要是如下情况： 除了顶层的启动类加载器外，其余的类加载器都应当有自己的”父类”加载器 不同类加载器看似是继承(Inheritance)关系，实际上是包含关系。在下层加载器中，包含着上层加载器的引用 “父类加载器”并非下层加载器extends上层加载器，而且包含了上层加载器的引用。 具体代码 12345678910111213141516171819class ClassLoader &#123; ClassLoader parent; //父类加载器 public ClassLoader(ClassLoader parent) &#123; this.parent = parent; &#125;&#125;class ParentClassLoader extends ClassLoader &#123; public ParentClassLoader(ClassLoader parent) &#123; super(parent); &#125;&#125;class ChildClassLoader extends ClassLoader &#123; public ChildClassLoader(ClassLoader parent) &#123; //parent = new ParentClassLoader(); super(parent); &#125;&#125; 1）Bootstrap ClassLoader 这个类加载使用 C/C++ 语言实现的，嵌套在 JVM 内部 它用来加载 Java 的核心库(JAVA_HOME/jre/lib/rt.jar 或 sun.boot.class.path 路径下的内容)。用于提供 JVM 自身需要的类 并不继承自 java.lang.ClassLoader，没有父加载器 出于安全考虑，Bootstrap 启动类加载器之加载包名为 java、javax、sun 等开头的类 加载扩展类和应用程序类加载器，并指定为他们的父类加载器。 使用 -XX:+TraceClassLoading 参数 启动类加载器使用 C++ 编写的？Yes！ C/C++：指针函数 &amp; 函数指针、C++ 支持多继承、更加高效 Java ：由 C++ 演变而来，(C++)– 版，单继承 2）Extension ClassLoaderJava 语言编写，由 sun.misc.Launcher$ExtClassLoader 实现 继承于 ClassLoader 类 父类加载器为启动类加载器 从 java.ext.dirs 系统属性所指定的目录中加载类库，或从 JDK 的安装目录的 jre/lib/ext 子目录下加载类库。如果用户创建的 JAR 放在此目录下，也会自动由扩展类加载器加载 3）AppClassLoader Java 语言编写，由 sun.misc.Launcher$AppClassLoader 实现 继承于 ClassLoader 类 父类加载器为扩展类加载器 它负责加载环境变量 classpath 或系统属性 java.class.path 指定路径下的类库 应用程序中的类加载器默认是系统类加载器 它是用户自定义类加载器的默认父加载器 通过 ClassLoader 的 getSystemClassLoader() 方法可以获取到该类加载器 4）User DIY ClassLoader 在 Java 的日常应用程序开发中，类的加载几乎是由上述3种类加载器相互配合执行的。在必要时，我们还可以自定义类加载器，来定制类的加载方式 体现 Java 语言强大生命力和巨大魅力的关键因素之一便是，Java 开发者可以自定义类加载器来实现类库的动态加载，加载源可以是本地的 JAR 包，也可以是网络上的远程资源 通过类加载器可以实现非常绝妙的插件机制，这方面的实际应用案例不胜枚举。例如，著名的 OSGI 组件框架，再如 Eclipse 的插件机制。类加载器为应用程序提供了一种动态增加新功能的机制，这种机制无需重新打包发布应用程序就能实现 同时，自定义加载器能够实现应用隔离，例如 Tomcat、Spring 等中间件和组件框架都在内部实现了自定义的加载器，并通过自定义加载器隔离不同的组件模块。这种机制比 C/C++ 程序要好太多，想不修改 C/C++ 程序就能为其新增功能，几乎是不可能的，仅仅一个兼容性便能阻挡所有美好的设想 自定义类加载器通常需要继承于 ClassLoader 3，测试不同的类加载器每个 Class 对象都会包含一个定义它的 ClassLoader 的一个引用 获取 ClassLoader 的途径 说明： 站在程序的角度看，引导类加载器与另外两种类加载器(系统类加载器和扩展类加载器)并不是同一个层次意义上的加载器，引导类加载器是使用 C++ 语言编写而成的，而另外两种类加载器则是使用 Java 语言编写的。由于引导类加载器压根儿就不是一个 Java 类，因此在 Java 程序中只能打印出空值 数组类的 Class 对象，不是由类加载器去创建的，而是在 Java 运行期 JVM 根据需要自动创建的。对于数组类的类加载器来说，是通过 Class.geetClassLoader() 返回的，与数组当中元素类型的类加载器是一样的：如果数组当中的元素类型是基本数据类型，数组类是没有类加载器的 1234567891011String[] strArr = new String[6];System.out.println(strArr.getClass().getClassLoader());//运行结果：nullClassLoaderTest[] test = new ClassLoaderTest[1];System.out.println(test.getClass().getClassLoader());//运行结果：sun.misc.Launcher$AppClassLoader@18b4aac2int[] inst = new int[2];System.out.println(inst.getClass().getClassLoader());//运行结果：null 4，ClassLoader源码解析ClassLoader 与现有类加载的关系： 除了以上虚拟机自带的加载器外，用户还可以定制自己的类加载器。Java 提供了抽象类 java.lang.ClassLoader，所有用户自定义的类加载器都应该继承 ClassLoader 类。 1）ClassLoader的主要方法public final ClassLoader getParent()返回该类加载器的超类加载器 public Class&lt;?&gt; loadClass(String name) throws ClassNotFoundException加载名称为name的类，返回结果为java.lang.Class类的实例。如果找不到类，则返回ClassNotFoundException异常。 该方法中的逻辑就是双亲委派模式的实现。 protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException查找二进制名称为name的类，返回结果为java.lang.Class类的实例。这是一个受保护的方法，JVM鼓励我们重写此方法，需要自定义加载器遵循双亲委托机制，该方法会在检查完父类加载器之后被loadClass()方法调用。 在JDK1.2之前，在自定义类加载时，总会去继承ClassLoader类并重写loadClass方法，从而实现自定义的类加载类。但是在JDK1.2之后已不再建议用户去覆盖loadClass()方法，而是建议把自定义的类加载逻辑写在findClass()方法中，从前面的分析可知， findClass()方法是在loadClass()方法中被调用的，当loadClass()方法中父加载器加载失败后，则会调用自己的findClass()方法来完成类加载，这样就可以保证自定义的类加载器也符合双亲委托模式。需要注意的是ClassLoader类中并没有实现findClass()方法的具体代码逻辑，取而代之的是抛出ClassNotFoundException异常，同时应该知道的是findClass方法通常是和defineClass方法一起使用的。一般情况下，在自定义类加载器时，会直接覆盖ClassLoader的findClass()方法并编写加载规则，取得要加载类的字节码后转换成流，然后调用defineClass()方法生成类的Class对象。 protected final Class&lt;?&gt; defineClass(byte[] b, int off, int len)根据给定的字节数组b转换为Class的实例，off和len参数表示实际Class信息在byte数组中的位置和长度，其中byte数组b是ClassLoader从外部获取的。这是受保护的方法，只有在自定义ClassLoader子类中可以使用。 defineClass()方法是用来将byte字节流解析成JVM能够识别的Class对象(ClassLoader中已实现该方法逻辑)，通过这个方法不仅能够通过class文件实例化class对象，也可以通过其他方式实例化class对象，如通过网络接收一个类的字节码，然后转换为byte字节流创建对应的Class对象。 defineClass()方法通常与findClass()方法一起使用，一般情况下，在自定义类加载器时，会直接覆盖ClassLoader的findClass()方法并编写加载规则，取得要加载类的字节码后转换成流，然后调用defineClass()方法生成类的Class对象。 123456789101112131415/** * 编写findClass方法的逻辑 */ @Override protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException &#123; // 获取类的class文件字节数组 byte[] classData = getClassData(name); if (classData == null) &#123; throw new ClassNotFoundException(); &#125; else &#123; //直接生成class对象 return defineClass(name, classData, 0, classData.length); &#125; &#125; protected final void resolveClass(Class&lt;?&gt; c)链接指定的一个Java类。使用该方法可以使用类的Class对象创建完成的同时也被解析(即加载的同时也进行解析)。前面我们说链接阶段主要是对字节码进行验证，为类变量分配内存并设置初始值同时将字节码文件中的符号引用转换为直接引用。 protected final Class&lt;?&gt; findLoadedClass(String name)查找名称为name的已经被加载过的类，返回结果为java.lang.Class类的实例。这个方法是final方法，无法被修改。 private final ClassLoader parent它也是一个ClassLoader的实例，这个字段所表示的ClassLoader也称为这个ClassLoader的双亲。在类加载的过程中,ClassLoader可能会将某些请求交予自己的双亲处理。 ①loadClass()的剖析测试代码 1ClassLoader.getSystemClassLoader().loadClass(&quot;com.atguig.java.User&quot;); 涉及到对如下方法的调用 123456789101112131415161718192021222324252627282930313233343536373839404142protected Class&lt;?&gt; loadClass(String name, boolean resolve)//resolve:true-加载class的同时进行解析操作。 throws ClassNotFoundException&#123; synchronized (getClassLoadingLock(name)) &#123; //同步操作，保证只能加载一次。 // First, check if the class has already been loaded //首先，在缓存中判断是否已经加载同名的类 Class&lt;?&gt; c = findLoadedClass(name); if (c == null) &#123; long t0 = System.nanoTime(); try &#123; //获取当前类加载器的父类加载器。 if (parent != null) &#123; //如果存在父类加载器，则调用父类加载器进行类的加载（递归） c = parent.loadClass(name, false); &#125; else &#123; //parent为null:父类加载器是引导类加教器 c = findBootstrapClassOrNull(name); &#125; &#125; catch (ClassNotFoundException e) &#123; // ClassNotFoundException thrown if class not found // from the non-null parent class loader &#125; if (c == null) &#123;//当前类的加载器的父类加载器未加载此类or此类的加载器未加载此类 // If still not found, then invoke findClass in order // to find the class. long t1 = System.nanoTime(); //调用当前ClassLoader的findClass() c = findClass(name); // this is the defining class loader; record the stats sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0); sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); &#125; &#125; if (resolve) &#123; //是否进行解析操作 resolveClass(c); &#125; return c; &#125;&#125; 2）SecureClassLoader与URLClassLoader接着SecureClassLoader扩展了ClassLoader，新增了几个与使用相关的代码源(对代码源的位置及其证书的验证)和权限定义类验证(主要指对class源码的访问权限)的方法，一般我们不会直接跟这个类打交道，更多是与它的子类URLClassLoader有所关联。 前面说过，ClassLoader是一个抽象类，很多方法是空的没有实现，比如 findClass()、findResource()等。而URLClassLoader这个实现类为这些方法提供了具体的实现。并新增了URLClassPath类协助取得Class字节码流等功能。在编写自定义类加载器时，如果没有太过于复杂的需求，可以直接继承URLClassLoader类，这样就可以避免自己去编写findClass()方法及其获取字节码流的方式，使自定义类加载器编写更加简洁。 3）ExtClassLoader与AppClassLoader了解完URLClassLoader后接着看看剩余的两个类加载器，即拓展类加载器ExtClassLoader和系统类加载AppClassLoader，这两个类都继承自URLClassLoader，是sun.misc.Launcher的静态内部类。 sun.misc.Launcher主要被系统用于启动主应用程序，ExtClassLoader和AppClassLoader都是由sun.misc.Launcher创建的，其类主要类结构如下: 我们发现ExtClassLoader并没有重写loadClass()方法，这足矣说明其遵循双亲委派模式，而AppClassLoader重载了loadclass()方法，但最终调用的还是父类loadClass()方法，因此依然遵守双亲委派模式。 4）Class.forName与ClassLoader.loadClass()Class.forName():是一个静态方法,最常用的是Class.forName(String className);根据传入的类的全限定名返回一个Class对象。该方法在将Class文件加载到内存的同时,会执行类的初始化。 如:Class.forName( &quot;com.atguigu.java.Helloworld&quot;) ; ClassLoader.loadClass():这是一个实例方法,需要一个ClassLoader对象来调用该方法。该方法将class文件加载到内存时,并不会执行类的初始化,直到这个类第一次使用时才进行初始化。该方法因为需要得到个ClassLoader对象,所以可以根据需要指定使用哪个类加载器.如: ClassLoader cl=......; cl.loadClass (&quot;com.atguigu.java.Helloworld&quot; ); 5,双亲委派模型1)定义与本质类加载器用来把类加载到Java虚拟机中。从JDK1.2版本开始，类的加载过程采用双亲委派机制，这种机制能更好地保证java平台的安全。 ①定义如果一个类加载器在接到加载类的请求时，它首先不会自己尝试去加载这个类，而是把这个请求任务委托给父类加载器去完成，依次递归，如果父类加载器可以完成类加载任务，就成功返回。只有父类加载器无法完成此加载任务时，才自己去加载。 ②本质规定了类加载的顺序是:引导类加载器先加载，若加载不到，由扩展类加载器加载，若还加载不到，才会由系统类加载器或自定义的类加载器进行加载。 2）优势与劣势①双亲委派机制优势避免类的重复加载，确保一个类的全局唯一性 Java类随着它的类加载器一起具备了一种带有优先级的层次关系，通过这种层级关可以避免类的重复加载，当父亲已经加载了该类时，就没有必要子ClassLoader再加载一次。 保护程序安全，防止核心API被随意篡改 ②代码支持双亲委派机制在java.lang.ClassLoader.loadClass(String,boolean)接口中体现。该接口的逻辑如下: (1)先在当前加载器的缓存中查找有无目标类，如果有，直接返回。 (2)判断当前加载器的父加载器是否为空，如果不为空，则调用parent.loadClass(name，false)接口进行加载。 (3)反之，如果当前加载器的父类加载器为空，则调用findBootstrapClassOrNull(name)接口，让引导类加载器进加载。 (4)如果通过以上3条路径都没能成功加载，则调用findClass(name)接口进行加载。该接口最终会调用java.lang.ClassLoader接口的defineClass系列的native接口加载目标Java类。 双亲委派的模型就隐藏在这第2和第3步中。 ③举例假设当前加载的是java.lang.Object这个类，很显然，该类属于JDK中核心得不能再核心的一个类，因此一定只能由引导类加载器进行加载。当JVM准备加载java.lang.Object时，JVM默认会使用系统类加载器去加载，按照上面4步加载的逻辑，在第1步从系统类的缓存中肯定查找不到该类，于是进入第2步。由于从系统类加载器的父加载器是扩展类加载器，于是扩展类加载器继续从第1步开始重复。由于扩展类加载器的缓存中也一定查找不到该类，因此进入第2步。扩展类的父加载器是null,因此系统调用findClass(String)，最终通过引导类加载器进行加载。 ④思考如果在自定义的类加载器中重写java.lang.ClassLoader.loadClass(String)或java.lang.ClassLoader.loadClass(String, boolean)方法,抹去其中的双亲委派机制,仅保留上面这4步中的第1步与第4步，那么是不是就能够加载核心类库了呢? 这也不行!因为JDK还为核心类库提供了一层保护机制。不管是自定义的类加载器，还是系统类加载器还是扩展类加载器，最终都必须调用java.lang.classLoader.defineClass(String,byte[], int, int, ProtectionDomain)方法，而该方法会执行preDefineClass()接口，该接口中提供了对JDK核心类库的保护。 ⑤双亲委托模式的弊端检查类是否加载的委托过程是单向的，这个方式虽然从结构上说比较清晰，使各个ClassLoader的职责非常明确，但是同时会带来一个问题，即顶层的ClassLoader无法访问底层的ClassLoader所加载的类。 通常情况下，启动类加载器中的类为系统核心类，包括一些重要的系统接口，而在应用类加载器中，为应用类。按照这种模式，应用类访问系统类自然是没有问题，但是系统类访问应用类就会出现问题。比如在系统类中提供了一个接口，该接口需要在应用类中得以实现，该接口还绑定一个工厂方法，用于创建该接口的实例，而接口和工厂方法都在启动类加载器中。这时，就会出现该工厂方法无法创建由应用类加载器加载的应用实例的问题。 ⑥结论:由于Java虚拟机规范并没有明确要求类加载器的加载机制一定要使用双亲委派模型，只是建议采用这种方式而己。 比如在Tomcat中，类加载器所采用的加载机制就和传统的双亲委派模型有一定区别，当缺省的类加载器接收到一个类的加载任务时，首先会由它自行加载，当它加载失败时，才会将类的加载任务委派给它的超类加载器去执行，这同时也是Servlet规范推荐的一种做法。 3）破坏双亲委派机制①破坏双亲委派机制1双亲委派模型并不是一个具有强制性约束的模型，而是Java设计者推荐给开发者们的类加载器实现方式。 在Java的世界中大部分的类加载器都遵循这个模型，但也有例外的情况，直到Java模块化出现为止，双亲委派模型主要出现过3次较大规模“被破坏”的情况。 第一次破坏双亲委派机制: 双亲委派模型的第一次“被破坏”其实发生在双亲委派模型出现之前–即JDK1.2面世以前的“远古”时代。 由于双亲委派模型在JDK 1.2之后才被引入，但是类加载器的概念和抽象类java.lang.ClassLoader则在Java的第一个版本中就已经存在，面对已经存在的用户自定义类加载器的代码，Java设计者们引入双亲委派模型时不得不做出一些妥协，为了兼容这些已有代码，无法再以技术手段避免loadClass()被子类覆盖的可能性，只能在JDK1.2之后的java.lang.ClassLoader中添加一个新的protected方法findClass()，并引导用户编写的类加载逻辑时尽可能去重写这个方法，而不是在loadClass()中编写代码。上节我们已经分析过loadClass()方法，双亲委派的具体逻辑就实现在这里面，按照loadClass()方法的逻辑，如果父类加载失败，会自动调用自己的findClass()方法来完成加载，这样既不影响用户按照自己的意愿去加载类，又可以保证新写出来的类加载器是符合双亲委派规则的。 ②破坏双亲委派机制2第二次破坏双亲委派机制:线程上下文类加载器 双亲委派模型的第二次“被破坏”是由这个模型自身的缺陷导致的，双亲委派很好地解决了各个类加载器协作时基础类型的一致性问题〈越基础的类由越上层的加载器进行加载），基础类型之所以被称为“基础”，是因为它们总是作为被用户代码继承、调用的API存在，但程序设计往往没有绝对不变的完美规则，如果有基础类型又要调用回用户的代码,那该怎么办呢? 这并非是不可能出现的事情，一个典型的例子便是JNDI服务，JNDI现在已经是Java的标准服务，它的代码由启动类加载器来完成加载（在JDK 1.3时加入到rt.jar的)，肯定属于Java中很基础的类型了。但JNDI存在的目的就是对资源进行查找和集中管理，它需要调用由其他厂商实现并部署在应用程序的ClassPath下的NDI服务提供者接口（Service Provider Interface，SPI）的代码，现在问题来了，启动类加载器是绝不可能认识、加载这些代码的，那该怎么办?(SPI:在Java平台中，通常把核心类rt.jar中提供外部服务、可由应用层自行实现的接口称为SPI) 为了解决这个困境，Java的设计团队只好引入了一个不太优雅的设计:线程上下文类加载器（Thread ContextClassLoader)。这个类加载器可以通过java.lang.Thread类的setContextClassLoader()方法进行设置，如果创建线程时还未设置，它将会从父线程中继承一个，如果在应用程序的全局范围内都没有设置过的话，那这个类加载器默认就是应用程序类加载器。 有了线程上下文类加载器，程序就可以做一些“舞弊”的事情了。JNDI服务使用这个线程上下文类加载器去加载所需的SPI服务代码，这是一种父类加载器去请求子类加载器完成类加载的行为，这种行为实际上是打通了双亲委派模型的层次结构来逆向使用类加载器，已经违背了双亲委派模型的一般性原则，但也是无可奈何的事情。Java中涉及SPI的加载基本上都采用这种方式来完成，例如NDI、JDBC、JCE、JAXB和BT等。不过，当SPI的服务提供者多于一个的时候，代码就只能根据具体提供者的类型来硬编码判断，为了消除这种极不优雅的实现方式，在JDK 6时，JDK提供了java.util.ServiceLoader类，以META-INF/services中的配置信息，辅以责任链模式，这才算是给SPI的加载提供了一种相对合理的解决方案。 默认上下文加载器就是应用类加载器，这样以上下文加载器为中介，使得启动类加载器中的代码也可以访问应用类加载器中的类。 ③破坏双亲委派机制3第三次破坏双亲委派机制: 双亲委派模型的第三次“被破坏”是由于用户对程序动态性的追求而导致的。如:代码热替换（Hot Swap)、模块热部署（Hot Deployment）等 IBM公司主导的JSR-291(即OSGi R4.2）实现模块化热部署的关键是它自定义的类加载器机制的实现，每一个程序模块（OSGi中称为Bundle)都有一个自己的类加载器，当需要更换一个Bundle时，就把Bundle连同类加载器一起换掉以实现代码的热替换。在OSGi环境下，类加载器不再双亲委派模型推荐的树状结构，而是进一步发展为更加复杂的网状结构。 当收到类加载请求时，OSGi将按照下面的顺序进行类搜索:（不细讲） 1）将以java.*开头的类，委派给父类加载器加载。 2）否则，将委派列表名单内的类，委派给父类加载器加载。 3）否则，将Import列表中的类，委派给Export这个类的Bundle的类加载器加载。 4）否则，查找当前Bundle的ClassPath，使用自己的类加载器加载。 5）否则，查找类是否在自己的Fragment Bundle中，如果在，则委派给Fragment Bundle的类加载器加载。 6）否则，查找Dynamic Import列表的Bundle，委派给对应Bundle的类加载器加载。 7）否则，类查找失败。 说明:只有开头两点仍然符合双亲委派模型的原则，其余的类查找都是在平级的类加载器中进行的 小结: 这里，我们使用了“被破坏”这个词来形容上述不符合双亲委派模型原则的行为，但这里“被破坏”并不一定是带有贬义的。只要有明确的目的和充分的理由，突破旧有原则无疑是一种创新。 正如:OSGi中的类加载器的设计不符合传统的双亲委派的类加载器架构，且业界对其为了实现热部署而带来的额外的高复杂度还存在不少争议，但对这方面有了解的技术人员基本还是能达成一个共识，认为OSGi中对类加载器的运用是值得学习的，完全弄懂了OSGi的实现，就算是掌握了类加载器的精粹。 4）热替换的实现热替换是指在程序的运行过程中，不停止服务，只通过替换程序文件来修改程序的行为。热替换的关键需求在于服务不能中断，修改必须立即表现正在运行的系统之中。基本上大部分脚本语言都是天生支持热替换的，比如: PHP，只要替换了PHP源文件，这种改动就会立即生效，而无需重启Web服务器。 但对Java来说，热替换并非天生就支持，如果一个类已经加载到系统中，通过修改类文件，并无法让系统再来加载并重新定义这个类。因此，在Java中实现这一功能的一个可行的方法就是灵活运用ClassLoader。 注意:由不同ClassLoader加载的同名类属于不同的类型，不能相互转换和兼容。即两个不同的ClassLoader加载同个类，在虚拟机内部，会认为这2个类是完全不同的。 根据这个特点，可以用来模拟热替换的实现，基本思路如下图所示: 6，沙箱安全机制 保护程序安全 保护 Java 原生的 JDK 代码 Java 安全模型的核心就是 Java 沙箱(Sandbox)。什么是沙箱？沙箱就是一个限制程序运行的环境 沙箱机制就是将 Java 代码限定在虚拟机(JVM)特定的运行范围中，并且严格限制代码对本地系统资源访问。通过这样的措施来保证对代码的有限隔离，防止对本地系统造成破坏 沙箱主要限制系统资源访问，那系统资源包括什么？CPU、内存、文件系统、网络。不同级别的沙箱对这些资源访问的限制也可以不一样 所有的 Java 程序运行都可以指定沙箱，可以定制安全策略 1）JDK1.0时期在Java中将执行程序分成本地代码和远程代码两种，本地代码默认视为可信任的，而远程代码则被看作是不受信的。对于授信的本地代码，可以访问一切本地资源。而对于非授信的远程代码在早期的Java实现中，安全依赖于沙箱（Sandbox）机制。如下图所示JDK1.0安全模型 2）JDK1.1时期JDK1.0中如此严格的安全机制也给程序的功能扩展带来障碍，比如当用户希望远程代码访问本地系统的文件时候，就无法实现。 因此在后续的Java1.1版本中，针对安全机制做了改进，增加了安全策略。允许用户指定代码对本地资源的访问权限。如下图所示JDK1.1安全模型 3）JDK1.2时期在Java1.2版本中，再次改进了安全机制，增加了代码签名。不论本地代码或是远程代码，都会按照用户的安全策略设定，由类加载器加载到虚拟机中权限不同的运行空间，来实现差异化的代码执行权限控制。如下图所示JDK1.2安全模型 4）JDK1.6时期虚拟机会把所有代码加载到不同的系统域和应用域。系统域部分专门负责与关键资源进行交互，而各个应用域部分则通过系统域的部分代理来对各种需要的资源进行访问。虚拟机中不同的受保护域(Protected Domain)，对应不一样的权限（Permission)。存在于不同域中的类文件就具有了当前域的全部权限，如下图所示，最新的安全模型（jdk1.6) 7，自定义类加载器1）为什么要自定义类加载器? 隔离加载类 在某些框架内进行中间件与应用的模块隔离，把类加载到不同的环境。比如:阿里内某容器框架通过自定义类加载器确保应用中依赖的jar包不会影响到中间件运行时使用的jar包。再比如: Tomcat这类web应用服务器，内部自定义了好几种类加载器，用于隔离同一个web应用服务器上的不同应用程序。（类的仲裁–&gt;类冲突） 修改类加载的方式 类的加载模型并非强制，除Bootstrap外，其他的加载并非一定要引入，或者根据实际情况在某个时间点进行按需进行动态加载 扩展加载源 比如从数据库、网络、甚至是电视机机顶盒进行加载 防止源码泄漏 Java代码容易被编译和篡改，可以进行编译加密。那么类加载也需要自定义，还原加密的字节码。 2）常见的场景 实现类似进程内隔离，类加载器实际上用作不同的命名空间，以提供类似容器、模块化的效果。例如，两个模块依赖于某个类库的不同版本，如果分别被不同的容器加载，就可以互不干扰。这个方面的集大成者是Java EE和OSGI、JPMS等框架。 应用需要从不同的数据源获取类定义信息，例如网络数据源，而不是本地文件系统。或者是需要自己操纵字节码，动态修改或者生成类型。 3）注意:在一般情况下，使用不同的类加载器去加载不同的功能模块，会提高应用程序的安全性。但是，如果涉及Java类型转换，则加载器反而容易产生不美好的事情。在做Java类型转换时，只有两个类型都是由同一个加载器所加载，才能进行类型转换，否则转换时会发生异常。 4）实现方式用户通过定制自己的类加载器，这样可以重新定义类的加载规则，以便实现一些自定义的处理逻辑。 ①实现方式Java提供了抽象类java.lang.ClassLoader，所有用户自定义的类加载器都应该继承ClassLoader类。 在自定义ClassLoader 的子类时候，我们常见的会有两种做法: 方式一:重写loadclass()方法 重写findClass()方法【推荐】 ②对比这两种方法本质上差不多，毕竟loadClass()也会调用findClass()，但是从逻辑上讲我们最好不要直接修改loadClass()的内部逻辑。建议的做法是只在findClass()里重写自定义类的加载方法，根据参数指定类的名字，返回对应的Class对象的引用。 loadClass()这个方法是实现双亲委派模型逻辑的地方，擅自修改这个方法会导致模型被破坏，容易造成问题。因此我们最好是在双亲委派模型框架内进行小范围的改动，不破坏原有的稳定结构。同时，也避免了自己重写loadClass()方法的过程中必须写双亲委托的重复代码，从代码的复用性来看，不直接修改这个方法始终是比较好的选择。 当编写好自定义类加载器后，便可以在程序中调用loadClass(）方法来实现类加载操作。 ③说明 其父类加载器是系统类加载器 JVM中的所有类加载都会使用java.lang.ClassLoader.loadClass(String)接口(自定义类加载器并重写java.lang.ClassLoader.loadClass(String)接口的除外)，连JDK的核心类库也不能例外。 8，Java9新特性为了保证兼容性，JDK 9没有从根本上改变三层类加载器架构和双亲委派模型，但为了模块化系统的顺利运行，仍然发生了一些值得被注意的变动。 1.扩展机制被移除，扩展类加载器由于向后兼容性的原因被保留，不过被重命名为平台类加载器（platform classloader)。可以通过ClassLoader的新方法getPlatformClassLoader()来获取。 JDK 9时基于模块化进行构建（原来的 rt.jar 和 tools.jar 被拆分成数十个 JMOD 文件)，其中的Java类库就已天然地满足了可扩展的需求，那自然无须再保留\\lib\\ext 目录，此前使用这个目录或者 java.ext.dirs系统变量来扩展JDK功能的机制已经没有继续存在的价值了。 2.平台类加载器和应用程序类加载器都不再继承自 java.net.URLClassLoader。 现在启动类加载器、平台类加载器、应用程序类加载器全都继承于jdk.internal.loader.BuiltinClassLoader。 如果有程序直接依赖了这种继承关系，或者依赖了URLClassLoader类的特定方法，那代码很可能会在 JDK9及更高版本的JDK中崩溃。 3.在Java 9中，类加载器有了名称。该名称在构造方法中指定，可以通过getName()方法来获取。平台类加载器的名称是platform，应用类加载器的名称是app。类加载器的名称在调试与类加载器相关的问题时会非常有用。 4.启动类加载器现在是在jvm内部和java类库共同协作实现的类加载器〈以前是C++实现)，但为了与之前代码兼容，在获取启动类加载器的场景中仍然会返回null，而不会得到BootClassLoader实例。 5.类加载的委派关系也发生了变动。 当平台及应用程序类加载器收到类加载请求，在委派给父加载器加载前，要先判断该类是否能够归属到某一个系统模块中，如果可以找到这样的归属关系，就要优先委派给负责那个模块的加载器完成加载。 在Java模块化系统明确规定了三个类加载器负责各自加载的模块。 十九，概述1，背景说明1）生产环境中的问题生产环境发生了内存溢出该如何处理 生产环境应该给服务器分配多少内存合适？ 如何对垃圾回收器的性能进行调优？ 生产环境CPU负载飙高该如何处理？ 生产环境应该给应用分配多少线程合适？ 不加log，如何确定请求是否执行了某一行代码？ 不加log，如何实时查看某个方法的入参与返回值？ 2）为什么要调优防止出现OOM 解决OOM 减少Full GC 出现的频率 3）不同阶段的思考上线前 项目运行中 线上OOM 2，调优概述1）监控的依据运行日志 异常堆栈 GC日志 线程快照 堆转储快照 2）调优方向合力编写代码 充分利用硬件资源 合理进行JVM调优 3，性能优化的步骤1）性能监控一种以非强行或者入侵方式收集或查看应用运营性能数据的活动。 监控通常是指种在生产、质量评估或者开发环境下实施的带有预防或主动性的活动。 当应用相关干系人提出性能问题却没有提供足够多的线索时，首先我们需要进行性能监控，随后是性能分析。 监控的维度： GC频繁 cpu load 过高 OOM 内存泄漏 死锁 程序响应时间较长 2）性能分析一种以侵入方式收集运行性能数据的活动，它会影响应用的吞吐量或响应性。 性能分析是针对性能问题的答复结果，关注的范围通常比性能监控更加集中。 性能分析很少在生产环境下进行，通常是在质量评估、系统测试或者开发环境下进行，是性能监控之后的步骤。 方式 打印GC日志，通过GCviewer或者http://gceasy.io来分析异常信息 灵活运用命令行工具、jstack、jmap、jinfo等 dump出堆文件，使用内存分析工具分析文件 使用阿里Arthas、jconsole、JVisualVM来实时查看JVM状态 jstack查看堆栈信息 3）性能调优一种为改善应用响应性或吞吐量而更改参数，源代码，属性配置的活动，性能调优是在性能监控，性能分析之后的活动。 适当增加内存，根据业务背景选择垃圾回收器 优化代码，控制内存使用 增加机器，分散节点压力 合理设置线程池线程数量 使用中间件提高程序效率，比如缓存、消息队列等 4，性能评价/测试指标1）停顿时间提交请求和返回该请求的响应之间使用的时间，一般比较关注平均响应时间。 在垃圾回收环节中： 暂停时间：执行垃圾收集时，程序的工作线程被暂停的时间。 1-XX:MaxGCPauseMillis 2)吞吐量对单位时间内完成的工作量（请求）的量度。 在GC中：运行用户代码的时间占总运行时间的比例（总运行时间：程序的运行时间+内存回收的时间） 1吞吐量为1-1/(1+n)，其中-XX::GCTimeRatio=n 3）并发数同一时刻，对服务器有实际交互的请求数。 4）内存占用java堆区所占内存大小 5）相互间的关系吞吐量：每天通过高速公路收费站的车辆的数据 并发数：高速公路上正在行驶的车辆的数目 响应时间：车速 二十，JVM监控及诊断工具-命令行篇1，JPS-查看正在运行的java进程1）概述jps：显式指定系统内所有的HotSpot虚拟机进程（查看虚拟机进程信息），可用于查询正在运行的虚拟机进程。 说明：对于本地虚拟机进程来说，进程的本地虚拟机ID与操作系统的进程ID时一致的，是唯一的。 2）基本语法1jps [options] [hostid] //可以通过追加参数，打印额外信息 ①options参数-q :仅仅显示LVMID (local virtual machine id), 即本地虚拟机唯一-id。 不显示主类的名称等 -l:输出应用程序主类的全类名或如果进程执行的是jar包，则输出jar完整路径 -m:输出虚拟机进程启动时传递给主类main()的参数 -v: 列出虚拟机进程启动时的JVM参数。比如: -Xms20m -Xmx50m是 启动程序指定的jvm参数。 说明:以上参数可以综合使用。 补充: 如果某Java进程关闭了默认开启的UsePerfData参数(即使用参数**-XX:-UsePerfData**) ，那么jps命令(以及下面介绍的jstat)将无法探知该Java进程。 如何将信息输入到同级文件中： 语法：命令 &gt; 文件名称 ②hostid参数RMI注册表中注册的主机名。 如果想要远程监控主机上的java 程序，需要安装jstatd。 对于具有更严格的安全实践的网络场所而言，可能使用-一个自定义的策略文件来显示对特定的可信主机或网络的访问，尽管这种技术容易受到IP地址欺诈攻击。 如果安全问题无法使用一个定制的策略文件来处理，那么最安全的操作是不运行 jstatd服务器， 而是在本地使用jstat和jps工具。 2，jstat-查看JVM统计信息1）概述jstat(JVM Statistics Monitoring Tool): 用于监视虚拟机各种运行状态信息的命令行工具。它可以显示本地或者远程虚拟机进程中的类装载、内存、垃圾收集、JIT编译等运行数据。 在没有GUI图形界面，只提供了纯文本控制台环境的服务器上，它将是运行期定位虚拟机性能问题的首选工具。常用于检测垃圾回收问题以及内存泄漏问题。 2）基本语法1jstat -&lt;option&gt; [-t] [-h &lt;lines&gt;] &lt;vmid&gt; [&lt;interval&gt; [&lt;count&gt;] ] 其中vmid是进程id号 ①option参数选项option可以由以下值构成。 ●类装载相关的: -class: 显示ClassLoader的相关信息: 类的装载、卸载数量、总空间、类装载所消耗的时间等 ●垃圾回收相关的: -gc: 显示与GC相关的堆信息。包括Eden区、两个Survivor区、 老年代、永久代等的容量、己用空间、GC时间合计等信息。 -gccapacity:显示内容与-gc基本相同，但输出主要关注Java堆各个区域使用到的最大、最小空间。 -gcutil:显示内容与-gc基本相同，但输出主要关注已使用空间占总空间的百分比。 -gccause:与-gcuti1功能一样，但是会额外输出导致最后-一次或当前正在发生的GC产生的原因。 -gcnew: 显示新生代GC状况 -gcnewcapacity: 显示内容与-gcnew基 本相同，输出主要关注使用到的最大、最小空间 -geold: 显示老年代GC状况 -gcoldcapacity: 显示内容与-gcold基本相同，输出主要关注使用到的最大、最小空间 -gcpermcapacity:显示永久代使用到的最大、最小空间。 ●JIT相关的: -compiler: 显示JIT编译器编译过的方法、耗时等信息 -printcompilation: 输出已经被JIT编译的方法 以-GC为例： 新生代相关 S0C是第一个幸存者区的大小（字节） S1C是第二个幸存者区的大小（字节） S0U是第一个幸存者区已使用的大小（字节） S1U是第二个幸存者区已使用的大小（字节） EC是Eden空间的大小（字节） EU是Eden空间已使用大小（字节） 老年代相关 OC是老年代的大小（字节） OU是老年代已使用的大小（字节） 方法区相关 MC是方法区的大小 MU是方法区已使用的大小 CCSC是压缩类空间的大小 CCSU是压缩类空间已使用的大小 其他 YGC是从应用程序启动到采样时young gc的次数 YGCT是指从应用程序启动到采样时young gc消耗时间（秒） FGC是从应用程序启动到采样时full gc的次数 FGCT是从应用程序启动到采样时的full gc的消耗时间（秒） GCT是从应用程序启动到采样时gc的总时间 ②interval参数用于指定输出统计数据的周期，单位为毫秒。即：查询间隔 ③count参数用于指定查询的总次数 ④-t参数可以在输出信息前加上一个Timestamp列，显示程序的运行时间。单位：秒 我们可以比较Java进程的启动时间以及总GC时间(GCT列)，或者两次测量的间隔时间以及总GC时间的增量，来得出GC时间占运行时间的比例。 如果该比例超过20%， 则说明目前堆的压力较大;如果该比例超过90%，则说明堆里几乎没有可用空间，随时都可能抛出00M异常。 我们执行jstat -gc -t 13152 1000 10，这代表1秒打印出1行，一共10行，-t代表打印出Timestamp总运行时间，结果如下所示： ⑤-h参数可以在周期性数据输出时，输出多少行数据后输出一个表头信息 3）补充jstat还可以用来判断是否出现内存泄漏。 第1步: 在长时间运行的Java程序中，我们可以运行jstat命令连续获取多行性能数据，并取这几行数据中ou列(即己占用的老年代内存)的最小值。 第2步: 然后，我们每隔- 段较长的时间重复- -次上述操作， 来获得多组ou最小值。 如果这些值呈 上 涨趋势，则说明该Java 程序的老年代内存已使用量在不断上涨，这意味着无法回收的对象在不断增加，因此很有可能存在内存泄漏。 3，jinfo-实时查看和修改JVM参数1）概述jinfo(Configuration Info for Java) 查看虛拟机配置参数信息，也可用于调整虚拟机的配置参数。 在很多情况不，Java应用程序不会指定所有的Java虚拟机参数。而此时，开发人员可能不知道某一个具体的Java虛拟机参数的默认值。在这种情况下，可能需要通过查找文档获取某个参数的默认值。这个查找过程可能是非常艰难的。但有了jinfo工具，开发人员可以很方便地找到Java虛拟机参数的当前值。 2）基本语法①查看jinfo -sysprops 进程id 可以查看由System.getProperties()取得的参数 jinfo -flags 进程id 查看曾经赋过值的一些参数 jinfo -flag 参数名称 进程id 查看某个java进程的具体参数信息 ②修改jinfo不仅可以查看运行时某-个Java虚拟机参数的实际取值，甚至可以在运行时修改部分参数，并使之立即生效。 但是，并非所有参数都支持动态修改。参数只有被标记为manageable的f1ag可以被实时修改其实，这个修改能力是极其有限的。 可以查看被标记为manageable的参数 1java -XX: +PrintFlagsFinal -version| grep manageable 针对boolean类型 jinfo -flag [+|-]参数名称 进程id 1jinfo -flag +PrintGCDetails 18232 针对非boolean类型 jinfo -flag 参数名称=参数值 进程id 1jinfo -flag MaxHeapFreeRatio=90 8600 3）拓展java -XX:+PrintFlagsInitial 查看所有JVM参数启动的初始值 java -XX:+PrintFlagsFinal查看所有JVM参数的最终值 java -参数名称:+PrintCommandLineFlags查看那些已经被用户或者JVM设置过的详细的XX参数的名称和值 4，jmap-导出内存映像文件&amp;内存使用情况1）概述jmap(JVM Memory Map): 作用一 方面是获取dump文件(堆转储快照文件，二进制文件)，它还可以获取目标Java进程的内存相关信息，包括Java堆各区域的使用情况、堆中对象的统计信息、类加载信息等。 开发人员可以在控制台中输入命令“jmap -help“查阅jmap工具的具体使用方式和一 些标准选项配置。 2）基本语法123jmap [option] &lt;pid&gt;jmap [option] &lt;executable &lt;core&gt;jmap [option] [server_id@]&lt;remote server IP or hostname&gt; 使用语法可以通过在DOS窗口中使用jmap/jmap -h/jmap -help查看 &lt;executable &lt;core&gt;代表可执行的代码，比如使用&gt; 文件名称来指定生成的dump文件的生成位置 [server_id@]&lt;……&gt;是为远程连接准备的 -dump:生成Java堆转储快照：dump文件,特别的：-dump:live只保存堆中的存活对象 -heap:输出整个堆空间的详细信息，包括GC的使用、堆配置信息，以及内存的使用信息等 -histo:输出堆中对象的同级信息，包括类、实例数量和合计容量,特别的：-histo:live只统计堆中的存活对象 -permstat:以ClassLoader为统计口径输出永久代的内存状态信息(仅linux/solaris平台有效) -finalizerinfo:显示在F-Queue中等待Finalizer线程执行finalize方法的对象(仅linux/solaris平台有效) -F:当虚拟机进程对-dump选项没有任何响应时，可使用此选项强制执行生成dump文件(仅linux/solaris平台有效) 3)导出内存映像文件一般来说， 使用jmap指 令生成dump文件的操作算得上是最常用的jmap命令之一， 将堆中所有存活对象导出至一一个文件之中。 Heap Dump又叫做堆存储文件，指-个Java进 程在某个时间点的内存快照。 说明: 1.通常在写Heap Dump文件前会触发一 次Fu1l GC， 所以heap dump文件里保存的都是Fu11GC后留下的对象信息。 2.由于生成dump文件比较耗时，因此大家需要耐心等待，尤其是大内存镜像生成dump文件则需要耗费更长的时间来完成。 注意： 对于以上说明中的第1点是自动方式才会这样做，而手动不会在Full GC之后生成Dump 使用手动方式生成dump文件，一般指令执行之后就会生成，不用等到快出现OOM的时候 使用自动方式生成dump文件，当出现OOM之前先生成dump文件 如果使用手动方式，一般使用第2种，毕竟生成堆中存活对象的dump文件是比较小的，便于传输和分析 ①手动12jmap -dump:format=b,file=&lt;filename.hprof&gt; &lt;pid&gt;jmap -dump:live,format=b,file=&lt;filename.hprof&gt; &lt;pid&gt; 由于jmap将访问堆中的所有对象，为了保证在此过程中不被应用线程干扰，jmap 需要借助安 全点机制，让所有线程停留在不改变堆中数据的状态。也就是说，由jmap 导出的堆快照必定是安全点位置的。这可能导致基于该堆快照的分析结果存在偏差。 举个例子，假设在编译生成的机器码中，某些对象的生命周期在两个安全点之间，那么:live选项将无法探知到这些对象。 另外，如果某个线程长时间无法跑到安全点，jmap将一 直等 下去。与前面讲的jstat则不同，垃圾回收器会主动将jstat所需要的摘要数据保存至固定位置之中，而jstat只需直接读取即可。 ②自动12-XX:+HeapDumpOnOutOfMemoryError-XX:HeapDumpPath=&lt;filename.hprof&gt; 当程序发生00M退出系统时，一些瞬时信息都随着程序的终止而消失，而重现00M问题往往比较困难或者耗时。此时若能在0OM时，自动导出dump文件就显得非常迫切。 这里介绍一种比较常用的取得堆快照文件的方法，即使用:-XX: +HeapDump0nOutOfMemoryError:在程序发生0OM时，导出应用程序的当前堆快照。-XX:HeapDumpPath:可以指定堆快照的保存位置。 比如:-Xmx100m -XX: +HeapDumpOnOutOfMemoryError -XX: HeapDumpPath=D: \\m . hprof 4)显示堆内存相关信息①jmap -heap 进程idjmap -heap 进程id只是时间点上的堆信息，而jstat后面可以添加参数，可以指定时间动态观察数据改变情况，而图形化界面工具，例如jvisualvm等，它们可以用图表的方式动态展示出相关信息，更加直观明了 jmap -heap 3540 &gt;a.txt ②jmap -histo 进程id输出堆中对象的同级信息，包括类、实例数量和合计容量，也是这一时刻的内存中的对象信息 jmap -histo 3540 &gt;b.txt 5）其他作用这两个指令仅linux/solaris平台有效，所以无法在windows操作平台上演示。 jmap -permstat 进程id 查看系统的ClassLoader信息 jmap -finalizerinfo查看堆积在finalizer队列中的对象 5，jhat-JDK自带堆分析工具jhat命令在jdk9及其之后就被移除了，官方建议使用jvisualvm代替jhat，所以该指令只需简单了解一下即可. 1)概述Sun JDK提供的jhat命令与jmap命令搭配使用，用于分析jmap生成的heap dump文件(堆转 储快照)。jhat内置了-个微 型的HTTP/HTML服务器， 生成dump 文件的分析结果后，用户可以在浏览器中查看分析结果(分析虚拟机转储快照信息)。 使用了jhat命令， 就启动了一个http服务，端口是7000， 即http://localhost:7000/,就可以在浏览器里分析。 说明: jhat 命令在JDK9、JDK10中已经被删除，官方建议用VisualVM代替。 2）基本语法1jhat [option] [dumpfile] 其中dumpfile代表dump文件的地址以及名称,jhat d:\\3.hprof。 ①options参数| -stack falseltrue | 关闭|打开对象分配调用栈跟踪 || — | — || -refs falseltrue | 关闭|打开对象引用跟踪 || -port port-number | 设置jhat HTTP，Server的端口号，默认7000 || -exclude exclude-file | 执行对象查询时需要排除的数据成员 || -baseline exclude-file | 指定一个基准堆转储 || -debug int | 设置debug级别 || -version | 启动后显示版本信息就退出 || -J | 传入启动参数，比如-J -Xmx512m | 6，jstack-打印JVM中线程快照1）概述jstack(JVM Stack Trace): 用于生成虚拟机指定进程当前时刻的线程快照(虚拟机堆栈跟踪)。线程快照就是当前虛拟机内指定进程的每–条线程正在执行的方法堆栈的集合。 生成线程快照的作用:可用于定位线程出现长时间停顿的原因，如线程间死锁、死循环、请求外部资源导致的长时间等待等问题。这些都是导致线程长时间停顿的常见原因。当线程出现停顿时，就可以用jstack显示各个线程调用的堆栈情况。 在thread dump中， 要留意下面几种状态： 死锁，Deadlock (重点关注) 等待资源，Waiting on condition (重点关注) 等待获取监视器，Waiting on monitor entry (重点关注) 阻塞，Blocked (重点关注) 执行中，Runnable 暂停，Suspended 对象等待中，object .wait()或TIMED_ WAITING 停止，Parked 2）基本语法1jstack option pid 如果程序出现等待问题，可以使用该指令去查看问题所在。 ①option参数-F：当正常输出的请求不被响应时，强制输出线程堆栈 -l：除堆栈外，显示关于锁的附加信息 -m：如果调用本地方法的话，可以显示C/C++的堆栈 -h：帮助操作 7，jcmd-多功能命令行1）概述在JDK 1.7以后，新增了一个命令行工具jcmd。 它是一个多功能的工具，可以用来实现前面除了jstat之外所有命令的功能。比如:用它来导出堆、内存使用、查看Java进程、导出线程信息、执行GC、JVM运行时间等。 jcmd拥有jmap的大部分功能， 并且在0racle的官方网站上也推荐使用jcmd命令代jmap命令。 2）语法jcmd -l 列出所有的JVM进程 jcmd 进程号 help 针对指定的进程，列出支持的所有具体命令 jcmd 进程号 具体命令 显示指定进程的指令命令的数据 8，jstatd-远程主机信息收集之前的指令只涉及到监控本机的Java应用程序，而在这些工具中，一些监控工具也支持对远 程计算机的监控(如jps、 jstat)。为了启用远程监控，则需要配合使用jstatd 工具。 命令jstatd是-一个RMI服务端程序，它的作用相当于代理服务器，建立本地计算机与远程监 控工具的通信。 jstatd服务器将本机的Java应用程序信息传递到远程计算机。 二十一，JVM监控及诊断工具-GUI篇1，概述使用上一章命令行工具或组合能获取目标Java应用性能相关的基础信息，但它们存在下列局限: 1.无法获取方法级别的分析数据，如方法间的调用关系、各方法的调用次数和调用时间等(这对定位应用性能瓶颈至关重要)。 2.要求用户登录到目标Java应用所在的宿主机上，使用起来不是很方便。 3.分析数据通过终端输出，结果展示不够直观。 为此，JDK提供 了一些内存泄漏的分析工具， 如jconsole, jvisualvm等， 用于辅助开发人员定位问题，但是这些工具很多时候并不足以满足快速定位的需求。所以这里我们介绍的工具相对多一些、丰富一些。 图形化综合诊断工具 ●JDK自带的工具 ●jconsole: JDK自带的可视化监控工具。查看Java应用程序的运行概况、监控堆信息、永久区(或元空间)使用情况、类加载情况等 位置: jdk\\bin\\jconsole. exe ●Visual VM:Visual VM是一个工具，它提供了一个可视界面，用于查看Java虚拟机上运行的基于Java技术的应用程序的详细信息。 位置: jdk\\bin\\jvisualvm. exe JMC:Java Mission Control,内置Java Flight Recorder。 能够以极低的性能开销收集Java虚拟机的性能数据。 ●第三方工具 2，JConsole1）概述从JDK1.5开始，在JDK中自带的java监控和管理控制台。 用于对JVM中内存，线程和类等的监控，是一个基于JMX的监控工具。 2）启动1jconsole 3)三种连接方式①Local使用JConsole连接一个正在本地系统运行的JVM，并且执行程序的和运行JConsole的需要是同一个用户。JConsole使用文件系统的授权通过RMI连接起链接到平台的MBean的服务器上。这种从本地连接的监控能力只有Sun的JDK具有。 ②Remote使用下面的URL通过RMI连接器连接到一个JMX代理，service:jmx:rmi:///jndi/rmi://hostName:portNum/jmxrmi。JConsole为建立连接，需要在环境变量中设置mx.remote.credentials来指定用户名和密码，从而进行授权。 ③Advanced使用一个特殊的URL连接JMX代理。一般情况使用自己定制的连接器而不是RMI提供的连接器来连接JMX代理，或者是一个使用JDK1.4的实现了JMX和JMX Rmote的应用。 4）主要作用①概览 ②内存 ③根据线程检测死锁 ④线程 ⑤VM概要 3，Visual VMjvisualvm和visual vm的区别： visual vm是单独下载的工具，然后将visual vm结合到jdk中就变成了jvisualvm，仅仅是添加了一个j而已，这个j应该是java的用处，所以说jvisualvm其实就是visual vm。 1）基本概述VisualVM是一个功能强大的多合一故障诊断和性能监控的可视化工具。 它集成了多个JDK命令行工具，使用VisualVM可用于显示虚拟机进程及进程的配置和环境变量，监控应用程序的CPU，GC,堆，方法区及线程的信息等，替换JConsole。 2）插件安装1https://visualvm.github.io/pluginscenters.html 3)连接方式①本地连接监控本地Java进程的CPU、类、线程等。 ②远程连接123456781-确定远程服务器的ip地址2-添加JMX（通过JMX技术具体监控远程服务器哪个Java进程）3-修改bin/catalina.sh文件，连接远程的tomcat4-在…/conf中添加jmxremote.access和jmxremote.password文件5-将服务器地址改成公网ip地址6-设置阿里云安全策略和防火墙策略7-启动tomcat，查看tomcat启动日志和端口监听8-JMX中输入端口号、用户名、密码登录 4）主要功能1.生成/读取堆内存快照 生成堆内存快照 方式一：在类上单击右键，点击Dump 方式二：在监视页面点击堆按钮 装入堆内存快照 2.查看JVM参数和系统属性 3.查看运行中的虚拟机进程 4.生成/读取线程快照 生成线程快照 方式一：在类上单击右键，点击线程Dump。 方式二：同上 装入线程快照 同装入堆快照 5.程序资源的实时监控 6.其他功能 JMX代理连接 远程环境监控 CPU分析和内存分析 4，Eclipse MAT1）概述功能强大的java堆内存分析器，可以用于查找堆内存泄漏和内存消耗情况。MAT是基于eclipse开发的，可以单独使用或者以插件形式嵌入到eclipse。 2）获取堆dump文件①dump文件内存MAT可以分析heap dump文件。在进行内存分析时，只要获得了反映当前设备内存映像的hprof文件，通过MAT打开就可以直观地看到当前的内存信息。 一般说来，这些内存信息包含: ●所有的对象信息，包括对象实例、成员变量、存储于栈中的基本类型值和存储于堆中的其他对象的引用值。 ● 所有的类信息，包括classloader、 类名称、父类、静态变量等 GCRoot到所有的这些对象的引用路径 线程信息，包括线程的调用栈及此线程的线程局部变量(TLS) 2）说明说明1:缺点: MAT 不是一个万能工具，它并不能处理所有类型的堆存储文件。但是比较主流的厂家和格式，例如 Sun, HP, SAP所采用的HPROF 二进制堆存储文件，以及IBM的PHD堆存储文件等都能被很好的解析。 说明2： 最吸引人的还是能够快速为开发人员生成内存泄漏报表，方便定位问题和分析问题。虽然MAT有如此强大的功能，但是内存分析也没有简单到-键完成的程度，很多内存问题还是需要我们从MAT展现给我们的信息当中通过经验和直觉来判断才能发现。 3）获取dump文件方法一:通过前一章介绍的jmap工具生成，可以生成任意-一个java进程的dump文件; 方法二:通过配置JVM参数生成。 ● 选项”-XX: +HeapDumpOnOutOfMemoryError”或”-XX: +HeapDumpBeforeFullGC” ● 选项”-XX:HeapDumpPath”所代表的含义就是当程序出现0utofMemory时，将会在相应的目录下 生成一份dump文件。如果不指定选项“-XX:HeapDumpPath” 则在当前目录下生成dump文件。 对比:考虑到生产环境中几乎不可能在线对其进行分析，大都是采用离线分析，因此使用jmap+MAT工具是最常见的组合。 方法三:使用VisualVM可以导出堆dump文件 方法四: 使用MAT既可以打开一个已有的堆快照，也可以通过MAT直接从活动Java程序中导出堆快照。 该功能将借助jps列出当前正在运行的Java 进程，以供选择并获取快照。 4）分析堆dump文件①histogram展示了各个类的实例数目以及这些实例的Shallow heap或者Retained heap的总和。 具体内容： ②thread overview查看系统中的Java线程 查看局部变量的信息 ③获得对象互相引用的关系with outgoing references with incoming references ④浅堆与深堆shallow heap 对象头代表根据类创建的对象的对象头，还有对象的大小不是可能向8字节对齐，而是就向8字节对齐。 浅堆(Shallow Heap)是指一个对象所消耗的内存。在32位系统中，一个对象引用会占据4个字节，一个int类型会占据4个字节，long型变量会占据8个字节，每个对象头需要占用8个字节。根据堆快照格式不同，对象的大小可能会成8字节进行对齐。 以String为例: 2 个int值共占8字节，对象引用占用4字节，对象头8字节，合计20字节，向8字节对齐，故占24字节。(jdk7中) int hash32 0 int hash 0 ref value C:\\Users\\ASUS 这24字节为String对象的浅堆大小。它与String的value实际取值无关，无论字符串长度如何，浅堆大小始终是24字节。 retained heap 保留集(Retained Set): 对象A的保留集指当对象A被垃圾回收后，可以被释放的所有的对象集合(包括对象A本身)，即对象A的保留集可以被认为是只能通过对象A被直接或间接访问到的所有对象的集合。通俗地说，就是指仅被对象A所持有的对象的集合。 深堆(Retained Heap): 深堆是指对象的保留集中所有的对象的浅堆大小之和。 注意:浅堆指对象本身占用的内存，不包括其内部引用对象的大小。一个对象的深堆指只能通过该对象访问到的(直接或间接)所有对象的浅堆之和，即对象被回收后，可以释放的真实空间。 对象实际大小 另外一个常用的概念是对象的实际大小。这里，对象的实际大小定义为一个对象所能触及的所有对象的浅堆大小之和，也就是通常意义上我们说的对象大小。与深堆相比，似乎这个在日常开发中更为直观和 被人接受， 但实际上，这个概念和垃圾回收无关。 下图显示了一个简单的对象引用关系图，对象A引用了C和D，对象B引用了C和E。那么对象A的浅堆大 小只是A本身， 不含C和D,而A的实际大小为A、 C、D三者之 和。而A的深堆大小为A与D之和， 由于对象C还可以通过对象B访问到，因此不在对象A的深堆范围内。 练习 A对象的Retained Size=A对象的Shallow Size B对象的Retained Size=B对象的Shallow Size+C对象的Shallow Size 这里不包括D对象，因为D对象被GC Roots直接引用 如果GC Roots不引用D对象呢？ 案例分析：StudentTrace 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103/** * 有一个学生浏览网页的记录程序，它将记录 每个学生访问过的网站地址。 * 它由三个部分组成：Student、WebPage和StudentTrace三个类 * * -XX:+HeapDumpBeforeFullGC -XX:HeapDumpPath=c:\\code\\student.hprof * @author shkstart * @create 16:11 */public class StudentTrace &#123; static List&lt;WebPage&gt; webpages = new ArrayList&lt;WebPage&gt;(); public static void createWebPages() &#123; for (int i = 0; i &lt; 100; i++) &#123; WebPage wp = new WebPage(); wp.setUrl(&quot;http://www.&quot; + Integer.toString(i) + &quot;.com&quot;); wp.setContent(Integer.toString(i)); webpages.add(wp); &#125; &#125; public static void main(String[] args) &#123; createWebPages();//创建了100个网页 //创建3个学生对象 Student st3 = new Student(3, &quot;Tom&quot;); Student st5 = new Student(5, &quot;Jerry&quot;); Student st7 = new Student(7, &quot;Lily&quot;); for (int i = 0; i &lt; webpages.size(); i++) &#123; if (i % st3.getId() == 0) st3.visit(webpages.get(i)); if (i % st5.getId() == 0) st5.visit(webpages.get(i)); if (i % st7.getId() == 0) st7.visit(webpages.get(i)); &#125; webpages.clear(); System.gc(); &#125;&#125;class Student &#123; private int id; private String name; private List&lt;WebPage&gt; history = new ArrayList&lt;&gt;(); public Student(int id, String name) &#123; super(); this.id = id; this.name = name; &#125; public int getId() &#123; return id; &#125; public void setId(int id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public List&lt;WebPage&gt; getHistory() &#123; return history; &#125; public void setHistory(List&lt;WebPage&gt; history) &#123; this.history = history; &#125; public void visit(WebPage wp) &#123; if (wp != null) &#123; history.add(wp); &#125; &#125;&#125;class WebPage &#123; private String url; private String content; public String getUrl() &#123; return url; &#125; public void setUrl(String url) &#123; this.url = url; &#125; public String getContent() &#123; return content; &#125; public void setContent(String content) &#123; this.content = content; &#125;&#125; 结论： elementData数组的浅堆是80个字节，而elementData数组中的所有WebPage对象的深堆之和是1208个字节，所以加在一起就是elementData数组的深堆之和，也就是1288个字节。 解释： 我说“elementData数组的浅堆是80个字节”，其中15个对象一共是60个字节，对象头8个字节，数组对象本身4个字节，这些的和是72个字节，然后总和要是8的倍数，所以“elementData数组的浅堆是80个字节” 我说“WebPage对象的深堆之和是1208个字节”，一共有15个对象，其中0、21、42、63、84、35、70不仅仅是7的倍数，还是3或者5的倍数，所以这几个数值对应的i不能计算在深堆之内，这15个对象中大多数的深堆是152个字节，但是i是0和7的那两个深堆是144个字节，所以(13_152+144_2)-(6*152+144)=1208，所以这也印证了我上面的话，即“WebPage对象的深堆之和是1208个字节”。 因此“elementData数组的浅堆80个字节”加上“WebPage对象的深堆之和1208个字节”，正好是1288个字节，说明“elementData数组的浅堆1288个字节”。 ⑤支配树支配树( Dominator Tree)支配树的概念源自图论。 MAT提供了一个称为支配树(Dominator Tree)的对象图。支配树体现了对象实例间的支配关系。在对象引用图中，所有指向对象B的路径都经过对象A，则认为对象A支配对象B。如果对象A是离对象B最近的一个支配对象，则认为对象A为对象B的直接支配者。支配树是基于对象间的引用图所建立的，它有以下基本性质: ●对象A的子树(所有被对象A支配的对象集合)表示对象A的保留集(retained set) ，即深堆。 ●如果对象A支配对象B，那么对象A的直接支配者也支配对象B。 ●支配树的边与对象引用图的边不直接对应。 如下图所示:左图表示对象引用图，右图表示左图所对应的支配树。对象A和B由根对象直接支配，由于在到对象C的路径中，可以经过A，也可以经过B，因此对象C的直接支配者也是根对象。对象F与对象D相互引用，因为到对象F的所有路径必然经过对象D，因此，对象D是对象F的直接支配者。而到对象D的所有路径中，必然经过对象C，即使是从对象F到对象D的引用，从根节点出发，也是经过对象C的，所以，对象D的直接支配者为对象C。 同理，对象E支配对象G。到达对象H的可以通过对象D，也可以通过对象E，因此对象D和E都不能支配对象H，而经过对象C既可以到达对象D也可以到达对象E，因此对象C为对象H的直接分配者。 注意： 跟随我一起来理解如何从“对象引用图—》支配树”，首先需要理解支配者（如果要到达对象B，毕竟经过对象A，那么对象A就是对象B的支配者，可以想到支配者大于等于1），然后需要理解直接支配者（在支配者中距离对象B最近的对象A就是对象B的直接支配者，你要明白直接支配者不一定就是对象B的上一级，然后直接支配者只有一个），然后还需要理解支配树是怎么画的，其实支配树中的对象与对象之间的关系就是直接支配关系，也就是上一级是下一级的直接支配者，只要按照这样的方式来作图，肯定能从“对象引用图—》支配树”。 在Eclipse MAT工具中如何查看支配树： 5）案例：Tomcat堆溢出分析①说明Tomcat是最常用的Java Servlet容器之一，同时也可以当做单独的Web服务器使用。Tomcat本身使 用Java实现，并运行于Java虚拟机之上。在大规模请求时，Tomcat有 可能会因为无法承受压力而发生内存溢出错误。这里根据一个被压垮的Tomcat的堆快照文件，来分析Tomcat在崩溃时的内部情况。 ②分析过程 6）支持使用OQL语言查询对象信息SELECT子句 FROM子句 WHERE子句 内置对象与方法 5，JProfiler1）基本概述①介绍在运行Java的时候有时候想测试运行时占用内存情况，这时候就需要使用测试工具查看了。在 eclipse里面有Eclipse Memory Analyzer tool (MAT )插件可以测试，而在IDEA中也有这么一- 个插件，就是JProfiler。 JProfiler是由ej-technologies 公司开发的一款Java应用性能诊断工具。功能强大，但是收费。 ②特点使用方便、界面操作友好(简单 且强大) 对被分析的应用影响小( 提供模板) CPU, Thread , Memory分析功能尤其强大 支持对jdbc,noSq1, jsp, servlet, socket 等进行分析支持多种模式(离线，在线)的分析 支持监控本地、远程的JVM 跨平台,拥有多种操作系统的安装版本 ③主要功能方法调用 对方法调用的分析可以帮助您了解应用程序正在做什么，并找到提高其性能的方法 内存分配 通过分析堆上对象、引用链和垃圾收集能帮您修复内存泄露问题，优化内存使用 线程和锁 JProfiler提供多种针对线程和锁的分析视图助您发现多线程问题 高级子系统 许多性能问题都发生在更高的语义级别上。例如，对于JDBC调用，您可能希望找出执行最慢的SQL语句。JProfiler支持对这些子系统进行集成分析 2）安装与配置①JProfiler中配置IDEAIDE Integrations 选择合适的IDE版本 开始集成 正式集成 ②IDEA集成JProfiler安装JProfiler插件 将JProfiler配置到IDEA中 3）具体使用①常见操作Starter Center 如果程序已经保存了Quick Attach，那么即使下次程序运行的时候没有启动JProfiler，而我们启动JProfiler，然后找到该Quick Attach中的对应位置，点击就可以运行了。 垃圾回收 标记 手动刷新 Live memory中的Recorded Objects可以查看对象信息，根据View中的Change Liveness Mode来更改查看的对象类型。 如果通过Telemetries中的Memory看到垃圾回收之后内存占用还是越来越多，那就需要注意内存泄露问题了，这个时候我们可以查看Live memory中的Recorded Objects中的对象信息，可以先查看Live Objects中的，也就是存活的对象，然后在查看Garbage Collected Objects，如果某对象只在Live Objects中出现，但是没有在Garbage Collected Objects中出现，那么说明该对象就没有进行垃圾回收，即该对象有可能造成内存泄露。 保存堆快照dump文件 ②数据采集方式JProfier数据采集方式分为两种: Sampling(样本采 集)和Instrumentation (重构模式) ●Instrumentation: 这是JProfiler全功能模式。 在class加载之前，JProfier把相关 功能代码写入到需要分析的class的bytecode中，对正在运行的jvm有-一定影响。 ●●优点:功能强大。在此设置中，调用堆栈信息是准确的。 ●●缺点:若要分析的class较多，则对应用的性能影响较大，CPU开销可能很高(取决于Filter的控制)。因此使用此模式一般配合Filter使用，只对特定的类或包进行分析 ●Sampling: 类似于样本统计，每隔- - 定时间(5ms )将每个线程栈中方法栈中的信息统计出来。 ●●优点:对CPU的开销非常低，对应用影响小(即使你不配置任何Filter) ●●缺点:一些数据/特性不能提供(例如:方法的调用次数、执行时间) 注: JProfiler本 身没有指出数据的采集类型，这里的采集类型是针对方法调用的采集类型。因为JProfiler的绝大多数核心功能都依赖方法调用采集的数据，所以可以直接认为是JProfiler的数据采集类型。 推荐使用Sampling方式，足够用来分析OOM问题了。 ③遥感监测 Telemetries 其中Telemetries就是遥感监测的意思。 ④内存视图 Live MemoryLive memory 内存剖析: class/class instance的相关信息。例如对 象的个数，大小，对象创建的方法执行栈，对象创建的热点。 所有对象All Objects 显示所有加载的类的列表和在堆上分配的实例数。只有Java 1.5 (JVMTI)才会显示此视图。 记录对象Record 0bjects 查看特定时间段对象的分配，并记录分配的调用堆栈。 分配访问树Allocation Call Tree 显示一-棵请求树或者方法、类、包或对已选择类有带注释的分配信息的J2EE组件。 分配热点Allocation Hot Spots 显示一个列表，包括方法、类、包或分配已选类的J2EE组件。你可以标注当前值并且显示差异值。对于每个热点都可以显示它的跟踪记录树。 类追踪器Class Tracker 类跟踪视图可以包含任意数量的图表，T显示选定的类和包的实例与时间。 分析:内存中的对象的情况 频繁创建的Java对象:死循环、循环次数过多 存在大的对象:读取文件时，byte[]应该边读边写。–&gt;如果长时间不写出的话，导致byte[]过大 存在内存泄漏 注意： All Objects后面的Size大小是浅堆大小 Record Objects在判断内存泄露的时候使用，可以通过观察Telemetries中的Memory，如果里面出现垃圾回收之后的内存占用逐步提高，这就有可能出现内存泄露问题，所以可以使用Record Objects查看，但是该分析默认不开启，毕竟占用CPU性能太多 ⑤堆遍历 heap walker ⑥cpu视图 cpu viewsJProfiler提供不同的方法来记录访问树以优化性能和细节。线程或者线程组以及线程状况可以被所有的视图选择。所有的视图都可以聚集到方法、类、包或J2EE组件等不同层上。 访问树Call Tree 显示一个积累的自顶向下的树，树中包含所有在JVM中已记录的访问队列。JDBC, JMS和JNDI服务请求都被注释在请求树中。请求树可以根据Servlet和JSP对URL的不同需要进行拆分。 热点Hot Spots 显示消耗时间最多的方法的列表。对每个热点都能够显示回溯树。该热点可以按照方法请求，JDBC,JMS和JNDI服务请求以及按照URL请求来进行计算。 访问图Call Graph 显示一个从已选方法、类、包或J2EE组件开始的访问队列的图。 方法统计Method Statistis 显示一段时间内记录的方法的调用时间细节。 具体使用： 1、记录方法统计信息 2、方法统计 3、具体分析 ⑦线程视图 threadsJProfiler通过对线程历史的监控判断其运行状态，并监控是否有线程阻塞产生，还能将一- 个线程所管理的方法以树状形式呈现。对线程剖析。 线程历史Thread History 显示一个与线程活动和线程状态在-起的活动时间表 。 线程监控Thread Monitor 显示一个列表，包括所有的活动线程以及它们目前的活动状况。 线程转储Thread Dumps 显示所有线程的堆栈跟踪。 线程分析主要关心三个方面: 1-web容器的线程最大数。比如: Tomcat的线 程容量应该略大于最大并发数。 2-线程阻塞 3-线程死锁 具体使用 ⑧监视器&amp;锁 Monitors&amp;locks所有线程持有锁的情况。 观察JVM的内部线程并查看状态： 死锁探测图表Current Locking Graph : 显示JVM中的当前死锁图表。 目前使用的监测器CurrentMonitors:显示目前使用的监测器并且包括它们的关联线程。 锁定历史图表Locking History Graph : 显示记录在JVM中的锁定历史。 历史检测记录Monitor History : 显示重大的等待事件和阻塞事件的历史记录。 监控器使用统计Monitor Usage Statistics : 显示分组监测，线程和监测类的统计监测数据。 4）案例分析①案例1234567891011121314151617181920212223242526/** * 功能演示测试 * @author shkstart * @create 12:19 */public class JProfilerTest &#123; public static void main(String[] args) &#123; while (true)&#123; ArrayList list = new ArrayList(); for (int i = 0; i &lt; 500; i++) &#123; Data data = new Data(); list.add(data); &#125; try &#123; TimeUnit.MILLISECONDS.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125;class Data&#123; private int size = 10; private byte[] buffer = new byte[1024 * 1024];//1mb private String info = &quot;hello,atguigu&quot;;&#125; ②案例12345678910111213141516171819202122232425public class MemoryLeak &#123; public static void main(String[] args) &#123; while (true) &#123; ArrayList beanList = new ArrayList(); for (int i = 0; i &lt; 500; i++) &#123; Bean data = new Bean(); data.list.add(new byte[1024 * 10]);//10kb beanList.add(data); &#125; try &#123; TimeUnit.MILLISECONDS.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125;class Bean &#123; int size = 10; String info = &quot;hello,atguigu&quot;; static ArrayList list = new ArrayList();&#125; 通过JProfiler来看一下，如下： 你可以看到内存一个劲的往上涨，但是就是没有下降的趋势，说明这肯定有问题，过不了多久就会出现OOM，我们来到Live memory中，先标记看一下到底是哪些对象在进行内存增长，等一小下看看会不会触发垃圾回收，如果不触发的话，我们自己来触发垃圾回收，之后观察哪些对象没有被回收掉，如下： 我上面点击了Mark Current，发现有些对象在持续增长，然后点击了一下Run GC，结果如下所示： 可以看出byte[]没有被回收，说明它是有问题的，点击Show Selection In Heap Walker，如下： 然后看一下该对象被谁引用，如下： 结果如下： 可以看出byte[]来自于Bean类是的list中，并且这个list是ArrayList类型的静态集合，所以找到了：static ArrayList list = new ArrayList(); 发现list是静态的，这不妥，因为我们的目的是while结束之后Bean对象被回收，并且Bena对象中的所有字段都被回收，但是list是静态的，那就是类的，众所周知，类变量随类而生，随类而灭，因此每次我们往list中添加值，都是往同一个list中添加值，这会造成list不断增大，并且不能回收，所以最终会导致OOM。 6，Arthas 1）基本概述①背景不需要远程连接，也不需要配制监控参数，同时也提供了丰富的性能监控数据 ②概述Arthas (阿尔萨斯) 是Alibaba开源的Java诊断工具，深受开发者喜爱。在线排查问题，无需重启 ;动态跟踪Java代码; 实时监控JVM状态。 Arthas 支持JDK 6+， 支持Linux/Mac/Windows, 采用命令行交互模式，同时提供丰富的Tab 自动补全功能，进–步方便进行问题的定位和诊断。 当你遇到以下类似问题而束手无策时，Arthas 可以帮助你解决: ●这个类从哪个jar包加载的?为什么会报各种类相关的Exception? ●我改的代码为什么没有执行到?难道是我没commit?分支搞错了? ●遇到问题无法在线上debug, 难道只能通过加日志再重新发布吗? ●线上遇到某个用户的数据处理有问题，但线上同样无法debug,线下无法重现! ●是否有一个全局视角来查看系统的运行状况? ●有什么办法可以监控到JVM的实时运行状态? ●怎么快速定位应用的执占，生成火焰图2 ③基于哪些工具开发而来greys- anatomy: Arthas代码基 于Greys =二次开发而来，非常感谢Greys之前所有的工作，以及Greys原作者对Arthas提出的意见和建议! termd: Arthas 的命令行实现基于termd开发，是一款优秀的命令行程序开发框架，感谢termd提供了优秀的框架。 crash: Arthas的文本渲染功能基于crash中的文本渲染功能开发，可以从这里看到源码，感谢crash在这方面所做的优秀工作。 cli: Arthas的命令行界面基于vert. x提供的cli库进行开发，感谢vert . x在这方面做的优秀工作。 compiler Arthas 里的内存编绎器代码来源 Apache Commons Net Arthas 里的Telnet Client 代码来源 JavaAgent:运行在main方法之前的拦截器，它内定的方法名叫premain ，也就是说先执行premain方法然后再执行main方法 ASM:一个通用的Java字节码操作和分析框架。它可以用于修改现有的类或直接以二进制形式动 态生成类。ASM提供了一些常见的字节码转换和分析算法，可以从它们构建定制的复杂转换和代码分析工具。ASM提供了与其他Java字节码框架类似的功能，但是主要关注性能。因为它被设计和实现得尽可能小和快，所以非常适合在动态系统中使用(当然也可以以静态方式使用，例如在编译器中) ④官方文档1https://arthas.aliyun.com/doc/quick-start.html 2）安装与使用①安装1wget https://alibaba.github.io/arthas/arthas-boot.jar ②工程目录arthas-agent:基于JavaAgent技术的代理 bin:–些启动脚本 arthas- boot: Java版 本的一键安装启动脚本 arthas-client: telnet client代码 arthas - common:一些共用的工具类和枚举类 arthas-core:核心库，各种arthas命令的交互和实现 arthas-demo:示例代码 arthas -memorycompiler:内存编绎器代码，Fork from https://github.com/skalogs/SkaETL/tree/master/compiler arthas-packaging: maven打 包相关的 arthas-site: arthas站点 arthas-spy:编织到目标类中的各个切面 static:静态资源 arthas-testcase:测试 ③启动1java -jar arthas-boot.jar ④查看进程1jps ⑤查看日志1cat ~/logs/arthas/arthas.log ⑥查看帮助1java -jar arthas-boot.jar -h ⑦web console除了在命令行查看外，Arthas 目前还支持Web Console。 在成功启动连接进程之后就已经自动启 动，可以直接访问http://127.0.0.1:8563/ 访问，页面上的操作模式和控制台完全一样。 ⑧退出最后一行[arthas@7457]$， 说明打开进入了监控客户端，在这里就可以执行相关命令进行查看了。 使用quit\\exit:退出当前客户端 使用stop\\shutdown:关闭arthas服 务端,并退出所有客户端。 3）相关诊断指令①基础指令●help- -查看命令帮助信息 ●cat- - -打印文件内容,和linux里的cat命令类似 ●echo-打印参数，和linux里的echo命令类似 ●grep– -匹配查找，和linux里的grep命令类似 ●tee– -复制标准输入到标准输出和指定的文件，和linux里的tee命令类似 pwd- - -返回当前的工作目录，和linux命令类似 ●cls– -清空当前屏幕区域 ●session- - 查看当前会话的信息 ●reset– 重置增强类，将被Arthas增强过的类全部还原，Arthas服务端关闭时会重置所有增强过的类 ●version– 输出当前目标Java进程所加载的Arthas版本号 ●history- – 打印命令历史 ●quit- -退出当前Arthas客户端，其他Arthas客户端不受影响 ●stop–关闭Arthas服务端，所有Arthas客户端全部退出 ●keymap- - - Arthas快捷键列表及自定义快捷键 ②jvm相关●dashboard- -当前系统的实时数据面板 ●thread- - _查看当前JVM的线程堆栈信息 ●jvm–查看当前JVM的信息 ●sysprop–查看和修改JVM的系统属性 ●sysenv- –查看JVM的环境变量 ●vmoption- – -查看和修改JVM里诊断相关的option ●perfcounter- – 查看当前JVM的Perf Counter信息 ●logger–查看和修改logger ●getstatic– 查看类的静态属性 ●ognl- – 执行ogn装达式 ●mbean– _查看Mbean的信息 ●heapdump- – -dump java heap,类似jmap命令的heap dump功能 ③class/classloader相关●SC–查看JM已加载的类信息 ●sm–查看已加载类的方法信息 ●jad–反编译指定已加载类的源码 ●mc- -内存编译器,内存编译. java文件为.class文件 ●retransform- - - 加载外部的.class文件, retransform到JVM里， ● redefine– 加载外部的.class文件, redefine到JVM里 ●dump– -dump已加载类的byte code到特定目录 ●classloader– 查看classloader的继承树， urls, 类加载信息,使用classloader去getResource ④monitor/watch/trace相关monitor–方法抉行監控 watch- – -方法丸行数跼規測 trace– -方法内部凋用路径，并輸出方法路径上的毎个芍点上耗肘 stack- – -輸出当前方法被凋用的凋用路径 tt–方法丸行数据的吋空隧道，記彖下指定方法毎次調用的入参和返回信息，并能対文些不同的吋同下凋用迸行規測 ⑤其他使用&gt;将结果重写到日志文件，使用&amp;指令命令是后台运行，session断开不影响任务执行(生命周期默认为1天) jobs:列出所有job kill:强制终止任务 fg:将暂停的任务拉到前台执行 bg:将暂停的任务放到后台执行 grep:搜索满足条件的结果 plaintext :将命令的结果去除ANSI颜色 WC :;按行统计输出结果 options:查看或设置Arthas全局开关 profiler :使用async-profiler对应用采样，生成火焰图 7，Java Misssion Control1）历史在Oracle收购Sun之前，Oracle 的JRockit 虚拟机提供了一款叫做JRockit Mission Control的虚拟机诊断工具。 在0racle收购Sun之后，Oracle公 司同时拥有了Sun Hotspot 和JRockit两款虚拟机。根据 Oracle对于Java的战略，在今后的发展中，会将JRockit的优 秀特性移植到Hotspot上。其中，一个重要的改进就是在Sun的JDK中加入了JRockit的支持。 在Oracle JDK 7u40之后，Mission Control这款 工具已经绑定在Oracle JDK中发布。 自Java 11开始，本节介绍的JFR已经开源。但在之前的Java版本，JFR属于Commercial Feature, 可要通过Java 虚拟机参数-XX: +UnlockCommercialFeatures开启。 2）启动1jmc 3）概述Java Mission Control (简称JMC) ,Java官 方提供的性能强劲的工具I是一一个用于对Java应用程序进行管理、监视、概要分析和故障排除的工具套件。 它包含一个GUI 客户端，以及众多用来收集Java 虚拟机性能数据的插件，如JMX Console(能够访问用来存放虚拟机各个子系统运行数据的MXBeans)，以及虚拟机内置的高效 profiling工具Java Flight Recorder (JFR) 。 JMC的另一个优点就是:采用取样，而不是传统的代码植入技术，对应用性能的影响非常非常小 完全可以开着JMC 来做压测(唯- - 影响可能是full gc多了)。 4）功能：实时监控JVM运行时的状态如果是远程服务器，使用前要开JMX。 -Dcom. sun. management . jmxremote . port=$[YOUR PORT] -Dcom. sun. management. jmxremote -Dcom. sun. management. jmxremote . authenticate=false -Dcom. sun. management . jmxremote.ssl=false -Djava. rmi. server . hostname=$(YOUR HOST/IP] 文件-&gt;连接-&gt;创建新连接，填入上面JMX参数的host和port。 5）Java Flight RecorderJava Flight Recorder 是JMC 的其中一个组件。 Java Flight Recorder能够以极低的性能开销收集Java虚拟机的性能数据。 JFR的性能开销很小，在默认配置下平均低于1%。 与其他工具相比，JFR能够直接访问虚拟机内的数据，并且不会影响虚拟机的优化。因此，它非常适用于生产环境下满负荷运行的Java程序。 Java Flight Recorder和JDK Mission Contro1共同创建了一个完整的工具链。JDK Mission Control可对Java Flight Recorder连续收集低水平和详细的运行时信息进行高效详细的分析。 ①事件类型当启用时，JFR 将记录运行过程中发生的一系列事件。其中包括Java 层面的事件，如线程事件、锁事件，以及Java虚拟机内部的事件，如新建对象、垃圾回收和即时编译事件。 按照发生时机以及持续时间来划分，JFR 的事件共有四种类型，它们分别为以下四种。 1.瞬时事件(Instant Event)，用户关心的是它们发生与否，例如异常、线程启动事件。 2.持续事件(Duration Event) ，用户关心的是它们的持续时间，例如垃圾回收事件。 3.计时事件(Timed Event)，是时长超出指定阀值的持续事件。 4.取样事件(Sample Event)，是周期性取样的事件。 取样事件的其中一个常见例子便是方法抽样(Method Sampling) ，即每隔段时 间统计 各个线 程的栈轨迹。如果在这些抽样取得的栈轨迹中存在一个反复出现的方法，那么我们可以推测该方法是热点方法。 ②启动方式方式1：使用-XX:StartFlightRecording=参数 方式2：使用jcmd的JFR.*子命令 方式3：JMC的JFR插件 ③Java Flight Recorder 取样分析要采用取样，必须先添加参数: 1-XX: +UnlockCommercialFeatures , -XX: +FlightRecorder 取样时间默认1分钟，可自行按需调整，事件设置选为profiling, 然后可以设置取样profile 哪些信息 然后就开始Profile，到时间后Profile结束，会自动把记录下载下来，在JMC中展示。 从展示信息，大致可以读到内存和CPU信息，代码，线程和IO等比较重要的信息展示。 代码 12345678910111213141516171819202122232425262728293031323334/** * -Xms600m -Xmx600m -XX:SurvivorRatio=8 * @author shkstart shkstart@126.com * @create 2020 21:12 */public class OOMTest &#123; public static void main(String[] args) &#123; ArrayList&lt;Picture&gt; list = new ArrayList&lt;&gt;(); while(true)&#123; try &#123; Thread.sleep(5); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; list.add(new Picture(new Random().nextInt(100 * 50))); &#125; &#125;&#125;class Picture&#123; private byte[] pixels; public Picture(int length) &#123; this.pixels = new byte[length]; &#125; public byte[] getPixels() &#123; return pixels; &#125; public void setPixels(byte[] pixels) &#123; this.pixels = pixels; &#125;&#125; 结果 一般信息 内存 代码 线程 I/O 系统 事件 8，其他工具1）Flame Graphs（火焰图）在追求极致性能的场景下，了解你的程序运行过程中cpu在干什么很重要，非常直观的显示出调用栈中的CPU消耗瓶颈。 火焰图，简单通过x轴横条宽度来度量时间指标，y轴代表线程栈的层次。 2）Tprofiler案例: 使用JDK自身提供的工具进行JVM调优可以将TPS由2.5提升到20 (提升了7倍)，并准确定位系统瓶颈。 系统瓶颈有:应用里静态对象不是太多、有大量的业务线程在频繁创建一些生命周期很长的临时对象，代码里有问题。 那么，如何在海量业务代码里边准确定位这些性能代码?这里使用阿里开源工具TProfiler 来定位 这些性能代码，成功解决掉了GC过于频繁的性能瓶颈，并最终在上次优化的基础上将TPS 再提升了4倍，即提升到100。 TProfiler 配置部署、远程操作、日志阅读都不太复杂，操作还是很简单的。但是其却是能够起到一针见血、立竿见影的效果，帮我们解决了GC过于频繁的性能瓶颈。 TProfiler最重要的特性就是能够统计出你指定时间段内JVM的topmethod,这些top method极有可能就是造成你JVM 性能瓶颈的元凶。这是其他大多数JVM调优工具所不具备的 包括JRockit Mission Control。 JRokit首席开发者Marcus Hirt 在其私人博客《 Low Overhead Method Profiling with Java Mission Control》 下的评论中曾明确指出JRMC并不支持TOP方法的统计。 3）BtraceJava运行时追踪工具 常见的动态追踪工具有BTrace、HouseMD (该项目已经停止开发)、Greys -Anatomy (国人开发， 个人开发者)、Byteman (JBoss出品)，注意Java运行时追踪工具并不限于这几种，但是这几个是相对比较常用的。 BTrace是SUN Kenai云计算开发平台下的一一个开源项目，旨在为java提供安全可靠的动态跟踪分析工具。先看一下BTrace的官方定义: BTrace is a safe, dynamic tracing tool for the Java platform. BTrace can be used to dynamically trace a running Java program (similar to DTrace for OpenSolaris applications and 0S). BTrace dynamically instruments the classes of the target application to inject tracing code (“bytecode tracing”)。 简洁明了，大意是一个Java平台的安全的动态追踪工具。可以用来动态地追踪一个运 行的Java程序。BTrace动态调整目标应用程序的类以注入跟踪代码(“字节码跟踪”) 4）YourKit5）JProbe6）Spring Insight9，再谈内存泄漏1）内存泄露的理解与分析可达性分析算法来判断对象是否是不再使用的对象，本质都是判断一个对象是否还被引用。那么对于这种情况下，由于代码的实现不同就会出现很多种内存泄漏问题(让JVM误以为此对象还在引用中，无法回收，造成内存泄漏)。 是否还被使用? 是 是否还被需要? 否 内存泄漏(memory leak) 的理解 严格来说，只有对象不会再被程序用到了，但是GC又不能回收他们的情况，才叫内存泄漏。 但实际情况很多时候一些不太好的实践(或疏忽)会导致对象的生命周期变得很长甚至导致0OM，也可以叫做宽泛意义上的“内存泄漏”。 对象X引用对象Y,X的生命周期比Y的生命周期长; 那么当Y生命周期结束的时候，X依然引用着Y，这时候，垃圾回收期是不会回收对象Y的; 如果对象X还引用着生命周期比较短的A、B、C,对象A又引用着对象a、b、c，这样就可能造成大量无用的对象不能被回收，进而占据了内存资源，造成内存泄漏，直到内存溢出。 内存泄漏与内存溢出的关系: 1.内存泄漏(memory leak ) 申请了内存用完了不释放，比如一共有1024M的内存，分配了512M 的内存一-直不回收，那么可以 用的内存只有512M 了，仿佛泄露掉了一部分; 通俗一点讲的话，内存泄漏就是[占着茅坑不拉shi]。 2.内存溢出(out of memory ) 申请内存时，没有足够的内存可以使用; 通俗一点儿讲，-一个厕所就三个坑，有两个站着茅坑不走的(内存泄漏)，剩下最后一个坑，厕所表示接待压力很大，这时候一下子来了两个人，坑位(内存)就不够了，内存泄漏变成内存溢出了。 可见，内存泄漏和内存溢出的关系:内存泄漏的增多，最终会导致内存溢出。 泄漏的分类 经常发生:发生内存泄露的代码会被多次执行，每次执行，泄露一块内存; 偶然发生:在某些特定情况下才会发生 一次性:发生内存泄露的方法只会执行一次; 隐式泄漏:一 直占着内存不释放，直到执行结束;严格的说这个不算内存泄漏，因为最终释放掉了，但是如果执行时间特别长，也可能会导致内存耗尽。 2）Java中内存泄露的8种情况1-静态集合类 静态集合类，如HashMap、 LinkedList等等。 如果这些容器为静态的，那么它们的生命周期与JVM程序一致，则容器中的对象在程序结束之前将不能被释放，从而造成内存泄漏。简单而言，长生命周期的对象持有短生命周期对象的引用，尽管短生命周期的对象不再使用，但是因为长生命周期对象持有它的引用而导致不能被回收。 2-单例模式 单例模式，和静态集合导致内存泄露的原因类似，因为单例的静态特性，它的生命周期和JVM的生命周期一样长，所以如果单例对象如果持有外部对象的引用，那么这个外部对象也不会被回收，那么就会造成内存泄漏。 3-内部类持有外部类 内部类持有外部类，如果一个外部类的实例对象的方法返回了一个内部类的实例对象。 这个内部类对象被长期引用了，即使那个外部类实例对象不再被使用，但由于内部类持有外部类的实例对象，这个外部类对象将不会被垃圾回收，这也会造成内存泄漏。 4-各种连接，如数据库连接、网络连接和IO连接等 各种连接，如数据库连接、网络连接和IO连接等。 在对数据库进行操作的过程中，首先需要建立与数据库的连接，当不再使用时，需要调用close方法来释放与数据库的连接。只有连接被关闭后，垃圾回收器才会回收对应的对象。 否则，如果在访问数据库的过程中，对Connection、 Statement或ResultSet不显性地关闭，将会造成大量的对象无法被回收，从而引起内存泄漏。 5-变量不合理的作用域 变量不合理的作用域。一般而言，一个变量的定义的作用范围大于其使用范围，很有可能会造成内存泄漏。另一方面，如果没有及时地把对象设置为null,很有可能导致内存泄漏的发生。 6-改变哈希值 改变哈希值，当一个对象被存储进HashSet集合中以后，就不能修改这个对象中的那些参与计算哈希值的字段了。 否则，对象修改后的哈希值与最初存储进HashSet集合中时的哈希值就不同了，在这种情况下，即使在contains方法使用该对象的当前引用作为的参数去HashSet集合中检索对象，也将返回找不到对象的结果，这也会导致无法从HashSet集合中单独删除当前对象，造成内存泄漏。 这也是String为什么被设置成了不可变类型，我们可以放心地把String 存入HashSet,或者把 String当做HashMap 的key值; 当我们想把自己定义的类保存到散列表的时候，需要保证对象的hashCode不可变。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118public class ChangeHashCode &#123; public static void main(String[] args) &#123; HashSet set = new HashSet(); Person p1 = new Person(1001, &quot;AA&quot;); Person p2 = new Person(1002, &quot;BB&quot;); set.add(p1); set.add(p2); p1.name = &quot;CC&quot;;//导致了内存的泄漏 set.remove(p1); //删除失败 System.out.println(set); set.add(new Person(1001, &quot;CC&quot;)); System.out.println(set); set.add(new Person(1001, &quot;AA&quot;)); System.out.println(set); &#125;&#125;class Person &#123; int id; String name; public Person(int id, String name) &#123; this.id = id; this.name = name; &#125; @Override public boolean equals(Object o) &#123; if (this == o) return true; if (!(o instanceof Person)) return false; Person person = (Person) o; if (id != person.id) return false; return name != null ? name.equals(person.name) : person.name == null; &#125; @Override public int hashCode() &#123; int result = id; result = 31 * result + (name != null ? name.hashCode() : 0); return result; &#125; @Override public String toString() &#123; return &quot;Person&#123;&quot; + &quot;id=&quot; + id + &quot;, name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125;&#125;例2：/** * 演示内存泄漏 * @author shkstart * @create 14:47 */public class ChangeHashCode1 &#123; public static void main(String[] args) &#123; HashSet&lt;Point&gt; hs = new HashSet&lt;Point&gt;(); Point cc = new Point(); cc.setX(10);//hashCode = 41 hs.add(cc); cc.setX(20);//hashCode = 51 此行为导致了内存的泄漏 System.out.println(&quot;hs.remove = &quot; + hs.remove(cc));//false hs.add(cc); System.out.println(&quot;hs.size = &quot; + hs.size());//size = 2 System.out.println(hs); &#125;&#125;class Point &#123; int x; public int getX() &#123; return x; &#125; public void setX(int x) &#123; this.x = x; &#125; @Override public int hashCode() &#123; final int prime = 31; int result = 1; result = prime * result + x; return result; &#125; @Override public boolean equals(Object obj) &#123; if (this == obj) return true; if (obj == null) return false; if (getClass() != obj.getClass()) return false; Point other = (Point) obj; if (x != other.x) return false; return true; &#125; @Override public String toString() &#123; return &quot;Point&#123;&quot; + &quot;x=&quot; + x + &#x27;&#125;&#x27;; &#125;&#125; 7-缓存泄露 内存泄漏的另一个常见来源是缓存，- -旦你把对象引用放入到缓存中，他就很容易遗忘。比如:之前项目在一次上线的时候，应用启动奇慢直到夯死，就是因为代码中会加载-个表中的数据到缓存(内存)中，测试环境只有几百条数据，但是生产环境有几百万的数据。 对于这个问题，可以使用WeakHashMap代表缓存，此种Map的特点是，当除了自身有对key的引用外，此key没有其他引用那么此map会自动丢弃此值。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public class MapTest &#123; static Map wMap = new WeakHashMap(); static Map map = new HashMap(); public static void main(String[] args) &#123; init(); testWeakHashMap(); testHashMap(); &#125; public static void init() &#123; String ref1 = new String(&quot;obejct1&quot;); String ref2 = new String(&quot;obejct2&quot;); String ref3 = new String(&quot;obejct3&quot;); String ref4 = new String(&quot;obejct4&quot;); wMap.put(ref1, &quot;cacheObject1&quot;); wMap.put(ref2, &quot;cacheObject2&quot;); map.put(ref3, &quot;cacheObject3&quot;); map.put(ref4, &quot;cacheObject4&quot;); System.out.println(&quot;String引用ref1，ref2，ref3，ref4 消失&quot;); &#125; public static void testWeakHashMap() &#123; System.out.println(&quot;WeakHashMap GC之前&quot;); for (Object o : wMap.entrySet()) &#123; System.out.println(o); &#125; try &#123; System.gc(); TimeUnit.SECONDS.sleep(5); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;WeakHashMap GC之后&quot;); for (Object o : wMap.entrySet()) &#123; System.out.println(o); &#125; &#125; public static void testHashMap() &#123; System.out.println(&quot;HashMap GC之前&quot;); for (Object o : map.entrySet()) &#123; System.out.println(o); &#125; try &#123; System.gc(); TimeUnit.SECONDS.sleep(5); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;HashMap GC之后&quot;); for (Object o : map.entrySet()) &#123; System.out.println(o); &#125; &#125;&#125; 上面代码和图示主演演示WeakHashMap如何自动释放缓存对象，当init函 数执行完成后，局部变量字 符串引用weakd1 ,weakd2,d1,d2都会消失，此时只有静态map中保存中对字符串对象的引用，可以 看到，调用gc之后，HashMap的没有被回收，而WeakHashMap 里面的缓存被回收了。 8-监听器和回调 内存泄漏第三个常见来源是监听器和其他回调，如果客户端在你实现的API中注册回调，却没有显示的取消，那么就会积聚。 需要确保回调立即被当作垃圾回收的最佳方法是只保存它的弱引用，例如将他们保存成为WeakHashMap中的键。 3）内存泄露案例分析①代码123456789101112131415161718192021222324252627public class Stack &#123; private Object[] elements; private int size = 0; private static final int DEFAULT_INITIAL_CAPACITY = 16; public Stack() &#123; elements = new Object[DEFAULT_INITIAL_CAPACITY]; &#125; public void push(Object e) &#123; //入栈 ensureCapacity(); elements[size++] = e; &#125; public Object pop() &#123; if (size == 0) throw new EmptyStackException(); Object result = elements[--size]; elements[size] = null; return result; &#125; private void ensureCapacity() &#123; if (elements.length == size) elements = Arrays.copyOf(elements, 2 * size + 1); &#125;&#125; ②分析上述程序并没有明显的错误，但是这段程序有一个内存泄漏，随着GC活动的增加，或者内存占用的不断增加，程序性能的降低就会表现出来，严重时可导致内存泄漏，但是这种失败情况相对较少。 代码的主要问题在pop函数，下面通过这张图示展现 假设这个栈- -直增长，增长后如下图所示 当进行大量的POP操作时，由于引用未进行置空，gc是不会释放的，如下图所示： 从上图可以看出，如果栈先增长，在收缩，那么从栈中弹出的对象将不会被当作垃圾回收，即使程序不再使用栈中的这些对象，他们也不会回收，因为栈中仍然保存这对象的引用，这个内存泄漏很隐蔽。 ③解决办法将代码中的pop()方法变成如下方法： 12345public Object pop() &#123; //出栈 if (size == 0) throw new EmptyStackException(); return elements[--size];&#125; 一旦引用过期，清空这些引用，将引用置空。 10，支持使用OQL语言查询对象信息 栗子 123456789select * from java.util.ArrayList（列出所有的ArrayList对象信息）select v.elementData from java.util.ArrayList v（注意：elementData代表ArrayList中的数组，结果最终以数组形式将结果呈现出来）select objects v.elementData from java.util.ArrayList v（注意：elementData代表ArrayList中的数组，objects代表对象类型，所以最终以对象形式将结果呈现出来，同时展示出来的还有浅堆、深堆）select as retained set * from com.atguigu.mat.Student（得到对象的保留级）select * from 0x6cd57c828（0x6cd57c828是Student类的地址值）select * from char[] s where s.@length &gt; 10（char型数组长度大于10的数组）select * from java.lang.String s where s.value != null（字符串值不为空的字符串信息）select toString(f.path.value) from java.io.File f（列出文件的路径值）select v.elementData.@length from java.util.ArrayList v（列出Arraylist对象中ArrayList中的数组长度） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103/** * 有一个学生浏览网页的记录程序，它将记录 每个学生访问过的网站地址。 * 它由三个部分组成：Student、WebPage和StudentTrace三个类 * * -XX:+HeapDumpBeforeFullGC -XX:HeapDumpPath=c:\\code\\student.hprof * @author shkstart * @create 16:11 */public class StudentTrace &#123; static List&lt;WebPage&gt; webpages = new ArrayList&lt;WebPage&gt;(); public static void createWebPages() &#123; for (int i = 0; i &lt; 100; i++) &#123; WebPage wp = new WebPage(); wp.setUrl(&quot;http://www.&quot; + Integer.toString(i) + &quot;.com&quot;); wp.setContent(Integer.toString(i)); webpages.add(wp); &#125; &#125; public static void main(String[] args) &#123; createWebPages();//创建了100个网页 //创建3个学生对象 Student st3 = new Student(3, &quot;Tom&quot;); Student st5 = new Student(5, &quot;Jerry&quot;); Student st7 = new Student(7, &quot;Lily&quot;); for (int i = 0; i &lt; webpages.size(); i++) &#123; if (i % st3.getId() == 0) st3.visit(webpages.get(i)); if (i % st5.getId() == 0) st5.visit(webpages.get(i)); if (i % st7.getId() == 0) st7.visit(webpages.get(i)); &#125; webpages.clear(); System.gc(); &#125;&#125;class Student &#123; private int id; private String name; private List&lt;WebPage&gt; history = new ArrayList&lt;&gt;(); public Student(int id, String name) &#123; super(); this.id = id; this.name = name; &#125; public int getId() &#123; return id; &#125; public void setId(int id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public List&lt;WebPage&gt; getHistory() &#123; return history; &#125; public void setHistory(List&lt;WebPage&gt; history) &#123; this.history = history; &#125; public void visit(WebPage wp) &#123; if (wp != null) &#123; history.add(wp); &#125; &#125;&#125;class WebPage &#123; private String url; private String content; public String getUrl() &#123; return url; &#125; public void setUrl(String url) &#123; this.url = url; &#125; public String getContent() &#123; return content; &#125; public void setContent(String content) &#123; this.content = content; &#125;&#125; 二十二，JVM运行时参数1，JVM参数选项1)类型一：标准参数选项①特点比较稳定，后续版本基本不会变化。 以-开头 ②各种选项直接在DOS窗口中运行java或者java -help可以看到所有的标准选项 1234567891011121314151617181920212223242526272829303132333435363738394041424344-d32 使用 32 位数据模型 (如果可用)-d64 使用 64 位数据模型 (如果可用)-server 选择 &quot;server&quot; VM 默认 VM 是 server. -cp &lt;目录和 zip/jar 文件的类搜索路径&gt;-classpath &lt;目录和 zip/jar 文件的类搜索路径&gt; 用 ; 分隔的目录, JAR 档案 和 ZIP 档案列表, 用于搜索类文件。-D&lt;名称&gt;=&lt;值&gt; 设置系统属性-verbose:[class|gc|jni] 启用详细输出-version 输出产品版本并退出-version:&lt;值&gt; 警告: 此功能已过时, 将在 未来发行版中删除。 需要指定的版本才能运行-showversion 输出产品版本并继续-jre-restrict-search | -no-jre-restrict-search 警告: 此功能已过时, 将在 未来发行版中删除。 在版本搜索中包括/排除用户专用 JRE-? -help 输出此帮助消息-X 输出非标准选项的帮助-ea[:&lt;packagename&gt;...|:&lt;classname&gt;]-enableassertions[:&lt;packagename&gt;...|:&lt;classname&gt;] 按指定的粒度启用断言-da[:&lt;packagename&gt;...|:&lt;classname&gt;]-disableassertions[:&lt;packagename&gt;...|:&lt;classname&gt;] 禁用具有指定粒度的断言-esa | -enablesystemassertions 启用系统断言-dsa | -disablesystemassertions 禁用系统断言-agentlib:&lt;libname&gt;[=&lt;选项&gt;] 加载本机代理库 &lt;libname&gt;, 例如 -agentlib:hprof 另请参阅 -agentlib:jdwp=help 和 -agentlib:hprof=help-agentpath:&lt;pathname&gt;[=&lt;选项&gt;] 按完整路径名加载本机代理库-javaagent:&lt;jarpath&gt;[=&lt;选项&gt;] 加载 Java 编程语言代理, 请参阅 java.lang.instrument-splash:&lt;imagepath&gt; 使用指定的图像显示启动屏幕 2)类型二：-X参数选项①特点非标准化参数 功能还是比较稳定的。但官方说后续版本可能会变更 以-X开头 ②各种选项12345678910111213141516171819202122232425262728293031323334-Xmixed 混合模式执行 (默认)-Xint 仅解释模式执行-Xcomp 仅采用即时编译器模式-Xbootclasspath:&lt;用 ; 分隔的目录和 zip/jar 文件&gt; 设置搜索路径以引导类和资源-Xbootclasspath/a:&lt;用 ; 分隔的目录和 zip/jar 文件&gt; 附加在引导类路径末尾-Xbootclasspath/p:&lt;用 ; 分隔的目录和 zip/jar 文件&gt; 置于引导类路径之前-Xdiag 显示附加诊断消息-Xnoclassgc 禁用类垃圾收集-Xincgc 启用增量垃圾收集-Xloggc:&lt;file&gt; 将 GC 状态记录在文件中 (带时间戳)-Xbatch 禁用后台编译-Xms&lt;size&gt; 设置初始 Java 堆大小-Xmx&lt;size&gt; 设置最大 Java 堆大小-Xss&lt;size&gt; 设置 Java 线程堆栈大小-Xprof 输出 cpu 配置文件数据-Xfuture 启用最严格的检查, 预期将来的默认值-Xrs 减少 Java/VM 对操作系统信号的使用 (请参阅文档)-Xcheck:jni 对 JNI 函数执行其他检查-Xshare:off 不尝试使用共享类数据-Xshare:auto 在可能的情况下使用共享类数据 (默认)-Xshare:on 要求使用共享类数据, 否则将失败。-XshowSettings 显示所有设置并继续-XshowSettings:all 显示所有设置并继续-XshowSettings:vm 显示所有与 vm 相关的设置并继续-XshowSettings:properties 显示所有属性设置并继续-XshowSettings:locale 显示所有与区域设置相关的设置并继续 -X 选项是非标准选项，如有更改，恕不另行通知 ③JVM的JIT编译模式相关的选项-Xint 只使用解释器：所有字节码都被解释执行，这个模式的速度是很慢的。 -Xcomp 只使用编译器：所有字节码第一次使用就被编译成本地代码，然后在执行。 -Xmixed混合模式：这是默认模式，刚开始的时候使用解释器慢慢解释执行，后来让JIT即时编译器根据程序运行的情况，有选择地将某些热点代码提前编译并缓存在本地，在执行的时候效率就非常高了。 默认使用的就是这种模式。 ④特别地-Xmx -Xms -Xss属于XX参数？ 12单位：k/K、m/M、g/G设置：-Xmx、-Xms最好设置成一样的值，避免扩容带来的损耗 -Xms&lt;size&gt; 设置初始Java堆大小，等价于-XX:InitialHeapSize -Xmx&lt;size&gt; 设置最大Java堆大小，等价于-XX:MaxHeapSize -Xss&lt;size&gt; 设置Java线程堆栈大小，等价于-XX:ThreadStackSize 3)类型三：-XX参数选项①特点非标准化参数 使用的最多的参数类型 这类选项属于实验性，不稳定 以-XX开头 ②作用用于开发和调试JVM ③分类Boolean类型格式 -XX:+&lt;option&gt; 表示启用option属性 -XX:-&lt;option&gt;表示禁用option属性 举例 -XX:+UseParallelGC选择垃圾收集器为并行收集器 -XX:+UseG1GC表示启用61收集器 -XX:+UseAdaptiveSizePolicy自动选择年轻代区大小和相应的Survivor区比例 说明：因为有的指令默认是开启的，所以可以使用-关闭 非Boolean类型格式（key-value类型） 子类型1：数值型格式-XX:&lt;option&gt;=&lt;number&gt; 12345number表示数值，number可以带上单位，比如: &#x27;m’、&#x27;W’表示兆，‘k’、&#x27;K’表示Kb，&#x27;g’、&#x27;G’表示g（例如32k跟32768是一样的效果)例如:-XX:NewSize=1024m表示设置新生代初始大小为1024兆-XX:MaxGCPauseMillis=500表示设置Gc停顿时间:50o毫秒-XX:GCTimeRatio=19表示设置吞吐量-XX:NewRatio=2 表示新生代与老年代的比例 子类型2：非数值型格式-XX:&lt;name&gt;=&lt;string&gt; 12例如:-XX:HeapDumpPath=/usr/local/heapdump.hprof用来指定heap转存文件的存储路径。 ④特别的-XX:+PrintFlagsFinal 输出所有参数的名称和默认值 默认不包括Diagnostic和Experimental的参数 可以配合-XX:+UnlockDiagnosticVMOptions和-XX:UnlockExperimentalVMOptions使用 2，添加JVM参数选项1）IDEA 2）运行jar包1java -Xms50m -Xmx50m -XX:+PrintGCDetails -XX:+PrintGYTimeStamps -jar demo.jar 3）通过Tomcat运行war包Linux系统下可以在tomcat/bin/catalina.sh中添加类似如下配置：JAVA_OPTS=&quot;-Xms512M -Xmx1024M&quot; Windows系统下载catalina.bat中添加类似如下配置：set &quot;JAVA_OPTS=-Xms512M -Xmx1024M&quot; 4）程序运行过程中使用jinfo -flag &lt;name&gt;=&lt;value&gt; &lt;pid&gt;设置非Boolean类型参数 使用jinfo -flag [+|-]&lt;name&gt; &lt;pid&gt;设置Boolean类型参数 3，常用的JVM参数选项1）打印设置的XX选项及值 参数 说明 -XX:+PrintCommandLineFlags 可以让程序运行前打印出用户手动设置或者JVM自动设置的XX选项 -XX:+PrintFlagsInitial 表示打印出所有XX选项的默认值 -XX:+PrintFlagsFinal 表示打印出XX选项在运行程序时生效的值 -XX:+PrintVMOptions 打印JVM的参数 如果值的前面加上了:=，说明该值不是初始值，该值可能被jvm自动改变了，也可能被我们设置的参数改变了。 2）堆、栈、方法区等内存大小设置①栈1-Xss128k 等价于-XX:ThreadStackSize，设置每个线程的栈大小为128k ②堆 参数 说明 -Xms3550m 等价于-XX:InitialHeapSize，设置JVM初始堆内存为3500M -Xmx3550m 等价于-XX:MaxHeapSize，设置JVM最大堆内存为3500M -Xmn2g 设置年轻代大小为2G，即等价于-XX:NewSize=2g -XX:MaxNewSize=2g，也就是设置年轻代初始值和年轻代最大值都是2G。官方推荐配置为整个堆大小的3/8 -XX:NewSize=1024m 设置年轻代初始值为1024M -XX:MaxNewSize=1024m 设置年轻代最大值为1024M -XX:SurvivorRatio=8 设置年轻代中Eden区与一个Survivor区的比值，默认为8。只有显示使用Eden区和Survivor区的比例，才会让比例生效，否则比例都会自动设置，至于其中的原因，请看下面的-XX:+UseAdaptiveSizePolicy中的解释，最后推荐使用默认打开的-XX:+UseAdaptiveSizePolicy设置，并且不显示设置-XX:SurvivorRatio -XX:+UseAdaptiveSizePolicy 自动选择各区大小比例，默认开启 -XX:NewRatio=2 设置老年代与年轻代（包括1个Eden区和2个Survivor区）的比值，默认为2。根据实际情况进行设置，主要根据对象生命周期来进行分配，如果对象生命周期很长，那么让老年代大一点，否则让新生代大一点 -XX:PretenureSizeThreadshold=1024 设置让大于此阈值的对象直接分配在老年代，单位为字节，只对Serial、ParNew收集器有效 -XX:MaxTenuringThreshold=15 默认值为15，新生代每次MinorGC后，还存活的对象年龄+1，当对象的年龄大于设置的这个值时就进入老年代 -XX:+PrintTenuringDistribution 让JVM在每次MinorGC后打印出当前使用的Survivor中对象的年龄分布 -XX:TargetSurvivorRatio 表示MinorGC结束后Survivor区域中占用空间的期望比例 -XX:+UseAdaptiveSizePolicy 1、分析 默认开启，将会导致Eden区和Survivor区的比例自动分配，因此也会引起我们默认值-XX:SurvivorRatio=8失效，所以真实比例可能不是8，比如可能是6等 2、如何设置Eden区和Survivor区的比例 -XX:SurvivorRatio=8 显示使用Eden区和Survivor区的比例，那就使用我自己的 没有显示使用Eden区和Survivor区的比例，无论打开或者关闭-XX:+UseAdaptiveSizePolicy，都会自动设置Eden区和Survivor区的比例 结论： 只有显示使用Eden区和Survivor区的比例，才会让比例生效，否则比例都会自动设置，最后推荐使用默认打开的-XX:+UseAdaptiveSizePolicy设置，并且不显示设置-XX:SurvivorRatio ③方法区 参数 说明 -XX:MetaspaceSize 初始空间大小 -XX:MaxMetaspaceSize 最大空间，默认没有限制 -XX:+UseCompressedOops 使用压缩对象指针 -XX:+UseCompressedClassPointers 使用压缩类指针 -XX:CompressedClassSpaceSize 设置Klass Metaspace的大小，默认1G ④直接内存1-XX:MaxDirectMemorySize 指定DirectMemory容量，若未指定，则默认与Java堆最大值一样 3）OutOfMemory相关的选项-XX:+HeapDumpOnOutMemoryError(在出现OOM的时候生成dump文件)和-XX:+HeapDumpBeforeFullGC(在出现Full GC的时候生成dump文件)只能设置1个 ，如果不设置-XX:HeapDumpPath=&lt;path&gt;，那么将会在当前目录下生成dump文件，如果设置的话，将会在指定位置生成dump文件。 -XX:+HeapDumpOnOutMemoryError 表示在内存出现OOM的时候，生成Heap转储文件，以便后续分析，-XX:+HeapDumpBeforeFullGC和-XX:+HeapDumpOnOutMemoryError只能设置1个 -XX:+HeapDumpBeforeFullGC 表示在出现FullGC之前，生成Heap转储文件，以便后续分析，-XX:+HeapDumpBeforeFullGC和-XX:+HeapDumpOnOutMemoryError只能设置1个，请注意FullGC可能出现多次，那么dump文件也会生成多个 -XX:HeapDumpPath= 指定heap转存文件的存储路径，如果不指定，就会将dump文件放在当前目录中 -XX:OnOutOfMemoryError 指定一个可行性程序或者脚本的路径，当发生OOM的时候，去执行这个脚本 对OnOutOfMemoryError的运维处理 以部署在linux系统/opt/Server目录下的Server.jar为例 1.在run.sh启动脚本中添加jvm参数: -XX:OnoutOfMemoryError=/opt/Server/restart.sh 2.restart.sh脚本 linux环境: 123#!/bin/bashpid=$(ps -ef|grep Server.jar /awk &#x27;&#123;if($8==&quot;java&quot; ) &#123;print $2&#125;&#125;&#x27;)kill -9 $pidcd /opt/Serverl ;sh run.sh windows环境: 123echo offwmic process where Name= &#x27;java.exe&#x27; deletecd D: \\Serverstart run.bat 4）垃圾收集器相关选项 ①查看默认的垃圾回收器-XX:+PrintCommandLineFlags:查看命令行相关参数（包含使用的垃圾收集器) 使用命令行指令:jinfo - flag相关垃圾回收器参数进程ID 以上两种方式都可以查看默认使用的垃圾回收器，第一种方式更加准备，但是需要程序的支持；第二种方式需要去尝试，如果使用了，返回的值中有+号，否则就是-号。 ②Serial回收器Seria收集器作为HotSpot中Client模式下的默认新生代垃圾收集器。Serial old是运行在Client模式下默认的老年代的垃圾回收器。 -XX:+UseSerialGc 指定年轻代和老年代都使用串行收集器。等价于新生代用Serial Gc，且老年代用Serial old GC。可以获得最高的单线程收集效率。 ③Parnew回收器-XX :+UseParNewGC 手动指定使用ParNew收集器执行内存回收任务。它表示年轻代使用并行收集器，不影响老年代。 -XX:ParallelGCThreads设置年轻代并行收集器的线程数。一般地，最好与CPU数量相等，以避免过多的线程数影响垃圾收集性能。 ·在默认情况下，当CPU 数量小于8个，ParallelGCThreads 的值等于CPU 数量。·当CPU数量大于8个，ParallelGCThreads的值等于3+[5*CPu_Count]/8]。 该回收器最终将会没有搭档，那就相当于被遗弃了。 ④Parallel回收器-XX:+UseParallelGc手动指定年轻代使用Parallel并行收集器执行内存回收任务。 -XX:+UseParalle101dGc手动指定老年代都是使用并行回收收集器。 ·分别适用于新生代和老年代。默认jdk8是开启的。 .上面两个参数，默认开启一个，另一个也会被开启。（互相激活) -XX: ParallelGCThreads设置年轻代并行收集器的线程数。一般地，最好与CPU数量相等，以避免过多的线程数影响垃圾收集性能。 ·在默认情况下，当CPU 数量小于8个，ParallelGCThreads 的值等于CPU 数量。·当CPu数量大于8个，ParallelGCThreads 的值等于3+[5*CPu_Count]/8]。 -XX:MaxGCPauseMillis 设置垃圾收集器最大停顿时间(即STw的时间)。单位是毫秒。 ·为了尽可能地把停顿时间控制在MaxGCPauseNills以内，收集器在工作时会调整Java堆大小或者其他一些参数。 ·对于用户来讲，停顿时间越短体验越好。但是在服务器端，我们注重高并发，整体的吞吐量。所以服务器端适合Parallel，进行控制。 ·该参数使用需谨慎。 -XX:GCTimeRatio垃圾收集时间占总时间的比例(= 1/ (N + 1))。用于衡量吞吐量的大小。 ·取值范围（0,100）。默认值99，也就是垃圾回收时间不超过1%。 ·与前一个-XX:MaxGCPauseMillis参数有一定矛盾性。暂停时间越长，Radio参数就容易超过设定的比例。 -XX:+UseAdaptiveSizePolicy设置Parallel Scavenge收集器具有自适应调节策略 ·在这种模式下，年轻代的大小、Eden和Survivor的比例、晋升老年代的对象年龄等参数会被自动调整，已达到在堆大小、吞吐量和停顿时间之间的平衡点。 在手动调优比较困难的场合，可以直接使用这种自适应的方式，仅指定虚拟机的最大堆、目标的吞吐量(GCTimeRatio）和停顿时间(MaxGCPauseMills），让虚拟机自己完成调优工作。 注意： Parallel回收器主打吞吐量，而CMS和G1主打低延迟，如果主打吞吐量，那么就不应该限制最大停顿时间，所以-XX:MaxGCPauseMills不应该设置 -XX:MaxGCPauseMills中的调整堆大小通过默认开启的-XX:+UseAdaptiveSizePolicy来实现 -XX:GCTimeRatio用来衡量吞吐量，并且和-XX:MaxGCPauseMills矛盾，因此不会同时使用 ⑤CMS回收器-XX:+UseConcMarkSweepGC手动指定使用CMS 收集器执行内存回收任务。 ·开启该参数后会自动将-XX:+UseParNewGc打开。即:ParNew(Youngl区用)+CNS(01d区用)+Serial o1d的组合。 -XX:CMS1nitiating0ccupanyFraction设置堆内存使用率的阀值，一旦达到该阈值，便开始进行回收。 JDK5及以前版本的默认值为68,即当老年代的空间使用率达到68%时，会执行一次CNS回收。JDK6及以上版本默认值为92% ·如果内存增长缓慢，则可以设置一个稍大的值，大的阈值可以有效降低CMS的触发频率，减少老年代回收的次数可以较为明显地改善应用程序性能。反之，如果应用程序内存使用率增长很快，则应该降低这个阈值，以避免频繁触发老年代串行收集器。因此通过该选项便可以有效降低Full GC的执行次数。 -XX:+UseCNSCompactAtFullCollection用于指定在执行完Full GC后对内存空间进行压缩整理，以此避免内存碎H的产生。不过由于内存压缩整理过程无法并发执行，所带来的问题就是停顿时间变得更长了。 -XX:CNSFul1GCsBeforeCompaction设置在执行多少次Full GC后对内存空间进行压缩整理。 -XX:Paralle1CMSThreads设置CMS的线程数量。 . CMS默认启动的线程数是(ParallelGCThreads+3)/4，Paralle1GCThreads是年轻代并行收集器的线程数。当CPU 资源比较紧张时，受到CNS收集器线程的影响，应用程序的性能在垃圾回收阶段可能会非常糟糕。 -XX:ParallelCMSThreads和ParallelGCThreads有关系，ParallelGCThreads在上面Parnew回收器中有提到。 补充参数 另外，CMS收集器还有如下常用参数: -XX:ConcGCThreads:设置并发垃圾收集的线程数，默认该值是基于ParallelGCThreads计算出来的; -XX:+UseCMSInitiatingoccupancyonly:是否动态可调，用这个参数可以使CMS一直按CMSInitiatingoccupancyFraction设定的值启动 -XX:+CMSScavengeBeforeRemark:强制hotspot虚拟机在cms remark阶段之前做一次minorgc，用于提高remark阶段的速; -XX:+CMSClassUnloadingEnable:如果有的话，启用回收Perm 区(JDK8之前) -XX:+CMSParallelInitialEnabled:用于开启CNS initial-mark阶段采用多线程的方式进行标记，用于提高标记速度，在Java8开始已经默认开启; -XX:+CMSParallelRemarkEnabled:用户开启CNS remark阶段采用多线程的方式进行重新标记，默认开启; -XX:+ExplicitGCInvokesConcurrent ，-XX:+ExplicitGCInvokesConcurrentAndUnloadsclasses这两个参数用户指定hotspot虚拟在执行System.gc()时使用CMS周期; -XX:+CMSPrecleaningEnabled:指定CMS是否需要进行Pre cleaning这个阶段。 ⑥G1回收器-XX:+UseG1GC手动指定使用G1收集器执行内存回收任务。 -XX:G1HeapRegionSize设置每个Region的大小。值是2的幂，范围是1NB到32MB之间，目标是根据最小的Java堆大小划分出约2048个区域。默认是堆内存的1/2000。 -XX: MaxGCPauseMillis设置期望达到的最大GC停顿时间指标(JVM会尽力实现，但不保证达到)。默认值是200ms -XX:Paralle1GCThread设置STw时Gc线程数的值。最多设置为8 -XX:ConcGCThreads设置并发标记的线程数。将n设置为并行垃圾回收线程数(ParallelGCThreads)的1/4左右。 -XX: InitiatingHeap0ccupancyPercent设置触发并发GC周期的Java堆占用率阈值。超过此值，就触发Gc。默认值是45。 -XX:G1NewSizePercent、-XX:G1MaxNewSizePercent新生代占用整个堆内存的最小百分比（默认5%）、最大百分比（默认60%) -XX:G1ReservePercent=10保留内存区域，防止 to space ( Survivor中的to区）溢出 如果使用G1垃圾收集器，不建议设置-Xmn和-XX:NewRatio，毕竟可能影响G1的自动调节 Mixed GC调优参数 注意:G1收集器主要涉及到Mixed GC，Mixed Gc会回收young区和部分old区。 G1关于Mixed GC调优常用参数: -XX:InitiatingHeapOccupancyPercent:设置堆占用率的百分比（0到100）达到这个数值的时候触发global concurrent marking(全局并发标记），默认为45%。值为8表示间断进行全局并发标记。 -XX:G1MixedGCLiveThresholdPercent:设置old区的region被回收时候的对象占比，默认占用率为85%。只有old区的region中存活的对象占用达到了这个百分比，才会在Mixed Gc中被回收。 -XX:G1HeapwastePercent:在global concurrent marking（全局并发标记)结束之后，可以知道所有的区有多少空间要被回收，在每次young GC之后和再次发生Mixed GC之前，会检查垃圾占比是否达到此参数，只有达到了，下次才会发生Mixed GC。 -XX:G1MixedGCCountTarget:一次global concurrent marking(全局并发标记）之后，最多执行Mixed GC的次数，默认是8。 -XX:G101dCSetRegionThresholdPercent:设置Mixed GC收集周期中要收集的old region数的上限。默认值是Java堆的10% ⑦怎么选择垃圾收集器优先调整堆的大小让JVM自适应完成。如果内存小于100M，使用串行收集器 如果是单核、单机程序，并且没有停顿时间的要求，串行收集器 如果是多CPU、需要高吞吐量、允许停顿时间超过1秒，选择并行或者JVM自己选择如果是多CPU、追求低停顿时间，需快速响应（比如延迟不能超过1秒，如互联网应用），使用并发收集器。官方推荐G1，性能高。现在互联网的项目，基本都是使用G1。 特别说明: 1．没有最好的收集器，更没有万能的收集; 2．调优永远是针对特定场景、特定需求，不存在一劳永逸的收集器 5）GC日志相关选项①常用参数 参数 说明 -verbose:gc 输出日志信息，默认输出的标准输出。可以独立使用 -XX:+PrintGC 等同于-verbose:gc表示打开简化的日志。可以独立使用 -XX:+PrintGCDetails 在发生垃圾回收时打印内存回收详细的日志，并在进程退出时输出当前内存各区域的分配情况。可以独立使用 -XX:+PrintGCTimeStamps 程序启动到GC发生的时间秒数。不可以独立使用，需要配合-XX:+PrintGCDetails使用 -XX:+PrintGCDateStamps 输出GC发生时的时间戳（以日期的形式，例如：2013-05-04T21:53:59.234+0800）不可以独立使用，可以配合-XX:+PrintGCDetails使用 -XX:+PrintHeapAtGC 每一次GC前和GC后，都打印堆信息，可以独立使用 -XIoggc: 把GC日志写入到一个文件中去，而不是打印到标准输出中 ②其他参数 参数 说明 -XX:TraceClassLoading 监控类的加载 -XX:PrintGCApplicationStoppedTime 打印GC时线程的停顿时间 -XX:+PrintGCApplicationConcurrentTime 垃圾收集之前打印出应用未中断的执行时间 -XX:+PrintReferenceGC 记录回收了多少种不同引用类型的引用 -XX:+PrintTenuringDistribution 让JVM在每次MinorGC后打印出当前使用的Survivor中对象的年龄分布 -XX:+UseGCLogFileRotation 启用GC日志文件的自动转储 -XX:NumberOfGCLogFiles=1 GC日志文件的循环数目 -XX:GCLogFileSize=1M 控制GC日志文件的大小 6）其他参数 参数 说明 -XX:+DisableExplicitGC 禁用hotspot执行System.gc()，默认禁用 -XX:ReservedCodeCacheSize=[g m -XX:+UseCodeCacheFlushing 使用该参数让jvm放弃一些被编译的代码，避免代码缓存被占满时JVM切换到interpreted-only的情况 -XX:+DoEscapeAnalysis 开启逃逸分析 -XX:+UseBiasedLocking 开启偏向锁 -XX:+UseLargePages 开启使用大页面 -XX:+PrintTLAB 打印TLAB的使用情况 -XX:TLABSize 设置TLAB大小 4，通过Java代码获取JVM参数Java提供了java.lang.management包用于监视和管理Java虚拟机和3ava运行时中的其他组件，它允许本地和远程监控和管理运行的Java虚拟机。其中ManagementFactory这个类还是挺常用的。另外还有Runtime类也可以获取一些内存、CPuU核数等相关的数据。 通过这些api可以监控我们的应用服务器的堆内存使用情况，设置一些阈值进行报警等处理。 12345678910111213141516171819202122232425/** * * 监控我们的应用服务器的堆内存使用情况，设置一些阈值进行报警等处理 * * @author shkstart * @create 15:23 */public class MemoryMonitor &#123; public static void main(String[] args) &#123; MemoryMXBean memorymbean = ManagementFactory.getMemoryMXBean(); MemoryUsage usage = memorymbean.getHeapMemoryUsage(); System.out.println(&quot;INIT HEAP: &quot; + usage.getInit() / 1024 / 1024 + &quot;m&quot;); System.out.println(&quot;MAX HEAP: &quot; + usage.getMax() / 1024 / 1024 + &quot;m&quot;); System.out.println(&quot;USE HEAP: &quot; + usage.getUsed() / 1024 / 1024 + &quot;m&quot;); System.out.println(&quot;\\nFull Information:&quot;); System.out.println(&quot;Heap Memory Usage: &quot; + memorymbean.getHeapMemoryUsage()); System.out.println(&quot;Non-Heap Memory Usage: &quot; + memorymbean.getNonHeapMemoryUsage()); System.out.println(&quot;=======================通过java来获取相关系统状态============================ &quot;); System.out.println(&quot;当前堆内存大小totalMemory &quot; + (int) Runtime.getRuntime().totalMemory() / 1024 / 1024 + &quot;m&quot;);// 当前堆内存大小 System.out.println(&quot;空闲堆内存大小freeMemory &quot; + (int) Runtime.getRuntime().freeMemory() / 1024 / 1024 + &quot;m&quot;);// 空闲堆内存大小 System.out.println(&quot;最大可用总堆内存maxMemory &quot; + Runtime.getRuntime().maxMemory() / 1024 / 1024 + &quot;m&quot;);// 最大可用总堆内存大小 &#125;&#125; 1)通过Runtime获取1234567891011public class HeapSpaceInitial &#123; public static void main(String[]args) &#123; //返回ava虚拟机中的堆内存总量 long initialMemory = Runtime.getRuntime( ).totalMemory() / 1024/ 1024;//返回Java虚拟机试图使用的最大堆内存量 long maxMemory = Runtime.getRuntime( ).maxMemory() / 1024 / 1024; System.out.print1n( &quot;-Xms : &quot; + initialMemory +&quot;&quot;&quot;&quot;); System.out.print1n(&quot;-Xmx : &quot; + maxMemory +“&quot;&quot;); system.out.print1n(&quot;系统内存大小为: &quot; + maxMemory * 4.0 / 1024 +&quot;G&quot;); system.out.println(&quot;系统内存大小为: &quot; + initialMemory * 64.0 / 1024 + &quot;G&quot;); &#125;&#125; 二十三，分析GC日志1，GC日志参数 参数 说明 -verbose:gc 输出gc日志信息，默认输出到标准输出 -XX:+PrintGC 输出GC日志。类似：-verbose:gc -XX:+PrintGCDetails 在发生垃圾回收时打印内存回收相处的日志，并在进程退出时输出当前内存各区域分配情况 -XX:+PrintGCTimeStamps 输出GC发生时的时间戳 -XX:+PrintGCDateStamps 输出GC发生时的时间戳（以日期的形式，例如：2013-05-04T21:53:59.234+0800） -XX:+PrintHeapAtGC 每一次GC前和GC后，都打印堆信息 -Xloggc: 表示把GC日志写入到一个文件中去，而不是打印到标准输出中 2，GC日志格式1）GC分类针对HotSpot VM的实现，它里面的GC按照回收区域又分为两大种类型:一种是部分收集（Partial GC)，一种是整堆收集（Full GC) 部分收集:不晕完整收集整个Java堆的垃圾收集。其中又分为: ·新生代收集(Minor GC / Young GC）:只是新生代（Eden\\Se,S1）的垃圾收集·老年代收集（Major Gc / old GC):只是老年代的垃圾收集。 ·目前，只有cMS GC会有单独收集老年代的行为。 ·注意，很多时候Major GC会和FullGC混淆使用，需要具体分辨是老年代回收还是整堆回收。 ·混合收集（Mixed GC):收集整个新生代以及部分老年代的垃圾收集。 ·目前,只有G1 GC会有这种行为。 整堆收集（Ful1 GC):收集整个java堆和方法区的垃圾收集。 新生代收集：当Eden区满的时候就会进行新生代收集，所以新生代收集和S0区域和S1区域无关。 老年代收集和新生代收集的关系：进行老年代收集之前会先进行一次年轻代的垃圾收集，原因如下：一个比较大的对象无法放入新生代，那它自然会往老年代去放，如果老年代也放不下，那会先进行一次新生代的垃圾收集，之后尝试往新生代放，如果还是放不下，才会进行老年代的垃圾收集，之后在往老年代去放，这是一个过程，我来说明一下为什么需要往老年代放，但是放不下，而进行新生代垃圾收集的原因，这是因为新生代垃圾收集比老年代垃圾收集更加简单，这样做可以节省性能。 进行垃圾收集的时候，堆包含新生代、老年代、元空间/永久代：可以看出Heap后面包含着新生代、老年代、元空间，但是我们设置堆空间大小的时候设置的只是新生代、老年代而已，元空间是分开设置的。 哪些情况会触发Full GC：老年代空间不足、方法区空间不足、显示调用System.gc()、Minior GC进入老年代的数据的平均大小 大于 老年代的可用内存、大对象直接进入老年代，而老年代的可用空间不足。 2）不同GC分类的GC细节123456789101112131415161718192021222324252627282930/** * -XX:+PrintCommandLineFlags * * -XX:+UseSerialGC:表明新生代使用Serial GC ，同时老年代使用Serial Old GC * * -XX:+UseParNewGC：标明新生代使用ParNew GC * * -XX:+UseParallelGC:表明新生代使用Parallel GC * -XX:+UseParallelOldGC : 表明老年代使用 Parallel Old GC * 说明：二者可以相互激活 * * -XX:+UseConcMarkSweepGC：表明老年代使用CMS GC。同时，年轻代会触发对ParNew 的使用 * @author shkstart * @create 17:19 */public class GCUseTest &#123; public static void main(String[] args) &#123; ArrayList&lt;byte[]&gt; list = new ArrayList&lt;&gt;(); while(true)&#123; byte[] arr = new byte[1024 * 10];//10kb list.add(arr);// try &#123;// Thread.sleep(5);// &#125; catch (InterruptedException e) &#123;// e.printStackTrace();// &#125; &#125; &#125;&#125; ①老年代使用CMS GCGC设置方法：参数中使用-XX:+UseConcMarkSweepGC，说明老年代使用CMS GC，同时年轻代也会触发对ParNew的使用，因此添加该参数之后，新生代使用ParNew GC，而老年代使用CMS GC，整体是并发垃圾收集，主打低延迟。 ②新生代使用Serial GCGC设置方法：参数中使用-XX:+UseSerialGC，说明新生代使用Serial GC，同时老年代也会触发对Serial Old GC的使用，因此添加该参数之后，新生代使用Serial GC，而老年代使用Serial Old GC，整体是串行垃圾收集。 3）GC日志分类①MinorGC ②FullGC 4）GC日志结构剖析①垃圾收集器使用Serial收集器在新生代的名字是Default New Generation，因此显示的是&quot;[DefNew&quot;· 使用ParNew收集器在新生代的名字会变成&quot;[ParNew&quot;,意思是&quot;Parallel New Generation&quot;· 使用parallel Scavenge收集器在新生代的名字是&quot;[PSYoungGen&quot; ,这里的3DK1.7使用的就是PSYoungGen 使用Parallel old Generation收集器在老年代的名字是&quot;[ParoldGen&quot;· 使用G1收集器的话，会显示为&quot;garbage-first heap&quot; Allocation Failure 表明本次引起GC的原因是因为在年轻代中没有足够的空间能够存储新的数据了。 ②GC前后情况通过图示，我们可以发现Gc日志格式的规律一般都是:GC前内存占用一&gt;GC后内存占用（该区域内存总大小) [PSYoungGen: 5986K-&gt;696K(8704K)]5986K-&gt;704K(9216K) 中括号内:Gc回收前年轻代堆大小，回收后大小，（年轻代堆总大小) 括号外:GC回收前年轻代和老年代大小，回收后大小，（年轻代和老年代总大小) ③GC时间Gc日志中有三个时间:user，sys和real user - 进程执行用户态代码（核心之外）所使用的时间。这是执行此进程所使用的实际CPU时间，其他进程和此进程阻塞的时间并不包括在内。在垃圾收集的情况下，表示GC线程执行所使用的CPU总时间。 sys -进程在内核态消耗的 CPU时间，即在内核执行系统调用或等待系统事件所使用的CPU时间 real -程序从开始到结束所用的时钟时间。这个时间包括其他进程使用的时间片和进程阻塞的时间（比如等待I/0 完成）。对于并行gc，这个数字应该接近（用户时间+系统时间）除以垃圾收集器使用的线程数。 由于多核的原因，一般的Gc事件中，real time是小于sys + user time的，因为一般是多个线程并发的去做GCc，所以real time是要小于sys+user time的。如果real&gt;sys+user的话，则你的应用可能存在下列问题:IO负载非常重或者是CPU不够用。 5)Minor GC 日志解析123456789101112131415161718192021222324252627282020-11-20T17:19:43.265-0800: 0.822:[GC(ALLOCATION FAILURE)[PSYOUNGGEN:76800K-&gt;8433K(89600K)] 76800K-&gt;8449K(294400K)，0.0088371 SECS][TIMES:USER=0.02svs=e.01,REAL=0.01 SECS]##注释2020-11-20T17:19:43.265-0800 添加-XX:+PrintGCDateStamps参数 日志打印时间 日期格式 如2013-05-04T21:53:59.234+0800 0.822: 添加-XX:+PrintGCTimeStamps该参数 gc发生时，Java虚拟机启动以来经过的秒数[GC(Allocation Failure) 发生了一次垃圾回收，这是一次Minior GC。它不区分新生代还是老年代GC，括号里的内容是gc发生的原因，这里的Allocation Failure的原因是新生代中没有足够区域能够存放需要分配的数据而失败。[PSYoungGen:76800K-&gt;8433K(89600K) PSYoungGen：表示GC发生的区域，区域名称与使用的GC收集器是密切相关的 Serial收集器：Default New Generation 显示Defnew ParNew收集器：ParNew Parallel Scanvenge收集器：PSYoung 老年代和新生代同理，也是和收集器名称相关 76800K-&gt;8433K(89600K)：GC前该内存区域已使用容量-&gt;GC后盖区域容量(该区域总容量) 如果是新生代，总容量则会显示整个新生代内存的9/10，即eden+from/to区 如果是老年代，总容量则是全身内存大小，无变化76800K-&gt;8449K(294400K) 虽然本次是Minor GC，只会进行新生代的垃圾收集，但是也肯定会打印堆中总容量相关信息在显示完区域容量GC的情况之后，会接着显示整个堆内存区域的GC情况：GC前堆内存已使用容量-&gt;GC后堆内存容量（堆内存总容量），并且堆内存总容量 = 9/10 新生代 + 老年代，然后堆内存总容量肯定小于初始化的内存大小。,0.0088371 整个GC所花费的时间，单位是秒[Times：user=0.02 sys=0.01,real=0.01 secs] user：指CPU工作在用户态所花费的时间 sys：指CPU工作在内核态所花费的时间 real：指在此次事件中所花费的总时间 6)Full GC 日志解析123456789101112131415161718192021222324252627282930312020-11-20T17:19:43.794-0800: 1.351:[FULL GC (METADATA GC THRESHOLD)[PsYOUNGGEN: 10082K-&gt;eK(89600K)][ PAROLDGEN: 32K-&gt;9638K(204800K)]10114K-&gt;9638K ( 29440OK),[METASPACE: 20158K-&gt;20156K(1067008K)]，0.0285388 SECS] [TIMES: USER=0.11sYS=0.00，REAL=6.03 SECS]##注释2020-11-20T17:19:43.794-0800 日志打印时间 日期格式 如2013-05-04T21:53:59.234+0800 添加-XX:+PrintGCDateStamps参数1.351 gc发生时，Java虚拟机启动以来经过的秒数 添加-XX:+PrintGCTimeStamps该参数Full GC(Metadata GCThreshold) 括号中是gc发生的原因，原因：Metaspace区不够用了。 除此之外，还有另外两种情况会引起Full GC，如下： 1、Full GC(FErgonomics) 原因：JVM自适应调整导致的GC 2、Full GC（System） 原因：调用了System.gc()方法[PSYoungGen: 100082K-&gt;0K(89600K)] PSYoungGen：表示GC发生的区域，区域名称与使用的GC收集器是密切相关的 Serial收集器：Default New Generation 显示DefNew ParNew收集器：ParNew Parallel Scanvenge收集器：PSYoungGen 老年代和新生代同理，也是和收集器名称相关 10082K-&gt;0K(89600K)：GC前该内存区域已使用容量-&gt;GC该区域容量(该区域总容量) 如果是新生代，总容量会显示整个新生代内存的9/10，即eden+from/to区 如果是老年代，总容量则是全部内存大小，无变化 ParOldGen：32K-&gt;9638K(204800K) 老年代区域没有发生GC，因此本次GC是metaspace引起的10114K-&gt;9638K(294400K),在显示完区域容量GC的情况之后，会接着显示整个堆内存区域的GC情况：GC前堆内存已使用容量-&gt;GC后堆内存容量（堆内存总容量），并且堆内存总容量 = 9/10 新生代 + 老年代，然后堆内存总容量肯定小于初始化的内存大小[Meatspace:20158K-&gt;20156K(1067008K)], metaspace GC 回收2K空间 3，GC日志分析工具上节介绍了GC日志的打印及含义，但是GC日志看起来比较麻烦，本节将会介绍一下GC日志可视化分析工具GCeasy和GCviewer等。通过Gc日志可视化分析工具，我们可以很方便的看到JVM各个分代的内存使用情况、垃圾回收次数、垃圾回收的原因、垃圾回收占用的时间、吞吐量等，这些指标在我们进行VM调优的时候是很有用的。 如果想把GC日志存到文件的话，是下面这个参数:-Xloggc : /path/to/gc. log 然后就可以用一些工具去分析这些gc日志。 1）GCEasy2）GCViewer1.下载GCViewer工具 源码下载:https://github.com/chewiebug/GCViewer 运行版本下载: https://github.com/chewiebug/GCViewer/wiki/Changelog 2.只需双击gcviewer-1.3x.jar或运行java -jar gcviewer-1.3x.jar(它需要运行java1.8 vm) ，即可启动GCViewer。 3）HPjmeter工具很强大，但是只能打开由以下参数生成的GC log，-verbose:gc -Xloggc:gc.log。添加其他参数生成的gc.log无法打开。 HPjmeter集成了以前的HPjtune功能，可以分析在HP机器上产生的垃圾回收日志文件。","categories":[{"name":"JVM","slug":"JVM","permalink":"https://yinhuidong.github.io/categories/JVM/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"https://yinhuidong.github.io/tags/JVM/"}]},{"title":"HashMap","slug":"JAVA基础/深度解析HashMap底层原理","date":"2022-01-12T00:19:55.246Z","updated":"2022-01-12T00:19:55.246Z","comments":true,"path":"2022/01/12/JAVA基础/深度解析HashMap底层原理/","link":"","permalink":"https://yinhuidong.github.io/2022/01/12/JAVA%E5%9F%BA%E7%A1%80/%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90HashMap%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86/","excerpt":"","text":"一，概念分析1.数组和链表的对比数组：占用内存连续的空间，空间占用大，寻址容易，插入删除困难 链表：内存空间不连续，空间占用比较小，寻址困难，插入删除容易 2.散列表12整合数组和链表两者的特性，数组里面每一个位置都保存一个链表。hash也称为散列，基本原理就是把任意长度的输入，通过hash算法变成固定长度的输出。这个映射的规则就是对应的Hash算法，而原始数据映射后的二进制串就是哈希值。 Hash的特点： 1.从hash值不可以反向推导出原始的数据 2.输入数据的微小变化会得到完全不同的hash值，相同的数据会得到相同的值 3.哈希算法的执行效率要高效，长的文本也能快速地计算出哈希值 4.hash算法的冲突概率要小 由于hash的原理是将输入空间的值映射成hash空间内，而hash值的空间远小于输入的空间。根据抽屉原理，一定会存在不同的输入被映射成相同输出的情况。 抽屉原理：桌上有十个苹果，要把这十个苹果放到九个抽屉里，无论怎样放，我们会发现至少会有一个抽屉里面放不少于两个苹果。 二，手写源码1.属性123456789101112//如果没有指定长度，默认的长度static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16//table的最大长度static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;//默认的加载因子，与table的扩容有关static final float DEFAULT_LOAD_FACTOR = 0.75f;//默认的树化阈值static final int TREEIFY_THRESHOLD = 8;//默认的反树化阈值static final int UNTREEIFY_THRESHOLD = 6;//树化的另一个参数static final int MIN_TREEIFY_CAPACITY = 64; 由此可见，当数组四分之三的位置上长度上都有元素时，就会引发数组扩容，当数组长度大于64且单个桶位元素大于8个时就会导致桶位上的元素从链表转化为红黑树。当红黑树的元素少于6个时就会导致桶位上的红黑树退化为链表，为什么是6呢？防止不停地树化反树化。 2.构造器123public HashMap() &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted&#125; 123public HashMap(int initialCapacity) &#123; this(initialCapacity, DEFAULT_LOAD_FACTOR);&#125; 12345678910111213141516171819public HashMap(int initialCapacity, float loadFactor) &#123; //其实就是做了一些校验 //初始化值必须大于0 if (initialCapacity &lt; 0) throw new IllegalArgumentException(&quot;Illegal initial capacity: &quot; + initialCapacity); //如果初始化值大于最大值，就将容量改为最大值 if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; //加载因子必须大于0并且是一个数字 if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(&quot;Illegal load factor: &quot; + loadFactor); this.loadFactor = loadFactor; this.threshold = tableSizeFor(initialCapacity);&#125; 从这里我们可以看到，hashmap的构造器采用了套娃的模式，实际上执行的构造器是最下面的一次经历过条件判断的构造器。如果我们能确定集合的大概长度，尽量在创建的时候指定长度，防止map不停地扩容操作，降低效率。 3.内部类 123456789101112131415161718192021222324252627282930313233343536373839static class Node&lt;K,V&gt; implements Entry&lt;K,V&gt; &#123; final int hash; final K key; V value; Node&lt;K,V&gt; next; Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final String toString() &#123; return key + &quot;=&quot; + value; &#125; public final int hashCode() &#123; return Objects.hashCode(key) ^ Objects.hashCode(value); &#125; public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; public final boolean equals(Object o) &#123; if (o == this) return true; if (o instanceof Map.Entry) &#123; Entry&lt;?,?&gt; e = (Entry&lt;?,?&gt;)o; if (Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue())) return true; &#125; return false; &#125;&#125; 4.hash1234static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; 如果key==null，hash值就是0，hash值就等于key的hash值^h无符号右移16位。 异或：相同则返回0，不同返回1 这就是哈希map底层使用的扰动，为了让高16位也参与运算。防止哈希冲突。 5.put123public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125; 可以看到这里又开始套娃了，实际上执行的是putVal方法。 6.putVal12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970/** * Implements Map.put and related methods * * @param hash hash for key * @param key the key * @param value the value to put * @param onlyIfAbsent if true, don&#x27;t change existing value * 当插入的key和原有的key一致时，根据这个属性判断value是否覆盖 * @param evict if false, the table is in creation mode. * 如果为false，就扩容。 * @return previous value, or null if none */final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; //tab：引用当前hash的散列表 //p：当前散列表的元素 //n：散列表的数组长度 //i：表示路由寻址 结果 Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; //延迟初始化逻辑，第一次调用putVal时会初始化hashMap对象中的最耗费内存的散列表 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; //最简单的一种情况：寻址找到的桶位 刚好是 null，这个时候，直接将当前k-v=&gt;node 扔进去就可以了 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; //e：不为null的话，找到了一个与当前要插入的key-value一致的key的元素 //k：表示临时的一个key Node&lt;K,V&gt; e; K k; //表示桶位中的该元素，与你当前插入的元素的key完全一致，表示后续需要进行替换操作 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; //表示当前桶位的结构时红黑树 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; //当前桶位上是多个以链表形式存在的node，而且链表的头结点与我们要插入的数据不一致 for (int binCount = 0; ; ++binCount) &#123; //一直遍历到链表最后，也没找到一个与要插入的元素一样的，所以直接放到链表末尾 if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); //当前链表长度达到了树化的标准 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash);//树化 break; &#125; //条件成立的话，说明找到了相同key的node元素，需要进行替换操作 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; //e不等于null，条件成立说明，找到了一个与你插入元素key完全一致的数据，需要进行替换 if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; //modCount：表示散列表结构被修改的次数，替换Node元素的value不计数 //插入新元素，size自增，如果自增后的值大于扩容阈值，则触发扩容。 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125; map.put(“yhd”,”yhd”)； 底层执行的操作流程 1.获取key的哈希值 2.经过哈希值扰动函数，使这个哈希值更散列 3.构造出一个node对象 4.使用哪个路由寻址算法，找出node对象应该存放的数组位置 关于路由寻址算法： (table.length-1)&amp;node.hash 7.resize123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125final Node&lt;K,V&gt;[] resize() &#123; //oldTab：引用扩容前的哈希表 Node&lt;K,V&gt;[] oldTab = table; //oldCap：表示扩容之前table数组的长度 int oldCap = (oldTab == null) ? 0 : oldTab.length; //oldThr：表示扩容之前的扩容阈值，触发本次扩容的阈值 int oldThr = threshold; //newCap：扩容之后table数组的大小 //newThr：扩容之后，下次再次触发扩容的条件 int newCap, newThr = 0; //条件如果成立说明 hashMap中的散列表已经初始化过了，这是一次正常扩容 if (oldCap &gt; 0) &#123; //扩容之前的table数组大小已经达到 最大阈值后，则不扩容，且设置扩容条件为 int 最大值。 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; //oldCap左移一位实现数值翻倍，并且赋值给newCap， newCap 小于数组最大值限制 且 扩容之前的阈值 &gt;= 16 //这种情况下，则 下一次扩容的阈值 等于当前阈值翻倍 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; //oldCap == 0,说明hashMap中的散列表是null //1.new HashMap(initCap, loadFactor); //2.new HashMap(initCap); //3.new HashMap(map); 并且这个map有数据 else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; //oldCap == 0，oldThr == 0 //new HashMap(); else &#123; // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY;//16 newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);//12 &#125; //newThr为零时，通过newCap和loadFactor计算出一个newThr if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; //创建出一个更长 更大的数组 @SuppressWarnings(&#123;&quot;rawtypes&quot;,&quot;unchecked&quot;&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; //说明，hashMap本次扩容之前，table不为null if (oldTab != null) &#123; for (int j = 0; j &lt; oldCap; ++j) &#123; //当前node节点 Node&lt;K,V&gt; e; //说明当前桶位中有数据，但是数据具体是 单个数据，还是链表 还是 红黑树 并不知道 if ((e = oldTab[j]) != null) &#123; //方便JVM GC时回收内存 oldTab[j] = null; //第一种情况：当前桶位只有一个元素，从未发生过碰撞，这情况 直接计算出当前元素应存放在 新数组中的位置，然后 //扔进去就可以了 if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; //第二种情况：当前节点已经树化 else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // preserve order //第三种情况：桶位已经形成链表 //低位链表：存放在扩容之后的数组的下标位置，与当前数组的下标位置一致。 Node&lt;K,V&gt; loHead = null, loTail = null; //高位链表：存放在扩容之后的数组的下表位置为 当前数组下标位置 + 扩容之前数组的长度 Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; //hash-&gt; .... 1 1111 //hash-&gt; .... 0 1111 // 0b 10000 if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; 8.get1234public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value;&#125; 同样的套娃原理，实际上执行的是getNode() 123456789101112131415161718192021222324252627282930final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; //tab：引用当前hashMap的散列表 //first：桶位中的头元素 //e：临时node元素 //n：table数组长度 Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; //第一种情况：定位出来的桶位元素 即为咱们要get的数据 if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; //说明当前桶位不止一个元素，可能 是链表 也可能是 红黑树 if ((e = first.next) != null) &#123; //第二种情况：桶位升级成了 红黑树 if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); //第三种情况：桶位形成链表 do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null;&#125; 9.remove12345public V remove(Object key) &#123; Node&lt;K,V&gt; e; return (e = removeNode(hash(key), key, null, false, true)) == null ? null : e.value;&#125; jdk专业套娃removeNode（） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879/** * Implements Map.remove and related methods. * * @param hash hash for key * @param key the key * @param value the value to match if matchValue, else ignored * @param matchValue if true only remove if value is equal * @param movable if false do not move other nodes while removing * @return the node, or null if none */final Node&lt;K,V&gt; removeNode(int hash, Object key, Object value, boolean matchValue, boolean movable) &#123; //tab：引用当前hashMap中的散列表 //p：当前node元素 //n：表示散列表数组长度 //index：表示寻址结果 Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, index; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (p = tab[index = (n - 1) &amp; hash]) != null) &#123; //说明路由的桶位是有数据的，需要进行查找操作，并且删除 //node：查找到的结果 //e：当前Node的下一个元素 Node&lt;K,V&gt; node = null, e; K k; V v; //第一种情况：当前桶位中的元素 即为 你要删除的元素 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) node = p; else if ((e = p.next) != null) &#123; //说明，当前桶位 要么是 链表 要么 是红黑树 if (p instanceof TreeNode)//判断当前桶位是否升级为 红黑树了 //第二种情况 //红黑树查找操作 node = ((TreeNode&lt;K,V&gt;)p).getTreeNode(hash, key); else &#123; //第三种情况 //链表的情况 do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) &#123; node = e; break; &#125; p = e; &#125; while ((e = e.next) != null); &#125; &#125; //判断node不为空的话，说明按照key查找到需要删除的数据了 if (node != null &amp;&amp; (!matchValue || (v = node.value) == value || (value != null &amp;&amp; value.equals(v)))) &#123; //第一种情况：node是树节点，说明需要进行树节点移除操作 if (node instanceof TreeNode) ((TreeNode&lt;K,V&gt;)node).removeTreeNode(this, tab, movable); //第二种情况：桶位元素即为查找结果，则将该元素的下一个元素放至桶位中 else if (node == p) tab[index] = node.next; else //第三种情况：将当前元素p的下一个元素 设置成 要删除元素的 下一个元素。 p.next = node.next; ++modCount; --size; afterNodeRemoval(node); return node; &#125; &#125; return null;&#125; 10.replace1234567891011@Overridepublic boolean replace(K key, V oldValue, V newValue) &#123; Node&lt;K,V&gt; e; V v; if ((e = getNode(hash(key), key)) != null &amp;&amp; ((v = e.value) == oldValue || (v != null &amp;&amp; v.equals(oldValue)))) &#123; e.value = newValue; afterNodeAccess(e); return true; &#125; return false;&#125; 本文参照哔哩哔哩小刘讲源码和jdk的HashMap源码。","categories":[{"name":"JAVA基础","slug":"JAVA基础","permalink":"https://yinhuidong.github.io/categories/JAVA%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"JAVA基础","slug":"JAVA基础","permalink":"https://yinhuidong.github.io/tags/JAVA%E5%9F%BA%E7%A1%80/"}]},{"title":"字符串踩坑","slug":"JAVA基础/面试题踩坑之String","date":"2022-01-12T00:19:55.246Z","updated":"2022-01-12T00:19:55.246Z","comments":true,"path":"2022/01/12/JAVA基础/面试题踩坑之String/","link":"","permalink":"https://yinhuidong.github.io/2022/01/12/JAVA%E5%9F%BA%E7%A1%80/%E9%9D%A2%E8%AF%95%E9%A2%98%E8%B8%A9%E5%9D%91%E4%B9%8BString/","excerpt":"","text":"12345678public static void main(String[] args) &#123; String str = new StringBuilder(&quot;ja&quot;).append(&quot;va&quot;).toString(); System.out.println(str == str.intern());//false System.out.println(&quot;=====================================================&quot;); String str2 = new StringBuilder(&quot;red&quot;).append(&quot;is&quot;).toString(); System.out.println(str2 == str2.intern());//true&#125; 输出结果为什么不一样？ 原因： 实际上Version类里面已经有了一个定义好的字符串java，所以我们new出来的，和人家的实际上并不是同一个，此外，Version类里面的其他几个字符串也一样是如此的。详情参照《深入理解java虚拟机》第三版。","categories":[{"name":"JAVA基础","slug":"JAVA基础","permalink":"https://yinhuidong.github.io/categories/JAVA%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"JAVA基础","slug":"JAVA基础","permalink":"https://yinhuidong.github.io/tags/JAVA%E5%9F%BA%E7%A1%80/"}]},{"title":"JAVA-SE新特性","slug":"JAVA基础/Java8新特性","date":"2022-01-12T00:19:55.244Z","updated":"2022-01-12T00:19:55.244Z","comments":true,"path":"2022/01/12/JAVA基础/Java8新特性/","link":"","permalink":"https://yinhuidong.github.io/2022/01/12/JAVA%E5%9F%BA%E7%A1%80/Java8%E6%96%B0%E7%89%B9%E6%80%A7/","excerpt":"","text":"一，Lambda表达式的使用1.举例：Comparator c= Comparator.comparingInt(o -&gt; o); 2.格式：-&gt;:Lambda操作符 左边叫做形参列表，其实就是接口中的抽象方法的形参列表 右边叫做Lambda体（重写的抽象方法的方法体） 3.关于Lambda表达式的使用 总结： -&gt; 左边：lambda形参列表的参数类型可以省略（类型推断），如果形参列表只有一个参数，（）可以省略。 -&gt; 右边：Lambda体应该使用一对{}包裹，如果Lambda体只有一条执行语句（可能是return语句），{}和return也可以省略。 要求接口只有一个抽象方法。 4.Lambda表达式的本质：作为函数式接口的实例。 5.如果一个接口中只声明了一个抽象方法，则此接口称为函数式接口。 可以使用注解@FunctionalInterface检查是否是一个函数式接口。 用匿名实现类表示的现在都可以用Lambda表达式表示。 语法格式12345678910111213141516171819202122232425262728293031/** * @author yhd * @createtime 2020/11/13 22:43 */public class DemoA &#123; //语法格式一：无参数，无返回值 @Test public void test1() &#123; Runnable run = () -&gt; System.out.println(&quot;语法格式一：无参数，无返回值&quot;); run.run(); &#125; //语法格式二：一个参数，无返回值 @Test public void test2() &#123; Consumer&lt;String&gt; consumer = args -&gt; System.out.println(args); consumer.accept(&quot;语法格式二：一个参数，无返回值&quot;); &#125; //语法格式三：Lambda 需要两个以上参数，多条执行语句，并且有返回值。 @Test public void test3() &#123; Comparator&lt;Integer&gt; comparable = (args1, args2) -&gt; &#123; System.out.println(&quot;args1 = &quot; + args1); System.out.println(&quot;args2 = &quot; + args2); return args1 &gt; args2 ? 1 : (args1 == args2 ? 0 : -1); &#125;; System.out.println(&quot;comparable.compare(1,2) = &quot; + comparable.compare(1, 2)); &#125;&#125; 二，方法引用Java内置四大核心函数式接口（要求能看懂） 消费性接口 Consumer void accept(T t) 供给型接口 Supplier T get() 函数型接口 Function&lt;T,R&gt; R apply(T t) 断定型接口 Predicate boolean test(T t) 使用情景：当要传递给Lambda体的操作，已经有实现的方法了，可以使用方法引用。 方法引用：本质上就是Lambda表达式，而Lambda表达式作为函数式接口的实例， 所以方法引用，也是函数式接口的实例。 使用格式： 类（对象）::方法名 具体分为如下三种情况： 对象：：非静态方法 类：：静态方法 类：：非静态方法 方法引用的使用要求，要求接口中的抽象方法的形参列表和返回值类型与方法引用 的方法的形参列表和返回值类型相同！ 语法格式1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * @author yhd * @createtime 2020/11/13 22:43 */public class DemoA &#123; private static void run() &#123; System.out.println(&quot;语法格式一：无参数，无返回值&quot;); &#125; private static int compare(Integer args1, Integer args2) &#123; System.out.println(&quot;args1 = &quot; + args1); System.out.println(&quot;args2 = &quot; + args2); return args1 &gt; args2 ? 1 : (args1 == args2 ? 0 : -1); &#125; //语法格式一：无参数，无返回值 @Test public void test1() &#123; Runnable run = DemoA::run; run.run(); &#125; //语法格式二：一个参数，无返回值 @Test public void test2() &#123; Consumer&lt;String&gt; consumer = System.out::println; consumer.accept(&quot;语法格式二：一个参数，无返回值&quot;); &#125; //语法格式三：Lambda 需要两个以上参数，多条执行语句，并且有返回值。 @Test public void test3() &#123; Comparator&lt;Integer&gt; comparable = DemoA::compare; System.out.println(&quot;comparable.compare(1,2) = &quot; + comparable.compare(1, 2)); &#125; //语法格式四： @Test public void test4()&#123; Comparator&lt;Integer&gt;comparator= Integer::compareTo; System.out.println(&quot;comparator.compare(1,2) = &quot; + comparator.compare(1, 2)); &#125;&#125; 三，构造器引用和方法引用类似，函数式接口的抽象方法的形参列表和构造器的形参列表一致，抽象方法的返回值类型就是构造器所属的类的类型。 数组引用 大家可以把数组看作一个特殊的类，则写法与构造器引用一致。 语法格式12345678910111213141516171819202122232425262728293031323334/** * @author yhd * @createtime 2020/11/14 0:18 */public class DemoB &#123; @Test public void test1()&#123; BiFunction&lt;String,String,Employee&gt; emp = Employee::new; Employee employee = emp.apply(&quot;AA&quot;, &quot;20&quot;); System.out.println(&quot;employee = &quot; + employee); &#125; @Test public void test2()&#123; Supplier&lt;Employee&gt; emp = Employee::new; Employee employee = emp.get(); System.out.println(&quot;employee.toString() = &quot; + employee.toString()); &#125; @Test public void test3()&#123; Function&lt;Integer,Integer[]&gt; params = Integer[]::new; Integer[] arrs = params.apply(5); System.out.println(Arrays.toString(arrs)); &#125;&#125;@Data@NoArgsConstructor@AllArgsConstructorclass Employee&#123; private String name; private String age;&#125; 四，Stream1.Stream关注的是数据的运算，与CPU打交道。 集合关注的是是数据的存储，与内存打交道。 2. ①Stream自己不会存储元素。 ②Stream不会改变源对象，相反，他们会返回一个持有结果的新Stream。 ③Stream操作是延迟执行的，这意味着它们会等到需要结果的时候才执行。 3.Stream的执行流程 ①Stream的实例化 ②一系列的中间操作（过滤，映射，。。。） ③终止操作 4.说明： ①一个中间链操作，对数据源的数据进行处理。 ②一旦执行终止操作，就执行中间操作链，并产生结果。之后，不会再被使用。 1.实例化1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/** * @author yhd * @createtime 2020/11/14 20:18 */public class DemoC &#123; /** * 流的4种实例化方式一： * 通过集合 * 顺序流和并行流的区别： * 顺序流按照集合的顺序挨个取出元素 * 并行流开多个线程去取，取出元素的顺序可能会发生变化 */ @Test public void test1()&#123; List&lt;Integer&gt; list=new ArrayList&lt;&gt;(); //创建一个顺序流 Stream&lt;Integer&gt; stream = list.stream(); //创建一个并行流 Stream&lt;Integer&gt; parallelStream = list.parallelStream(); &#125; /** * 流的4种实例化方式二： * 通过数组 * */ @Test public void test2()&#123; Integer []a=new Integer[]&#123;1,2,3,4,5,6&#125;; Stream&lt;Integer&gt; stream = Arrays.stream(a); &#125; /** * 流的4种实例化方式三： * 通过of */ @Test public void test3()&#123; Stream&lt;Integer&gt; stream = Stream.of(1, 2, 3); &#125; /** * 流的4种实例化方式四： * 无限流 */ @Test public void test4()&#123; //迭代 Stream.iterate(0,t-&gt;t+2).limit(10).forEach(System.out::print); //生成 Stream.generate(Math::random).limit(10).forEach(System.out::print); &#125;&#125; 2，中间操作1.过滤12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/** * @author yhd * @createtime 2020/11/14 21:20 */public class Demo4 &#123; private static List&lt;Person&gt; persons = new ArrayList&lt;&gt;(); private static Stream&lt;Person&gt; stream; static &#123; persons.add(new Person(&quot;1&quot;,&quot;马云&quot;,200.00)); persons.add(new Person(&quot;2&quot;,&quot;马化腾&quot;,200.00)); persons.add(new Person(&quot;3&quot;,&quot;李彦宏&quot;,200.00)); persons.add(new Person(&quot;4&quot;,&quot;刘强东&quot;,200.00)); persons.add(new Person(&quot;5&quot;,&quot;张朝阳&quot;,200.00)); persons.add(new Person(&quot;5&quot;,&quot;张朝阳&quot;,200.00)); stream = persons.stream(); &#125; //limit()截断取前面 @Test public void test1() &#123; stream.limit(2).forEach(System.out::println); &#125; //filter()过滤出需要的元素 @Test public void test2()&#123; stream.filter(Demo4::test).forEach(System.out::println); &#125; private static boolean test(Person person) &#123; return person.getSalary() &gt; 7000; &#125; //skip()截断取后面 @Test public void test3()&#123; stream.skip(2).forEach(System.out::println); &#125; //distinct()去除重复元素 @Test public void test4()&#123; stream.distinct().forEach(System.out::println); &#125;&#125;@Data@NoArgsConstructor@AllArgsConstructor@EqualsAndHashCodeclass Person implements Serializable &#123; private String id; private String name; private Double salary;&#125; 2.映射12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * @author yhd * @createtime 2020/11/14 22:04 */public class DemoD &#123; private static List&lt;Person&gt; persons = new ArrayList&lt;&gt;(); private static Stream&lt;Person&gt; stream; static &#123; persons.add(new Person(&quot;1&quot;, &quot;马云&quot;, 200.00)); persons.add(new Person(&quot;2&quot;, &quot;马化腾&quot;, 200.00)); persons.add(new Person(&quot;3&quot;, &quot;李彦宏&quot;, 200.00)); persons.add(new Person(&quot;4&quot;, &quot;刘强东&quot;, 200.00)); persons.add(new Person(&quot;5&quot;, &quot;张朝阳&quot;, 200.00)); stream = persons.stream(); &#125; /** * map和flatMap的区别： * list.add(list); * list.addAll(list); */ @Test public void test1() &#123; stream.map(Person::getName).filter(e -&gt; e.length() &gt; 2).forEach(System.out::println); &#125; @Test public void test2() &#123; Arrays.asList(&quot;aa&quot;, &quot;bb&quot;, &quot;cc&quot;).stream().map(DemoD::StringToStream).forEach(DemoD::accept); Arrays.asList(&quot;aa&quot;, &quot;bb&quot;, &quot;cc&quot;).stream().flatMap(DemoD::StringToStream).forEach(System.out::println); &#125; private static void accept(Stream&lt;Character&gt; e) &#123; e.forEach(System.out::println); &#125; public static Stream&lt;Character&gt; StringToStream(String str) &#123; List&lt;Character&gt; result = new ArrayList&lt;&gt;(); for (Character s : str.toCharArray()) result.add(s); return result.stream(); &#125;&#125; 3.排序1234567891011121314151617181920212223242526272829/** * @author yhd * @createtime 2020/11/14 22:41 */public class DemoE &#123; private static List&lt;Person&gt; persons = new ArrayList&lt;&gt;(); private static Stream&lt;Person&gt; stream; static &#123; persons.add(new Person(&quot;1&quot;, &quot;马云&quot;, 200.00)); persons.add(new Person(&quot;2&quot;, &quot;马化腾&quot;, 200.00)); persons.add(new Person(&quot;3&quot;, &quot;李彦宏&quot;, 200.00)); persons.add(new Person(&quot;4&quot;, &quot;刘强东&quot;, 200.00)); persons.add(new Person(&quot;5&quot;, &quot;张朝阳&quot;, 200.00)); persons.add(new Person(&quot;5&quot;, &quot;张朝阳&quot;, 200.00)); stream = persons.stream(); &#125; @Test public void test1() &#123; stream.sorted(DemoE::compare); &#125; private static int compare(Person o1, Person o2) &#123; return o1.getSalary() &gt; o2.getSalary() ? 1 : (o1.getSalary() == o2.getSalary() ? 0 : -1); &#125;&#125; 3,终止操作1.匹配与查找1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768/** * @author yhd * @createtime 2020/11/14 22:48 * 终止操作：匹配与查找 */public class DemoF &#123; private static List&lt;Person&gt; persons = new ArrayList&lt;&gt;(); private static Stream&lt;Person&gt; stream; static &#123; persons.add(new Person(&quot;1&quot;, &quot;马云&quot;, 200.00)); persons.add(new Person(&quot;2&quot;, &quot;马化腾&quot;, 200.00)); persons.add(new Person(&quot;3&quot;, &quot;李彦宏&quot;, 200.00)); persons.add(new Person(&quot;4&quot;, &quot;刘强东&quot;, 200.00)); persons.add(new Person(&quot;5&quot;, &quot;张朝阳&quot;, 200.00)); persons.add(new Person(&quot;5&quot;, &quot;张朝阳&quot;, 200.00)); stream = persons.stream(); &#125; //全部匹配返回true @Test public void test1() &#123; System.out.println(stream.allMatch(p -&gt; p.getSalary() &gt; 100)); &#125; //任意匹配返回true @Test public void test2() &#123; System.out.println(stream.anyMatch(p -&gt; p.getSalary() &gt; 100)); &#125; //都不匹配返回true @Test public void test3() &#123; System.out.println(stream.noneMatch(p -&gt; p.getSalary() &gt; 300)); &#125; //获取流里面第一个 @Test public void test4() &#123; System.out.println(stream.findFirst().get()); &#125; //获取流里面随机一个 @Test public void test5() &#123; System.out.println(stream.findAny()); &#125; //获取流里面元素个数 @Test public void test6() &#123; System.out.println(stream.count()); &#125; //获取最大 @Test public void test7() &#123; System.out.println(stream.max(Comparator.comparing(Person::getSalary)).get()); &#125; //获取最小 @Test public void test8() &#123; System.out.println(stream.min(Comparator.comparing(Person::getSalary)).get()); &#125;&#125; 2.归约12345678910111213141516171819202122232425public class DemoF &#123; private static List&lt;Person&gt; persons = new ArrayList&lt;&gt;(); private static Stream&lt;Person&gt; stream; static &#123; persons.add(new Person(&quot;1&quot;, &quot;马云&quot;, 200.00)); persons.add(new Person(&quot;2&quot;, &quot;马化腾&quot;, 200.00)); persons.add(new Person(&quot;3&quot;, &quot;李彦宏&quot;, 200.00)); persons.add(new Person(&quot;4&quot;, &quot;刘强东&quot;, 200.00)); persons.add(new Person(&quot;5&quot;, &quot;张朝阳&quot;, 200.00)); persons.add(new Person(&quot;5&quot;, &quot;张朝阳&quot;, 200.00)); stream = persons.stream(); &#125; @Test public void test9()&#123; System.out.println(Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9).stream().reduce(10, Integer::sum)); &#125; @Test public void test10()&#123; System.out.println(stream.map(Person::getSalary).reduce(0.0, Double::sum)); &#125; &#125; 3.收集collect(Collector()) Collector 接口中方法的实现决定了如何对流执行收集的操作。 Collectors实用类提供了很多静态方法，可以方便的创建常见的收集器实例。 1234567891011121314151617181920212223242526272829303132/** * @author yhd * @createtime 2020/11/14 22:48 * */public class DemoF &#123; private static List&lt;Person&gt; persons = new ArrayList&lt;&gt;(); private static Stream&lt;Person&gt; stream; static &#123; persons.add(new Person(&quot;1&quot;, &quot;马云&quot;, 200.00)); persons.add(new Person(&quot;2&quot;, &quot;马化腾&quot;, 200.00)); persons.add(new Person(&quot;3&quot;, &quot;李彦宏&quot;, 200.00)); persons.add(new Person(&quot;4&quot;, &quot;刘强东&quot;, 200.00)); persons.add(new Person(&quot;5&quot;, &quot;张朝阳&quot;, 200.00)); persons.add(new Person(&quot;5&quot;, &quot;张朝阳&quot;, 200.00)); stream = persons.stream(); &#125; //列出员工的所有工资 @Test public void test1()&#123; stream.map(Person::getSalary).collect(Collectors.toList()).forEach(System.out::println); &#125; //列出每个员工的工资 @Test public void test2()&#123; //System.out.println(stream.distinct().collect(Collectors.toMap(Person::getName, person -&gt; person))); System.out.println(stream.distinct().collect(Collectors.toMap(Person::getName, Person::getSalary))); &#125;&#125; 五，OptionalOptional提供了很多有用的方法，这样我们就不用显示的进行空值检测。 1.创建Optional类对象的方法： Optional.of(T t):创建一个Optional实例，t必须非空。 Optional.empty()：创建一个空的Optional实例。 Optional.ofNullable(T t):t可以为null。 2.判断Optional容器中是否包含对象： boolean isPresent() 判断是否包含对象 void ifPresent(Consumer&lt;? super T&gt; consumer):如果有值，就执行Consumer接口的实现代码，并且该值作为参数传给他。 3.获取Optional容器的对象 T get() 如果调用对象包含值，返回该值，否则抛出异常 T orElse(T other) 如果有值则将其返回，否则返回指定的other对象 T orElseGet(Supperlier&lt;? super T&gt; other) 如果有值则将其返回，否则返回由Supplier接口实现提供的对象。 T orElseThrow(Supperlier&lt;? super X&gt; exceptionSupplier) 如果有值则将其返回，否则抛出由Supplier接口实现提供的异常。 1234567891011121314151617181920212223242526/** * @author yhd * @createtime 2020/11/15 0:44 */public class DemoG &#123; @Test public void test() &#123; Girl girl = new Girl(); System.out.println(Optional.ofNullable(Optional.ofNullable(girl).orElseGet(Girl::new).getBoy()).orElseGet(Boy::new).getName()); &#125;&#125;@Data@NoArgsConstructor@AllArgsConstructorclass Girl &#123; private Boy boy;&#125;@Data@NoArgsConstructor@AllArgsConstructorclass Boy &#123; private String name;&#125;","categories":[{"name":"JAVA基础","slug":"JAVA基础","permalink":"https://yinhuidong.github.io/categories/JAVA%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"JAVA基础","slug":"JAVA基础","permalink":"https://yinhuidong.github.io/tags/JAVA%E5%9F%BA%E7%A1%80/"}]},{"title":"JAVA-WEB","slug":"JAVA基础/java基础之Web全套知识点梳理","date":"2022-01-12T00:19:55.244Z","updated":"2022-01-12T00:19:55.246Z","comments":true,"path":"2022/01/12/JAVA基础/java基础之Web全套知识点梳理/","link":"","permalink":"https://yinhuidong.github.io/2022/01/12/JAVA%E5%9F%BA%E7%A1%80/java%E5%9F%BA%E7%A1%80%E4%B9%8BWeb%E5%85%A8%E5%A5%97%E7%9F%A5%E8%AF%86%E7%82%B9%E6%A2%B3%E7%90%86/","excerpt":"","text":"一，HTML1.基本标签1）结构标签1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&lt;!DOCTYPE html&gt;&lt;!-- 文档声明（doctype） 用来告诉浏览器当前的网页版本 html5的文档声明 &lt;!DOCTYPE&gt;--&gt;&lt;html&gt;&lt;!-- html根标签,网页中所有内容都要写到根标签里面 --&gt; &lt;head&gt; &lt;!-- head中的内容不会在网页中直接出现，主要来帮助浏览器或搜索引擎来解析网页 --&gt; &lt;meta charset=&quot;utf-8&quot; /&gt; &lt;!-- 元数据，设置字符集，避免乱码问题 --&gt; &lt;!--字符编码： 字符集：编码和解码采用的规则。 乱码：编码和解码的字符集不一致。 iso8859-1 ascii utf-8 utf-16 gb2312 gbk 编码：数据在计算机都以二进制存储，所以一段文字存储到内存中，都需要转换为二进制编码 解码：当我们读取这段文字时，计算机会将编码转换为字符，供我们阅读。 --&gt; &lt;title&gt;主页&lt;/title&gt; &lt;!-- 网页的标题：title中的内容会显示在浏览器的标识栏上， 搜索引擎主要根据title中的内容来判断网页的主要内容 --&gt; &lt;!-- title标签的内容：会作为搜索结果上的超链接上的文字显示 --&gt; &lt;/head&gt; &lt;body&gt; &lt;!-- 网页的主体：网页中所有的可见内容都应该写在body里。 --&gt; &lt;!-- 作者：尹会东 时间：2020-02-07 描述： --&gt; &lt;!-- 属性：在标签中（开始或自结束标签）还可以设置属性 属性是一个名值对（x=y） 属性是用来设置标签中的内容如何显示 属性和标签名或其他属性应该使用空格隔开 有些属性有属性值，有些没有，如果有属性值，属性值应该用引号引起来 --&gt; &lt;h1&gt;&lt;font color=&quot;red&quot; size=&quot;7&quot;&gt;这是我的第一个网页&lt;/font&gt;&lt;/h1&gt; &lt;/body&gt;&lt;/html&gt; 2）排版标签12345678910111.注释标签：&lt;!--注释--&gt; 2.换行标签：&lt;br/&gt; 3.段落标签：&lt;p&gt;文本文字&lt;/p&gt; 特点：段与段之间行高 属性：align对齐方式 (left：左对齐 center：居中 right：右对齐) 4.水平线标签:&lt;hr/&gt; 属性:width：水平线的长度(两种:第一种:像素表示；第二种，百分比表示) size: 水平线的粗细 (像素表示，例如：10px) color:水平线的颜色 align:水平线的对齐方式(left：左对齐 center：居中 right：右对齐) 3）标题标签12345678910111213&lt;!-- 在网页中，html专门用来负责网页的结构 标题标签：h1-h6共六级标签 h1-h6重要性递减， h1的重要性仅次于title标签 一般情况下，一个网页只会有一个一级标签 在页面中独占一行的元素叫做块元素（block element --&gt;&lt;h1&gt;一级标题&lt;/h1&gt;&lt;h2&gt;二级标题&lt;/h2&gt;&lt;h3&gt;级标签&lt;/h3&gt;&lt;h4&gt;四级标签&lt;/h4&gt;&lt;h5&gt;五级标签&lt;/h5&gt;&lt;h6&gt;六级标签&lt;/h6&gt; 4）容器标签1234&lt;div&gt;&lt;/div&gt;:块级标签，独占一行，换行 &lt;span&gt;&lt;/span&gt;:行级标签，所内容都在同一行 作用： &lt;div&gt;&lt;/div&gt;:主要是结合css页面分块布局 &lt;span&gt;&lt;/span&gt;:进行友好提示信息 5）列表标签 12345678910无序列表标签： &lt;ul&gt;&lt;/ul&gt; 属性：type :个值，分别为 circle(空心圆) ,disc(默认,实心圆),square(黑色方块) 列表项：&lt;li&gt;&lt;/li&gt; 示例如下：&lt;ul type=&quot;square&quot;&gt;无序列表 &lt;li&gt;苹果&lt;/li&gt; &lt;li&gt;香蕉&lt;/li&gt; &lt;li&gt;橘子&lt;/li&gt; &lt;/ul&gt; 123456789有序列表标签：&lt;ol&gt;&lt;/ol&gt; 属性：type：1、A、a、I、i（数字、字母、罗马数字 列表项： &lt;li&gt;&lt;/li&gt; 示例如下： &lt;ol type=&quot;I&quot;&gt;序列表 &lt;li&gt;苹果&lt;/li&gt; &lt;li&gt;香蕉&lt;/li&gt; &lt;li&gt;橘子&lt;/li&gt; &lt;/ol&gt; 123456789定义列表dl (defination list) 定义列表 dt (defination title) 定义标题 dd (defination description) 定义描述 定义列表 &lt;dl&gt; &lt;dt&gt;苹果&lt;/dt&gt; &lt;dd&gt;苹果是一种水果，富含维生素C，美容养颜，吃了长寿....&lt;/dd&gt; &lt;/dl&gt; 12345678910111213141516列表嵌套 &lt;ul&gt; &lt;li&gt;咖啡&lt;/li&gt; &lt;li&gt;茶&lt;ul&gt;&lt;li&gt;红茶&lt;/li&gt; &lt;li&gt;绿茶&lt;ul&gt;&lt;li&gt;中国茶&lt;/li&gt; &lt;li&gt;非洲茶&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;牛奶&lt;/li&gt; &lt;/ul&gt; 5）图片标签1234567891011图片标签 &lt;img/&gt; 独立标签 属性:src：图片地址： 相对路径 （同一个网站） 绝对路径 （不同网站） width:宽度 height:高度 border:边框 align:对齐方式，代表图片与相邻的文本的相当位置（个属性值：top middle bottom alt:图片的文字说明 title:图片的悬停显示 hspace 和 vspace 设定图片边沿上下左右空白，以免文字或其它图片过于贴近 6）超链接标签1234567891011超链接可以是文本，也可以是一幅图像，您可以点击这些内容来跳转到新的文档或者当前文档中的某 个部分。 （1页面跳转 （2锚链接&lt;a&gt;文本或图片&lt;/a&gt; 属性：href:跳转页面的地址(跳转到外网需要添加协议) target:_self(自己) _blank(新页面，之前页面存在) _parent _top 默认_self _search相等于给页面起一个名字，如果再次打开时，如果页面存在，则不再打开新的页面。可以是任 意名字。name:名称，锚点(回到锚点: 顶部，底部，中间)，在访问锚点的书写格式:#name的值 7）表格标签表格由 标签来定义。每个表格均若干行（由 标签定义），每行被分割为若干单元格，由标签定义。 数据单元格可以包含文本、图片、列表、段落、表单、水平线、表格等等。 普通表格12345678910111213141516&lt;!--border:边框的宽度 bordercolor:边框的颜色 cellspacing:单元格的边距 width:宽度 height:高度--&gt; &lt;!--table的 align属性控制表格的对齐方式 left center right--&gt; &lt;!--td 的align属性控制内容对齐方式 left center right --&gt; &lt;!--td 的valign属性控制内容对齐方式 top middle bottom --&gt; &lt;table border=&quot;1&quot; bordercolor=&quot;red&quot; cellspacing=&quot;0&quot; align=&quot;center&quot; width=&quot;200&quot; height=&quot;100&quot;&gt; &lt;tr&gt;&lt;td&gt;学号&lt;/td&gt; &lt;td&gt;姓名&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt;&lt;td&gt;1&lt;/td&gt; &lt;td&gt;aa&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; 表格的表头1234567891011&lt;table border=&quot;1&quot; bordercolor=&quot;red&quot; cellspacing=&quot;0&quot; align=&quot;center&quot;&gt; &lt;caption&gt;学生表&lt;/caption&gt; &lt;tr&gt;&lt;th&gt;学号&lt;/th&gt; &lt;th&gt;姓名&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt;&lt;td&gt;1&lt;/td&gt; &lt;td&gt;aa&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; 表格的列合并12345678910111213141516&lt;table border=&quot;1&quot; bordercolor=&quot;red&quot; cellspacing=&quot;0&quot; align=&quot;center&quot;&gt; &lt;tr&gt;&lt;td colspan=&quot;4&quot; align=&quot;center&quot;&gt;学生表&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt;&lt;td&gt;学号&lt;/td&gt; &lt;td&gt;姓名&lt;/td&gt; &lt;td colspan=&quot;2&quot;&gt;各科成绩&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt;&lt;td&gt;1&lt;/td&gt; &lt;td&gt;aa&lt;/td&gt; &lt;td&gt;80&lt;/td&gt; &lt;td&gt;90&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt; 表格的行合并123456789101112131415161718192021&lt;table border=&quot;1&quot; bordercolor=&quot;red&quot; cellspacing=&quot;0&quot; align=&quot;center&quot;&gt; &lt;tr&gt; &lt;td colspan=&quot;4&quot; align=&quot;center&quot;&gt;学生表&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt;&lt;td&gt;学号&lt;/td&gt; &lt;td&gt;姓名&lt;/td&gt; &lt;td&gt;语文成绩&lt;/td&gt; &lt;td&gt;数学成绩&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt;&lt;td rowspan=&quot;2&quot;&gt;1&lt;/td&gt; &lt;td rowspan=&quot;2&quot;&gt;aa&lt;/td&gt; &lt;td&gt;80&lt;/td&gt; &lt;td&gt;90&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt;&lt;td&gt;80&lt;/td&gt; &lt;td&gt;90&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; 8）文本格式化标签12345678910&lt;b&gt; 定义粗体文本。 &lt;big&gt; 定义大号字。 &lt;em&gt; 定义着重文字。 &lt;i&gt; 定义斜体字。 &lt;small&gt; 定义小号字。 &lt;strong&gt; 定义加重语气。 &lt;sub&gt; 定义下标字。 &lt;sup&gt; 定义上标字。 &lt;ins&gt; 定义插入字。 &lt;del&gt; 定义删除字。 9）音视频标签1234567891011121314151617&lt;!-- 音乐视频播放 音视频文件引入时，默认情况下不允许用户控制 audio:引入外部音频文件的标签。 src：指定引入文件的路径。 controls=&quot;controls&quot;：出现播放窗口 autoplay=&quot;autoplay&quot;:自动播放 loop=&quot;loop&quot;循环播放 video:引入外部视频文件的标签。--&gt;&lt;audio src=&quot;../img/testMP3i18n.mp3&quot; controls=&quot;controls&quot; autoplay=&quot;autoplay&quot; loop=&quot;loop&quot;&gt;&lt;/audio&gt;&lt;!-- 除了用src指定播放路径，还可以用source来指定 --&gt;&lt;audio&gt; 对不起，您的浏览器不支持！ &lt;source=&quot;../img/testMP3i18n.mp3&quot;&gt;&lt;/source&gt;&lt;/audio&gt;&lt;video&gt;&lt;/video&gt; 2.基本标签综合案例效果图源码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;utf-8&quot;/&gt; &lt;title&gt;基本标签综合案例&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;!--顶部视图：第一个div块--&gt;&lt;div&gt; &lt;table&gt; &lt;tr&gt; &lt;td align=&quot;center&quot; width=&quot;22%&quot;&gt;&amp;nbsp;&amp;nbsp;千锋教育-稀有的坚持全 程面授品质的大型IT教育机构&lt;/td&gt; &lt;td width=&quot;1200px&quot;&gt;&lt;/td&gt; &lt;td align=&quot;center&quot; width=&quot;22%&quot;&gt;&lt;a&gt;&amp;nbsp;&amp;nbsp;好程序员特训营&amp;nbsp;&amp;nbsp;&lt;/a&gt; &lt;a&gt;&amp;nbsp;&amp;nbsp;扣丁学堂&amp;nbsp;&amp;nbsp;&lt;/a&gt; &lt;a&gt;&amp;nbsp;&amp;nbsp;练习我们&amp;nbsp;&amp;nbsp;&lt;/a&gt; &lt;a&gt;&amp;nbsp;&amp;nbsp;加入收藏&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt;&lt;/div&gt;&lt;br&gt; &lt;!--logo图标块--&gt;&lt;div&gt; &lt;table&gt; &lt;tr&gt; &lt;td align=&quot;center&quot; width=&quot;15%&quot;&gt;&lt;img src=&quot;http://www.qfedu.com/images/new_logo.png&quot;/&gt;&lt;/td&gt; &lt;td width=&quot;1000px&quot;&gt;&lt;/td&gt; &lt;td align=&quot;center&quot; width=&quot;15%&quot;&gt;&lt;img src=&quot;http://www.mobiletrain.org/images/index/nav_r_ico.png&quot;/&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;/td&gt; &lt;/tr&gt; &lt;/table&gt;&lt;/div&gt; &lt;!--菜单视图：第二个div块--&gt;&lt;div align=&quot;center&quot;&gt; &lt;hr&gt; &lt;span&gt;&amp;nbsp;&amp;nbsp;首页&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;课程培训 &amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;教学保障&amp;nbsp;&amp;nbsp;&lt;/span&gt; &lt;span&gt;&amp;nbsp;&amp;nbsp;免费视频&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;公开课 &amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;企业合作&amp;nbsp;&amp;nbsp;&lt;/span&gt; &lt;span&gt;&amp;nbsp;&amp;nbsp;免就业喜报&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;学员天 地&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;关于千锋&amp;nbsp;&amp;nbsp;&lt;/span&gt; &lt;span&gt;&amp;nbsp;&amp;nbsp;学员论坛&amp;nbsp;&amp;nbsp;&lt;/span&gt; &lt;hr&gt;&lt;/div&gt; &lt;!--导航视图：第四个div块--&gt;&lt;div align=&quot;right&quot;&gt;首页&amp;gt;课程培训&amp;gt;JavaEE列表&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/div&gt; &lt;!--分类名称：第五个div块--&gt;&lt;div&gt;&lt;h2&gt;&lt;strong&gt;课程培训&lt;/strong&gt;&lt;/h2&gt; &lt;h4&gt;&lt;strong&gt;共XX种课程视频&lt;/strong&gt;&lt;/h4&gt; &lt;hr&gt;&lt;/div&gt; &lt;!--分割图片--&gt;&lt;div&gt;&lt;img src=&quot;img/productlist.gif&quot; width=&quot;100%&quot; height=&quot;50px&quot;/&gt;&lt;/div&gt; &lt;!--图书展示块--&gt;&lt;div&gt; &lt;table width=&quot;100%&quot;&gt; &lt;tr&gt; &lt;td width=&quot;20%&quot; align=&quot;center&quot;&gt; &lt;div&gt;&lt;img src=&quot;bookcover/1.png&quot; width=&quot;230px&quot; height=&quot;320px&quot; border=&quot;1&quot;/&gt;&lt;br&gt; &lt;div&gt;书名：XX&lt;/div&gt; &lt;div&gt;售价：XX&lt;/div&gt; &lt;/div&gt; &lt;/td&gt; &lt;td width=&quot;20%&quot; align=&quot;center&quot;&gt; &lt;div&gt;&lt;img src=&quot;bookcover/2.png&quot; width=&quot;230px&quot; height=&quot;320px&quot; border=&quot;1&quot;/&gt;&lt;br&gt; &lt;div&gt;书名：XX&lt;/div&gt; &lt;div&gt;售价：XX&lt;/div&gt; &lt;/div&gt; &lt;/td&gt; &lt;td width=&quot;20%&quot; align=&quot;center&quot;&gt; &lt;div&gt;&lt;img src=&quot;bookcover/3.png&quot; width=&quot;230px&quot; height=&quot;320px&quot; border=&quot;1&quot;/&gt;&lt;br&gt; &lt;div&gt;书名：XX&lt;/div&gt; &lt;div&gt;售价：XX&lt;/div&gt; &lt;/div&gt; &lt;/td&gt; &lt;td width=&quot;20%&quot; align=&quot;center&quot;&gt; &lt;div&gt;&lt;img src=&quot;bookcover/4.png&quot; width=&quot;230px&quot; height=&quot;320px&quot; border=&quot;1&quot;/&gt;&lt;br&gt; &lt;div&gt;书名：XX&lt;/div&gt; &lt;div&gt;售价：XX&lt;/div&gt; &lt;/div&gt; &lt;/td&gt; &lt;td width=&quot;20%&quot; align=&quot;center&quot;&gt; &lt;div&gt;&lt;img src=&quot;bookcover/5.png&quot; width=&quot;230px&quot; height=&quot;320px&quot; border=&quot;1&quot;/&gt;&lt;br&gt; &lt;div&gt;书名：XX&lt;/div&gt; &lt;div&gt;售价：XX&lt;/div&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td width=&quot;20%&quot; align=&quot;center&quot;&gt; &lt;div&gt;&lt;img src=&quot;bookcover/6.png&quot; width=&quot;230px&quot; height=&quot;320px&quot; border=&quot;1&quot;/&gt;&lt;br&gt; &lt;div&gt;书名：XX&lt;/div&gt; &lt;div&gt;售价：XX&lt;/div&gt; &lt;/div&gt; &lt;/td&gt; &lt;td width=&quot;20%&quot; align=&quot;center&quot;&gt; &lt;div&gt;&lt;img src=&quot;bookcover/7.png&quot; width=&quot;230px&quot; height=&quot;320px&quot; border=&quot;1&quot;/&gt;&lt;br&gt; &lt;div&gt;书名：XX&lt;/div&gt; &lt;div&gt;售价：XX&lt;/div&gt; &lt;/div&gt; &lt;/td&gt; &lt;td \\width=&quot;20%&quot; align=&quot;center&quot;&gt; &lt;div&gt;&lt;img src=&quot;bookcover/8.png&quot; width=&quot;230px&quot; height=&quot;320px&quot; border=&quot;1&quot;/&gt;&lt;br&gt; &lt;div&gt;书名：XX&lt;/div&gt; &lt;div&gt;售价：XX&lt;/div&gt; &lt;/div&gt; &lt;/td&gt; &lt;td width=&quot;20%&quot; align=&quot;center&quot;&gt; &lt;div&gt;&lt;img src=&quot;bookcover/9.png&quot; width=&quot;230px&quot; height=&quot;320px&quot; border=&quot;1&quot;/&gt;&lt;br&gt; &lt;div&gt;书名：XX&lt;/div&gt; &lt;div&gt;售价：XX&lt;/div&gt; &lt;/div&gt; &lt;/td&gt; &lt;td width=&quot;20%&quot; align=&quot;center&quot;&gt; &lt;div&gt;&lt;img src=&quot;bookcover/10.png&quot; width=&quot;230px&quot; height=&quot;320px&quot; border=&quot;1&quot;/&gt;&lt;br&gt; &lt;div&gt;书名：XX&lt;/div&gt; &lt;div&gt;售价：XX&lt;/div&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/table&gt;&lt;/div&gt; &lt;!--底部栏的内容块--&gt;&lt;div&gt; &lt;table bgcolor=&quot;#efeedc&quot; width=&quot;100%&quot;&gt; &lt;tr&gt; &lt;td align=&quot;center&quot; width=&quot;15%&quot;&gt;&lt;img src=&quot;http://www.qfedu.com/images/new_logo.png&quot;/&gt;&lt;/td&gt; &lt;td width=&quot;1800px&quot;&gt;&lt;/td&gt; &lt;td align=&quot;center&quot; width=&quot;15%&quot;&gt; &lt;div&gt;联系我们&lt;/div&gt; &lt;div&gt;&amp;copy;2008&amp;nbsp;&amp;reg;千锋教育&amp;nbsp;All Rights&lt;/div&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/table&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 3.表单标签1）form标签1234567891011121314151617181920212223&lt;--常用属性：action：表示动作，值为服务器的地址，把表单的数据提交到该地址上处理 method:请求方式：get 和post enctype:表示是表单提交的类型 默认值：application/x-www-form-urlencoded 普通表单 multipart/form-data 多部分表单(一般用于文件上传) text/plain 普通文本 get:1.数据存在地址栏中,请求参数都在地址后拼接 path?name=张&amp;password=123456 ;2.不安全 3.效率高 4.get请求大小限制，不同浏览器不同，但是大约是2KB 使用情况：一般情况用于查询数据。 post： 1.地址栏没数据：请求参数单独处理。 2.安全可靠 3.效率低 4.post请求大小理论上无限。 使用情况：一般用于插入修改等操作 put delete header--&gt; 2) input标签1234567891011121314151617181920212223242526&lt;--type: 以下为type可能要取的值： 1.1 text 文本框 输入内容 1.2 password 密码框 密文或者掩码 1.3 radio 表示是单，name必须一致；value：提交给服务器的数据 表示同一组中只能选中一个( checked =&quot;checked&quot; 表示中) 1.4 checkbox 表示多 ，name必须一致， 表示同一组中可以选多个，返回值是个数组( checked =&quot;checked&quot; 表示中) 1.5 file ：表示上传控件 以上具输入性质的必须要name属性,初始开始写value表示是默认值(以后获取输入框的内容要根 据name来取) 以下钮不具输入性质，不需要name属性，但是钮上的文字提示使用value属性 1.6 submit 提交 1.7 reset 重置 1.9 image 图片提交钮 1.10 button 普通钮 1.11 hidden 表示隐藏域，该框的内容服务器需要，但是不想让用户知道(不想清楚的显示在界面 上) 1.12 email 表示邮箱格式的数据 name属性：表单元素名字，只name属性才能提交给服务器。 value属性：提交给服务器的数据 placeholder：提示信息 高级属性： disabled:禁用 readonly:只读 --&gt; 3)select标签下拉列表123456789&lt;select name=&quot;city&quot;&gt; &lt;!--select标签添加该属性multiple=&quot;multiple&quot;表示多 、size表示 显示的个数--&gt; &lt;!--option表示下拉列表项--&gt; &lt;option value=&quot;北京&quot;&gt;北京&lt;/option&gt; &lt;!--selected=&quot;selected&quot;表示中该项--&gt; &lt;option value=&quot;上海&quot; selected=&quot;selected&quot;&gt;上海&lt;/option&gt; &lt;option value=&quot;广州&quot;&gt;广州&lt;/option&gt; &lt;option value=&quot;杭州&quot;&gt;杭州&lt;/option&gt; &lt;/select&gt; 4)textarea元素123(文本域) 需要指定输入的区域位置大小 &lt;textarea cols=&quot;100&quot; rows=&quot;5&quot;&gt; 表示5行100列的区域可以输入内容，该元素没value属性 5)案例效果图 源码：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;form action=&quot;&quot; method=&quot;get&quot;&gt; &lt;table align=&quot;center&quot;&gt; &lt;caption&gt; &lt;h1&gt;注册&lt;/h1&gt; &lt;/caption&gt; &lt;tr&gt; &lt;td align=&quot;right&quot;&gt;用户名：&lt;/td&gt; &lt;td&gt; &lt;input type=&quot;text&quot; name=&quot;username&quot; value=&quot;bluesky&quot;/&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align=&quot;right&quot;&gt;密码：&lt;/td&gt; &lt;td&gt; &lt;input type=&quot;password&quot; name=&quot;password&quot;/&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align=&quot;right&quot;&gt;确认密码：&lt;/td&gt; &lt;td&gt; &lt;input type=&quot;password&quot; name=&quot;confirmpwd&quot;/&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align=&quot;right&quot;&gt;性别：&lt;/td&gt; &lt;td&gt; &lt;input type=&quot;radio&quot; name=&quot;sex&quot; value=&quot;男&quot;/&gt;男 &lt;input type=&quot;radio&quot; name=&quot;sex&quot; value=&quot;女&quot; checked=&quot;checked&quot;/&gt;女 &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align=&quot;right&quot;&gt;爱好：&lt;/td&gt; &lt;td&gt; &lt;input type=&quot;checkbox&quot; name=&quot;hobby&quot; value=&quot;篮球&quot; checked=&quot;checked&quot;/&gt;篮球 &lt;input type=&quot;checkbox&quot; name=&quot;hobby&quot; value=&quot;足球 &quot;/&gt;足球 &lt;input type=&quot;checkbox&quot; name=&quot;hobby&quot; value=&quot;乒乓球 &quot;/&gt;乒乓球 &lt;input type=&quot;checkbox&quot; name=&quot;hobby&quot; value=&quot;羽毛球 &quot;/&gt;羽毛球 &lt;/td&gt; &lt;/tr&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align=&quot;right&quot;&gt;上传头像：&lt;/td&gt; &lt;td&gt;&lt;input type=&quot;file&quot; name=&quot;upload&quot;/&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align=&quot;right&quot;&gt;居住地：&lt;/td&gt; &lt;td&gt;&lt;select name=&quot;city&quot;&gt; &lt;option value=&quot;北京&quot;&gt;北京&lt;/option&gt; &lt;option value=&quot;上海&quot; selected=&quot;selected&quot;&gt;上海&lt;/option&gt; &lt;option value=&quot;广州&quot;&gt;广州&lt;/option&gt; &lt;option value=&quot;杭州&quot;&gt;杭州&lt;/option&gt; &lt;/select&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align=&quot;right&quot;&gt;个人介绍：&lt;/td&gt; &lt;td&gt; &lt;textarea cols=&quot;100&quot; rows=&quot;5&quot;&gt; &lt;/textarea&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;/td&gt; &lt;td align=&quot;center&quot;&gt; &lt;input type=&quot;submit&quot; value=&quot;注册&quot;/&gt; &lt;input type=&quot;reset&quot; value=&quot;重置&quot;/&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/table&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 4.框架标签通过使用框架，你可以在同一个浏览器窗口中显示不止一个页面。每份HTML文档称为一个框架，并且每个 框架都独立于其他的框架。 使用框架的缺点： 开发人员必须同时跟踪更多的HTML文档 很难打印整张页面 frameset1234567891011121314151617181920框架结构标签，把body删掉，使用framset代替body 框架结构标签（&lt;frameset&gt;定义如何将窗口分割为框架 每个 frameset 定义了一系列行或列 rows/columns 的值规定了每行或每列占据屏幕的面积实例1 &lt;!--上下分割 上面20%,下面剩余部分--&gt; &lt;frameset rows=&quot;20%,*&quot;&gt; &lt;frame name=&quot;frame1&quot; src=&quot;top.html&quot;&gt; &lt;frame name=&quot;frame2&quot; src=&quot;bottom.html&quot; /&gt; &lt;/frameset&gt; 实例2 &lt;!--左右分割--&gt; &lt;frameset cols=&quot;20%,*&quot;&gt; &lt;frame name=&quot;frame1&quot; src=&quot;left.html&quot; /&gt; &lt;frame name=&quot;frame2&quot; src=&quot;right.html&quot; /&gt; &lt;/frameset&gt; 框架标签iframe 标签定义了放置在每个框架中的 HTML 文档。 12345678&lt;!-- 内联框架：用于向当前页面中引入其他页面。 src：指定要引入的网页路径。 frameborder:指定是否边框。 width： height： --&gt;&lt;iframe src=&quot;1.html&quot; width=&quot;800&quot; height=&quot;600&quot; frameborder=&quot;0&quot;&gt;&lt;/iframe&gt; 1234567&lt;--1. 不能将 &lt;body&gt;&lt;/body&gt; 标签与 &lt;frameset&gt;&lt;/frameset&gt; 标签同时使用 2. 假如一个框架有可见边框，用户可以拖动边框来改变它的大小。为了避免这种情况发生，可以在 &lt;frame&gt; 标签中加入：noresize=&quot;noresize&quot; 3 frameset的属性 frameborder=&quot;1|0|yes|no&quot; 表示是否边框 border=&quot;1&quot; 表示边框的粗细 bordercolor表示边框颜色--&gt; 案例效果图 源码123456789101112131415161718&lt;--1.框架标签不能和body同时出现 2.frameset: border去除框架标签的框 ，示例：border=&quot;0&quot; border=&quot;10px&quot; bordercolor=&quot;yellow&quot; 3.frame框大小不变：两种情况: 第一种：border =&quot;0&quot; 第二种： noresize=&quot;noresize&quot; 不改变大小 备注:scrolling是否显示滚动条 yes 显示 no 不显示 auto 如果内容高度超过屏幕高度直接显示滚动， 4. frame 是框，内容使用src来填充， 定位显示到指定位置： 使用name属性 例如：点击left.html的标签跳转内容显示在right.html 1.给right.html的frame添加name属性，方便定位。 2.在left.html中使用target目标定位，根据name名查找 --&gt; 123456789101112&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;iframe src=&quot;https://www.baidu.com/&quot; width=&quot;1200px&quot; height=&quot;200px&quot; align=&quot;center&quot; frameborder=&quot;1&quot; border=&quot;1&quot;&gt;&lt;/iframe&gt;&lt;iframe src=&quot;https://www.taobao.com/&quot; width=&quot;600px&quot; height=&quot;500px&quot; align=&quot;left&quot; frameborder=&quot;1&quot; border=&quot;1&quot;&gt;&lt;/iframe&gt;&lt;iframe src=&quot;https://www.jd.com/&quot; width=&quot;600px&quot; height=&quot;500px&quot; align=&quot;right&quot; frameborder=&quot;1&quot; border=&quot;1&quot;&gt;&lt;/iframe&gt;&lt;/body&gt;&lt;/html&gt; 5.其他标签和特殊字符1）其它标签1234567891011121314&lt;!--该网页的关键字--&gt;&lt;meta name=&quot;keywords&quot; content=&quot;keyword1,keyword2,keyword3&quot;&gt; &lt;!--该网页的描述--&gt;&lt;meta name=&quot;description&quot; content=&quot;this is my page&quot;&gt; &lt;!--该网页的编码--&gt;&lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot;&gt; html4.01 &lt;!--页面自动跳转,2秒后跳转到百度--&gt;&lt;meta http-equiv=&quot;refresh&quot; content=&quot;2;URL=https://www.baidu.com&quot;&gt; &lt;!--该网页的编码--&gt;&lt;meta charset=&quot;UTF-8&quot;&gt; html5 &lt;!-- href：引入css文件的地址--&gt;&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;./styles.css&quot;&gt; &lt;!--src：js的文件地址--&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;&quot;&gt;&lt;/script&gt; 2）特殊字符12345678&lt; 小于号 &gt; 大于号 &amp; 与字符 &quot; 引号 ® 己注册 © 版权 &amp;trade; 商标 空格 二，css31.css简介12345678910111213&lt;--CSS :层叠样式表(英文全称：Cascading Style Sheets)是一种用来表现HTML（标准通用标记语言的一个应 用或XML（标准通用标记语言的一个子集）等文件样式的计算机语言。CSS不仅可以静态地修饰网页，还可以配合各 种脚本语言动态地对网页各元素进行格式化。 多个样式可以层层覆盖叠加，如果不同的css样式对同一个html标签进行修饰，样式冲突的,应用优先级高的，不 冲突的样式规则共同作用。 作用1. 修饰美化html网页。 2. 外部样式表可以提高代码复用性从而提高工作效率。 3. html内容与样式表现分离，便于后期维护。 --&gt; 1234567&lt;--2.注意事项：a.如果值为若干单词，则要给值加引号；font-family: &quot;黑体&quot;,&quot;华文彩云&quot;,&quot;微软雅黑&quot;,&quot;arial&quot;; b.多个声明之间使用分号;分开； c.css对大小不敏感，如果涉及到与html文档一起使用时，class与id名称对大小写敏感 d.css注释/*...*/--&gt; 2.css使用方式12345678910111213141516171819202122232425262728293031323334353637383940414243/*内联方式把CSS样式嵌入到html标签当中，类似属性的用法，示例如下： &lt;div style=&quot;color:blue;font-size:50px&quot;&gt;This is my HTML page. &lt;/div&gt; 好处:可以单独设置某个元素样式，缺点：不利于样式重用 内部样式在head标签中使用style标签引入css,示例如下: &lt;style type=“text/css”&gt; //告诉浏览器使用css解析器去解析 div &#123;color:red; font-size:50px&#125; &lt;/style&gt; 好处：可以控制页面中多个元素样式，缺点：只能一个页面使用 外部样式将css样式抽成一个单独文件，谁用谁就引用，好处：多个页面可以同时使用。 示例如下：单独文件div.css: 内容示例：div&#123;color:green;font-size:50px&#125; 引用语句写在head标签内部， 链接式： &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=“div.css&quot;&gt;&lt;/link&gt; rel:代表当前页面与href所指定文档的关系 type:文件类型，告诉浏览器使用css解析器去解析 href:css文件地址 导入式： &lt;style type=&quot;text/css&quot;&gt; @import url(&quot;div.css&quot;) &lt;/style&gt; 该内容放在head标签中 备注:link和@import区别： 1.link所浏览器都支持，@import某些版本低的IE不支持 2.@import是等待html加载完成才加载，link解析到这个语句，就加载 3.@import不支持js动态修改 优先级：内联样式&gt;内部样式&gt;外部样式，就近原则。注意：内部样式和外部样式的位置*/ 3.css选择器1）基本选择器123456789101112131415161718192021222324252627282930313233343536373839404142&lt;--标签择器 在head中使用style标签引入在其中声明标签择器: html标签&#123;属性:属性值&#125;， 具体示例如下: &lt;style type=&quot;text/css&quot;&gt; span&#123;color: red;font-size: 100px&#125; &lt;/style&gt; id择器 给需要修改样式的html元素添加id属性标识，在head中使用style标签引入在其中声明id择器: #id 值&#123;属性:属性值&#125; 具体示例如下: 创建id择器： &lt;div id=&quot;s1&quot;&gt;hello,everyone!&lt;/div&gt; &lt;div id=&quot;s2&quot;&gt;hello,everyone!&lt;/div&gt; &lt;div id=&quot;s3&quot;&gt;hello,everyone!&lt;/div&gt; 根据id择器进行html文件修饰 &lt;style type=&quot;text/css&quot;&gt; #s1&#123;color: red;font-size: 100px&#125; #s2&#123;color: green;font-size: 100px&#125; #s3&#123;color: blue;font-size: 100px&#125; &lt;/style&gt; class择器 给需要修改样式的html元素添加class属性标识，在head中使用style标签引入在其中声明class择 器: .class名&#123;属性:属性值&#125;，具体示例如下: 创建class择器： &lt;div class=&quot;s1&quot;&gt;hello,everyone!&lt;/div&gt; &lt;div class=&quot;s2&quot;&gt;hello,everyone!&lt;/div&gt; &lt;div class=&quot;s3&quot;&gt;hello,everyone!&lt;/div&gt; 根据id择器进行html文件修饰 &lt;style type=&quot;text/css&quot;&gt; .s1&#123;color: purple;font-size: 100px&#125; .s2&#123;color: pink;font-size: 100px&#125; .s3&#123;color: yellow;font-size: 100px&#125; &lt;/style&gt; --&gt; 2）属性选择器12345678910111213141516171819202122232425262728&lt;--根据元素的属性及属性值来择元素。在head中使用style标签引入其中声明, 格式为:html标签[属性=&#x27;属性值&#x27;]&#123;css属性:css属性值;&#125; 或者html标签[属性]&#123;css属性:css属性值;&#125;， 具体示例如下: --&gt;body内容： &lt;form name=&quot;login&quot; action=&quot;#&quot; method=&quot;get&quot;&gt; &lt;font size=&quot;3&quot;&gt;用户名：&lt;font&gt; &lt;input type=“text&quot; name=&quot;username&quot; value=&quot;zhangsan&quot; /&gt; &lt;/br&gt; 密码： &lt;input type=&quot;password&quot; name=&quot;password&quot; value=&quot;123456&quot; /&gt; &lt;/br&gt; &lt;input type=&quot;submit&quot; value=&quot;登录&quot;&gt;&lt;/input&gt;&lt;/form&gt; head中书写： &lt;style type=&quot;text/css&quot;&gt; input[type=&#x27;text&#x27;] &#123; background-color: pink &#125;input[type=&#x27;password&#x27;] &#123; background-color: yellow &#125; font[size] &#123; color: green &#125;a[href] &#123; color: blue; &#125; &lt;/style&gt; 3）伪元素选择器1234567891011121314151617181920&lt;--主要是针对a标签语法： 静止状态 a:link&#123;css属性&#125; 悬浮状态 a:hover&#123;css属性&#125; 触发状态 a:active&#123;css属性&#125; 完成状态 a:visited&#123;css属性&#125; 具体示例如下： --&gt;&lt;a href=&quot;https://hao.360.cn/&quot;&gt;点我吧&lt;/a&gt; &lt;style type=&quot;text/css&quot;&gt; &lt;!--静止状态 --&gt; a:link &#123;color: red;&#125; &lt;!--悬浮状态 --&gt; a:hover &#123;color: green;&#125; &lt;!--触发状态 --&gt; a:active &#123;color: yellow;&#125; &lt;!--完成状态 --&gt; a:visited &#123;color: blue;&#125; &lt;/style&gt; 4）层级选择器123456789101112131415161718192021222324252627282930具体示例如下： 后代择器 div p&#123;...&#125; 表示div中的p标签，所的p,后代 div span&#123;....&#125; 表示div中的span标签，包括所的span，后代 子代择器 div&gt;span&#123;....&#125; 表示 div中一个span， span是子代 相邻兄弟 + 通用兄弟 ~ &lt;div id=&quot;div1&quot;&gt; &lt;div class=&quot;div11&quot;&gt;&lt;span&gt;span1-1&lt;/span&gt; &lt;/div&gt; &lt;div class=&quot;div12&quot;&gt; &lt;span&gt;span1-2&lt;/span&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;div2&quot;&gt; &lt;div id=&quot;div22&quot;&gt; &lt;span&gt;span2-1&lt;/span&gt; &lt;/div&gt; &lt;div id=&quot;div23&quot;&gt; &lt;span&gt;span2-2&lt;/span&gt; &lt;/div&gt; &lt;/div&gt; &lt;style type=&quot;text/css&quot;&gt; #div1 .div11&#123;color:red;&#125; #div1 .div12&#123;color:purple;&#125; .div2 #div22&#123;color:green;&#125; .div2 #div23&#123;color:blue;&#125; &lt;/style&gt; 5）其他选择器1234567891011121314全局择器 * &#123; font-size: 35px; margin: 0; padding: 0; &#125; 群组择器 h1,h2 &#123; background-color: red; &#125; 4.css属性1)文字和文本属性1234567891011121314151617181920212223242526272829303132333435363738394041/**文字属性 1&gt;.font-size:设置字体大小 2&gt;.font-family:设置文字的字体,常见的值为 :黑体，宋体，楷体等 3&gt;.font-style:规定斜体字,常见的值：normal - 文本正常显示 italic - 文本斜体显示 字体斜体 oblique - 文本倾斜显示 变形斜体 4&gt;.font-weight 属性设置文本的粗细。关键字 100 ~ 900 为字体指定了 9 级加粗度。 100 对应最细的字体变形，900 对应最粗的字体变形。 数字 400 等价于 normal，而 700 等价于 bold。 备注： 斜体（italic是对每个字母的结构一些小改动，来反映变化的外观。 倾斜（oblique文本则是正常竖直文本的一个倾斜版本。 通常情况下，italic 和 oblique 文本在 web 浏览器中看上去完全一样简写： font: italic bold 30px &quot;幼圆&quot;,&quot;黑体&quot;; （style weight size family swsf ）文本属性 1&gt;.color:设置文本颜色 2&gt;.text-indent:缩进元素中文本的首行,取值类型如下： 1》text-indent:5em;表示此段落第一行缩进5个字符的宽度 2》text-indent:20%:表示此段落第一行缩进父容器宽度的百分之二十 3&gt;.text-decoration: none:会关闭原本应用到一个元素上的所有装饰 underline: 添加下划线 overline:在文本的顶端画一个上划线 line-through:在文本中间画一个贯穿线 blink:让文本闪烁(无效果) 4&gt;.text-align:一个元素中的文本行互相之间的对齐方式,值left(左对齐)、right(右对齐) 和 center(居 中) 5&gt;.word-spacing: 字符之间的间隔 6&gt;.letter-spacing: 单词或者字母之间的间隔 7&gt;.line-height:设置行高 line-height:25px; */ 2）背景属性1234567891&gt;.background-color：设置背景颜色，默认透明 2&gt;.background-image:url(&quot;图片路径&quot;):设置背景图片 3&gt;.background-repeat:repeat-y:只在垂直方向都平铺 repeat-x:只在水平方向都平铺 repeat:在水平垂直方向都平铺 no-repeat:任何方向都不平铺 4&gt;.background-position: 改变图像在背景中的位置。top、bottom、left、right 和 center /*简写 没有顺序*/ background: red center no-repeat url(img/003.jpg); 12345678#d1&#123; width: 200px; height: 200px; background-image: url(&quot;/img/b.jpg&quot;); background-position:left top ,right bottom; background-size: 200px 200px; &#125; 3）列表属性12345678list-style-type:decimal;改变列表的标志类型 list-style-image: url(&quot;images/dog.gif&quot;);用图像表示标志 list-style-position: inside;确定标志出现在列表项内容之外还是内容内部 简写list-style: decimal url(img/001.png) inside; 去掉样式: list-style:none; list-style-type:none; 4）尺寸显示轮廓属性123456789101112width:设置元素的宽度 height:设置元素的高度 显示属性(display) display: none 不显示 block:块级显示 inline:行级显示 inline-block:行级块 轮廓（outline 绘制于元素周围的一条线，位于边框边缘的外围，可起到突出元素的作用。常用属性： outline-style:solid(实线)/dotted(虚线)/dashed(虚线，虚线的每段较长)/double(框为空心);设置轮廓 的样outline-color:red;设置轮廓的颜色 outline-width:10px设置轮廓的宽度 5）浮动属性12345678910111213141516171819202122232425262728293031&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;CSS语法&lt;/title&gt; &lt;style type=&quot;text/css&quot;&gt; div&#123; width:150px; height: 150px; &#125; #first&#123; background-color: pink; float:left; &#125; #second&#123; background-color: blue; float:left; &#125; #third&#123; background-color: green; float:right; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div id=&quot;first&quot;&gt;123&lt;/div&gt;&lt;div id=&quot;second&quot;&gt;456&lt;/div&gt;&lt;div id=&quot;third&quot;&gt;789&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 6）定位属性静态定位（默认定位方式）static相对定位(relative)相对于原来的位置偏移某个距离。元素仍保持其未定位前的形状，它原本所占的空间仍保留。 示例代码： 1234567891011121314151617181920&lt;html&gt;&lt;head&gt; &lt;style type=&quot;text/css&quot;&gt; h2.pos_left &#123; position: relative; left: -20px &#125; h2.pos_right &#123; position: relative; left: 20px &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h2&gt;这是位于正常位置的标题&lt;/h2&gt;&lt;h2 class=&quot;pos_left&quot;&gt;这个标题相对于其正常位置向左移动&lt;/h2&gt;&lt;h2 class=&quot;pos_right&quot;&gt;这个标题相对于其正常位置向右移动&lt;/h2&gt;&lt;p&gt;相对定位会照元素的原始位置对该元素进行移动。&lt;/p&gt;&lt;p&gt;样式 &quot;left:-20px&quot; 从元素的原始左侧位置减去 20 像素。&lt;/p&gt;&lt;p&gt;样式 &quot;left:20px&quot; 向元素的原始左侧位置增加 20 像素。&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; 绝对定位(absolute)1234567&lt;--元素框从文档流完全删除，并相对于其包含块定位。包含块可能是文档中的另一个元素或者是视窗 本身。元素原先在正常文档流中所占的空间会关闭，就好像元素原来不存在一样。元素定位后生成一个 块级框，而不论原来它在正常流中生成何种类型的框。 绝对定位的元素的位置相对于最近的已定位祖先元素，如果元素没有已定位的祖先元素，那么它的位 置相对于视窗本身。 --&gt; 123456789101112&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;utf-8&quot;/&gt; &lt;style type=&quot;text/css&quot;&gt; h2.pos_abs &#123; position: absolute; left: 100px; top: 150px &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h2 class=&quot;pos_abs&quot;&gt;这是带绝对定位的标题&lt;/h2&gt;&lt;p&gt;通过绝对定位，元素可以放置到页面上的任何位置。下面的标题距离页面左侧 100px，距离页面顶部 150px。&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; 固定定位(fixed)元素框的表现类似于将 position 设置为 absolute，不过其位置相对于视窗本身。 示例如下(网站左下角和右下角广告)： 1234567891011121314151617181920212223242526272829303132333435363738&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;&lt;/title&gt; &lt;style&gt; #left &#123; width: 200px; height: 200px; background-color: red; position: fixed; left: 0; bottom: 0; &#125; #right &#123; width: 200px; height: 200px; background-color: green; position: fixed; right: 0; bottom: 0; &#125; #middle &#123; width: 200px; height: 200px; background-color: blue; position: fixed; left: 0; bottom: 50%; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div id=&quot;left&quot;&gt;&lt;/div&gt;&lt;div id=&quot;right&quot;&gt;&lt;/div&gt;&lt;div id=&quot;middle&quot;&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 5.css盒子模型 12345678910111213141516171819202122232425262728293031323334353637383940&lt;--边框属性 border-style:边框样式，值以下情况： solid:实线 double:空心线 dashed:虚线组成的边框 dotted:圆点组成的边框 border-color:边框颜色 border-width:边框宽度 简写border: 1px solid red;外边距属性 margin:外间距,边框和边框外层的元素的距离 margin:四个方向的距离(top right bottom left) margin-top: margin-bottom: margin-left: margin-right: 内边距属性padding:内间距,元素内容和边框之间的距离((top right bottom left)) padding-left: padding-right: padding-top: padding-bottom: 盒子模型的实际的宽度：width+2*（padding+border+margin 盒子模型的实际的高度：height+2*（padding+border+margin--&gt; 思考思考1：如何实现div水平居中显示 12345678910111213141516&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;&lt;/title&gt; &lt;style type=&quot;text/css&quot;&gt; #container &#123; width: 1000px; height: 300px; background-color: beige; /*使用margin实现水平居中*/ margin: 0 auto; /*0 上下 0 左右 auto*/ &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div id=&quot;container&quot;&gt;xxx&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 思考2：如何实现div垂直居中显示 12345678910111213141516171819202122232425262728293031323334&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;&lt;/title&gt; &lt;style type=&quot;text/css&quot;&gt; #box &#123; height: 300px; background-color: #FFC0CB; /*弹性盒子*/ display: flex; /*垂直对齐*/ align-items: center; /*水平对齐*/ justify-content: center; &#125; #div1, #div2 &#123; width: 100px; height: 100px; &#125; #div1 &#123; background-color: #6495ED; &#125; #div2 &#123; background-color: #7FFFD4; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div id=&quot;box&quot;&gt; &lt;div id=&quot;div1&quot;&gt;第一个div&lt;/div&gt; &lt;div id=&quot;div2&quot;&gt;第二个div&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 注意，设为 Flex 布局以后，子元素的float、clear和vertical-align属性将失效。 6.css拓展属性 7.案例1）html页面1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;注册界面&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;/css/register.css&quot;/&gt; &lt;!--&lt;script src=&quot;/js/register.js&quot;/&gt;--&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&quot;class1&quot;&gt; &lt;div class=&quot;class2&quot;&gt; &lt;p class=&quot;p1&quot;&gt;新用户注册&lt;/p&gt; &lt;p class=&quot;p2&quot;&gt;USER REGISTER&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;class3&quot;&gt; &lt;div class=&quot;class5&quot;&gt; &lt;form action=&quot;#&quot; method=&quot;get&quot; id=&quot;form&quot;&gt; &lt;table align=&quot;center&quot; width=&quot;500&quot;&gt; &lt;tr&gt; &lt;td class=&quot;td_left&quot;&gt;&lt;label for=&quot;username&quot;&gt;用户名&lt;/label&gt;&lt;/td&gt; &lt;td class=&quot;td_right&quot;&gt;&lt;input type=&quot;text&quot; name=&quot;username&quot; id=&quot;username&quot; placeholder=&quot;请输入用户名&quot;/&gt; &lt;span id=&quot;s_username&quot; class=&quot;error&quot;&gt;&lt;/span&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td class=&quot;td_left&quot;&gt;&lt;label for=&quot;password&quot;&gt;密码&lt;/label&gt;&lt;/td&gt; &lt;td class=&quot;td_right&quot;&gt;&lt;input type=&quot;password&quot; name=&quot;password&quot; id=&quot;password&quot; placeholder=&quot;请输入密码&quot;/&gt; &lt;span id=&quot;s_password&quot; class=&quot;error&quot;&gt;&lt;/span&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td class=&quot;td_left&quot;&gt;&lt;label for=&quot;email&quot;&gt;邮箱&lt;/label&gt;&lt;/td&gt; &lt;td class=&quot;td_right&quot;&gt;&lt;input type=&quot;email&quot; name=&quot;email&quot; id=&quot;email&quot; placeholder=&quot;请输入邮箱&quot;/&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td class=&quot;td_left&quot;&gt;&lt;label for=&quot;name&quot;&gt;姓名&lt;/label&gt;&lt;/td&gt; &lt;td class=&quot;td_right&quot;&gt;&lt;input type=&quot;text&quot; name=&quot;name&quot; id=&quot;name&quot; placeholder=&quot;请输入姓名&quot;/&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td class=&quot;td_left&quot;&gt;&lt;label for=&quot;tel&quot;&gt;手机号&lt;/label&gt;&lt;/td&gt; &lt;td class=&quot;td_right&quot;&gt;&lt;input type=&quot;text&quot; name=&quot;tel&quot; id=&quot;tel&quot; placeholder=&quot;请输入手机号&quot;/&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td class=&quot;td_left&quot;&gt;&lt;label &gt;性别&lt;/label&gt;&lt;/td&gt; &lt;td class=&quot;td_right&quot;&gt;&lt;input type=&quot;radio&quot; name=&quot;gender&quot; value=&quot;男&quot; checked/&gt;男 &lt;input type=&quot;radio&quot; name=&quot;gender&quot; value=&quot;女&quot; /&gt;女 &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td class=&quot;td_left&quot;&gt;&lt;label for=&quot;birthday&quot;&gt;出生日期&lt;/label&gt;&lt;/td&gt; &lt;td class=&quot;td_right&quot;&gt;&lt;input type=&quot;date&quot; name=&quot;birthday&quot; id=&quot;birthday&quot; placeholder=&quot;请输入出生日期&quot;/&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;input type=&quot;submit&quot; value=&quot;注册&quot;/&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/form&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;class4&quot;&gt; &lt;a href=&quot;#&quot;&gt;已帐号？立即登录&lt;/a&gt; &lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 2）css样式123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475body &#123; background: url(&quot;/img/register.jpg&quot;) no-repeat center; background-size: 100% 200%; background-position: left top, right bottom;&#125;.class1 &#123; width: 900px; height: 500px; border: 5px solid #EEEEEE; background-color: white; margin: auto; margin-top: 15px;&#125;.class2 &#123; /*border: 1px solid red;*/ float: left; margin: 15px;&#125;.class3 &#123; border: 1px solid red; float: left; width: 500px;&#125;.class4 &#123; /*border: 1px solid red;*/ float: right; margin: 15px;&#125;a&#123; color: blue; font-size:20px;&#125;* &#123; margin: 0px; padding: 0px; box-sizing: border-box;&#125;.p1&#123; color: #FFD026; font-size:20px;&#125;.p2&#123; color: #A6A6A6; font-size:20px;&#125;.td_left&#123; width:100px; text-align: right; height: 45px;&#125;.td_right&#123; padding-left: 50px;&#125;#username,#password,#email,#name,#tel,#birthday&#123; width: 251px; height: 32px; border:1px solid #A6A6A6; padding-left: 15px;&#125;input[type=&quot;submit&quot;]&#123; background-color: yellow; width: 150px; height: 50px; border: 1px solid yellow; font-size: 30px;&#125;.error&#123; color: red; font-size:8px;&#125; 三，javascriptjs简介123456789概念：一门客户端脚本语言。 运行在客户端浏览器中，每一个浏览器都有js的解析引擎。 脚本语言：不需要编译，直接就可以被浏览器解析执行了。功能： 可以来增强用户和html页面交互过程，可以来控制html元素，让页面具有一些动态效果，增强用户的体验。发展历程： 1.C-- 2.LiveScript 3.JavaScript 1.基本语法1）与html结合方式和注释1234567891011121314①与html的结合方式 1.内部js &lt;script&gt; alert(&quot;hello World!&quot;); &lt;/script&gt; 2.外部js &lt;script src=&quot;/js/a.js&quot;&gt;&lt;/script&gt; 注意： 1.&lt;script&gt;标签可以定义在html页面的任何位置，但是定义的位置会影响执行顺序。 2.script标签可以定义多个。②注释 1.单行注释：// 2.多行注释：/**/ 2）数据类型1234567原始数据类型（基本数据类型） 1.number：数字。整数，小数，NaN 2.string：字符和字符串。没有字符的概念，都是字符串。 3.boolean：true和false 4.null：一个对象为空的占位符。 5.undefined：未定义。如果一个变量没有初始化值，默认为undefined。引用数据类型：对象 3）变量123456789101112131415161718192021222324变量：一小块存储数据的内存区域。Java语言是强类型的语言。Js是弱类型语言。 强类型：在开辟变量存储空间时，定义了空间将来存储的数据的数据类型。只能存储固定类型的数据。 弱类型：在开辟变量存储空间时，不定义空间将来存储的数据的数据类型，可以存放任意类型的数据。语法： var 变量名=变量值; //定义number类型的变量 var num1=1; var num2=1.2; var num3=NaN; //定义string类型 var str1=&quot;abc&quot;; var str2=&quot;def&quot;; //定义布尔 var flag=true; //定义null undefined var obj1=null; var obj2; //输出到页面上 document.write(num1+&quot;&lt;br/&gt;&quot;,num2+&quot;&lt;br/&gt;&quot;,num3+&quot;&lt;br/&gt;&quot;); document.write(str1+&quot;&lt;br/&gt;&quot;,str2+&quot;&lt;br/&gt;&quot;+&quot;&lt;br/&gt;&quot;); document.write(flag+&quot;&lt;br/&gt;&quot;); document.write(obj1+&quot;&lt;br/&gt;&quot;,obj2+&quot;&lt;br/&gt;&quot;); 4）运算符12345678910111213141516171819202122232425262728typeof：获得变量的类型 document.write(typeof num1+&quot;&lt;br/&gt;&quot;, typeof num2+&quot;&lt;br/&gt;&quot;,typeof num3+&quot;&lt;br/&gt;&quot;);一元运算符：只有一个运算数的运算符 ++，--，+（正号） 注意：在js中，如果运算数不是运算符所要求的类型，那么js引擎会自动地对运算数类型进行转换。 其他类型转number： string转number：按照字面值转换，如果字面值不是数字，转换为NaN。 boolean转为number：true转为1，false转为0；算术运算符： +，-，*，/，%赋值运算符： =，+=，-=，比较运算符： &gt;,&lt;,&lt;=,&gt;=,==,===(全等于) 比较方式： 1.类型相同，直接比较 字符串：按照字典顺序比较，按位注意比较，知道得出大小为止。 2.类型不同先进行类型转换，在比较。 ===：在比较之前先判断类型，如果类型不一致，直接返回false。逻辑运算符： &amp;&amp;，||，！ 其他类型转换为boolean： 1.number：0/NaN为假，其他为真。 2.string：除了空字符串，其他都为true。 3.null/undefined都为false。 4.对象：所有对象都为true。三元运算符： ？： 5）流程控制vs特殊语法1234567891011⑥流程控制 1.if...else 2.switch(在Js中，case可以接受任意类型) 3.while 4.do...while 5.for⑦特殊语法 1.如果一行只有一个语句，分号可以省略。 2.变量的定义可以省略var关键字。 用：定义一个局部变量。 不用：全局变量。 6）网页打印99乘法表1234567891011121314151617练习：网页打印99乘法表。document.write(&quot;&lt;table align=&#x27;center&#x27;&gt;&quot;);for(var i=1;i&lt;=9;i++)&#123; document.write(&quot;&lt;tr&gt;&quot;) for (var j=1;j&lt;=i;j++)&#123; document.write(&quot;&lt;td&gt;&quot;) document.write(i+&quot;*&quot;+j+&quot;=&quot;+(i*j)+&quot;&amp;nbsp;&amp;nbsp;&amp;nbsp;&quot;); document.write(&quot;&lt;/td&gt;&quot;) &#125; document.write(&quot;&lt;/tr&gt;&quot;)&#125;document.write(&quot;&lt;/table&gt;&quot;) &lt;style type=&quot;text/css&quot;&gt; td&#123; border: 1px solid; &#125; &lt;/style&gt; 2.基本对象1)function1234567891011121314151617181920212223242526272829描述一个方法或函数的对象。1.创建 1.var fun1=new Function (&quot;a&quot;,&quot;b&quot;,&quot;alert(a+b);&quot;); fun1(3,4); 2.function fun2(a,b)&#123; alert(a*b); &#125; fun2(3,4); 3.var fun3=function (a,b) &#123; alert(a-b); &#125; fun3(10,8);2.方法 1.fun1.length:输出参数个数。3.属性4.特点 1.方法定义时，形参的类型不用写。 2.方法是对象，如果定义名称相同的方法，会被覆盖。 3.在Js中，方法的调用只与方法名有关，和参数列表无关。 4.在方法声明中有一个隐藏的内置对象（数组），arguments,封装所有的实际参数。 //求任意个数的和 function add() &#123; var sum=0; for (var i = 0; i &lt;arguments.length ; i++) &#123; sum+=arguments[i]; &#125; return sum; &#125; alert(add(1,2,3,4,5,6,7,8,9)); 2)Array：数组对象123456789101112131.创建 1.var arr=new Array(元素列表); 2.var arr=new Array(默认长度/不写); 3.var arr=[元素列表];2.方法 join():将数组中的元素按照指定的分隔符拼接为字符串。 push():添加元素。3.属性 length：数组的长度。4.特点 1.Js中，数组元素的类型是可变的。 var arr=[1,&#x27;abc&#x27;,true]; 2.Js中，数组长度时可变的。 3)Date:日期对象123451.创建 var date=new Date();2.方法 date.toLocaleString();返回当前对象对应的本地时间格式。 getTime();时间戳。 4)Math：数学对象123456789101112131415161.创建 特点：不用创建，直接使用。 Math.方法名直接使用。2.方法： PI圆周率 random返回0-1之间的随机数。（包含0不包含1） abs(x) 返回 x 的绝对值 floor(x) 对 x 进行下舍入 max(x,y,z,...,n) 返回最高值 min(x,y,z,...,n) 返回最低值 pow(x,y) 返回 x 的 y 次幂 round(x) 把 x 四舍五入为最接近的整数 sqrt(x) 返回 x 的平方根 //产生1-100之间的随机数 var num=Math.floor(Math.random()*100)+1; document.write(num); 5)RegExp:正则表达式对象12345678910111213141516171819202122 1.正则表达式：定义字符串的组成规则。 1.单个字符：[] 如：[a],[ab],[a-zA-Z0-9_] 特殊符号代表特殊含义的单个字符： \\d：单个数字字符[0-9] \\w:单个单词字符[a-zA-Z0-9_] 2.量词符号： ?:表示出现0次或1次 *:表示出现0次或多次 +:表示出现1次或多次 &#123;m,n&#125;:表示m&lt;=数量&lt;=n m如果省略：代表最多n次 n如果省略，代表最少m次 3.开始或结束符号 ^\\w&#123;6,12&#125;$ 2.正则对象： 1.创建 1.var reg=new RegExp(正则表达式); 2.var reg= /正则表达式/; 2.方法 test(参数);验证指定字符串是否符合规范。校验中文var reg = /^[\\u4E00-\\u9FA5\\uf900-\\ufa2d·s]&#123;2,20&#125;$/;//验证姓名正则 6)Global1234567891011121314151.特点：全局对象，该对象中封装的方法不需要对象就可以直接调用。方法名（）；2.方法： 1. encodeURI():url编码 decodeURI()：url解码 encodeURIComponent():url编码 decodeURIComponent()：url解码 var code=encodeURI(&quot;尹会东&quot;) document.write(code);//10%E5%B0%B9%E4%BC%9A%E4%B8%9C 下面两个方法编码的字符更多。 2.parseInt()：将字符串转化为数字。 挨个判断每个字符是否时数字，直到不是数字为止，将前面的数字部分转换为number。 3.isNaN():判断一个值是否是NaN NaN参与的==比较全部为false 4.eval()：计算js字符串，并把它作为脚本代码来执行。 3.BOM：浏览器对象模型1) Window:窗口对象123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657581.创建2.方法 1.alert();窗口弹窗 2.confirm();确认取消对话框 3.prompt();可以进行输入的对话框 var flag=confirm(&quot;确认删除？&quot;);&#123; if (flag)&#123; alert(&quot;删除成功！&quot;); &#125;else&#123; alert(&quot;手别抖！&quot;); &#125; &#125; 4.open(); 5.close(); &lt;input type=&quot;button&quot; id=&quot;i1&quot; value=&quot;打开新窗口&quot;/&gt; &lt;input type=&quot;button&quot; id=&quot;i2&quot; value=&quot;close新窗口&quot;/&gt; &lt;script type=&quot;text/javascript&quot;&gt; var a= document.getElementById(&quot;i1&quot;); var newWin; a.onclick=function () &#123; newWin=open(&quot;https://www.baidu.com&quot;); &#125; var b=document.getElementById(&quot;i2&quot;); b.onclick=function () &#123; newWin.close(); &#125; &lt;/script&gt; 6.与定时器有关的方法 setTimeout:在指定的毫秒数后调用函数或计算表达式。 clearTimeout:取消上一个函数设置的timeout。 setInterval:按照指定的周期来调用函数或计算表达式。 clearInterval：取消上一个函数。 var id = setTimeout(&quot;alert(&#x27;爆炸啦！&#x27;)&quot;, 3000); clearTimeout(id); var id2 = setInterval(&quot;alert(&#x27;爆炸啦&#x27;)&quot;, 2000); clearInterval(id2);3.属性 1.获取其他BOM对象。 2.获取DOM对象。document4.特点 Window对象不需要创建，可以直接使用。window.方法名； window也可以省略。5.案例：轮播图 &lt;img src=&quot;/img/a.jpg&quot; id=&quot;img1&quot;/&gt; &lt;script type=&quot;text/javascript&quot;&gt; var flag=true; var fun=function () &#123; var a=document.getElementById(&quot;img1&quot;); if (flag)&#123; a.src=&quot;/img/b.jpg&quot;; flag=false; &#125;else&#123; a.src=&quot;/img/a.jpg&quot;; flag=true; &#125; &#125; setInterval(fun,2000); &lt;/script&gt; 2)History：历史记录对象12345678910111213141.创建：window.history/history2.属性： length：返回当前窗口历史列表中的url数量3.方法： back()加载history列表中的前一个url。 forward()加载history列表中的下一个url。 go()加载history列表中的某个具体页面。 &lt;input type=&quot;button&quot; id=&quot;btn&quot; value=&quot;获取历史记录&quot;/&gt; &lt;script&gt; var btn=document.getElementById(&quot;btn&quot;); btn.onclick=function () &#123; document.write(history.length); &#125; &lt;/script&gt; 3)Location：地址栏对象12345678910111213141516171.创建：window.location2.方法：reload（）刷新3.href：设置或返回当前url。 &lt;input type=&quot;button&quot; id=&quot;btn&quot; value=&quot;刷新&quot;/&gt; &lt;input type=&quot;button&quot; id=&quot;btn2&quot; value=&quot;百度&quot;/&gt; &lt;script type=&quot;text/javascript&quot;&gt; var btn=document.getElementById(&quot;btn&quot;); btn.onclick=function () &#123; location.reload(); &#125; var btn2=document.getElementById(&quot;btn2&quot;); btn2.onclick=function () &#123; location.href=&quot;https://www.baidu.com&quot;; &#125; &lt;/script&gt;4.案例：自动跳转首页 源码：test.html test.html 12345678910111213141516171819202122232425262728293031&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;电灯开关&lt;/title&gt; &lt;style type=&quot;text/css&quot;&gt; p &#123; text-align: center; &#125; span &#123; color: red; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;p&gt;&lt;span id=&quot;time&quot;&gt;5&lt;/span&gt;秒之后，自动跳转。。。&lt;/p&gt;&lt;script type=&quot;text/javascript&quot;&gt; var second = 5; var time = document.getElementById(&quot;time&quot;); function show() &#123; second--; time.innerHTML = second + &quot;&quot;; if (second == 0) &#123; location.href = &quot;https://www.baidu.com&quot;; &#125; &#125; setInterval(show, 1000);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 4.DOM:文档对象模型1)Document：文档对象123456789101112131.创建：window.document/document2.方法： 1.获取Element对象的方法 getElementById():根据id属性值获取元素对象。id属性值一般唯一。 getElementsByTagName():根据元素名称获取元素对象们。返回值是一个数组。 getElementsByClassName():根据class属性值获取元素对象。 getElementsByName();根据name属性值获取元素对象 2.创建其他Dom对象 createAttribute(name) createComment() createElement()创建标签 createTextNode() 3.属性 2) Element：元素对象123456789101.创建：通过document来创建2.方法： removeAttribute():删除属性 setAttribute()：设置属性 &lt;a&gt;去百度&lt;/a&gt; &lt;script&gt; var e_a=document.getElementsByTagName(&quot;a&quot;)[0]; e_a.setAttribute(&quot;href&quot;,&quot;https://www.baidu.com&quot;); e_a.removeAttribute(&quot;href&quot;); &lt;/script&gt; 3)Node：节点对象12345678910111213141.特点：所有的Dom对象都可以被认为是一个节点。 2.方法： CRUD appendChild() removeChild() replaceChild() 3.案例：删除子节点,添加子节点 a.html 4.属性： ParentNode：获取父节点。其他 Attribute：属性对象。 Text：文本对象。 Comment：注释对象。 案例 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt; &lt;style type=&quot;text/css&quot;&gt; div &#123; border: 1px solid red; &#125; #div1 &#123; width: 200px; height: 200px; &#125; #div2 &#123; width: 100px; height: 100px; &#125; #div3 &#123; width: 100px; height: 100px; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div id=&quot;div1&quot;&gt; &lt;div id=&quot;div2&quot;&gt;div2&lt;/div&gt; div1&lt;/div&gt;&lt;a href=&quot;javascript:void(0)&quot; id=&quot;del&quot;&gt;删除子节点&lt;/a&gt;&lt;a href=&quot;javascript:void(0)&quot; id=&quot;add&quot;&gt;添加子节点&lt;/a&gt;&lt;!--href这样写可以使超链接可以被点击，但是不发生页面跳转--&gt;&lt;script&gt; var a = document.getElementById(&quot;del&quot;); a.onclick = function () &#123; var div1 = document.getElementById(&quot;div1&quot;); var div2 = document.getElementById(&quot;div2&quot;); div1.removeChild(div2); &#125; var b = document.getElementById(&quot;add&quot;); b.onclick=function () &#123; var div1 = document.getElementById(&quot;div1&quot;); var div3=document.createElement(&quot;div&quot;); div3.setAttribute(&quot;id&quot;,&quot;div3&quot;); div1.appendChild(div3); &#125;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 4）案例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;动态表格&lt;/title&gt; &lt;style type=&quot;text/css&quot;&gt; table &#123; border: 1px solid; margin: auto; width: 500px; &#125; td, th &#123; text-align: center; border: 1px solid; &#125; div &#123; text-align: center; margin: 50px; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div&gt; &lt;input type=&quot;text&quot; id=&quot;id&quot; placeholder=&quot;请输入编号：&quot;/&gt; &lt;input type=&quot;text&quot; id=&quot;name&quot; placeholder=&quot;请输入姓名：&quot;/&gt; &lt;input type=&quot;text&quot; id=&quot;gender&quot; placeholder=&quot;请输入性别：&quot;/&gt; &lt;input type=&quot;button&quot; id=&quot;btn_add&quot; value=&quot;添加&quot;/&gt;&lt;/div&gt;&lt;table&gt; &lt;caption&gt;学生信息表&lt;/caption&gt; &lt;tr&gt; &lt;th&gt;编号&lt;/th&gt; &lt;th&gt;姓名&lt;/th&gt; &lt;th&gt;性别&lt;/th&gt; &lt;th&gt;操作&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;尹会东&lt;/td&gt; &lt;td&gt;男&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;javascript:void(0)&quot; onclick=&quot;delTr(this)&quot;&gt;删除&lt;/a&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;张贝贝&lt;/td&gt; &lt;td&gt;女&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;javascript:void(0)&quot; onclick=&quot;delTr(this)&quot;&gt;删除&lt;/a&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;刘淼&lt;/td&gt; &lt;td&gt;男&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;javascript:void(0)&quot; onclick=&quot;delTr(this)&quot;&gt;删除&lt;/a&gt; &lt;/td&gt; &lt;/tr&gt; &lt;script type=&quot;text/javascript&quot;&gt; /** * 添加： * 1.给添加钮绑定单击事件 * 2.获取文本框的内容 * 3.创建td，设置td的内容为文本框的内容 * 4.创建tr * 5.将td添加到tr中 * 6.获取table，将tr添加到table中 */ // document.getElementById(&quot;btn_add&quot;).onclick=function () &#123; // var id=document.getElementById(&quot;id&quot;).value; // var name=document.getElementById(&quot;name&quot;).value; // var gender=document.getElementById(&quot;gender&quot;).value; // var td_id=document.createElement(&quot;td&quot;); // var text_id=document.createTextNode(id); // td_id.appendChild(text_id); // var td_name=document.createElement(&quot;td&quot;); // var text_name=document.createTextNode(name); // td_name.appendChild(text_name); // var td_gender=document.createElement(&quot;td&quot;); // var text_gender=document.createTextNode(gender); // td_gender.appendChild(text_gender); // var td_a=document.createElement(&quot;td&quot;); // var e_a=document.createElement(&quot;a&quot;); // e_a.setAttribute(&quot;href&quot;,&quot;javascript:void(0)&quot;); // e_a.setAttribute(&quot;onclick&quot;,&quot;delTr(this)&quot;); // e_a.innerHTML=&quot;删除&quot;; // td_a.appendChild(e_a); // var tr=document.createElement(&quot;tr&quot;); // tr.appendChild(td_id); // tr.appendChild(td_name); // tr.appendChild(td_gender); // tr.appendChild(td_a); // var table=document.getElementsByTagName(&quot;table&quot;)[0]; // table.appendChild(tr); // &#125;; //使用innerHtml实现添加重构 document.getElementById(&quot;btn_add&quot;).onclick=function () &#123; var id = document.getElementById(&quot;id&quot;).value; var name = document.getElementById(&quot;name&quot;).value; var gender = document.getElementById(&quot;gender&quot;).value; var table=document.getElementsByTagName(&quot;table&quot;)[0]; table.innerHTML+=&quot;&lt;tr&gt;\\n&quot; + &quot; &lt;td&gt;&quot;+id+&quot;&lt;/td&gt;\\n&quot; + &quot; &lt;td&gt;&quot;+name+&quot;&lt;/td&gt;\\n&quot; + &quot; &lt;td&gt;&quot;+gender+&quot;&lt;/td&gt;\\n&quot; + &quot; &lt;td&gt;&lt;a href=\\&quot;javascript:void(0)\\&quot; onclick=\\&quot;delTr(this)\\&quot;&gt;删除&lt;/a&gt; &lt;/td&gt;\\n&quot; + &quot; &lt;/tr&gt;&quot;; &#125;; /** * 删除 * 1.确定电机的是哪一个超链接 * 利用this，代表当前超链接对象 * 2.怎么删除？ * removeChild() */ function delTr(obj) &#123; var table=obj.parentNode.parentNode.parentNode; var tr=obj.parentNode.parentNode; table.removeChild(tr); &#125; &lt;/script&gt;&lt;/table&gt;&lt;/body&gt;&lt;/html&gt; 5）html DOM1234567891011121314151617181920211.标签体的设置和获取：innerHtml 替换用=，追加用+=2.使用html元素对象的属性3.控制样式 1.通过引入css的style属性 var div1=document.getElementById(&quot;div1&quot;); div1.onclick=function () &#123; div1.style.border=&quot;1px solid red&quot;; div1.style.width=&quot;200px&quot;; div1.style.fontsize=&quot;20px&quot;; &#125; 2.通过定义classname属性 var div2=document.getElementById(&quot;div2&quot;); div2.onclick=function () &#123; div2.className=&quot;d1&quot;; &#125; .d1&#123; border:1px solid red; width:100px; height:100px; &#125; 6） xml DOM12345678910111213功能：控制html文档的内容。代码：获取页面的标签对象 Element document.getElementById();操作Element对象： 1.设置属性值 1.明确获取的对象 2.设置想要设置的属性 var a=document.getElementById(&quot;img.b&quot;); alert(a); a.src=&quot;/img/a.jpg&quot;; 2.修改标签体内容 var b=document.getElementById(&quot;h1.h&quot;); b.innerText=&quot;不是凄美刘强东&quot;; 5.事件1234567891011121314151617181920212223功能：某些组件被执行了某些操作后，触发某些代码的执行。属性： onchange HTML 元素内容改变(离开光标触发) onclick 用户点击 HTML 元素 onmouseover 光标移动到HTML元素 onmouseout 光标离开HTML元素 onkeydown 用户按下键盘按键 onload 浏览器已完成页面的加载如何绑定事件： 1.直接在html标签上，指定事件的属性，属性值就是Js代码。 &lt;img id=&quot;light&quot; src=&quot;/img/b.jpg&quot; onclick=&quot;fun()&quot;/&gt; &lt;script&gt; var fun=function () &#123; alert(&quot;别碰我！&quot;); &#125; &lt;/script&gt; 2.通过Js获取元素对象指定事件属性，设置一个函数。 var fun=function () &#123; alert(&quot;别碰我！&quot;); &#125; var light2=document.getElementById(&quot;light&quot;); light2.onclick=fun(); 案例：电灯开关1234567891011121314&lt;img id=&quot;i1&quot; src=&quot;/img/a.jpg&quot;/&gt; &lt;script type=&quot;text/javascript&quot;&gt; var img1=document.getElementById(&quot;i1&quot;); var flag =true;//代表灯是灭的。 img1.onclick=function () &#123; if (flag)&#123; img1.src=&quot;/img/b.jpg&quot;; flag=false; &#125;else&#123; img1.src=&quot;/img/a.jpg&quot;; flag=true; &#125; &#125; &lt;/script&gt; 6.案例表格全选 d.html123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;动态表格&lt;/title&gt; &lt;style type=&quot;text/css&quot;&gt; table &#123; border: 1px solid; margin: auto; width: 500px; &#125; td, th &#123; text-align: center; border: 1px solid; &#125; div &#123; text-align: center; margin: 50px; &#125; .out&#123; background-color: white; &#125; .over&#123; background-color: pink; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div&gt; &lt;input type=&quot;text&quot; id=&quot;id&quot; placeholder=&quot;请输入编号：&quot;/&gt; &lt;input type=&quot;text&quot; id=&quot;name&quot; placeholder=&quot;请输入姓名：&quot;/&gt; &lt;input type=&quot;text&quot; id=&quot;gender&quot; placeholder=&quot;请输入性别：&quot;/&gt; &lt;input type=&quot;button&quot; id=&quot;btn_add&quot; value=&quot;添加&quot;/&gt;&lt;/div&gt;&lt;table&gt; &lt;caption&gt;学生信息表&lt;/caption&gt; &lt;tr&gt; &lt;td&gt; 状态&lt;/td&gt; &lt;th&gt;编号&lt;/th&gt; &lt;th&gt;姓名&lt;/th&gt; &lt;th&gt;性别&lt;/th&gt; &lt;th&gt;操作&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;input type=&quot;checkbox&quot; name=&quot;cb&quot;&gt; &lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;尹会东&lt;/td&gt; &lt;td&gt;男&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;javascript:void(0)&quot; onclick=&quot;delTr(this)&quot;&gt;删除&lt;/a&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;input type=&quot;checkbox&quot; name=&quot;cb&quot;&gt; &lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;张&lt;/td&gt; &lt;td&gt;女&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;javascript:void(0)&quot; onclick=&quot;delTr(this)&quot;&gt;删除&lt;/a&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;input type=&quot;checkbox&quot; name=&quot;cb&quot;&gt; &lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;刘淼&lt;/td&gt; &lt;td&gt;男&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;javascript:void(0)&quot; onclick=&quot;delTr(this)&quot;&gt;删除&lt;/a&gt; &lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;div&gt; &lt;input type=&quot;button&quot; id=&quot;selectAll&quot; value=&quot;全&quot;/&gt; &lt;input type=&quot;button&quot; id=&quot;unSelectAll&quot; value=&quot;全部&quot;/&gt; &lt;input type=&quot;button&quot; id=&quot;selectRev&quot; value=&quot;反&quot;/&gt;&lt;/div&gt;&lt;script type=&quot;text/javascript&quot;&gt; /** * 添加： * 1.给添加钮绑定单击事件 * 2.获取文本框的内容 * 3.创建td，设置td的内容为文本框的内容 * 4.创建tr * 5.将td添加到tr中 * 6.获取table，将tr添加到table中 */ //使用innerHtml实现添加重构 document.getElementById(&quot;btn_add&quot;).onclick=function () &#123; var id = document.getElementById(&quot;id&quot;).value; var name = document.getElementById(&quot;name&quot;).value; var gender = document.getElementById(&quot;gender&quot;).value; var table=document.getElementsByTagName(&quot;table&quot;)[0]; table.innerHTML+=&quot;&lt;tr&gt;\\n&quot; + &quot;&lt;td&gt;&lt;input type=&#x27;checkbox&#x27; name=&#x27;cb&#x27;/&gt; &lt;/td&gt;&quot;+ &quot; &lt;td&gt;&quot;+id+&quot;&lt;/td&gt;\\n&quot; + &quot; &lt;td&gt;&quot;+name+&quot;&lt;/td&gt;\\n&quot; + &quot; &lt;td&gt;&quot;+gender+&quot;&lt;/td&gt;\\n&quot; + &quot; &lt;td&gt;&lt;a href=\\&quot;javascript:void(0)\\&quot; onclick=\\&quot;delTr(this)\\&quot;&gt;删除&lt;/a&gt; &lt;/td&gt;\\n&quot; + &quot; &lt;/tr&gt;&quot;; &#125;; /** * 删除 * 1.确定电机的是哪一个超链接 * 利用this，代表当前超链接对象 * 2.怎么删除？ * removeChild() */ function delTr(obj) &#123; var table=obj.parentNode.parentNode.parentNode; var tr=obj.parentNode.parentNode; table.removeChild(tr); &#125; window.onload=function () &#123; //全 document.getElementById(&quot;selectAll&quot;).onclick=function () &#123; var cbs=document.getElementsByName(&quot;cb&quot;); for (var i = 0; i &lt;cbs.length ; i++) &#123; cbs[i].checked=true; &#125; &#125; //全不 document.getElementById(&quot;unSelectAll&quot;).onclick=function () &#123; var cbs=document.getElementsByName(&quot;cb&quot;); for (var i = 0; i &lt;cbs.length ; i++) &#123; cbs[i].checked=false; &#125; &#125; //反 document.getElementById(&quot;selectRev&quot;).onclick=function () &#123; var cbs=document.getElementsByName(&quot;cb&quot;); for (var i = 0; i &lt;cbs.length ; i++) &#123; cbs[i].checked=!cbs[i].checked; &#125; &#125; var trs=document.getElementsByTagName(&quot;tr&quot;); for (var i = 0; i &lt;trs.length ; i++) &#123; //鼠标放到上面 trs[i].onmouseover=function () &#123; this.className=&quot;over&quot;; &#125; //鼠标拿下来 trs[i].onmouseout=function () &#123; this.className=&quot;out&quot;; &#125; &#125; &#125;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 表单校验 e.html123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;注册界面&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;/css/register.css&quot;/&gt; &lt;!--&lt;script src=&quot;/js/register.js&quot;/&gt;--&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&quot;class1&quot;&gt; &lt;div class=&quot;class2&quot;&gt; &lt;p class=&quot;p1&quot;&gt;新用户注册&lt;/p&gt; &lt;p class=&quot;p2&quot;&gt;USER REGISTER&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;class3&quot;&gt; &lt;div class=&quot;class5&quot;&gt; &lt;form action=&quot;#&quot; method=&quot;get&quot; id=&quot;form&quot;&gt; &lt;table align=&quot;center&quot; width=&quot;500&quot;&gt; &lt;tr&gt; &lt;td class=&quot;td_left&quot;&gt;&lt;label for=&quot;username&quot;&gt;用户名&lt;/label&gt;&lt;/td&gt; &lt;td class=&quot;td_right&quot;&gt;&lt;input type=&quot;text&quot; name=&quot;username&quot; id=&quot;username&quot; placeholder=&quot;请输入用户名&quot;/&gt; &lt;span id=&quot;s_username&quot; class=&quot;error&quot;&gt;&lt;/span&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td class=&quot;td_left&quot;&gt;&lt;label for=&quot;password&quot;&gt;密码&lt;/label&gt;&lt;/td&gt; &lt;td class=&quot;td_right&quot;&gt;&lt;input type=&quot;password&quot; name=&quot;password&quot; id=&quot;password&quot; placeholder=&quot;请输入密码&quot;/&gt; &lt;span id=&quot;s_password&quot; class=&quot;error&quot;&gt;&lt;/span&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td class=&quot;td_left&quot;&gt;&lt;label for=&quot;email&quot;&gt;邮箱&lt;/label&gt;&lt;/td&gt; &lt;td class=&quot;td_right&quot;&gt;&lt;input type=&quot;email&quot; name=&quot;email&quot; id=&quot;email&quot; placeholder=&quot;请输入邮箱&quot;/&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td class=&quot;td_left&quot;&gt;&lt;label for=&quot;name&quot;&gt;姓名&lt;/label&gt;&lt;/td&gt; &lt;td class=&quot;td_right&quot;&gt;&lt;input type=&quot;text&quot; name=&quot;name&quot; id=&quot;name&quot; placeholder=&quot;请输入姓名&quot;/&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td class=&quot;td_left&quot;&gt;&lt;label for=&quot;tel&quot;&gt;手机号&lt;/label&gt;&lt;/td&gt; &lt;td class=&quot;td_right&quot;&gt;&lt;input type=&quot;text&quot; name=&quot;tel&quot; id=&quot;tel&quot; placeholder=&quot;请输入手机号&quot;/&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td class=&quot;td_left&quot;&gt;&lt;label &gt;性别&lt;/label&gt;&lt;/td&gt; &lt;td class=&quot;td_right&quot;&gt;&lt;input type=&quot;radio&quot; name=&quot;gender&quot; value=&quot;男&quot; checked/&gt;男 &lt;input type=&quot;radio&quot; name=&quot;gender&quot; value=&quot;女&quot; /&gt;女 &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td class=&quot;td_left&quot;&gt;&lt;label for=&quot;birthday&quot;&gt;出生日期&lt;/label&gt;&lt;/td&gt; &lt;td class=&quot;td_right&quot;&gt;&lt;input type=&quot;date&quot; name=&quot;birthday&quot; id=&quot;birthday&quot; placeholder=&quot;请输入出生日期&quot;/&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;input type=&quot;submit&quot; value=&quot;注册&quot;/&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/form&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;class4&quot;&gt; &lt;a href=&quot;#&quot;&gt;已帐号？立即登录&lt;/a&gt; &lt;/div&gt;&lt;/div&gt;&lt;script&gt; window.onload=function () &#123; //1.给表单绑定onsubmit事件 document.getElementById(&quot;form&quot;).onsubmit=function () &#123; return checkUsername()&amp;&amp;checkPassword(); &#125; //局部刷新 document.getElementById(&quot;username&quot;).onblur=checkUsername; document.getElementById(&quot;password&quot;).onblur=checkPassword; &#125; //校验用户名 function checkUsername() &#123; //1.获取用户名的值 var username=document.getElementById(&quot;username&quot;).value; //2.定义正则表达式 var reg_username=/^\\w&#123;6,12&#125;$/; //3.判断值是否符合表达式 var flag=reg_username.test(username); var s_username=document.getElementById(&quot;s_username&quot;); //4.提示信息 if (flag)&#123; //用户名和密码后面加上空的span&lt;span id=&quot;s_username&quot; class=&quot;error&quot;&gt;&lt;/span&gt; s_username.innerHTML=&quot;&lt;img src=&#x27;/img/a.jpg&#x27; width=&#x27;25px&#x27; height=&#x27;35px&#x27; /&gt;&quot;; &#125; else&#123; s_username.innerHTML=&quot;用户名格式误&quot;; &#125; return flag; &#125; function checkPassword() &#123; //1.获取用户名的值 var password=document.getElementById(&quot;password&quot;).value; //2.定义正则表达式 var reg_password=/^\\w&#123;6,12&#125;$/; //3.判断值是否符合表达式 var flag=reg_password.test(password); var s_password=document.getElementById(&quot;s_password&quot;); //4.提示信息 if (flag)&#123; //用户名和密码后面加上空的span&lt;span id=&quot;s_username&quot; class=&quot;error&quot;&gt;&lt;/span&gt; s_password.innerHTML=&quot;&lt;img src=&#x27;/img/a.jpg&#x27; width=&#x27;25px&#x27; height=&#x27;35px&#x27; /&gt;&quot;; &#125; else&#123; s_password.innerHTML=&quot;密码格式误&quot;; &#125; return flag; &#125;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 四，Bootstrap1.BootStrap123456789101112131415161718192021222324252627282930313233343536373839404142434445464748BootStrap: 概念：一个前端开发的框架。 框架：一个半成品软件。开发人员可以在框架基础上，再进行开发，简化编码。 好处： 1.定义了很多css样式和js插件。直接可以使用这些样式和插件得到丰富的页面效果。 2.响应式布局。 同一套页面可以兼容不同分辨率的设备。 快速入门： 1.下载bootStrap 2.在项目中将这三个文件夹复制。 3.创建html页面，引入必要的资源文件。 &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge&quot;&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1&quot;&gt; &lt;!-- 上述3个meta标签*必须*放在最前面，任何其他内容都*必须*跟随其后！ --&gt; &lt;title&gt;Bootstrap 101 Template&lt;/title&gt; &lt;!-- Bootstrap --&gt; &lt;link href=&quot;/css/bootstrap.min.css&quot; rel=&quot;stylesheet&quot;&gt; &lt;!-- jQuery (Bootstrap 的所有 JavaScript 插件都依赖 jQuery，所以必须放在前边) --&gt; &lt;script src=&quot;/js/jquery-1.9.1.min.js&quot;&gt;&lt;/script&gt; &lt;!-- 加载 Bootstrap 的所有 JavaScript 插件。你也可以根据需要只加载单个插件。 --&gt; &lt;script src=&quot;/js/bootstrap.min.js&quot;&gt;&lt;/script&gt; 响应式页面：同一套页面可以兼容不同分辨率的设备。 实现：依赖于栅格系统：将一行平均分为12个格子。可以指定元素占几个格子。 步骤： 1.定义容器。相当于之前的定义table 容器分类： 1.container：两边有留白 2.container-fluid:100%宽度 2.定义行。相当于之前的定义tr row 3.定义元素。指定该元素在不同设备上，所占的格子数。col-设备代号-格子数目 设备代号：超小屏幕 手机 (&lt;768px) 小屏幕 平板 (≥768px) 中等屏幕 桌面显示器 (≥992px) 大屏幕 大桌面显示器 .col-xs- .col-sm- .col-md- .col-lg- 4.注意：一行的格子超过12个会自动换行。 栅格类属性可以向上兼容。 如果真实设备宽度小于了设置栅格类属性的设备代码最小值，会一个元素占满一行。 css样式和js插件： 全局css样式： 按钮 03 图片 04:任意尺寸都占100% 表格05 表单05 组件： 导航条06 分页条07 插件： 轮播图08 2.代码1234567891011121314151617181920212223&lt;!DOCTYPE html&gt;&lt;html lang=&quot;zh-CN&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge&quot;&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1&quot;&gt; &lt;!-- 上述3个meta标签*必须*放在最前面，任何其他内容都*必须*跟随其后！ --&gt; &lt;title&gt;Bootstrap 101 Template&lt;/title&gt; &lt;!-- Bootstrap --&gt; &lt;link href=&quot;/css/bootstrap.min.css&quot; rel=&quot;stylesheet&quot;&gt; &lt;!-- jQuery (Bootstrap 的所 JavaScript 插件都依赖 jQuery，所以必须放在前边) --&gt; &lt;script src=&quot;/js/jquery-1.9.1.min.js&quot;&gt;&lt;/script&gt; &lt;!-- 加载 Bootstrap 的所 JavaScript 插件。你也可以根据需要只加载单个插件。 --&gt; &lt;script src=&quot;/js/bootstrap.min.js&quot;&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;你好，世界！&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt; 补充：Tomcat知识点补充1)Tomcat部署项目的三种方式：1.直接放在webapps文件夹下就可以/hello：项目的访问路径 --&gt;虚拟目录 简化部署：直接将war包放到webapps文件夹下，Tomcat会自动解压缩。 2.server.xml配置&lt;!--部署项目 --&gt; &lt;Context docBase=&quot;D:\\hello&quot; path=&quot;/hello&quot; /&gt; 在server.xml的&lt;host&gt;标签内配置 docBase：代表项目存放的路径。path：虚拟目录。&lt;/host&gt; 缺点：配置一个项目可能导致整个Tomcat运行失败。 3.apache-tomcat-9.0.7\\conf\\Catalina\\localhost下创建任意名称的xml文件配置： &lt;Context docBase=&quot;D:\\hello&quot; /&gt; 虚拟目录就是xml文件的名称。（热部署） 2)静态项目和动态项目​ 目录结构的区别： java动态项目的目录结构： 五，servlet1.Servlet概念及创建123456789101112131415161718概念：运行在服务器端的小程序。*Servlet就是一个接口，定义了Java类被浏览器访问到（tomcat识别的规则。*我们自定义一个类，实现Servlet接口，重写方法。案例： 1.创建JavaEE项目， 2.定义一个类，实现Servlet接口 3.实现接口中的抽象方法 4.配置Servlet &lt;servlet&gt; &lt;servlet-name&gt;ServletDemo1&lt;/servlet-name&gt; &lt;servlet-class&gt;com.atguigu.servlet.ServletDemo1&lt;/servlet-class&gt; &lt;!--指定Servlet创建时间:默认值-1，&gt;0则服务器启动时创建--&gt; &lt;load-on-startup&gt;&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;ServletDemo1&lt;/servlet-name&gt; &lt;url-pattern&gt;/demo1&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; 2.执行原理与生命周期执行原理1234561.当服务器接收到客户端浏览器的请求后，会解析请求URL路径，获取访问的servlet资源路径。2.查找web.xml文件，是否对应的url-pattern标签体内容。3.如果，则再找到对应的&lt;servlet-class&gt;全类名。4.tomcat会将字节码文件加载到内存，并且创建其对象。5.调用其方法。 生命周期1234567891011121314151617 servlet中的生命周期方法： 1.被创建：执行init方法，只执行一次。 servlet什么时候被创建？可以配置执行servlet的创建时机。 在servlet标签下配置， 1.第一次访问时创建， &lt;load-on-startup&gt;负数&lt;/load-on-startup&gt; 2.在服务器启动时，创建 &lt;load-on-startup&gt;正数或0&lt;/load-on-startup&gt; *servlet的init方法，只执行一次，说明一个Servlet在内存中只存在一个对象，Servlet是单例的。 多个用户同时访问可能存在线程安全问题。 2.提供服务：执行service方法，执行多次。 每次访问Servlet时，Service方法都会被调用一次。 3.被销毁：执行destroy方法，只执行一次。 servlet被销毁时执行，服务器关闭时，servlet被销毁。 只服务器正常关闭时，才会执行destroy方法。--&gt; 3.servlet3.0基于注解的方式配置123456789101112131415161718192021222324252627@WebServlet(&quot;/servlet1&quot;)public class Servlet1 implements Servlet &#123; @Override public void init(ServletConfig servletConfig) throws ServletException &#123; &#125; @Override public ServletConfig getServletConfig() &#123; return null; &#125; @Override public void service(ServletRequest servletRequest, ServletResponse servletResponse) throws ServletException, IOException &#123; System.out.println(123456789); &#125; @Override public String getServletInfo() &#123; return null; &#125; @Override public void destroy() &#123; &#125;&#125; 4.Servlet接口的两个实现类vs urlpartten配置123456789101112131415Servlet 两个实现类： 1.GenericServlet（父类 将Servlet接口中的其他方法做了默认空实现，只将service方法作为抽象 将来定义servlet类时，只需要实现service方法即可。 2.HttpServlet（子类：对http协议的一种封装，简化操作 1.定义类继承httpServlet 2.重写doGet，doPost方法。 urlpartten配置： Servlet访问路径 1.一个servlet可以定义多个访问路径：@WebServlet(&#123;&quot;/d4&quot;,&quot;/d5&#125;) 2.路径定义规则： 1./xxx:路径匹配 2./xxx/xxx：多层路径，目录结构 3.*.do：拓展名匹配 5.Http协议123456789101112131415161718192021222324252627282930313233343536373839&lt;--概念：超文本传输协议*传输协议：定义了客户端与服务器发生通信时，传送数据的格式。特点： 1.基于TCP/IP的高级协议 2.默认端口号：80 3.基于请求/响应模型的：一次请求对应一次响应 4.无状态的：每次请求之间相互独立，不能交互数据。历史版本： 1.0：每一次请求都会建立新的连接 1.1：服用连接（传完等一会在断开，看看还有没有数据需要传送）请求消息数据格式 1.请求行 请求方式 请求url 请求协议/版本 GET /login.html HTTP/1.1 请求方式： HTTP协议七种请求方式：常用的两种： GET: 1.请求参数在请求行中，在url后。 2.请求的url长度限制的 3.不太安全 POST： 1.请求参数在请求体中 2.请求的url长度没限制 3.相对安全 2.请求头：客户端浏览器告诉服务器一些信息 请求头名称：请求头值 常见请求头： 1.User-Agent:浏览器告诉服务器，我访问你使用的浏览器版本信息 *可以在服务器获取该信息，解决浏览器兼容问题。 2.Referer:http://localhost/login.html *告诉服务器，当前请求从哪里来？ 作用：1.防止盗取链接 2.统计工作 3.请求空行 空行，就是用于分割POST请求的请求头，和请求体的。 4.请求体 GET方法没请求体，封装POST请求消息的请求参数的。 --&gt; 6.Request1)Request和Reponse执行原理：1234567891011propertiesRequest和Reponse执行原理： 1.tomcat服务器会根据请求url中的资源路径，创建对应的servlet对象。 2.tomcat服务器会创建request和response对象，request中封装请求消息数据。 3.tomcat将request和response两个对象传递给service方法，并且调用service方法 4.程序员通过request对象获取请求消息数据，通过response设置响应消息数据。 5.服务器在给浏览器做出响应之前，会从response对象中拿程序员设置的响应消息数据。 2)request对象继承体系结构1234567request对象继承体系结构 ServletRequest ----接口 HttpServletRequest ----接口 org.apache.catalina.connector.RequestFacade ---类（tomcat） 3)获取请求消息123456789101112131415161718192021222324252627282930313233&lt;--获取请求消息 1.获取请求行数据 1.获取请求方式：GET String getMethod(); *2.获取虚拟目录：/day14 String getContextPath() *3.获取servlet路径：/demo1 String getServletPath() 4.获取get方式请求参数：name=zhangsan String getQueryString() *5.获取请求URI/URL：day14/demo1 String getRequestURI():/day14/demo1 StringBuffer getRequestURL():https:localhost:8080/day14/demo1 *URL：统一资源定位符:https:localhost:8080/day14/demo1 中华人民共和国 *URI:统一资源标识符:/day14/demo1 共和国 6.获取协议及版本：HTTP/1.1 String getProtocol() 7.获取客户机IP地址 String getRemoteAddr() 2.获取请求头数据 *1.String getHeader(String name):通过请求头的名称获取请求头的值。 2.Enumeration&lt;String&gt; getHeaderNames():获取所的请求头名称 代码：Servlet3 3.获取请求体数据 请求体：只POST请求方式，才请求体，在请求体中封装了POST请求的请求参数。 步骤： 1.获取流对象 *BufferedReader getReader():获取字符输入流，只能操作字符数据 *ServletInputStream getInputStream（：获取字节输入流，可以操作所类型数据。 2.从流对象中拿数据 代码：Servlet4，register.html--&gt; Servlet31234567891011121314151617181920212223242526protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; response.setContentType(&quot;text/html;charset=utf-8&quot;); /** * 演示获取请求头数据 * user-agent */ String header = request.getHeader(&quot;user-agent&quot;); if (header.contains(&quot;Chrome&quot;))&#123; System.out.println(&quot;谷歌浏览器！&quot;); &#125;else if(header.contains(&quot;Firefox&quot;))&#123; System.out.println(&quot;这是火狐浏览器！&quot;); &#125; /** *演示获取请求头数据 * referer */ String referer = request.getHeader(&quot;referer&quot;); if (referer!=null)&#123; if (referer.contains(&quot;/JavaServlet&quot;))&#123; response.getWriter().write(&quot;播放电影...&quot;); &#125;else&#123; //盗链 response.getWriter().write(&quot;想看电影吗？来优酷&quot;); &#125; &#125;&#125; Servlet41234567protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; BufferedReader reader = request.getReader(); String line=null; while ((line=reader.readLine())!=null)&#123; System.out.println(line); &#125;&#125; register.html1234&lt;form action=&quot;/JavaServlet/servlet4&quot; method=&quot;post&quot;&gt; 用户名：&lt;input type=&quot;text&quot; name=&quot;username&quot; placeholder=&quot;请输入用户名&quot;/&gt;&lt;br/&gt; &lt;input type=&quot;submit&quot; value=&quot;提交&quot;/&gt;&lt;/form&gt; 4)获取请求参数vs 请求转发1234567891011121314151617其他功能1.获取请求参数通用方式（兼容GET和POST 1.String getParameter（String name:根据参数名获取参数值 2. String[] getParameterValues（String name:根据请求参数的名称获取参数值的数组 3.getParameterNames();获取所请求参数的名字 4.Map&lt;String,String[]&gt; getParameterMap():获取所参数的Map集合 5.代码：servlet5,register2.html2.请求转发 服务器内部资源跳转的方式。 1.步骤： 1.通过request对象获取请求转发器对象：getRequestDispatcher(String path) 2.使用RequestDispatcher对象来进行转发：forward(ServletRequest req,ServletResponse res) 2.特点： 1.浏览器地址栏路径不发生变化 2.只能转发到当前服务器内部资源中 3.转发是一次请求 3.代码： Servlet512345678910111213protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; request.setCharacterEncoding(&quot;utf-8&quot;); response.setContentType(&quot;text/html;utf-8&quot;); String username = request.getParameter(&quot;username&quot;); System.out.println(username); System.out.println(&quot;-------------------------------------&quot;); Enumeration&lt;String&gt; names = request.getParameterNames(); while (names.hasMoreElements())&#123; System.out.println(names.nextElement()); &#125; System.out.println(&quot;-------------------------------------&quot;);&#125; register2.html12345&lt;form action=&quot;/JavaServlet/servlet5&quot; method=&quot;post&quot;&gt; 用户名：&lt;input type=&quot;text&quot; name=&quot;username&quot; placeholder=&quot;请输入用户名&quot;/&gt;&lt;br/&gt; 密码：&lt;input type=&quot;text&quot; name=&quot;password&quot; placeholder=&quot;请输入密码&quot;/&gt;&lt;br/&gt; &lt;input type=&quot;submit&quot; value=&quot;提交&quot;/&gt;&lt;/form&gt; 转发测试123protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; request.getRequestDispatcher(&quot;/views/Login.html&quot;).forward(request,response);&#125; 123&lt;form action=&quot;/JavaServlet/servlet7&quot; method=&quot;post&quot;&gt; &lt;input type=&quot;submit&quot; value=&quot;已帐户？立即登录&quot;/&gt;&lt;/form&gt; 5)共享数据vs中文乱码问题1234567891011121314共享数据 *域对象：一个有作用范围的对象，可以在范围内共享数据。 *request域：代表一次请求的范围，一般用于请求转发的多个资源中共享数据 *方法： 1.void serAttribute(String name,Object obj);存储数据 2.Object getAttribute(String name);通过键获取值 3.void removeAttribute(String name);通过键移除键值对 *代码 获取servletContext 1.getServletContext();中文乱码问题：request.setCharacterEncoding(&quot;utf-8&quot;);response.setContentType(&quot;text/html;charset=utf-8&quot;); 6)案例：用户登录1*form表单的action写法：虚拟目录+servlet的资源路径 7)BeanUtils工具类12345678910111213141516171819202122232425262728293031323334353637383940414243BeanUtils工具类，简化封装数据*用于封装JavaBean的1.JavaBean：标准的Java类 1.要求： 1.类必须被public修饰 2.必须提供空参的构造器 3.成员变量必须使用private修饰 4.提供公共setter和getter方法 2.功能：封装数据2.概念： 1.成员变量：用private修饰的变量 2.属性：setter和getter方法截取后的产物 *例如：getUsername()--&gt;Username--&gt;username3.方法： 1.setProperty(User user,&quot;属性名&quot;,&quot;属性值&quot;) 2.getProperty() 3.populate(Object obj,Map map):将map集合的键值对信息，封装到对应的JavaBean对象中。4.BeanUtils的使用 Map&lt;String, String[]&gt; map = request.getParameterMap(); User user1 = new User(); //将请求参数封装进Bean BeanUtils.populate(user1,map); 代码实现： 封装请求参数最优解：导入request接收对象包3个protected void doGet(HttpServletRequest request, HttpServletResponse response)throws ServletException, IOException &#123; //获取编码 request.setCharacterEncoding(&quot;utf-8&quot;); response.setContentType(&quot;text/html;charset=utf-8&quot;); User user=new User(); try &#123; BeanUtils.populate(user, request.getParameterMap()); System.out.println(user.toString()); &#125; catch (IllegalAccessException e) &#123; // TODO Auto-generated catch block //e.printStackTrace(); &#125; catch (InvocationTargetException e) &#123; // TODO Auto-generated catch block //e.printStackTrace(); &#125; &#125; 7.Reponse1)HTTP协议123456789101112131415161718192021222324252627HTTP协议：1.请求消息：客户端发送给服务器的数据。2.响应消息：服务端发送给客户端的数据。 格式： 1.响应行 1.组成：协议/版本 响应状态码 状态码描述 2.响应状态码：服务器告诉客户端浏览器本次请求和相应的一个状态。 1.状态码都是3位数字。 2.分类： 1.1xxx:服务器接收客户端信息，但没接收完成，等待一段时间后，发送1xxx状态码。 2.2xxx：成功。代表：200 3.3xxx：重定向。代表：302（重定向，304（访问缓存 4.4xxx：客户端错误，请求路径出错 *代表： *404（请求路径没对应的资源 *405请求方式没对应的doXxxx方法 5.5xxx：服务器端错误。代表：500（服务器内部异常 2.响应头 1.格式：头名称：值 2.常见的响应头： 1.Content-type:服务器告诉客户端本次响应体数据格式以及编码格式 2.Content-disposition:服务器告诉客户端以什么格式打开响应体数据 *值： *in-line:1 默认值，在当前页面内打开 *attachment;file-name=xxx: 以附件形式打开响应体，文件下载。 3.响应空行 4.响应体 2)Response对象12345678910111213Response对象 *功能：设置响应消息 1.设置响应行 1.格式：HTTP/1.1 200 OK 2.设置状态码：setStatus(int sc) 2.设置响应头 1.setHeader(String name,String value) 3.设置响应体 *使用步骤： 1.获取输出流 *字符输出流：PrintWriter getWriter() *字节输出流:ServletOutputStream getOutputStream() 2.使用输出流，将数据输出到客户端浏览器 3)重定向12345678910111213141516重定向：资源跳转方式代码实现： //1.设置状态码为302 response.setStatus(302); //2.设置响应头location response.setHeader(&quot;location&quot;,&quot;/day15/responseDemo2&quot;); *//简单的重定向方法 response.sendRedirect(&quot;/day15/responseDemo2&quot;);*重定向的特点 1.地址栏发生变化 2.可以访问其他站点资源 3.重定向至少是两次请求，不能使用request对象来共享数据。*转发的特点 1.转发的地址栏路径不变 2.转发只能访问当前服务器下的资源 3.转发是一次请求。可以使用request来共享数据。 测试代码123&lt;form action=&quot;/JavaServlet/servlet8&quot; method=&quot;post&quot;&gt; &lt;input type=&quot;submit&quot; value=&quot;跳转&quot;/&gt;&lt;/form&gt; 12345protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; request.setCharacterEncoding(&quot;utf-8&quot;); response.setContentType(&quot;text/html;charset=utf-8&quot;); response.sendRedirect(request.getContextPath()+&quot;/servlet9&quot;);&#125; 12345protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; request.setCharacterEncoding(&quot;utf-8&quot;); response.setContentType(&quot;text/html;charset=utf-8&quot;); response.getWriter().write(&quot;我是servlet9&quot;);&#125; 4)相对路径与绝对路径1234567891011121314151617路径写法1.路径分类 1.相对路径：通过相对路径不可以确定唯一资源 *如：./index.html *不以/开头，以.开头 *规则：找到当前资源和目标资源的相对位置关系 *./：当前目录（默认可以省略 *../：上一级目录 2.绝对路径：通过绝对路径可以确定唯一资源 *如/:https://localhost/day15/responseDemo2 /day15/responseDemo2 *以/开头的路径 *规则：判断定义的路径是给谁用的？判断请求将来从哪里发出 *给客户端浏览器使用：需要加虚拟目录，（项目的访问路径） *建议虚拟目录动态获取：request.getContextPath() *&lt;a&gt;,&lt;form&gt;,重定向... *给服务器使用：不需要加虚拟目录 *转发路径 5)服务器输出数据到浏览器12345678910111213141516服务器输出字符数据到浏览器*步骤： 1.获取字符输入流：PrintWriter pw=response.getWriter(); 2.输出数据:pw.write();*注意： *乱码问题： 1.PrintWriter pw=response.getWriter();获取的流的默认编码是iso-8859-1 2.设置该流的默认编码 3.告诉浏览器响应体使用的编码 response.setContentType(&quot;text/html;charset=utf-8&quot;);服务器输出字节数据到浏览器1.获取字节输出流:ServletOutputStream ops = response.getOutputStream();2.输出数据:ops.write(); 6)验证码1.本质：图片 2.目的：防止恶意注册表单 案例：验证码注册。 register.html 12345678910111213141516171819202122232425262728293031&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;用户注册&lt;/title&gt;&lt;/head&gt;&lt;script type=&quot;text/javascript&quot;&gt; window.onload = function () &#123; var img = document.getElementById(&quot;img&quot;); img.onclick = function () &#123; var date = new Date().getTime(); img.src = &quot;/web_text/codeServlet?&quot; + date; &#125; var change = document.getElementById(&quot;change&quot;); change.onclick = function () &#123; var date = new Date().getTime(); img.src = &quot;/web_text/codeServlet?&quot; + date; &#125; &#125;&lt;/script&gt;&lt;body&gt;&lt;form action=&quot;/web_text/registerServlet&quot; method=&quot;post&quot;&gt; 用户名&lt;input type=&quot;text&quot; name=&quot;name&quot; placeholder=&quot;请输入用户名&quot;/&gt;&lt;br&gt; 密码&lt;input type=&quot;password&quot; name=&quot;password&quot; placeholder=&quot;请输入密码&quot;/&gt;&lt;br/&gt; 确认密码&lt;input type=&quot;password&quot; name=&quot;repwd&quot; placeholder=&quot;请确认您的密码&quot;/&gt;&lt;br/&gt; 验证码&lt;input type=&quot;text&quot; name=&quot;code&quot; placeholder=&quot;请输入验证码&quot;/&gt; &lt;img id=&quot;img&quot; src=&quot;/web_text/codeServlet&quot;/&gt;&lt;a id=&quot;change&quot; href=&quot;javascript:void(0)&quot;&gt;看不清？换一张&lt;/a&gt;&lt;br/&gt; &lt;input type=&quot;submit&quot; value=&quot;注册&quot;/&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; registerServlet12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package com.atguigu.web.servlet;import com.atguigu.bean.User;import com.atguigu.service.UserService;import javax.servlet.ServletException;import javax.servlet.annotation.WebServlet;import javax.servlet.http.HttpServlet;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import javax.servlet.http.HttpSession;import java.io.IOException;/** * @author yinhuidong * @createTime 2020-02-12-16:41 */@WebServlet(&quot;/registerServlet&quot;)public class RegisterServlet extends HttpServlet &#123; protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; doGet(request,response); &#125; protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException&#123; request.setCharacterEncoding(&quot;utf-8&quot;); response.setContentType(&quot;text/html;charset=utf-8&quot;); HttpSession session = request.getSession(); String code = (String) session.getAttribute(&quot;code&quot;); String code1 = request.getParameter(&quot;code&quot;); if (code.equalsIgnoreCase(code1))&#123; String name = request.getParameter(&quot;name&quot;); UserService service = new UserService(); User user = service.getUserMessage(name); if (user!=null)&#123; response.getWriter().write(&quot;用户名已经存在！&quot;); return ; &#125;else&#123; String password = request.getParameter(&quot;password&quot;); String repwd = request.getParameter(&quot;repwd&quot;); if (repwd.equals(password))&#123; int i = service.addUser(new User(name, password)); if (i&gt;0)&#123; response.sendRedirect(request.getContextPath()+&quot;/views/Login.html&quot;); &#125;else&#123; response.getWriter().write(&quot;注册失败！&quot;); return ; &#125; &#125;else&#123; response.getWriter().write(&quot;两次密码不一致！&quot;); return ; &#125; &#125; &#125;else&#123; response.getWriter().write(&quot;验证码错误！&quot;); return ; &#125; &#125;&#125; codeServlet12345678910111213141516171819202122232425262728293031package com.atguigu.web.servlet;import cn.dsna.util.images.ValidateCode;import javax.servlet.ServletException;import javax.servlet.annotation.WebServlet;import javax.servlet.http.HttpServlet;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import javax.servlet.http.HttpSession;import java.io.IOException;/** * @author yinhuidong * @createTime 2020-02-12-17:00 */@WebServlet(&quot;/codeServlet&quot;)public class CodeServlet extends HttpServlet &#123; protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; doGet(request,response); &#125; protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; request.setCharacterEncoding(&quot;utf-8&quot;); response.setContentType(&quot;text/html;charset=utf-8&quot;); ValidateCode code = new ValidateCode(200,50,4,20); HttpSession session = request.getSession(); session.setAttribute(&quot;code&quot;,code.getCode()); code.write(response.getOutputStream()); &#125;&#125; 发邮件的工具类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172package com.atguigu.java.utils;import com.sun.mail.util.MailSSLSocketFactory;import javax.mail.Message;import javax.mail.MessagingException;import javax.mail.Session;import javax.mail.Transport;import javax.mail.internet.InternetAddress;import javax.mail.internet.MimeMessage;import java.util.Properties;/** * @author yinhuidong * @createTime 2020-02-23-22:25 */public class SendEmailUtils &#123; public static void sendEmail(String code, String email) &#123; Transport ts = null; Properties prop=null; try &#123; prop = new Properties(); // 设置邮件服务器主机名 prop.setProperty(&quot;mail.host&quot;, &quot;smtp.qq.com&quot;); // 发送服务器需要身份验证 prop.setProperty(&quot;mail.smtp.auth&quot;, &quot;true&quot;); // 发送邮件协议名称 prop.setProperty(&quot;mail.transport.protocol&quot;, &quot;smtp&quot;); // 开启SSL加密，否则会失败 MailSSLSocketFactory sf = new MailSSLSocketFactory(); sf.setTrustAllHosts(true); prop.put(&quot;mail.smtp.ssl.enable&quot;, &quot;true&quot;); prop.put(&quot;mail.smtp.ssl.socketFactory&quot;, sf); // 创建session Session session = Session.getInstance(prop); // 通过session得到transport对象 ts = session.getTransport(); // 连接邮件服务器：邮箱类型，帐号，授权码代替密码（更安全 // 后面的字符是授权码，用qq密码反正我是失败了（用自己的，别用我的，这个号是我瞎编的，为了。。。。 ts.connect(&quot;smtp.qq.com&quot;, &quot;1972039773&quot;, &quot;sqnsaywwdnfddgie&quot;); // 创建邮件 Message message = createSimpleMail(session, code, email); // 发送邮件 ts.sendMessage(message, message.getAllRecipients()); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; ts.close(); &#125; catch (MessagingException e) &#123; e.printStackTrace(); &#125; &#125; &#125; public static MimeMessage createSimpleMail(Session session, String code, String email) throws Exception &#123; // 创建邮件对象 MimeMessage message = new MimeMessage(session); // 指明邮件的发件人 message.setFrom(new InternetAddress(&quot;1972039773@qq.com&quot;)); // 指明邮件的收件人，现在发件人和收件人是一样的，那就是自己给自己发 message.setRecipient(Message.RecipientType.TO, new InternetAddress(email)); // 邮件的标题 message.setSubject(&quot;您好，请查看验证码&quot;); // 邮件的文本内容 message.setContent(code, &quot;text/html;charset=UTF-8&quot;); // 返回创建好的邮件对象 return message; &#125;&#125; 生成uuid12345678910111213141516171819package com.atguigu.java.utils;import org.junit.Test;import java.util.UUID;/** * 产生UUID随机字符串工具类 */public final class UuidUtil &#123; private UuidUtil()&#123;&#125; public static String getUuid()&#123; return UUID.randomUUID().toString().replace(&quot;-&quot;,&quot;&quot;); &#125; /** * 测试 */&#125; 8.ServletContext对象1234567891011121314151617181920212223242526ServletContext对象1.概念：代表整个web应用，可以和程序的容器(服务器)来通信。2.获取： 1.通过request对象获取 request.getServletContext(); 2.通过HttpServlet获取 this.getServletContext(); 3.功能： 1.获取MITE类型 *MIME类型：在互联网通信过程中定义的一种文件数据类型 *格式： 大类型/小类型 text/html image/jpeg *获取：String getMimeType(String file) 2.域对象：共享数据 1.setAttribute(String name.Object value) 2.getAttribute(String name) 3.removeAttribute(String name) *ServletContext对象作用范围：所用户所请求的数据 3.获取文件的真实（服务器路径 ServletContext context=this.getServletContext(); 获取文件的服务器路径 1.web目录下资源访问 String realPath=context.getRealPath(&quot;/b.txt&quot;); 2.WEB-INF下的资源访问 String c=context.getRealPath(&quot;/WEB-INF/c.txt&quot;); 3.src目录下的资源访问 String a=context.getRealPath(/WEB-INF/classes/a.txt); 9.文件下载12345678910111213141516171819202122案例： 文件下载需求： 1.页面显示超链接 2.点击链接后弹出下载提示框 3.完成图片文件下载 分析： 1.超链接指向的资源如果能被服务器解析，则在浏览器中显示，如果不能解析，则弹出下载提示框，不满足需求。 2.任何资源都必须弹出下载提示框 3.使用响应头设置资源的打开方式： *content-disposition:attachment;filename=xxx 步骤： 1.定义页面，编辑超链接href属性。指向Servlet，传递资源名称filename 2.定义Servlet 1.获取文件名 2.使用字节输入流加载文件进内存 3.指定response的响应头：content-disposition:attachment;filename=xxx 4.将数据写出到response输出流*问题：中文文件名问题 *解决思路： 1.获取客户端使用的浏览器版本信息 2.根据不同的版本信息，设置filename的编码方式不同 使用封装好的工具类 源码12345678910111213141516171819202122232425262728293031323334353637383940414243444546package com.atguigu.web.servlet;import com.atguigu.utils.DownLoadUtils;import javax.servlet.ServletException;import javax.servlet.ServletOutputStream;import javax.servlet.annotation.WebServlet;import javax.servlet.http.HttpServlet;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import java.io.FileInputStream;import java.io.IOException;/** * @author yinhuidong * @createTime 2020-02-12-15:36 */@WebServlet(&quot;/download&quot;)public class DownLoadServlet extends HttpServlet &#123; protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; doGet(request,response); &#125; protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; //1.获取文件名称 String filename = request.getParameter(&quot;filename&quot;); //2..使用字节输入流加载文件 String path = this.getServletContext().getRealPath(&quot;/img/&quot;+filename); FileInputStream fis = new FileInputStream(path); //解决中文文件名问题 String agent = request.getHeader(&quot;user-agent&quot;); filename=DownLoadUtils.getFileName(agent,filename); //指定response的响应头：content-disposition:attachment;filename=xxx String type = this.getServletContext().getMimeType(filename); response.setHeader(&quot;content-type&quot;,type); response.setHeader(&quot;content-disposition&quot;,&quot;attachment;filename=&quot;+filename); //将数据写出到response输出流 ServletOutputStream sos = response.getOutputStream(); byte[] buff = new byte[1024 * 8]; int len=0; while ((len=fis.read(buff))!=-1)&#123; sos.write(buff,0,len); &#125; fis.close(); &#125;&#125; 解决中文文件名乱码的工具类12345678910111213141516171819202122232425262728package com.atguigu.utils;import sun.misc.BASE64Encoder;import java.io.UnsupportedEncodingException;import java.net.URLEncoder;/** * @author yinhuidong * @createTime 2020-02-12-16:05 */public class DownLoadUtils &#123; public static String getFileName(String agent,String filename) throws UnsupportedEncodingException &#123; if (agent.contains(&quot;MSIE&quot;))&#123; //IE浏览器 filename=URLEncoder.encode(filename,&quot;utf-8&quot;); filename=filename.replace(&quot;+&quot;,&quot; &quot;); &#125;else if (agent.contains(&quot;Firefox&quot;))&#123; //火狐浏览器 BASE64Encoder base64Encoder = new BASE64Encoder(); filename=&quot;=?utf-8?B?&quot;+base64Encoder.encode(filename.getBytes(&quot;utf-8&quot;))+&quot;?=&quot;; &#125;else&#123; //其他浏览器 filename=URLEncoder.encode(filename,&quot;utf-8&quot;); &#125; return filename; &#125;&#125; html页面12345678910&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;文件下载&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;a href=&quot;/JavaServlet/download?filename=a.jpg&quot;&gt;图片&lt;/a&gt;&lt;/body&gt;&lt;/html&gt; 10.会话技术1234567会话技术 1.概念：一次会话中包含多次请求和响应。 *一次会话：浏览器第一次给服务器资源发送请求，会话建立，直到一方断开为止。 2.功能：在一次会话的范围内的多次请求间来共享数据。 3.方式： 1.客户端会话技术：Cookie 2.服务端会话技术：Session 1）cookie123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;--概念：将数据保存在客户端的会话技术。代码演示： 1.创建cookie对象，绑定数据 new Cookie（String name,String value 2.发送cookie response.addCookie(Cookie cookie) 3.获取cookie，拿到数据 Cookies[] request.getCookies() 4.获取缓存源码cookie原理分析： 基于响应头set-cookie和请求头cookie实现cookie细节： 1.一次可不可以发送多个cookie？ *可以创建多个cookie对象，使用response调用多次addCookie（方法发送cookie即可。 源码：一次发送多个cookie对象 2.cookie在浏览器中保存多长时间 1.默认情况下，关闭浏览器，Cookie数据被销毁 2.设置cookie生命周期，持久化存储 *setMaxAge(int second); 1.正数：将cookie数据写到硬盘的文件中。持久化存储 2.负数：默认值 3.0：删除cookie信息 3.源码：清除cookie 3.cookie能不能存放中文 在tomcat8之前cookie不能存储中文数据 *需要将中文数据转码-- 一般采用URL编码 在tomcat8之后可以存储中文数据 4.cookie获取范围多大 1.一个服务器下的多个项目，cookie能不能共享？ *默认情况下cookie不能共享 *setPath（String path：设置cookie的获取范围，默认情况下：设置当前的虚拟目录 *如果要共享，可以将path设置为“/” 2.不同服务器间cookie如何共享？ setDomain（String path）如果设置一级域名相同，那么多个服务器之间cookie可以共享 *setDomain（&quot;.baidu.com&quot;）,那么tieba.baidu.com和news.baidu.com中cookie可以共享。cookie特点和作用 1.cookie存放在客户端浏览器 2.浏览器对于单个cookie的大小有限制，以及对同一个域名下的总cookie数量也限制 *作用： 1.cookie一般用于存储少量不太敏感的数据 2.在不登陆的情况下，完成服务器对客户端的身份识别案例：记住上一次访问时间--&gt; 获取缓存12345protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; Cookie cookie = new Cookie(&quot;name&quot;, &quot;尹会东&quot;); cookie.setMaxAge(60*60); response.addCookie(cookie);&#125; 12345678protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; Cookie[] cookies = request.getCookies(); if (cookies!=null)&#123; for (Cookie cookie:cookies)&#123; System.out.println(cookie.getValue()); &#125; &#125;&#125; 清除cookie123456789protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; Cookie[] cookies = request.getCookies(); if (cookies!=null)&#123; for (Cookie cookie:cookies)&#123; if(cookie.getValue().equals(&quot;name&quot;)||cookie.getValue().equals(&quot;lastTime&quot;)||cookie.getValue().equals(&quot;sex&quot;))&#123; cookie.setMaxAge(0); &#125; &#125; &#125; 记住上一次访问时间12345678910111213141516171819202122232425protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; request.setCharacterEncoding(&quot;utf-8&quot;); response.setContentType(&quot;text/html;charset=utf-8&quot;); Cookie[] cookies = request.getCookies(); boolean flag=false; if (cookies!=null&amp;&amp;cookies.length&gt;0)&#123; for (Cookie cookie:cookies)&#123; if (&quot;lastTime&quot;.equals(cookie.getName()))&#123; flag=true; response.getWriter().write(&quot;欢迎回来，您的上次登录时间为：&quot;+cookie.getValue()); Date date = new Date(); SimpleDateFormat format = new SimpleDateFormat(&quot;yyyy年MM月dd日_HH:mm:ss&quot;); String time = format.format(date); response.addCookie(new Cookie(&quot;lastTime&quot;,time)); break; &#125; &#125; &#125;else if (flag=false||cookies==null||cookies.length==0)&#123; Date date = new Date(); SimpleDateFormat format = new SimpleDateFormat(&quot;yyyy年MM月dd日_HH:mm:ss&quot;); String time = format.format(date); response.addCookie(new Cookie(&quot;lastTime&quot;,time)); response.getWriter().write(&quot;您好，欢迎您的首次登陆&quot;); &#125;&#125; 2）session123456789101112131415161718192021222324252627282930313233343536373839404142&lt;--Session1.概念：服务器端会话技术，在一次会话间的多次请求间共享数据，将数据保存在服务器端的对象中。HttpSession2.快速入门： 1.获取HttpSession对象： HttpSession session=request.getSession(); 2.使用HttpSession对象： Object getAttribute(String name) void setAttribute(String name,Object value) void removeAttribute(String name)3.原理 *Session的实现是依赖于cookie的。 原理图：4.细节： 1.当客户端关闭后，服务器不关闭，两次获取session是否为同一个？ *默认情况下不是同一个。 *客户端关闭后服务器也能相同 session1,session2 2.客户端关闭，服务器关闭，两次获取的session是同一个么？ *默认情况下不是同一个 *如果需要相同 Session的钝化 *在服务器正常关闭之前，将session对象序列化到硬盘上 Session的活化 *在服务器启动后，jiangsession对象转化为内存中的session对象 3.默认失效时间问题？ 1.服务器关闭 2.session对象调用invalidate()。 3.session默认失效时间：30分钟 择性配置修改web.xml文件 &lt;session-config&gt; &lt;session-timeout&gt;30&lt;/session-timeout&gt; &lt;/session-config&gt;5.session特点： 1.session用于存储一次会话的多次请求的数据，存在服务器端 2.session可以存储任意大小，任意类型的数据。 3.session与cookie的区别： 1.session存储数据在服务器端，cookie存储在客户端 2.session没数据大小限制，cookie 3.session数据安全，cookie相对于不安全6.案例：验证码的显示和切换--&gt; 图解session session1,session21234567891011protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; HttpSession session = request.getSession(); String id = session.getId(); Cookie cookie = new Cookie(&quot;JSESSIONID&quot;, id); cookie.setMaxAge(60*60); response.addCookie(cookie);&#125;protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; HttpSession session = request.getSession(); System.out.println(session.getId());&#125; 11.分页12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public class PageBean&lt;T&gt; &#123; private int totalCount;//总记录数 private int totalPage;//总页码 private List&lt;T&gt; list;//每页的数据 private int currentPage;//当前页码 private int rows;//每页显示的记录数 public int getTotalCount() &#123; return totalCount; &#125; public void setTotalCount(int totalCount) &#123; this.totalCount = totalCount; &#125; public int getTotalPage() &#123; return totalPage; &#125; public void setTotalPage(int totalPage) &#123; this.totalPage = totalPage; &#125; public List&lt;T&gt; getList() &#123; return list; &#125; public void setList(List&lt;T&gt; list) &#123; this.list = list; &#125; public int getCurrentPage() &#123; return currentPage; &#125; public void setCurrentPage(int currentPage) &#123; this.currentPage = currentPage; &#125; public int getRows() &#123; return rows; &#125; public void setRows(int rows) &#123; this.rows = rows; &#125; @Override public String toString() &#123; return &quot;PageBean&#123;&quot; + &quot;totalCount=&quot; + totalCount + &quot;, totalPage=&quot; + totalPage + &quot;, list=&quot; + list + &quot;, currentPage=&quot; + currentPage + &quot;, rows=&quot; + rows + &#x27;&#125;&#x27;; &#125;&#125; 12.过滤器1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162Filter:过滤器概念：当我们访问服务器的资源时，过滤器可以将该请求拦截下来，完成一些功能。功能：登录验证，编码统一处理，敏感字符过滤快速入门： 1.步骤： 1.定义一个类，实现接口Filter 2.复写方法 3.配置拦截路径 1.注解 2.web.xml细节： 1.web.xml配置 &lt;filter&gt; &lt;filter-name&gt;demo1&lt;/filter-name&gt; &lt;filter-class&gt;com.atguigu.Filter.FilterDemo1&lt;/filter-class&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;demo1&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt; 2.过滤器执行流程 1.执行过滤器 2.执行放行后的资源 3.执行过滤器放行代码后面的代码 System.out.println(&quot;Filter...&quot;); //对request对象进行增强 chain.doFilter(req, resp); //对response对象进行增强 System.out.println(&quot;Filter...123&quot;); 3.过滤器生命周期方法 1.init：在服务器启动后，会创建Filter对象，然后调用init方法。只执行一次，用于加载资源 2.doFilter：每一次请求被拦截资源时，会执行。执行多次 3.destory：在服务器关闭后，Filter对象被销毁。如果服务器时正常关闭，则会执行destroy方法。 只执行一次，用于释放资源。 4.过滤器配置详解 *拦截路径配置 1.具体的资源路径：/index.jsp 只访问index.jsp资源时，过滤器才会执行。 2.拦截目录：/views/* 访问views下的所资源，过滤器都会被执行。 3.后缀名拦截：*.jsp 访问所后缀名为.jsp的资源时，过滤器都会被执行 4.拦截所资源：/* 访问所资源时，过滤器都会被执行 *拦截方式的配置：资源被访问的方式 *注解配置 设置dispatcherTypes属性 1.REQUEST：默认值，浏览器直接请求资源 2.FORWORD：转发访问资源 3.INCLUDE：包含访问资源 4.ERROR：错误跳转资源 5.ASYNC：异步访问资源 @WebFilter(value=&quot;/*&quot;,dispatcherTypes = &#123;DispatcherType.REQUEST,DispatcherType.FORWARD&#125;) *web.xml配置 设置&lt;dispatcher&gt;&lt;/dispatcher&gt;标签 5.过滤器链（配置多个过滤器 *执行顺序：如果两个过滤器：过滤器1和过滤器2 1.过滤器1 2.过滤器2 3资源执行 4.过滤器2 5.过滤器1 *过滤器先后顺序问题： 1.注解配置“照类名的字符串比较规则比较，值小的先执行 *如：AFilter和BFilter，AFilter先执行 2.web.xml配置：&lt;filter-mapping&gt;谁定义在上边，谁先执行 案例：登陆验证：12345678910111213141516public void doFilter(ServletRequest req, ServletResponse resp, FilterChain chain) throws ServletException, IOException &#123; HttpServletRequest request= (HttpServletRequest) req; String uri = request.getRequestURI(); if (uri.contains(&quot;/css/&quot;)||uri.contains(&quot;/fonts/&quot;)||uri.contains(&quot;/js/&quot;)||uri.contains(&quot;/index.jsp&quot;)||uri.contains(&quot;/Login.jsp&quot;)||uri.contains(&quot;/login&quot;)||uri.contains(&quot;/code&quot;)||uri.contains(&quot;/exit&quot;))&#123; chain.doFilter(req, resp); &#125;else&#123; Object root = request.getSession().getAttribute(&quot;root&quot;); if (root!=null)&#123; chain.doFilter(req,resp); &#125;else&#123; request.setAttribute(&quot;login_msg&quot;,&quot;您尚未登录，请登录&quot;); request.getRequestDispatcher(&quot;/views/Login.jsp&quot;).forward(request,resp); &#125; &#125; &#125; 案例：过滤字符编码1234567public void doFilter(ServletRequest req, ServletResponse resp, FilterChain chain) throws ServletException, IOException &#123; HttpServletRequest request= (HttpServletRequest) req; HttpServletResponse response= (HttpServletResponse) resp; request.setCharacterEncoding(&quot;utf-8&quot;); response.setContentType(&quot;text/html;charset=utf-8&quot;); chain.doFilter(request, response);&#125; 13.监听器123456789101112131415161718192021Listener：监听器1.概念：事件监听机制 *事件：一件事情 *事件源：事件发生的地方 *监听器：一个对象 *注册监听：将事件，事件源和监听器绑定在一起。当事件源上发生某个事件后，执行监听器代码。2.ServletContextListener 接口监听ServletContext对象的创建和销毁destroy():ServletContext对象销毁之前会调用该方法init():ServletContext对象创建后会调用该方法3.编写步骤：1.定义类实现ServletContextListener 接口2.重写方法3.配置 1.web.xml &lt;listener&gt; &lt;listener-class&gt;com.atguigu.Listener.ListenerDemo1&lt;/listener-class&gt; &lt;/listener&gt; 通过&lt;context-param&gt;指定方法初始化时加载进内存的文件 2.注解 @webListener 六 jspjsp入门12345678910111213141516171819202122jsp入门1.概念： Java服务器端页面 可以理解为：一个特殊的页面，其中既可以指定定义html标签，又可以定义Java代码 用于简化书写2.原理 jsp本质上就是一个servlet3.jsp的脚本：jsp定义Java代码的方式 1.&lt;% %&gt;:定义的Java代码，在service方法中。service方法可以定义什么，该脚本就可以定义什么。 2.&lt;%! %&gt;:定义的Java代码，在jsp转换后的Java类的成员位置 3.&lt;%= %&gt;:定义的Java代码，会输出到页面上，输出语句中可以定义什么，该脚本就可以定义什么。4.jsp的内置对象 *在jsp页面中不需要获取和创建，可以直接使用的对象。 *jsp一共有9个内置对象 *入门阶段学习个 *request *response *out：字符输出流对象，可以将数据输出到页面上。和response.getWriter()类似 *response.getWriter()和out.write()的区别： 在tomcat服务器真正给客户端做出响应之前，会先找response缓冲区数据，再找out缓冲区数据。 response.getWriter()数据输出永远在out.write()之前。5案例：使用jsp页面重构显示上次登陆时间 使用jsp页面重构显示上次登陆时间12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&lt;%@ page import=&quot;java.util.Date&quot; %&gt;&lt;%@ page import=&quot;java.text.SimpleDateFormat&quot; %&gt;&lt;%-- Created by IntelliJ IDEA. User: 尹会东 Date: 2020/2/13 Time: 13:46 To change this template use File | Settings | File Templates.--%&gt;&lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;上次登录时间&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;% Cookie[] cookies = request.getCookies(); boolean flag=false; if (cookies!=null&amp;&amp;cookies.length&gt;0)&#123; for(Cookie cookie:cookies)&#123; String name = cookie.getName(); if (&quot;lastTime&quot;.equals(name))&#123; flag=true; String value = cookie.getValue(); %&gt;&lt;span&gt;欢迎回来，您的上次访问时间是：&lt;%=value%&gt;&lt;/span&gt;&lt;% Date date = new Date(); SimpleDateFormat s = new SimpleDateFormat(&quot;yyyy-MM-dd_HH:mm:ss&quot;); String format = s.format(date); cookie.setValue(format); cookie.setMaxAge(60*60*24*30); response.addCookie(cookie); break; &#125; &#125; &#125; if (cookies==null||cookies.length==0||flag==false)&#123; Date date = new Date(); SimpleDateFormat s = new SimpleDateFormat(&quot;yyyy-MM-dd_HH:mm:ss&quot;); String format = s.format(date); Cookie cookie = new Cookie(&quot;lastTime&quot;,format); cookie.setMaxAge(60*60*24*30); response.addCookie(cookie); %&gt; &lt;span&gt;您好，欢迎您首次登陆&lt;/span&gt;&lt;% &#125;%&gt;&lt;/body&gt;&lt;/html&gt; 指令，注释和内置对象1234567891011121314151617181920212223242526272829303132331.指令 *作用：用于配置jsp页面，导入资源文件 *格式： &lt;%@ 指令名称 属性名1=属性值1；属性名2=属性值2 %&gt; *分类： 1.page：配置jsp页面 *contentType：等同于response.setContentType() 1.设置响应体的mime类型以及字符集 *import：导包 *errorPage：当前页面发生异常后，会自动跳转到指定的错误页面 *isErrorPage：标识当前页面是否是错误页面 *true：是，可以使用内置对象exception 2.include：页面包含的，导入页面的资源信息 *&lt;%include file=&quot;top.jsp&quot;%&gt; 3.taglib：导入资源 *&lt;% taglib prefix=&quot;c&quot; uri=&quot;http://java.sun.com/jsp/jstl/core&quot; %&gt; prefix:前缀，自定义的2.注释 1.html注释：只能注释html代码片段 2.jsp注释：推荐使用&lt;%-- --%&gt;：可以注释所，在网页源代码上不显示3.内置对象 *在jsp页面中不需要创建，直接使用的对象 *一共有9个： 变量名 真实类型 作用 pageContext PageContext 当前页面共享数据，还可以获取其他八个内置对象 request HttpServletRequest 一次请求访问的多个资源（转发） session HttpSession 一次会话的多个请求间 application ServletContext 所用户间共享数据 response HttpResponse 响应对象 page Object 当前页面的对象 out JspWriter 输出对象，数据输出到页面上 config ServletConfig Servlet的配置对象 exception Throwable 异常对象 MVC开发模式123456789101112131415161718192021MVC开发模式1.jsp演变历史 1.早期只servlet，只能使用response输出标签数据，非常麻烦 2.后来了jsp，简化了servlet的开发，如果过度使用jsp，在jsp中即写大量的Java代码，又写html 造成难于维护，难于分工协作。 3.再后来，Java的web开发，借鉴mvc开发模式，使得程序的设计更加合理2.MVC： 1.M：Model，模型。JavaBean *完成具体的业务操作，如：查询数据库，封装对象 2.V：View，视图。jsp *展示数据 3.C：Controller，控制器。Servlet *获取用户的输入 *调用模型 *将数据交给视图进行展示 *优缺点： 1.优点： 1.耦合性低，方便维护，可以利于分工协作 2.重用性高 2.缺点： 使的项目结构变得复杂，对开发人员要求高 EL表达式123456789101112131415161718192021222324252627282930313233343536373839404142EL表达式1.概念：表达式语言2.作用：替换和简化jsp页面中Java代码的编写。3.语法：$&#123;表达式&#125;4.注意： *jsp默认支持el表达式。如果要忽略el表达式 1.设置jsp中page指令中：isElIgnored=”true&quot;，忽略当前jsp页面中所的el表达式。 2.\\$&#123;表达式&#125; ：忽略当前这个el表达式5.使用： 1.运算： *运算符： 1.算数运算符：+- */ % 2.比较运算符：&gt;&lt;&gt;=&lt;= == != 3.逻辑运算符：&amp;&amp; || ！ 4.空运算符：empty *功能：用于判断字符串，集合，数组对象是否为0 *$&#123;empty list&#125;：判断字符串，集合，数组对象是否为null或者长度是否为0 *$&#123;not empty str&#125;：表示判断字符串，集合，数组对象是否不为null 并且长度&gt;0 2.获取值 1.el表达式只能从域对象中获取值 2.语法： 1.$&#123;域名称.键名&#125;：从指定域中获取指定键的值 *域名称： 1.pageScope ----&gt;pageContext 2.requestScope ----&gt;request 3.sessionScope ----&gt;session 4.applicationScope ----&gt;application(ServletContext) *举例：在request域中存储了name=张 *获取：$&#123;requestScope.name&#125; 2.$&#123;键名&#125;：表示依次从最小的域中查找是否有该键对应的值，知道找到为止。 3.获取对象，List集合，Map集合的值 1.对象：$&#123;域名称.键名.属性名&#125; *本质上会去调用对象的getter方法 2.List集合:$&#123;域名称.键名[索引]&#125; 3.Map集合： $&#123;域名称.键名.key名称&#125; $&#123;域名称.键名[key名称]&#125; 3.隐式对象： el表达式11个隐式对象 pageContext： 获取jsp其他八个内置对象 $&#123;pageContext.request.contextPath&#125;:动态获取虚拟目录 JSTL表达式123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566JSTL1.概念：jsp标准标签库 *Apache组织提供提供的开源免费的jsp标签2.作用：用于简化和替换jsp页面上的Java代码3.使用步骤： 1.导入jstl相关jar包 2.引入标签库：taglib指令：&lt;%@taglib prefix=&quot;c&quot; uri=&quot;http://java.sun.com/jsp/jstl/core&quot; %&gt; 3.使用标签4.if标签 1.属性： *test必须属性，接受boolean表达式 *如果表达式为true，则显示if标签体的内容，如果为false，则不显示标签体内容。 *一般情况下，test属性会结合el表达式一起使用。 2.注意：c:if标签else情况，想要else情况，则可以再定义一个c:if标签。 &lt;c:if test=&quot;true&quot;&gt; &lt;h1&gt;我是真。。。&lt;/h1&gt; &lt;/c:if&gt;5.choose标签 完成数字编号对应星期几案例 1.域中存储一数字 2.使用choose标签取出数字 相当于switch声明 3.使用when标签做数字判断 相当于case 4.otherwise标签做其他情况的声明 相当于default &lt;c:choose&gt; &lt;c:when test=&quot;$&#123;number==1&#125;&quot;&gt;星期一&lt;/c:when&gt; &lt;c:when test=&quot;$&#123;number==2&#125;&quot;&gt;星期二&lt;/c:when&gt; &lt;c:when test=&quot;$&#123;number==3&#125;&quot;&gt;星期&lt;/c:when&gt; &lt;c:when test=&quot;$&#123;number==4&#125;&quot;&gt;星期四&lt;/c:when&gt; &lt;c:when test=&quot;$&#123;number==5&#125;&quot;&gt;星期五&lt;/c:when&gt; &lt;c:when test=&quot;$&#123;number==6&#125;&quot;&gt;星期六&lt;/c:when&gt; &lt;c:when test=&quot;$&#123;number==7&#125;&quot;&gt;星期七&lt;/c:when&gt; &lt;c:otherwise&gt;内部错误&lt;/c:otherwise&gt; &lt;/c:choose&gt;6.foreach标签 相当于Java代码的for语句 1.完成重复的操作 属性： begin：开始值 end：结束值 var：临时变量 step：步长 varStatus：循环状态对象 index：容器中元素的索引，从0开始 count：循环次数，从1开始 2.遍历集合 属性： items：容器对象 var：容器中元素的临时变量 varStatus：循环状态对象 index：容器中元素的索引，从0开始 count：循环次数，从1开始 3.案例 &lt;c:forEach begin=&quot;1&quot; end=&quot;10&quot; var=&quot;i&quot; step=&quot;2&quot; varStatus=&quot;s&quot;&gt; $&#123;i&#125; $&#123;s.index&#125; $&#123;s.count&#125; &lt;br/&gt; &lt;/c:forEach&gt; &lt;% List list = new ArrayList(); list.add(&quot;aaa&quot;); list.add(&quot;bbb&quot;); list.add(&quot;ccc&quot;); request.setAttribute(&quot;list&quot;,list); %&gt; &lt;c:forEach items=&quot;$&#123;list&#125;&quot; var=&quot;str&quot; varStatus=&quot;s&quot;&gt; $&#123;s.index&#125; $&#123;s.count&#125; $&#123;str&#125; &lt;/c:forEach&gt;7.综合案例：new.jsp new.jsp123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&lt;%@ page import=&quot;java.util.List&quot; %&gt;&lt;%@ page import=&quot;com.atguigu.bean.User&quot; %&gt;&lt;%@ page import=&quot;java.util.ArrayList&quot; %&gt;&lt;%-- Created by IntelliJ IDEA. User: 尹会东 Date: 2020/2/14 Time: 10:50 To change this template use File | Settings | File Templates.--%&gt;&lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt;&lt;%@taglib prefix=&quot;c&quot; uri=&quot;http://java.sun.com/jsp/jstl/core&quot; %&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;% List&lt;User&gt; list=new ArrayList&lt;&gt;(); list.add(new User(&quot;张&quot;,&quot;123&quot;)); list.add(new User(&quot;李四&quot;,&quot;234&quot;)); list.add(new User(&quot;王五&quot;,&quot;345&quot;)); request.setAttribute(&quot;list&quot;,list);%&gt;&lt;table border=&quot;1&quot; align=&quot;center&quot; width=&quot;500px&quot;&gt; &lt;tr&gt; &lt;th&gt;编号&lt;/th&gt; &lt;th&gt;姓名&lt;/th&gt; &lt;th&gt;密码&lt;/th&gt; &lt;/tr&gt; &lt;c:forEach items=&quot;$&#123;list&#125;&quot; var=&quot;user&quot; varStatus=&quot;s&quot;&gt; &lt;c:if test=&quot;$&#123;s.count%2==0&#125;&quot;&gt; &lt;tr bgcolor=&quot;yellow&quot;&gt; &lt;td&gt;$&#123;s.count&#125;&lt;/td&gt; &lt;td&gt;$&#123;user.name&#125;&lt;/td&gt; &lt;td&gt;$&#123;user.password&#125;&lt;/td&gt; &lt;/tr&gt; &lt;/c:if&gt; &lt;c:if test=&quot;$&#123;s.count%2!=0&#125;&quot;&gt; &lt;tr bgcolor=&quot;green&quot;&gt; &lt;td&gt;$&#123;s.count&#125;&lt;/td&gt; &lt;td&gt;$&#123;user.name&#125;&lt;/td&gt; &lt;td&gt;$&#123;user.password&#125;&lt;/td&gt; &lt;/tr&gt; &lt;/c:if&gt; &lt;/c:forEach&gt;&lt;/table&gt;&lt;/body&gt;&lt;/html&gt; 七 Ajax和Json1.Ajax12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788891. 概念： ASynchronous JavaScript And XML 异步的JavaScript 和 XML 1. 异步和同步：客户端和服务器端相互通信的基础上 * 客户端必须等待服务器端的响应。在等待的期间客户端不能做其他操作。 * 客户端不需要等待服务器端的响应。在服务器处理请求的过程中，客户端可以进行其他的操作。 Ajax 是一种在无需重新加载整个网页的情况下，能够更新部分网页的技术。 [1] 通过在后台与服务器进行少量数据交换，Ajax 可以使网页实现异步更新。这意味着可以在不重新加载整个网页的情况下，对网页的某部分进行更新。 传统的网页（不使用 Ajax）如果需要更新内容，必须重载整个网页页面。 提升用户的体验2. 实现方式： 1. 原生的JS实现方式（了解） //1.创建核心对象 var xmlhttp; if (window.XMLHttpRequest) &#123;// code for IE7+, Firefox, Chrome, Opera, Safari xmlhttp=new XMLHttpRequest(); &#125; else &#123;// code for IE6, IE5 xmlhttp=new ActiveXObject(&quot;Microsoft.XMLHTTP&quot;); &#125; //2. 建立连接 /* 参数： 1. 请求方式：GET、POST * get方式，请求参数在URL后边拼接。send方法为空参 * post方式，请求参数在send方法中定义 2. 请求的URL： 3. 同步或异步请求：true（异步）或 false（同步） */ xmlhttp.open(&quot;GET&quot;,&quot;ajaxServlet?username=tom&quot;,true); //3.发送请求 xmlhttp.send(); //4.接受并处理来自服务器的响应结果 //获取方式 ：xmlhttp.responseText //什么时候获取？当服务器响应成功后再获取 //当xmlhttp对象的就绪状态改变时，触发事件onreadystatechange。 xmlhttp.onreadystatechange=function() &#123; //判断readyState就绪状态是否为4，判断status响应状态码是否为200 if (xmlhttp.readyState==4 &amp;&amp; xmlhttp.status==200) &#123; //获取服务器的响应结果 var responseText = xmlhttp.responseText; alert(responseText); &#125; &#125;************************************************************************************ 2. JQeury实现方式 1. $.ajax() * 语法：$.ajax(&#123;键值对&#125;); //使用$.ajax()发送异步请求 $.ajax(&#123; url:&quot;ajaxServlet1111&quot; , // 请求路径 type:&quot;POST&quot; , //请求方式 //data: &quot;username=jack&amp;age=23&quot;,//请求参数 data:&#123;&quot;username&quot;:&quot;jack&quot;,&quot;age&quot;:23&#125;, success:function (data) &#123; alert(data); &#125;,//响应成功后的回调函数 error:function () &#123; alert(&quot;出错啦...&quot;) &#125;,//表示如果请求响应出现错误，会执行的回调函数 dataType:&quot;text&quot;//设置接受到的响应数据的格式 &#125;); 2. $.get()：发送get请求 * 语法：$.get(url, [data], [callback], [type]) * 参数： * url：请求路径 * data：请求参数 * callback：回调函数 * type：响应结果的类型 3. $.post()：发送post请求 * 语法：$.post(url, [data], [callback], [type]) * 参数： * url：请求路径 * data：请求参数 * callback：回调函数 * type：响应结果的类型 2.Json12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394951. 概念： JavaScript Object Notation JavaScript对象表示法 Person p = new Person(); p.setName(&quot;张三&quot;); p.setAge(23); p.setGender(&quot;男&quot;); var p = &#123;&quot;name&quot;:&quot;张三&quot;,&quot;age&quot;:23,&quot;gender&quot;:&quot;男&quot;&#125;; * json现在多用于存储和交换文本信息的语法 * 进行数据的传输 * JSON 比 XML 更小、更快，更易解析。2. 语法： 1. 基本规则 * 数据在名称/值对中：json数据是由键值对构成的 * 键用引号(单双都行)引起来，也可以不使用引号 * 值得取值类型： 1. 数字（整数或浮点数） 2. 字符串（在双引号中） 3. 逻辑值（true 或 false） 4. 数组（在方括号中） &#123;&quot;persons&quot;:[&#123;&#125;,&#123;&#125;]&#125; 5. 对象（在花括号中） &#123;&quot;address&quot;:&#123;&quot;province&quot;：&quot;陕西&quot;....&#125;&#125; 6. null * 数据由逗号分隔：多个键值对由逗号分隔 * 花括号保存对象：使用&#123;&#125;定义json 格式 * 方括号保存数组：[] 2. 获取数据: 1. json对象.键名 2. json对象[&quot;键名&quot;] 3. 数组对象[索引] 4. 遍历 //1.定义基本格式 var person = &#123;&quot;name&quot;: &quot;张三&quot;, age: 23, &#x27;gender&#x27;: true&#125;; var ps = [&#123;&quot;name&quot;: &quot;张三&quot;, &quot;age&quot;: 23, &quot;gender&quot;: true&#125;, &#123;&quot;name&quot;: &quot;李四&quot;, &quot;age&quot;: 24, &quot;gender&quot;: true&#125;, &#123;&quot;name&quot;: &quot;王五&quot;, &quot;age&quot;: 25, &quot;gender&quot;: false&#125;]; //获取person对象中所有的键和值 //for in 循环 /* for(var key in person)&#123; //这样的方式获取不行。因为相当于 person.&quot;name&quot; //alert(key + &quot;:&quot; + person.key); alert(key+&quot;:&quot;+person[key]); &#125;*/ //获取ps中的所有值 for (var i = 0; i &lt; ps.length; i++) &#123; var p = ps[i]; for(var key in p)&#123; alert(key+&quot;:&quot;+p[key]); &#125; &#125;3. JSON数据和Java对象的相互转换 * JSON解析器： * 常见的解析器：Jsonlib，Gson，fastjson，jackson 1. JSON转为Java对象 1. 导入jackson的相关jar包 2. 创建Jackson核心对象 ObjectMapper 3. 调用ObjectMapper的相关方法进行转换 1. readValue(json字符串数据,Class) 2. Java对象转换JSON 1. 使用步骤： 1. 导入jackson的相关jar包 2. 创建Jackson核心对象 ObjectMapper4444 3. 调用ObjectMapper的相关方法进行转换 1. 转换方法： * writeValue(参数1，obj): 参数1： File：将obj对象转换为JSON字符串，并保存到指定的文件中 Writer：将obj对象转换为JSON字符串，并将json数据填充到字符输出流中 OutputStream：将obj对象转换为JSON字符串，并将json数据填充到字节输出流中 * writeValueAsString(obj):将对象转为json字符串 2. 注解： 1. @JsonIgnore：排除属性。 2. @JsonFormat：属性值得格式化 * @JsonFormat(pattern = &quot;yyyy-MM-dd&quot;) 3. 复杂java对象转换 1. List：数组 2. Map：对象格式一致 json方法 1. JSON.stringify( &#123;&#125; , [ ] , &quot;&quot;)，把数据序列化为json字符串 参数一 ：要序列化的数据（object） 参数二 ：控制对象的键值，只想输出指定的属性，传入一个数组 参数三 ：序列化后，打印输出的格式（一个Tab ，可以更直观查看json） 2.JSON.parse(json字符串)； 把json数据反序列化为一个js对象。 [外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-oGpHNjRI-1601990855927)(E:\\note\\img\\json.png)] 3.案例：校验用户名12345* 校验用户名是否存在 1. 服务器响应的数据，在客户端使用时，要想当做json数据格式使用。有两种解决方案： 1. $.get(type):将最后一个参数type指定为&quot;json&quot; 2. 在服务器端设置MIME类型 response.setContentType(&quot;application/json;charset=utf-8&quot;); register.jsp12345678910111213141516171819202122232425262728293031323334353637383940&lt;%-- Created by IntelliJ IDEA. User: 尹会东 Date: 2020/2/20 Time: 10:02 To change this template use File | Settings | File Templates.--%&gt;&lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;Title&lt;/title&gt; &lt;script src=&quot;../js/jquery-3.3.1.min.js&quot;&gt;&lt;/script&gt; &lt;script&gt; $(function () &#123; $(&quot;#username&quot;).blur(function () &#123; var user=$(&quot;#username&quot;).val(); $.get( &quot;$&#123;pageContext.request.contextPath&#125;/test&quot;, &#123;username:user&#125;, function (data) &#123; if (data.userExsit)&#123; $(&quot;span&quot;).html(data.msg); &#125;else&#123; $(&quot;span&quot;).html(data.msg); &#125; &#125;, &quot;json&quot; ); &#125;); &#125;); &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;form&gt; &lt;input type=&quot;text&quot; name=&quot;username&quot; id=&quot;username&quot; placeholder=&quot;请输入用户名&quot;/&gt; &lt;span&gt;&lt;/span&gt; &lt;br/&gt; &lt;input type=&quot;submit&quot; value=&quot;注册&quot;/&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; testServlet1234567891011121314151617181920212223242526272829303132333435363738package com.atguigu.servlet;import com.fasterxml.jackson.databind.ObjectMapper;import javax.servlet.ServletException;import javax.servlet.annotation.WebServlet;import javax.servlet.http.HttpServlet;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import java.io.IOException;import java.util.HashMap;import java.util.Map;/** * @author yinhuidong * @createTime 2020-02-20-10:11 */@WebServlet(&quot;/test&quot;)public class Test extends HttpServlet &#123; protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; doGet(request, response); &#125; protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; response.setContentType(&quot;text/html;charset=utf-8&quot;); String username = request.getParameter(&quot;username&quot;); Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(); if (&quot;tom&quot;.equals(username)) &#123; map.put(&quot;userExsit&quot;,true); map.put(&quot;msg&quot;,&quot;此用户名太受欢迎，请更换一个&quot;); &#125; else &#123; map.put(&quot;userExsit&quot;,false); map.put(&quot;msg&quot;,&quot;此用户名可用&quot;); &#125; ObjectMapper mapper = new ObjectMapper(); mapper.writeValue(response.getWriter(),map); &#125;&#125; 八 Jquery1.Jquery基础12345678910111213141516171819202122231.概念：一个js框架，简化js的开发。 *简化了HTML文档操作，事件处理，动画设计，Ajax交互。 *本质上就是一些js文件，封装了js的原生代码。2.快速入门 1.基本步骤 1.下载JQuery jquery.xxx.js和jquery.xxx.min.js的区别： jquery.xxx.js：开发版本，给程序员看 jquery.xxx.min.js：生产版本，开发用，体积小，加载快。 2.导入JQuery的js文件:导入min.js文件 &lt;script src=&quot;$&#123;pageContext.request.contextPath&#125;/js/jquery-3.3.1.min.js&quot;&gt;&lt;/script&gt; 3.使用 &lt;script type=&quot;text/javascript&quot;&gt; var div1=$(&quot;#div1&quot;); alert(div1.html()); &lt;/script&gt;3.JQuery对象和js对象的区别与转换 *jquery对象是object类型，js对象是数组类型 1.jquery对象在操作时更加方便 2.jquery对象和js对象方法不通用 3.两者相互转换： 1.jquery---&gt;js:jq对象[索引]或者jq对象.get(索引) 2.js---&gt;jquery：jq：$(js对象) 选择器1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465664.选择器：筛具相同特征的元素（标签） 1.基本语法学习： 1.事件绑定： //给id为btn1的钮添加单击事件 $(&quot;#btn1&quot;).click(function () &#123; alert(); &#125;); 2.入口函数：(相当于js中的onload函数) $(function () &#123; &#125;)； 入口函数和window.onload()的区别： 1.window.onload()在一个DOM文档只能定义一次，如果定义多次，后面的会将前面的覆盖掉。 2.入口函数可以定义多次。 3.样式控制： $(&quot;div1&quot;).css(&quot;background-color&quot;,&quot;red&quot;); 2. 分类 1. 基本择器 1. 标签择器（元素择器 * 语法： $(&quot;html标签名&quot;) 获得所匹配标签名称的元素 2. id择器 * 语法： $(&quot;#id的属性值&quot;) 获得与指定id属性值匹配的元素 3. 类择器 * 语法： $(&quot;.class的属性值&quot;) 获得与指定的class属性值匹配的元素 4. 并集择器： * 语法： $(&quot;择器1,择器2....&quot;) 获取多个择器中的所元素 2. 层级择器 1. 后代择器 * 语法： $(&quot;A B &quot;) 择A元素内部的所B元素 2. 子择器 * 语法： $(&quot;A &gt; B&quot;) 择A元素内部的所B子元素 3. 属性择器 1. 属性名称择器 * 语法： $(&quot;A[属性名]&quot;) 包含指定属性的择器 2. 属性择器 * 语法： $(&quot;A[属性名=&#x27;值&#x27;]&quot;) 包含指定属性等于指定值的择器 3. 复合属性择器 * 语法： $(&quot;A[属性名=&#x27;值&#x27;][]...&quot;) 包含多个属性条件的择器 4. 过滤择器 1. 首元素择器 * 语法： :first 获得择的元素中的第一个元素 2. 尾元素择器 * 语法： :last 获得择的元素中的最后一个元素 3. 非元素择器 * 语法： :not(selector) 不包括指定内容的元素 4. 偶数择器 * 语法： :even 偶数，从 0 开始计数 5. 奇数择器 * 语法： :odd 奇数，从 0 开始计数 6. 等于索引择器 * 语法： :eq(index) 指定索引元素 7. 大于索引择器 * 语法： :gt(index) 大于指定索引元素 8. 小于索引择器 * 语法： :lt(index) 小于指定索引元素 9. 标题择器 * 语法： :header 获得标题（h1~h6元素，固定写法 5. 表单过滤择器 1. 可用元素择器 * 语法： :enabled 获得可用元素 2. 不可用元素择器 * 语法： :disabled 获得不可用元素 3. 选中选择器 * 语法： :checked 获得单/复框中的元素 4. 选中选择器 * 语法： :selected 获得下拉框中的元素 案例1.事件绑定与入口函数.html12345678910111213141516171819202122232425&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt; &lt;script src=&quot;../js/jquery-3.3.1.min.js&quot;&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;script&gt; //1.事件绑定 $(function () &#123; //2.入口函数 $(&quot;#btn1&quot;).click(function () &#123; alert(&quot;我被点击了！&quot;); &#125;); &#125;); //3.样式控制 $(function () &#123;//与window.onload的区别：入口函数可以定义多个，window.onload只能定义一次 //如果定义多次，后面的会将前面的覆盖掉 $(&quot;#btn1&quot;).css(&quot;backgroundColor&quot;,&quot;green&quot;); &#125;);&lt;/script&gt;&lt;input type=&quot;button&quot; name=&quot;name&quot; id=&quot;btn1&quot; value=&quot;点我试试&quot;/&gt;&lt;/body&gt;&lt;/html&gt; 01-基本选择器.html123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104&lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 4.01 Transitional//EN&quot;&gt;&lt;html&gt; &lt;head&gt; &lt;title&gt;基本择器&lt;/title&gt; &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot;&gt; &lt;script src=&quot;../js/jquery-3.3.1.min.js&quot;&gt;&lt;/script&gt; &lt;style type=&quot;text/css&quot;&gt; div,span&#123; width: 180px; height: 180px; margin: 20px; background: #9999CC; border: #000 1px solid; float:left; font-size: 17px; font-family:Roman; &#125; div .mini&#123; width: 50px; height: 50px; background: #CC66FF; border: #000 1px solid; font-size: 12px; font-family:Roman; &#125; div .mini01&#123; width: 50px; height: 50px; background: #CC66FF; border: #000 1px solid; font-size: 12px; font-family:Roman; &#125; &lt;/style&gt; &lt;script type=&quot;text/javascript&quot;&gt; $(function () &#123; $(&quot;#b1&quot;).click(function () &#123; $(&quot;#one&quot;).css(&quot;backgroundColor&quot;,&quot;red&quot;); &#125;); $(&quot;#b2&quot;).click(function () &#123; $(&quot;div&quot;).css(&quot;backgroundColor&quot;,&quot;red&quot;); &#125;); $(&quot;#b3&quot;).click(function () &#123; $(&quot;.mini&quot;).css(&quot;backgroundColor&quot;,&quot;red&quot;); &#125;); $(&quot;#b4&quot;).click(function () &#123; $(&quot;span,#two&quot;).css(&quot;backgroundColor&quot;,&quot;red&quot;); &#125;); &#125;); &lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;input type=&quot;button&quot; value=&quot;保存&quot; class=&quot;mini&quot; name=&quot;ok&quot; class=&quot;mini&quot; /&gt; &lt;input type=&quot;button&quot; value=&quot;改变 id 为 one 的元素的背景色为 红色&quot; id=&quot;b1&quot;/&gt; &lt;input type=&quot;button&quot; value=&quot; 改变元素名为 &lt;div&gt; 的所元素的背景色为 红色&quot; id=&quot;b2&quot;/&gt; &lt;input type=&quot;button&quot; value=&quot; 改变 class 为 mini 的所元素的背景色为 红色&quot; id=&quot;b3&quot;/&gt; &lt;input type=&quot;button&quot; value=&quot; 改变所的&lt;span&gt;元素和 id 为 two 的元素的背景色为红色&quot; id=&quot;b4&quot;/&gt; &lt;h1&gt;一种奇迹叫坚持&lt;/h1&gt; &lt;h2&gt;自信源于努力&lt;/h2&gt; &lt;div id=&quot;one&quot;&gt; id为one &lt;/div&gt; &lt;div id=&quot;two&quot; class=&quot;mini&quot; &gt; id为two class是 mini &lt;div class=&quot;mini&quot; &gt;class是 mini&lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;one&quot; &gt; class是 one &lt;div class=&quot;mini&quot; &gt;class是 mini&lt;/div&gt; &lt;div class=&quot;mini&quot; &gt;class是 mini&lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;one&quot; &gt; class是 one &lt;div class=&quot;mini01&quot; &gt;class是 mini01&lt;/div&gt; &lt;div class=&quot;mini&quot; &gt;class是 mini&lt;/div&gt; &lt;/div&gt; &lt;span class=&quot;spanone&quot;&gt;class为spanone的span元素&lt;/span&gt; &lt;span class=&quot;mini&quot;&gt;class为mini的span元素&lt;/span&gt; &lt;input type=&quot;text&quot; value=&quot;zhang&quot; id=&quot;username&quot; name=&quot;username&quot;&gt; &lt;/body&gt; &lt;/html&gt; 02-属性选择器.html123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123&lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 4.01 Transitional//EN&quot;&gt;&lt;html&gt; &lt;head&gt; &lt;title&gt;属性过滤择器&lt;/title&gt; &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot;&gt; &lt;script src=&quot;../js/jquery-3.3.1.min.js&quot;&gt;&lt;/script&gt; &lt;style type=&quot;text/css&quot;&gt; div,span&#123; width: 180px; height: 180px; margin: 20px; background: #9999CC; border: #000 1px solid; float:left; font-size: 17px; font-family:Roman; &#125; div .mini&#123; width: 50px; height: 50px; background: #CC66FF; border: #000 1px solid; font-size: 12px; font-family:Roman; &#125; div .mini01&#123; width: 50px; height: 50px; background: #CC66FF; border: #000 1px solid; font-size: 12px; font-family:Roman; &#125; div.visible&#123; display:none; &#125; &lt;/style&gt; &lt;script type=&quot;text/javascript&quot;&gt; $(function () &#123; $(&quot;#b1&quot;).click(function () &#123; $(&quot;div[title]&quot;).css(&quot;backgroundColor&quot;,&quot;red&quot;); &#125;); $(&quot;#b2&quot;).click(function () &#123; $(&quot;div[title=&#x27;test&#x27;]&quot;).css(&quot;backgroundColor&quot;,&quot;red&quot;); &#125;); $(&quot;#b3&quot;).click(function () &#123; $(&quot;div[title!=&#x27;test&#x27;]&quot;).css(&quot;backgroundColor&quot;,&quot;red&quot;); &#125;); $(&quot;#b4&quot;).click(function () &#123; $(&quot;div[title^=&#x27;te&#x27;]&quot;).css(&quot;backgroundColor&quot;,&quot;red&quot;); &#125;); $(&quot;#b5&quot;).click(function () &#123; $(&quot;div[title$=&#x27;est&#x27;]&quot;).css(&quot;backgroundColor&quot;,&quot;red&quot;); &#125;); $(&quot;#b6&quot;).click(function () &#123; $(&quot;div[title*=&#x27;es&#x27;]&quot;).css(&quot;backgroundColor&quot;,&quot;red&quot;); &#125;); $(&quot;#b7&quot;).click(function () &#123; $(&quot;div[id][title*=&#x27;es&#x27;]&quot;).css(&quot;backgroundColor&quot;,&quot;red&quot;); &#125;); &#125;) &lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;input type=&quot;button&quot; value=&quot;保存&quot; class=&quot;mini&quot; name=&quot;ok&quot; class=&quot;mini&quot; /&gt; &lt;input type=&quot;button&quot; value=&quot; 含属性title 的div元素背景色为红色&quot; id=&quot;b1&quot;/&gt; &lt;input type=&quot;button&quot; value=&quot; 属性title值等于test的div元素背景色为红色&quot; id=&quot;b2&quot;/&gt; &lt;input type=&quot;button&quot; value=&quot; 属性title值不等于test的div元素(没属性title的也将被中)背景色为红色&quot; id=&quot;b3&quot;/&gt; &lt;input type=&quot;button&quot; value=&quot; 属性title值 以te开始 的div元素背景色为红色&quot; id=&quot;b4&quot;/&gt; &lt;input type=&quot;button&quot; value=&quot; 属性title值 以est结束 的div元素背景色为红色&quot; id=&quot;b5&quot;/&gt; &lt;input type=&quot;button&quot; value=&quot;属性title值 含es的div元素背景色为红色&quot; id=&quot;b6&quot;/&gt; &lt;input type=&quot;button&quot; value=&quot;取属性id的div元素，然后在结果中取属性title值含“es”的 div 元素背景色为红色&quot; id=&quot;b7&quot;/&gt; &lt;div id=&quot;one&quot;&gt; id为one div &lt;/div&gt; &lt;div id=&quot;two&quot; class=&quot;mini&quot; title=&quot;test&quot;&gt; id为two class是 mini div title=&quot;test&quot; &lt;div class=&quot;mini&quot; &gt;class是 mini&lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;visible&quot; &gt; class是 one &lt;div class=&quot;mini&quot; &gt;class是 mini&lt;/div&gt; &lt;div class=&quot;mini&quot; &gt;class是 mini&lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;one&quot; title=&quot;test02&quot;&gt; class是 one title=&quot;test02&quot; &lt;div class=&quot;mini01&quot; &gt;class是 mini01&lt;/div&gt; &lt;div class=&quot;mini&quot; style=&quot;margin-top:0px;&quot;&gt;class是 mini&lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;visible&quot; &gt; 这是隐藏的 &lt;/div&gt; &lt;div class=&quot;one&quot;&gt; &lt;/div&gt; &lt;div id=&quot;mover&quot; &gt; 动画 &lt;/div&gt; &lt;input type=&quot;text&quot; value=&quot;zhang&quot; id=&quot;username&quot; name=&quot;username&quot;&gt; &lt;/body&gt; &lt;/html&gt; 03-层级选择器.html12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091&lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 4.01 Transitional//EN&quot;&gt;&lt;html&gt; &lt;head&gt; &lt;title&gt;层次择器&lt;/title&gt; &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot;&gt; &lt;script src=&quot;../js/jquery-3.3.1.min.js&quot;&gt;&lt;/script&gt; &lt;style type=&quot;text/css&quot;&gt; div,span&#123; width: 180px; height: 180px; margin: 20px; background: #9999CC; border: #000 1px solid; float:left; font-size: 17px; font-family:Roman; &#125; div .mini&#123; width: 50px; height: 50px; background: #CC66FF; border: #000 1px solid; font-size: 12px; font-family:Roman; &#125; div .mini01&#123; width: 50px; height: 50px; background: #CC66FF; border: #000 1px solid; font-size: 12px; font-family:Roman; &#125; &lt;/style&gt; &lt;script type=&quot;text/javascript&quot;&gt; $(function () &#123; $(&quot;#b1&quot;).click(function () &#123; $(&quot;body div&quot;).css(&quot;backgroundColor&quot;,&quot;red&quot;); &#125;); $(&quot;#b2&quot;).click(function () &#123; $(&quot;body &gt;div&quot;).css(&quot;backgroundColor&quot;,&quot;red&quot;); &#125;); &#125;) &lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;input type=&quot;button&quot; value=&quot;保存&quot; class=&quot;mini&quot; name=&quot;ok&quot; class=&quot;mini&quot; /&gt; &lt;input type=&quot;button&quot; value=&quot; 改变 &lt;body&gt; 内所 &lt;div&gt; 的背景色为红色&quot; id=&quot;b1&quot;/&gt; &lt;input type=&quot;button&quot; value=&quot; 改变 &lt;body&gt; 内子 &lt;div&gt; 的背景色为 红色&quot; id=&quot;b2&quot;/&gt; &lt;h1&gt;一种奇迹叫坚持&lt;/h1&gt; &lt;h2&gt;自信源于努力&lt;/h2&gt; &lt;div id=&quot;one&quot;&gt; id为one &lt;/div&gt; &lt;div id=&quot;two&quot; class=&quot;mini&quot; &gt; id为two class是 mini &lt;div class=&quot;mini&quot; &gt;class是 mini&lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;one&quot; &gt; class是 one &lt;div class=&quot;mini&quot; &gt;class是 mini&lt;/div&gt; &lt;div class=&quot;mini&quot; &gt;class是 mini&lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;one&quot;&gt; class是 one &lt;div class=&quot;mini01&quot; &gt;class是 mini01&lt;/div&gt; &lt;div class=&quot;mini&quot; &gt;class是 mini&lt;/div&gt; &lt;/div&gt; &lt;span class=&quot;spanone&quot;&gt; span &lt;/span&gt; &lt;/body&gt; &lt;/html&gt; 04-过滤选择器.html1不可见：display属性设置为none，或visible设置为hidden 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160&lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 4.01 Transitional//EN&quot;&gt;&lt;html&gt; &lt;head&gt; &lt;title&gt;基本过滤择器&lt;/title&gt; &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot;&gt; &lt;script src=&quot;../js/jquery-3.3.1.min.js&quot;&gt;&lt;/script&gt; &lt;style type=&quot;text/css&quot;&gt; div,span&#123; width: 180px; height: 180px; margin: 20px; background: #9999CC; border: #000 1px solid; float:left; font-size: 17px; font-family:Roman; &#125; div .mini&#123; width: 50px; height: 50px; background: #CC66FF; border: #000 1px solid; font-size: 12px; font-family:Roman; &#125; div .mini01&#123; width: 50px; height: 50px; background: #CC66FF; border: #000 1px solid; font-size: 12px; font-family:Roman; &#125; &lt;/style&gt; &lt;script type=&quot;text/javascript&quot;&gt; // &lt;input type=&quot;button&quot; value=&quot;保存&quot; class=&quot;mini&quot; name=&quot;ok&quot; class=&quot;mini&quot; /&gt; // &lt;input type=&quot;button&quot; value=&quot; 改变第一个 div 元素的背景色为 红色&quot; id=&quot;b1&quot;/&gt; $(function () &#123; $(&quot;#b1&quot;).click( function () &#123; $(&quot;div:first&quot;).css(&quot;backgroundColor&quot;,&quot;red&quot;); &#125; ); &#125;); // &lt;input type=&quot;button&quot; value=&quot; 改变最后一个 div 元素的背景色为 红色&quot; id=&quot;b2&quot;/&gt; $(function () &#123; $(&quot;#b2&quot;).click( function () &#123; $(&quot;div:last&quot;).css(&quot;backgroundColor&quot;,&quot;red&quot;); &#125; ); &#125;); // &lt;input type=&quot;button&quot; value=&quot; 改变class不为 one 的所 div 元素的背景色为 红色&quot; id=&quot;b3&quot;/&gt; $(function () &#123; $(&quot;#b3&quot;).click( function () &#123; $(&quot;div[class!=&#x27;one&#x27;]&quot;).css(&quot;backgroundColor&quot;,&quot;red&quot;); &#125; ); &#125;) // &lt;input type=&quot;button&quot; value=&quot; 改变索引值为偶数的 div 元素的背景色为 红色&quot; id=&quot;b4&quot;/&gt; $(function () &#123; $(&quot;#b4&quot;).click( function () &#123; $(&quot;div:even&quot;).css(&quot;backgroundColor&quot;,&quot;red&quot;); &#125; ); &#125;) // &lt;input type=&quot;button&quot; value=&quot; 改变索引值为奇数的 div 元素的背景色为 红色&quot; id=&quot;b5&quot;/&gt; $(function () &#123; $(&quot;#b5&quot;).click( function () &#123; $(&quot;div:odd&quot;).css(&quot;backgroundColor&quot;,&quot;red&quot;); &#125; ); &#125;); // &lt;input type=&quot;button&quot; value=&quot; 改变索引值为大于 3 的 div 元素的背景色为 红色&quot; id=&quot;b6&quot;/&gt; $(function () &#123; $(&quot;#b6&quot;).click( function () &#123; $(&quot;div:gt(3)&quot;).css(&quot;backgroundColor&quot;,&quot;red&quot;); &#125; ); &#125;); // &lt;input type=&quot;button&quot; value=&quot; 改变索引值为等于 3 的 div 元素的背景色为 红色&quot; id=&quot;b7&quot;/&gt; $(function () &#123; $(&quot;#b7&quot;).click( function () &#123; $(&quot;div:eq(3)&quot;).css(&quot;backgroundColor&quot;,&quot;red&quot;); &#125; ); &#125;); // &lt;input type=&quot;button&quot; value=&quot; 改变索引值为小于 3 的 div 元素的背景色为 红色&quot; id=&quot;b8&quot;/&gt; $(function () &#123; $(&quot;#b8&quot;).click( function () &#123; $(&quot;div:lt(3)&quot;).css(&quot;backgroundColor&quot;,&quot;red&quot;); &#125; ); &#125;); // &lt;input type=&quot;button&quot; value=&quot; 改变所的标题元素的背景色为 红色&quot; id=&quot;b9&quot;/&gt; $(function () &#123; $(&quot;#b9&quot;).click( function () &#123; $(&quot;:header&quot;).css(&quot;backgroundColor&quot;,&quot;red&quot;); &#125; ); &#125;); &lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;input type=&quot;button&quot; value=&quot;保存&quot; class=&quot;mini&quot; name=&quot;ok&quot; class=&quot;mini&quot; /&gt; &lt;input type=&quot;button&quot; value=&quot; 改变第一个 div 元素的背景色为 红色&quot; id=&quot;b1&quot;/&gt; &lt;input type=&quot;button&quot; value=&quot; 改变最后一个 div 元素的背景色为 红色&quot; id=&quot;b2&quot;/&gt; &lt;input type=&quot;button&quot; value=&quot; 改变class不为 one 的所 div 元素的背景色为 红色&quot; id=&quot;b3&quot;/&gt; &lt;input type=&quot;button&quot; value=&quot; 改变索引值为偶数的 div 元素的背景色为 红色&quot; id=&quot;b4&quot;/&gt; &lt;input type=&quot;button&quot; value=&quot; 改变索引值为奇数的 div 元素的背景色为 红色&quot; id=&quot;b5&quot;/&gt; &lt;input type=&quot;button&quot; value=&quot; 改变索引值为大于 3 的 div 元素的背景色为 红色&quot; id=&quot;b6&quot;/&gt; &lt;input type=&quot;button&quot; value=&quot; 改变索引值为等于 3 的 div 元素的背景色为 红色&quot; id=&quot;b7&quot;/&gt; &lt;input type=&quot;button&quot; value=&quot; 改变索引值为小于 3 的 div 元素的背景色为 红色&quot; id=&quot;b8&quot;/&gt; &lt;input type=&quot;button&quot; value=&quot; 改变所的标题元素的背景色为 红色&quot; id=&quot;b9&quot;/&gt; &lt;h1&gt;一种奇迹叫坚持&lt;/h1&gt; &lt;h2&gt;自信源于努力&lt;/h2&gt; &lt;div id=&quot;one&quot;&gt; id为one &lt;/div&gt; &lt;div id=&quot;two&quot; class=&quot;mini&quot; &gt; id为two class是 mini &lt;div class=&quot;mini&quot; &gt;class是 mini&lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;one&quot; &gt; class是 one &lt;div class=&quot;mini&quot; &gt;class是 mini&lt;/div&gt; &lt;div class=&quot;mini&quot; &gt;class是 mini&lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;one&quot; &gt; class是 one &lt;div class=&quot;mini01&quot; &gt;class是 mini01&lt;/div&gt; &lt;div class=&quot;mini&quot; &gt;class是 mini&lt;/div&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt; 05-表单过滤选择器.html123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134&lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 4.01 Transitional//EN&quot;&gt;&lt;html&gt; &lt;head&gt; &lt;title&gt;表单属性过滤择器&lt;/title&gt; &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot;&gt; &lt;script src=&quot;../js/jquery-3.3.1.min.js&quot;&gt;&lt;/script&gt; &lt;style type=&quot;text/css&quot;&gt; div,span&#123; width: 180px; height: 180px; margin: 20px; background: #9999CC; border: #000 1px solid; float:left; font-size: 17px; font-family:Roman; &#125; div .mini&#123; width: 50px; height: 50px; background: #CC66FF; border: #000 1px solid; font-size: 12px; font-family:Roman; &#125; div .mini01&#123; width: 50px; height: 50px; background: #CC66FF; border: #000 1px solid; font-size: 12px; font-family:Roman; &#125; #job&#123; margin: 20px; &#125; #edu&#123; margin-top:-70px; &#125; &lt;/style&gt; &lt;script type=&quot;text/javascript&quot;&gt; $(function () &#123; // &lt;input type=&quot;button&quot; value=&quot; 利用 jQuery 对象的 val() 方法改变表单内可用 &lt;input&gt; 元素的值&quot; id=&quot;b1&quot;/&gt; $(&quot;#b1&quot;).click(function () &#123; $(&quot;input[type=&#x27;text&#x27;]:enabled&quot;).val(6666666); &#125;); // &lt;input type=&quot;button&quot; value=&quot; 利用 jQuery 对象的 val() 方法改变表单内不可用 &lt;input&gt; 元素的值&quot; id=&quot;b2&quot;/&gt; $(&quot;#b2&quot;).click(function () &#123; $(&quot;:disabled&quot;).val(6666666); &#125;); // &lt;input type=&quot;button&quot; value=&quot; 利用 jQuery 对象的 length 属性获取复框中的个数&quot; id=&quot;b3&quot;/&gt; $(&quot;#b3&quot;).click(function () &#123; $(&quot;:checked&quot;).length(); &#125;); // &lt;input type=&quot;button&quot; value=&quot; 利用 jQuery 对象的 length 属性获取下拉框中的个数&quot; id=&quot;b4&quot;/&gt; $(&quot;#b4&quot;).click(function () &#123; $(&quot;:selected&quot;).length(); &#125;); &#125;); &lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;input type=&quot;button&quot; value=&quot;保存&quot; class=&quot;mini&quot; name=&quot;ok&quot; class=&quot;mini&quot; /&gt; &lt;input type=&quot;button&quot; value=&quot; 利用 jQuery 对象的 val() 方法改变表单内可用 &lt;input&gt; 元素的值&quot; id=&quot;b1&quot;/&gt; &lt;input type=&quot;button&quot; value=&quot; 利用 jQuery 对象的 val() 方法改变表单内不可用 &lt;input&gt; 元素的值&quot; id=&quot;b2&quot;/&gt; &lt;input type=&quot;button&quot; value=&quot; 利用 jQuery 对象的 length 属性获取复框中的个数&quot; id=&quot;b3&quot;/&gt; &lt;input type=&quot;button&quot; value=&quot; 利用 jQuery 对象的 length 属性获取下拉框中的个数&quot; id=&quot;b4&quot;/&gt; &lt;br&gt;&lt;br&gt; &lt;input type=&quot;text&quot; value=&quot;不可用值1&quot; disabled=&quot;disabled&quot;&gt; &lt;input type=&quot;text&quot; value=&quot;可用值1&quot; &gt; &lt;input type=&quot;text&quot; value=&quot;不可用值2&quot; disabled=&quot;disabled&quot;&gt; &lt;input type=&quot;text&quot; value=&quot;可用值2&quot; &gt; &lt;br&gt;&lt;br&gt; &lt;input type=&quot;checkbox&quot; name=&quot;items&quot; value=&quot;美容&quot; &gt;美容 &lt;input type=&quot;checkbox&quot; name=&quot;items&quot; value=&quot;IT&quot; &gt;IT &lt;input type=&quot;checkbox&quot; name=&quot;items&quot; value=&quot;金融&quot; &gt;金融 &lt;input type=&quot;checkbox&quot; name=&quot;items&quot; value=&quot;管理&quot; &gt;管理 &lt;br&gt;&lt;br&gt; &lt;input type=&quot;radio&quot; name=&quot;sex&quot; value=&quot;男&quot; &gt;男 &lt;input type=&quot;radio&quot; name=&quot;sex&quot; value=&quot;女&quot; &gt;女 &lt;br&gt;&lt;br&gt; &lt;select name=&quot;job&quot; id=&quot;job&quot; multiple=&quot;multiple&quot; size=4&gt; &lt;option&gt;程序员&lt;/option&gt; &lt;option&gt;中级程序员&lt;/option&gt; &lt;option&gt;高级程序员&lt;/option&gt; &lt;option&gt;系统分析师&lt;/option&gt; &lt;/select&gt; &lt;select name=&quot;edu&quot; id=&quot;edu&quot;&gt; &lt;option&gt;本科&lt;/option&gt; &lt;option&gt;博士&lt;/option&gt; &lt;option&gt;硕士&lt;/option&gt; &lt;option&gt;大专&lt;/option&gt; &lt;/select&gt; &lt;br/&gt; &lt;div id=&quot;two&quot; class=&quot;mini&quot; &gt; id为two class是 mini div &lt;div class=&quot;mini&quot; &gt;class是 mini&lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;one&quot; &gt; class是 one &lt;div class=&quot;mini&quot; &gt;class是 mini&lt;/div&gt; &lt;div class=&quot;mini&quot; &gt;class是 mini&lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;one&quot; &gt; class是 one &lt;div class=&quot;mini01&quot; &gt;class是 mini01&lt;/div&gt; &lt;div class=&quot;mini&quot; &gt;class是 mini&lt;/div&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt; DOM操作123456789101112131415161718192021222324252627282930313233343536373839404142431. 内容操作 1. html(): 获取/设置元素的标签体内容 &lt;a&gt;&lt;font&gt;内容&lt;/font&gt;&lt;/a&gt; --&gt; &lt;font&gt;内容&lt;/font&gt; 2. text(): 获取/设置元素的标签体纯文本内容 &lt;a&gt;&lt;font&gt;内容&lt;/font&gt;&lt;/a&gt; --&gt; 内容 3. val()： 获取/设置元素的value属性值2. 属性操作 1. 通用属性操作 1. attr(): 获取/设置元素的属性 2. removeAttr():删除属性 3. prop():获取/设置元素的属性 4. removeProp():删除属性 * attr和prop区别？ 1. 如果操作的是元素的固属性，则建议使用prop 2. 如果操作的是元素自定义的属性，则建议使用attr 2. 对class属性操作 1. addClass():添加class属性值 2. removeClass():删除class属性值 3. toggleClass():切换class属性 * toggleClass(&quot;one&quot;): * 判断如果元素对象上存在class=&quot;one&quot;，则将属性值one删除掉。 如果元素对象上不存在class=&quot;one&quot;，则添加 4. css():3. CRUD操作: 1. append():父元素将子元素追加到末尾 * 对象1.append(对象2): 将对象2添加到对象1元素内部，并且在末尾 2. prepend():父元素将子元素追加到开头 * 对象1.prepend(对象2):将对象2添加到对象1元素内部，并且在开头 3. appendTo(): * 对象1.appendTo(对象2):将对象1添加到对象2内部，并且在末尾 4. prependTo()： * 对象1.prependTo(对象2):将对象1添加到对象2内部，并且在开头 5. after():添加元素到元素后边 * 对象1.after(对象2)： 将对象2添加到对象1后边。对象1和对象2是兄弟关系 6. before():添加元素到元素前边 * 对象1.before(对象2)： 将对象2添加到对象1前边。对象1和对象2是兄弟关系 7. insertAfter() * 对象1.insertAfter(对象2)：将对象2添加到对象1后边。对象1和对象2是兄弟关系 8. insertBefore() * 对象1.insertBefore(对象2)： 将对象2添加到对象1前边。对象1和对象2是兄弟关系 9. remove():移除元素 * 对象.remove():将对象删除掉 10. empty():清空元素的所有后代元素。 * 对象.empty():将对象的后代元素全部清空，但是保留当前对象以及其属性节点 案例01-html&amp;text&amp;val.html12345678910111213141516171819202122232425262728293031&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;&lt;/title&gt; &lt;script src=&quot;../js/jquery-3.3.1.min.js&quot;&gt;&lt;/script&gt; &lt;script&gt; // 获取myinput 的value值 $(function () &#123; alert($(&quot;#myinput&quot;).val()); &#125;); // 获取mydiv的标签体内容 $(function () &#123; alert($(&quot;#mydiv&quot;).html()); &#125;); // 获取mydiv文本内容 $(function () &#123; alert($(&quot;#mydiv&quot;).text()); &#125;); &lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;input id=&quot;myinput&quot; type=&quot;text&quot; name=&quot;username&quot; value=&quot;张&quot; /&gt; &lt;div id=&quot;mydiv&quot;&gt;&lt;p&gt;&lt;a href=&quot;#&quot;&gt;标题标签&lt;/a&gt;&lt;/p&gt;&lt;/div&gt; &lt;/body&gt;&lt;/html&gt; 02-prop.html12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576&lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 4.01 Transitional//EN&quot;&gt;&lt;html&gt; &lt;head&gt; &lt;title&gt;获取属性&lt;/title&gt; &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot;&gt; &lt;script src=&quot;../js/jquery-3.3.1.min.js&quot;&gt;&lt;/script&gt; &lt;style type=&quot;text/css&quot;&gt; div,span&#123; width: 140px; height: 140px; margin: 20px; background: #9999CC; border: #000 1px solid; float:left; font-size: 17px; font-family:Roman; &#125; div.mini&#123; width: 30px; height: 30px; background: #CC66FF; border: #000 1px solid; font-size: 12px; font-family:Roman; &#125; div.visible&#123; display:none; &#125; &lt;/style&gt; &lt;script type=&quot;text/javascript&quot;&gt; //获取北京节点的name属性值 $(function () &#123; alert($(&quot;#bj&quot;).attr(&quot;name&quot;)); &#125;) //设置北京节点的name属性的值为dabeijing $(function () &#123; alert($(&quot;#bj&quot;).attr(&quot;name&quot;,&quot;dabeijing&quot;)); &#125;) //新增北京节点的discription属性 属性值是didu $(function () &#123; alert($(&quot;#bj&quot;).attr(&quot;discription&quot;,&quot;didu&quot;)); &#125;) //删除北京节点的name属性并检验name属性是否存在 $(function () &#123; alert($(&quot;#bj&quot;).removeAttr(&quot;name&quot;)); &#125;) //获得hobby的的中状态 $(function () &#123; alert($(&quot;#hobby&quot;).prop(&quot;checked&quot;)); &#125;) &lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;ul&gt; &lt;li id=&quot;bj&quot; name=&quot;beijing&quot; xxx=&quot;yyy&quot;&gt;北京&lt;/li&gt; &lt;li id=&quot;tj&quot; name=&quot;tianjin&quot;&gt;天津&lt;/li&gt; &lt;/ul&gt; &lt;input type=&quot;checkbox&quot; id=&quot;hobby&quot;/&gt; &lt;/body&gt; &lt;/html&gt; 03-class&amp;css.html123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151&lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 4.01 Transitional//EN&quot;&gt;&lt;html&gt; &lt;head&gt; &lt;title&gt;样式操作&lt;/title&gt; &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot;&gt; &lt;script src=&quot;../js/jquery-3.3.1.min.js&quot;&gt;&lt;/script&gt; &lt;style type=&quot;text/css&quot;&gt; .one&#123; width: 200px; height: 140px; margin: 20px; background: red; border: #000 1px solid; float:left; font-size: 17px; font-family:Roman; &#125; div,span&#123; width: 140px; height: 140px; margin: 20px; background: #9999CC; border: #000 1px solid; float:left; font-size: 17px; font-family:Roman; &#125; div .mini&#123; width: 40px; height: 40px; background: #CC66FF; border: #000 1px solid; font-size: 12px; font-family:Roman; &#125; div .mini01&#123; width: 40px; height: 40px; background: #CC66FF; border: #000 1px solid; font-size: 12px; font-family:Roman; &#125; /*待用的样式*/ .second&#123; width: 300px; height: 340px; margin: 20px; background: yellow; border: pink 3px dotted; float:left; font-size: 22px; font-family:Roman; &#125; &lt;/style&gt; &lt;script type=&quot;text/javascript&quot;&gt; //&lt;input type=&quot;button&quot; value=&quot;采用属性增加样式(改变id=one的样式)&quot; id=&quot;b1&quot;/&gt; $(function () &#123; $(&quot;#b1&quot;).click(function () &#123; $(&quot;#one&quot;).prop(&quot;class&quot;,&quot;second&quot;); &#125;); &#125;); //&lt;input type=&quot;button&quot; value=&quot; addClass&quot; id=&quot;b2&quot;/&gt; $(function () &#123; $(&quot;#b2&quot;).click(function () &#123; $(&quot;#one&quot;).addClass(&quot;second&quot;); &#125;); &#125;); //&lt;input type=&quot;button&quot; value=&quot;removeClass&quot; id=&quot;b3&quot;/&gt; $(function () &#123; $(&quot;#b3&quot;).click(function () &#123; $(&quot;#one&quot;).removeClass(&quot;second&quot;); &#125;); &#125;); //&lt;input type=&quot;button&quot; value=&quot; 切换样式&quot; id=&quot;b4&quot;/&gt; $(function () &#123; $(&quot;#b4&quot;).click(function () &#123; $(&quot;#one&quot;).toggleClass(&quot;second&quot;); &#125;); &#125;); //&lt;input type=&quot;button&quot; value=&quot; 通过css()获得id为one背景颜色&quot; id=&quot;b5&quot;/&gt; $(function () &#123; $(&quot;#b5&quot;).click(function () &#123; var color=$(&quot;#one&quot;).css(&quot;backgroundColor&quot;); alert(color); &#125;); &#125;); //&lt;input type=&quot;button&quot; value=&quot; 通过css()设置id为one背景颜色为绿色&quot; id=&quot;b6&quot;/&gt; $(function () &#123; $(&quot;#b6&quot;).click(function () &#123; $(&quot;#one&quot;).css(&quot;backgroundColor&quot;,&quot;green&quot;); &#125;); &#125;); &lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;input type=&quot;button&quot; value=&quot;保存&quot; class=&quot;mini&quot; name=&quot;ok&quot; class=&quot;mini&quot; /&gt; &lt;input type=&quot;button&quot; value=&quot;采用属性增加样式(改变id=one的样式)&quot; id=&quot;b1&quot;/&gt; &lt;input type=&quot;button&quot; value=&quot; addClass&quot; id=&quot;b2&quot;/&gt; &lt;input type=&quot;button&quot; value=&quot;removeClass&quot; id=&quot;b3&quot;/&gt; &lt;input type=&quot;button&quot; value=&quot; 切换样式&quot; id=&quot;b4&quot;/&gt; &lt;input type=&quot;button&quot; value=&quot; 通过css()获得id为one背景颜色&quot; id=&quot;b5&quot;/&gt; &lt;input type=&quot;button&quot; value=&quot; 通过css()设置id为one背景颜色为绿色&quot; id=&quot;b6&quot;/&gt; &lt;h1&gt;一种奇迹叫坚持&lt;/h1&gt; &lt;h2&gt;自信源于努力&lt;/h2&gt; &lt;div id=&quot;one&quot;&gt; id为one &lt;/div&gt; &lt;div id=&quot;two&quot; class=&quot;mini&quot; &gt; id为two class是 mini &lt;div class=&quot;mini&quot; &gt;class是 mini&lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;one&quot; &gt; class是 one &lt;div class=&quot;mini&quot; &gt;class是 mini&lt;/div&gt; &lt;div class=&quot;mini&quot; &gt;class是 mini&lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;one&quot; &gt; class是 one &lt;div class=&quot;mini01&quot; &gt;class是 mini01&lt;/div&gt; &lt;div class=&quot;mini&quot; &gt;class是 mini&lt;/div&gt; &lt;/div&gt; &lt;span class=&quot;spanone&quot;&gt; span &lt;/span&gt; &lt;/body&gt; &lt;/html&gt; 04-create&amp;append.html1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889&lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 4.01 Transitional//EN&quot;&gt;&lt;html&gt; &lt;head&gt; &lt;title&gt;内部插入脚本&lt;/title&gt; &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot;&gt; &lt;script src=&quot;../js/jquery-3.3.1.min.js&quot;&gt;&lt;/script&gt; &lt;style type=&quot;text/css&quot;&gt; div,span&#123; width: 140px; height: 140px; margin: 20px; background: #9999CC; border: #000 1px solid; float:left; font-size: 17px; font-family:Roman; &#125; div .mini&#123; width: 30px; height: 30px; background: #CC66FF; border: #000 1px solid; font-size: 12px; font-family:Roman; &#125; div.visible&#123; display:none; &#125; &lt;/style&gt; &lt;script type=&quot;text/javascript&quot;&gt; // &lt;input type=&quot;button&quot; value=&quot;将反恐放置到city的后面&quot; id=&quot;b1&quot;/&gt; $(function () &#123; $(&quot;#b1&quot;).click(function () &#123; $(&quot;#city&quot;).append($(&quot;#fk&quot;)); &#125;); &#125;); // &lt;input type=&quot;button&quot; value=&quot;将反恐放置到city的最前面&quot; id=&quot;b2&quot;/&gt; $(function () &#123; $(&quot;#b2&quot;).click(function () &#123; $(&quot;#city&quot;).prepend($(&quot;#fk&quot;)); &#125;); &#125;); // &lt;input type=&quot;button&quot; value=&quot;将反恐插入到天津后面&quot; id=&quot;b3&quot;/&gt; $(function () &#123; $(&quot;#b3&quot;).click(function () &#123; $(&quot;#fk&quot;).before($(&quot;#tj&quot;)); &#125;); &#125;); // &lt;input type=&quot;button&quot; value=&quot;将反恐插入到天津前面&quot; id=&quot;b4&quot;/&gt; $(function () &#123; $(&quot;#b4&quot;).click(function () &#123; $(&quot;#fk&quot;).after($(&quot;#tj&quot;)); &#125;); &#125;); &lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;input type=&quot;button&quot; value=&quot;将反恐放置到city的后面&quot; id=&quot;b1&quot;/&gt; &lt;input type=&quot;button&quot; value=&quot;将反恐放置到city的最前面&quot; id=&quot;b2&quot;/&gt; &lt;input type=&quot;button&quot; value=&quot;将反恐插入到天津后面&quot; id=&quot;b3&quot;/&gt; &lt;input type=&quot;button&quot; value=&quot;将反恐插入到天津前面&quot; id=&quot;b4&quot;/&gt; &lt;ul id=&quot;city&quot;&gt; &lt;li id=&quot;bj&quot; name=&quot;beijing&quot;&gt;北京&lt;/li&gt; &lt;li id=&quot;tj&quot; name=&quot;tianjin&quot;&gt;天津&lt;/li&gt; &lt;li id=&quot;cq&quot; name=&quot;chongqing&quot;&gt;重庆&lt;/li&gt; &lt;/ul&gt; &lt;ul id=&quot;love&quot;&gt; &lt;li id=&quot;fk&quot; name=&quot;fankong&quot;&gt;反恐&lt;/li&gt; &lt;li id=&quot;xj&quot; name=&quot;xingji&quot;&gt;星际&lt;/li&gt; &lt;/ul&gt; &lt;div id=&quot;foo1&quot;&gt;Hello1&lt;/div&gt; &lt;/body&gt; &lt;/html&gt; 05-remove.html123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566&lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 4.01 Transitional//EN&quot;&gt;&lt;html&gt; &lt;head&gt; &lt;title&gt;删除节点&lt;/title&gt; &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot;&gt; &lt;script src=&quot;../js/jquery-3.3.1.min.js&quot;&gt;&lt;/script&gt; &lt;style type=&quot;text/css&quot;&gt; div,span&#123; width: 140px; height: 140px; margin: 20px; background: #9999CC; border: #000 1px solid; float:left; font-size: 17px; font-family:Roman; &#125; div.mini&#123; width: 30px; height: 30px; background: #CC66FF; border: #000 1px solid; font-size: 12px; font-family:Roman; &#125; div.visible&#123; display:none; &#125; &lt;/style&gt; &lt;script type=&quot;text/javascript&quot;&gt; // &lt;input type=&quot;button&quot; value=&quot;删除&lt;li id=&#x27;bj&#x27; name=&#x27;beijing&#x27;&gt;北京&lt;/li&gt;&quot; id=&quot;b1&quot;/&gt; // &lt;input type=&quot;button&quot; value=&quot;删除city所的li节点 清空元素中的所后代节点(不包含属性节点)&quot; id=&quot;b2&quot;/&gt; $(function () &#123; $(&quot;#b1&quot;).click(function () &#123; $(&quot;#bj&quot;).remove(); &#125;); $(&quot;#b2&quot;).click(function () &#123; $(&quot;#city&quot;).empty(); &#125;); &#125;); &lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;input type=&quot;button&quot; value=&quot;删除&lt;li id=&#x27;bj&#x27; name=&#x27;beijing&#x27;&gt;北京&lt;/li&gt;&quot; id=&quot;b1&quot;/&gt; &lt;input type=&quot;button&quot; value=&quot;删除所的子节点 清空元素中的所后代节点(不包含属性节点)&quot; id=&quot;b2&quot;/&gt; &lt;ul id=&quot;city&quot;&gt; &lt;li id=&quot;bj&quot; name=&quot;beijing&quot;&gt;北京&lt;/li&gt; &lt;li id=&quot;tj&quot; name=&quot;tianjin&quot;&gt;天津&lt;/li&gt; &lt;li id=&quot;cq&quot; name=&quot;chongqing&quot;&gt;重庆&lt;/li&gt; &lt;/ul&gt; &lt;p class=&quot;hello&quot;&gt;Hello&lt;/p&gt; how are &lt;p&gt;you?&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; 案例01.隔行换色.html12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;&lt;/title&gt; &lt;script src=&quot;../../js/jquery-3.3.1.min.js&quot;&gt;&lt;/script&gt; &lt;script&gt; //需求：将数据行的奇数行背景色设置为 pink，偶数行背景色设置为 yellow $(function () &#123; $(&quot;tr:gt(0):odd&quot;).css(&quot;backgroundColor&quot;,&quot;yellow&quot;); $(&quot;tr:gt(0):even&quot;).css(&quot;backgroundColor&quot;,&quot;pink&quot;); &#125;) &lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;table id=&quot;tab1&quot; border=&quot;1&quot; width=&quot;800&quot; align=&quot;center&quot; &gt; &lt;tr&gt; &lt;td colspan=&quot;5&quot;&gt;&lt;input type=&quot;button&quot; value=&quot;删除&quot;&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr style=&quot;background-color: #999999;&quot;&gt; &lt;th&gt;&lt;input type=&quot;checkbox&quot;&gt;&lt;/th&gt; &lt;th&gt;分类ID&lt;/th&gt; &lt;th&gt;分类名称&lt;/th&gt; &lt;th&gt;分类描述&lt;/th&gt; &lt;th&gt;操作&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;input type=&quot;checkbox&quot;&gt;&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;手机数码&lt;/td&gt; &lt;td&gt;手机数码类商品&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;&quot;&gt;修改&lt;/a&gt;|&lt;a href=&quot;&quot;&gt;删除&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;input type=&quot;checkbox&quot;&gt;&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;电脑办公&lt;/td&gt; &lt;td&gt;电脑办公类商品&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;&quot;&gt;修改&lt;/a&gt;|&lt;a href=&quot;&quot;&gt;删除&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;input type=&quot;checkbox&quot;&gt;&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;鞋靴箱包&lt;/td&gt; &lt;td&gt;鞋靴箱包类商品&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;&quot;&gt;修改&lt;/a&gt;|&lt;a href=&quot;&quot;&gt;删除&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;input type=&quot;checkbox&quot;&gt;&lt;/td&gt; &lt;td&gt;4&lt;/td&gt; &lt;td&gt;家居饰品&lt;/td&gt; &lt;td&gt;家居饰品类商品&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;&quot;&gt;修改&lt;/a&gt;|&lt;a href=&quot;&quot;&gt;删除&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/body&gt;&lt;/html&gt; 全选和全不选.html12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;&lt;/title&gt; &lt;script src=&quot;../../js/jquery-3.3.1.min.js&quot;&gt;&lt;/script&gt; &lt;script&gt; function selectAll(obj) &#123; $(&quot;.itemSelect&quot;).prop(&quot;checked&quot;,obj.checked); &#125; &lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;table id=&quot;tab1&quot; border=&quot;1&quot; width=&quot;800&quot; align=&quot;center&quot; &gt; &lt;tr&gt; &lt;td colspan=&quot;5&quot;&gt;&lt;input type=&quot;button&quot; value=&quot;删除&quot;&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;&lt;input type=&quot;checkbox&quot; onclick=&quot;selectAll(this)&quot; &gt;&lt;/th&gt; &lt;th&gt;分类ID&lt;/th&gt; &lt;th&gt;分类名称&lt;/th&gt; &lt;th&gt;分类描述&lt;/th&gt; &lt;th&gt;操作&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;input type=&quot;checkbox&quot; class=&quot;itemSelect&quot;&gt;&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;手机数码&lt;/td&gt; &lt;td&gt;手机数码类商品&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;&quot;&gt;修改&lt;/a&gt;|&lt;a href=&quot;&quot;&gt;删除&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;input type=&quot;checkbox&quot; class=&quot;itemSelect&quot;&gt;&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;电脑办公&lt;/td&gt; &lt;td&gt;电脑办公类商品&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;&quot;&gt;修改&lt;/a&gt;|&lt;a href=&quot;&quot;&gt;删除&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;input type=&quot;checkbox&quot; class=&quot;itemSelect&quot;&gt;&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;鞋靴箱包&lt;/td&gt; &lt;td&gt;鞋靴箱包类商品&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;&quot;&gt;修改&lt;/a&gt;|&lt;a href=&quot;&quot;&gt;删除&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;input type=&quot;checkbox&quot; class=&quot;itemSelect&quot;&gt;&lt;/td&gt; &lt;td&gt;4&lt;/td&gt; &lt;td&gt;家居饰品&lt;/td&gt; &lt;td&gt;家居饰品类商品&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;&quot;&gt;修改&lt;/a&gt;|&lt;a href=&quot;&quot;&gt;删除&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/body&gt;&lt;/html&gt; QQ表情选择.html12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot; /&gt; &lt;title&gt;QQ表情择&lt;/title&gt; &lt;script src=&quot;../../js/jquery-3.3.1.min.js&quot;&gt;&lt;/script&gt; &lt;style type=&quot;text/css&quot;&gt; *&#123;margin: 0;padding: 0;list-style: none;&#125; .emoji&#123;margin:50px;&#125; ul&#123;overflow: hidden;&#125; li&#123;float: left;width: 48px;height: 48px;cursor: pointer;&#125; .emoji img&#123; cursor: pointer; &#125; &lt;/style&gt; &lt;script&gt; //需求：点击qq表情，将其追加到发言框中 $(function () &#123; $(&quot;ul img&quot;).click(function () &#123; $(&quot;.word&quot;).append($(this).clone()); &#125;); &#125;); &lt;/script&gt; &lt;/head&gt;&lt;body&gt; &lt;div class=&quot;emoji&quot;&gt; &lt;ul&gt; &lt;li&gt;&lt;img src=&quot;../img/01.gif&quot; height=&quot;22&quot; width=&quot;22&quot; alt=&quot;&quot; /&gt;&lt;/li&gt; &lt;li&gt;&lt;img src=&quot;../img/02.gif&quot; height=&quot;22&quot; width=&quot;22&quot; alt=&quot;&quot; /&gt;&lt;/li&gt; &lt;li&gt;&lt;img src=&quot;../img/03.gif&quot; height=&quot;22&quot; width=&quot;22&quot; alt=&quot;&quot; /&gt;&lt;/li&gt; &lt;li&gt;&lt;img src=&quot;../img/04.gif&quot; height=&quot;22&quot; width=&quot;22&quot; alt=&quot;&quot; /&gt;&lt;/li&gt; &lt;li&gt;&lt;img src=&quot;../img/05.gif&quot; height=&quot;22&quot; width=&quot;22&quot; alt=&quot;&quot; /&gt;&lt;/li&gt; &lt;li&gt;&lt;img src=&quot;../img/06.gif&quot; height=&quot;22&quot; width=&quot;22&quot; alt=&quot;&quot; /&gt;&lt;/li&gt; &lt;li&gt;&lt;img src=&quot;../img/07.gif&quot; height=&quot;22&quot; width=&quot;22&quot; alt=&quot;&quot; /&gt;&lt;/li&gt; &lt;li&gt;&lt;img src=&quot;../img/08.gif&quot; height=&quot;22&quot; width=&quot;22&quot; alt=&quot;&quot; /&gt;&lt;/li&gt; &lt;li&gt;&lt;img src=&quot;../img/09.gif&quot; height=&quot;22&quot; width=&quot;22&quot; alt=&quot;&quot; /&gt;&lt;/li&gt; &lt;li&gt;&lt;img src=&quot;../img/10.gif&quot; height=&quot;22&quot; width=&quot;22&quot; alt=&quot;&quot; /&gt;&lt;/li&gt; &lt;li&gt;&lt;img src=&quot;../img/11.gif&quot; height=&quot;22&quot; width=&quot;22&quot; alt=&quot;&quot; /&gt;&lt;/li&gt; &lt;li&gt;&lt;img src=&quot;../img/12.gif&quot; height=&quot;22&quot; width=&quot;22&quot; alt=&quot;&quot; /&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p class=&quot;word&quot;&gt; &lt;strong&gt;请发言：&lt;/strong&gt; &lt;img src=&quot;../img/12.gif&quot; height=&quot;22&quot; width=&quot;22&quot; alt=&quot;&quot; /&gt; &lt;/p&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 左右移动.html123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;&lt;/title&gt; &lt;script src=&quot;../js/jquery-3.3.1.min.js&quot;&gt;&lt;/script&gt; &lt;style&gt; #leftName, #btn, #rightName &#123; float: left; width: 100px; height: 300px; &#125; #toRight, #toLeft &#123; margin-top: 100px; margin-left: 30px; width: 50px; &#125; .border &#123; height: 500px; padding: 100px; &#125; &lt;/style&gt; &lt;script&gt; //需求：实现下拉列表选择条目左右选择功能 $(function () &#123; $(&quot;#toRight&quot;).click(function () &#123; $(&quot;#rightName&quot;).append($(&quot;#leftName&gt;option:selected&quot;)); &#125;); $(&quot;#toLeft&quot;).click(function () &#123; $(&quot;#leftName&quot;).append($(&quot;#rightName&gt;option:selected&quot;)); &#125;); &#125;); &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&quot;border&quot;&gt; &lt;select id=&quot;leftName&quot; multiple=&quot;multiple&quot;&gt; &lt;option&gt;张&lt;/option&gt; &lt;option&gt;李四&lt;/option&gt; &lt;option&gt;王五&lt;/option&gt; &lt;option&gt;赵六&lt;/option&gt; &lt;/select&gt; &lt;div id=&quot;btn&quot;&gt; &lt;input type=&quot;button&quot; id=&quot;toRight&quot; value=&quot;--&gt;&quot;&gt;&lt;br&gt; &lt;input type=&quot;button&quot; id=&quot;toLeft&quot; value=&quot;&lt;--&quot;&gt; &lt;/div&gt; &lt;select id=&quot;rightName&quot; multiple=&quot;multiple&quot;&gt; &lt;option&gt;钱七&lt;/option&gt; &lt;/select&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 2.Jquery高级动画123456789101112131415161718192021221. 三种方式显示和隐藏元素 1. 默认显示和隐藏方式 1. show([speed,[easing],[fn]]) 1. 参数： 1. speed：动画的速度。三个预定义的值(&quot;slow&quot;,&quot;normal&quot;, &quot;fast&quot;)或表示动画时长的毫秒数值(如：1000) 2. easing：用来指定切换效果，默认是&quot;swing&quot;，可用参数&quot;linear&quot; * swing：动画执行时效果是 先慢，中间快，最后又慢 * linear：动画执行时速度是匀速的 3. fn：在动画完成时执行的函数，每个元素执行一次。 2. hide([speed,[easing],[fn]]) 3. toggle([speed],[easing],[fn]) 2. 滑动显示和隐藏方式 1. slideDown([speed],[easing],[fn]) 2. slideUp([speed,[easing],[fn]]) 3. slideToggle([speed],[easing],[fn]) 3. 淡入淡出显示和隐藏方式 1. fadeIn([speed],[easing],[fn]) 2. fadeOut([speed],[easing],[fn]) 3. fadeToggle([speed,[easing],[fn]]) 01-显示隐藏动画.html123456789101112131415161718192021222324252627282930313233&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Insert title here&lt;/title&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;../js/jquery-3.3.1.min.js&quot;&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;input type=&quot;button&quot; value=&quot;点击钮隐藏div&quot; onclick=&quot;hideFn()&quot;&gt;&lt;input type=&quot;button&quot; value=&quot;点击钮显示div&quot; onclick=&quot;showFn()&quot;&gt;&lt;input type=&quot;button&quot; value=&quot;点击钮切换div显示和隐藏&quot; onclick=&quot;toggleFn()&quot;&gt;&lt;script&gt; function hideFn() &#123; // $(&quot;#showDiv&quot;).hide(); // $(&quot;#showDiv&quot;).slideDown(); $(&quot;#showDiv&quot;).fadeOut(); &#125; function showFn() &#123; // $(&quot;#showDiv&quot;).show(); // $(&quot;#showDiv&quot;).slideUp(); $(&quot;#showDiv&quot;).fadeIn(); &#125; function toggleFn() &#123; // $(&quot;#showDiv&quot;).toggle(); // $(&quot;#showDiv&quot;).slideToggle(); $(&quot;#showDiv&quot;).fadeToggle(); &#125;&lt;/script&gt;&lt;div id=&quot;showDiv&quot; style=&quot;width:300px;height:300px;background:pink&quot;&gt; div显示和隐藏&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 遍历123456789101112131415161. js的遍历方式 * for(初始化值;循环结束条件;步长)2. jq的遍历方式 1. jq对象.each(callback) 1. 语法： jquery对象.each(function(index,element)&#123;&#125;); * index:就是元素在集合中的索引 * element：就是集合中的每一个元素对象 * this：集合中的每一个元素对象 2. 回调函数返回值： * true:如果当前function返回为false，则结束循环(break)。 * false:如果当前function返回为true，则结束本次循环，继续下次循环(continue) 2. $.each(object, [callback]) 3. for..of: jquery 3.0 版本之后提供的方式 for(元素对象 of 容器对象) 01-遍历.html12345678910111213141516171819202122232425262728293031323334353637383940414243&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;&lt;/title&gt; &lt;script src=&quot;../js/jquery-3.3.1.min.js&quot; type=&quot;text/javascript&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot;&gt; $(function () &#123; var citys=$(&quot;#city li&quot;); //第一种遍历方式 citys.each(function () &#123; this.innerHTML; &#125;); //第2种遍历方式 citys.each(function (index, element) &#123; if (&quot;上海&quot;==$(element).html()) &#123; //return false;//跳出循环，相当于break return true;//相当于continue &#125; alert(index+&quot;:&quot;+$(element).html()); &#125;); //第3种遍历方式 $.each(citys,function () &#123; $(this).html(); &#125;); //第四种遍历方式 for...of jquery3.0版本 for(li of citys)&#123; alert($(li).html()); &#125; &#125;); &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;ul id=&quot;city&quot;&gt; &lt;li&gt;北京&lt;/li&gt; &lt;li&gt;上海&lt;/li&gt; &lt;li&gt;天津&lt;/li&gt; &lt;li&gt;重庆&lt;/li&gt;&lt;/ul&gt;&lt;/body&gt;&lt;/html&gt; 事件绑定和插件1234567891011121314151617181920211. jquery标准的绑定方式 * jq对象.事件方法(回调函数)； * 注：如果调用事件方法，不传递回调函数，则会触发浏览器默认行为。 * 表单对象.submit();//让表单提交2. on绑定事件/off解除绑定 * jq对象.on(&quot;事件名称&quot;,回调函数) * jq对象.off(&quot;事件名称&quot;) * 如果off方法不传递任何参数，则将组件上的所有事件全部解绑3. 事件切换：toggle * jq对象.toggle(fn1,fn2...) * 当单击jq对象对应的组件后，会执行fn1.第二次点击会执行fn2..... * 注意：1.9版本 .toggle() 方法删除,jQuery Migrate（迁移）插件可以恢复此功能。 &lt;script src=&quot;../js/jquery-migrate-1.0.0.js&quot; type=&quot;text/javascript&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;插件：增强JQuery的功能1. 实现方式： 1. $.fn.extend(object) * 增强通过Jquery获取的对象的功能 $(&quot;#id&quot;) 2. $.extend(object) * 增强JQeury对象自身的功能 $/jQuery 01-绑定事件.html12345678910111213141516171819&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;&lt;/title&gt; &lt;script src=&quot;js/jquery-3.3.1.min.js&quot; type=&quot;text/javascript&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot;&gt; $(function () &#123; $(&quot;input[type=&#x27;text&#x27;]&quot;).click(function () &#123; alert(&quot;666&quot;); &#125;); &#125;); &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;input id=&quot;name&quot; type=&quot;text&quot; value=&quot;绑定点击事件&quot;&gt;&lt;/body&gt;&lt;/html&gt; 02-on绑定事件.html1234567891011121314151617181920212223&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;&lt;/title&gt; &lt;script src=&quot;js/jquery-3.3.1.min.js&quot; type=&quot;text/javascript&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot;&gt; $(function () &#123; $(&quot;#btn&quot;).on(&quot;click&quot;,function () &#123; alert(&quot;6666&quot;); &#125;); $(&quot;#btn2&quot;).click(function () &#123; $(&quot;#btn&quot;).off(&quot;click&quot;); &#125;); &#125;); &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;input id=&quot;btn&quot; type=&quot;button&quot; value=&quot;使用on绑定点击事件&quot;&gt;&lt;input id=&quot;btn2&quot; type=&quot;button&quot; value=&quot;使用off解绑点击事件&quot;&gt;&lt;/body&gt;&lt;/html&gt; 03-事件切换.html123456789101112131415161718192021222324252627&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;&lt;/title&gt; &lt;script src=&quot;js/jquery-3.3.1.min.js&quot; type=&quot;text/javascript&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt; &lt;script src=&quot;js/jquery-migrate-1.0.0.js&quot; type=&quot;text/javascript&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot;&gt; $(function () &#123; $(&quot;#btn&quot;).toggle(function () &#123; $(&quot;#myDiv&quot;).css(&quot;backgroundColor&quot;,&quot;red&quot;); &#125;,function () &#123; $(&quot;#myDiv&quot;).css(&quot;backgroundColor&quot;,&quot;yellow&quot;); &#125;); &#125;); &lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;input id=&quot;btn&quot; type=&quot;button&quot; value=&quot;事件切换&quot;&gt; &lt;div id=&quot;myDiv&quot; style=&quot;width:300px;height:300px;background:pink&quot;&gt; 点击钮变成绿色，再次点击红色 &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 01-jQuery对象进行方法扩展.html12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;01-jQuery对象进行方法扩展&lt;/title&gt; &lt;script src=&quot;../js/jquery-3.3.1.min.js&quot; type=&quot;text/javascript&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot;&gt; //使用jquery插件 给jq对象添加2个方法 check()中所复框，uncheck()取消中所复框 //1.定义jqeury的对象插件 $.fn.extend(&#123; //定义了一个check()方法。所的jq对象都可以调用该方法 check:function () &#123; //让复框中 //this:调用该方法的jq对象 this.prop(&quot;checked&quot;,true); &#125;, uncheck:function () &#123; //让复框不中 this.prop(&quot;checked&quot;,false); &#125; &#125;); $(function () &#123; // 获取钮 //$(&quot;#btn-check&quot;).check(); //复框对象.check(); $(&quot;#btn-check&quot;).click(function () &#123; //获取复框对象 $(&quot;input[type=&#x27;checkbox&#x27;]&quot;).check(); &#125;); $(&quot;#btn-uncheck&quot;).click(function () &#123; //获取复框对象 $(&quot;input[type=&#x27;checkbox&#x27;]&quot;).uncheck(); &#125;); &#125;); &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;input id=&quot;btn-check&quot; type=&quot;button&quot; value=&quot;点击中复框&quot; onclick=&quot;checkFn()&quot;&gt;&lt;input id=&quot;btn-uncheck&quot; type=&quot;button&quot; value=&quot;点击取消复框中&quot; onclick=&quot;uncheckFn()&quot;&gt;&lt;br/&gt;&lt;input type=&quot;checkbox&quot; value=&quot;football&quot;&gt;足球&lt;input type=&quot;checkbox&quot; value=&quot;basketball&quot;&gt;篮球&lt;input type=&quot;checkbox&quot; value=&quot;volleyball&quot;&gt;排球&lt;/body&gt;&lt;/html&gt; 02-jQuery全局进行方法扩展.html123456789101112131415161718192021222324252627282930313233&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;01-jQuery对象进行方法扩展&lt;/title&gt; &lt;script src=&quot;../js/jquery-3.3.1.min.js&quot; type=&quot;text/javascript&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot;&gt; //对全局方法扩展2个方法，扩展min方法：求2个值的最小值；扩展max方法：求2个值最大值 $.extend(&#123; max:function (a,b) &#123; //返回两数中的较大值 return a &gt;= b ? a:b; &#125;, min:function (a,b) &#123; //返回两数中的较小值 return a &lt;= b ? a:b; &#125; &#125;); //调用全局方法 var max = $.max(4,3); //alert(max); var min = $.min(1,2); alert(min); &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;/body&gt;&lt;/html&gt; 案例广告的自动显示与隐藏.html123456789101112131415161718192021222324252627282930313233343536373839&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;广告的自动显示与隐藏&lt;/title&gt; &lt;style&gt; #content&#123;width:100%;height:500px;background:#999&#125; &lt;/style&gt; &lt;!--引入jquery--&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;js/jquery-3.3.1.min.js&quot;&gt;&lt;/script&gt; &lt;script&gt; $(function () &#123; setTimeout(show,3000); setTimeout(hide,8000); &#125;); function show() &#123; $(&quot;#ad&quot;).show(&quot;slow&quot;); &#125; function hide() &#123; $(&quot;#ad&quot;).hide(&quot;slow&quot;); &#125; &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;!-- 整体的DIV --&gt;&lt;div&gt; &lt;!-- 广告DIV --&gt; &lt;div id=&quot;ad&quot; style=&quot;display: none;&quot;&gt; &lt;img style=&quot;width:100%&quot; src=&quot;img/adv.jpg&quot; /&gt; &lt;/div&gt; &lt;!-- 下方正文部分 --&gt; &lt;div id=&quot;content&quot;&gt; 正文部分 &lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 抽奖.html12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;jquery案例之抽奖&lt;/title&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;js/jquery-3.3.1.min.js&quot;&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;!-- 小像框 --&gt;&lt;div style=&quot;border-style:dotted;width:160px;height:100px&quot;&gt; &lt;img id=&quot;img1ID&quot; src=&quot;img/man00.jpg&quot; style=&quot;width:160px;height:100px&quot;/&gt;&lt;/div&gt;&lt;!-- 大像框 --&gt;&lt;div style=&quot;border-style:double;width:800px;height:500px;position:absolute;left:500px;top:10px&quot;&gt; &lt;img id=&quot;img2ID&quot; src=&quot;img/man00.jpg&quot; width=&quot;800px&quot; height=&quot;500px&quot;/&gt;&lt;/div&gt;&lt;!-- 开始按钮 --&gt;&lt;input id=&quot;startID&quot; type=&quot;button&quot; value=&quot;点击开始&quot; style=&quot;width:150px;height:150px;font-size:22px&quot; onclick=&quot;imgStart()&quot;&gt;&lt;!-- 停止钮 --&gt;&lt;input id=&quot;stopID&quot; type=&quot;button&quot; value=&quot;点击停止&quot; style=&quot;width:150px;height:150px;font-size:22px&quot; onclick=&quot;imgStop()&quot;&gt;&lt;script language=&#x27;javascript&#x27; type=&#x27;text/javascript&#x27;&gt; //准备一个一维数组，装用户的像片路径 var imgs = [ &quot;img/man00.jpg&quot;, &quot;img/man01.jpg&quot;, &quot;img/man02.jpg&quot;, &quot;img/man03.jpg&quot;, &quot;img/man04.jpg&quot;, &quot;img/man05.jpg&quot;, &quot;img/man06.jpg&quot; ]; var startID; var index; $(function () &#123; $(&quot;#startID&quot;).prop(&quot;disabled&quot;,false); $(&quot;#stopID&quot;).prop(&quot;disabled&quot;,true); $(&quot;#startID&quot;).click(function () &#123; $(&quot;#startID&quot;).prop(&quot;disabled&quot;,true); $(&quot;#stopID&quot;).prop(&quot;disabled&quot;,false); startID=setInterval(function () &#123; index=Math.floor(Math.random()*7); $(&quot;#img1ID&quot;).prop(&quot;src&quot;,imgs[index]); &#125;,20); &#125;); $(&quot;#stopID&quot;).click(function () &#123; $(&quot;#startID&quot;).prop(&quot;disabled&quot;,false); $(&quot;#stopID&quot;).prop(&quot;disabled&quot;,true); clearInterval(startID); $(&quot;#img2ID&quot;).prop(&quot;src&quot;,imgs[index]).hide(); $(&quot;#img2ID&quot;).show(1000); &#125;); &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 九，xml12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455&lt;--1.概念 可扩展编辑语言：标签都是自定义的。 功能：存储数据，用来做配置文件或在网络中传输数据。 xml与html的区别： 1.xml标签都是自定义的，html标签事预定义的。 2.xml语法严格，html语法松散 3.xml用来存储数据，html用来展示数据。2.语法 基本语法： 1.第一行必须定义为文档声明。 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; 2.文档后缀名.xml 3.且仅一个根标签 4.属性值必须使用引号引起来 5.标签必须正确关闭 6.标签名称区分大小写 快速入门： 组成部分： 1.文档声明 格式：&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; 属性列表：version ,encoding（默认值iso8859-1,standalone（是否独立：取值yes，no 2.指令：结合css的（了解&lt;?xml-stylesheet type=&quot;text/css&quot; href=&quot;&quot; ?&gt; 3.标签:自定义的规则： 名称可以含字母、数字以及其他的字符 名称不能以数字或者标点符号开始 名称不能以字符 “xml”（或者 XML、Xml开始 名称不能包含空格 4.属性：id属性值唯一。 5.文本CDATA区：该地方的数据会被原样展示。&lt;![CDATA[]]&gt; 6.约束：规定xml文档的书写规则 1.能够在xml中引入约束文档 2.能够简单的读懂约束文档3.解析 概念：操作html文档，将文档中的数据集读取到内存中 方式： 1.DOM：将标记语言文档一次性加载进内存，在内存中形成一颗dom树。 优点：操作方便，可以对文档进行crud的所操作。 缺点：消耗内存。 2.SAX：逐行读取，基于事件驱动。 优点：不占内存 缺点：只能读取，不能增删改。 3.常见解析器： 1.JAXP：sun公司提供的解析器，支持两种思想。（了解 2.DOM4J： 3.Jsoup：HTML解析器。 4.PULL：安卓内置解析器。 4.Jsoup解析器： 步骤：导入jar包。 获取Document对象。 获取相应的标签。 获取数据--&gt; 读取XML文件内容123456789public static void main(String[] args) throws IOException &#123; //获取Document对象 String path = ClassLoader.getSystemClassLoader().getResource(&quot;users.xml&quot;).getPath(); Document document = Jsoup.parse(new File(path), &quot;utf-8&quot;); Elements elements = document.getElementsByTag(&quot;name&quot;); Element element = elements.get(0); String text = element.text(); System.out.println(text);&#125; Dom4j读取xmlbooks.xml 123456789101112131415&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;books&gt; &lt;book&gt; &lt;sn&gt;sn2017021119&lt;/sn&gt; &lt;name&gt;少年阿宾&lt;/name&gt; &lt;price&gt;9.9&lt;/price&gt; &lt;author&gt;ben&lt;/author&gt; &lt;/book&gt; &lt;book&gt; &lt;sn&gt;sn2017021120&lt;/sn&gt; &lt;name&gt;金鳞岂是池中物&lt;/name&gt; &lt;price&gt;9.9&lt;/price&gt; &lt;author&gt;monkey&lt;/author&gt; &lt;/book&gt;&lt;/books&gt; book 12345678910/** * @author yinhuidong * @createTime 2020-04-22-18:50 */public class Book &#123; private String sn; private String name; private BigDecimal price; private String author;&#125; test 1234567891011121314151617181920@Testpublic void test1() throws DocumentException &#123; //读取xml文件,生成document文档对象 SAXReader reader = new SAXReader(); String path=ClassLoader.getSystemClassLoader().getResource(&quot;books.xml&quot;).getPath(); Document document = reader.read(new File(path)); //通过document对象获取root根元素 Element element = document.getRootElement(); //通过根元素遍历里面的每一个book标签 List&lt;Element&gt; list = element.elements(&quot;book&quot;); for (Element l : list) &#123; String sn = l.elementText(&quot;sn&quot;); String name = l.elementText(&quot;name&quot;); String price = l.elementText(&quot;price&quot;); String author = l.elementText(&quot;author&quot;); Book book = new Book(sn, name, new BigDecimal(price), author); System.out.println(book); &#125;&#125; 十，Redis1.redis概念与下载安装12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576771. 概念： redis是一款高性能的NOSQL系列的非关系型数据库1.1.什么是NOSQL NoSQL(NoSQL = Not Only SQL)，意即“不仅仅是SQL”，是一项全新的数据库理念，泛指非关系型的数据库。 随着互联网web2.0网站的兴起，传统的关系数据库在应付web2.0网站，特别是超大规模和高并发的SNS类型的web2.0纯动态网站已经显得力不从心，暴露了很多难以克服的问题，而非关系型的数据库则由于其本身的特点得到了非常迅速的发展。NoSQL数据库的产生就是为了解决大规模数据集合多重数据种类带来的挑战，尤其是大数据应用难题。 1.1.1. NOSQL和关系型数据库比较 优点： 1）成本：nosql数据库简单易部署，基本都是开源软件，不需要像使用oracle那样花费大量成本购买使用，相比关系型数据库价格便宜。 2）查询速度：nosql数据库将数据存储于缓存之中，关系型数据库将数据存储在硬盘中，自然查询速度远不及nosql数据库。 3）存储数据的格式：nosql的存储格式是key,value形式、文档形式、图片形式等等，所以可以存储基础类型以及对象或者是集合等各种格式，而数据库则只支持基础类型。 4）扩展性：关系型数据库有类似join这样的多表查询机制的限制导致扩展很艰难。 缺点： 1）维护的工具和资料有限，因为nosql是属于新的技术，不能和关系型数据库10几年的技术同日而语。 2）不提供对sql的支持，如果不支持sql这样的工业标准，将产生一定用户的学习和使用成本。 3）不提供关系型数据库对事务的处理。 1.1.2. 非关系型数据库的优势： 1）性能NOSQL是基于键值对的，可以想象成表中的主键和值的对应关系，而且不需要经过SQL层的解析，所以性能非常高。 2）可扩展性同样也是因为基于键值对，数据之间没有耦合性，所以非常容易水平扩展。 1.1.3. 关系型数据库的优势： 1）复杂查询可以用SQL语句方便的在一个表以及多个表之间做非常复杂的数据查询。 2）事务支持使得对于安全性能很高的数据访问要求得以实现。对于这两类数据库，对方的优势就是自己的弱势，反之亦然。 1.1.4. 总结 关系型数据库与NoSQL数据库并非对立而是互补的关系，即通常情况下使用关系型数据库，在适合使用NoSQL的时候使用NoSQL数据库， 让NoSQL数据库对关系型数据库的不足进行弥补。 一般会将数据存储在关系型数据库中，在nosql数据库中备份存储关系型数据库的数据 1.2.主流的NOSQL产品 • 键值(Key-Value)存储数据库 相关产品： Tokyo Cabinet/Tyrant、Redis、Voldemort、Berkeley DB 典型应用： 内容缓存，主要用于处理大量数据的高访问负载。 数据模型： 一系列键值对 优势： 快速查询 劣势： 存储的数据缺少结构化 • 列存储数据库 相关产品：Cassandra, HBase, Riak 典型应用：分布式的文件系统 数据模型：以列簇式存储，将同一列数据存在一起 优势：查找速度快，可扩展性强，更容易进行分布式扩展 劣势：功能相对局限 • 文档型数据库 相关产品：CouchDB、MongoDB 典型应用：Web应用（与Key-Value类似，Value是结构化的） 数据模型： 一系列键值对 优势：数据结构要求不严格 劣势： 查询性能不高，而且缺乏统一的查询语法 • 图形(Graph)数据库 相关数据库：Neo4J、InfoGrid、Infinite Graph 典型应用：社交网络 数据模型：图结构 优势：利用图结构相关算法。 劣势：需要对整个图做计算才能得出结果，不容易做分布式的集群方案。 1.3 什么是Redis Redis是用C语言开发的一个开源的高性能键值对（key-value）数据库，官方提供测试数据，50个并发执行100000个请求,读的速度是110000次/s,写的速度是81000次/s ，且Redis通过提供多种键值数据类型来适应不同场景下的存储需求，目前为止Redis支持的键值数据类型如下： 1) 字符串类型 string 2) 哈希类型 hash 3) 列表类型 list 4) 集合类型 set 5) 有序集合类型 sortedset 1.3.1 redis的应用场景 • 缓存（数据查询、短连接、新闻内容、商品内容等等） • 聊天室的在线好友列表 • 任务队列。（秒杀、抢购、12306等等） • 应用排行榜 • 网站访问统计 • 数据过期处理（可以精确到毫秒 • 分布式集群架构中的session分离2. 下载安装 1. 官网：https://redis.io 2. 中文网：http://www.redis.net.cn/ 3. 解压直接可以使用： * redis.windows.conf：配置文件 * redis-cli.exe：redis的客户端 * redis-server.exe：redis服务器端 2.命令操作123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107命令操作 1. redis的数据结构： * redis存储的是：key,value格式的数据，其中key都是字符串，value有5种不同的数据结构 * value的数据结构： 1) 字符串类型 string 2) 哈希类型 hash ： map格式 3) 列表类型 list ： linkedlist格式。支持重复元素 4) 集合类型 set ： 不允许重复元素 5) 有序集合类型 sortedset：不允许重复元素，且元素有顺序 2. 字符串类型 string 1. 存储： set key value 127.0.0.1:6379&gt; set username zhangsan OK 2. 获取： get key 127.0.0.1:6379&gt; get username &quot;zhangsan&quot; 3. 删除： del key 127.0.0.1:6379&gt; del age (integer) 1 3. 哈希类型 hash 1. 存储： hset key field value 127.0.0.1:6379&gt; hset myhash username lisi (integer) 1 127.0.0.1:6379&gt; hset myhash password 123 (integer) 1 2. 获取： * hget key field: 获取指定的field对应的值 127.0.0.1:6379&gt; hget myhash username &quot;lisi&quot; * hgetall key：获取所有的field和value 127.0.0.1:6379&gt; hgetall myhash 1) &quot;username&quot; 2) &quot;lisi&quot; 3) &quot;password&quot; 4) &quot;123&quot; 3. 删除： hdel key field 127.0.0.1:6379&gt; hdel myhash username (integer) 1 4. 列表类型 list:可以添加一个元素到列表的头部（左边）或者尾部（右边） 1. 添加： 1. lpush key value: 将元素加入列表左表 2. rpush key value：将元素加入列表右边 127.0.0.1:6379&gt; lpush myList a (integer) 1 127.0.0.1:6379&gt; lpush myList b (integer) 2 127.0.0.1:6379&gt; rpush myList c (integer) 3 2. 获取： * lrange key start end ：范围获取 127.0.0.1:6379&gt; lrange myList 0 -1 1) &quot;b&quot; 2) &quot;a&quot; 3) &quot;c&quot; 3. 删除： * lpop key： 删除列表最左边的元素，并将元素返回 * rpop key： 删除列表最右边的元素，并将元素返回 5. 集合类型 set ： 不允许重复元素 1. 存储：sadd key value 127.0.0.1:6379&gt; sadd myset a (integer) 1 127.0.0.1:6379&gt; sadd myset a (integer) 0 2. 获取：smembers key:获取set集合中所有元素 127.0.0.1:6379&gt; smembers myset 1) &quot;a&quot; 3. 删除：srem key value:删除set集合中的某个元素 127.0.0.1:6379&gt; srem myset a (integer) 1 6. 有序集合类型 sortedset：不允许重复元素，且元素有顺序.每个元素都会关联一个double类型的分数。redis正是通过分数来为集合中的成员进行从小到大的排序。 1. 存储：zadd key score value 127.0.0.1:6379&gt; zadd mysort 60 zhangsan (integer) 1 127.0.0.1:6379&gt; zadd mysort 50 lisi (integer) 1 127.0.0.1:6379&gt; zadd mysort 80 wangwu (integer) 1 2. 获取：zrange key start end [withscores] 127.0.0.1:6379&gt; zrange mysort 0 -1 1) &quot;lisi&quot; 2) &quot;zhangsan&quot; 3) &quot;wangwu&quot; 127.0.0.1:6379&gt; zrange mysort 0 -1 withscores 1) &quot;zhangsan&quot; 2) &quot;60&quot; 3) &quot;wangwu&quot; 4) &quot;80&quot; 5) &quot;lisi&quot; 6) &quot;500&quot; 3. 删除：zrem key value 127.0.0.1:6379&gt; zrem mysort lisi (integer) 1 7. 通用命令 1. keys * : 查询所有的键 2. type key ： 获取键对应的value的类型 3. del key：删除指定的key value 3.持久化12345678910111213141516171819202122234. 持久化1. redis是一个内存数据库，当redis服务器重启，或者电脑重启，数据会丢失，我们可以将redis内存中的数据持久化保存到硬盘的文件中。2. redis持久化机制：1. RDB：默认方式，不需要进行配置，默认就使用这种机制* 在一定的间隔时间中，检测key的变化情况，然后持久化数据1. 编辑redis.windwos.conf文件# after 900 sec (15 min) if at least 1 key changedsave 900 1# after 300 sec (5 min) if at least 10 keys changedsave 300 10# after 60 sec if at least 10000 keys changedsave 60 100002. 重新启动redis服务器，并指定配置文件名称D:\\JavaWeb2018\\day23_redis\\资料\\redis\\windows-64\\redis-2.8.9&gt;redis-server.exe redis.windows.conf2. AOF：日志记录的方式，可以记录每一条命令的操作。可以每一次命令操作后，持久化数据1. 编辑redis.windwos.conf文件appendonly no（关闭aof --&gt; appendonly yes （开启aof# appendfsync always ： 每一次操作都进行持久化appendfsync everysec ： 每隔一秒进行一次持久化# appendfsync no ： 不进行持久化 4.Java客户端-Jedis12345678910* Jedis: 一款java操作redis数据库的工具.* 使用步骤： 1. 下载jedis的jar包 2. 使用 //1. 获取连接 Jedis jedis = new Jedis(&quot;localhost&quot;,6379); //2. 操作 jedis.set(&quot;username&quot;,&quot;zhangsan&quot;); //3. 关闭连接 jedis.close(); 字符串类型 string1234567891011121314151617181) 字符串类型 string set get //1. 获取连接 Jedis jedis = new Jedis();//如果使用空参构造，默认值 &quot;localhost&quot;,6379端口 //2. 操作 //存储 jedis.set(&quot;username&quot;,&quot;zhangsan&quot;); //获取 String username = jedis.get(&quot;username&quot;); System.out.println(username); //可以使用setex()方法存储可以指定过期时间的 key value jedis.setex(&quot;activecode&quot;,20,&quot;hehe&quot;);//将activecode：hehe键值对存入redis，并且20秒后自动删除该键值对 //3. 关闭连接 jedis.close(); 哈希类型hash123456789101112131415161718192021222324252627282) 哈希类型 hash ： map格式 hset hget hgetAll //1. 获取连接 Jedis jedis = new Jedis();//如果使用空参构造，默认值 &quot;localhost&quot;,6379端口 //2. 操作 // 存储hash jedis.hset(&quot;user&quot;,&quot;name&quot;,&quot;lisi&quot;); jedis.hset(&quot;user&quot;,&quot;age&quot;,&quot;23&quot;); jedis.hset(&quot;user&quot;,&quot;gender&quot;,&quot;female&quot;); // 获取hash String name = jedis.hget(&quot;user&quot;, &quot;name&quot;); System.out.println(name); // 获取hash的所map中的数据 Map&lt;String, String&gt; user = jedis.hgetAll(&quot;user&quot;); // keyset Set&lt;String&gt; keySet = user.keySet(); for (String key : keySet) &#123; //获取value String value = user.get(key); System.out.println(key + &quot;:&quot; + value); &#125; //3. 关闭连接 jedis.close(); 列表类型list1234567891011121314151617181920212223242526272829 3) 列表类型 list ： linkedlist格式。支持重复元素lpush / rpushlpop / rpoplrange start end : 范围获取 //1. 获取连接 Jedis jedis = new Jedis();//如果使用空参构造，默认值 &quot;localhost&quot;,6379端口 //2. 操作 // list 存储 jedis.lpush(&quot;mylist&quot;,&quot;a&quot;,&quot;b&quot;,&quot;c&quot;);//从左边存 jedis.rpush(&quot;mylist&quot;,&quot;a&quot;,&quot;b&quot;,&quot;c&quot;);//从右边存 // list 范围获取 List&lt;String&gt; mylist = jedis.lrange(&quot;mylist&quot;, 0, -1); System.out.println(mylist); // list 弹出 String element1 = jedis.lpop(&quot;mylist&quot;);//c System.out.println(element1); String element2 = jedis.rpop(&quot;mylist&quot;);//c System.out.println(element2); // list 范围获取 List&lt;String&gt; mylist2 = jedis.lrange(&quot;mylist&quot;, 0, -1); System.out.println(mylist2); //3. 关闭连接 jedis.close(); 集合类型set123456789101112131415164) 集合类型 set ： 不允许重复元素 sadd smembers:获取所元素 //1. 获取连接 Jedis jedis = new Jedis();//如果使用空参构造，默认值 &quot;localhost&quot;,6379端口 //2. 操作 // set 存储 jedis.sadd(&quot;myset&quot;,&quot;java&quot;,&quot;php&quot;,&quot;c++&quot;); // set 获取 Set&lt;String&gt; myset = jedis.smembers(&quot;myset&quot;); System.out.println(myset); //3. 关闭连接 jedis.close(); 有序集合类型1234567891011121314151617185) 有序集合类型 sortedset：不允许重复元素，且元素顺序 zadd zrange //1. 获取连接 Jedis jedis = new Jedis();//如果使用空参构造，默认值 &quot;localhost&quot;,6379端口 //2. 操作 // sortedset 存储 jedis.zadd(&quot;mysortedset&quot;,3,&quot;亚瑟&quot;); jedis.zadd(&quot;mysortedset&quot;,30,&quot;后裔&quot;); jedis.zadd(&quot;mysortedset&quot;,55,&quot;孙悟空&quot;); // sortedset 获取 Set&lt;String&gt; mysortedset = jedis.zrange(&quot;mysortedset&quot;, 0, -1); System.out.println(mysortedset); //3. 关闭连接 jedis.close(); jedis连接池： JedisPool123456789101112131415161718* jedis连接池： JedisPool * 使用： 1. 创建JedisPool连接池对象 2. 调用方法 getResource()方法获取Jedis连接 //0.创建一个配置对象 JedisPoolConfig config = new JedisPoolConfig(); config.setMaxTotal(50); config.setMaxIdle(10); //1.创建Jedis连接池对象 JedisPool jedisPool = new JedisPool(config,&quot;localhost&quot;,6379); //2.获取连接 Jedis jedis = jedisPool.getResource(); //3. 使用 jedis.set(&quot;hehe&quot;,&quot;heihei&quot;); //4. 关闭 归还到连接池中 jedis.close(); 连接池工具类12345678910111213141516171819202122232425262728293031* 连接池工具类 public class JedisPoolUtils &#123; private static JedisPool jedisPool; static&#123; //读取配置文件 InputStream is = JedisPoolUtils.class.getClassLoader().getResourceAsStream(&quot;jedis.properties&quot;); //创建Properties对象 Properties pro = new Properties(); //关联文件 try &#123; pro.load(is); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; //获取数据，设置到JedisPoolConfig中 JedisPoolConfig config = new JedisPoolConfig(); config.setMaxTotal(Integer.parseInt(pro.getProperty(&quot;maxTotal&quot;))); config.setMaxIdle(Integer.parseInt(pro.getProperty(&quot;maxIdle&quot;))); //初始化JedisPool jedisPool = new JedisPool(config,pro.getProperty(&quot;host&quot;),Integer.parseInt(pro.getProperty(&quot;port&quot;))); &#125; /** * 获取连接方法 */ public static Jedis getJedis()&#123; return jedisPool.getResource(); &#125; &#125; jedis配置文件12345678910111213141516171819202122#最大活动对象数 redis.pool.maxTotal=1000 #最大能够保持idel状态的对象数 redis.pool.maxIdle=100 #最小能够保持idel状态的对象数 redis.pool.minIdle=50 #当池内没有返回对象时，最大等待时间 redis.pool.maxWaitMillis=10000 #当调用borrow Object方法时，是否进行有效性检查 redis.pool.testOnBorrow=true #当调用return Object方法时，是否进行有效性检查 redis.pool.testOnReturn=true #“空闲链接”检测线程，检测的周期，毫秒数。如果为负值，表示不运行“检测线程”。默认为-1. redis.pool.timeBetweenEvictionRunsMillis=30000 #向调用者输出“链接”对象时，是否检测它的空闲超时； redis.pool.testWhileIdle=true # 对于“空闲链接”检测线程而言，每次检测的链接资源的个数。默认为3. redis.pool.numTestsPerEvictionRun=50 #redis服务器的IP redis.ip=xxxxxx #redis服务器的Port redis1.port=6379 十一，Mavenidea下maven下载插件慢的问题12在pom.xml中添加maven 依赖包时，我就发现不管是否用了FQ，下载速度都好慢，就1M的东西能下半天，很是苦恼，于是到网上搜资料，然后让我查到了。说是使用阿里的maven镜像就可以了。我于是亲自试了下，速度快的飞起！！！右键项目中maven选项，然后选择“open settings.xml”或者 “create settings.xml”，然后把如下代码粘贴进去就可以了。重启IDE。 12345678910111213141516171819202122&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;settings xmlns=&quot;http://maven.apache.org/SETTINGS/1.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd&quot;&gt; &lt;localRepository&gt;G:\\maven\\maven_repository&lt;/localRepository&gt; &lt;pluginGroups&gt; &lt;/pluginGroups&gt; &lt;proxies&gt; &lt;/proxies&gt; &lt;servers&gt; &lt;/servers&gt; &lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt; &lt;/mirror&gt; &lt;/mirrors&gt; &lt;profiles&gt; &lt;/profiles&gt;&lt;/settings&gt; maven配置123在maven的running VM option 里面加入这句话，再次创建新的maven工程就可以不用下载插件-DarchetypeCatalog=internal Maven概念1.Maven的作用maven是项目管理工具，用于构建项目，管理项目jar包依赖。 2..Maven的9个核心概念？①POM1项目对象模型。将Java工程的相关信息封装为对象作为便于操作和管理的模型。Maven工程的核心配置 ②约定的目录结构 ③坐标123groupId：公司或组织的域名倒序+当前项目名称artifactId：当前项目的模块名称version：当前模块的版本 ④依赖管理1234567891011121314151617181920212223242526272829303132333435当A jar包需要用到B jar包中的类时，我们就说A对B有依赖。例如：commons-fileupload-1.3.jar依赖于commons-io-2.0.1.jar。①compile[1]main目录下的Java代码可以访问这个范围的依赖[2]test目录下的Java代码可以访问这个范围的依赖[3]部署到Tomcat服务器上运行时要放在WEB-INF的lib目录下例如：对Hello的依赖。主程序、测试程序和服务器运行时都需要用到。②test[1]main目录下的Java代码不能访问这个范围的依赖[2]test目录下的Java代码可以访问这个范围的依赖[3]部署到Tomcat服务器上运行时不会放在WEB-INF的lib目录下例如：对junit的依赖。仅仅是测试程序部分需要。③provided[1]main目录下的Java代码可以访问这个范围的依赖[2]test目录下的Java代码可以访问这个范围的依赖[3]部署到Tomcat服务器上运行时不会放在WEB-INF的lib目录下例如：servlet-api在服务器上运行时，Servlet容器会提供相关API，所以部署的时候不需要。④runtime[了解][1]main目录下的Java代码不能访问这个范围的依赖[2]test目录下的Java代码可以访问这个范围的依赖[3]部署到Tomcat服务器上运行时会放在WEB-INF的lib目录下例如：JDBC驱动。只有在测试运行和在服务器运行的时候才决定使用什么样的数据库连接。⑤其他：import、system等。依赖排除：&lt;dependency&gt; &lt;groupId&gt;com.atguigu.maven&lt;/groupId&gt; &lt;artifactId&gt;B&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;!-- 依赖排除 --&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt; ⑤仓库管理用jar包先去本地仓库找，没有的话，如果有网，默认先去中央仓库找。 ⑥生命周期Maven有三套相互独立的生命周期，分别是： 123①Clean Lifecycle在进行真正的构建之前进行一些清理工作。②Default Lifecycle构建的核心部分，编译，测试，打包，安装，部署等等。③Site Lifecycle生成项目报告，站点，发布站点。 ⑦插件和目标1234●Maven的核心仅仅定义了抽象的生命周期，具体的任务都是交由插件完成的。●每个插件都能实现多个功能，每个功能就是一个插件目标。●Maven的生命周期与插件目标相互绑定，以完成某个具体的构建任务。例如：compile就是插件maven-compiler-plugin的一个功能；pre-clean是插件maven-clean-plugin的一个目标。 ⑧继承创建打包方式为pom的父工程，在子工程pom文件引入父工程，在父工程中管理依赖 ⑨聚合12345&lt;modules&gt; &lt;module&gt;A&lt;/module&gt; &lt;module&gt;B&lt;/module&gt; &lt;module&gt;C&lt;/module&gt;&lt;/modules&gt; 3.如何做到依赖jar包的统一管理？创建打包方式为pom的父工程，在子工程pom文件引入父工程，在父工程中管理依赖 4.和标签的区别？12345dependencyManagement里只是声明依赖，并不实现引入，因此，子项目需要显示声明依赖。如果不在子项目中声明依赖，是不会从父项目中继承下来的；子项目中声明该依赖项，并且没有指定具体版本，才会从父项目中继承该配置项，并且version和scope都读取自父pom；--相对于dependencyManagement，所有声明在dependencies里的依赖都会自动引入，并默认被所有的子项目继承。dependencies即使在子项目中不写该依赖项，那么，子项目仍然会从父项目中继承该依赖项（全部继承） 5.自动化构建的环节有哪些？清理编译测试报告打包安装部署 6.常用的maven命令？clean package install 7.依赖范围有哪些取值，作用是什么？①compile1234[1]main目录下的Java代码可以访问这个范围的依赖[2]test目录下的Java代码可以访问这个范围的依赖[3]部署到Tomcat服务器上运行时要放在WEB-INF的lib目录下例如：对Hello的依赖。主程序、测试程序和服务器运行时都需要用到。 ②test1234[1]main目录下的Java代码不能访问这个范围的依赖[2]test目录下的Java代码可以访问这个范围的依赖[3]部署到Tomcat服务器上运行时不会放在WEB-INF的lib目录下例如：对junit的依赖。仅仅是测试程序部分需要。 ③provided1234[1]main目录下的Java代码可以访问这个范围的依赖[2]test目录下的Java代码可以访问这个范围的依赖[3]部署到Tomcat服务器上运行时不会放在WEB-INF的lib目录下例如：servlet-api在服务器上运行时，Servlet容器会提供相关API，所以部署的时候不需要。 ④runtime[了解]1234[1]main目录下的Java代码不能访问这个范围的依赖[2]test目录下的Java代码可以访问这个范围的依赖[3]部署到Tomcat服务器上运行时会放在WEB-INF的lib目录下例如：JDBC驱动。只有在测试运行和在服务器运行的时候才决定使用什么样的数据库连接。 ⑤其他：import、system等。","categories":[{"name":"JAVA基础","slug":"JAVA基础","permalink":"https://yinhuidong.github.io/categories/JAVA%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"JAVA基础","slug":"JAVA基础","permalink":"https://yinhuidong.github.io/tags/JAVA%E5%9F%BA%E7%A1%80/"}]},{"title":"JAVA-SE核心高级篇","slug":"JAVA基础/JAVA-SE核心高级篇","date":"2022-01-12T00:19:55.243Z","updated":"2022-01-12T00:19:55.244Z","comments":true,"path":"2022/01/12/JAVA基础/JAVA-SE核心高级篇/","link":"","permalink":"https://yinhuidong.github.io/2022/01/12/JAVA%E5%9F%BA%E7%A1%80/JAVA-SE%E6%A0%B8%E5%BF%83%E9%AB%98%E7%BA%A7%E7%AF%87/","excerpt":"","text":"十一，IO流1.File类java.io.File 1）File类的使用123451.file类的一个对象，代表一个文件或一个文件目录（俗称：文件夹）2.File类声明在java.io包下*相对路径：hello.txt 相较于某个路径下，指明的路径。 绝对路径：D://temp.txt 包含盘符在内的文件或文件目录的路径。*路径分隔符：window用\\，url用/。 2）如何创建File类的实例：123456public static void main(String[] args) &#123; File file=new File(&quot;D:\\\\input\\\\input.txt.txt&quot;); System.out.println(file);//输出文件路径 D:\\input\\input.txt.txt File file1 = new File(&quot;jdbc.properties&quot;); System.out.println(file1);jdbc.properties&#125; 3）常用方法：1234567891011121314151617181920212223242526272829303132333435public static void main(String[] args) &#123; File file=new File(&quot;D:\\\\input\\\\input.txt.txt&quot;); File file1 = new File(&quot;jdbc.properties&quot;); String path = file.getAbsolutePath();//获取绝对路径 System.out.println(path);//D:\\input\\input.txt.txt System.out.println(file1.getAbsolutePath());//E:\\IEDADEMO\\jdbc.properties String filePath = file.getPath();//获取路径 System.out.println(filePath);//D:\\input\\input.txt.txt System.out.println(file1.getPath());//jdbc.properties String name = file.getName();//获取名字 System.out.println(name);//input.txt.txt String parent = file.getParent();//获取上一层目录 System.out.println(parent);//D:\\input long length = file.length();//长度,字节 System.out.println(length);//292 long modified = file.lastModified();//最近修改时间 System.out.println(modified);//1572326293899 /** * 仅适用于目录 */ File file2 = new File(&quot;D:\\\\wokespace&quot;); String[] list = file2.list();//列出其下一级文件目录 for (String s:list)&#123; System.out.println(s); &#125; System.out.println(&quot;***********************************************&quot;); File[] files = file2.listFiles();//列出其下一级文件目录的绝对路径 for (File f: files)&#123; System.out.println(f); &#125; System.out.println(&quot;**************************&quot;); boolean b = file1.renameTo(file);//把file1重命名为file的名字 //要想保证成功，file1在硬盘中是真实存在的，file在硬盘中是不存在的 System.out.println(b);&#125; 12345678910public static void main(String[] args) &#123; File file = new File(&quot;jdbc.properties&quot;); //判断功能 System.out.println(file.isDirectory());//判断是不是一个目录 false System.out.println(file.isFile());//判断是不是一个文件true System.out.println(file.exists());//判断是否真实存在true System.out.println(file.canRead());//判断是否可读true System.out.println(file.canWrite());//判断是否可写true System.out.println(file.isHidden());//判读是否隐藏 false&#125; 123456789101112131415161718public static void main(String[] args) &#123; //File的创建功能和删除功能 File file = new File(&quot;jdbc1.properties&quot;); if (!file.exists())&#123; try &#123; file.createNewFile();//创建文件 System.out.println(&quot;文件创建成功！&quot;); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;else&#123; file.delete();//删除文件,不走回收站，直接删除 System.out.println(&quot;文件删除成功！&quot;); &#125; file.mkdir();//创建单层目录 file.mkdirs();//创建多层目录&#125; file.delete()删除目录时，要想删除成功，该目录下不能有文件或目录。 4）面试题：123456789101112131415161718/** * 创建一个与File同目录下的另外一个文件，文件名为：haha.txt * @param args */public static void main(String[] args) &#123; File file = new File(&quot;D:\\\\temp\\\\temp1.txt&quot;); File file1 = new File(file.getParent() , &quot;haha.txt&quot;); System.out.println(file.getParent()); try &#123; boolean newFile = file1.createNewFile(); if (newFile)&#123; System.out.println(&quot;创建成功！&quot;); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;&#125; 1234567891011121314151617181920212223242526/** * 遍历指定目录下所文件名称，包括子文件目录中的文件 * 1.并计算指定目录占用空间的大小 * 2.删除指定文件下的目录及其下的文件夹 */public static void main(String[] args) &#123; File file = new File(&quot;D:\\\\temp&quot;); int sum=print(file); System.out.println(sum);&#125;//遍历指定目录下所文件名称，包括子文件目录中的文件,并计算指定目录占用空间的大小public static int print(File file)&#123; File []files=file.listFiles(); int sum=0; for (File f:files)&#123; if (f.isDirectory())&#123; print(f); f.delete(); &#125;else&#123; System.out.println(f.getAbsolutePath()); sum+=f.length(); f.delete(); &#125; &#125; return sum;&#125; 123456789101112131415161718public static void main(String[] args) &#123; /** * 判断指定目录下是否存在后缀名为.jpg的文件，如果就输出该文件名； */ File file = new File(&quot;D:\\\\QQ&quot;); Find(file);&#125;public static void Find(File file)&#123; File []files=file.listFiles(); for (File f: files)&#123; if (f.isDirectory())&#123; Find(f); &#125;else&#123; if (f.getName().endsWith(&quot;.jpg&quot;))&#123; System.out.println(f.getAbsolutePath()); &#125; &#125; &#125; 2.IO流1）概述12345678910111213141516IO流原理及流的分类input/output的缩写，处理设备之间的数据传输。input：读取外部数据output：将程序中的数据输出按照操作数据单位的不同，分为字节流（8bit）和字符流（6bit）适合于文本。 输入流：inputStream，reader 输出流：outputStream，Writer按照流的角色不同：作用在文件上的：节点流，作用在已经有的流上的：处理流。按照数据流向：输入流，输出流。IO流的体系结构抽象基类 节点流 缓冲流InputStream FileInputStream BufferedInputStreamOutputStream FileOutputStream BufferedOutputStreamReader FileReader BufferedReaderWriter FileWriter BufferedWriter 2）节点流-FileReader和FileWriterFileReader读入数据的基本操作说明点： ①read()地理解：返回读入的一个字符，如果达到文件末尾，返回-1 ②异常的处理：为了保证流资源一定可以执行关闭操作，需要使用try-catch-finally处理。 ③读入的文件一定要存在，否则就会报FileNotFoundException 1234567891011121314151617181920212223242526272829303132333435/** * FileReader读取硬盘文件 * @param args */public static void main(String[] args)&#123; FileReader reader=null; BufferedReader reader1=null; try &#123; reader = new FileReader(new File(&quot;jdbc.properties&quot;));//相较于当前工程 reader1 = new BufferedReader(reader); int data ;//返回读入的一个字符，如果达到文件末尾，返回-1. while ((data=reader1.read())!=-1)&#123;//循环读取数据 System.out.print((char)data); &#125; &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123;//关闭流 if (reader!=null)&#123; try &#123; reader.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; if(reader1!=null)&#123; try &#123; reader1.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637public static void main(String[] args) &#123; //对read（的升级，使用read的重载方法 FileReader reader=null; BufferedReader br=null; try &#123; reader = new FileReader(new File(&quot;jdbc.properties&quot;)); br=new BufferedReader(reader); char []buf=new char[1024]; int len=0; while ((len=br.read(buf))!=-1)&#123;//返回每次读入buf[]数组中的字符个数，如果达到文件末尾，返回-1// for(int i=0;i&lt;len;i++)&#123;// System.out.print(buf[i]);// &#125; String str=new String(buf,0,len); System.out.print(str); &#125; &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally&#123; if (reader!=null)&#123; try &#123; reader.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; if (br!=null)&#123; try &#123; br.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; FileWriter写出数据的操作说明： ①输出操作：对应的File可以不存在，如果不存在，在输出过程中，会自动创建此文件。 ②如果存在：如果流使用的构造器是FileWriter（file,false）/FileWriter（file）:对原有文件覆盖。 如果流使用FileWriter（file,true）：不会对原有文件覆盖。 12345678910111213141516171819202122/** * 从内存写出数据到硬盘文件 * @param args */public static void main(String[] args) &#123; FileWriter writer = null; try &#123; writer= new FileWriter(new File(&quot;hello.txt&quot;),true); //true时表示对原文件的追加，false或不写表示对原文件的覆盖 writer.write(&quot;I have a dream !&quot;);//写入文件 &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;finally&#123; if (writer!=null)&#123; try &#123; writer.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 使用FileReader和FileWriter实现文本文件的复制字符流不能处理图片文件 123456789101112131415161718192021222324252627282930313233343536373839/** * 将一个文件写入另一个文件 * @param args */public static void main(String[] args) &#123; FileReader fr=null; FileWriter fw=null; try &#123; fr=new FileReader(new File(&quot;E:\\\\IEDADEMO\\\\day07\\\\src\\\\com\\\\atguigu\\\\IO\\\\IO.txt&quot;)); fw=new FileWriter(new File(&quot;hello.txt&quot;)); //数据的读入和写出操作 char []buf=new char[1024]; int len=0;//记录每次读入到数组的数据多少个 while ((len=fr.read(buf))!=-1)&#123; fw.write(buf,0,len); &#125; &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;finally&#123; if (fw!=null)&#123; try &#123; fw.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; if (fr!=null)&#123; try &#123; fr.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 3）节点流-FileInputStream和FileOutputStream使用FileInputStream不能读取文本文件的测试: 结论：对于文本文件，使用字符流处理 对于非文本文件，使用字节流处理 123456789101112131415161718192021222324252627282930313233/** * 使用FileInputStream不能读取文本文件的测试 */public static void main(String[] args) &#123; FileInputStream fis=null; FileOutputStream fos=null; try &#123; fis=new FileInputStream(new File(&quot;E:\\\\IEDADEMO\\\\day07\\\\src\\\\com\\\\atguigu\\\\IO\\\\IO.txt&quot;)); fos=new FileOutputStream(new File(&quot;hello.txt&quot;)); byte [] bytes=new byte[1024]; int len=0; while ((len=fis.read(bytes))!=-1)&#123; fos.write(bytes,0,len); &#125; &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;finally&#123; try &#123; if (fos!=null)&#123; fos.close(); &#125; if (fis!=null)&#123; fis.close(); &#125; &#125;catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; FileInputStream和FileOutputStream读写非文本文件123456789101112131415161718192021222324252627282930/** * FileInputStream和FileOutputStream读写非文本文件 * @param args */public static void main(String[] args) &#123; FileInputStream fis=null; FileOutputStream fos=null; try &#123; fis=new FileInputStream(new File(&quot;E:\\\\IEDADEMO\\\\day07\\\\src\\\\下载.png&quot;)); fos=new FileOutputStream(new File(&quot;D:\\\\test.png&quot;)); byte[] bytes=new byte[1024*1024]; int len=0; while ((len=fis.read(bytes))!=-1) &#123; fos.write(bytes,0,len); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if (fos!=null)&#123; fos.close(); &#125; if (fis!=null)&#123; fis.close(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 写一个复制文件的方法，可以用来直接调用，1234567891011121314151617181920212223242526272829303132public static void main(String[] args) &#123; File file = new File(&quot;hello.txt&quot;); File file1 = new File(&quot;D://hello.txt&quot;); Copy(file,file1);&#125;public static void Copy(File file,File file1)&#123; FileInputStream fis=null; FileOutputStream fos=null; try &#123; fis=new FileInputStream(file); fos=new FileOutputStream(file1); byte [] bytes=new byte [1024]; int len; while((len=fis.read(bytes))!=-1)&#123; fos.write(bytes,0,len); &#125; System.out.println(&quot;复制完成！&quot;); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if (fos!=null)&#123; fos.close(); &#125; if (fis!=null)&#123; fis.close(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 4）处理流之一：缓冲流缓冲流作用：提高流的读取写入速度 提高读写速度的原因：内部提供了一个缓冲区：1024*8=8192 缓冲流（字节型）实现非文本文件的复制 12345678910111213141516171819202122232425262728293031323334353637public static void main(String[] args) &#123; FileInputStream fis=null; FileOutputStream fos=null; BufferedInputStream bis=null; BufferedOutputStream bos=null; try &#123; fis=new FileInputStream(new File(&quot;hello.txt&quot;)); fos=new FileOutputStream(new File(&quot;D://hello.txt&quot;)); bis=new BufferedInputStream(fis); bos=new BufferedOutputStream(fos); byte [] bytes=new byte[1024]; int len; while ((len=bis.read(bytes))!=-1)&#123; bos.write(bytes,0,len); &#125; System.out.println(&quot;复制完成！&quot;); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if (bos!=null)&#123; bos.close(); &#125; if (bis!=null)&#123; bis.close(); &#125; if (fos!=null)&#123; fos.close(); &#125; if (fis!=null)&#123; fis.close(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 缓冲流（字符型）实现文本文件的复制 处理流：就是套接在已有流的基础上 1234567891011121314151617181920212223242526272829303132333435363738394041424344public static void main(String[] args) &#123; FileReader fr=null; FileWriter fw=null; BufferedReader br=null; BufferedWriter bw=null; try &#123; fr=new FileReader(new File(&quot;hello.txt&quot;)); fw=new FileWriter(new File(&quot;D://hello.txt&quot;)); br=new BufferedReader(fr); bw=new BufferedWriter(fw); //方式一：// char [] chars=new char[1024];// int len=0;// while ((len=br.read(chars))!=-1)&#123;// bw.write(chars,0,len);// &#125; //方式二： String data; while ((data=br.readLine())!=null)&#123; bw.write(data);//一次读取一行 bw.newLine();//换行 &#125; System.out.println(&quot;文本复制完成！&quot;); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if (bw!=null)&#123; bw.close(); &#125; if (br!=null)&#123; br.close(); &#125; if (fw!=null)&#123; fw.close(); &#125; if (fr!=null)&#123; fr.close(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; 5）面试题1.图片加密和图片解密12345678910111213141516171819202122232425262728293031323334/*** * 图片加密 * @param args */public static void main(String[] args) &#123; BufferedInputStream bis=null; BufferedOutputStream bos=null; try &#123; bis=new BufferedInputStream(new FileInputStream(new File(&quot;D:\\\\尹会东.jpg&quot;))); bos=new BufferedOutputStream(new FileOutputStream(new File(&quot;D:\\\\copy.jpg&quot;))); int len=0; byte [] bytes=new byte[1024]; while ((len=bis.read(bytes))!=-1)&#123; for (int i = 0; i &lt;len ; i++) &#123; bytes[i]= (byte) (bytes[i]^5);//加密操作 &#125; bos.write(bytes,0,len); &#125; System.out.println(&quot;加密完成！&quot;); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if (bis!=null)&#123; bis.close(); &#125; if (bos!=null)&#123; bos.close(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 123456789101112131415161718192021222324252627282930313233/** * 图片解密 */public static void main(String[] args) &#123; BufferedInputStream bis=null; BufferedOutputStream bos=null; try &#123; bis=new BufferedInputStream(new FileInputStream(new File(&quot;D:\\\\copy.jpg&quot;))); bos=new BufferedOutputStream(new FileOutputStream(new File(&quot;D:\\\\copy1.jpg&quot;))); int len=0; byte [] bytes=new byte[1024]; while ((len=bis.read(bytes))!=-1)&#123; for (int i = 0; i &lt;len ; i++) &#123; bytes[i]= (byte) (bytes[i]^5);//解密操作 &#125; bos.write(bytes,0,len); &#125; System.out.println(&quot;解密完成！&quot;); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if (bis!=null)&#123; bis.close(); &#125; if (bos!=null)&#123; bos.close(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 2.获取文本上每个字符出现的次数12345678910111213141516171819202122232425262728293031323334353637383940/** * 获取文本上每个字符出现的次数 */public static void main(String[] args) &#123; HashMap&lt;Character, Integer&gt; map = new HashMap&lt;&gt;(); BufferedReader br=null; BufferedWriter bw=null; try &#123; br=new BufferedReader(new FileReader(new File(&quot;hello.txt&quot;))); bw=new BufferedWriter(new FileWriter(new File(&quot;D:\\\\WorldCount.txt&quot;))); int c=0; while ((c=br.read())!=-1)&#123; char ch= (char) c; if (map.get(ch)==null)&#123; map.put(ch,1); &#125;else&#123; map.put(ch,map.get(ch)+1); &#125; &#125; Set&lt;Map.Entry&lt;Character, Integer&gt;&gt; set = map.entrySet(); Iterator&lt;Map.Entry&lt;Character, Integer&gt;&gt; iterator = set.iterator(); while (iterator.hasNext())&#123; bw.write(iterator.next().getKey()+&quot;--------------&quot;+iterator.next().getValue()); bw.newLine(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if (bw!=null)&#123; bw.close(); &#125; if (br!=null)&#123; br.close(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 3.将一个文件写入另一个文件123456789101112131415161718192021222324252627282930313233343536373839/** * 将一个文件写入另一个文件 * @param args */public static void main(String[] args) &#123; FileReader fr=null; FileWriter fw=null; try &#123; fr=new FileReader(new File(&quot;E:\\\\IEDADEMO\\\\day07\\\\src\\\\com\\\\atguigu\\\\IO\\\\IO.txt&quot;)); fw=new FileWriter(new File(&quot;hello.txt&quot;)); //数据的读入和写出操作 char []buf=new char[1024]; int len=0;//记录每次读入到数组的数据多少个 while ((len=fr.read(buf))!=-1)&#123; fw.write(buf,0,len); &#125; &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;finally&#123; if (fw!=null)&#123; try &#123; fw.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; if (fr!=null)&#123; try &#123; fr.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 6）处理流之二：转换流1.作用：提供了字节流和字符流的转换 2.转换流属于字符流 InputStreamReader：将一个字节的输入流转换为字符的输入流 OutputStreamWriter：将一个字符的输出流转换为字节的输出流 3.解码：字节，字节数组—–&gt;字符数组，字符串 编码：字符数组，字符串—–&gt;字节，字节数组 4.字符集 12345678910111213141516171819202122232425262728293031323334353637383940414243public static void main(String[] args) &#123; FileInputStream fis=null; InputStreamReader isr=null; FileOutputStream fos=null; OutputStreamWriter osw=null; try &#123; fis = new FileInputStream(new File(&quot;hello.txt&quot;));// InputStreamReader isr = new InputStreamReader(fis);//使用系统默认字符集 isr = new InputStreamReader(fis, StandardCharsets.UTF_8);//指定字符集 fos=new FileOutputStream(new File(&quot;D:\\\\Temps.txt&quot;)); osw=new OutputStreamWriter(fos,&quot;utf-8&quot;); char [] chars=new char[1024]; int len; while ((len=isr.read(chars))!=-1)&#123; osw.write(chars,0,len); &#125;// while ((len=isr.read(chars))!=-1)&#123;// String str=new String(chars,0,len);// System.out.println(str);// &#125; &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if (isr!=null)&#123; isr.close(); &#125; if (fis!=null)&#123; fis.close(); &#125; if (osw!=null)&#123; osw.close(); &#125; if (fos!=null)&#123; fos.close(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; 7）其他流（了解）其他流（了解） 1.标准的输入输出流 1.1 System.in标准的输入流 System.out标准的输出流 1.2我们可以通过System类的setIn(InputStream)/setOut(OutputStream)方式重新指定输入和输出的流。 1.3练习：IO5 12345678910111213141516171819202122232425262728293031323334 public static void main(String[] args) &#123;/** * 键盘输入字符，如果是e或exit程序结束，否则转化为大写输出 */ InputStreamReader isr=new InputStreamReader(System.in); BufferedReader br=new BufferedReader(isr); while (true)&#123; String data= null; try &#123; data = br.readLine(); if (data.equalsIgnoreCase(&quot;e&quot;)||data.equalsIgnoreCase(&quot;exit&quot;))&#123; System.out.println(&quot;程序结束！&quot;); break; &#125;else&#123; System.out.println(data.toUpperCase()); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;finally &#123; try &#123; if (isr!=null)&#123; isr.close(); &#125; if (br!=null)&#123; br.close(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; 2.打印流 PrintStream PrintWriter 3.数据流 DataInputStream DataOutputStream 4.随机存取文件流 RandomAccessFile 既可以读，也可以写。 实现了DataInput和DataOutput接口 java.io下，但是直接继承Object类 如果RandomAccessFile作为输出流时，写出到的文件不存在，会自动创建， 如果写出到的文件存在，会对原有文件进行覆盖，（默认情况下，从头覆盖） 1234567891011121314151617181920212223242526public static void main(String[] args) &#123; RandomAccessFile file = null; RandomAccessFile file1 = null; try &#123; file=new RandomAccessFile(new File(&quot;hello.txt&quot;),&quot;r&quot;); file1=new RandomAccessFile(new File(&quot;D:\\\\object.txt&quot;),&quot;rw&quot;); byte []bytes=new byte[1024]; int len=0; while ((len=file.read(bytes))!=-1)&#123; file1.write(bytes,0,len); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if (file!=null)&#123; file.close(); &#125; if (file1!=null)&#123; file1.close(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; RandomAccessFile类实现数据插入 1234567891011121314151617181920212223public static void main(String[] args) &#123; RandomAccessFile raf=null; try &#123; raf=new RandomAccessFile(&quot;hello.txt&quot;,&quot;rw&quot;); //raf.seek(3);//指定开始覆盖的位置 raf.seek(new File(&quot;hello.txt&quot;).length());//最后面插入 /** * 如果想要在中间插入，可以先将后面的内容读取到内存的某个变量， * 然后写入要插入的数据，再将变量中的内容插入末尾。 */ raf.write(&quot;xyz&quot;.getBytes()); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if (raf!=null)&#123; raf.close(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 8）对象流123ObjectInputStream用于存储和读取基本数据类型或对象的流处理。序列化：用ObjectOutputStream将数据从内存写入硬盘反序列化：ObjectInputStream将数据从硬盘读入内存。 要求对象所属的类是可序列化的—&gt;实现了Seralizable接口 面试题：如何理解对象序列化机制？123把内存中的Java对象转换成平台无关的二进制流，从而允许把这种二进制流持久的保存在硬盘上，或者通过网络，将这种二进制流传输到另一个网络节点，当其他程序获取了这种二进制流，就可以恢复成原来的Java对象。序列化的好处在于将任何实现了Seralizable接口的对象转化为字节数据，使其在保存和传输中可被还原。 序列化123456789101112131415161718192021public static void main(String[] args) &#123; //序列化过程： ObjectOutputStream oos=null; try &#123; oos=new ObjectOutputStream(new FileOutputStream(new File(&quot;D:\\\\object.txt&quot;))); oos.writeObject(new String(&quot;我爱北京天安门&quot;)); oos.flush(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; if(oos!=null)&#123; try &#123; oos.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 反序列化1234567891011121314151617181920212223//反序列化public static void main(String[] args) &#123; ObjectInputStream ois =null; try &#123; ois=new ObjectInputStream(new FileInputStream(new File(&quot;D://object.txt&quot;))); Object object = ois.readObject(); String str= (String) object; System.out.println(str); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; finally &#123; if (ois!=null)&#123; try &#123; ois.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 自定义类实现序列化与反序列化操作123451.需要实现java.io.Serializable接口2. public static final long serialVersionUID= 6080347956336285349L;3.除了当前Person类需要实现Serializable接口之外，还必须保证其内部所有属性也必须是可序列化的。（默认情况下基本数据类型是可序列化的）4.不能序列化static和transient的 12345678910111213141516171819202122232425262728293031323334353637public class Person implements java.io.Serializable&#123; public static final long serialVersionUID= 6080347956336285349L; private String name; private int age; public Person() &#123; &#125; public Person(String name, int age) &#123; this.name = name; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; @Override public String toString() &#123; return &quot;Person&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, age=&quot; + age + &#x27;&#125;&#x27;; &#125;&#125; 123456789101112131415161718192021222324252627282930public static void main(String[] args) &#123; //序列化 ObjectOutputStream oos =null; ObjectInputStream ois=null; try &#123; oos=new ObjectOutputStream(new FileOutputStream(new File(&quot;D://object.txt&quot;))); Person person = new Person(&quot;张贝贝&quot;, 25); oos.writeObject(person); //反序列化 ois=new ObjectInputStream(new FileInputStream(new File(&quot;D:\\\\object.txt&quot;))); Object obj=ois.readObject(); Person p1= (Person) obj; System.out.println(p1.toString()); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if (ois!=null)&#123; ois.close(); &#125; if (oos!=null)&#123; oos.close(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; serialVersionUID的理解如果不手动写，Java会自动生成一个。 如果序列化以后修改了类，serialVersionUID也会被Java修改， 反序列化会失败！ 十二，网络编程网络编程中有两个主要的问题：121.如何准确的定位到网络上一台或者多台主机：定位主机上的特定应用2.找到主机后如何可靠高效的进行数据传输 网络编程中的两个要素121.对应的问题：IP和端口号2.对应问题二：提供网络通信协议：TCP/IP参考模型 通信要素一：IP和端口号12345671.IP：唯一标识Internet上的计算机（通信实体）2.在Java中使用InetAddress类代表IP3.IP分类：IPV4和IPV6；万维网和局域网的区别4.域名：www.baidu.com5.域名--》NDS--》网络服务器6.本地回路地址：127.0.0.1 对应着localhost7.如何实例化InetAddress：两个方法;getByName(String host),getLocalHost(); 端口号：表示计算机上运行的程序1234不同进程有不同的端口号Tomcat：8080，mysql：3306，oracle：1521端口号与IP地址组合得出一个网络套接字，Socket范围：0-65535 1234567891011public static void main(String[] args) &#123; try &#123; InetAddress name = InetAddress.getByName(&quot;127.0.0.1&quot;); System.out.println(name); InetAddress name1 = InetAddress.getByName(&quot;www.baidu.com&quot;); System.out.println(name1); System.out.println(name.getHostName()); &#125; catch (UnknownHostException e) &#123; e.printStackTrace(); &#125;&#125; TCP：3次握手，UDP：封装数据包Tcp网络编程12345678910111213141516171819202122232425262728293031323334353637/** * @author yinhuidong * @createTime 2020-04-10-21:24 * TCP网络编程1：客户端向服务端发送消息,服务端将消息显示在控制台上 */public class Test1 &#123; //客户端 @Test public void client()throws Exception&#123; InetAddress address = InetAddress.getByName(&quot;127.0.0.1&quot;); Socket socket = new Socket(address, 3307); OutputStream os = socket.getOutputStream(); os.write(&quot;你好，我是客户端&quot;.getBytes()); os.close(); socket.close(); &#125; //服务端 @Test public void server()throws Exception&#123; ServerSocket serverSocket = new ServerSocket(3307); Socket socket = serverSocket.accept(); InputStream is = socket.getInputStream(); ByteArrayOutputStream baos = new ByteArrayOutputStream(); int len; byte[] buffer = new byte[10]; while ((len=is.read(buffer))!=-1)&#123; baos.write(buffer,0,len); &#125; System.out.println(baos.toString()); baos.close(); is.close(); socket.close(); serverSocket.close(); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768/** * @author yinhuidong * @createTime 2020-04-10-21:40 * 客户端给服务端发送文件，服务端保存文件并告诉客户端自己接收成功 * //如果不关闭资源，会抛出：java.net.SocketException: Connection reset异常 */public class Test2 &#123; //客户端 @Test public void test1() throws Exception &#123; //获取IP地址 InetAddress address = InetAddress.getByName(&quot;127.0.0.1&quot;); //获取socket对象 Socket socket = new Socket(address, 3308); //写入本地图片 FileInputStream is = new FileInputStream(new File(&quot;E:\\\\9.jpg&quot;)); byte[] buffer = new byte[1024 * 8]; int len; //将本地图片写出到服务端 OutputStream os = socket.getOutputStream(); while ((len = is.read(buffer)) != -1) &#123; os.write(buffer, 0, len); &#125; socket.shutdownOutput(); //获取输入流准备接受服务端的消息 InputStream stream = socket.getInputStream(); byte[] buffer2 = new byte[10]; int len2; ByteArrayOutputStream baos = new ByteArrayOutputStream(); while ((len2 = stream.read(buffer2)) != -1) &#123; baos.write(buffer2, 0, len2); &#125; //输出消息到控制台 System.out.println(baos.toString()); baos.close(); stream.close(); os.close(); is.close(); socket.close(); &#125; @Test public void test2() throws Exception &#123; //创建一个服务端ServerSocket ServerSocket serverSocket = new ServerSocket(3308); //获取socket Socket socket = serverSocket.accept(); //获取输入流 InputStream is = socket.getInputStream(); //指定输出流输出文件位置 FileOutputStream fos = new FileOutputStream(new File(&quot;E:\\\\10.jpg&quot;)); int len; byte[] buffer = new byte[1024 * 8]; while ((len = is.read(buffer)) != -1) &#123; fos.write(buffer, 0, len); &#125; //获取输出流 OutputStream stream = socket.getOutputStream(); //输出内容 stream.write(&quot;接收文件成功&quot;.getBytes()); stream.close(); fos.close(); is.close(); socket.close(); serverSocket.close(); &#125;&#125; UDP网络编程123456789101112131415161718192021222324252627/** * @author yinhuidong * @createTime 2020-04-10-23:03 * UDP网络编程 */public class Test4 &#123; //发送端 @Test public void send()throws Exception&#123; DatagramSocket socket = new DatagramSocket(); byte []data=&quot;我是UDP方式的发送端&quot;.getBytes(); InetAddress inet = InetAddress.getLocalHost(); DatagramPacket packet = new DatagramPacket(data, 0, data.length,inet, 8081); socket.send(packet); &#125; //接收端 @Test public void receiver()throws Exception&#123; DatagramSocket socket = new DatagramSocket(8081); byte[] buffer = new byte[100]; DatagramPacket packet = new DatagramPacket(buffer, 0, buffer.length); socket.receive(packet); System.out.println(new String(packet.getData(), 0, buffer.length)); &#125;&#125; URL类的理解与实例化URL:统一资源定位符，他表示internet上某一资源的地址。 格式：协议，主机名，端口号，片段名，参数列表 123456public static void main(String[] args) throws IOException &#123; URL url = new URL(&quot;www.baidu.com&quot;); HttpURLConnection connection= (HttpURLConnection) url.openConnection();//获取连接 connection.connect();//连接 connection.getInputStream();//获取流&#125; 十三，反射1.概述反射1234反射机制允许程序在执行时借助于反射API取得任何类的内部信息，并能直接操作任意对象的内部属性及方法。反射相关的主要APIjava.lang.Classjava.lang.reflect. 关于java.lang.Class类的理解：*12341.类的加载过程：程序在经过javac.exe命令后，会生成一个或多个字节码文件（.class结尾），接着我们使用java.exe命令对某个字节码文件进行解释运行。相当于将某个字节码文件加载到内存中。此过程称为类的加载。加载到内存中的类，我们就称为运行时类，此运行的类，就作为Class的一个实例。2.换句话说：Class的实例就对应着一个运行时类。 类的加载过程（了解）123①类的加载：将类的class文件读入内存，并为之创建一个java.lang.Class对象，此过程由类的加载器完成。②类的链接：将类的二进制数据合并到JRE中，赋默认值。③类的初始化：JVM负责对类进行初始化。将静态代码块和类中赋值操作的语句读取，给变量/常量赋值。 类的加载器ClassLoader的理解：（了解）1作用：把类加载进内存。 2.获取Class的实例的方式（重点）123加载到内存中的运行时类，会缓存一定的时间，在此时间内，我们可以通过不同的方式来获取此运行时的类。Class实例对应的结构说明：①claass，②interface，③数组④枚举类⑤注解⑥基本数据类型⑦void 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263/** * @author yinhuidong * @createTime 2020-04-11-9:57 * 获取Class实例的四种方式 */public class Test1 &#123; //通过对象.getClass()的方式 @Test public void test1() &#123; Class&lt;? extends Person&gt; clazz = new Person().getClass(); &#125; //通过类.class @Test public void test2() &#123; Class&lt;Person&gt; clazz = Person.class; &#125; //通过Class的静态方法 @Test public void test3() throws Exception &#123; Class&lt;?&gt; clazz = Class.forName(&quot;com.atguigui.java1.Person&quot;); &#125; //通过类加载器 @Test public void test4() throws Exception&#123; ClassLoader classLoader = Person.class.getClassLoader(); classLoader.loadClass(&quot;com.atguigui.java1.Person&quot;); &#125;&#125;class Person &#123; private String name; private Integer age; public Person() &#123; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public Integer getAge() &#123; return age; &#125; public void setAge(Integer age) &#123; this.age = age; &#125; @Override public String toString() &#123; return &quot;Person&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, age=&quot; + age + &#x27;&#125;&#x27;; &#125;&#125; 3.使用ClassLoader加载配置文件12345678910111213141516@Test public void test4() throws Exception &#123; Properties prop=new Properties(); //读取配置文件的方式一： //此时配置文件默认在当前module下// FileInputStream fis = new FileInputStream(&quot;jdbc.properties&quot;);// prop.load(fis); //读取配置文件的方式二：使用ClassLoader //配置文件默认识别为：当前module的src下 ClassLoader classLoader=Test1.class.getClassLoader(); InputStream is = classLoader.getResourceAsStream(&quot;driud.properties&quot;); prop.load(is); String user=prop.getProperty(&quot;user&quot;); String password=prop.getProperty(&quot;password&quot;); System.out.println(user+&quot; &quot;+password); &#125; 4.创建运行时类对象与反射的动态性通过反射，创建运行时类对象1234567newInstance()调用此方法，创建对应的运行时类的对象，内部调用了运行时类的空参构造器。要想此方法正常运行：①运行时类必须提供空参的构造器②空参构造器访问权限得够。在JavaBean中要求提供一个空参构造器的原因：①便于通过反射，创建运行时类的对象。②便于子类继承此运行类时，默认调用super方法时，保证父类有此构造器。 123456@Testpublic void test5() throws IllegalAccessException, InstantiationException &#123; Class&lt;Person&gt;clazz=Person.class; Person obj=clazz.newInstance(); System.out.println(obj);&#125; 反射的动态性举例12345678910@Testpublic void test6() throws Exception &#123; Object instance = getInstance(&quot;java.util.Date&quot;); System.out.println(instance);&#125;//创建一个指定类的对象。classPath：指定类的全类名。public Object getInstance(String classPath)throws Exception&#123; Class clazz=Class.forName(classPath); return clazz.newInstance();&#125; 5.调用运行时类的指定结构属性，1234567891011121314151617181920212223@Testpublic void test() throws Exception &#123; Class clazz=Person.class; //创建运行时类的对象 Object o = clazz.newInstance(); //获取指定的属性:public Field name = clazz.getField(&quot;name&quot;); name.set(o,&quot;yinhuidong&quot;); String o1 = (String) name.get(o); System.out.println(o1);&#125;@Test//掌握public void test2() throws Exception &#123; Class clazz=Person.class; //创建运行时类的对象 Object o = clazz.newInstance(); //获取指定的属性:无关权限修饰符，不包含父类 Field age = clazz.getDeclaredField(&quot;age&quot;); age.setAccessible(true);//给与修改权限 age.set(o,20); Object obj = age.get(o); System.out.println(obj);&#125; 方法，123456789101112131415161718192021@Test//掌握public void test3()throws Exception&#123; //如何操作运行时类中的指定非静态方法 Class&lt;Person&gt; clazz = Person.class; Person person = clazz.newInstance(); Method show = clazz.getDeclaredMethod(&quot;show&quot;); show.setAccessible(true); Object obj = show.invoke(person); Method add = clazz.getDeclaredMethod(&quot;add&quot;, int.class, int.class); add.setAccessible(true); Integer invoke = (Integer) add.invoke(person, 1, 1); System.out.println(invoke);&#125;@Testpublic void test4() throws Exception&#123; //如何操作运行时类中的指定静态方法 Class clazz=Person.class; Method eat = clazz.getDeclaredMethod(&quot;eat&quot;); eat.setAccessible(true); eat.invoke(Person.class);&#125; 构造器123456789@Test//不是很常用，了解public void test5() throws Exception&#123; //调用运行时类中的指定构造器 Class clazz=Person.class; Constructor constructor = clazz.getDeclaredConstructor(String.class, Integer.class); constructor.setAccessible(true); Object obj = constructor.newInstance(&quot;Tom&quot;,22); System.out.println(obj);&#125; 6.代理代理设计模式的原理：使用一个代理将对象包装起来，然后用该代理对象取代原始对象。 任何对原始对象的调用都要通过代理。代理对象决定是否以及何时将方法调用转到原始对象上。 静态代理举例特点：编译期间，代理类和被代理类就被确定下来了。 1234567891011121314151617181920212223242526interface ClothFactory&#123; void produceCloth();&#125;//代理类class ProxyClothFactory implements ClothFactory&#123; private ClothFactory factory;//就拿被代理对象进行实例化 public ProxyClothFactory(ClothFactory factory) &#123; this.factory = factory; &#125; @Override public void produceCloth() &#123; System.out.println(&quot;代理工厂进行准备工作&quot;); factory.produceCloth(); System.out.println(&quot;代理工厂做后续工作！&quot;); &#125;&#125;//被代理类class Nike implements ClothFactory&#123; @Override public void produceCloth() &#123; System.out.println(&quot;耐克工厂生产一批运动服！&quot;); &#125;&#125; 123456@Testpublic void test()&#123; Nike nike=new Nike(); ProxyClothFactory proxyClothFactory = new ProxyClothFactory(nike); proxyClothFactory.produceCloth();&#125; 动态代理示例动态代理：可以通过一个代理类完成全部的代理功能。 要求：理解过程。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162/** * @author yinhuidong * @createTime 2020-04-11-11:17 * 基于接口的动态代理 */public class Test1 &#123; private PersonDao dao=new PersonDaoImpl(); @Test public void test()&#123; //动态代理 PersonDao personDao = (PersonDao) Proxy.newProxyInstance( //被代理类的类加载器 dao.getClass().getClassLoader(), //被代理类实现的接口 dao.getClass().getInterfaces(), //InvocationHandler接口的实现类 new InvocationHandler() &#123; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; Object o=null; if (!method.getName().equals(&quot;show&quot;))&#123; Integer a= (Integer) args[0]; Integer b= (Integer) args[1]; //对方法进行增强 o=method.invoke(dao,a*2,b*2); return o; &#125; o=method.invoke(dao); return o; &#125; &#125; ); System.out.println(personDao.add(1, 1)); personDao.del(2,1); personDao.show(); &#125;&#125;//被代理类实现的接口interface PersonDao&#123; int add(int a,int b); void del(int a,int b); void show();&#125;//被代理类class PersonDaoImpl implements PersonDao&#123; @Override public int add(int a, int b) &#123; return a+b; &#125; @Override public void del(int a, int b) &#123; System.out.println(a-b); &#125; @Override public void show() &#123; System.out.println(&quot;show()....&quot;); &#125;&#125; 7.总结123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157import java.util.Properties;/** * @author yinhuidong * @createTime 2020-02-25-23:36 */public class Test3 &#123; /** * 第二轮复习 * 获取运行时类的Class实例 */ @Test public void test1() throws ClassNotFoundException &#123; //方式一： Class&lt;Person&gt; clazz = Person.class; System.out.println(clazz); //方式二： Person person = new Person(); Class&lt;? extends Person&gt; clazz1 = person.getClass(); System.out.println(clazz1); //方式： Class&lt;?&gt; clazz2 = Class.forName(&quot;com.atguigu.java3.Person&quot;); System.out.println(clazz2); //方式四： ClassLoader loader = Test3.class.getClassLoader(); Class&lt;?&gt; clazz3 = loader.loadClass(&quot;com.atguigu.java3.Person&quot;); System.out.println(clazz3); &#125; /** * 使用ClassLoader加载配置文件 */ @Test public void test2() throws IOException &#123; InputStream is = Test3.class.getClassLoader().getResourceAsStream(&quot;xxx.properties&quot;); Properties prop = new Properties(); prop.load(is); &#125; /** * 通过反射，创建运行时类的对象 */ @Test public void test3() throws IllegalAccessException, InstantiationException &#123; Class&lt;Person&gt; clazz = Person.class; Person person = clazz.newInstance(); System.out.println(person.toString()); &#125; /** * 反射的动态性举例 */ @Test public void test4() &#123; try &#123; Object p = GetInstance(&quot;com.atguigu.java3.Person&quot;); System.out.println(p.toString()); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; &#125; public Object GetInstance(String path) throws ClassNotFoundException &#123; Class&lt;?&gt; clazz = Class.forName(path); return clazz; &#125; /** * 调用运行时类的指定结构 * 1.属性 */ @Test//只能获取公的属性 public void test5() throws IllegalAccessException, InstantiationException, NoSuchFieldException &#123; Class&lt;Person&gt; clazz = Person.class; Person p = clazz.newInstance(); Field name = clazz.getField(&quot;name&quot;); name.set(p,&quot;张&quot;); System.out.println(name.get(p)); &#125; @Test//可以获取私的属性 public void test6() throws IllegalAccessException, InstantiationException, NoSuchFieldException &#123; Class&lt;Person&gt; clazz = Person.class; Person p = clazz.newInstance(); //可以获取所属性 Field name = clazz.getDeclaredField(&quot;name&quot;); name.setAccessible(true); name.set(p,&quot;张&quot;); System.out.println(name.getName()); Field age = clazz.getDeclaredField(&quot;age&quot;); age.setAccessible(true); age.set(p,12); System.out.println(age.get(p)); System.out.println(p.toString()); &#125; @Test//调用非静态方法 public void test7() throws IllegalAccessException, InstantiationException, NoSuchMethodException, InvocationTargetException &#123; Class&lt;Person&gt; clazz = Person.class; Person p = clazz.newInstance(); Method show = clazz.getDeclaredMethod(&quot;show&quot;, String.class);//方法名和形参类型 show.setAccessible(true); Object args = show.invoke(p, &quot;args&quot;);//对象，实参 System.out.println(args);//方法的返回值 &#125; @Test//调用静态方法 public void test8() throws IllegalAccessException, InstantiationException, NoSuchMethodException, InvocationTargetException &#123; Class&lt;Person&gt; clazz = Person.class; Person p = clazz.newInstance(); Method eat = clazz.getDeclaredMethod(&quot;eat&quot;, String.class); eat.setAccessible(true); Object o = eat.invoke(Person.class, &quot;可比克&quot;); System.out.println(o); &#125;&#125;class Person &#123; private String name; private int age; public Person() &#123; &#125; private int show(String args)&#123; System.out.println(args); return 1; &#125; private static void eat(String food)&#123; System.out.println(&quot;正在吃。。。&quot;+food); &#125; public Person(String name, int age) &#123; this.name = name; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; @Override public String toString() &#123; return &quot;Person&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, age=&quot; + age + &#x27;&#125;&#x27;; &#125;&#125; 十四，jdk8新特性1.Lambda表达式1234567891011121314Lambda表达式的使用1.举例：Comparator&lt;Integer&gt; c= Comparator.comparingInt(o -&gt; o);2.格式：-&gt;:Lambda操作符左边叫做形参列表，其实就是接口中的抽象方法的形参列表右边叫做Lambda体（重写的抽象方法的方法体）3.关于Lambda表达式的使用总结：-&gt; 左边：lambda形参列表的参数类型可以省略（类型推断），如果形参列表只有一个参数，（）可以省略。-&gt; 右边：Lambda体应该使用一对&#123;&#125;包裹，如果Lambda体只有一条执行语句（可能是return语句），&#123;&#125;和return也可以省略。要求接口只有一个抽象方法。4.Lambda表达式的本质：作为函数式接口的实例。5.如果一个接口中只声明了一个抽象方法，则此接口称为函数式接口。可以使用注解@FunctionalInterface检查是否是一个函数式接口。用匿名实现类表示的现在都可以用Lambda表达式表示。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class LambdaTest1 &#123; //语法格式一：无参数，无返回值 @Test public void test() &#123; Runnable r = () -&gt; System.out.println(&quot;语法格式一：无参数，无返回值&quot;); r.run(); &#125; //语法格式二：一个参数，无返回值 @Test public void test2() &#123; Consumer&lt;String&gt; c = (String s) -&gt; &#123; System.out.println(s); &#125;; c.accept(&quot;语法格式二：一个参数，无返回值&quot;); &#125; //语法格式：类型推断 @Test public void test3() &#123; Consumer&lt;String&gt; c = (s) -&gt; &#123; System.out.println(s); &#125;; c.accept(&quot;语法格式：类型推断&quot;); &#125; //语法格式四：只一个参数时，省略小括号 @Test public void test4() &#123; Consumer&lt;String&gt; c = s -&gt; &#123; System.out.println(s); &#125;; c.accept(&quot;语法格式四：只一个参数时，省略小括号&quot;); &#125; //语法格式五：Lambda 需要两个以上参数，多条执行语句，并且有返回值。 @Test public void test5() &#123; Comparator&lt;Integer&gt; c = (o1, o2) -&gt; &#123; System.out.println(&quot;语法格式五：Lambda 需要两个以上参数，多条执行语句，并且有返回值。&quot;); return o1.compareTo(o2); &#125;; int compare = c.compare(32, 21); System.out.println(compare); &#125; //语法格式六：当Lambda体只一条语句时，reurn与大括号若，都可以省略。 @Test public void test6() &#123; Comparator&lt;Integer&gt; c = (o1, o2) -&gt; o1.compareTo(o2); int compare = c.compare(32, 21); System.out.println(compare); &#125;&#125; 12345678910111213141516Java内置四大核心函数式接口（要求能看懂）消费性接口 Consumer&lt;T&gt; void accept(T t)供给型接口 Supplier&lt;T&gt; T get()函数型接口 Function&lt;T,R&gt; R apply(T t)断定型接口 Predicate&lt;T&gt; boolean test(T t)test()使用情景：当要传递给Lambda体的操作，已经有实现的方法了，可以使用方法引用。方法引用：本质上就是Lambda表达式，而Lambda表达式作为函数式接口的实例，所以方法引用，也是函数式接口的实例。使用格式： 类（对象）::方法名具体分为如下三种情况：对象：：非静态方法类：：静态方法类：：非静态方法方法引用的使用要求，要求接口中的抽象方法的形参列表和返回值类型与方法引用的方法的形参列表和返回值类型相同！ 2.Stream API123456789101112131.Stream关注的是数据的运算，与CPU打交道。集合关注的是是数据的存储，与内存打交道。2.①Stream自己不会存储元素。②Stream不会改变源对象，相反，他们会返回一个持有结果的新Stream。③Stream操作是延迟执行的，这意味着它们会等到需要结果的时候才执行。3.Stream的执行流程①Stream的实例化②一系列的中间操作（过滤，映射，。。。）③终止操作4.说明：①一个中间链操作，对数据源的数据进行处理。②一旦执行终止操作，就执行中间操作链，并产生结果。之后，不会再被使用。 Stream的实例化：12345678910111213141516171819202122232425262728293031//创建Stream的方式一：通过集合创建@Testpublic void test() &#123; List&lt;Employee&gt; employees = EmployeeData.getEmployees(); //返回一个顺序流 Stream&lt;Employee&gt; stream = employees.stream(); //返回一个并行流 Stream&lt;Employee&gt; employeeStream = employees.parallelStream();&#125;//创建Stream的方式二：通过数组@Testpublic void test2() &#123; int[] arr = new int[]&#123;1, 2, 3, 4, 5, 6&#125;; IntStream stream = Arrays.stream(arr);&#125;//创建方式：通过Stream的of()@Testpublic void test3() &#123; Stream&lt;Integer&gt; stream = Stream.of(1, 2, 3, 4, 5, 6);&#125;//创建Stream的方式四，无限流@Testpublic void test4() &#123; //迭代 Stream.iterate(0, t -&gt; t + 2).limit(10).forEach(System.out::println); //生成 Stream.generate(Math::random).limit(10).forEach(System.out::println);&#125; Stream的中间操作：1.筛选与切片123456789101112131415161718192021222324@Test//filter()过滤出需要的元素public void test()&#123; List&lt;Employee&gt; employees = EmployeeData.getEmployees(); Stream&lt;Employee&gt; stream = employees.stream(); stream.filter(e-&gt;e.getAge()&gt;25).forEach(System.out::println);&#125;@Test//limit()截断取前面public void test2()&#123; List&lt;Employee&gt; employees = EmployeeData.getEmployees(); Stream&lt;Employee&gt; stream = employees.stream(); stream.limit(5).forEach(System.out::println);&#125;@Test//skip()截断取后面public void test3()&#123; List&lt;Employee&gt; employees = EmployeeData.getEmployees(); Stream&lt;Employee&gt; stream = employees.stream(); stream.skip(5).forEach(System.out::println);&#125;@Test//public void test4()&#123;//distinct()去除重复元素 List&lt;Employee&gt; employees = EmployeeData.getEmployees(); Stream&lt;Employee&gt; stream = employees.stream(); stream.distinct().forEach(System.out::println);&#125; 2.映射12345678910111213@Test//map(str -&gt; str + str)public void test() &#123; int a[] = new int[]&#123;1, 2, 3, 4, 5, 6, 7, 8, 9&#125;; IntStream stream = Arrays.stream(a); stream.map(str -&gt; str + str).forEach(System.out::println);&#125;@Test////map(Employee::getAge)public void test2() &#123; List&lt;Employee&gt; employees = EmployeeData.getEmployees(); Stream&lt;Employee&gt; stream = employees.stream(); stream.map(Employee::getAge).filter(age -&gt; age &gt; 25).forEach(System.out::println);&#125; 3.排序12345678910111213141516171819202122232425262728@Test//自然排序public void test() &#123; int a[] = new int[]&#123;1, 2, 3, 5, 9, 7, 6, 45, 65&#125;; IntStream stream = Arrays.stream(a); stream.sorted().forEach(System.out::print);&#125;@Test//定制排序public void test2() &#123; List&lt;Employee&gt; employees = EmployeeData.getEmployees(); Stream&lt;Employee&gt; stream = employees.stream(); stream.sorted((o1, o2) -&gt; &#123; return -o1.getAge().compareTo(o2.getAge()); &#125;).forEach(System.out::println);&#125;@Test//定制排序public void test3() &#123; List&lt;Employee&gt; employees = EmployeeData.getEmployees(); Stream&lt;Employee&gt; stream = employees.stream(); stream.sorted((o1, o2) -&gt; &#123; if (o1.getAge() != o2.getAge()) &#123; return o1.getAge().compareTo(o2.getAge()); &#125; else &#123; return o1.getId().compareTo(o2.getId()); &#125; &#125;).forEach(System.out::println);&#125; Stream的终止操作1.匹配与查找1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556@Testpublic void test()&#123; List&lt;Employee&gt; employees = EmployeeData.getEmployees(); Stream&lt;Employee&gt; stream = employees.stream(); boolean b = stream.allMatch(e -&gt; e.getAge() &gt; 23); System.out.println(b);&#125;@Testpublic void test2()&#123; List&lt;Employee&gt; employees = EmployeeData.getEmployees(); Stream&lt;Employee&gt; stream = employees.stream(); boolean b = stream.anyMatch(e -&gt; e.getAge() &gt; 23); System.out.println(b);&#125;@Testpublic void test3()&#123; List&lt;Employee&gt; employees = EmployeeData.getEmployees(); Stream&lt;Employee&gt; stream = employees.stream(); boolean b = stream.noneMatch(e -&gt; e.getName().contains(&quot;java&quot;)); System.out.println(b);&#125;@Testpublic void test4()&#123; List&lt;Employee&gt; employees = EmployeeData.getEmployees(); Stream&lt;Employee&gt; stream = employees.stream(); Optional&lt;Employee&gt; first = stream.findFirst(); System.out.println(first);&#125;@Testpublic void test5()&#123; List&lt;Employee&gt; employees = EmployeeData.getEmployees(); Stream&lt;Employee&gt; stream = employees.stream(); Optional&lt;Employee&gt; any = stream.findAny(); System.out.println(any);&#125;@Testpublic void test6()&#123; List&lt;Employee&gt; employees = EmployeeData.getEmployees(); Stream&lt;Employee&gt; stream = employees.stream(); long count = stream.count(); System.out.println(count);&#125;@Testpublic void test7()&#123; List&lt;Employee&gt; employees = EmployeeData.getEmployees(); Stream&lt;Employee&gt; stream = employees.stream(); Optional&lt;Employee&gt; max = stream.max((e1, e2) -&gt; e1.getAge().compareTo(e2.getAge())); System.out.println(max);&#125;@Testpublic void test8()&#123; List&lt;Employee&gt; employees = EmployeeData.getEmployees(); Stream&lt;Employee&gt; stream = employees.stream(); Optional&lt;Employee&gt; min = stream.max((e1, e2) -&gt; -(e1.getAge().compareTo(e2.getAge()))); System.out.println(min);&#125; 2.归约1234567891011121314@Testpublic void test()&#123; List&lt;Employee&gt; employees = EmployeeData.getEmployees(); Stream&lt;Employee&gt; stream = employees.stream(); Optional&lt;Integer&gt; reduce = stream.map(e -&gt; e.getAge()).reduce((d1, d2) -&gt; d1 + d2); System.out.println(reduce);&#125;@Testpublic void test2()&#123; Integer a[]=new Integer []&#123;1,2,3,4,5,6,7,8,9&#125;; Stream&lt;Integer&gt; stream = Arrays.stream(a); Integer reduce = stream.reduce(0, Integer::sum); System.out.println(reduce);&#125; 3.收集123456789@Testpublic void test()&#123; List&lt;Employee&gt; employees = EmployeeData.getEmployees(); Stream&lt;Employee&gt; stream = employees.stream(); List&lt;Employee&gt; collect = stream.collect(Collectors.toList()); for (Employee employee : collect) &#123; System.out.print(employee+&quot; &quot;); &#125;&#125; 3.Optional类1234Optional类：为了在程序中避免出现空指针异常而创建的。常用的方法：ofNullable(T t)//允许存null ofElse(T t)//如果是null，使用形参中的对象。 get()://如果调用对象包含值，返回调用对象，否则抛出异常。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public class OptionalTest &#123; @Test public void test() &#123; Person p=new Person(); String boyName = getBoyName(p); System.out.println(boyName);//java.lang.NullPointerException原因：男孩为null &#125; @Test public void test2()&#123; Person p=null; String boyName = getBoyName(p); System.out.println(boyName);//java.lang.NullPointerException原因：Person为null &#125; @Test public void test3()&#123; Person p=null; System.out.println(getName(p)); &#125; public String getBoyName(Person person)&#123; return person.getBoy().getName(); &#125; public String getName(Person person)&#123; Optional&lt;Person&gt; person1 = Optional.ofNullable(person); Person person2 = person1.orElse(new Person()); Optional&lt;Boy&gt; boy = Optional.ofNullable(person2.getBoy()); Boy boy1 = boy.orElse(new Boy(&quot;Tom&quot;)); return boy1.getName(); &#125;&#125;class Person &#123; private Boy boy; public Person() &#123; &#125; public Person(Boy boy) &#123; this.boy = boy; &#125; public Boy getBoy() &#123; return boy; &#125; public void setBoy(Boy boy) &#123; this.boy = boy; &#125;&#125;class Boy &#123; private String name; public Boy() &#123; &#125; public Boy(String name) &#123; this.name = name; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125;&#125; 十五，JDBC核心技术1.概述软件架构方式： B/S：浏览器 C/S：客户端 数据的持久化1把数据保存到可掉电式存储设备中以供之后使用 JDBC的理解：123JDBC(Java Database Connectivity)是一个独立于特定数据库管理系统、通用的SQL数据库存取和操作的公共接口（一组API）简单理解为：JDBC，是SUN提供的一套 API，使用这套API可以实现对具体数据库的操作（获取连接、关闭连接、DML、DDL、DCL) 图示理解： 2.获取连接驱动和url123456Driver driver=new com.mysql.jdbc.Driver();//MySQL具体driver的实现类String url=&quot;jdbc:mysql://localhost:3306/test&quot;;jdbc:mysql :协议localhost :ip地址3306 ：端口号test：数据库 方式一123456789@Testpublic void test() throws SQLException &#123; Driver driver = new com.mysql.jdbc.Driver(); String url = &quot;jdbc:mysql://localhost:3306/test&quot;; Properties prop = new Properties(); prop.setProperty(&quot;user&quot;, &quot;root&quot;); prop.setProperty(&quot;password&quot;, &quot;yhd666&quot;); Connection connect = driver.connect(url, prop);&#125; 上述代码中显式出现了第三方数据库的API 方拾二1234567891011121314@Test//方式二：对方式一的迭代,不出现第方api，使程序具更好的可移植性public void test2() throws Exception &#123; //1.利用反射获取Driver的具体实现类对象 Class clazz = Class.forName(&quot;com.mysql.jdbc.Driver&quot;); Driver driver = (Driver) clazz.newInstance(); //2.提供要链接的数据库 String url = &quot;jdbc:mysql://localhost:3306/test&quot;; Properties prop = new Properties();//将用户名和密码封装在prop中。 prop.setProperty(&quot;user&quot;, &quot;root&quot;); prop.setProperty(&quot;password&quot;, &quot;yhd666&quot;); //3..获取连接 Connection connect = driver.connect(url, prop); System.out.println(connect);&#125; 方式三12345678@Test //方式：使用DriverManager替换driverpublic void test3() throws Exception &#123; //注册驱动 DriverManager.registerDriver((Driver) Class.forName(&quot;com.mysql.jdbc.Driver&quot;).newInstance()); //获取连接 Connection connection = DriverManager.getConnection(&quot;jdbc:mysql://localhost:3306/test&quot;, &quot;root&quot;, &quot;yhd666&quot;); System.out.println(connection);&#125; 使用DriverManager实现数据库的连接。体会获取连接必要的4个基本要素。 方式四1234567891011121314151617181920212223242526@Testpublic void testConnection4() throws Exception &#123; //1.数据库连接的4个基本要素： String url = &quot;jdbc:mysql://localhost:3306/test&quot;; String user = &quot;root&quot;; String password = &quot;yhd666&quot;; String driverName = &quot;com.mysql.jdbc.Driver&quot;; //2.加载驱动 （①实例化Driver ②注册驱动 Class.forName(driverName); //Driver driver = (Driver) clazz.newInstance(); //3.注册驱动 //DriverManager.registerDriver(driver); /* 可以注释掉上述代码的原因，是因为在mysql的Driver类中声明： static &#123; try &#123; DriverManager.registerDriver(new Driver()); &#125; catch (SQLException var1) &#123; throw new RuntimeException(&quot;Can&#x27;t register driver!&quot;); &#125; &#125; */ //3.获取连接 Connection conn = DriverManager.getConnection(url, user, password); System.out.println(conn);&#125; 方式五123456789101112131415@Test//最终版,配置文件版//1.实现了数据与代码的分离，解耦//2.如果需要修改配置文件信息，就可以避免程序重新打包public void test5() throws Exception &#123; InputStream is = Test1.class.getClassLoader().getResourceAsStream(&quot;jdbc.properties&quot;); Properties prop = new Properties(); prop.load(is); String user = prop.getProperty(&quot;user&quot;); String password = prop.getProperty(&quot;password&quot;); String url = prop.getProperty(&quot;url&quot;); String driverClass = prop.getProperty(&quot;driverClass&quot;); Class.forName(driverClass); Connection connection = DriverManager.getConnection(url, user, password); System.out.println(connection);&#125; 其中，配置文件声明在工程的src目录下：【jdbc.properties】 12345```propertiesuser=rootpassword=yhd666url=jdbc:mysql://localhost:3306/testdriverClass=com.mysql.jdbc.Driver 3.PreparedStatementStatement123456789101112131415161718数据库连接被用于向数据库服务器发送命令和 SQL 语句，并接受数据库服务器返回的结果。其实一个数据库连接就是一个Socket连接。- 在 java.sql 包中有 3 个接口分别定义了对数据库的调用的不同方式： - Statement：用于执行静态 SQL 语句并返回它所生成结果的对象。 - PrepatedStatement：SQL 语句被预编译并存储在此对象中，可以使用此对象多次高效地执行该语句。 - CallableStatement：用于执行 SQL 存储过程 使用Statement操作数据表的弊端- 通过调用 Connection 对象的 createStatement() 方法创建该对象。该对象用于执行静态的 SQL 语句，并且返回执行结果。- Statement 接口中定义了下列方法用于执行 SQL 语句： ```sql int excuteUpdate(String sql)：执行更新操作INSERT、UPDATE、DELETE ResultSet executeQuery(String sql)：执行查询操作SELECT 但是使用Statement操作数据表存在弊端： 问题一：存在拼串操作，繁琐 问题二：存在SQL注入问题其他问题：Statement没办法操作Blob类型变量Statement实现批量插入时，效率较低 SQL 注入是利用某些系统没有对用户输入的数据进行充分的检查，而在用户输入数据中注入非法的 SQL 语句段或命令(如：SELECT user, password FROM user_table WHERE user=’a’ OR 1 = ‘ AND password = ‘ OR ‘1’ = ‘1’) ，从而利用系统的 SQL 引擎完成恶意行为的做法。 对于 Java 而言，要防范 SQL 注入，只要用 PreparedStatement(从Statement扩展而来) 取代 Statement 就可以了。 12345678910111213141516171819202122232425262728293031323334353637383940### JDBCUtils封装获取连接和关闭资源```java/** * @author yinhuidong * @createTime 2020-02-04-19:26 * JDBC工具类 */public class JDBCUtils &#123; //获取连接 public static Connection getConnection()throws Exception&#123; InputStream is = JDBCUtils.class.getClassLoader().getResourceAsStream(&quot;jdbc.properties&quot;); Properties prop = new Properties(); prop.load(is); String driverClass=prop.getProperty(&quot;driverClass&quot;); String url=prop.getProperty(&quot;url&quot;); String user = prop.getProperty(&quot;user&quot;); String password = prop.getProperty(&quot;password&quot;); Class.forName(driverClass); Connection connection = DriverManager.getConnection(url, user, password); return connection; &#125; //关闭资源 public static void closeResource(Connection co, PreparedStatement ps, ResultSet rs)&#123; try &#123; if (co!=null)&#123; co.close(); &#125; if (ps!=null)&#123; ps.close(); &#125; if (rs!=null)&#123; rs.close(); &#125; &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 增删改操作添加操作1234567891011121314151617181920@Test//添加操作public void add()&#123; Connection co=null; PreparedStatement ps=null; try &#123; co=JDBCUtils.getConnection(); String sql=&quot;insert into customers(name,email,birth) values(?,?,?);&quot;; ps = co.prepareStatement(sql); ps.setObject(1,&quot;yinhuidong&quot;); ps.setObject(2,&quot;1972039773@qq.com&quot;); SimpleDateFormat format = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;); java.util.Date date = format.parse(&quot;1998-11-16&quot;); ps.setObject(3,new Date(date.getTime())); ps.execute(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; JDBCUtils.closeResource(co,ps,null); &#125;&#125; 删除操作12345678910111213141516@Test//删除操作public void delete()&#123; Connection co=null; PreparedStatement ps=null; try &#123; co=JDBCUtils.getConnection(); String sql=&quot;delete from customers where id = ?;&quot;; ps=co.prepareStatement(sql); ps.setObject(1,20); ps.execute(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; JDBCUtils.closeResource(co,ps,null); &#125;&#125; 修改操作：1234567891011121314151617@Test//修改public void update()&#123; Connection co=null; PreparedStatement ps=null; try &#123; co=JDBCUtils.getConnection(); String sql=&quot;update customers set name=? where id =?;&quot;; ps=co.prepareStatement(sql); ps.setObject(1,&quot;尹会东&quot;); ps.setObject(2,1); ps.execute(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; JDBCUtils.closeResource(co,ps,null); &#125;&#125; 通用增删改：1234567891011121314151617//通用增删改public static void update(String sql,Object...args)&#123; Connection conn=null; PreparedStatement ps=null; try &#123; conn=getConnection(); ps=conn.prepareStatement(sql); for (int i = 0; i &lt;args.length ; i++) &#123; ps.setObject(i+1,args[i]); &#125; ps.execute(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; closeResource(conn,ps,null); &#125;&#125; 通用查询操作123456789101112131415161718192021222324252627282930313233public static &lt;T&gt;List&lt;T&gt; find1(Class&lt;T&gt;clazz,String sql,Object...args) &#123; Connection co=null; PreparedStatement ps=null; ResultSet rs=null; try &#123; co=getConnection(); ps=co.prepareStatement(sql); for (int i = 0; i &lt;args.length ; i++) &#123; ps.setObject(i+1,args[i]); &#125; rs=ps.executeQuery(); ResultSetMetaData data = rs.getMetaData();//获取数据源 int count = data.getColumnCount();//获取一共有多少列 ArrayList&lt;T&gt; list = new ArrayList&lt;&gt;(); while (rs.next())&#123; T t = clazz.newInstance(); for (int i = 0; i &lt;count ; i++) &#123; Object value=rs.getObject(i+1); String name = data.getColumnLabel(i + 1);//获取别名，如果没别名就获取列名 Field field = clazz.getDeclaredField(name); field.setAccessible(true); field.set(t,value); &#125; list.add(t); &#125; return list; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; closeResource(co,ps,rs); &#125; return null;&#125; 表的列名和对象的属性名不一致的解决办法12345678910111213141516171819Java与SQL对应数据类型转换表| Java类型 | SQL类型 || ------------------ | ------------------------ || boolean | BIT || byte | TINYINT || short | SMALLINT || int | INTEGER || long | BIGINT || String | CHAR,VARCHAR,LONGVARCHAR || byte array | BINARY , VAR BINARY || java.sql.Date | DATE || java.sql.Time | TIME || java.sql.Timestamp | TIMESTAMP |1. **如何获取 ResultSetMetaData**： 调用 ResultSet 的 getMetaData() 方法即可2. **获取 ResultSet 中有多少列**：调用 ResultSetMetaData 的 getColumnCount() 方法3. **获取 ResultSet 每一列的列的别名是什么**：调用 ResultSetMetaData 的getColumnLabel() 方法 JDBC API小结123456789101112131415161718- 两种思想 - 面向接口编程的思想 - ORM思想(object relational mapping) - 一个数据表对应一个java类 - 表中的一条记录对应java类的一个对象 - 表中的一个字段对应java类的一个属性 &gt; sql是需要结合列名和表的属性名来写。注意起别名。- 两种技术 - JDBC结果集的元数据：ResultSetMetaData - 获取列数：getColumnCount() - 获取列的别名：getColumnLabel() - 通过反射，创建指定类的对象，获取指定的属性并赋值面试题：statement和preparedStatement的区别 操作Blob类型的变量12345678910111213141516171819202122@Test//像数据表中插入一个图片public void test()&#123; Connection co=null; PreparedStatement ps=null; try &#123; co=JDBCUtils.getConnection(); String sql=&quot;insert into customers(name,email,birth,photo)values(?,?,?,?);&quot;; ps=co.prepareStatement(sql); ps.setObject(1,&quot;宋红康&quot;); ps.setObject(2,&quot;shk@126.com&quot;); ps.setObject(3,&quot;1988-11-13&quot;); FileInputStream fis = new FileInputStream(&quot;D:\\\\copy1.jpg&quot;); ps.setObject(4,fis); ps.execute(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; finally &#123; JDBCUtils.closeResource(co,ps,null); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647@Test//查询blob字段public void test2()&#123; Connection co=null; PreparedStatement ps=null; ResultSet rs=null; InputStream bs=null; FileOutputStream fos=null; try &#123; co=JDBCUtils.getConnection(); String sql=&quot;select * from customers where id=?;&quot;; ps=co.prepareStatement(sql); ps.setObject(1,22); rs=ps.executeQuery(); while (rs.next()) &#123; Integer id = (Integer) rs.getObject(1); String name = (String) rs.getObject(2); String email = (String) rs.getObject(3); Date birth = (Date) rs.getObject(4); Customer customer = new Customer(id,name,email,birth); System.out.println(customer); Blob photo=rs.getBlob(5); bs = photo.getBinaryStream(); fos = new FileOutputStream(&quot;2.jpg&quot;); byte[]bytes=new byte[1024]; int len; while ((len=bs.read(bytes))!=-1) &#123; fos.write(bytes,0,len); &#125; &#125; &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; JDBCUtils.closeResource(co,ps,rs); try &#123; if (bs!=null)&#123; bs.close(); &#125; if (fos!=null)&#123; fos.close(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 插入中遇到的问题： 批处理123456789101112131415161718@Test//像数据库一张表插入两万条数据public void test() &#123; Connection conn = null; PreparedStatement ps = null; try &#123; conn = JDBCUtils.getConnection(); String sql = &quot;insert into goods(name)values(?)&quot;; ps = conn.prepareStatement(sql); for (int i = 1; i &lt;= 20000; i++) &#123; ps.setString(1, &quot;name_&quot; + i); ps.executeUpdate(); &#125; &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; finally &#123; JDBCUtils.closeResource(conn, ps, null); &#125;&#125; 1234567891011121314151617181920212223242526@Test//插入优化public void test3()&#123; Connection conn = null; PreparedStatement ps = null; try &#123; conn = JDBCUtils.getConnection(); //设置不允许自动提交数据 conn.setAutoCommit(false); String sql = &quot;insert into goods(name)values(?)&quot;; ps = conn.prepareStatement(sql); for (int i = 0; i &lt;=20000 ; i++) &#123; ps.setString(1, &quot;name_&quot; + i); ps.addBatch(); if (i%500==0)&#123; ps.executeBatch(); ps.clearBatch(); &#125; &#125; conn.commit(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; finally &#123; JDBCUtils.closeResource(conn,ps,null); &#125;&#125; 4.事务123456789101112131415161718192021221.事务：一组逻辑操作单元,使数据从一种状态变换到另一种状态。 * &gt; 一组逻辑操作单元：一个或多个DML操作。 &gt; 2.事务处理的原则： &gt; 保证所事务都作为一个工作单元来执行，即使出现了故障，都不能改变这种执行方式。 &gt; 当在一个事务中执行多个操作时，要么所有的事务都被提交(commit)，那么这些修改就永久地保存 &gt; 下来；要么数据库管理系统将放弃所作的所有修改，整个事务回滚(rollback)到最初状态。说明：1.数据一旦提交，就不可回滚 * 2.哪些操作会导致数据的自动提交？ * &gt;DDL操作一旦执行，都会自动提交。 * &gt;set autocommit = false 对DDL操作失效 * &gt;DML默认情况下，一旦执行，就会自动提交。 * &gt;我们可以通过set autocommit = false的方式取消DML操作的自动提交。 * &gt;默认在关闭连接时，会自动的提交数据 考虑事务以后的转账问题12345678910111213141516171819202122232425@Testpublic void test() &#123; Connection co = null; try &#123; co = JDBCUtils.getConnection(); co.setAutoCommit(false); String sql1 = &quot;update user_table set balance=balance-? where user=?&quot;; String sql2 = &quot;update user_table set balance=balance+? where user=?&quot;; update(co,sql1,100,&quot;AA&quot;); //System.out.println(10/0); update(co,sql2,100,&quot;BB&quot;); co.commit(); System.out.println(&quot;转账成功！&quot;); &#125; catch (SQLException e) &#123; try &#123; co.rollback(); System.out.println(&quot;转账失败！&quot;); &#125; catch (SQLException e1) &#123; e1.printStackTrace(); &#125; &#125; finally &#123; JDBCUtils.closeResource(co,null,null); &#125;&#125; 123456789101112131415public static int update(Connection co, String sql, Object... args) &#123; PreparedStatement ps = null; try &#123; ps = co.prepareStatement(sql); for (int i = 0; i &lt; args.length; i++) &#123; ps.setObject(i + 1, args[i]); &#125; return ps.executeUpdate(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; finally &#123; JDBCUtils.closeResource(null, ps, null); &#125; return 0;&#125; 事务的属性ACID 数据操作过程中可能出现的问题：(针对隔离性) 数据库的四种隔离级别：(一致性和并发性：一致性越好，并发性越差) 如何查看并设置隔离级别： 5.连接池技术传统连接存在的问题： 使用数据库连接池大的好处1231.提高程序的响应速度(减少了创建连接相应的时间)2.降低资源的消耗(可以重复使用已经提供好的连接)3.便于连接的管理 c3p0连接池1234567//使用c3p0数据库连接池的配置文件方式，获取数据库的连接：推荐private static DataSource cpds = new ComboPooledDataSource(&quot;helloc3p0&quot;);public static Connection getConnection() throws SQLException &#123; Connection conn = cpds.getConnection(); return conn;&#125; c3p0-config.xml 1234567891011121314151617181920212223242526&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;c3p0-config&gt; &lt;named-config name=&quot;helloc3p0&quot;&gt; &lt;!-- 提供获取连接的4个基本信息 --&gt; &lt;property name=&quot;driverClass&quot;&gt;com.mysql.jdbc.Driver&lt;/property&gt; &lt;property name=&quot;jdbcUrl&quot;&gt;jdbc:mysql:///test&lt;/property&gt; &lt;property name=&quot;user&quot;&gt;root&lt;/property&gt; &lt;property name=&quot;password&quot;&gt;yhd666&lt;/property&gt; &lt;!-- 进行数据库连接池管理的基本信息 --&gt; &lt;!-- 当数据库连接池中的连接数不够时，c3p0一次性向数据库服务器申请的连接数 --&gt; &lt;property name=&quot;acquireIncrement&quot;&gt;5&lt;/property&gt; &lt;!-- c3p0数据库连接池中初始化时的连接数 --&gt; &lt;property name=&quot;initialPoolSize&quot;&gt;10&lt;/property&gt; &lt;!-- c3p0数据库连接池维护的最少连接数 --&gt; &lt;property name=&quot;minPoolSize&quot;&gt;10&lt;/property&gt; &lt;!-- c3p0数据库连接池维护的最多的连接数 --&gt; &lt;property name=&quot;maxPoolSize&quot;&gt;100&lt;/property&gt; &lt;!-- c3p0数据库连接池最多维护的Statement的个数 --&gt; &lt;property name=&quot;maxStatements&quot;&gt;50&lt;/property&gt; &lt;!-- 每个连接中可以最多使用的Statement的个数 --&gt; &lt;property name=&quot;maxStatementsPerConnection&quot;&gt;2&lt;/property&gt; &lt;/named-config&gt;&lt;/c3p0-config&gt; dbcp连接池123456789101112131415161718//使用dbocp数据库连接池的配置文件方式，获取数据库的连接：推荐private static DataSource source = null; static&#123; try &#123; Properties pros = new Properties(); InputStream is = ClassLoader.getSystemClassLoader().getResourceAsStream(&quot;dbcp.properties&quot;); pros.load(is); //根据提供的BasicDataSourceFactory创建对应的DataSource对象 source = BasicDataSourceFactory.createDataSource(pros); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;public static Connection getConnection2() throws SQLException &#123; Connection conn = source.getConnection(); return conn;&#125; dbcp.properties 123456driverClassName=com.mysql.jdbc.Driverurl=jdbc:mysql:///testusername=rootpassword=yhd666initialSize=10 druid连接池12345678910111213141516//使用druid数据库连接池的配置文件方式，获取数据库的连接：推荐 private static DataSource ds = null; static &#123; try &#123; Properties pro = new Properties();// pro.load(PoolTest.class.getClassLoader().getResourceAsStream(&quot;druid.properties&quot;));//加载配置文件 pro.load(ClassLoader.getSystemClassLoader().getResourceAsStream(&quot;druid.properties&quot;)); ds = DruidDataSourceFactory.createDataSource(pro);//通过工厂创建数据源 &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; public static Connection getConnection3() &#123; return ds.getConnection();//通过数据源获取连接 &#125; druid.properties 12345678910111213141516url=jdbc:mysql://localhost:3306/test?rewriteBatchedStatements=trueusername=rootpassword=yhd666driverClassName=com.mysql.jdbc.Driver#初始化时的连接数initialSize=10#最大连接池数量maxActive=20#获取连接时最大等待时间，单位毫秒。maxWait=1000#防御sql注入的filter:wallfilters=wall#mysql下建议关闭poolPreparedStatements=false#建议配置为true，不影响性能，并且保证安全性。testWhileIdle=true 6.DBUtils工具类通用增删改12345678910111213141516171819使用现成的jar中的QueryRunner测试增、删、改的操作://测试插入 @Test public void testInsert() &#123; Connection conn = null; try &#123; QueryRunner runner = new QueryRunner(); conn = JDBCUtils.getConnection3(); String sql = &quot;insert into customers(name,email,birth)values(?,?,?)&quot;; int insertCount = runner.update(conn, sql, &quot;蔡徐坤&quot;,&quot;caixukun@126.com&quot;,&quot;1997-09-08&quot;); System.out.println(&quot;添加了&quot; + insertCount + &quot;条记录&quot;); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125;finally&#123; JDBCUtils.closeResource(conn, null); &#125; &#125; 查询操作112345678910111213141516171819202122232425使用现成的jar中的QueryRunner测试查询的操作://测试查询 /* * BeanHander:是ResultSetHandler接口的实现类，用于封装表中的一条记录。 */ @Test public void testQuery1()&#123; Connection conn = null; try &#123; QueryRunner runner = new QueryRunner(); conn = JDBCUtils.getConnection3(); String sql = &quot;select id,name,email,birth from customers where id = ?&quot;; BeanHandler&lt;Customer&gt; handler = new BeanHandler&lt;&gt;(Customer.class); Customer customer = runner.query(conn, sql, handler, 23); System.out.println(customer); &#125; catch (SQLException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125;finally&#123; JDBCUtils.closeResource(conn, null); &#125; &#125; 查询操作21234567891011121314151617181920212223/* * BeanListHandler:是ResultSetHandler接口的实现类，用于封装表中的多条记录构成的集合。 */ @Test public void testQuery2() &#123; Connection conn = null; try &#123; QueryRunner runner = new QueryRunner(); conn = JDBCUtils.getConnection3(); String sql = &quot;select id,name,email,birth from customers where id &lt; ?&quot;; BeanListHandler&lt;Customer&gt; handler = new BeanListHandler&lt;&gt;(Customer.class); List&lt;Customer&gt; list = runner.query(conn, sql, handler, 23); list.forEach(System.out::println); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125;finally&#123; JDBCUtils.closeResource(conn, null); &#125; &#125; 查询操作312345678910111213141516171819202122/* * MapHander:是ResultSetHandler接口的实现类，对应表中的一条记录。 * 将字段及相应字段的值作为map中的key和value */ @Test public void testQuery3()&#123; Connection conn = null; try &#123; QueryRunner runner = new QueryRunner(); conn = JDBCUtils.getConnection3(); String sql = &quot;select id,name,email,birth from customers where id = ?&quot;; MapHandler handler = new MapHandler(); Map&lt;String, Object&gt; map = runner.query(conn, sql, handler, 23); System.out.println(map); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125;finally&#123; JDBCUtils.closeResource(conn, null); &#125; &#125; 查询操作41234567891011121314151617181920212223/* * MapListHander:是ResultSetHandler接口的实现类，对应表中的多条记录。 * 将字段及相应字段的值作为map中的key和value。将这些map添加到List中 */ @Test public void testQuery4()&#123; Connection conn = null; try &#123; QueryRunner runner = new QueryRunner(); conn = JDBCUtils.getConnection3(); String sql = &quot;select id,name,email,birth from customers where id &lt; ?&quot;; MapListHandler handler = new MapListHandler(); List&lt;Map&lt;String, Object&gt;&gt; list = runner.query(conn, sql, handler, 23); list.forEach(System.out::println); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125;finally&#123; JDBCUtils.closeResource(conn, null); &#125; &#125; 查询操作51234567891011121314151617181920212223242526272829303132333435363738394041424344/* * ScalarHandler:用于查询特殊值 */ @Test public void testQuery5()&#123; Connection conn = null; try &#123; QueryRunner runner = new QueryRunner(); conn = JDBCUtils.getConnection3(); String sql = &quot;select count(*) from customers&quot;; ScalarHandler handler = new ScalarHandler(); Long count = (Long) runner.query(conn, sql, handler); System.out.println(count); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125;finally&#123; JDBCUtils.closeResource(conn, null); &#125; &#125; @Test public void testQuery6()&#123; Connection conn = null; try &#123; QueryRunner runner = new QueryRunner(); conn = JDBCUtils.getConnection3(); String sql = &quot;select max(birth) from customers&quot;; ScalarHandler handler = new ScalarHandler(); Date maxBirth = (Date) runner.query(conn, sql, handler); System.out.println(maxBirth); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125;finally&#123; JDBCUtils.closeResource(conn, null); &#125; &#125; 自定义ResultSetHandler的实现类12345678910111213141516171819202122232425262728293031323334353637383940414243/* * 自定义ResultSetHandler的实现类 */ @Test public void testQuery7()&#123; Connection conn = null; try &#123; QueryRunner runner = new QueryRunner(); conn = JDBCUtils.getConnection3(); String sql = &quot;select id,name,email,birth from customers where id = ?&quot;; ResultSetHandler&lt;Customer&gt; handler = new ResultSetHandler&lt;Customer&gt;()&#123; @Override public Customer handle(ResultSet rs) throws SQLException &#123;// System.out.println(&quot;handle&quot;);// return null; // return new Customer(12, &quot;成龙&quot;, &quot;Jacky@126.com&quot;, new Date(234324234324L)); if(rs.next())&#123; int id = rs.getInt(&quot;id&quot;); String name = rs.getString(&quot;name&quot;); String email = rs.getString(&quot;email&quot;); Date birth = rs.getDate(&quot;birth&quot;); Customer customer = new Customer(id, name, email, birth); return customer; &#125; return null; &#125; &#125;; Customer customer = runner.query(conn, sql, handler,23); System.out.println(customer); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125;finally&#123; JDBCUtils.closeResource(conn, null); &#125; &#125; 资源关闭1234567891011121314151617使用dbutils.jar包中的DbUtils工具类实现连接等资源的关闭：/** * * @Description 使用dbutils.jar中提供的DbUtils工具类，实现资源的关闭 * @author shkstart * @date 下午4:53:09 * @param conn * @param ps * @param rs */ public static void closeResource1(Connection conn,Statement ps,ResultSet rs)&#123; DbUtils.closeQuietly(conn); DbUtils.closeQuietly(ps); DbUtils.closeQuietly(rs); &#125; 7.DAO设计模式BaseDao类12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576package com.atguigu.utils;import org.apache.commons.dbutils.QueryRunner;import org.apache.commons.dbutils.handlers.*;import java.lang.reflect.ParameterizedType;import java.lang.reflect.Type;import java.sql.Connection;import java.sql.SQLException;import java.util.List;import java.util.Map;/** * @author yinhuidong * @createTime 2020-02-12-21:07 */public class BaseDao&lt;T&gt; &#123; private Class&lt;T&gt; type;// 定义一个变量来接收泛型的类型 public BaseDao() &#123; //当子类调用父类方法时获取当前子类的父类类型 ParameterizedType parameterizedType = (ParameterizedType) this.getClass().getGenericSuperclass(); //获取具体的泛型的类型 Type[] types = parameterizedType.getActualTypeArguments(); this.type = (Class&lt;T&gt;) types[0]; &#125; private QueryRunner runner = new QueryRunner(); /** * 考虑到事务以后的通用增删改 */ public int update(Connection co, String sql, Object... args) throws SQLException &#123; return runner.update(co, sql, args); &#125; /** * 考虑到事务以后的通用查询一 */ public &lt;T&gt; T select1(Connection co, String sql, Object... args) throws SQLException &#123; BeanHandler&lt;T&gt; handler = new BeanHandler&lt;&gt;((Class&lt;T&gt;) type); return runner.query(co, sql, handler, args); &#125; /** * 考虑到事务以后的通用查询二 */ public &lt;T&gt; List&lt;T&gt; select2(Connection co, String sql, Object... args) throws SQLException &#123; BeanListHandler&lt;T&gt; handler = new BeanListHandler&lt;&gt;((Class&lt;T&gt;) type); return runner.query(co, sql, handler, args); &#125; /** * MapHandler */ public Map&lt;String, Object&gt; select3(Connection co, String sql, Object... args) throws SQLException &#123; MapHandler handler = new MapHandler(); return runner.query(co, sql, handler, args); &#125; /** * MapListHandler */ public List&lt;Map&lt;String, Object&gt;&gt; select4(Connection co, String sql, Object... args) throws SQLException &#123; MapListHandler handler = new MapListHandler(); return runner.query(co, sql, handler, args); &#125; /** * 利用ScalarHandler获取单一值 * 考虑事务问题 */ public Object select5(Connection co, String sql, Object... args) throws SQLException &#123; ScalarHandler handler = new ScalarHandler(); return runner.query(co, sql, handler, args); &#125;&#125; Dao接口123456789101112/** * @author yinhuidong * @createTime 2020-02-06-14:49 */public interface CustomerDao &#123; int add(Connection co,Object...args) throws SQLException; int delete(Connection co,Object...args) throws SQLException; int updates(Connection co,Object...args) throws SQLException; Customer FindByValue(Connection co,Object...args) throws SQLException; List&lt;Customer&gt; FindMore(Connection co, Object...args) throws SQLException; Long count(Connection co,Object...args) throws SQLException;&#125; DaoImpl实现类1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * @author yinhuidong * @createTime 2020-02-06-14:56 */public class CustomerDaoImpl extends BaseDao&lt;Customer&gt; implements CustomerDao &#123; @Override public int add(Connection co, Object... args) throws SQLException &#123; String sql=&quot;insert into customers (name,email,birth)values(?,?,?);&quot;; return update(co,sql,args); &#125; @Override public int delete(Connection co, Object... args) throws SQLException &#123; String sql=&quot;delete from customers where id=?;&quot;; return update(co,sql,args); &#125; @Override public int updates(Connection co, Object... args) throws SQLException &#123; String sql=&quot;update customers set name=?,email=?,birth=? where id=?;&quot;; return update(co,sql,args); &#125; @Override public Customer FindByValue(Connection co, Object... args) throws SQLException &#123; String sql=&quot;select id,name,email,birth from customers where id=?;&quot;; return selectById(co,sql,args); &#125; @Override public List&lt;Customer&gt; FindMore(Connection co, Object... args) throws SQLException &#123; String sql=&quot;select id,name,email,birth from customers ;&quot;; return selectMore(co,sql,args); &#125; @Override public Long count(Connection co,Object...args) throws SQLException &#123; String sql=&quot;select count(*) from customers ;&quot;; return (Long) selectOne(co,sql,args); &#125;&#125; 测试类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/** * @author yinhuidong * @createTime 2020-02-06-15:10 */public class TestDao &#123; private CustomerDao dao=new CustomerDaoImpl(); @Test public void test()&#123; Connection co=null; try &#123; co=JDBCUtils.getConnection(); co.setAutoCommit(false); SimpleDateFormat format = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;); Date date = format.parse(&quot;2017-02-21&quot;); int add = dao.add(co, &quot;name&quot;, &quot;email&quot;, new java.sql.Date(date.getTime())); co.commit(); if (add&gt;0)&#123; System.out.println(&quot;添加成功！&quot;); &#125;else&#123; System.out.println(&quot;添加失败！&quot;); &#125; &#125; catch (SQLException e) &#123; try &#123; co.rollback(); &#125; catch (SQLException e1) &#123; e1.printStackTrace(); &#125; &#125; catch (ParseException e) &#123; e.printStackTrace(); &#125; finally &#123; JDBCUtils.closeResource(co); &#125; &#125; @Test public void test2()&#123; Connection co=null; try &#123; co=JDBCUtils.getConnection(); co.setAutoCommit(false); Customer customer = dao.FindByValue(co, 21); co.commit(); System.out.println(customer); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; finally &#123; JDBCUtils.closeResource(co); &#125; &#125;&#125; JDBCUtils工具类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263package com.atguigu.java6;import com.alibaba.druid.pool.DruidDataSourceFactory;import com.alibaba.druid.util.JdbcUtils;import com.mchange.v2.c3p0.ComboPooledDataSource;import org.apache.commons.dbcp.BasicDataSourceFactory;import javax.sql.DataSource;import java.io.InputStream;import java.sql.Connection;import java.sql.SQLException;import java.util.Properties;/** * @author yinhuidong * @createTime 2020-02-27-21:52 */public class JDBCUtils &#123; /** * c3p0 */ private static DataSource source1=new ComboPooledDataSource(&quot;helloc3p0&quot;); public static Connection getConnection() throws SQLException &#123; return source1.getConnection(); &#125; /** * dbcp */ private static DataSource source2=null; static&#123; try &#123; InputStream is = JDBCUtils.class.getClassLoader().getResourceAsStream(&quot;dbcp.properties&quot;); Properties prop = new Properties(); prop.load(is); source2=BasicDataSourceFactory.createDataSource(prop); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; public static Connection getConnection2() throws SQLException &#123; return source2.getConnection(); &#125; private static DataSource source3=null; static &#123; try &#123; Properties prop = new Properties(); InputStream is = JDBCUtils.class.getClassLoader().getResourceAsStream(&quot;druid.properties&quot;); prop.load(is); source3=DruidDataSourceFactory.createDataSource(prop); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; public static Connection getConnection3() throws SQLException &#123; return source3.getConnection(); &#125; /** * closeResource */ public static void closeResource(Connection co)&#123; JdbcUtils.close(co); &#125;&#125;","categories":[{"name":"JAVA基础","slug":"JAVA基础","permalink":"https://yinhuidong.github.io/categories/JAVA%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"JAVA基础","slug":"JAVA基础","permalink":"https://yinhuidong.github.io/tags/JAVA%E5%9F%BA%E7%A1%80/"}]},{"title":"JAVA-SE核心基础篇","slug":"JAVA基础/JAVA-SE核心基础篇","date":"2022-01-12T00:19:55.241Z","updated":"2022-01-12T00:19:55.243Z","comments":true,"path":"2022/01/12/JAVA基础/JAVA-SE核心基础篇/","link":"","permalink":"https://yinhuidong.github.io/2022/01/12/JAVA%E5%9F%BA%E7%A1%80/JAVA-SE%E6%A0%B8%E5%BF%83%E5%9F%BA%E7%A1%80%E7%AF%87/","excerpt":"","text":"一，java语言概述1.常用Dos命令123456789dir：列出当前目录下的文件以及文件夹md：创建目录rd：删除目录（空目录）cd：进入指定目录cd..:退回上一级目录cd/:退回根目录del:删除文件echo 1 &gt;java.txt:创建文件exit：退出 2，java语言运行机制及运行过程1234Java语言特点：跨平台性Java两种核心机制：Java虚拟机（jvm）：Java程序运行环境垃圾回收机制（gc） 3.jdk，jre，jvm的关系12jdk：Java开发工具包。（Java开发工具包和jre）jre：Java运行环境。（包括jvm） 4 环境变量的配置12345678为什么要添加环境变量？让Java工具包在任何路径下都可以使用。高级系统设置-环境变量:系统变量-path（window系统执行命令时需要搜索的路径）加上 jdk的安装路径\\bin开发时的配置(推荐)：在系统变量里面新建变量JAVA_HOME 值为jdk安装路径。然后再classpath变量中加入：%JAVA_HOME%\\bin 5.HelloWorld12345如何显示记事本后缀：我的电脑-查看-文件拓展名Java代码写在.java结尾的文件中。（源文件）通过javac命令对该Java文件进行编译。（字节码文件）通过Java命令对。class文件进行运行。 6 注释12341.单行注释//2.多行注释/**/3.文档注释/** */注释内容可以被jdk提供的工具javadoc解析，生成一套以网页文件形式体现的该程序的说明文档。 二，java基本语法1.关键字和保留字12关键字：在Java语言里有特殊含义的字符串。保留字：目前没用到，以后的版本可能会用到。 2.标识符123456标识符：自己起的名字。包括：包名，类名。。。0-9 _ $数字不可以开头不能使用关键字和保留字，可以包含严格区分大小写不能包含空格 3.变量12345678910111213141516171819202122232425262728293031323334353637383940变量变量：内存中的一个存储区域。 可以在同一类型范围内变化。包含：类型，名，存储的值。作用：内存中保存数据。 先声明，后使用。 同一个作用域内，不能定义重复的变量名。变量分类：按照数据类型分类 基本数据类型：数值（整数，浮点数），字符，布尔 引用数据类型：类（包括字符串），接口，数组变量分类：按照声明位置不同分类 成员变量：（类内，方法体外） 包括：实例变量（不以static修饰），类变量（以static修饰） 局部变量：（方法体内） 包括：形参，方法局部变量，代码块局部变量1.整型：byte,short,int,long 字节(1.2.4.8)1字节=8bit *long型变量的声明必须以l或L结尾2.浮点型：float（4），double（8） float：单精度浮点数，精确7位 *Java的浮点型常量默认为double类型，声明float常量，后加f或F。3.字符类型 char（2） 表示方式：1.声明一个字符，2.定义一个转义符。 char c=&#x27;\\n&#x27;;4.布尔类型 boolean:true,false 双引号里面如果想要使用双引号，前面需要加\\ char+int=int基本数据类型之间的运算1.自动类型提升 byte+int=int Java支持自动向上转型。 当byte，char，short三种变量做运算时，结果为int。2.强制类型转换 向下转型 强制类型转换符（int），可能会损失精度。 *long赋值的数后面不加l可能会导致编译失败，过大的整数。整型常量默认类型为int，浮点型默认常量为double。String类型变量的使用String属于引用类型。String可以和所有类型做运算。 4.运算符1234567891011121314算术运算符 a=2;b=++a;=&gt;a=3,b=3; a=2;b=a++;=&gt;a=3,b=2;赋值运算符比较运算符逻辑运算符 &amp;和&amp;&amp;的异同： 1）都表示且 2）&amp;&amp;短路且位运算符三元运算符三元运算符的嵌套： int sum=(a&gt;b):a?((a==b)?&quot;a==b&quot;:b) 另一个三目运算符作为表达式出现 5.流程控制12345678910111213141516171819202122232425顺序结构分支结构 if-else switch-case switch(表达式)&#123; case 常量1： 执行语句； break； default: 执行语句； &#125; switch结构中的表达式，只能是如下6种数据类型： byte，short，char，int，String，枚举。例题：年份累加循环结构： for while do&#123; &#125;while（）；键盘输入：import java.util.Scanner;Scanner sc=new Scanner(System.in);int a=sc.nextInt();String b=sc.next(); 三，数组1.一维数组1）数组默认值123456数组元素的默认初始化值：整型数组元素默认值为0浮点型数组元素默认值0.0布尔型数组元素默认值false字符型数组元素默认值Ascii值位0的元素引用数据类型数组元素默认值null 2）一位数组内存解析 2.二维数组1）二维数组的使用1234567891011121314151617181920// 静态初始化int[][] arr = new int[][] &#123; &#123; 1, 2, 3 &#125;, &#123; 4, 5 &#125;, &#123; 6, 8, 7 &#125; &#125;;// 动态初始化1int[][] arr1 = new int[3][2];// 动态初始化2int[][] arr2 = new int[3][];// 调用arr2arr2[1] = new int[3];// 先指定列的个数for (int i = 0; i &lt; arr2[1].length; i++) &#123; System.out.println(arr2[1][i]);&#125;// 如何获取数组的长度System.out.println(arr.length);// 3System.out.println(arr[0].length);// 3// 二维数组元素的调用for (int i = 0; i &lt; arr.length; i++) &#123; for (int j = 0; j &lt; arr[i].length; j++) &#123; System.out.println(arr[i][j]); &#125;&#125; 2）二维数组默认初始值1234int a[][]=new int[3][3];System.out.println(a[0]);//输出地址[I@15db9742System.out.println(a[0][0]);//0System.out.println(a[2][3]);//java.lang.ArrayIndexOutOfBoundsException 3）二维数组的内存解析 3.数组中设计的常见算法：1）反转 2）线性查找 3）二分查找 4）冒泡排序 4.Arrays工具类的使用12345678Arrays工具类的使用int a[] = new int[] &#123; 43, 26, 25, 65, 89, 75, 45, 13, 23, 15, 65 &#125;;int b[] = new int[] &#123; 43, 26, 25, 65, 89, 75, 45, 13, 23, 15, 65 &#125;;System.out.println(Arrays.equals(a, b));//判断两个数组是否相等Arrays.sort(a);//对数组进行从小到大排序int key=89;System.out.println(Arrays.binarySearch(a, key));//对排序后的数组进行二分查找System.out.println(Arrays.toString(a));//输出数组信息 5.数组中的常见异常123456789101112131.下标越界异常2.空指针异常/** int a[] = new int[] &#123; 43, 26, 25, 65, 89, 75, 45, 13, 23, 15, 65 &#125;; a=null;* System.out.println(a[0]);*//** int[][]arr=new int [4][];* System.out.println(arr[0][0]);*/String arr[]=new String [] &#123;&quot;aa&quot;,&quot;bb&quot;,&quot;cc&quot;&#125;;arr[0]=null;System.out.println(arr[0].toString()); 四，面向对象1.成员变量（属性）与局部变量的区别12345678910111213不同点：1.类中的声明位置不同 1）类内，方法体外 2）方法的形参列表，方法体内，构造器形参，构造器内部变量2.权限修饰符的不同 1）可以在声明属性时指明其权限，使用权限修饰符 2）不能使用权限修饰符3.默认初始化值 1）根据其类型都有默认的初始化值 2）没有默认初始化值（调用之前显示赋值），形参在调用时赋值。4.内存中加载的位置 1）堆中（非static） 2）栈中 2.return关键字1234567891适用范围：使用在方法体中2.作用：1）结束方法2）针对有返回值类型的方法，返回值。3.return 后面不可以声明执行语句方法中使用的注意点：可以调用当前类的属性和方法方法中不可以定义方法 3.对象数组内存解析 4.匿名对象与方法重载匿名对象123new Phone().price=1999;new Phone().showPrice;//0.0每次new的都是堆空间的一个新对象。 方法重载12341.定义：在同一个类中，允许存在一个以上同名方法，只要他们的参数个数/顺序，参数类型不同即可。2.特点：与返回值无关，与权限修饰符无关。常见的：构造器重载 5.可变个数形参与变量赋值可变个数形参12Object...args如果有多个参数，必须放到最后。 变量赋值123如果变量是基本数据类型：此时赋值的是变量所保存的数据值。如果变量时引用数据类型：此时赋值的是变量所保存数据的地址值。此时改变变量的值，相当于改变地址对应的值。 6.值传递机制与递归方法值传递机制1234形参：方法定义时，小括号内的参数。实参：方法调用时，实际传递给形参的值。如果参数是基本数据类型，此时赋值的是变量所保存的数据值。如果参数是引用数据类型，此时赋给形参的值，是变量所指向地址的值。 递归方法一个方法体内，自己调用自己。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/** * 递归方法求1-100的和 */public static void main(String[] args) &#123; int n = 100; int sum = getSum(n); System.out.println(sum);&#125;public static int getSum(int sum) &#123; if (sum == 1) &#123; return 1; &#125; else &#123; return sum + getSum(sum - 1); &#125;&#125;/** * 已知一个数列，f(0)=1,f(1)=4,f(n+2)=2*f(n+1)+f(n); * 其中n是大于0的整数，求f（10。 * * @param sum * @return */public static int getSum(int sum) &#123; if (sum == 0) &#123; return 1; &#125; else if (sum == 1) &#123; return 4; &#125; else &#123; return 2 * getSum(sum - 1) + getSum(sum - 2); &#125;&#125;/** * 递归方法求斐波那契数列的前n项，并输出。 * @return */public static void main(String[] args) &#123; int arg=10; int avg=getArgs(arg); System.out.println(avg);&#125;public static int getArgs(int args)&#123; if(args&lt;=2)&#123; return 1; &#125; else&#123; return getArgs(args-1)+getArgs(args-2); &#125;&#125; 7.封装和隐藏封装和隐藏：1234特点：高内聚：类的内部数据操作细节自己完成，不允许外部干涉低耦合：仅仅对外暴漏少量方法用于使用。体现：私有化属性，设置公有的方法来获取和设置属性。需要权限修饰符来配合。 权限修饰符：123451.private：类内部2.default：同一包下的类3.protected：不同包的子类4.public：同一项目下用来修饰类：public和default，和类的内部结构：属性，方法，构造器，内部类。 构造器：construct1234作用：1）创建对象2）初始化对象构造器重载 属性赋值的先后顺序：12341）默认初始化2）显示初始化3）构造器初始化4）set方法 javabean123类是公有的有一个无参的公共构造器有属性，且有对应的get，set方法 关键字this的使用123456789101112131）this可以用来修饰属性和方法this代表当前对象2）this可以用来修饰和调用构造器调用构造器：public Student(Integer number, Integer state) &#123;this.number = number;this.state = state;&#125;public Student(Integer number, Integer state, Double score) &#123;this(number,state);//调用其他构造器，不能调用自己。this.score = score;&#125; MVC设计模式：123视图模型层view控制器层controller：service，base，activity模型层model：bean，dao，db 继承性：extends12345678减少了代码的量，提高了代码复用性便于功能的拓展为多态的实现提供了前提一旦子类A继承父类B以后，A就获取了B中声明的所有属性和方法。规定：一个类可以被多个子类继承一个类只能由一个父类允许多层继承 8.Eclipse中的Debug1234567Debug：1.设置断点：2.按键：F5进入方法，F6一行一行执行，F7从方法中出来resume：终止此处断点，进入下一处。 Terminate：强行终止 debug中step into功能失灵问题：更换eclipsejre为jdk的。 9.方法重写与权限修饰符方法重写123456789101.重写：子类继承父类以后，可以对父类中同名同参数的方法，进行覆盖操作。2.应用：重写以后，当子类执行该方法，实际上执行的是子类重写父类的方法。3.规定：方法名形参列表必须相同，子类重写方法的权限修饰符不小于父类被重写的方法，*子类不能重写父类的私有方法，返回值类型：void--》void，其他的小于等于父类的返回值类型。子类抛出的异常类型不能大于父类。非static（因为静态方法不能被覆盖，随着类的加载而加载） 权限修饰符123四种权限修饰符在不同包的子类中，能调用order类中声明为protected和public的属性，方法。不同包下的不同类（非子类）只能调用order类中的public的属性和方法。 10.super关键字123456789101112super调用属性和方法1.我们可以在子类的方法或构造器中，通过使用&quot;super.&quot;属性/方法的方式，显示的调用父类中声明的属性或方法，通常省略。2.特殊情况下，当子类和父类定义了同名的属性时，用super来调用父类的方法。3.当子类重写了父类的方法，在子类的方法中调用父类中被重写的方法，必须使用super。4.super调用构造器：public Student(int id,String name,String school)&#123;super(id,name);//调用父类的构造器this.school=school;&#125;在子类构造器中用super（形参列表）调用父类中声明的指定的构造器。super声明在子类构造器首行。 11.子类对象实例化全过程：12345671.从结果上看，子类继承父类以后，就获取了父类中声明的属性和方法；创建子类对象，在堆空间中，就会加载所有父类中声明的属性。2.从过程上看，当我们通过子类的构造器创建子类对象时，我们一定会直接或间接地调用其父类构造器，进而调用父类的父类的构造器，知道调用了Object的无参构造器为止。正因为加载过所有的父类结构，所以才可以看到内存中有父类的结构，子类对象才可以进行调用。 12.向下转型123向下转型：使用强制类型转换符Person p=new Student();//此时p并不能调用Student中重写的方法。Student stu=(Student)p;//此时就可以了 13.instanceof关键字：1234stu instanceof p：判断对象stu是否是类p的实例，如果是，返回true，如果不是，返回false。使用情景：为了避免向下转型时出现类型转换异常，我们在向下转型之前，先用instanceof进行判断，如果返回true在进行转型。 14.java.lang.Object1234561.Object类时左右Java类的父类2.如果在声明类时没有指明类的直接父类，默认类的父类为Object。3.Object类中的功能（属性，方法）具有通用性。属性：无。方法：equals()/toString()/getClass()/hashCode（）/clone（）克隆/finalize（）垃圾回收4.Object类只声明了一个空参构造器 15.==和equals()的区别1234567891011121.==可以使用在基本数据类型和引用数据类型变量中2.==如果比较基本数据类型变量，比较的是数据是否相等（不一定类型要相同）。 如果比较引用类型变量，比较的是地址值是否相等，即两个对象是否指向同一个对象实体。1.equals（）是一个方法，不是运算符。2.只适用引用数据类型。3.Object类中equals的定义： public boolean equals(Object obj) &#123; return (this == obj); &#125; String类中定义的equals方法： STring对equals方法进行了重写，如果地址相同，返回true， 否则比较字符串的值是否相同。 16.equals（）方法的重写：1234567public boolean equals(Object o) &#123; if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; Customer customer = (Customer) o; return age == customer.age &amp;&amp; Objects.equals(name, customer.name); &#125; 17.Object类中toString()的使用：123456781.当我们输出一个对象的引用，实际上就是调用当前对象的toString（）2.Object类中对toString（）的定义：public String toString() &#123;return getClass().getName() + &quot;@&quot; + Integer.toHexString(hashCode());&#125;3.向String，Date，File。包装类实际上都重写了Object的toString方法。使得在调用对象的toString方法时，返回实体对象信息。4.自定义类重写toString方法 18.包装类八种数据类型都有对应的包装类 char–&gt;Character 12345678910111213141516171819202122public static void main(String[] args) &#123; /** * 基本数据类型和包装类，String的转换 * jdk5.0新特性：自动拆箱与自动装箱 */ //包装类转换为基本数据类型 Integer a=10; int b=a;//自动拆箱 int c=a.intValue();//int-&gt;integer //基本数据类型转换为包装类 int num1=10; Integer num2=num1;//自动装箱 Integer num3=new Integer(num1); //integer-&gt;int //基本数据类型和包装类与String的转换 int num=1; String s1=num+&quot;&quot;;//int-&gt;String String s2=String.valueOf(num);//int-&gt;String Integer number=10; String s3=String.valueOf(number);//integer-&gt;String int aa=Integer.parseInt(s3);//String-&gt;int Integer bb= Integer.parseInt(s3);//String-&gt;integer&#125; 123456/** * 面试题： * 三目运算符后面两个条件语句对应的类型会在比较前进行统一。 */Object o1=true?new Integer(1):new Double(2.0);System.out.println(o1);//1.0 Integer内部存在一个Integer[]数组，范围-128–+127，如果我们是用自动装箱的方式， 给integer赋值的范围在此范围内直接从数组中取。 19.static关键字123456789101112131415161718192021221.可以用来修饰属性，方法，代码块，内部类2.修饰属性：静态属性（类变量）和非静态属性（实例变量）实例变量：每个对象都有自己的实例变量。静态变量：所有对象共享静态变量。通过一个对象修改，别的对象调用的也是修改过的。public static void setCountry(String country) &#123;Chinese.country = country;&#125;静态变量随着类的加载而加载。静态变量的加载早于对象的创建。由于类只会加载一次，则静态变量在内存中也只存在一份，在方法区的静态域中。3.修饰方法：静态方法_随着类的加载而加载。静态方法中只能调用静态的方法或属性。非静态方法中，既可以调用非静态的方法或属性，也可以调用静态的方法或属性。4.开发中如何确定一个属性要声明为static？所有对象都相同的属性。开发中如何确定一个方法要声明为static？操作静态属性的方法工具类中的方法static实现id自增private static int idadd=1001； 类变量与实例变量的内存解析 20.单例设计模式：12345678910111213141516171819202122232425262728293031323334353637381.采取一定的方法，保证在整个软件系统中，对某个类只能存在一个对象实例。2.饿汉式vs懒汉式实现：//饿汉式public class Test1 &#123; //私有的构造器 private Test1()&#123; &#125; //私有的创建对象 private static Test1 test=new Test1(); //公有的方法供外部调用 public static Test1 getTest1()&#123; return test; &#125;&#125;//懒汉式public class Test1 &#123; //私有的构造器 private Test1()&#123; &#125; //私有的创建对象 private static Test1 test=null; //公有的方法供外部调用 public static Test1 getTest1()&#123; if(test==null)&#123; test=new Test1(); &#125; return test; &#125;&#125;3.懒汉式和饿汉式的对比：饿汉式占用内存，线程安全的。懒汉式好处：延迟对象的创建，线程不安全。4.应用场景：网站计数器，应用程序的日志应用，数据库连接池，Application。main（）方法的使用说明也是一个普通的静态方法可以做输入 21.代码块：和属性赋值顺序完结篇12345678910作用：用来初始化对象。只能用static来修饰分类：静态代码块，非静态代码块非静态能调用静态的属性方法，静态的不能掉用非静态的属性方法。1.静态代码块随着类的加载而执行一共只执行一次，因此可以初始化类的信息。2.非静态代码块随着对象的创建而执行每次创建对象都会执行一次，因此可以创建对象时初始化对象。 12345属性赋值顺序完结篇1）默认初始化2）显示初始化/在代码块中赋值3）构造器初始化4）set方法 22.final关键字123456789101.final修饰类和方法用final修饰的类不能被继承，称为最终类。eg：String，System，StringBuffer用final修饰的方法不能被重写。2.final修饰变量此时的变量就称为常量。1）修饰属性显式初始化，代码块中初始化。2）修饰局部变量该局部变量的值不能被再次修改 23.抽象类与抽象方法12345678910111213141516171819202122abstract关键字的使用1.abstract抽象的2.可以用来修饰的结构：类和方法3.abstract修饰类：抽象类1）此类不能实例化2）抽象类中一定有构造器，便于子类实例化时调用。3）开发中，都会提供抽象类的子类，让子类对象实例化，完成相关操作。4.abstract修饰方法：抽象方法public abstract void test（）；包含抽象方法的类一定是抽象类，抽象类不一定包含抽象方法。如果继承了抽象类，必须继承抽象类的抽象方法。5.注意1）abstract不能用来修饰：属性，构造器。2）abstract不能用来修饰私有方法，静态方法，final方法，final的类。创建抽象类的匿名子类对象Person p=new Person()&#123;//Person是一个抽象类public void eat()&#123;&#125;public void walk()&#123;&#125;&#125;; 24.接口：interface12345678910111213141516171819201.接口的使用用interface来定义2.Java中，接口和类是并列的两个结构。3.如何定义接口，定义接口的成员 1)jdk7以前，只能定义全局常量和抽象方法 全局常量：public static final的，但是书写时，可以省略不写 抽象方法：public abstract的，但是书写时，可以省略不写。 2）jdk8：除了定义全局常量和抽象方法外，还可以定义静态方法，默认方法。4.接口中不能定义构造器，接口不能实例化。5.Java中，通过类去实现接口。implements6.Java中允许实现多个接口。7.接口与接口之间可以多继承8.接口的实现体现了多态性。面试题：接口与抽象类的比较： 1.抽象类：通过extends来继承，只能继承一个抽象类， 抽象类中一定有构造方法，创建子类对象时被调用。 抽象类中不光有抽象方法，还可以有其他方法。 2.接口：接口通过implements来实现，允许实现多个接口， 接口中不存在构造方法，接口中只能声明全局常量和抽象方法， jdk&#x27;8.0以后，还可以定义静态方法和默认方法。 3.不能实例化，都可以被继承。 接口的应用：代理模式123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657/** * 代理模式：将两者都要实现的行为封装在接口中，被代理对象和代理对象都继承该接口 * 代理对象中声明被代理对象属性，并创建拥该属性的构造器，通过创建代理对象完成 * 被代理对象的方法。 */class TEST&#123; public static void main(String[] args) &#123; Zhuli zhuli=new Zhuli(new Star()); zhuli.sing(); zhuli.buyThing(); zhuli.getMoney(); &#125;&#125;public class Star implements Proxy &#123;//被代理对象 @Override public void sing() &#123; System.out.println(&quot;明星唱歌&quot;); &#125; @Override public void getMoney() &#123; &#125; @Override public void buyThing() &#123; &#125;&#125;interface Proxy &#123; public void sing(); public void getMoney(); public void buyThing();&#125;class Zhuli implements Proxy&#123;//代理对象private Star star;//声明被代理对象 public Zhuli(Star star)&#123;//存在被代理对象的带参构造器 this.star=star; &#125; @Override public void sing() &#123;star.sing();//被代理对象完成 &#125; @Override public void getMoney() &#123; System.out.println(&quot;助理替歌手收钱&quot;); &#125; @Override public void buyThing() &#123; System.out.println(&quot;助理替歌手买东西&quot;); &#125;&#125; 工厂模式实现创建者与调用者的分离，将创建对象的具体过程隔离起来。 123456789101112131415161718192021222324public interface Java8 &#123; /** * 接口中定义的静态方法只能通过接口来调用 */ public static void test1()&#123; System.out.println(&quot;***&quot;); &#125;; /** * 通过实现类的对象可以调用/重写接口中的默认方法 */ public default void test2()&#123; System.out.println(&quot;***&quot;); &#125; /** * 如果一个类继承的父类和实现的接口存在同名同参数的方法， * 子类在没有重写这个方法的前提下，默认调用父类的方法 */ /** * 如果一个类实现的多个接口中存在同名同参数的方法， * 子类在没有重写这个方法的前提下，会报错，接口冲突。 * 如果重写的方法想要调用其中的一个：接口.super.method */&#125; 25.内部类的使用：1.Java中允许将一个类声明在另一个类的内部 2.内部类的分类：成员内部类（静态，非静态）vs局部内部类（方法，代码块，构造器内） 3.成员内部类： 1234567891011121314151617181920212223242526272829303132333435363738394041class Demo&#123; public static void main(String[] args) &#123; /** * 创建内部类实例 */ Person.Dog dog=new Person.Dog();//静态内部类 Person p=new Person();//非静态内部类 Person.Bird bird=p.new Bird(); &#125;&#125;class Person &#123; private String name; public void eat()&#123; System.out.println(&quot;吃饭&quot;); &#125; /** * 内部类可以被final修饰，表示此类不能被继承 * 内部可以定义属性方法构造器 * 可以被abstract修饰 */ static class Dog&#123; //eat();不能调用外部的eat方法 &#125; class Bird&#123; public Bird()&#123; &#125; /** * 调用外部成员的方法 */ public void sing()&#123; eat(); name=&quot;bird&quot;; &#125; &#125;&#125; 123456面试题：创建静态内部类对象和非静态内部类对象：//静态内部类对象Person.Dog dog=new Person.Dog();//非静态内部类对象Person p=new Person();Person.Bird bird=new p.bird(); 五，异常处理1.异常的体系结构1234567异常：在Java中，程序执行发生的不正常情况称为异常。java.long.Throwable1.java.long.Error,:一般不编写针对性代码进行处理2.java.long,Exception：可以进行异常的处理1）编译时异常：IOException,FileNotFindException,ClassNotFindException2)运行时异常：NullPointException,ArrayIndexOutOfBoundsException,ClassCastException,NumberFormatException,InputMismatchException,ArithmeticException 2.异常处理机制一：123456789101112131415try-catch-finally一旦抛出异常，其后的代码就不再执行。try &#123; //可能会出现异常的代码&#125; catch (异常处理类型1 变量名1) &#123;e.printStackTrace&#125; catch (异常处理类型2 变量名2) &#123;e.printStackTrace&#125; finally &#123; //最终一定要执行的代码&#125;编译时异常和运行时异常的不同处理开发时运行时异常不需要try-catch编译时异常用try-catch来解决 3.异常处理机制二：Throws +异常类型12345678910111213“throws+异常类型”写在方法声明处，指明方法执行时，可能会抛出的异常。一旦当方法体执行时，出现异常，仍会在异常代码处生成一个异常类的对象，此对象满足throws的异常类型时就会抛出，异常代码后续的代码，就不要执行。try-catch真正的将异常处理掉了，而throws只是将异常抛给了方法的调用者，并没有真正的处理掉。重写方法异常抛出的规则子类重写的方法抛出的异常类型不大于父类被重写的方法抛出的异常类型开发中如何选择try-catch还是throws1.如果父类被重写的方法没有抛出异常，则子类重写的方法中的异常只能用try-catch2.执行的方法a中，先后又调用了另外的几个方法，这几个方法是递进关系执行的，我们建议这几个方法使用throws的方法进行处理，而方法a可以用try-catch进行处理。 4.手动生成一个异常对象并抛出：throw1234567891011121314151617181920212223242526public class MyException &#123; public static void main(String[] args) &#123; person p=new person(); try &#123; p.setAge(-1001); &#125; catch (Exception e) &#123; System.out.println(e.getMessage()); &#125; &#125;&#125;class person &#123; private int age; public person()&#123; &#125; public void setAge(int age) throws Exception&#123; if (age&gt;0)&#123; this.age=age; &#125;else&#123; throw new Exception(&quot;年龄不能小于0！&quot;); &#125; &#125; public int getAge()&#123; return age; &#125;&#125; 5.用户自定义异常：12345678910111213141516/** * 自定义异常 * 1.继承于现的异常结构，RuntimeException,Exception * 2.提供全局常量， static final long serialVersionUID * 3.提供重载的构造器 */public class TestException extends IOException &#123; static final long serialVersionUID=-7034897190745766939L; public TestException()&#123; &#125; public TestException(String msg)&#123; super(msg); &#125;&#125; 六，多线程1.概述1）概念1234567程序：为完成特定任务，用某种语言编写的一组指令的集合。一段静态的代码，静态对象。进程：程序的一次执行过程，或是正在运行的一个程序。是一个动态的过程，有他本身的生命周期。独立的方法区和堆空间线程：一个程序内部的执行路径。独立的计数器和栈 1234567单核cpu和多核cpu假的多线程，多个线程交替进行。并行和并发并行：多个cpu同时执行多个任务。并发：一个cpu执行多个任务。并行：传输的数据8位一送出去串行：传输的数据1位1送出去 2）优点1231.提高应用程序的响应，增强图形化界面用户的体验。2.提高cpu利用率3.改善程序结构，将复杂的程序分为多个线程。 3）何时需要1231.程序需要同时执行多个任务。2.程序需要实现一些需要等待的任务时，用户输入，文件读写，网络操作，搜索。3.需要一些后台运行的程序。 2.创建线程的方式一：继承Thread类123456789101112131415161718/** * 多线程的创建：继承Thread类 * 重写run方法--&gt;将线程执行的操作声明在run方法 * 创建子类的对象，通过此对象调用start方法 */public class MyThread extends Thread &#123; public void run()&#123; for (int i = 0; i &lt;100 ; i++) &#123; System.out.println(this.currentThread().getName()+&quot; &quot;+i); &#125; &#125;&#125;class Test1&#123; public static void main(String[] args) &#123; MyThread m1=new MyThread(); m1.start(); &#125;&#125; 1）线程的常用方法：123456789*start():启动当前线程：调用当前线程的run方法* run();通常需要重写Thread类的run方法，将创建线程要执行的操作声明在此方法* currentThread():静态方法，返回执行当前代码的线程。* getName():获取当前线程名* setName():设置当前线程的名字* yield():释放当前cpu执行权* join()：在线程a中调用线程b的join方法，a进入阻塞状态，直到b执行完以后，a才结束阻塞状态。优先权* sleep():挂起一会儿，单位ms* stop（）：强制终止，死亡 2）线程优先级：123456781.MAX_PRIORITY:10MIN_PRIORITY:1NORM_PRIORITY:52.如何获取和设置当前线程优先级：m1.setPriority();m1.getPriority()；优先级高并不一定代表一定先执行，只是概率大一点。 3）案例：多窗口卖票123456789101112131415161718192021222324/** * 继承Thread方法实现多窗口卖票，存在线程安全问题 */public class ThreadTest1 &#123; public static void main(String[] args) &#123; Window w1=new Window(); Window w2=new Window(); w1.start(); w2.start(); &#125;&#125;class Window extends Thread&#123; private static int piao=100; public void run()&#123; while (true)&#123; if (piao&gt;0)&#123; System.out.println(currentThread().getName()+&quot; &quot;+piao); piao--; &#125;else&#123; break; &#125; &#125; &#125;&#125; 3.创建多线程的方式二：实现runnerable接口的方式1234567891011121314151617181920212223/** * 通过实现runnable接口创建多线程 * 1.创建类a实现runnable接口 * 2.类a重写runnable的run（方法 * 3.在调用方法里创建a的对象； * 4创建Thread对象b并传入a的对象 * 5.b.start（； */public class RunnableTest2 &#123; public static void main(String[] args) &#123; myRunnable m1=new myRunnable(); Thread t1=new Thread(m1); t1.start(); &#125;&#125;class myRunnable implements Runnable&#123; public void run()&#123; for (int i = 0; i &lt;10 ; i++) &#123; System.out.println(Thread.currentThread().getName()+&quot; &quot;+i); &#125; &#125;&#125; 1）案例：多窗口卖票1234567891011121314151617181920212223242526272829303132** * runnable方式实现多窗口卖票，存在线程安全问题 */public class RunnableTest &#123; public static void main(String[] args) &#123; MyRunnable m1=new MyRunnable(); Thread t1=new Thread(m1); Thread t2=new Thread(m1); Thread t3=new Thread(m1); t1.start(); t2.start(); t3.start(); &#125;&#125;class MyRunnable implements Runnable&#123; private int piao; /** * 此处不用static修饰的原因，因为上面类中的方法仅仅new了一个此类的对象， * 所以个Thread对象实际上只是使用了同一个对象的票。 */ public void run()&#123; while(true)&#123; if (piao&gt;0)&#123; System.out.println(Thread.currentThread().getName()+&quot; :&quot;+piao); piao--; &#125;else&#123; break; &#125; &#125; &#125;&#125; 2）线程的生命周期1234567线程的生命周期Thread.state();1.新建 new2.就绪 start3.运行 run4.阻塞 join,sleep,等待同步锁，wait，（过时的挂起）5.死亡 stop 4.线程的同步12345678910111213141516171819202122232425/** * runnable方式实现多窗口卖票，存在线程安全问题 * 1.买票过程中出现重票错票 * 2.问题描述：当某个线程操作车票的过程中，尚未操作完成，其他线程参与进来，也操作车票。 * 3.如何解决：当一个线程在操作共享数据的时候，其他线程不能参与进来，直到线程a操作完，其他线程才能参与进来， * 即使线程a出现了阻塞，也不能被改变。 * 4.在Java中我们通过同步机制，来解决线程安全问题。 * 方式一：同步代码块 * synchronized (同步监视器)&#123; * 需要被同步的代码 *不能包多了，也不能包少了 * &#125; * 说明：1.操作共享数据的代码，就是需要被同步的代码。 * 2.共享数据：多个线程共同操作的数据。 * 3.同步监视器：俗称锁。任何一个类的对象都可以来充当锁。 * 要求：多个线程必须公用同一把锁。 * 补充：在实现runnable接口创建的多线程方式中，我么可以考虑使用this关键字充当锁， * 在继承Thread类创建的多线程方式中，我们可以考虑使用当前类.class的方式充当锁。 * 方式二：同步方法 *如果操作共享数据的代码，完整的声明在一个方法中，我们不妨将此方法声明为同步的。 * 1.同步方法仍然涉及到同步监视器，只是不需要我i们显示的声明 * 2.非静态的同步方法，同步监视器是this，静态方法的同步监视器是类的本身。 * 5.同步的方式：解决了安全问题--好处 * 操作同步代码时，只能有一个线程参与，其他线程等待，相当于是一个单线程的过程，效率低。--缺点 */ 通过继承Thread类实现多窗口卖票12345678910111213141516171819202122232425262728293031323334353637/** * 使用同步代码块方式解决线程安全问题 * 通过继承Thread类实现多窗口卖票 */public class TreadTest1 &#123; public static void main(String[] args) &#123; window1 w1=new window1(); window1 w2=new window1(); w1.setName(&quot;窗口一&quot;); w2.setName(&quot;窗口二&quot;); w1.start(); w2.start(); &#125;&#125;class window1 extends Thread &#123; private static int ticket=100; @Override public void run() &#123; while (true)&#123; synchronized (window1.class)&#123; if (ticket&gt;0)&#123; try &#123; sleep(10); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(getName()+&quot;卖出了一张票：&quot;+ticket); ticket--; &#125;else&#123; break; &#125; &#125; &#125; &#125;&#125; 继承Runnable方式实现多窗口卖票1234567891011121314151617181920212223242526272829303132/** * 使用同步代码块解决线程安全问题 * 继承Runnable方式实现多窗口卖票 */public class RunnableTest1 &#123; public static void main(String[] args) &#123; window2 w1=new window2(); Thread t1=new Thread(w1); Thread t2=new Thread(w1); t1.setName(&quot;窗口一：&quot;); t1.setName(&quot;窗口二：&quot;); t1.start(); t2.start(); &#125;&#125;class window2 implements Runnable&#123;private int ticket=100; @Override public void run() &#123; while (true)&#123; synchronized (this)&#123; if (ticket&gt;0)&#123; System.out.println(Thread.currentThread().getName()+&quot;卖出了票：&quot;+ticket); ticket--; &#125;else&#123; break; &#125; &#125; &#125; &#125;&#125; 通过继承Thread类来创建多线程123456789101112131415161718192021222324252627282930313233343536/** * @author 尹会东 * @create 2020 -01 - 21 - 16:41 *//** * 通过同步方法解决线程安全问题 * 通过继承Thread类来创建多线程 */public class ThreadTest2 &#123; public static void main(String[] args) &#123; window3 t1=new window3(); window3 t2=new window3(); t1.setName(&quot;窗口一：&quot;); t2.setName(&quot;窗口二：&quot;); t1.start(); t2.start(); &#125;&#125;class window3 extends Thread&#123; private static int ticket=100; @Override public void run() &#123; while (true)&#123; show(); &#125; &#125; public static synchronized void show()&#123;//加static：此时的锁相当于当前类的对象 if (ticket&gt;0)&#123; System.out.println(Thread.currentThread().getName()+&quot;卖出了票：&quot;+ticket); ticket--; &#125; &#125;&#125; 通过继承runnable方式来创建多线程12345678910111213141516171819202122232425262728293031/** * 使用同步方法解决线程安全问题 * 通过继承runnable方式来创建多线程 */public class RunnableTest2 &#123; public static void main(String[] args) &#123; window4 w=new window4(); Thread t1=new Thread(w); Thread t2=new Thread(w); t1.setName(&quot;窗口一：&quot;); t2.setName(&quot;窗口二：&quot;); t1.start(); t2.start(); &#125;&#125;class window4 implements Runnable&#123; private int ticket=100; @Override public void run() &#123; while (true)&#123; show(); &#125; &#125; public synchronized void show()&#123; if (ticket&gt;0)&#123; System.out.println(Thread.currentThread().getName()+&quot;卖出了：&quot;+ticket); ticket--; &#125; &#125;&#125; 5.线程安全的单例模式之懒汉式1）通过同步代码块解决懒汉式单例设计模式的线程安全问题1234567891011121314151617181920212223242526/** * 通过同步代码块解决懒汉式单例设计模式的线程安全问题 */public class Thread1 &#123; private static Thread1 instance=null; public Thread1 getInstance()&#123; //效率低// synchronized (Thread1.class) &#123;// if(instance==null)&#123;// instance=new Thread1();// &#125;// return instance;// &#125; /** * 先判断是否为空，如果为空，进行锁住，否则可以直接获取该对象。 */ if (instance==null)&#123; synchronized (Thread1.class)&#123; if (instance==null)&#123; instance=new Thread1(); &#125; &#125; &#125; return instance; &#125;&#125; 2）通过同步方法解决懒汉式单例设计模式的线程安全问题1234567891011121314/** * 通过同步方法解决懒汉式单例设计模式的线程安全问题 */public class Thread2 &#123; private static Thread2 instance = null; public synchronized Thread2 getInstance() &#123; if (instance == null) &#123; instance = new Thread2(); &#125; return instance; &#125;&#125; 6.死锁的问题：12345678/** * 死锁问题 * 1.死锁的理解：不同的线程分别占用对方的同步资源不放弃， * 都在等待对方放弃自己需要的同步资源，就形成了线程的死锁 * 2.说明： * 1）出现死锁后，不会出现异常，不会出现提示，只是所有的线程都处于阻塞状态，无法继续。 * 2）我们使用同步时要避免出现死锁。 */ 123456789101112131415161718192021222324252627282930public class DeadTest &#123; public static void main(String[] args) &#123; StringBuffer s1=new StringBuffer(); StringBuffer s2=new StringBuffer(); new Thread()&#123; public void run()&#123; synchronized (s1)&#123; s1.append(&quot;123&quot;); s2.append(&quot;666&quot;); synchronized (s2)&#123; s1.append(&quot;456&quot;); s2.append(&quot;888&quot;); &#125; &#125; &#125; &#125;.start(); new Thread()&#123; public void run()&#123; synchronized (s2)&#123; s1.append(&quot;123&quot;); s2.append(&quot;666&quot;); synchronized (s1)&#123; s1.append(&quot;456&quot;); s2.append(&quot;888&quot;); &#125; &#125; &#125; &#125;.start(); &#125;&#125; 7.Lock锁方式解决线程安全问题：12345678/** * 解决线程安全的方式三：Lock锁--jdk5.0新特性 * 面试题：synchronized与lock的异同： * 同：二者都可以解决线程安全问题 * 异：synchronized机制在执行完相应的同步代码自动解锁（释放同步监视器），lock需要手动启动同步和解锁。 * @author 尹会东 * @create 2020 -01 - 21 - 12:09 */ 123456789101112131415161718192021222324252627282930313233public class LockTest &#123; public static void main(String[] args) &#123; testlock t=new testlock(); Thread t1=new Thread(t); Thread t2=new Thread(t); t1.setName(&quot;窗口一！&quot;); t2.setName(&quot;窗口二：&quot;); t1.start(); t2.start(); &#125;&#125;class testlock implements Runnable &#123; private int ticket = 100; private ReentrantLock lock = new ReentrantLock(); @Override public void run() &#123; while (true) &#123; try &#123; lock.lock(); if (ticket &gt; 0) &#123; System.out.println(Thread.currentThread().getName() + &quot;卖出了票：&quot; + ticket); ticket--; &#125; else &#123; break; &#125; &#125; finally &#123; lock.unlock(); &#125; &#125; &#125;&#125; 练习题：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/** * 银行一个账户，有两个储户分别向同一个账户存300元，每次存一百，分三次，每次存完打印账户余额。 * 分析： * 1.是否是多线程问题？是，两个储户线程 * 2.是否共想数据？，账户。 * 3.是否线程安全问题？ * 4.考虑如何解决线程安全问题？同步机制：种方式 */public class Bank &#123; public static void main(String[] args) &#123; Account account=new Account(); Customer c1=new Customer(account); Customer c2=new Customer(account); c1.setName(&quot;尹会东&quot;); c2.setName(&quot;张贝贝&quot;); c1.start(); c2.start(); &#125;&#125;class Account &#123; private double balance=0; public Account() &#123; &#125; public Account(double balance) &#123; this.balance = balance; &#125; public synchronized void save(double money) &#123; try &#123; Thread.sleep(1000); balance+=money; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName()+&quot;存钱成功&quot;+balance); &#125;&#125;class Customer extends Thread&#123; private Account account; public Customer(Account account)&#123; this.account=account; &#125; public void run()&#123; for (int i = 0; i &lt;3 ; i++) &#123; account.save(1000); &#125; &#125;&#125; 8.线程的通信：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/** * 线程通信的例子：使用两个线程交替打印1-100 * 涉及到的个方法： * wait():一旦执行此方法，当前线程进入阻塞状态，并释放监视器 * notify():一旦执行此方法，就会唤醒被wait的一个线程，如果有多个线程wait&#x27;，就会唤醒优先级高的那个。 * notifyAll():会唤醒所被wait的线程。 * 说明： * 1.使用前提：只能写在同步方法或同步代码块里面 * 2.方法的调用者必须是同步代码块或同步方法中的同步监视器，否则会出现异常。 * 3.这个方法是定义在Object类中的。 * 面试题：sleep和wait的异同： * 同：都可以让当前线程进入阻塞状态 * 异：1两个方法声明位置不一样：Thread类中声明sleep，Object类中声明wait * 2调用的范围或者要求是不一样的：sleep随时可以调用，wait只能在同步代码块或同步方法中调用。 * 3关于是否释放同步监视器：如果两个方法都使用在同步代码块或同步方法中，wait会释放同步监视器，sleep不会。 * */public class Number implements Runnable &#123; private int num=100; @Override public synchronized void run() &#123; while (true)&#123; notify();//唤醒 if (num&gt;0)&#123; System.out.println(Thread.currentThread().getName()+&quot;打印了：&quot;+num); num--; try &#123; wait();//使得调用wait方法的线程进入阻塞状态,此时会释放锁 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;else&#123; break; &#125; &#125; &#125;&#125;class test&#123; public static void main(String[] args) &#123; Number n=new Number(); Thread t1=new Thread(n); Thread t2=new Thread(n); t1.setName(&quot;线程一&quot;); t2.setName(&quot;线程二&quot;); t1.start(); t2.start(); &#125;&#125; 经典例题：生产者和消费者问题：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192/** * 生产者消费者问题：线程通信的应用 * 分析： * 1.多线程问题，生产者，消费者 * 2.存在共享数据，产品/店员 * 3.处理线程安全问题：同步机制 * 4.线程通信：产品超过20停止生产，产品低于0停止购买 */public class ProductTest &#123; public static void main(String[] args) &#123; Shop shop=new Shop(); Producer p=new Producer(shop); Customers c=new Customers(shop); Thread t1=new Thread(p); Thread t2=new Thread(c); Thread t3=new Thread(c); t1.setName(&quot;生产者&quot;); t2.setName(&quot;消费者一&quot;); t3.setName(&quot;消费者二&quot;); t1.start(); t2.start(); t3.start(); &#125;&#125;class Producer implements Runnable&#123; private Shop shop; public Producer(Shop shop)&#123; this.shop=shop; &#125; @Override public void run() &#123; System.out.println(&quot;生产者开始生产东西&quot;); while (true)&#123; try &#123; sleep(50); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; shop.in(); &#125; &#125;&#125;class Customers implements Runnable&#123; private Shop shop; public Customers(Shop shop)&#123; this.shop=shop; &#125; @Override public void run() &#123; System.out.println(&quot;消费者开始购买东西&quot;); while (true)&#123; try &#123; sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; shop.out(); &#125; &#125;&#125;class Shop&#123; private int thing=0; public synchronized void in() &#123; if (thing&lt;20)&#123; thing++; System.out.println(Thread.currentThread().getName()+&quot;生产了东西&quot;+thing); notify(); &#125;else&#123; try &#123; wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; public synchronized void out()&#123; if (thing&gt;0) &#123; System.out.println(Thread.currentThread().getName()+&quot;购买了东西&quot;+thing); thing--; notify(); &#125;else&#123; try &#123; wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 9.创建多线程的方式三：实现Callable接口123456789101112131415161718192021222324252627282930313233343536373839404142/** * 创建线程的方式：实现Callable接口的方式 * 1.创建一个实现Callable接口的实现类(重写call方法)的对象 * 2.创建一个FutureTask对象并传入Callable接口的实现类的对象 * 3.创建一个Thread类对象并传入FutureTask对象 * 4.get()方法的返回值就是FutureTask构造器参数callable实现类重写的call（的返回值。 * 面试题：如何理解实现Callable接口创建多线程比实现runnable接口创建多线程强大？ * 1call（方法可以返回值 * 2call（方法可以抛出异常被外面的操作捕获并获取异常信息‘ * 3callable接口支持泛型 * @author 尹会东 * @create 2020 -01 - 21 - 14:00 */public class Demo1 &#123; public static void main(String[] args) &#123; Share share = new Share(); new Thread(new FutureTask(()-&gt;&#123; for (int i = 0; i &lt; 100; i++) share.print(); return null;&#125;),&quot;AA&quot;).start(); new Thread(new FutureTask(()-&gt;&#123; for (int i = 0; i &lt; 100; i++) share.print(); return null;&#125;),&quot;BB&quot;).start(); &#125;&#125;class Share &#123; private Integer num=0; private ReentrantLock lock=new ReentrantLock(); private Condition cd=lock.newCondition(); public void print()&#123; try &#123; lock.lock(); while (num&lt;100)&#123; cd.signal(); System.out.println(Thread.currentThread().getName()+&quot;打印了一张票：&quot;+ ++num+&quot;还剩&quot;+ (100-num) +&quot;张票。&quot;); cd.await(); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125;&#125; 10.创建多线程的方式四：使用线程池1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * 创建线程的方式四：使用线程池 * 使用线程池的好处： * 1.提高响应速度 * 2.降低资源消耗 * 3.便于线程管理 * 属性： * corePoolSize:核心池的大小 * maximumPoolSize:最大线程数 * keepAliveTime:线程没任务时最多保持多长时间后终止 * * @author 尹会东 * @create 2020 -01 - 21 - 14:43 */class num implements Runnable &#123; private int num = 100; @Override public void run() &#123; while (true) &#123; synchronized (com.qtguigu.fuxi.num.class) &#123; if (num &gt; 0) &#123; System.out.println(Thread.currentThread().getName() + num); num--; &#125; else &#123; break; &#125; &#125; &#125; &#125;&#125;public class PoolTest &#123; public static void main(String[] args) &#123; ExecutorService service = Executors.newFixedThreadPool(10); num n1 = new num(); //如果需要设置属性需要把service类型转换为ThreadPoolExecutor //ThreadPoolExecutor e = (ThreadPoolExecutor) service; service.execute(n1);//2.1执行：适合适用于runnable（传入一个实现runnable接口的对象 // service.submit();//2.1提交：适合适用于callable（传入一个实现callable接口的对象 service.shutdown();//3.关闭线程 &#125;&#125; 七，常用类1.String1）String内存解析 2）String分析123456789101112/*** 1.声明为final，不可被继承。* 2.实现了java.io.Serializable接口，表示字符串是支持序列化的。* 3.实现了Comparable&lt;String&gt;接口，表示String可以比较大小。* 4.String内部定义了final char[] value用于存储字符串数据* 5.String代表了不可变的字符序列，简称：不可变性。* 1)当对字符串重新赋值，需要重新指定内存区域，不能再原有的地址重新赋值。* 2)当对现的字符串进行连接操作时，需要重新指定内存区域，不能再原有的地址重新赋值。* 3)当调用String的replace（方法修改字符或字符串时，也必须重新指定内存区域进行赋值。* 6.通过字面量的方式给字符串赋值，此时的字符串值声明在字符串常量池中* 7.字符串常量池不会存储相同内容的字符串的。*/ 123456789101112String s1=&quot;abc&quot;;//字面量的定义方式String s2=&quot;abc&quot;;//s1=&quot;hello&quot;;System.out.println(s1==s2);//比较s1和s2的地址值System.out.println(&quot;**********************************&quot;);String s3=&quot;abc&quot;;s3+=&quot;def&quot;;System.out.println(s1==s3);//falseSystem.out.println(&quot;**********************************&quot;);String s4=&quot;abc&quot;;String s5=s4.replace(&#x27;a&#x27;,&#x27;m&#x27;);System.out.println(s4+&quot; &quot;+s5);//abc mbc 3）String实例化123456789/*** String的实例化方式：* 1.通过字面量定义的方式：*数据声明在方法区对应的字符串常量池中* 2.通过new+构造器的方式：* 保存的地址值在堆空间中* 面试题：String s3=new String(&quot;java&quot;);在内存中创建了几个对象？* 两个：一个是堆空间中new的结构，一个是char【】数组对应的常量池的数据：java*/ 123456789//此时的s1和s2数据声明在方法区对应的字符串常量池中String s1=&quot;java&quot;;String s2=&quot;java&quot;;//此时s3和s4保存的地址值在堆空间中String s3=new String(&quot;java&quot;);String s4=new String(&quot;java&quot;);System.out.println(s1==s2);//trueSystem.out.println(s1==s3);//falseSystem.out.println(s3==s4);//false 4）图解两种创建字符串方式的区别 5）图解字符串的存储 6）图解字符串对象的存储 7）String不同拼接操作的对比：12345678910111213141516171819202122/** 1.常量与常量的拼接结果在常量池，且常量池中不会存在相同内容的常量。* 2.只要其中一个是变量，结果就在堆中。* 3.String s8=s5.intern();此时的返回值得到的s8是使用的常量池中已经存在的javahadoop* 4.intern的返回值在方法去的常量池*/String s1=&quot;java&quot;;//常量池String s2=&quot;hadoop&quot;;//常量池String s3=&quot;javahadoop&quot;;//常量池String s4=&quot;java&quot;+&quot;hadoop&quot;;//常量池String s5=s1+&quot;hadoop&quot;;//堆空间String s6=&quot;java&quot;+s2;//堆空间String s7=s1+s2;//堆空间String s8=s5.intern();//常量池System.out.println(s3==s4);//trueSystem.out.println(s4==s5);//falseSystem.out.println(s3==s5);//falseSystem.out.println(s5==s6);//falseSystem.out.println(s3==s7);//falseSystem.out.println(s5==s7);//falseSystem.out.println(s6==s7);//falseSystem.out.println(s8==s3);//true 面试题1234567891011121314151617181920/** * String的一道面试题： * str传递给change方法的形参，只是形参的地址也指向了str所指向的地址，形参改变的话，因为是String * 类型，不可变性，所以形参只是重新开辟一一个value=”test ok&quot;的地址。 * 而基本数据类型，改变他的值，就是把原地址的值给改变了。 */public class StringTest4 &#123; String str=new String(&quot;good&quot;); char []ch=&#123;&#x27;t&#x27;,&#x27;e&#x27;,&#x27;s&#x27;,&#x27;t&#x27;&#125;; public static void main(String[] args) &#123; StringTest4 ex=new StringTest4(); ex.change(ex.str,ex.ch); System.out.println(ex.str+&quot; &quot;+ex.ch);//good best &#125; private void change(String str, char[] ch) &#123; str=&quot;test ok&quot;; ch[0]=&#x27;b&#x27;; &#125;&#125; 8）String的常用方法：123456789101112131415public static void main(String[] args) &#123; String s1=&quot;hello&quot;; String s2=&quot;world&quot;; System.out.println(s1.length());//返回数组的长度 5 System.out.println(s1.charAt(3));//返回指定索引的字符 l System.out.println(s1.isEmpty());//判空 false System.out.println(s1.toLowerCase());//将String中所字符转换为小写 hello System.out.println(s1.toUpperCase());//将String中所字符转换为大写 HELLO System.out.println(s1.trim());//返回字符串副本，忽略前后的空白 hello System.out.println(s1.equals(s2));//比较字符串内容是否相同 false System.out.println(s1.equalsIgnoreCase(s2));//忽略大小写的比较字符串是否相同 false System.out.println(s1.compareTo(s2));//比较两个字符串的大小 -15 System.out.println(s1.substring(2));//从指定位置开始截取 llo System.out.println(s1.substring(2,4));//截取指定位置的字符串 ll&#125; 123456789101112public static void main(String[] args) &#123; String s1=&quot;helloworld&quot;; String s2=&quot;HelloWorld&quot;; System.out.println(s1.endsWith(&quot;ld&quot;));//是否已指定字符串结尾 true System.out.println(s1.startsWith(&quot;he&quot;));//是否以指定的字符串开始 true System.out.println(s1.startsWith(&quot;wo&quot;,5));//是否在指定位置以这个字符串开始 //true System.out.println(s1.contains(&quot;owo&quot;));//判断是否包含这个字符串 true System.out.println(s1.indexOf(&quot;lo&quot;));//指定字符串所在的位置 3，没的话返回-1 System.out.println(s1.indexOf(&quot;lo&quot;,5));//从指定位置找指定的字符串 -1 System.out.println(s1.lastIndexOf(&quot;lo&quot;));//从后往前找 3 System.out.println(s1.replace(&#x27;l&#x27;,&#x27;y&#x27;));//替换字符 System.out.println(s1.replace(&quot;hello&quot;,&quot;666&quot;));//替换字符串 9）String和其他类型之间的转换123456789101112131415161718192021222324/** * String和其他类型之间的转换 */public class StringTest7 &#123; public static void main(String[] args) &#123; //String与基本数据类型，包装类的转换 String str1=&quot;123&quot;; int num=Integer.parseInt(str1);//String--&gt;int/integer String str2=String.valueOf(num);//int/integer--&gt;String //String与char[]之间的转换 String str3=&quot;123456&quot;; char[] array = str3.toCharArray();//String--&gt;char[]数组 String str4 = new String(array);//char[]--&gt;String //String和字节数组之间的转换byte[] String str5=&quot;123456&quot;; byte [] bytes= str5.getBytes(); //String--&gt;byte[] 编码 String str6 = new String(bytes);//byte[]--&gt;String 解码 /** * 编码：字符串转换成字节。 * 解码：字节转换为字符串。 */ &#125;&#125; 10）String的四道面试题：12345678910111213141516171819/** * 模拟一个trim方法，去除字符串两端的空格 */public static void main(String[] args) &#123; String str1 = &quot; 123456 &quot;; char[] array = str1.toCharArray(); char[] array2 = new char[str1.length()]; int num = 0; for (int i = 0; i &lt; array.length; i++) &#123; if (array[i] == &#x27; &#x27;) &#123; continue; &#125; else &#123; array2[num] = array[i]; num++; &#125; &#125; String string = new String(array2); System.out.println(string.substring(0, num));&#125; 1234567891011121314151617/** * 交换指定位置字符串 * @param args */public static void main(String[] args) &#123; int start=2; int end=5; String str1=&quot;0123456789&quot;; char[] array = str1.toCharArray(); for (int i = start; i &lt;(start+end)/2+1 ; i++) &#123; char a=array[i]; array[i]=array[end+start-i]; array[end+start-i]=a; &#125; String s = new String(array); System.out.println(s);&#125; 1234567891011121314/** * 获取一个字符串在另一个字符串中出现的次数 */public static void main(String[] args) &#123; String str1=&quot;我爱中国，中国共产党万岁！&quot;; String str2=&quot;中国&quot;; int sum=0; while (str1.contains(str2))&#123; sum++; int num=str1.indexOf(str2); str1=str1.substring(num+str2.length()); &#125; System.out.println(sum);&#125; 12345678910/** * 对字符串中的字符进行自然排序 */public static void main(String[] args) &#123; String str1=&quot;9876543210&quot;; char[] array = str1.toCharArray(); Arrays.sort(array); String str2 = new String(array); System.out.println(str2);&#125; 11）StringBuffer和StringBuilder12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/** * 关于StringBuffer和StringBuilder的使用 * 1.StringBuffer的常用方法： * StringBuffer sb1 = new StringBuffer(&quot;abcdef&quot;); * sb1.append(&quot;A&quot;);//abcdefA * sb1.delete(1, 4);//aefA * sb1.replace(2,3,&quot;hello&quot;);//aehelloA * sb1.insert(2,&quot;HELLO&quot;);//aeHELLOhelloA * sb1.reverse();//反转 AollehOLLEHea * sb1.charAt(1); * sb1.indexOf(&quot;A&quot;); * System.out.println(sb1); * 2.StringBuilder的常用方法： * 同StringBuffer * String,StringBuffer,StringBuilder者的异同： * 相同点：底层结构使用char[]数组存取，String的char[]数组用了final修饰 * String:不可变字符串， * StringBuffer:可变的字符序列，线程安全的，效率低。 * StringBuilder:可变的字符序列，线程不安全的，效率高。jdk1.5 * String,StringBuffer,StringBuilder者效率对比： * StringBuilder效率最高，StringBuffer第二，String第3 * 源码分析： * String str=new String();//char[] value=new char[0]; * String str1=new String(&quot;abc&quot;);//char [] value=new char[] &#123;&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;&#125;; * &lt;p&gt; * StringBuffer sb1=new StringBuffer();//char value[]=new char[16];底层创建了一个长度为16的char数组。 * sb1.append(&#x27;a&#x27;);//value[0]=&#x27;a&#x27;; * sb1.append(&#x27;b&#x27;);//value[1]=&#x27;b&#x27;; * StringBuffer sb2=new StringBuffer(&quot;abc&quot;);//char []value=new char [&quot;abc&quot;.length()+16]; * 问题一：System.out.println(sb2.length());//3 * 因为底层返回的长度只是字符串的长度不是数组的长度 * 问题二：扩容问题：如果要添加的数据底层数组装不下了，那就需要扩容底层的数组。 * 默认情况下，扩容为原来容量的2倍+2，同时将原数组中的元素复制到新的数组中。 * 指导意义：开发中建议大家使用：StringBuffer或StringBuilder */public class StringBufferTest &#123; public static void main(String[] args) &#123; StringBuffer sb1 = new StringBuffer(&quot;abcdef&quot;); sb1.append(&quot;A&quot;);//abcdefA sb1.delete(1, 4);//aefA sb1.replace(2,3,&quot;hello&quot;);//aehelloA sb1.insert(2,&quot;HELLO&quot;);//aeHELLOhelloA sb1.reverse();//反转 AollehOLLEHea sb1.charAt(1); sb1.indexOf(&quot;A&quot;); System.out.println(sb1); &#125;&#125; 面试题1234567891011121314151617public static void main(String[] args) &#123; String s1=null; StringBuffer sb1=new StringBuffer(); sb1.append(s1); /** * StringBuffer的append方法会将添加进来的null字符串转化为”null“字符串添加进去。 * 所以此处不会出空指针异常 */ System.out.println(sb1.length());//4 System.out.println(sb1);//&quot;null&quot; StringBuffer sb2=new StringBuffer(s1); /** * StringBuffer的构造器并不会对传入的null进行处理 * 所以此处空指针异常 */ System.out.println(sb2);//java.lang.NullPointerException&#125; String，StringBuffer，StringBuilder的区别123456789101112/** * ： * 1.String：不可变字符串。任何对内容的修改，String指向的地址都发生了变化。 * 1String a=&quot;123&quot;; a在栈里面，指向字符串常量池中的123所对应的地址。当修改a的内容，就相当于 * 将a所指向的地址做出了改变。 * 2String a=new String(&quot;123&quot;);a在栈里面，指向堆空间中new出来的结构，new出来的结构指向字符串常量池 * 中123对应的地址，改变a的内容，还是相当于改变了a所指向的地址。 * 2.StringBuffer：线程安全的，jdk1.1，效率低。 * 3.StringBuilder：线程不安全，效率高。jdk1.5新特性。底层先创建一个长度为16的数组， * 每次添加值，就是给数组对应的位置赋值，扩容问题：每次扩容为原来的2倍+2.再将原来的数组复制进去。 * */ 2.时间日期类1）jdk8之前的时间日期API①时间戳12345@Testpublic void test()&#123; long millis = System.currentTimeMillis(); System.out.println(millis);&#125; ②java.util.Date​ –&gt;java.sql.Date 1两个构造器的使用 1两个方法的使用 1toString()显示当前的年月日时分秒 1date.getTime()时间戳 123456789101112@Testpublic void test2()&#123; Date date=new Date(); System.out.println(date.toString());//Thu Jan 30 17:00:01 CST 2020 System.out.println(date.getTime());//1580374801066&#125;@Testpublic void test3()&#123; Date date = new Date(System.currentTimeMillis()); System.out.println(date.toString());//Thu Jan 30 17:00:33 CST 2020&#125; ③java.sql.Date对应着数据库中的日期类型变量 如何实例化 sql.Date–&gt;util.Date直接赋值 util.Date–&gt;sql.Date 123456789@Testpublic void test4()&#123; java.sql.Date date=new java.sql.Date(System.currentTimeMillis()); System.out.println(date.toString()); Date date1=new Date(); long time = date1.getTime(); java.sql.Date date2=new java.sql.Date(time); System.out.println(date2.toString());&#125; ④SimpleDateFormat类 实例化 格式化：日期–&gt;文本(字符串) 解析：文本(字符串)–&gt;日期 123456789101112@Testpublic void test6() throws ParseException &#123; //实例化 SimpleDateFormat format = new SimpleDateFormat(&quot;yyyy-MM-dd hh:mm:ss&quot;); Date date=new Date(); //格式化 String s = format.format(date); System.out.println(s);//2020-01-30 05:24:23 //解析 Date date1 = format.parse(s); System.out.println(date1);//Thu Jan 30 05:24:23 CST 2020&#125; ⑤Calendar日历类的使用(抽象类) 1.实例化： ①创建子类的对象（不建议） ②调用其静态方法 123456789101112131415161718192021 @Test public void test7() &#123; Calendar calendar = Calendar.getInstance(); //2.常用方法：// calendar.get(); int days = calendar.get(Calendar.DAY_OF_MONTH); System.out.println(days);//这个月的第几天// calendar.set();//修改calendar本身 calendar.set(Calendar.DAY_OF_MONTH, 20); System.out.println(calendar.get(Calendar.DAY_OF_MONTH));// calendar.add();//修改calendar本身 calendar.add(Calendar.DAY_OF_MONTH, 20); System.out.println(calendar.get(Calendar.DAY_OF_MONTH));// calendar.getTime(); Date time = calendar.getTime(); System.out.println(time);// calendar.setTime(); Date date = new Date(); calendar.setTime(date); System.out.println(calendar.getTime()); &#125; ⑥练习字符串2020-09-08转换为java.sql.Date123456789@Testpublic void test8() throws ParseException &#123; String str=&quot;2020-09-08&quot;; SimpleDateFormat format = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;); Date date = format.parse(str); long time = date.getTime(); java.sql.Date sqlDate=new java.sql.Date(time); System.out.println(sqlDate.toString());&#125; 渔夫三天打鱼，两天晒网。1990-01-011234* 问：渔夫在打鱼还是在晒网？ * 2020-09-08 * 总天数%5==1，2，3打鱼；0，4晒网 * 总天数？ 12345678910111213141516@Testpublic void test9() throws ParseException &#123; String start=&quot;1990-01-01&quot;; String end=&quot;2020-09-08&quot;; SimpleDateFormat format = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;); Date date = format.parse(start); Date date1 = format.parse(end); long time = date.getTime(); long time1 = date1.getTime(); long day=(time1-time)/(1000*60*60*24)+1; if (day%5==0||day%5==4)&#123; System.out.println(&quot;今天筛网&quot;); &#125;else&#123; System.out.println(&quot;今天打🐟&quot;); &#125;&#125; 2）jdk8时间日期API①LocalDate,LocalTime,LocalDateTime的使用1234567891011121314151617181920212223242526272829303132333435363738@Testpublic void test() &#123; //实例化方式一：获取当前时间 LocalDate now = LocalDate.now(); LocalTime now1 = LocalTime.now(); LocalDateTime now2 = LocalDateTime.now(); System.out.println(now);//2020-01-30 System.out.println(now1);//13:57:33.240 System.out.println(now2);//2020-01-30T13:57:33.240&#125;@Testpublic void test2() &#123; //实例化方式二：获取指定的日期时间 LocalDate date = LocalDate.of(2020, 2, 2); System.out.println(date);//2020-02-02&#125;@Testpublic void test3() &#123; LocalDateTime time = LocalDateTime.of(2012, 2, 2, 12, 53, 23); System.out.println(time.getDayOfMonth());//2 System.out.println(time.getDayOfWeek());//THURSDAY System.out.println(time.getDayOfYear());//33 System.out.println(time.getHour());//12 System.out.println(time.getMonthValue());//2&#125;@Testpublic void test4() &#123; LocalDateTime time = LocalDateTime.of(2012, 2, 2, 12, 53, 23); LocalDateTime localDateTime = time.withDayOfMonth(2);//不可变性,重置 System.out.println(localDateTime); LocalDateTime plusDays = time.plusDays(108);//不可变性，加 System.out.println(plusDays); LocalDateTime minusDays = time.minusDays(20);//不可变性，减 System.out.println(minusDays);&#125; ②Instant类12345678910111213@Testpublic void test()&#123; //实例化 Instant now = Instant.now(); System.out.println(now);//本初子午线的时间 OffsetDateTime offsetDateTime = now.atOffset(ZoneOffset.ofHours(8)); System.out.println(offsetDateTime);//当前时间 long second = offsetDateTime.toEpochSecond(); System.out.println(second);//获取m数s Instant instant = Instant.ofEpochMilli(1580365450L*1000); System.out.println(instant);&#125; ③格式化或者解析时间日期1234567891011@Testpublic void test()&#123; //自定义格式 DateTimeFormatter formatter=DateTimeFormatter.ofPattern(&quot;yyyy-MM-dd hh:mm:ss&quot;); //格式化 String str=formatter.format(LocalDateTime.now());//2020-01-30 04:37:39 System.out.println(str);// //解析 TemporalAccessor accessor=formatter.parse(&quot;2020-02-18 03:52:09&quot;); System.out.println(accessor);&#125; 3.比较器与其他类1）比较器123456789101112Java中的对象，只能使用==或者！=进行比较，不能使用&lt;或&gt;进行比较，但是在实际开发中，我们需要对多个对象进行排序，言外之意，就需要比较对象的大小。如何实现？使用两个接口，Comparable或Comparator。liang者使用对比：1）Comparable：让类去继承接口，对象具有比较大小的属性2）Comparator：临时new一个匿名对象重写方法，传入对象进行比较，对象不具有比较大小的属性* Comparable接口的使用：自然排序* 1.String或者包装类实现了Comparable接口重写了ComepareTo方法，给出了比较两个对象大小的方法。* 2.重写CompareTo（）方法的规则：* 如果当前对象this大于形参对象obj，则返回正整数，* 如果当前对象this小于形参对象obj，则返回负整数，* 否则返回0；* 3.对于自定义类来说，如果需要排序，我们可以让自定义类实现Comparable接口，重写CompareTo方法。* 在方法中指明如何排序。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556 public static void main(String[] args) &#123; Goods[]arr=new Goods[4]; arr[0]=new Goods(&quot;lenovo&quot;,50.0); arr[1]=new Goods(&quot;honor&quot;,50.0); arr[2]=new Goods(&quot;iphone&quot;,888.8); arr[3]=new Goods(&quot;zte&quot;,66.6); Arrays.sort(arr); System.out.println(Arrays.toString(arr)); &#125;&#125;class Goods implements Comparable&#123; private String name; private double price; public Goods() &#123; &#125; public Goods(String name, double price) &#123; this.name = name; this.price = price; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public double getPrice() &#123; return price; &#125; @Override public String toString() &#123; return &quot;Goods&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, price=&quot; + price + &#x27;&#125;&#x27;; &#125; public void setPrice(double price) &#123; this.price = price; &#125; @Override public int compareTo(Object o) &#123; if (o instanceof Goods)&#123; Goods goods= (Goods) o; return this.price&gt;goods.price?1:(this.price&lt;goods.price?-1:(this.name.compareTo(goods.name))); // return this.name.compareTo(goods.name); &#125; throw new RuntimeException(&quot;传入的数据类型不一致！&quot;); &#125;&#125; 1234* Comparator接口：定制排序* 1.当元素的北京没有实现java.long.Comparable接口而又不方便修改代码* 2.实现了java.lang.Comparable接口的排序规则不适合当前的操作* 3.抽象方法：compare（Object obj1,Object obj2） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556 public static void main(String[] args) &#123; Dog[]arr=new Dog[4]; arr[0]=new Dog(&quot;lenovo&quot;,50); arr[1]=new Dog(&quot;honor&quot;,50); arr[2]=new Dog(&quot;iphone&quot;,40); arr[3]=new Dog(&quot;zte&quot;,60); Arrays.sort(arr, new Comparator&lt;Dog&gt;() &#123; @Override public int compare(Dog o1, Dog o2) &#123; //先照名字从低到高，再照年龄从高到低 if (o1.getName().equals(o2.getName()))&#123; return -Double.compare(o1.getAge(),o2.getAge()); &#125;else&#123; return o1.getName().compareTo(o2.getName()); &#125; &#125; &#125;); System.out.println(Arrays.toString(arr)); &#125;&#125;class Dog &#123; private String name; private int age; public Dog() &#123; &#125; public Dog(String name, int age) &#123; this.name = name; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; @Override public String toString() &#123; return &quot;Dog&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, age=&quot; + age + &#x27;&#125;&#x27;; &#125;&#125; 2）Math类1234567891011/*** Math.abs();//绝对值* Math.sqrt();//平方根* Math.pow();//a的b次幂* Math.log();//自然对数* Math.exp();//e为底指数* Math.max();//* Math.min();//* Math.random();//随机数* Math.round();//double/float转long*/ 3）BigInteger和BigDecimal高精度整数运算器和高精度浮点数运算器 八，枚举类和注解1.枚举类1）枚举类的使用 1.枚举的理解：类的对象只有有限个，确定的。我们称此类为枚举类。 2.当我们定义一组常量时，建议使用枚举类。 3.如果枚举类的对象只有一个，可以看作时单例设计模式。 2）如何定义枚举类方式一：jdk5.0之前自定义枚举类123456789101112131415161718192021222324252627282930313233343536373839/** * @author yinhuidong * @createTime 2020-04-08-13:11 * jdk5.0之前自定义枚举类 */public class Season &#123; private final String name; private final String desc; private Season(String name,String desc)&#123; this.name=name; this.desc=desc; &#125; public static final Season SPRING=new Season(&quot;春天&quot;,&quot;春暖花开&quot;); public static final Season SUMMER=new Season(&quot;夏天&quot;,&quot;夏日炎炎&quot;); public static final Season AUTUMO=new Season(&quot;秋天&quot;,&quot;秋高气爽&quot;); public static final Season WINTER=new Season(&quot;冬天&quot;,&quot;雪花飘飘&quot;); public String getName() &#123; return name; &#125; public String getDesc() &#123; return desc; &#125; @Override public String toString() &#123; return name; &#125;&#125;/** *测试jdk5.0之前的枚举类 */class Test1&#123; public static void main(String[] args) &#123; //System.out.println(Season.SPRING); Season season=Season.SPRING; &#125;&#125; jdk5.0使用enum关键字定义枚举类12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * @author yinhuidong * @createTime 2020-04-08-17:50 */interface Info&#123; void show();&#125;public enum Status implements Info&#123; FREE&#123; public void show()&#123; System.out.println(&quot;空闲&quot;); &#125; &#125;, BUSY&#123; public void show()&#123; System.out.println(&quot;忙碌&quot;); &#125; &#125;, WORK&#123; public void show()&#123; System.out.println(&quot;工作&quot;); &#125; &#125;; private Status()&#123; &#125;&#125;/** *jdk5.0之后使用enum关键字定义枚举类 */class Test2&#123; public static void main(String[] args) &#123; Status status= FREE; status.show(); Status[] values = Status.values(); for (int i=0;i&lt;values.length;i++)&#123; System.out.println(values[i]); &#125; System.out.println(FREE.toString()); System.out.println(Status.valueOf(&quot;FREE&quot;)); &#125;&#125; 3）Enum类中常用方法 value() valueof() toString() 123456State[] states = State.values();for (int i = 0; i &lt;states.length ; i++) &#123; System.out.print(states[i]+&quot; &quot;);//FREE HARD SLEEP&#125;System.out.println(free.toString());//FREESystem.out.println(State.valueOf(&quot;FREE&quot;));//FREE,如果没此对象就会报异常。java.lang.IllegalArgumentException 4）使用enum关键字定义的枚举类实现接口的情况*情况一：实现接口，在enum的枚举类中重写方法 情况二：实现接口，在enum的枚举类中声明的每个对象下都重写方法123456FREE&#123; @Override public void show() &#123; System.out.println(&quot;空闲！&quot;); &#125;&#125;, 2.注解1.Annotation使用示例12345678910111213141516/** * @author yinhuidong * @createTime 2020-04-08-20:56 * 1.Annotation使用示例 * 1）文档注释中的注解 * @return * @Exception * @param * @see * 2)jdk三个内置的注解 * 1.@Override 子类重写父类方法，编译期间校验 * 2.@Deprecated 过时的或危险的（可能造成线程死锁） * 3.@SuppressWarnings() 未使用提醒 * 3）组件框架，跟踪代码依赖性，代替配置文件 * @Autowrited */ 2.自定义注解与元注解123456789101112131415161718192021/** * @author yinhuidong * @createTime 2020-04-08-21:02 * 自定义注解：参照@SuppressWarnings() * 如果自定义的注解没有成员，那就代表一个标识 * 如果自定义注解有成员，需要在使用时指定成员的值 * 自定义注解必须配合反射 * 如果想食用反射操作注解，那么注解必须声明为RUNTIME * * 元注解：可以修饰其他注解的注解 * 1. Retention:指明修饰的注解的生命周期：SOURCE\\CLASS(默认行为)\\RUNTIME * *只有声明为runtime的注解，才能通过反射获取 * 2.Target：用于指定被修饰的结构有哪些 * 3.Documented:被他修饰的注解可以被文档注释读取，保留下来 * 4.Inherited：具有继承性，父类被此注解修饰，子类自动继承父类的注解 */public @interface MyAnnotation &#123; String value() default &quot;hello&quot;; //String类型的属性，默认值为hello&#125; 1.Retention12345678910111213141516171819202122232425262728293031323334353637@Documented@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.ANNOTATION_TYPE)public @interface Retention &#123; /** * Returns the retention policy. * @return the retention policy */ RetentionPolicy value();&#125;继续查看RetentionPolicy，这是一个枚举类public enum RetentionPolicy &#123; /** * Annotations are to be discarded by the compiler. */ SOURCE, /** * Annotations are to be recorded in the class file by the compiler * but need not be retained by the VM at run time. This is the default * behavior. */ CLASS, /** * Annotations are to be recorded in the class file by the compiler and * retained by the VM at run time, so they may be read reflectively. * * @see java.lang.reflect.AnnotatedElement */ RUNTIME&#125;SOURCE：编译时起作用CLASS：字节码文件RUNTIME：运行时 2.Target1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556@Documented@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.ANNOTATION_TYPE)public @interface Target &#123; /** * Returns an array of the kinds of elements an annotation type * can be applied to. * @return an array of the kinds of elements an annotation type * can be applied to */ ElementType[] value();&#125;继续点击进入ElementType又是一个枚举类：指定可以修饰的类型public enum ElementType &#123; /** Class, interface (including annotation type), or enum declaration */ TYPE,//类上 /** Field declaration (includes enum constants) */ FIELD,//属性 /** Method declaration */ METHOD,//方法 /** Formal parameter declaration */ PARAMETER,//成员变量 /** Constructor declaration */ CONSTRUCTOR,//构造器 /** Local variable declaration */ LOCAL_VARIABLE,//局部变量 /** Annotation type declaration */ ANNOTATION_TYPE,//注解类型 /** Package declaration */ PACKAGE,//包 /** * Type parameter declaration * * @since 1.8 */ TYPE_PARAMETER，//泛型 /** * Use of a type * * @since 1.8 */ TYPE_USE //可重复注解&#125; 3.Documented123456@Documented@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.ANNOTATION_TYPE)public @interface Documented &#123;&#125;指示默认情况下iavadoc和类似工具将记录具有类型的注释。此类型应用于对类型声明进行注释，这些类型的注释会影响其客户端对带注释的元素的使用。如果类型声明是用文档注释的，那么它的注释将成为公共API的一部分注释元素的。 4.Inherited123456@Documented@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.ANNOTATION_TYPE)public @interface Inherited &#123;&#125;表示注释类型是自动继承的。如果继承的元注释存在于注释类型上声明，用户查询类声明上的注释类型，而类声明没有针对这种类型的注释，然后类的超类将自动查询注释类型。此过程将重复进行，直到找到此类型的注释，或找到类层次结构的顶部(对象)是达到了。如果没有该类的超类，那么查询将表明所涉及的类没有这样的注释。&lt;p&gt;注意，这个元注释类型没有效果，如果注释的类型是用来注释类以外的任何东西。还要注意，这个元注释只会导致从超类继承注释;对实现接口的注释没有效果 3.jdk8新特性1.可重复注解一个类上写两个一样的注解 123456789101112131415161718192021222324252627282930313233@MyAnnotation(&quot;hi&quot;)@MyAnnotationpublic class AnnotationTest &#123;&#125;@Retention(RetentionPolicy.RUNTIME)//运行时//指定可以修饰哪些结构@Target(value = &#123;ElementType.ANNOTATION_TYPE, ElementType.CONSTRUCTOR, ElementType.FIELD, ElementType.LOCAL_VARIABLE, ElementType.METHOD, ElementType.PACKAGE, ElementType.PARAMETER, ElementType.TYPE, ElementType.TYPE_PARAMETER, ElementType.TYPE_USE&#125;)@Documented//文档注释保留识别@Inherited//被子类继承@Repeatable(MyAnnotations.class)//标识该注解可以实现重复注解public @interface MyAnnotation &#123; String value() default &quot;hello&quot;; //String类型的属性，默认值为hello&#125;@Retention(RetentionPolicy.RUNTIME)//运行时//指定可以修饰哪些结构@Target(value = &#123;ElementType.ANNOTATION_TYPE, ElementType.CONSTRUCTOR, ElementType.FIELD, ElementType.LOCAL_VARIABLE, ElementType.METHOD, ElementType.PACKAGE, ElementType.PARAMETER, ElementType.TYPE, ElementType.TYPE_PARAMETER, ElementType.TYPE_USE&#125;)@Documented//文档注释保留识别@Inherited//被子类继承public @interface MyAnnotations &#123; //声明一个MyAnnotation类型的数组 MyAnnotation []value();&#125; 2.类型注解1234class Test&lt;@MyAnnotation T&gt; &#123;&#125;//此时需要指明@Target(ElementType.TYPE_PARAMETER) 123456789class Test2&#123; public static void main(String[] args) &#123; Class&lt;Test&gt; clazz = Test.class; Annotation[] annotations = clazz.getAnnotations(); for (int i = 0; i &lt;annotations.length ; i++) &#123; System.out.println(annotations[i]); &#125; &#125;&#125; 九，集合框架1.collection接口1）Java集合框架的概述123456789101.集合和数组都是对多个数据进行存储操作的结构，简称Java容器。说明：此时的存储时内存方面的存储，不涉及持久化存储2.1数组在存储多个数据方面的特点：1）一旦初始化以后，长度就确定了。2）元素类型一旦指定，就不能改变，我们就只能操作指定类型的数据。2.2数组在存取数据方面的缺点：1）初始化以后，长度不可修改。2）数组中提供的方法非常有限，对于删除插入数据非常不方便，效率也不高。3）获取数组中实际元素的个数，数组并没有提供现成的方法。4）数组存储数据的特点：有序，可重复。 集合框架1234Collection接口：单列集合，用来存储一个一个对象list接口：有序的可重复的数据。“动态数组”Set接口：无序的不可重复的数据。Map接口：双列集合，用来存储一对一对的数据。（key，value） Collection接口中的方法的使用123456789101112131415public static void main(String[] args) &#123; Collection collection=new ArrayList(); collection.add(&quot;aa&quot;);//将元素添加到集合中 collection.add(&quot;bb&quot;); collection.add(&quot;cc&quot;); System.out.println(collection.size());//获取添加的元素的个数 Collection collection2=new ArrayList(); collection2.add(&quot;bbb&quot;); collection2.add(&quot;ccc&quot;); collection.addAll(collection2);//将一个集合的元素添加到另一个集合 System.out.println(collection.size()); System.out.println(collection);//输出集合 collection2.clear();//清空集合中的元素 System.out.println(collection.isEmpty());//判断当前集合是否为空 true false&#125; 123456789101112131415161718public static void main(String[] args) &#123; Collection collection=new ArrayList(); collection.add(123); collection.add(&quot;tom&quot;); collection.add(&quot;aa&quot;); Person p1=new Person(&quot;dong&quot;,23); collection.add(p1); /** * 集合中添加对象，最好重写该对象所在类的equals（方法 */ System.out.println(collection.contains(p1));//判断当前集合是否包含该元素,使用Obj对象所在类的equals（方法。 collection.containsAll(collection);//判断形参集合中的所元素是否都在该集合中。 collection.remove(p1);//移除某个元素 boolean返回true/false collection.removeAll(collection);//从当前集合移除两者都的元素 collection.retainAll(collection);//获取当前集合和形参集合的交集，并返回当前集合。 collection.equals(collection);//比较两个集合是否完全相同&#125; 1234567891011public static void main(String[] args) &#123; Collection collection=new ArrayList(); collection.add(&quot;123&quot;); collection.add(&quot;aaa&quot;); collection.add(&quot;bbbb&quot;); collection.add(&quot;dddd&quot;); System.out.println(collection.hashCode());//输出hash值 collection.toArray();//集合--&gt;数组 //数组--&gt;集合Arrays.asList(); //iterator返回这个接口的实例，用于遍历集合元素。&#125; 使用Iterator遍历Colllection集合 Collection集合实现了Iterator接口，重写了接口的hasNext（）和next（）方法。 内部定义了remove()方法，可以在便利的时候，删除集合中的元素， 此方法不同于集合直接调用remove(). 12345678910111213public static void main(String[] args) &#123; Collection co=new ArrayList(); co.add(&quot;123113212&quot;); co.add(&quot;456487874&quot;); co.add(&quot;56646665454654&quot;); Iterator&lt;Collection&gt;iterator=co.iterator(); while (iterator.hasNext())&#123;// if (iterator.next().equals(&quot;Tom&quot;))&#123;// iterator.remove();// &#125; System.out.println(iterator.next()); &#125; &#125; 增强for循环遍历集合 内部仍然调用了迭代器 把集合中的每个值一次一次赋值给Object类型的变量然后输出 123456789public static void main(String[] args) &#123; Collection co=new ArrayList(); co.add(&quot;123113212&quot;); co.add(&quot;456487874&quot;); co.add(&quot;56646665454654&quot;); for (Object c:co)&#123; System.out.println(c); &#125;&#125; Collection集合接口是支持泛型的，并且继承了Iterable接口，可以使用Iterator iterator()进行遍历 12345678910111213141516171819202122232425262728 * @see Set * @see List * @see Map * @see SortedSet * @see SortedMap * @see HashSet * @see TreeSet * @see ArrayList * @see LinkedList * @see Vector * @see Collections * @see Arrays * @see AbstractCollection * @since 1.2 */public interface Collection&lt;E&gt; extends Iterable&lt;E&gt; &#123;public interface Iterable&lt;T&gt; &#123; /** * Returns an iterator over elements of type &#123;@code T&#125;. * * @return an Iterator. */ Iterator&lt;T&gt; iterator(); 2）list比较ArrayList，LinkedList，Vector1234* 同：三个类都实现了list接口，存储数据的特点相同，有序，可重复。* ArrayList：作为list接口的主要实现类，jdk1.2，线程不安全的，效率高，底层使用Object []elementData存储* LinkedList：jdk1.2：底层使用双向链表存储，对于频繁的插入删除，他的效率高。* Vector：古老的实现类jdk1.0，线程安全的。 List接口常用方法测试：123456789101112131415161718192021public static void main(String[] args) &#123; ArrayList&lt;Object&gt; list = new ArrayList&lt;&gt;(); list.add(&quot;123&quot;); list.add(&quot;456&quot;); list.add(&quot;789&quot;); System.out.println(list);//[123, 456, 789] list.add(0,&quot;000&quot;);//指定位置插入 System.out.println(list);//[000, 123, 456, 789] List&lt;Integer&gt; list1 = Arrays.asList(1, 2, 3, 4, 5, 6);//数组转化为集合 list.addAll(list1);//将集合list1添加进集合list System.out.println(list.size());//输出集合多少个元素 10 Object o = list.get(0);//通过索引获取元素 System.out.println(o);//000 int index = list.indexOf(&quot;456&quot;);//查找元素的索引.如果不存在返回-1 System.out.println(index);//2 Object o1 = list.remove(1);//可以照索引或者对象删除 System.out.println(o1+&quot; &quot;+list);//123 [000, 456, 789, 1, 2, 3, 4, 5, 6] list.set(1,&quot;cc&quot;);//将某个索引的值改为 List&lt;Object&gt; list2 = list.subList(1, 5);//截取集合中的元素 System.out.println(list2);//[cc, 789, 1, 2]&#125; List遍历，及方法总结12345678910111213141516171819public static void main(String[] args) &#123; ArrayList&lt;Object&gt; list = new ArrayList&lt;&gt;(); list.add(&quot;123&quot;); list.add(&quot;456&quot;); list.add(&quot;789&quot;); //1.Iterator Iterator&lt;Object&gt; iterator = list.iterator(); while (iterator.hasNext())&#123; System.out.println(iterator.next()); &#125; //2.增强for循环 for (Object obj:list)&#123; System.out.println(obj); &#125; //3..普通for循环 for (int i=0;i&lt;list.size();i++)&#123; System.out.println(list.get(i)); &#125;&#125; List的一道面试题12345678910111213 public static void main(String[] args) &#123; List list=new ArrayList&lt;&gt;(); list.add(1); list.add(2); list.add(3); updatelist(list); System.out.println(list);//[1, 2] &#125; private static void updatelist(List list) &#123; list.remove(2);// list.remove(new Integer(2));//如果想删除元素2 &#125; jdk 1.2的接口 继承了Collection接口 sort方法需要传入一个比较器 定制排序 实际上使用了Arrays的sort方法 123456789101112public interface List&lt;E&gt; extends Collection&lt;E&gt; &#123;default void sort(Comparator&lt;? super E&gt; c) &#123; Object[] a = this.toArray(); Arrays.sort(a, (Comparator) c); ListIterator&lt;E&gt; i = this.listIterator(); for (Object e : a) &#123; i.next(); i.set((E) e); &#125; &#125; ArrayList源码分析：123456789101112131415/** jdk7：* ArrayList list=new ArrayList()//底层创建了长度是10的Object[]elementDate数组* list.add(123);//elementData[0]=new Integer(123);* ...* list.add();//如果此次的添加导致底层的数组容量不够，则扩容。 * 默认情况下，扩容为原来的1.5倍，同时需要将原有数组的数据复制到新的数组。* 结论：建议开发中使用带参数的构造器：ArrayList list=new ArrayList(int 数组长度);* jdk8:* ArrayList list=new ArrayList()//底层Object[]elementDate=&#123;&#125;，并没有创建长度为10的数组* list.add(123);//第一次调用add时，底层才创建了长度为10的数组，并将数据123添加到elementData[0]* ...后续的添加和扩容操作与jdk7无异。* 对比：jdk7中的对象创建类似于单例的饿汉式，而jdk8中的对象的创建类似于单例的懒汉式，* 延迟了数组的创建，节省内存空间。*/ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable&#123; //默认初始容量为10 private static final int DEFAULT_CAPACITY = 10; //空的ElementData数组 private static final Object[] EMPTY_ELEMENTDATA = &#123;&#125;; //用于默认大小的空实例。我们从空的ELEMENTDATA中解出这个问题，以了解在添加第一个元素时应该增加多少 private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = &#123;&#125;; //存储ArrayList的元素的数组缓冲区。arraylisis的容量是这个数组缓冲区的长度。当添加第一个元素时，任何带有elementData DBEAULTCAPACITY empty ELEMENTDATE的空ArrayList都将被扩展为默认容量（10） transient Object[] elementData; //带有指定长度的数组构造器 public ArrayList(int initialCapacity) &#123; if (initialCapacity &gt; 0) &#123; this.elementData = new Object[initialCapacity]; &#125; else if (initialCapacity == 0) &#123; this.elementData = EMPTY_ELEMENTDATA; &#125; else &#123; throw new IllegalArgumentException(&quot;Illegal Capacity: &quot;+ initialCapacity); &#125; &#125; //传入一个Collection类型的构造器 public ArrayList(Collection&lt;? extends E&gt; c) &#123; elementData = c.toArray(); if ((size = elementData.length) != 0) &#123; // c.toArray might (incorrectly) not return Object[] (see 6260652) if (elementData.getClass() != Object[].class) elementData = Arrays.copyOf(elementData, size, Object[].class); &#125; else &#123; // replace with empty array. this.elementData = EMPTY_ELEMENTDATA; &#125; &#125; //将这个&lt;tt&gt;ArrayList&lt;/tt&gt;实例的容量调整为列表的当前大小。集合创建的时候会流出来预留的空间，使用这个方法可以去掉预留空间，节省内存。 public void trimToSize() &#123; modCount++; if (size &lt; elementData.length) &#123; elementData = (size == 0) ? EMPTY_ELEMENTDATA : Arrays.copyOf(elementData, size); &#125; &#125; //设置数组的最大长度为integer的最大值-8，防止造成内存溢出异常 private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8; //数组的扩容问题：&gt;&gt;1意思就是/2 private void grow(int minCapacity) &#123; // overflow-conscious code //扩容前数组的长度 int oldCapacity = elementData.length; //新数组扩容为原来的1.5倍 int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); //如果扩容前比扩容后小 if (newCapacity - minCapacity &lt; 0) //还是原来的数组 newCapacity = minCapacity; //如果扩容后超出数组的最大值 if (newCapacity - MAX_ARRAY_SIZE &gt; 0) //数组长度变为数组的最大长度 newCapacity = hugeCapacity(minCapacity); //将原来的数组在复制进新的数组 elementData = Arrays.copyOf(elementData, newCapacity); &#125; //在列表中指定的位置插入指定的元素。将当前位于该位置的元素(如果有)和任何后续元素右移一位(将一个元素添加到它们的索引中) public void add(int index, E element) &#123; rangeCheckForAdd(index); ensureCapacityInternal(size + 1); // Increments modCount!! System.arraycopy(elementData, index, elementData, index + 1, size - index); elementData[index] = element; size++; &#125; //底层重写了sort方法 @Override @SuppressWarnings(&quot;unchecked&quot;) public void sort(Comparator&lt;? super E&gt; c) &#123; //记录集合的修改次数 final int expectedModCount = modCount; //排序 Arrays.sort((E[]) elementData, 0, size, c); if (modCount != expectedModCount) &#123;//应该是多线程考虑 throw new ConcurrentModificationException(); &#125; modCount++; &#125; LinkedList源码分析12345678910111213141516/***LinkedList list=new LinkedList();//内部声明了Node类型的first和last属性，默认值为null* list.add(123);//将123封装到了node中，创建了Node对象。* 其中，Node定义为：体现了LinkedList双向链表的说法。*/private static class Node&lt;E&gt; &#123;E item;Node&lt;E&gt; next;Node&lt;E&gt; prev;Node(Node&lt;E&gt; prev, E element, Node&lt;E&gt; next) &#123;this.item = element;this.next = next;this.prev = prev;&#125;&#125; jdk1.2 实现了List接口，Deque，Cloneable, java.io.Serializable Deque 线性集合，支持两端插入和移除元素。 名称deque是“双端队列”的缩写 Cloneable cloneable其实就是一个标记接口，只有实现这个接口后，然后在类中重写Object中的clone方法，然后通过类调用clone方法才能克隆成功，如果不实现这个接口，则会抛出CloneNotSupportedException(克隆不被支持)异常。 java.io.Serializable 支持序列化 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123public class LinkedList&lt;E&gt; extends AbstractSequentialList&lt;E&gt; implements List&lt;E&gt;, Deque&lt;E&gt;, Cloneable, java.io.Serializable&#123;//指向第一个节点的指针transient Node&lt;E&gt; first;//指向最后一个节点的指针transient Node&lt;E&gt; last;//LinkedList的数据结构就是双向链表 private static class Node&lt;E&gt; &#123; E item;//数据元素 Node&lt;E&gt; next;//后继节点 Node&lt;E&gt; prev;//前驱节点 Node(Node&lt;E&gt; prev, E element, Node&lt;E&gt; next) &#123; this.item = element; this.next = next; this.prev = prev; &#125; &#125; //构造器 transient int size = 0;//数据个数 transient Node&lt;E&gt; first;//表示链表的第一个节点 transient Node&lt;E&gt; last;//表示链表的最后一个节点 public LinkedList() &#123; &#125; public LinkedList(Collection&lt;? extends E&gt; c) &#123;//用于整合Collection类型的数据 this(); addAll(c); &#125; //add： public boolean add(E e) &#123; linkLast(e); return true; &#125; void linkLast(E e) &#123;//采用的是尾插法 final Node&lt;E&gt; l = last; final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null);//新节点的前驱指向last的地址，后继为null， //所以说这是一个双向链表，但不是循环的，循环的话，后继指向头节点 last = newNode;//让last指向新节点，也就说这个新节点是链表的最后一个元素 if (l == null)//当第一次添加时，first，last都是null，如果last是null，表明这是一个空链表 first = newNode;//就让新节点指向first，现在first和last都是同一个节点 else l.next = newNode;//当在添加数据时，就让老链表的最后一个节点的后继指向新节点（那个节点本来是null的） size++; //长度加1 modCount++;/**总结：新建一个节点，让新节点的前驱指向老链表的最后一个节点让老链表的最后一个节点的后继指向新节点让新节点变成链表的最后一个节点长度加1第一个节点前驱为null，最后一个节点后继为null*/ &#125; //get public E get(int index) &#123; checkElementIndex(index);//检查一下索引是否在0到size的范围内 return node(index).item; &#125;Node&lt;E&gt; node(int index) &#123; // assert isElementIndex(index); if (index &lt; (size &gt;&gt; 1)) &#123;//看看索引的位置是在链表的前半部分还是后半部分，决定正着搜索或倒着搜索，找到后返回就行啦 Node&lt;E&gt; x = first; for (int i = 0; i &lt; index; i++)//在这里看到链表是从0开始的 x = x.next; return x; &#125; else &#123; Node&lt;E&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prev; return x; &#125; &#125; //remove public E remove(int index) &#123; checkElementIndex(index);//先检查一下索引 return unlink(node(index)); &#125;//先拿着索引找到这个节点E unlink(Node&lt;E&gt; x) &#123; // assert x != null; final E element = x.item;//节点的元素 final Node&lt;E&gt; next = x.next;//节点的后继 final Node&lt;E&gt; prev = x.prev;//节点的前驱 if (prev == null) &#123; first = next; &#125; else &#123; prev.next = next; x.prev = null; &#125; if (next == null) &#123; last = prev; &#125; else &#123; next.prev = prev; x.next = null; &#125; x.item = null; size--; modCount++; return element; &#125; Vector源码分析先创建初始长度为十的数组，扩容默认为原来的二倍，线程安全的。 Vector 是矢量队列，底层是数组。它是JDK1.0版本添加的类。继承于AbstractList，实现了List, RandomAccess, Cloneable Vector 继承了AbstractList，实现了List；所以，它是一个队列，支持相关的添加、删除、修改、遍历等功能。 Vector 实现了RandmoAccess接口，即提供了随机访问功能。RandmoAccess是java中用来被List实现，为List提供快速访问功能的。在Vector中，我们即可以通过元素的序号快速获取元素对象；这就是快速随机访问。 Vector 实现了Cloneable接口，即实现clone()函数。它能被克隆。 Vector中的操作是线程安全的。因为Vector的方法前加了synchronized 关键字，所以效率不高。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465public class Vector&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable&#123;//向量的分量所在的数组缓冲区存储。向量的容量是这个数组缓冲区的长度，并且至少大到可以包含向量的所有元素。protected Object[] elementData;//这个对象有效组件的数量protected int elementCount;//当向量的大小大于其容量时，该向量的容量自动增加的量。如果容量增量小于或等于0，则每次需要增长时，向量的容量将增加一倍。protected int capacityIncrement;//用指定的初始容量和容量增量构造一个空向量。@paraminitialCapacity向量的初始容量@paramcapacitylncrement容量所占的量当向量溢出@抛出illeqalarqumentexceptionifspecifiedinitialcapacity时增加是负的 public Vector(int initialCapacity, int capacityIncrement) &#123; super(); if (initialCapacity &lt; 0) throw new IllegalArgumentException(&quot;Illegal Capacity: &quot;+ initialCapacity); this.elementData = new Object[initialCapacity]; this.capacityIncrement = capacityIncrement; &#125; //默认增量为0 public Vector(int initialCapacity) &#123; this(initialCapacity, 0); &#125; //默认长度为10 public Vector() &#123; this(10); &#125; //扩容方法 //@param minCapacity the desired minimum capacity 所需要的最低容量 如果最低容量&gt;0,记录集合又被修改一次 调用ensureCapacityHelper(minCapacity)方法 public synchronized void ensureCapacity(int minCapacity) &#123; if (minCapacity &gt; 0) &#123; modCount++; ensureCapacityHelper(minCapacity); &#125; &#125; //接下来，进入ensureCapacityHelper(minCapacity)方法 private void ensureCapacityHelper(int minCapacity) &#123; // overflow-conscious code 如果指定的扩容后长度比现在的容量大，说明扩容是合法的 调用grow(minCapacity)方法 if (minCapacity - elementData.length &gt; 0) grow(minCapacity); &#125; //继续点击，进入grow(minCapacity) private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length;//用来记录原长度 //新的长度的计算：如果增长的长度大于0就扩容为原来的长度+新增的长度，否则扩容为原来的2倍 int newCapacity = oldCapacity + ((capacityIncrement &gt; 0) ? capacityIncrement : oldCapacity); if (newCapacity - minCapacity &lt; 0) //如果闲的容量比所需要的最低容量小，新的长度就等于所需的最低容量 newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) //如果超出最大临界值，就让新数组长度等于最大临界值 newCapacity = hugeCapacity(minCapacity); //将原来的元素复制进来 elementData = Arrays.copyOf(elementData, newCapacity); &#125; 3）set1234567891011121314151617181920212223242526/*** 1.Set接口的框架结构：存储无顺序的，不可重复的数据。* HashSet：作为set接口的主要实现类，线程不安全，可以存储null** LinkedHashSet：HashSet的子类。遍历其内部数据时，可以按照添加的顺序遍历。** TreeSet：可以按照添加对象的指定属性，进行排序。** 2.如何理解Set的无序，不可重复。* ①无序性：不等于随机性。*以hashset为例，存储的数据在底层数组中并非按照数组索引的顺序添加，而是根据数据的哈希值添加。* ②不可重复性：保证添加的元素按照equals（）方法判断时，不能返回True。即，相同的元素只能添加一个。* hashset底层为数组加链表* 3.Set接口中没有额外定义新的方法，使用的都是Collection接口中声明过的方法。* 4.添加元素的过程：以HashSet为例* 我们想hashset中添加元素a，首先调用a所在类的hashcode方法，计算a的哈希值，* 此哈希值通过算法计算中在hashset底层数组的存放位置，判断数组此位置是否已经有元素，* 如果此位置上没有其他元素，a直接添加成功；如果此位置有其他元素b（或以链表形式存在多个元素），* 则比较a和b的哈希值，如果哈希值不相同，则，元素a添加成功，如果哈希值相同，调用元素a所在类的equals（）* 方法，equals（）返回true，元素添加失败，如果返回false，元素a添加成功。* 说明/：对于添加的位置有元素还添加成功的情况，与已经存在位置上数据以链表形式存储，* jdk7中a放到数组中，指向原来的元素，jdk8中原来的元素放在数组中，指向a元素。* 5.要求：* ①向set中添加的数据，其所在的类一定要重写hashCode()和equals()方法* ②重写hashCode()和equals()方法尽可能保持一致：相等的对象哈希值必须相同。*/ 123public interface Spliterator&lt;T&gt; &#123;Spliterator（splitable iterator可分割迭代器）接口是Java为了并行遍历数据源中的元素而设计的迭代器，这个可以类比最早Java提供的顺序遍历迭代器Iterator，但一个是顺序遍历，一个是并行遍历 1234567public interface Set&lt;E&gt; extends Collection&lt;E&gt; &#123;//证明了set集合不可重复 @Override default Spliterator&lt;E&gt; spliterator() &#123; return Spliterators.spliterator(this, Spliterator.DISTINCT); &#125; 面试题1234567891011121314151617181920/** * 面试题一：在list内去除重复数据值， */public static void main(String[] args) &#123; ArrayList&lt;String&gt; list = new ArrayList&lt;&gt;(); list.add(&quot;123&quot;); list.add(&quot;456&quot;); list.add(&quot;789&quot;); list.add(&quot;456&quot;); List list2=quChong(list); Iterator&lt;String&gt; iterator = list2.iterator(); while (iterator.hasNext())&#123; System.out.println(iterator.next ()); &#125;&#125;public static List&lt;String&gt; quChong(List list)&#123; HashSet&lt;String&gt; set = new HashSet&lt;&gt;(); set.addAll(list); return new ArrayList(set);&#125; HashSet123456789/** 我们想hashset中添加元素a，首先调用a所在类的hashcode方法，计算a的哈希值，* 此哈希值通过算法计算中在hashset底层数组的存放位置，判断数组此位置是否已经有元素，* 如果此位置上没有其他元素，a直接添加成功；如果此位置有其他元素b（或以链表形式存在多个元素），* 则比较a和b的哈希值，如果哈希值不相同，则，元素a添加成功，如果哈希值相同，调用元素a所在类的equals（）* 方法，equals（）返回true，元素添加失败，如果返回false，元素a添加成功。* 说明/：对于添加的位置有元素还添加成功的情况，与已经存在位置上数据以链表形式存储，* jdk7中a放到数组中，指向原来的元素，jdk8中原来的元素放在数组中，指向a元素。*/ 继承了AbstractSet 实现了Set, Cloneable, java.io.Serializable AbstractSet 12public abstract class AbstractSet&lt;E&gt; extends AbstractCollection&lt;E&gt; implements Set&lt;E&gt; &#123;里面写了equals hashcode removeAll方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170public class HashSet&lt;E&gt; extends AbstractSet&lt;E&gt; implements Set&lt;E&gt;, Cloneable, java.io.Serializable &#123; static final long serialVersionUID = -5024744406713321676L; // 底层使用HashMap来保存HashSet中所有元素。 private transient HashMap&lt;E,Object&gt; map; // 定义一个虚拟的Object对象作为HashMap的value，将此对象定义为static final。 private static final Object PRESENT = new Object(); //关于为什么不用null而是用一个Object类型的对象，因为map的key本身是可以为null的，二set存储元素成功与否 是需要返回一个true或者false，如果使用null来充当value，你就不知道到底存储成功没 /** * 默认的无参构造器，构造一个空的HashSet。 * * 实际底层会初始化一个空的HashMap，并使用默认初始容量为16和加载因子0.75。 */ public HashSet() &#123; map = new HashMap&lt;E,Object&gt;(); &#125; /** * 构造一个包含指定collection中的元素的新set。 * * 实际底层使用默认的加载因子0.75和足以包含指定 * collection中所有元素的初始容量来创建一个HashMap。 * @param c 其中的元素将存放在此set中的collection。 */ public HashSet(Collection&lt;? extends E&gt; c) &#123; map = new HashMap&lt;E,Object&gt;(Math.max((int) (c.size()/.75f) + 1, 16)); addAll(c); &#125; /** * 以指定的initialCapacity和loadFactor构造一个空的HashSet。 * * 实际底层以相应的参数构造一个空的HashMap。 * @param initialCapacity 初始容量。 * @param loadFactor 加载因子。 */ public HashSet(int initialCapacity, float loadFactor) &#123; map = new HashMap&lt;E,Object&gt;(initialCapacity, loadFactor); &#125; /** * 以指定的initialCapacity构造一个空的HashSet。 * * 实际底层以相应的参数及加载因子loadFactor为0.75构造一个空的HashMap。 * @param initialCapacity 初始容量。 */ public HashSet(int initialCapacity) &#123; map = new HashMap&lt;E,Object&gt;(initialCapacity); &#125; /** * 以指定的initialCapacity和loadFactor构造一个新的空链接哈希集合。 * 此构造函数为包访问权限，不对外公开，实际只是是对LinkedHashSet的支持。 * * 实际底层会以指定的参数构造一个空LinkedHashMap实例来实现。 * @param initialCapacity 初始容量。 * @param loadFactor 加载因子。 * @param dummy 标记。 */ HashSet(int initialCapacity, float loadFactor, boolean dummy) &#123; map = new LinkedHashMap&lt;E,Object&gt;(initialCapacity, loadFactor); &#125; /** * 返回对此set中元素进行迭代的迭代器。返回元素的顺序并不是特定的。 * * 底层实际调用底层HashMap的keySet来返回所有的key。 * 可见HashSet中的元素，只是存放在了底层HashMap的key上， * value使用一个static final的Object对象标识。 * @return 对此set中元素进行迭代的Iterator。 */ public Iterator&lt;E&gt; iterator() &#123; return map.keySet().iterator(); &#125; /** * 返回此set中的元素的数量（set的容量）。 * * 底层实际调用HashMap的size()方法返回Entry的数量，就得到该Set中元素的个数。 * @return 此set中的元素的数量（set的容量）。 */ public int size() &#123; return map.size(); &#125; /** * 如果此set不包含任何元素，则返回true。 * * 底层实际调用HashMap的isEmpty()判断该HashSet是否为空。 * @return 如果此set不包含任何元素，则返回true。 */ public boolean isEmpty() &#123; return map.isEmpty(); &#125; /** * 如果此set包含指定元素，则返回true。 * 更确切地讲，当且仅当此set包含一个满足(o==null ? e==null : o.equals(e)) * 的e元素时，返回true。 * * 底层实际调用HashMap的containsKey判断是否包含指定key。 * @param o 在此set中的存在已得到测试的元素。 * @return 如果此set包含指定元素，则返回true。 */ public boolean contains(Object o) &#123; return map.containsKey(o); &#125; /** * 如果此set中尚未包含指定元素，则添加指定元素。 * 更确切地讲，如果此 set 没有包含满足(e==null ? e2==null : e.equals(e2)) * 的元素e2，则向此set 添加指定的元素e。 * 如果此set已包含该元素，则该调用不更改set并返回false。 * * 底层实际将将该元素作为key放入HashMap。 * 由于HashMap的put()方法添加key-value对时，当新放入HashMap的Entry中key * 与集合中原有Entry的key相同（hashCode()返回值相等，通过equals比较也返回true）， * 新添加的Entry的value会将覆盖原来Entry的value，但key不会有任何改变， * 因此如果向HashSet中添加一个已经存在的元素时，新添加的集合元素将不会被放入HashMap中， * 原来的元素也不会有任何改变，这也就满足了Set中元素不重复的特性。 * @param e 将添加到此set中的元素。 * @return 如果此set尚未包含指定元素，则返回true。 */ public boolean add(E e) &#123; return map.put(e, PRESENT)==null; &#125; /** * 如果指定元素存在于此set中，则将其移除。 * 更确切地讲，如果此set包含一个满足(o==null ? e==null : o.equals(e))的元素e， * 则将其移除。如果此set已包含该元素，则返回true * （或者：如果此set因调用而发生更改，则返回true）。（一旦调用返回，则此set不再包含该元素）。 * * 底层实际调用HashMap的remove方法删除指定Entry。 * @param o 如果存在于此set中则需要将其移除的对象。 * @return 如果set包含指定元素，则返回true。 */ public boolean remove(Object o) &#123; return map.remove(o)==PRESENT; &#125; /** * 从此set中移除所有元素。此调用返回后，该set将为空。 * * 底层实际调用HashMap的clear方法清空Entry中所有元素。 */ public void clear() &#123; map.clear(); &#125; /** * 返回此HashSet实例的浅表副本：并没有复制这些元素本身。 * * 底层实际调用HashMap的clone()方法，获取HashMap的浅表副本，并设置到HashSet中。 */ public Object clone() &#123; try &#123; HashSet&lt;E&gt; newSet = (HashSet&lt;E&gt;) super.clone(); newSet.map = (HashMap&lt;E, Object&gt;) map.clone(); return newSet; &#125; catch (CloneNotSupportedException e) &#123; throw new InternalError(); &#125; &#125; &#125; LinkedHashSet12345/* * LinkedHashSet的使用 * LinkedHashSet作为hashSet的子类，再添加数据的同时，每个数据还维护了一对双向链表， * 记录此数据的前一个数据和后一个数据。对于频繁的遍历，LinkedHashSet的效率高于HashSet */ 12public class LinkedHashSet&lt;E&gt; extends HashSet&lt;E&gt; implements Set&lt;E&gt;, Cloneable, java.io.Serializable &#123;LinkHashSet底层只是单纯的继承了HashSet并没啥太大改变 TreeSet的使用123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182public class TreeSet&lt;E&gt; extends AbstractSet&lt;E&gt; implements NavigableSet&lt;E&gt;, Cloneable, java.io.Serializable&#123; /** * 可以排序的map. */ private transient NavigableMap&lt;E,Object&gt; m; // 用来存入map的value的 private static final Object PRESENT = new Object(); /** * 构造一个指定map集合的TreeSet */ TreeSet(NavigableMap&lt;E,Object&gt; m) &#123; this.m = m; &#125;//自然排序 构造一个新的空树集，根据其元素的自然顺序排序。所有插入到集合中的元素必须实现&#123;@link comparable&#125;接口。此外,所有这些元素都必须&lt;我&gt;相互可比&lt; / ix: f@code e1.compareTo (e2)&#125;不能抛出一个f@code ClassCastException&#125; &#123;@code el&#125;对任何元素集和f@code e2&#125;。如果用户试图添加一个字符串进入Integer类型的Set&#123;@code添加&#125;调用将抛出一个 public TreeSet() &#123; this(new TreeMap&lt;E,Object&gt;()); &#125; /** 定制排序 * Constructs a new, empty tree set, sorted according to the specified * comparator. All elements inserted into the set must be &lt;i&gt;mutually * comparable&lt;/i&gt; by the specified comparator: &#123;@code comparator.compare(e1, * e2)&#125; must not throw a &#123;@code ClassCastException&#125; for any elements * &#123;@code e1&#125; and &#123;@code e2&#125; in the set. If the user attempts to add * an element to the set that violates this constraint, the * &#123;@code add&#125; call will throw a &#123;@code ClassCastException&#125;. * * @param comparator the comparator that will be used to order this set. * If &#123;@code null&#125;, the &#123;@linkplain Comparable natural * ordering&#125; of the elements will be used. */ public TreeSet(Comparator&lt;? super E&gt; comparator) &#123; this(new TreeMap&lt;&gt;(comparator)); &#125; /** * Constructs a new tree set containing the elements in the specified * collection, sorted according to the &lt;i&gt;natural ordering&lt;/i&gt; of its * elements. All elements inserted into the set must implement the * &#123;@link Comparable&#125; interface. Furthermore, all such elements must be * &lt;i&gt;mutually comparable&lt;/i&gt;: &#123;@code e1.compareTo(e2)&#125; must not throw a * &#123;@code ClassCastException&#125; for any elements &#123;@code e1&#125; and * &#123;@code e2&#125; in the set. * * @param c collection whose elements will comprise the new set * @throws ClassCastException if the elements in &#123;@code c&#125; are * not &#123;@link Comparable&#125;, or are not mutually comparable * @throws NullPointerException if the specified collection is null */ public TreeSet(Collection&lt;? extends E&gt; c) &#123; this(); addAll(c); &#125; /** * Constructs a new tree set containing the same elements and * using the same ordering as the specified sorted set. * * @param s sorted set whose elements will comprise the new set * @throws NullPointerException if the specified sorted set is null */ public TreeSet(SortedSet&lt;E&gt; s) &#123; this(s.comparator()); addAll(s); &#125; /** * Returns an iterator over the elements in this set in ascending order. *以升序返回此集合中元素的迭代器。 * @return an iterator over the elements in this set in ascending order */ public Iterator&lt;E&gt; iterator() &#123; return m.navigableKeySet().iterator(); &#125; /** * Returns an iterator over the elements in this set in descending order. *按降序返回该集合中元素的迭代器。 * @return an iterator over the elements in this set in descending order * @since 1.6 */ public Iterator&lt;E&gt; descendingIterator() &#123; return m.descendingKeySet().iterator(); &#125; /** * @since 1.6 */ public NavigableSet&lt;E&gt; descendingSet() &#123; return new TreeSet&lt;&gt;(m.descendingMap()); &#125; public int size() &#123; return m.size(); &#125; public boolean isEmpty() &#123; return m.isEmpty(); &#125; public boolean contains(Object o) &#123; return m.containsKey(o); &#125; /** * Adds the specified element to this set if it is not already present. * More formally, adds the specified element &#123;@code e&#125; to this set if * the set contains no element &#123;@code e2&#125; such that * &lt;tt&gt;(e==null&amp;nbsp;?&amp;nbsp;e2==null&amp;nbsp;:&amp;nbsp;e.equals(e2))&lt;/tt&gt;. * If this set already contains the element, the call leaves the set * unchanged and returns &#123;@code false&#125;. *大概意思就是先根据hash值比交，不相同直接添加成功， hashcode相同在判断是不是null然后调用equals方法进行比较 equals方法返回true，则添加失败，否则添加成功。 * @param e element to be added to this set * @return &#123;@code true&#125; if this set did not already contain the specified * element * @throws ClassCastException if the specified object cannot be compared * with the elements currently in this set * @throws NullPointerException if the specified element is null * and this set uses natural ordering, or its comparator * does not permit null elements */ public boolean add(E e) &#123; return m.put(e, PRESENT)==null; &#125;如果指定的元素存在，则从该集合中移除它。更正式的说法是，删除元素f@code e&#125;&lt; (ttx - o = null&amp;nbsp;? e = -null&amp;nbsp &amp;nbsp;;o.equals &amp;nbsp; (e)) / tt &gt;、&lt;如果这个集合包含这样一个元素。如果该集合包含元素，则返回&#123;@code true&#125;(或者，如果该集合由于调用而改变，则返回相等的结果)。(一旦调用返回，这个集合将不包含元素。) * @param o object to be removed from this set, if present * @return &#123;@code true&#125; if this set contained the specified element * @throws ClassCastException if the specified object cannot be compared * with the elements currently in this set * @throws NullPointerException if the specified element is null * and this set uses natural ordering, or its comparator * does not permit null elements */ public boolean remove(Object o) &#123; return m.remove(o)==PRESENT; &#125; /** * Removes all of the elements from this set. * The set will be empty after this call returns. */ public void clear() &#123; m.clear(); &#125; /** * Adds all of the elements in the specified collection to this set. * * @param c collection containing elements to be added to this set * @return &#123;@code true&#125; if this set changed as a result of the call * @throws ClassCastException if the elements provided cannot be compared * with the elements currently in the set * @throws NullPointerException if the specified collection is null or * if any element is null and this set uses natural ordering, or * its comparator does not permit null elements */ public boolean addAll(Collection&lt;? extends E&gt; c) &#123; // Use linear-time version if applicable if (m.size()==0 &amp;&amp; c.size() &gt; 0 &amp;&amp; c instanceof SortedSet &amp;&amp; m instanceof TreeMap) &#123; SortedSet&lt;? extends E&gt; set = (SortedSet&lt;? extends E&gt;) c; TreeMap&lt;E,Object&gt; map = (TreeMap&lt;E, Object&gt;) m; Comparator&lt;?&gt; cc = set.comparator(); Comparator&lt;? super E&gt; mc = map.comparator(); if (cc==mc || (cc != null &amp;&amp; cc.equals(mc))) &#123; map.addAllForTreeSet(set, PRESENT); return true; &#125; &#125; return super.addAll(c); &#125; ①向TreeSet中添加的数据，要求是同一个类的对象，不能添加不同类的对象。 ②两种排序方式：自然排序和定制排序 ③自然排序中，比较两个对象是否相同的标准：compareTo()返回0，不再是equals()； 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172public static void main(String[] args) &#123; TreeSet&lt;Object&gt; treeSet = new TreeSet&lt;&gt;(); treeSet.add(123); treeSet.add(456); treeSet.add(789); treeSet.add(456); System.out.println(treeSet);//[123, 456, 789] System.out.println(&quot;*****************************************&quot;); TreeSet&lt;Object&gt; set = new TreeSet&lt;&gt;(); set.add(new User(&quot;Tom&quot;,22)); set.add(new User(&quot;Jerry&quot;,24)); set.add(new User(&quot;BeiBei&quot;,21)); set.add(new User(&quot;DongDong&quot;,20)); set.add(new User(&quot;MM&quot;,18)); //set.add(new Integer(123)); Iterator&lt;Object&gt; iterator = set.iterator(); while (iterator.hasNext())&#123; System.out.println(iterator.next()); &#125; &#125;&#125;class User implements Comparable&#123; private String name; private int age; public User(String name, int age) &#123; this.name = name; this.age = age; &#125; public User() &#123; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; @Override public String toString() &#123; return &quot;User&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, age=&quot; + age + &#x27;&#125;&#x27;; &#125; @Override public int compareTo(Object o) &#123; //姓名从大到小，年龄从小到大 if (o instanceof User)&#123; User user= (User) o; int num= this.name.compareTo(user.name); if (num!=0)&#123; return -num; &#125;else&#123; return Integer.compare(this.age,user.age); &#125; &#125;else&#123; throw new RuntimeException(&quot;类型不一致！&quot;); &#125; &#125;&#125; TreeSet定制排序 ①new一个Comparator对象，重写compare方法 ②将Comparator对象传入TreeSet的构造器 ③添加对象时就会按照compare方法进行比较 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public static void main(String[] args) &#123; //照年龄从小到大排列 Comparator comparator = new Comparator()&#123; @Override public int compare(Object o1, Object o2) &#123; if (o1 instanceof Dog&amp;&amp; o2 instanceof Dog)&#123; Dog d1= (Dog) o1; Dog d2= (Dog) o2; return Integer.compare(d1.getAge(),d2.getAge()); &#125;else&#123; throw new RuntimeException(&quot;类型不一致！&quot;); &#125; &#125; &#125;; TreeSet&lt;Object&gt; set = new TreeSet&lt;&gt;(comparator); set.add(new Dog(&quot;Tom&quot;,22)); set.add(new Dog(&quot;Jerry&quot;,22)); set.add(new Dog(&quot;BeiBei&quot;,21)); set.add(new Dog(&quot;DongDong&quot;,20)); set.add(new Dog(&quot;MM&quot;,18)); set.add(123); //set.add(new Integer(123)); Iterator&lt;Object&gt; iterator = set.iterator(); while (iterator.hasNext())&#123; System.out.println(iterator.next()); &#125; &#125;&#125;class Dog&#123; private String name; private int age; public Dog() &#123; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public Dog(String name, int age) &#123; this.name = name; this.age = age; &#125; @Override public String toString() &#123; return &quot;Dog&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, age=&quot; + age + &#x27;&#125;&#x27;; &#125;&#125; 2.map接口map底层源码分析 123456789101112131415161718192021222324252627282930/*** Map:双列数据，用于存储具有键值对的数据，类似于函数的概念。 * 1.HashMap：作为map的主要实现类,线程不安全的，效率高。可以存储null的key或value。 * * *LinkedHashMap：HashMap的子类，保证在遍历map元素时，可以按照添加的顺序进行遍历。 * 原因：在原有hashMap底层的基础结构上，添加了一对指针，指向前一个和后一个元素。 *对于频繁的遍历操作，此类执行效率高于hashMap。 * * 2.TreeMap：可以按照添加的键值对进行排序，实现便利排序。按照key来排序。 *底层使用红黑树。 * * 3.HashTable：古老的实现类。jdk1.0.,线程安全的，效率低，不可以存储null的key或value。 * * *Properties：HashTable的子类。常用来处理配置文件。key和value都是String类型。 * * HashMap的底层：jdk7 数组加链表 * jdk8数组+链表+红黑树 * 面试题： * 1.hashMap的底层实现原理： * * 2.HashMap和HashTable的异同： * * 二：Map中key-value的理解： * 1.key不可重复（无序），value可以重复。 * 2.实际上放入map集合的是entry，entry有两个属性key和value。 * 3.entry无序不可重复。 * 4.key所在的类要重写hashcode（）和equals（）方法，针对于HashMap * 5.判断该元素存不存在，需要重写equals（）。*/ 支持自然排序和定制排序 123456789101112131415161718192021222324public interface Map&lt;K,V&gt; &#123;//里面定义了一个Entry接口interface Entry&lt;K,V&gt; &#123; public static &lt;K extends Comparable&lt;? super K&gt;, V&gt; Comparator&lt;Map.Entry&lt;K,V&gt;&gt; comparingByKey() &#123; return (Comparator&lt;Map.Entry&lt;K, V&gt;&gt; &amp; Serializable) (c1, c2) -&gt; c1.getKey().compareTo(c2.getKey()); &#125; public static &lt;K, V extends Comparable&lt;? super V&gt;&gt; Comparator&lt;Map.Entry&lt;K,V&gt;&gt; comparingByValue() &#123; return (Comparator&lt;Map.Entry&lt;K, V&gt;&gt; &amp; Serializable) (c1, c2) -&gt; c1.getValue().compareTo(c2.getValue()); &#125; public static &lt;K, V&gt; Comparator&lt;Map.Entry&lt;K, V&gt;&gt; comparingByKey(Comparator&lt;? super K&gt; cmp) &#123; Objects.requireNonNull(cmp); return (Comparator&lt;Map.Entry&lt;K, V&gt;&gt; &amp; Serializable) (c1, c2) -&gt; cmp.compare(c1.getKey(), c2.getKey()); &#125; public static &lt;K, V&gt; Comparator&lt;Map.Entry&lt;K, V&gt;&gt; comparingByValue(Comparator&lt;? super V&gt; cmp) &#123; Objects.requireNonNull(cmp); return (Comparator&lt;Map.Entry&lt;K, V&gt;&gt; &amp; Serializable) (c1, c2) -&gt; cmp.compare(c1.getValue(), c2.getValue()); &#125; &#125; HashMap底层源码分析123456789101112131415161718192021/*** 以jdk7为例说明： * HashMap&lt;Object, Object&gt; map = new HashMap&lt;&gt;();//实例化以后，底层创建了长度为16的一维数组Entry[]table。 * ....已经执行过多次put操作。。。。 * map.put(1,666);//首先，计算key1的hash值，此hash值经过某种算法计算，得到在entry数组的存放位置。 * 如果此位置的数据为空，此时的key1添加成功（成功一）；如果此位置的数据不为空（意味着此位置存在一个或者多个数据）， * 比较key1和已经存在的一个或多个数据的哈希值，如果key1的哈希值与已经存在的都不相同，此时添加成功（成功二）。 * 如果如果key1的哈希值与已经存在的某个数据（key2-value2）的哈希值相同，继续比较， * 调用key1所在类的equals（）方法，比较： * 如果equals（）返回false：添加成功（成功三）；如果返回true：使用value1替换value2. * 关于成功二和成功三： * 此时key1value1和原来的数据一链表的方式存储。 * 在不断的添加过程中，涉及到扩容问题，当超出临界值，且要存放的位置非空时，默认的扩容方式，扩容为原来容量的2倍，并将原有的数据复制过来。 * 在jdk8中的底层实现： * jdk8相比于底层实现方面的不同： * 1.new HashMap();底层没有创建一个长度为16的数组。 * 2.jdk8底层是Node【】，不再是Entry【】。 * 3.首次调用put方法时，底层创建长度为16的数组。 * 4.原来jdk7底层结构只有数组加链表，jdk8又加入了红黑树，当数组某一个索引位置上的元素以链表形式存在的 * 数据个数&gt;8且当前数组长度&gt;64时，此时此索引位置上的所有数据改为使用红黑树存储。 */ HashMap基于Map接口实现，元素以键值对的方式存储，并且允许使用null 建和null 值， 因为key不允许重复，因此只能有一个键为null,另外HashMap不能保证放入元素的顺序，它是无序的，和放入的顺序并不能相同。HashMap是线程不安全的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165public class HashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable &#123; static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; //默认初始化大小 16 static final float DEFAULT_LOAD_FACTOR = 0.75f; //负载因子0.75static final Entry&lt;?,?&gt;[] EMPTY_TABLE = &#123;&#125;; //初始化的默认数组transient int size; //HashMap中元素的数量int threshold; //判断是否需要调整HashMap的容量 //当数组总长度&gt;64,且单个节点的元素大于8个时，该节点的元素使用红黑树存储当删除该节点元素时，当该节点的元素小于6个，从二叉树变为指针 static final int TREEIFY_THRESHOLD = 8; static final int UNTREEIFY_THRESHOLD = 6; static final int MIN_TREEIFY_CAPACITY = 64; //底层使用指针 static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; V value; Node&lt;K,V&gt; next; Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125; //HashMap计算hash对key的hashcode进行了二次hash，以获得更好的散列值，然后对table数组长度取摸。 int hash = hash(key.hashCode());int i = indexFor(hash, table.length); static int hash(int h) &#123; // This function ensures that hashCodes that differ only by // constant multiples at each bit position have a bounded // number of collisions (approximately 8 at default load factor). h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4); &#125; static int indexFor(int h, int length) &#123; return h &amp; (length-1);//在该方法中，添加键值对时，首先进行table是否初始化的判断，如果没有进行初始化（分配空间，Entry[]数组的长度）。然后进行key是否为null的判断，如果key==null ,放置在Entry[]的0号位置。计算在Entry[]数组的存储位置，判断该位置上是否已有元素，如果已经有元素存在，则遍历该Entry[]数组位置上的单链表。判断key是否存在，如果key已经存在，则用新的value值，替换点旧的value值，并将旧的value值返回。如果key不存在于HashMap中，程序继续向下执行。将key-vlaue, 生成Entry实体，添加到HashMap中的Entry[]数组中。public V put(K key, V value) &#123; if (table == EMPTY_TABLE) &#123; //是否初始化 inflateTable(threshold); &#125; if (key == null) //放置在0号位置 return putForNullKey(value); int hash = hash(key); //计算hash值 int i = indexFor(hash, table.length); //计算在Entry[]中的存储位置 for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; modCount++; addEntry(hash, key, value, i); //添加到Map中 return null;&#125;添加到方法的具体操作，在添加之前先进行容量的判断，如果当前容量达到了阈值，并且需要存储到Entry[]数组中，先进性扩容操作，空充的容量为table长度的2倍。重新计算hash值，和数组存储的位置，扩容后的链表顺序与扩容前的链表顺序相反。然后将新添加的Entry实体存放到当前Entry[]位置链表的头部。在1.8之前，新插入的元素都是放在了链表的头部位置，但是这种操作在高并发的环境下容易导致死锁，所以1.8之后，新插入的元素都放在了链表的尾部。/* * hash hash值 * key 键值 * value value值 * bucketIndex Entry[]数组中的存储索引 * / void addEntry(int hash, K key, V value, int bucketIndex) &#123; if ((size &gt;= threshold) &amp;&amp; (null != table[bucketIndex])) &#123; resize(2 * table.length); //扩容操作，将数据元素重新计算位置后放入newTable中，链表的顺序与之前的顺序相反 hash = (null != key) ? hash(key) : 0; bucketIndex = indexFor(hash, table.length); &#125; createEntry(hash, key, value, bucketIndex);&#125;void createEntry(int hash, K key, V value, int bucketIndex) &#123; Entry&lt;K,V&gt; e = table[bucketIndex]; table[bucketIndex] = new Entry&lt;&gt;(hash, key, value, e); size++;&#125; HashMap里面实现一个静态内部类Entry，其重要的属性有 hash，key，value，next。HashMap里面用到链式数据结构的一个概念。上面我们提到过Entry类里面有一个next属性，作用是指向下一个Entry。打个比方， 第一个键值对A进来，通过计算其key的hash得到的index=0，记做:Entry[0] = A。一会后又进来一个键值对B，通过计算其index也等于0，现在怎么办？HashMap会这样做:B.next = A,Entry[0] = B,如果又进来C,index也等于0,那么C.next = B,Entry[0] = C；这样index=0的地方其实存取了A,B,C三个键值对,他们通过next这个属性链接在一起。也就是说数组中存储的是最后插入的元素。 void addEntry(int hash, K key, V value, int bucketIndex) &#123; Entry&lt;K,V&gt; e = table[bucketIndex]; table[bucketIndex] = new Entry&lt;K,V&gt;(hash, key, value, e); //参数e, 是Entry.next //如果size超过threshold，则扩充table大小。再散列 if (size++ &gt;= threshold) resize(2 * table.length);&#125;//添加方法精讲public V put(K key, V value) &#123; //调用putVal()方法完成 return putVal(hash(key), key, value, false, true);&#125; final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; //判断table是否初始化，否则初始化操作 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; //计算存储的索引位置，如果没有元素，直接赋值 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; //节点若已经存在，执行赋值操作 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; //判断链表是否是红黑树 else if (p instanceof TreeNode) //红黑树对象操作 e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; //为链表， for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); //链表长度8，将链表转化为红黑树存储 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; //key存在，直接覆盖 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; //记录修改次数 ++modCount; //判断是否需要扩容 if (++size &gt; threshold) resize(); //空操作 afterNodeInsertion(evict); return null;&#125; LinkedHashMap在LinkedHashMap中，是通过双联表的结构来维护节点的顺序的。每个节点都进行了双向的连接，维持插入的顺序（默认）。head指向第一个插入的节点，tail指向最后一个节点。 LinkedHashMap是HashMap的亲儿子，直接继承HashMap类。LinkedHashMap中的节点元素为Entry，直接继承HashMap.Node。 12345678910在HashMap类的put方法中，新建节点是使用的newNode方法。而在LinkedHashMap没有重写父类的put方法，而是重写了newNode方法来构建自己的节点对象。 Node&lt;K,V&gt; newNode(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; return new Node&lt;&gt;(hash, key, value, next); &#125; Node&lt;K,V&gt; newNode(int hash, K key, V value, Node&lt;K,V&gt; e) &#123; LinkedHashMap.Entry&lt;K,V&gt; p = new LinkedHashMap.Entry&lt;K,V&gt;(hash, key, value, e); linkNodeLast(p); return p; &#125; LinkedHashMap相对于HashMap，增加了双链表的结果（即节点中增加了前后指针），其他处理逻辑与HashMap一致，同样也没有锁保护，多线程使用存在风险。 Map接口中定义的方法1123456789101112public static void main(String[] args) &#123; HashMap&lt;Object, Object&gt; map = new HashMap&lt;&gt;(); map.put(1, 666);//添加 map.putAll(map);//添加一个集合 map.remove(1);//通过key移除 map.clear();//移除集合中所元素 map.size();//集合大小 map.get(1);//通过key获取值 map.containsKey(1);//判断是否包含指定key map.containsValue(666);//判断是否包含指定value map.isEmpty();//判断是否为空 map.equals(map);//判断当前map和参数Object是否相等 Map中的常用方法二123456789101112131415161718192021222324public static void main(String[] args) &#123; HashMap&lt;Object, Object&gt; map = new HashMap&lt;&gt;(); map.put(1,666); map.put(2,888); map.put(3,555); //遍历所的key：keySet Set&lt;Object&gt; set = map.keySet(); Iterator&lt;Object&gt; iterator = set.iterator(); while (iterator.hasNext())&#123; System.out.println(iterator.next()); &#125; //遍历所的value：values Collection&lt;Object&gt; collection = map.values(); Iterator&lt;Object&gt; iterator1 = collection.iterator(); while (iterator1.hasNext())&#123; System.out.println(iterator1.next()); &#125; //遍历所的key，value:entrySet Set&lt;Map.Entry&lt;Object, Object&gt;&gt; entries = map.entrySet(); Iterator&lt;Map.Entry&lt;Object, Object&gt;&gt; iterator2 = entries.iterator(); while (iterator2.hasNext())&#123; System.out.println(iterator2.next()); &#125;&#125; HashTable123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124public class Hashtable&lt;K,V&gt; extends Dictionary&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, java.io.Serializable &#123;属性：table：为一个Entry[]数组类型，Entry代表了“拉链”的节点，每一个Entry代表了一个键值对，哈希表的&quot;key-value键值对&quot;都是存储在Entry数组中的。 count：HashTable的大小，注意这个大小并不是HashTable的容器大小，而是他所包含Entry键值对的数量。 threshold：Hashtable的阈值，用于判断是否需要调整Hashtable的容量。threshold的值=&quot;容量*加载因子&quot;。 loadFactor：加载因子。 modCount：用来实现“fail-fast”机制的（也就是快速失败）。所谓快速失败就是在并发集合中，其进行迭代操作时，若有其他线程对其进行结构性的修改，这时迭代器会立马感知到，并且立即抛出ConcurrentModificationException异常，而不是等到迭代完成之后才告诉你（你已经出错了） 构造方法： 1.默认构造函数，容量为11，加载因子为0.75： public Hashtable() &#123; this(11, 0.75f); &#125; 2. public Hashtable(int initialCapacity) &#123; this(initialCapacity, 0.75f); &#125; 3. public Hashtable(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException(&quot;Illegal Capacity: &quot;+ initialCapacity); if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(&quot;Illegal Load: &quot;+loadFactor); if (initialCapacity==0) initialCapacity = 1; this.loadFactor = loadFactor; table = new Entry&lt;?,?&gt;[initialCapacity]; threshold = (int)Math.min(initialCapacity * loadFactor, MAX_ARRAY_SIZE + 1); &#125; 重要方法 public synchronized V get(Object key) &#123; Entry&lt;?,?&gt; tab[] = table; int hash = key.hashCode(); int index = (hash &amp; 0x7FFFFFFF) % tab.length; for (Entry&lt;?,?&gt; e = tab[index] ; e != null ; e = e.next) &#123; if ((e.hash == hash) &amp;&amp; e.key.equals(key)) &#123; return (V)e.value; &#125; &#125; return null; &#125; public synchronized V put(K key, V value) &#123; // Make sure the value is not null if (value == null) &#123; throw new NullPointerException(); &#125; // Makes sure the key is not already in the hashtable. Entry&lt;?,?&gt; tab[] = table; int hash = key.hashCode(); int index = (hash &amp; 0x7FFFFFFF) % tab.length;//计算出索引 @SuppressWarnings(&quot;unchecked&quot;) //遍历该数组 Entry&lt;K,V&gt; entry = (Entry&lt;K,V&gt;)tab[index]; for(; entry != null ; entry = entry.next) &#123; if ((entry.hash == hash) &amp;&amp; entry.key.equals(key)) &#123; V old = entry.value; entry.value = value; return old; &#125; &#125; addEntry(hash, key, value, index); return null; &#125; private void addEntry(int hash, K key, V value, int index) &#123; modCount++; Entry&lt;?,?&gt; tab[] = table; if (count &gt;= threshold) &#123; // Rehash the table if the threshold is exceeded rehash(); tab = table; hash = key.hashCode(); index = (hash &amp; 0x7FFFFFFF) % tab.length; &#125; // Creates the new entry. @SuppressWarnings(&quot;unchecked&quot;) Entry&lt;K,V&gt; e = (Entry&lt;K,V&gt;) tab[index]; tab[index] = new Entry&lt;&gt;(hash, key, value, e); count++; &#125; //扩容 protected void rehash() &#123; int oldCapacity = table.length; Entry&lt;?,?&gt;[] oldMap = table; // overflow-conscious code int newCapacity = (oldCapacity &lt;&lt; 1) + 1;2倍+1 if (newCapacity - MAX_ARRAY_SIZE &gt; 0) &#123; if (oldCapacity == MAX_ARRAY_SIZE) // Keep running with MAX_ARRAY_SIZE buckets return; newCapacity = MAX_ARRAY_SIZE; &#125; Entry&lt;?,?&gt;[] newMap = new Entry&lt;?,?&gt;[newCapacity]; modCount++; threshold = (int)Math.min(newCapacity * loadFactor, MAX_ARRAY_SIZE + 1); table = newMap; for (int i = oldCapacity ; i-- &gt; 0 ;) &#123; for (Entry&lt;K,V&gt; old = (Entry&lt;K,V&gt;)oldMap[i] ; old != null ; ) &#123; Entry&lt;K,V&gt; e = old; old = old.next; int index = (e.hash &amp; 0x7FFFFFFF) % newCapacity; e.next = (Entry&lt;K,V&gt;)newMap[index]; newMap[index] = e; &#125; &#125; &#125; TreeMap12345678910public class TreeMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements NavigableMap&lt;K,V&gt;, Cloneable, java.io.Serializable&#123; private final Comparator&lt;? super K&gt; comparator; private transient Entry&lt;K,V&gt; root; public Comparator&lt;? super K&gt; comparator() &#123; return comparator; &#125; TreeMap两种添加方式的使用： 向treemap中添加数据，要求key必须是同一个类创建的对象，因为要按照类进行排序。 ①自然排序123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263 public static void main(String[] args) &#123; TreeMap&lt;Object, Object&gt; map = new TreeMap&lt;&gt;(); map.put(new User(&quot;尹会东&quot;,23),&quot;6666&quot;); map.put(new User(&quot;张贝贝&quot;,25),&quot;6666&quot;); map.put(new User(&quot;刘淼&quot;,23),&quot;6666&quot;); Set&lt;Map.Entry&lt;Object, Object&gt;&gt; set = map.entrySet(); Iterator&lt;Map.Entry&lt;Object, Object&gt;&gt; iterator = set.iterator(); while (iterator.hasNext())&#123; System.out.println(iterator.next()); &#125; &#125;&#125;class User implements Comparable&#123; private String name; private int age; public User(String name, int age) &#123; this.name = name; this.age = age; &#125; public User() &#123; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; @Override public String toString() &#123; return &quot;User&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, age=&quot; + age + &#x27;&#125;&#x27;; &#125; @Override public int compareTo(Object o) &#123; //姓名从大到小，年龄从小到大 if (o instanceof User)&#123; User user= (User) o; int num= this.name.compareTo(user.name); if (num!=0)&#123; return -num; &#125;else&#123; return Integer.compare(this.age,user.age); &#125; &#125;else&#123; throw new RuntimeException(&quot;类型不一致！&quot;); &#125; &#125;&#125; ②定制排序123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960 public static void main(String[] args) &#123; Comparator comparator = new Comparator() &#123; @Override public int compare(Object o1, Object o2) &#123; if (o1 instanceof Dog &amp;&amp; o2 instanceof Dog)&#123; Dog d1= (Dog) o1; Dog d2= (Dog) o2; return Integer.compare(d1.getAge(),d2.getAge()); &#125; throw new RuntimeException(&quot;类型不一致！&quot;); &#125; &#125;; TreeMap&lt;Object, Object&gt; map = new TreeMap&lt;&gt;(comparator); map.put(new Dog(&quot;尹会东&quot;,23),&quot;6666&quot;); map.put(new Dog(&quot;张贝贝&quot;,25),&quot;6666&quot;); map.put(new Dog(&quot;45646&quot;,56),&quot;888&quot;); Set&lt;Map.Entry&lt;Object, Object&gt;&gt; set = map.entrySet(); Iterator&lt;Map.Entry&lt;Object, Object&gt;&gt; iterator = set.iterator(); while (iterator.hasNext())&#123; System.out.println(iterator.next()); &#125; &#125;&#125;class Dog&#123; private String name; private int age; public Dog() &#123; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public Dog(String name, int age) &#123; this.name = name; this.age = age; &#125; @Override public String toString() &#123; return &quot;Dog&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, age=&quot; + age + &#x27;&#125;&#x27;; &#125;&#125; Properties1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public synchronized Object setProperty(String key, String value) &#123; return put(key, value); &#125;public synchronized void load(Reader reader) throws IOException &#123; load0(new LineReader(reader)); &#125; private void load0 (LineReader lr) throws IOException &#123; char[] convtBuf = new char[1024]; int limit; int keyLen; int valueStart; char c; boolean hasSep; boolean precedingBackslash; while ((limit = lr.readLine()) &gt;= 0) &#123; c = 0; keyLen = 0; valueStart = limit; hasSep = false; //System.out.println(&quot;line=&lt;&quot; + new String(lineBuf, 0, limit) + &quot;&gt;&quot;); precedingBackslash = false; while (keyLen &lt; limit) &#123; c = lr.lineBuf[keyLen]; //need check if escaped. if ((c == &#x27;=&#x27; || c == &#x27;:&#x27;) &amp;&amp; !precedingBackslash) &#123; valueStart = keyLen + 1; hasSep = true; break; &#125; else if ((c == &#x27; &#x27; || c == &#x27;\\t&#x27; || c == &#x27;\\f&#x27;) &amp;&amp; !precedingBackslash) &#123; valueStart = keyLen + 1; break; &#125; if (c == &#x27;\\\\&#x27;) &#123; precedingBackslash = !precedingBackslash; &#125; else &#123; precedingBackslash = false; &#125; keyLen++; &#125; while (valueStart &lt; limit) &#123; c = lr.lineBuf[valueStart]; if (c != &#x27; &#x27; &amp;&amp; c != &#x27;\\t&#x27; &amp;&amp; c != &#x27;\\f&#x27;) &#123; if (!hasSep &amp;&amp; (c == &#x27;=&#x27; || c == &#x27;:&#x27;)) &#123; hasSep = true; &#125; else &#123; break; &#125; &#125; valueStart++; &#125; String key = loadConvert(lr.lineBuf, 0, keyLen, convtBuf); String value = loadConvert(lr.lineBuf, valueStart, limit - valueStart, convtBuf); put(key, value); &#125; &#125; Properties处理配置文件1234567891011121314151617181920public static void main(String[] args) &#123; Properties prop=new Properties(); FileInputStream fileInputStream=null; try &#123; fileInputStream = new FileInputStream(&quot;jdbc.properties&quot;); prop.load(fileInputStream); String name=prop.getProperty(&quot;user&quot;); System.out.println(name); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;finally &#123; try &#123; if (fileInputStream!=null)&#123; fileInputStream.close(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 3.collections工具类Collections工具类：操作set，map，list的工具类 面试题：Collection和Collections的区别：1234567891011121314public static void main(String[] args) &#123; //排序 ArrayList&lt;Object&gt; list = new ArrayList&lt;&gt;(); list.add(new Integer(1)); list.add(new Integer(2)); list.add(new Integer(3)); Collections.reverse(list);//反转list本身 Collections.shuffle(list);//随机化处理 // Collections.sort(list);//升序排序 Collections.swap(list,1,2);//交换两处位置的元素 Collections.frequency(list,1);//指定元素出现的次数 //同步控制 List&lt;Object&gt; list1 = Collections.synchronizedList(list);//返回的list1就是线程安全的list&#125; 十，泛型jdk1.5新特性泛型 把元素的类型设计成一个参数，这个参数类型叫做泛型。 为什么要使用泛型？ 1.类型无限制，类型不安全。 2.类型强制转换时，容易出现异常。 ClassCastException 123456集合中使用泛型集合接口或类在jdk5.0都修改为带泛型的结构。在实例化集合类时，可以指明泛型的类型。指明完以后，在集合类或接口中，凡是定义接口或类时，内部结构使用到类的泛型位置，都指定为实例化的泛型。泛型的类型必须是一个类。，使用基本数据类型时，需要转换为包装类。如果实例化时未指明泛型，默认为Object类型。 1234567891011121314151617181920212223242526public static void main(String[] args) &#123; ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;(); list.add(99); list.add(100); list.add(88); for (Integer integer:list)&#123; System.out.print(integer+&quot; &quot;); &#125; System.out.println(); System.out.println(&quot;-------------------------------------------------------&quot;); Iterator&lt;Integer&gt; iterator = list.iterator(); while (iterator.hasNext())&#123; System.out.print(iterator.next()+&quot; &quot;); &#125; System.out.println(&quot;-------------------------------------------------------&quot;); HashMap&lt;String, Integer&gt; map = new HashMap&lt;&gt;(); map.put(&quot;第一个&quot;,123); map.put(&quot;第二个&quot;,456); map.put(&quot;第个&quot;,789); Set&lt;Map.Entry&lt;String, Integer&gt;&gt; set = map.entrySet(); Iterator&lt;Map.Entry&lt;String, Integer&gt;&gt; iterator1 = set.iterator(); while (iterator1.hasNext())&#123; System.out.print(iterator1.next()+&quot; &quot;); &#125; System.out.println(&quot;-------------------------------------------------------------&quot;);&#125; 自定义泛型结构：泛型类，接口，方法12345678910111213141516171819/*** 泛型类被某个类继承： * ①public class SubOrder extends Order&lt;Integer&gt;//此时子类时普通类 * 由于子类在继承带泛型的父类时，指明了泛型类型，则实例化子类对象时，不再需要指明泛型。 * ②public class SubOrder1&lt;T&gt; extends Order&lt;T&gt;//此时子类也是泛型类 * * 自定义泛型的注意点 * ①泛型不同的引用不能相互赋值 * ArrayList&lt;Integer&gt;list1=null; * ArrayList&lt;String&gt;list2=null; * 此时list1和list2不能相互赋值。 * ②类型推断 * Order&lt;String&gt; order1 = new Order&lt;&gt;(); * ③静态方法中不能使用类的泛型 * * 泛型方法：在方法中出现了泛型的结构，泛型参数与类的泛型参数没有任何关系。 * 换句话说，泛型方法所属的类是不是泛型类都没有关系。 * 泛型方法可以声明为静态的。原因：泛型参数是在调用方法时确定的。并非在实例化类时确定。*/ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108/** * @author yinhuidong * @createTime 2020-04-10-15:23 * 案例需求： * 一个学生类包含两个属性：String类型的name和不确定类型的score * 老师一登记成绩：优秀，良好，及格 * 老师二登记成绩：89.5,100.。。。 * 老师三登记成绩：A,B,C， * 需求二： * 一个指定类型的学生类 */public class Test1 &#123; @Test public void test1() &#123; //实例化子类对象时，指明带泛型的类型 //泛型不指定就相当于默认Object类型 //如果泛型结构是借口或抽象类，不可以实例化对象 //静态方法中不能使用类的泛型，原因：类的泛型是在对象实例化时指定的，而静态方法是在类加载时加载的 //异常类不能声明为泛型类 //jdk1.8类型推断 Student&lt;String&gt; student = new Student&lt;&gt;(&quot;张三&quot;, &quot;优秀&quot;); Student&lt;Double&gt; student1 = new Student&lt;&gt;(&quot;李四&quot;, 90.5); Student&lt;Character&gt; student2 = new Student&lt;&gt;(&quot;王五&quot;, &#x27;A&#x27;); &#125; @Test public void test2() &#123; //由于子类在继承带泛型的父类时，指明了带泛型的类型，则实例化子类对象时，不再需要指明泛型。 Student2 student = new Student2(&quot;zhangsan&quot;, &quot;优秀&quot;); &#125; @Test public void test3()&#123; Integer[] arr = new Integer[4]; arr[0]=1; arr[1]=2; arr[2]=3; arr[3]=4; Student2 student = new Student2(); //泛型方法在调用时指明泛型参数的类型 List&lt;Integer&gt; list = student.copy(arr); for (Integer i:list)&#123; System.out.println(i); &#125; &#125;&#125;class Student&lt;T&gt; &#123; private String name; private T score; public Student() &#123; &#125; public Student(String name, T score) &#123; this.name = name; this.score = score; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public T getScore() &#123; return score; &#125; public void setScore(T score) &#123; this.score = score; &#125; @Override public String toString() &#123; return &quot;Student&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, score=&quot; + score + &#x27;&#125;&#x27;; &#125;&#125;class Student2 extends Student&lt;String&gt; &#123; public Student2() &#123; &#125; public Student2(String name, String score) &#123; super(name, score); &#125; /** * 泛型方法： * 在方法中出现了泛型结构，泛型参数与类的泛型参数没有任何关系 * 泛型方法可以声明为static，原因：泛型参数是在调用方法时确定的，而不是在实例化类时确定的 */ public &lt;E&gt; List&lt;E&gt; copy(E[] arr) &#123; ArrayList&lt;E&gt; list = new ArrayList&lt;&gt;(); for (E e : arr) &#123; list.add(e); &#125; return list; &#125;&#125; 通配符的使用类A是类B的父类，G和G是没有关系的，二者共有的父类是G&lt;?&gt;。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/** * @author yinhuidong * @createTime 2020-04-10-15:57 * 1.泛型在继承方面的提现 * * 2.通配符的使用 */public class Test2 &#123; /** * 泛型在继承方面的提现 * 类A是类B的父类， * G&lt;A&gt;和G&lt;B&gt;不具有子父类关系 * 二者是并列关系,二者公共的父类时G&lt;?&gt; * * 类A是类B的父类，A&lt;G&gt;是B&lt;G&gt;的子父类 */ @Test public void test1()&#123; List&lt;Object&gt; list1=null; List&lt;Integer&gt; list2=null; //此时的list1和list2类型不具有子父类关系 //list1=list2; &#125; /** * 通配符的使用 * ？ * 有限制条件的通配符的使用 * &lt;? extends Person&gt; ？代表的类型必须是Person的子类 * &lt;? implement PersonDao&gt; ？代表的类型必须实现了PersonDao接口 * &lt;? super Person&gt; ？代表的类型必须是Person或Person的父类 */ public void show(List&lt;?&gt; list)&#123; //list.add(1); //此时list&lt;?&gt;并不能再添加数据（null除外） list.add(null); Iterator&lt;?&gt; iterator = list.iterator(); while (iterator.hasNext())&#123; System.out.println(iterator.next()); &#125; //读取数据：允许，返回类型为Object类型 Object o = list.get(0); &#125; @Test public void test2()&#123; ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;(); list.add(1); list.add(2); list.add(3); show(list); &#125;&#125;","categories":[{"name":"JAVA基础","slug":"JAVA基础","permalink":"https://yinhuidong.github.io/categories/JAVA%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"JAVA基础","slug":"JAVA基础","permalink":"https://yinhuidong.github.io/tags/JAVA%E5%9F%BA%E7%A1%80/"}]},{"title":"Elasticsearch优化&总结","slug":"ElasticSearch/Elasticsearch 优化与小结","date":"2022-01-12T00:19:55.241Z","updated":"2022-01-12T00:19:55.241Z","comments":true,"path":"2022/01/12/ElasticSearch/Elasticsearch 优化与小结/","link":"","permalink":"https://yinhuidong.github.io/2022/01/12/ElasticSearch/Elasticsearch%20%E4%BC%98%E5%8C%96%E4%B8%8E%E5%B0%8F%E7%BB%93/","excerpt":"","text":"第一章 Elasticsearch 优化1.1 硬件选择Elasticsearch 的基础是 Lucene,所有的索引和文档数据是存储在本地的磁盘中,具体的路径可在 ES 的配置文件../config/elasticsearch.yml 中配置,如下: 123456789101112#----------------------------------- Paths------------------------------------## Path to directory where to store the data (separate multiple locations by comma):##path.data: /path/to/data## Path to log files:##path.logs: /path/to/logs# 磁盘在现代服务器上通常都是瓶颈。Elasticsearch 重度使用磁盘,你的磁盘能处理的吞吐量越大,你的节点就越稳定。这里有一些优化磁盘 I/O 的技巧: 使用 SSD。就像其他地方提过的, 他们比机械磁盘优秀多了。 使用 RAID 0。条带化 RAID 会提高磁盘 I/O,代价显然就是当一块硬盘故障时整个就故障了。不要使用镜像或者奇偶校验 RAID 因为副本已经提供了这个功能。 另外,使用多块硬盘,并允许 Elasticsearch 通过多个 path.data 目录配置把数据条带化分配到它们上面。 不要使用远程挂载的存储,比如 NFS 或者 SMB/CIFS。这个引入的延迟对性能来说完全是背道而驰的。1.2 分片策略1.2.1 合理设置分片数 分片和副本的设计为 ES 提供了支持分布式和故障转移的特性,但并不意味着分片和副本是可以无限分配的。而且索引的分片完成分配后由于索引的路由机制,我们是不能重新修改分片数的。​ 可能有人会说,我不知道这个索引将来会变得多大,并且过后我也不能更改索引的大小,所以为了保险起见,还是给它设为 1000 个分片吧。但是需要知道的是,一个分片并不是没有代价的。需要了解: 一个分片的底层即为一个 Lucene 索引,会消耗一定文件句柄、内存、以及 CPU 运转。 每一个搜索请求都需要命中索引中的每一个分片,如果每一个分片都处于不同的节点还好, 但如果多个分片都需要在同一个节点上竞争使用相同的资源就有些糟糕了。 用于计算相关度的词项统计信息是基于分片的。如果有许多分片,每一个都只有很少的数据会导致很低的相关度。 ​ 一个业务索引具体需要分配多少分片可能需要架构师和技术人员对业务的增长有个预先的判断,横向扩展应当分阶段进行。为下一阶段准备好足够的资源。 只有当你进入到下一个阶段,你才有时间思考需要作出哪些改变来达到这个阶段。一般来说,我们遵循一些原则: 控制每个分片占用的硬盘容量不超过 ES 的最大 JVM 的堆空间设置(一般设置不超过 32G,参考下文的 JVM 设置原则),因此,如果索引的总容量在 500G 左右,那分片大小在 16 个左右即可;当然,最好同时考虑原则 2。 考虑一下 node 数量,一般一个节点有时候就是一台物理机,如果分片数过多,大大超过了节点数,很可能会导致一个节点上存在多个分片,一旦该节点故障,即使保持了 1 个以上的副本,同样有可能会导致数据丢失,集群无法恢复。所以, 一般都设置分片数不超过节点数的 3 倍。 主分片,副本和节点最大数之间数量,我们分配的时候可以参考以下关系:节点数&lt;=主分片数*(副本数+1) ​ 1.2.2 推迟分片分配对于节点瞬时中断的问题,默认情况,集群会等待一分钟来查看节点是否会重新加入,如果这个节点在此期间重新加入,重新加入的节点会保持其现有的分片数据,不会触发新的分片分配。这样就可以减少 ES 在自动再平衡可用分片时所带来的极大开销。​ 通过修改参数 delayed_timeout ,可以延长再均衡的时间,可以全局设置也可以在索引级别进行修改: 123456PUT /_all/_settings&#123; &quot;settings&quot;: &#123; &quot;index.unassigned.node_left.delayed_timeout&quot;: &quot;5m&quot; &#125;&#125; 1.3 路由选择当我们查询文档的时候, Elasticsearch 如何知道一个文档应该存放到哪个分片中呢?它其实是通过下面这个公式来计算出来:shard = hash(routing) % number_of_primary_shards​ routing 默认值是文档的 id,也可以采用自定义值,比如用户 id。​ 不带 routing 查询在查询的时候因为不知道要查询的数据具体在哪个分片上,所以整个过程分为 2 个步骤： 分发:请求到达协调节点后,协调节点将查询请求分发到每个分片上。 聚合: 协调节点搜集到每个分片上查询结果,在将查询的结果进行排序,之后给用户返回结果。 带 routing 查询查询的时候,可以直接根据 routing 信息定位到某个分配查询,不需要查询所有的分配,经过协调节点排序。​ 向上面自定义的用户查询,如果 routing 设置为 userid 的话,就可以直接查询出数据来,效率提升很多。 1.4 写入速度优化ES 的默认配置,是综合了数据可靠性、写入速度、搜索实时性等因素。实际使用时,我们需要根据公司要求,进行偏向性的优化。​ 针对于搜索性能要求不高,但是对写入要求较高的场景,我们需要尽可能的选择恰当写优化策略。综合来说,可以考虑以下几个方面来提升写索引的性能: 加大 Translog Flush ,目的是降低 Iops、Writeblock。 增加 Index Refresh 间隔,目的是减少 Segment Merge 的次数。 调整 Bulk 线程池和队列。 优化节点间的任务分布。 优化 Lucene 层的索引建立,目的是降低 CPU 及 IO。1.4.1 批量数据提交ES 提供了 Bulk API 支持批量操作,当我们有大量的写任务时,可以使用 Bulk 来进行批量写入。​ 通用的策略如下:Bulk 默认设置批量提交的数据量不能超过 100M。数据条数一般是根据文档的大小和服务器性能而定的,但是单次批处理的数据大小应从 5MB~15MB 逐渐增加,当性能没有提升时,把这个数据量作为最大值。​ 1.4.2 优化存储设备ES 是一种密集使用磁盘的应用,在段合并的时候会频繁操作磁盘,所以对磁盘要求较高,当磁盘速度提升之后,集群的整体性能会大幅度提高。 1.4.3 合理使用合并Lucene 以段的形式存储数据。当有新的数据写入索引时,Lucene 就会自动创建一个新的段。​ 随着数据量的变化,段的数量会越来越多,消耗的多文件句柄数及 CPU 就越多,查询效率就会下降。​ 由于 Lucene 段合并的计算量庞大,会消耗大量的 I/O,所以 ES 默认采用较保守的策略,让后台定期进行段合并​ 1.4.4 减少 Refresh 的次数Lucene 在新增数据时,采用了延迟写入的策略,默认情况下索引的 refresh_interval 为1 秒。​ Lucene 将待写入的数据先写到内存中,超过 1 秒(默认)时就会触发一次 Refresh,然后 Refresh 会把内存中的的数据刷新到操作系统的文件缓存系统中。​ 如果我们对搜索的实效性要求不高,可以将 Refresh 周期延长,例如 30 秒。​ 这样还可以有效地减少段刷新次数,但这同时意味着需要消耗更多的 Heap 内存。​ 1.4.5 加大Flush设置Flush 的主要目的是把文件缓存系统中的段持久化到硬盘,当 Translog 的数据量达到512MB 或者 30 分钟时,会触发一次 Flush。​ index.translog.flush_threshold_size 参数的默认值是 512MB,我们进行修改。​ 增加参数值意味着文件缓存系统中可能需要存储更多的数据,所以我们需要为操作系统的文件缓存系统留下足够的空间。 1.4.6 减少副本的数量ES 为了保证集群的可用性,提供了 Replicas(副本)支持,然而每个副本也会执行分析、索引及可能的合并过程,所以 Replicas 的数量会严重影响写索引的效率。​ 当写索引时,需要把写入的数据都同步到副本节点,副本节点越多,写索引的效率就越慢。​ 如果我们需要大批量进行写入操作,可以先禁止Replica 复 制 , 设 置index.number_of_replicas: 0 关闭副本。在写入完成后,Replica 修改回正常的状态。​ 1.5 内存设置ES 默认安装后设置的内存是 1GB,对于任何一个现实业务来说,这个设置都太小了。如果是通过解压安装的 ES,则在 ES 安装文件中包含一个 jvm.option 文件,添加如下命令来设置 ES 的堆大小, Xms 表示堆的初始大小, Xmx 表示可分配的最大内存,都是 1GB。​ 确保 Xmx 和 Xms 的大小是相同的,其目的是为了能够在 Java 垃圾回收机制清理完堆区后不需要重新分隔计算堆区的大小而浪费资源,可以减轻伸缩堆大小带来的压力。​ 假设你有一个 64G 内存的机器,按照正常思维思考,你可能会认为把 64G 内存都给ES 比较好,但现实是这样吗, 越大越好?虽然内存对 ES 来说是非常重要的,但是答案是否定的!​ 因为 ES 堆内存的分配需要满足以下两个原则: 不要超过物理内存的 50%:Lucene 的设计目的是把底层 OS 里的数据缓存到内存中。 Lucene 的段是分别存储到单个文件中的,这些文件都是不会变化的,所以很利于缓存,同时操作系统也会把这些段文件缓存起来,以便更快的访问。如果我们设置的堆内存过大,Lucene 可用的内存将会减少,就会严重影响降低 Lucene 的全文本查询性能。 堆内存的大小最好不要超过 32GB:在 Java 中,所有对象都分配在堆上,然后有一个 Klass Pointer 指针指向它的类元数据。 这个指针在 64 位的操作系统上为 64 位,64 位的操作系统可以使用更多的内存(2^64)。在 32 位的系统上为 32位,32 位的操作系统的最大寻址空间为 4GB(2^32)。但是 64 位的指针意味着更大的浪费,因为你的指针本身大了。浪费内存不算,更糟糕的是,更大的指针在主内存和缓存器(例如 LLC, L1 等)之间移动数据的时候,会占用更多的带宽。​ 最终我们都会采用 31 G 设置-Xms 31g-Xmx 31g假设你有个机器有 128 GB 的内存,你可以创建两个节点,每个节点内存分配不超过 32 GB。 也就是说不超过 64 GB 内存给 ES 的堆内存,剩下的超过 64 GB 的内存给 Lucene。 1.6 重要参数 参数名 参数值 说明 cluster.name elasticsearch 配置 ES 的集群名称,默认值是 ES,建议改成与所存数据相关的名称, ES 会自动发现在同一网段下的集群名称相同的节点 ​ node.name node-1 集群中的节点名,在同一个集群中不能重复。节点的名称一旦设置,就不能再改变了。当然,也可以 设 置 成 服 务 器 的 主 机 名 称 , 例 如 node.name:${HOSTNAME}。 ​ node.master true 指定该节点是否有资格被选举成为 Master 节点,默认是 True,如果被设置为 True,则只是有资格成为 Master 节点,具体能否成为 Master 节点,需要通 过选举产生。 ​ node.data true 指定该节点是否存储索引数据,默认为 True。数据的增、删、改、查都是在 Data 节点完成的。 ​ index.number_of_shards 1 设置都索引分片个数,默认是 1 片。也可以在创建索引时设置该值,具体设置为多大都值要根据数据 量的大小来定。如果数据量不大,则设置成 1 时效 率最高 ​ index.number_of_replicas 1 设置默认的索引副本个数,默认为 1 个。副本数越多,集群的可用性越好,但是写索引时需要同步的 数据越多。 ​ transport.tcp.compress true 设置在节点间传输数据时是否压缩,默认为 False,不压缩 ​ discovery.zen.minimum_master_nodes 1 设置在选举 Master 节点时需要参与的最少的候选主节点数,默认为 1。如果使用默认值,则当网络 不稳定时有可能会出现脑裂。 合 理 的 数 值 为 (master_eligible_nodes/2)+1 , 其 中 master_eligible_nodes 表示集群中的候选主节点数 ​ discovery.zen.ping.timeout 3s 设置在集群中自动发现其他节点时 Ping 连接的超时时间,默认为 3 秒。 在较差的网络环境下需要设置得大一点,防止因误 判该节点的存活状态而导致分片的转移 ​ 第二章 Elasticsearch小结2.1 为什么要使用 Elasticsearch?系统中的数据,随着业务的发展,时间的推移,将会非常多,而业务中往往采用模糊查询进行数据的搜索,而模糊查询会导致查询引擎放弃索引,导致系统查询数据时都是全表扫描,在百万级别的数据库中,查询效率是非常低下的,而我们使用 ES 做一个全文索引,将经常查询的系统功能的某些字段,比如说电商系统的商品表中商品名,描述、价格还有 id 这些字段我们放入 ES 索引库里,可以提高查询速度。 2.2 Elasticsearch 的 master 选举流程? Elasticsearch 的选主是 ZenDiscovery 模块负责的,主要包含 Ping (节点之间通过这个 RPC 来发现彼此)和 Unicast(单播模块包含一个主机列表以控制哪些节点需要 ping 通)这两部分 对所有可以成为 master 的节点(node.master: true)根据 nodeId 字典排序,每次选举每个节点都把自己所知道节点排一次序,然后选出第一个(第 0 位)节点,暂且认为它是 master 节点。 如果对某个节点的投票数达到一定的值(可以成为 master 节点数 n/2+1)并且该节点自己也选举自己,那这个节点就是 master。否则重新选举一直到满足上述条件。 master 节点的职责主要包括集群、节点和索引的管理,不负责文档级别的管理; data 节点可以关闭 http功能。 2.3 Elasticsearch 集群脑裂问题?“脑裂”问题可能的成因: 网络问题:集群间的网络延迟导致一些节点访问不到 master,认为 master 挂掉了从而选举出新的master,并对 master 上的分片和副本标红,分配新的主分片 节点负载:主节点的角色既为 master 又为 data,访问量较大时可能会导致 ES 停止响应造成大面积延迟,此时其他节点得不到主节点的响应认为主节点挂掉了,会重新选取主节点。 内存回收:data 节点上的 ES 进程占用的内存较大,引发 JVM 的大规模内存回收,造成 ES 进程失去响应。 ​ 脑裂问题解决方案: 减少误判:discovery.zen.ping_timeout 节点状态的响应时间,默认为 3s,可以适当调大,如果 master在 该 响 应 时 间 的 范 围 内 没 有 做 出 响 应 应 答 , 判 断 该 节 点 已 经 挂 掉 了 。 调 大 参 数 ( 如 6s ,discovery.zen.ping_timeout:6),可适当减少误判。 选举触发: discovery.zen.minimum_master_nodes:1 该参数是用于控制选举行为发生的最小集群主节点数量。当备选主节点的个数大于等于该参数的值,且备选主节点中有该参数个节点认为主节点挂了,进行选举。官方建议为(n/2)+1,n 为主节点个数(即有资格成为主节点的节点个数)。 角色分离:即 master 节点与 data 节点分离,限制角色 主节点配置为:node.master: true node.data: false从节点配置为:node.master: false node.data: true 2.4 Elasticsearch 索引文档的流程? 协调节点默认使用文档 ID 参与计算(也支持通过 routing),以便为路由提供合适的分片:shard = hash(document_id) % (num_of_primary_shards) 当分片所在的节点接收到来自协调节点的请求后,会将请求写入到 Memory Buffer,然后定时(默认是每隔 1 秒)写入到 Filesystem Cache, 这个从 Memory Buffer 到 Filesystem Cache 的过程就叫做 refresh; 当然在某些情况下,存在 Momery Buffer 和 Filesystem Cache 的数据可能会丢失,ES 是通过 translog的机制来保证数据的可靠性的。其实现机制是接收到请求后,同时也会写入到 translog 中,当 Filesystem cache 中的数据写入到磁盘中时,才会清除掉,这个过程叫做 flush; 在 flush 过程中,内存中的缓冲将被清除,内容被写入一个新段,段的 fsync 将创建一个新的提交点,并将内容刷新到磁盘,旧的 translog 将被删除并开始一个新的 translog。 flush 触发的时机是定时触发(默认 30 分钟)或者 translog 变得太大(默认为 512M)时; 2.5 Elasticsearch 更新和删除文档的流程? 删除和更新也都是写操作,但是 Elasticsearch 中的文档是不可变的,因此不能被删除或者改动以展示其变更; 磁盘上的每个段都有一个相应的.del 文件。当删除请求发送后,文档并没有真的被删除,而是在.del文件中被标记为删除。该文档依然能匹配查询,但是会在结果中被过滤掉。当段合并时,在.del 文件中被标记为删除的文档将不会被写入新段。 在新的文档被创建时,Elasticsearch 会为该文档指定一个版本号,当执行更新时,旧版本的文档在.del文件中被标记为删除,新版本的文档被索引到一个新段。旧版本的文档依然能匹配查询,但是会在结果中被过滤掉。 ​ 2.6 Elasticsearch 搜索的流程?​ 搜索被执行成一个两阶段过程,我们称之为 Query Then Fetch; 在初始查询阶段时,查询会广播到索引中每一个分片拷贝(主分片或者副本分片)。 每个分片在本地执行搜索并构建一个匹配文档的大小为 from + size 的优先队列。PS:在搜索的时候是会查询Filesystem Cache 的,但是有部分数据还在 Memory Buffer,所以搜索是近实时的。 每个分片返回各自优先队列中 所有文档的 ID 和排序值 给协调节点,它合并这些值到自己的优先队列中来产生一个全局排序后的结果列表。 接下来就是取回阶段,协调节点辨别出哪些文档需要被取回并向相关的分片提交多个 GET 请求。每个分片加载并丰富文档,如果有需要的话,接着返回文档给协调节点。一旦所有的文档都被取回了,协调节点返回结果给客户端。 Query Then Fetch 的搜索类型在文档相关性打分的时候参考的是本分片的数据,这样在文档数量较少的时候可能不够准确,DFS Query Then Fetch 增加了一个预查询的处理,询问 Term 和 Documentfrequency,这个评分更准确,但是性能会变差。 ​ 2.7 Elasticsearch 在部署时,对 Linux 的设置有哪些优化方法? 64 GB 内存的机器是非常理想的,但是 32 GB 和 16 GB 机器也是很常见的。少于 8 GB 会适得其反。 如果你要在更快的 CPUs 和更多的核心之间选择,选择更多的核心更好。多个内核提供的额外并发远胜过稍微快一点点的时钟频率。 如果你负担得起 SSD,它将远远超出任何旋转介质。 基于 SSD 的节点,查询和索引性能都有提升。如果你负担得起,SSD 是一个好的选择。 即使数据中心们近在咫尺,也要避免集群跨越多个数据中心。绝对要避免集群跨越大的地理距离。 请确保运行你应用程序的 JVM 和服务器的 JVM 是完全一样的。 在 Elasticsearch 的几个地方,使用 Java 的本地序列化。 通过设置 gateway.recover_after_nodes、 gateway.expected_nodes、 gateway.recover_after_time 可以在集群重启的时候避免过多的分片交换,这可能会让数据恢复从数个小时缩短为几秒钟。 Elasticsearch 默认被配置为使用单播发现,以防止节点无意中加入集群。只有在同一台机器上运行的节点才会自动组成集群。最好使用单播代替组播。 不要随意修改垃圾回收器(CMS)和各个线程池的大小。 把你的内存的(少于)一半给 Lucene(但不要超过 32 GB!),通过 ES_HEAP_SIZE 环境变量设置。 内存交换到磁盘对服务器性能来说是致命的。如果内存交换到磁盘上,一个 100 微秒的操作可能变成 10 毫秒。 再想想那么多 10 微秒的操作时延累加起来。 不难看出 swapping 对于性能是多么可怕。 Lucene 使用了大量的文件。同时,Elasticsearch 在节点和 HTTP 客户端之间进行通信也使用了大量的套接字。 所有这一切都需要足够的文件描述符。你应该增加你的文件描述符,设置一个很大的值,如 64,000。 ​ 补充:索引阶段性能提升方法 使用批量请求并调整其大小:每次批量数据 5–15 MB 大是个不错的起始点。 存储:使用 SSD 段和合并:Elasticsearch 默认值是 20 MB/s,对机械磁盘应该是个不错的设置。如果你用的是 SSD,可以考虑提高到 100–200 MB/s。如果你在做批量导入,完全不在意搜索,你可以彻底关掉合并限流。另外还可以增加 index.translog.flush_threshold_size 设置,从默认的 512 MB 到更大一些的值,比如 1GB,这可以在一次清空触发的时候在事务日志里积累出更大的段。 如果你的搜索结果不需要近实时的准确度,考虑把每个索引的 index.refresh_interval 改到 30s。 如果你在做大批量导入,考虑通过设置 index.number_of_replicas: 0 关闭副本。 2.8 GC 方面,在使用 Elasticsearch 时要注意什么? 倒排词典的索引需要常驻内存,无法 GC,需要监控 data node 上 segment memory 增长趋势。 各类缓存,field cache, filter cache, indexing cache, bulk queue 等等,要设置合理的大小,并且要应该根据最坏的情况来看 heap 是否够用,也就是各类缓存全部占满的时候,还有 heap 空间可以分配给其他任务吗?避免采用 clear cache 等“自欺欺人”的方式来释放内存。 避免返回大量结果集的搜索与聚合。确实需要大量拉取数据的场景,可以采用 scan &amp; scroll api 来实现。 cluster stats 驻留内存并无法水平扩展,超大规模集群可以考虑分拆成多个集群通过 tribe node 连接。 想知道 heap 够不够,必须结合实际应用场景,并对集群的 heap 使用情况做持续的监控。 ​ 2.9 Elasticsearch 对于大数据量(上亿量级)的聚合如何实现?Elasticsearch 提供的首个近似聚合是 cardinality 度量。它提供一个字段的基数,即该字段的 distinct或者 unique 值的数目。它是基于 HLL 算法的。HLL 会先对我们的输入作哈希运算,然后根据哈希运算的结果中的 bits 做概率估算从而得到基数。其特点是:可配置的精度,用来控制内存的使用(更精确 = 更多内存);小的数据集精度是非常高的;我们可以通过配置参数,来设置去重需要的固定内存使用量。无论数千还是数十亿的唯一值,内存使用量只与你配置的精确度相关。​ 2.10 在并发情况下,Elasticsearch 如果保证读写一致? 可以通过版本号使用乐观并发控制,以确保新版本不会被旧版本覆盖,由应用层来处理具体的冲突; 另外对于写操作,一致性级别支持 quorum/one/all,默认为 quorum,即只有当大多数分片可用时才允许写操作。但即使大多数可用,也可能存在因为网络等原因导致写入副本失败,这样该副本被认为故障,分片将会在一个不同的节点上重建。 对于读操作,可以设置 replication 为 sync(默认),这使得操作在主分片和副本分片都完成后才会返回;如果设置 replication 为 async 时,也可以通过设置搜索请求参数_preference 为 primary 来查询主分片,确保文档是最新版本。2.11 如何监控 Elasticsearch 集群状态?elasticsearch-head 插件​ 通过 Kibana 监控 Elasticsearch。你可以实时查看你的集群健康状态和性能,也可以分析过去的集群、索引和节点指标​ 2.12 是否了解字典树?字典树又称单词查找树,Trie 树,是一种树形结构,是一种哈希树的变种。典型应用是用于统计,排序和保存大量的字符串(但不仅限于字符串),所以经常被搜索引擎系统用于文本词频统计。它的优点是:利用字符串的公共前缀来减少查询时间,最大限度地减少无谓的字符串比较,查询效率比哈希树高。​ Trie 的核心思想是空间换时间,利用字符串的公共前缀来降低查询时间的开销以达到提高效率的目的。它有 3 个基本性质:​ 根节点不包含字符,除根节点外每一个节点都只包含一个字符。 从根节点到某一节点,路径上经过的字符连接起来,为该节点对应的字符串。 每个节点的所有子节点包含的字符都不相同。 ​ 对于中文的字典树,每个节点的子节点用一个哈希表存储,这样就不用浪费太大的空间,而且查询速度上可以保留哈希的复杂度 O(1)。 2.13 Elasticsearch 中的集群、节点、索引、文档、类型是什么? 集群是一个或多个节点(服务器)的集合,它们共同保存您的整个数据,并提供跨所有节点的联合索引和搜索功能。群集由唯一名称标识,默认情况下为“elasticsearch”。此名称很重要,因为如果节点设置为按名称加入群集,则该节点只能是群集的一部分。 节点是属于集群一部分的单个服务器。它存储数据并参与群集索引和搜索功能。 索引就像关系数据库中的“数据库”。它有一个定义多种类型的映射。索引是逻辑名称空间,映射到一个或多个主分片,并且可以有零个或多个副本分片。 MySQL =&gt;数据库 Elasticsearch =&gt;索引 文档类似于关系数据库中的一行。不同之处在于索引中的每个文档可以具有不同的结构(字段),但是对于通用字段应该具有相同的数据类型。 MySQL =&gt; Databases =&gt; Tables =&gt; Columns / Rows Elasticsearch =&gt; Indices =&gt; Types =&gt;具有属性的文档 类型是索引的逻辑类别/分区,其语义完全取决于用户。2.14 Elasticsearch 中的倒排索引是什么?倒排索引是搜索引擎的核心。搜索引擎的主要目标是在查找发生搜索条件的文档时提供快速搜索。ES中的倒排索引其实就是 lucene 的倒排索引,区别于传统的正向索引,倒排索引会再存储数据时将关键词和数据进行关联,保存到倒排表中,然后查询时,将查询内容进行分词后在倒排表中进行查询,最后匹配数据即可。​ ​ ​ ​ ​ ​","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"https://yinhuidong.github.io/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"https://yinhuidong.github.io/tags/Elasticsearch/"}]},{"title":"Elasticsearch集群&原理","slug":"ElasticSearch/ElasticSearch集群与原理","date":"2022-01-12T00:19:55.240Z","updated":"2022-01-12T00:19:55.241Z","comments":true,"path":"2022/01/12/ElasticSearch/ElasticSearch集群与原理/","link":"","permalink":"https://yinhuidong.github.io/2022/01/12/ElasticSearch/ElasticSearch%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%8E%9F%E7%90%86/","excerpt":"","text":"第一章 Elasticsearch 环境1.1 相关概念1.1.1 单机和集群单台 Elasticsearch 服务器提供服务,往往都有最大的负载能力,超过这个阈值,服务器性能就会大大降低甚至不可用,所以生产环境中,一般都是运行在指定服务器集群中。​ 除了负载能力,单点服务器也存在其他问题:​ 单台机器存储容量有限 单服务器容易出现单点故障,无法实现高可用 单服务的并发处理能力有限 ​ 配置服务器集群时,集群中节点数量没有限制,大于等于 2 个节点就可以看做是集群了。一般出于高性能及高可用方面来考虑集群中节点数量都是 3 个以上。​ 1.1.2 集群Cluster一个集群就是由一个或多个服务器节点组织在一起,共同持有整个的数据,并一起提供索 引 和 搜 索 功 能 。 一 个 Elasticsearch 集 群 有 一 个 唯 一 的 名 字 标 识 , 这 个 名 字 默 认 就是”elasticsearch”。这个名字是重要的,因为一个节点只能通过指定某个集群的名字,来加入这个集群。 1.1.3 节点 Node集群中包含很多服务器,一个节点就是其中的一个服务器。作为集群的一部分,它存储数据,参与集群的索引和搜索功能。​ 一个节点也是由一个名字来标识的,默认情况下,这个名字是一个随机的漫威漫画角色的名字,这个名字会在启动的时候赋予节点。这个名字对于管理工作来说挺重要的,因为在这个管理过程中,你会去确定网络中的哪些服务器对应于 Elasticsearch 集群中的哪些节点。​ 一个节点可以通过配置集群名称的方式来加入一个指定的集群。默认情况下,每个节点都会被安排加入到一个叫做“elasticsearch”的集群中,这意味着,如果你在你的网络中启动了若干个节点,并假定它们能够相互发现彼此,它们将会自动地形成并加入到一个叫做“elasticsearch”的集群中。​ 在一个集群里,只要你想,可以拥有任意多个节点。而且,如果当前你的网络中没有运行任何 Elasticsearch 节点,这时启动一个节点,会默认创建并加入一个叫做“elasticsearch”的集群。 1.2 Linux单机1.2.1 软件下载软件下载地址 1.2.2 软件安装1）解压软件1234# 解压缩tar -zxvf elasticsearch-7.8.0-linux-x86_64.tar.gz -C /opt/module# 改名mv elasticsearch-7.8.0 es 2）创建用户因为安全问题,Elasticsearch 不允许 root 用户直接运行,所以要创建新用户,在 root中创建新用户。 1234useradd es #新增 es 用户passwd es #为 es 用户设置密码userdel -r es #如果错了,可以删除再加chown -R es:es /opt/module/es #文件夹所有者 3）修改配置文件修改/opt/module/es/config/elasticsearch.yml 文件​ 1234567# 加入如下配置cluster.name: elasticsearchnode.name: node-1network.host: 0.0.0.0http.port: 9200cluster.initial_master_nodes:[&quot;node-1&quot;] 修改/etc/security/limits.conf 1234# 在文件末尾中增加下面内容# 每个进程可以打开的文件数的限制es soft nofile 65536es hard nofile 65536 修改/etc/security/limits.d/20-nproc.conf 12345678# 在文件末尾中增加下面内容# 每个进程可以打开的文件数的限制es soft nofile 65536es hard nofile 65536# 操作系统级别对每个用户创建的进程数的限制* hard nproc 4096# 注:* 带表 Linux 所有用户名称 修改/etc/sysctl.conf 123# 在文件中增加下面内容# 一个进程可以拥有的 VMA(虚拟内存区域)的数量,默认值为 65536vm.max_map_count=655360 重新加载 1sysctl -p 1.2.3 启动软件使用 ES 用户启动 12345cd /opt/module/es/#启动bin/elasticsearch#后台启动bin/elasticsearch -d 启动时,会动态生成文件,如果文件所属用户不匹配,会发生错误,需要重新进行修改用户和用户组。 1.3 Linux集群修改/opt/module/es/config/elasticsearch.yml 文件,分发文件 1234567891011121314151617181920212223242526272829# 加入如下配置#集群名称cluster.name: cluster-es#节点名称,每个节点的名称不能重复node.name: node-1#ip 地址,每个节点的地址不能重复network.host: linux1#是不是有资格主节点node.master: truenode.data: truehttp.port: 9200# head 插件需要这打开这两个配置http.cors.allow-origin: &quot;*&quot;http.cors.enabled: truehttp.max_content_length: 200mb#es7.x 之后新增的配置,初始化一个新的集群时需要此配置来选举 mastercluster.initial_master_nodes: [&quot;node-1&quot;]#es7.x 之后新增的配置,节点发现discovery.seed_hosts: [&quot;linux1:9300&quot;,&quot;linux2:9300&quot;,&quot;linux3:9300&quot;]gateway.recover_after_nodes: 2network.tcp.keep_alive: truenetwork.tcp.no_delay: truetransport.tcp.compress: true#集群内同时启动的数据任务个数,默认是 2 个cluster.routing.allocation.cluster_concurrent_rebalance: 16#添加或删除节点及负载均衡时并发恢复的线程个数,默认 4 个cluster.routing.allocation.node_concurrent_recoveries: 16#初始化数据恢复时,并发恢复线程的个数,默认 4 个cluster.routing.allocation.node_initial_primaries_recoveries: 16 其余修改同单机版 第二章 Elasticsearch 原理2.1 核心概念2.1.1 索引-index一个索引就是一个拥有几分相似特征的文档的集合。比如说,你可以有一个客户数据的索引,另一个产品目录的索引,还有一个订单数据的索引。一个索引由一个名字来标识(必须全部是小写字母),并且当我们要对这个索引中的文档进行索引、搜索、更新和删除的时候,都要使用到这个名字。在一个集群中,可以定义任意多的索引。​ 能搜索的数据必须索引,这样的好处是可以提高查询速度,比如:新华字典前面的目录就是索引的意思,目录可以提高查询速度。​ Elasticsearch 索引的精髓:一切设计都是为了提高搜索的性能。​ 2.1.2 类型-type在一个索引中,你可以定义一种或多种类型。​ 一个类型是你的索引的一个逻辑上的分类/分区,其语义完全由你来定。通常,会为具有一组共同字段的文档定义一个类型。不同的版本,类型发生了不同的变化 版本 Type 5.x 支持多种 type 6.x 只能有一种 type 7.x 默认不再支持自定义索引类型(默认类型为:_doc) 2.1.3 文档-document一个文档是一个可被索引的基础信息单元,也就是一条数据。​ 比如:你可以拥有某一个客户的文档,某一个产品的一个文档,当然,也可以拥有某个订单的一个文档。文档以 JSON(Javascript Object Notation)格式来表示,而 JSON 是一个到处存在的互联网数据交互格式。​ 在一个 index/type 里面,你可以存储任意多的文档。 2.1.4 字段-field相当于是数据表的字段,对文档数据根据不同属性进行的分类标识。​ 2.1.5 映射-mappingmapping 是处理数据的方式和规则方面做一些限制,如:某个字段的数据类型、默认值、分析器、是否被索引等等。这些都是映射里面可以设置的,其它就是处理 ES 里面数据的一些使用规则设置也叫做映射,按着最优规则处理数据对性能提高很大,因此才需要建立映射,并且需要思考如何建立映射才能对性能更好。​ 2.1.6 分片-shards​ 一个索引可以存储超出单个节点硬件限制的大量数据。比如,一个具有 10 亿文档数据的索引占据 1TB 的磁盘空间,而任一节点都可能没有这样大的磁盘空间。或者单个节点处理搜索请求,响应太慢。为了解决这个问题,Elasticsearch 提供了将索引划分成多份的能力,每一份就称之为分片。当你创建一个索引的时候,你可以指定你想要的分片的数量。每个分片本身也是一个功能完善并且独立的“索引”,这个“索引”可以被放置到集群中的任何节点上。​ 分片很重要,主要有两方面的原因:​ 1)允许你水平分割 / 扩展你的内容容量。​ 2)允许你在分片之上进行分布式的、并行的操作,进而提高性能/吞吐量。​ 至于一个分片怎样分布,它的文档怎样聚合和搜索请求,是完全由 Elasticsearch 管理的,对于作为用户的你来说,这些都是透明的,无需过分关心。​ 一个 Lucene 索引 我们在 Elasticsearch 称作 分片 。 一个Elasticsearch 索引 是分片的集合。 当 Elasticsearch 在索引中搜索的时候, 他发送查询到每一个属于索引的分片(Lucene 索引),然后合并每个分片的结果到一个全局的结果集。​ 2.1.7 副本-replicas在一个网络 / 云的环境里,失败随时都可能发生,在某个分片/节点不知怎么的就处于离线状态,或者由于任何原因消失了,这种情况下,有一个故障转移机制是非常有用并且是强烈推荐的。为此目的,Elasticsearch 允许你创建分片的一份或多份拷贝,这些拷贝叫做复制分片(副本)。​ 复制分片之所以重要,有两个主要原因:​ 在分片/节点失败的情况下,提供了高可用性。因为这个原因,注意到复制分片从不与原/主要(original/primary)分片置于同一节点上是非常重要的。 ​ 扩展你的搜索量/吞吐量,因为搜索可以在所有的副本上并行运行。 ​ ​ 总之,每个索引可以被分成多个分片。一个索引也可以被复制 0 次(意思是没有复制)或多次。一旦复制了,每个索引就有了主分片(作为复制源的原来的分片)和复制分片(主分片的拷贝)之别。分片和复制的数量可以在索引创建的时候指定。在索引创建之后,你可以在任何时候动态地改变复制的数量,但是你事后不能改变分片的数量。默认情况下,Elasticsearch 中的每个索引被分片 1 个主分片和 1 个复制,这意味着,如果你的集群中至少有两个节点,你的索引将会有 1 个主分片和另外 1 个复制分片(1 个完全拷贝),这样的话每个索引总共就有 2 个分片,我们需要根据索引需要确定分片个数。​ 2.1.8 分配-allocation​ 将分片分配给某个节点的过程,包括分配主分片或者副本。如果是副本,还包含从主分片复制数据的过程。这个过程是由 master 节点完成的。​ 2.2 系统架构 ![图片.png](https://cdn.nlark.com/yuque/0/2021/png/12610368/1622993459664-996a2783-008b-46cc-bfd1-7ea646850fc4.png#clientId=u4f593c09-99e9-4&amp;from=paste&amp;height=334&amp;id=ua5111f41&amp;margin=%5Bobject%20Object%5D&amp;name=%E5%9B%BE%E7%89%87.png&amp;originHeight=357&amp;originWidth=520&amp;originalType=binary&amp;ratio=2&amp;size=67183&amp;status=done&amp;style=none&amp;taskId=u373e9bd6-fac5-4d98-b672-818743d1a70&amp;width=486) 一个运行中的 Elasticsearch 实例称为一个节点,而集群是由一个或者多个拥有相同cluster.name 配置的节点组成, 它们共同承担数据和负载的压力。当有节点加入集群中或者从集群中移除节点时,集群将会重新平均分布所有的数据。​ 当一个节点被选举成为主节点时, 它将负责管理集群范围内的所有变更,例如增加、删除索引,或者增加、删除节点等。 而主节点并不需要涉及到文档级别的变更和搜索等操作,所以当集群只拥有一个主节点的情况下,即使流量的增加它也不会成为瓶颈。 任何节点都可以成为主节点。​ 作为用户,我们可以将请求发送到集群中的任何节点 ,包括主节点。 每个节点都知道任意文档所处的位置,并且能够将我们的请求直接转发到存储我们所需文档的节点。 无论我们将请求发送到哪个节点,它都能负责从各个包含我们所需文档的节点收集回数据,并将最终结果返回給客户端。 Elasticsearch 对这一切的管理都是透明的。​ 2.3 分布式集群2.3.1 单节点集群在包含一个空节点的集群内创建名为 users 的索引，分配 3个主分片和一份副本(每个主分片拥有一个副本分片)。​ 123456&#123; &quot;settings&quot;: &#123; &quot;number_of_shards&quot;: 3, &quot;number_of_replicas&quot;: 1 &#125;&#125; 我们的集群现在是拥有一个索引的单节点集群。所有 3 个主分片都被分配在 node-1 。​ 通过 elasticsearch-head 插件查看集群情况：​ 当前我们的集群是正常运行的,但是在硬件故障时有丢失数据的风险。 2.3.2 故障转移当集群中只有一个节点在运行时,意味着会有一个单点故障问题——没有冗余。 幸运的是,我们只需再启动一个节点即可防止数据丢失。当你在同一台机器上启动了第二个节点时,只要它和第一个节点有同样的 cluster.name 配置,它就会自动发现集群并加入到其中。但是在不同机器上启动节点的时候,为了加入到同一集群,你需要配置一个可连接到的单播主机列表。之所以配置为使用单播发现,以防止节点无意中加入集群。只有在同一台机器上运行的节点才会自动组成集群。​ 如果启动了第二个节点,我们的集群将会拥有两个节点的集群 : 所有主分片和副本分片都已被分配。​ ![1.png](https://cdn.nlark.com/yuque/0/2021/png/12610368/1622994042673-a59cc334-924d-45e9-b5ee-f6ce003d4ea5.png#clientId=u4f593c09-99e9-4&amp;from=ui&amp;id=u04da1742&amp;margin=%5Bobject%20Object%5D&amp;name=1.png&amp;originHeight=165&amp;originWidth=488&amp;originalType=binary&amp;ratio=2&amp;size=19549&amp;status=done&amp;style=none&amp;taskId=u5661fa35-6e0b-4b90-8ac6-f7622e7a44d) 通过 elasticsearch-head 插件查看集群情况：​ 2.3.3 水平扩容怎样为我们的正在增长中的应用程序按需扩容呢?当启动了第三个节点,我们的集群将会拥有三个节点的集群 : 为了分散负载而对分片进行重新分配。通过 elasticsearch-head 插件查看集群情况：但是如果我们想要扩容超过 6 个节点怎么办呢?​ 主分片的数目在索引创建时就已经确定了下来。实际上,这个数目定义了这个索引能够存储 的最大数据量。(实际大小取决于你的数据、硬件和使用场景。) 但是,读操作——搜索和返回数据——可以同时被主分片 或 副本分片所处理,所以当你拥有越多的副本分片时,也将拥有越高的吞吐量。​ 在运行中的集群上是可以动态调整副本分片数目的,我们可以按需伸缩集群。把副本数从默认的 1 增加到 2 123&#123; &quot;number_of_replicas&quot; : 2&#125; users 索引现在拥有 9 个分片:3 个主分片和 6 个副本分片。 这意味着我们可以将集群扩容到 9 个节点,每个节点上一个分片。相比原来 3 个节点时,集群搜索性能可以提升 3 倍。通过 elasticsearch-head 插件查看集群情况:当然,如果只是在相同节点数目的集群上增加更多的副本分片并不能提高性能,因为每个分片从节点上获得的资源会变少。 你需要增加更多的硬件资源来提升吞吐量。​ 但是更多的副本分片数提高了数据冗余量:按照上面的节点配置,我们可以在失去 2 个节点的情况下不丢失任何数据。 2.3.4 应对故障关闭第一个节点,这时集群的状态为:关闭了一个节点后的集群。​ 关闭的节点是一个主节点。而集群必须拥有一个主节点来保证正常工作,所以发生的第一件事情就是选举一个新的主节点: Node 2 。在我们关闭 Node 1 的同时也失去了主分片 1 和 2 ,并且在缺失主分片的时候索引也不能正常工作。 如果此时来检查集群的状况,我们看到的状态将会为 red :不是所有主分片都在正常工作。​ 幸运的是,在其它节点上存在着这两个主分片的完整副本, 所以新的主节点立即将这些分片在 Node 2 和 Node 3 上对应的副本分片提升为主分片, 此时集群的状态将会为yellow。这个提升主分片的过程是瞬间发生的,如同按下一个开关一般。​ 为什么我们集群状态是 yellow 而不是 green 呢?​ 虽然我们拥有所有的三个主分片,但是同时设置了每个主分片需要对应 2 份副本分片,而此时只存在一份副本分片。 所以集群不能为 green 的状态,不过我们不必过于担心:如果我们同样关闭了 Node 2 ,我们的程序 依然 可以保持在不丢任何数据的情况下运行,因为Node 3 为每一个分片都保留着一份副本。​ 如果我们重新启动 Node 1 ,集群可以将缺失的副本分片再次进行分配,那么集群的状态也将恢复成之前的状态。 如果 Node 1 依然拥有着之前的分片,它将尝试去重用它们,同时仅从主分片复制发生了修改的数据文件。和之前的集群相比,只是 Master 节点切换了。​ ​ 2.4 路由计算当索引一个文档的时候,文档会被存储到一个主分片中。 Elasticsearch 如何知道一个文档应该存放到哪个分片中呢?当我们创建文档时,它如何决定这个文档应当被存储在分片1 还是分片 2 中呢?首先这肯定不会是随机的,否则将来要获取文档的时候我们就不知道从何处寻找了。实际上,这个过程是根据下面这个公式决定的:​ shard=hash(routing)%number_of_primary_shards​ routing 是一个可变值,默认是文档的 _id ,也可以设置成一个自定义的值。 routing 通过hash 函数生成一个数字,然后这个数字再除以 number_of_primary_shards (主分片的数量)后得到余数 。这个分布在 0 到number_of_primary_shards-1 之间的余数,就是我们所寻求的文档所在分片的位置。​ 这就解释了为什么我们要在创建索引的时候就确定好主分片的数量 并且永远不会改变这个数量:因为如果数量变化了,那么所有之前路由的值都会无效,文档也再也找不到了。​ 所有的文档 API( get 、 index 、 delete 、 bulk 、 update 以及 mget )都接受一个叫做 routing 的路由参数 ,通过这个参数我们可以自定义文档到分片的映射。一个自定义的路由参数可以用来确保所有相关的文档——例如所有属于同一个用户的文档——都被存储到同一个分片中。​ 2.5 分片控制我们假设有一个集群由三个节点组成。 它包含一个叫 emps 的索引,有两个主分片,每个主分片有两个副本分片。相同分片的副本不会放在同一节点。通过 elasticsearch-head 插件查看集群情况,所以我们的集群是一个有三个节点和一个索引的集群。我们可以发送请求到集群中的任一节点。 每个节点都有能力处理任意请求。 每个节点都知道集群中任一文档位置,所以可以直接将请求转发到需要的节点上。 在下面的例子中,将所有的请求发送到 Node 1,我们将其称为 协调节点(coordinating node) 。​ 当发送请求的时候, 为了扩展负载,更好的做法是轮询集群中所有的节点。​ 2.5.1 写流程新建、索引和删除 请求都是 写 操作, 必须在主分片上面完成之后才能被复制到相关的副本分片。 新建,索引和删除文档所需要的步骤顺序: 客户端向 Node 1 发送新建、索引或者删除请求。 节点使用文档的 _id 确定文档属于分片 0 。请求会被转发到 Node 3,因为分片 0 的主分片目前被分配在 Node 3 上。 Node 3 在主分片上面执行请求。如果成功了,它将请求并行转发到 Node 1 和 Node 2的副本分片上。一旦所有的副本分片都报告成功, Node 3 将向协调节点报告成功,协调节点向客户端报告成功。 ​ 在客户端收到成功响应时,文档变更已经在主分片和所有副本分片执行完成,变更是安全的。有一些可选的请求参数允许您影响这个过程,可能以数据安全为代价提升性能。这些选项很少使用,因为 Elasticsearch 已经很快,但是为了完整起见,请参考下面表格:​ 参数 含义 consistency consistency,即一致性。在默认设置下,即使仅仅是在试图执行一个_写_操作之consistency前,主分片都会要求 必须要有 规定数量(quorum)(或者换种说法,也即必须要有大多数)的分片副本处于活跃可用状态,才会去执行_写_操作(其中分片副本可以是主分片或者副本分片)。这是为了避免在发生网络分区故障(network partition)的时候进行_写_操作,进而导致数据不一致。_规定数量_即:int( (primary + number_of_replicas) / 2 ) + 1 consistency 参数的值可以设为 one (只要主分片状态 ok 就允许执行_写_操作),all(必须要主分片和所有副本分片的状态没问题才允许执行_写_操作), 或quorum 。默认值为 quorum , 即大多数的分片副本状态没问题就允许执行_写_操作。 注意,规定数量 的计算公式中 number_of_replicas 指的是在索引设置中的设定副本分片数,而不是指当前处理活动状态的副本分片数。如果你的索引设置中指定了当前索引拥有三个副本分片,那规定数量的计算结果即:int( (primary + 3 replicas) / 2 ) + 1 = 3 如果此时你只启动两个节点,那么处于活跃状态的分片副本数量就达不到规定数量,也因此您将无法索引和删除任何文档。 ​ ​ || timeout | 如果没有足够的副本分片会发生什么? Elasticsearch 会等待,希望更多的分片出现。默认情况下,它最多等待 1 分钟。 如果你需要,你可以使用 timeout 参数使它更早终止: 100 100 毫秒,30s 是 30 秒。​ | 新索引默认有 1 个副本分片,这意味着为满足规定数量应该需要两个活动的分片副本。 但是,这些默认的设置会阻止我们在单一节点上做任何事情。为了避免这个问题,要求只有当 number_of_replicas 大于 1 的时候,规定数量才会执行。 2.5.5 读流程我们可以从主分片或者从其它任意副本分片检索文档 从主分片或者副本分片检索文档的步骤顺序: 客户端向 Node 1 发送获取请求。 节点使用文档的 _id 来确定文档属于分片 0 。分片 0 的副本分片存在于所有的三个节点上。 在这种情况下,它将请求转发到 Node 2 。 Node 2 将文档返回给 Node 1 ,然后将文档返回给客户端。 ​ 在处理读取请求时,协调结点在每次请求的时候都会通过轮询所有的副本分片来达到负载均衡。在文档被检索时,已经被索引的文档可能已经存在于主分片上但是还没有复制到副本分片。 在这种情况下,副本分片可能会报告文档不存在,但是主分片可能成功返回文档。 一旦索引请求成功返回给用户,文档在主分片和副本分片都是可用的。​ 2.5.3 更新流程部分更新一个文档结合了先前说明的读取和写入流程:​ ![1.png](https://cdn.nlark.com/yuque/0/2021/png/12610368/1623024623009-ca2242ca-ef00-4fb9-8e38-214aced99fbb.png#clientId=ubda5c093-cc76-4&amp;from=ui&amp;id=ubcbda964&amp;margin=%5Bobject%20Object%5D&amp;name=1.png&amp;originHeight=295&amp;originWidth=688&amp;originalType=binary&amp;ratio=2&amp;size=56178&amp;status=done&amp;style=none&amp;taskId=uefc2bf0c-99d1-45bb-a825-fe7bf923d19) 部分更新一个文档的步骤如下: 客户端向 Node 1 发送更新请求。 它将请求转发到主分片所在的 Node 3 。 Node 3 从主分片检索文档,修改 _source 字段中的 JSON ,并且尝试重新索引主分片的文档。如果文档已经被另一个进程修改,它会重试步骤 3 ,超过 retry_on_conflict 次后放弃。 如果 Node 3 成功地更新文档,它将新版本的文档并行转发到 Node 1 和 Node 2 上的副本分片,重新建立索引。一旦所有副本分片都返回成功, Node 3 向协调节点也返回成功,协调节点向客户端返回成功。 ​ 当主分片把更改转发到副本分片时, 它不会转发更新请求。 相反,它转发完整文档的新版本。请记住,这些更改将会异步转发到副本分片,并且不能保证它们以发送它们相同的顺序到达。 如果 Elasticsearch 仅转发更改请求,则可能以错误的顺序应用更改,导致得到损坏的文档。​ 2.5.4 多文档操作流程mget 和 bulk API 的模式类似于单文档模式。区别在于协调节点知道每个文档存在于哪个分片中。它将整个多文档请求分解成 每个分片 的多文档请求,并且将这些请求并行转发到每个参与节点。​ 协调节点一旦收到来自每个节点的应答,就将每个节点的响应收集整理成单个响应,返回给客户端。​ ![1.png](https://cdn.nlark.com/yuque/0/2021/png/12610368/1623024865729-26effe50-79f2-43f9-8e45-aed953fa51bc.png#clientId=ubda5c093-cc76-4&amp;from=ui&amp;id=ue0abbe02&amp;margin=%5Bobject%20Object%5D&amp;name=1.png&amp;originHeight=224&amp;originWidth=644&amp;originalType=binary&amp;ratio=2&amp;size=53325&amp;status=done&amp;style=none&amp;taskId=u93ed7c3a-2aa8-4bac-a35d-0cbb9b6d416) 用单个 mget 请求取回多个文档所需的步骤顺序:​ 客户端向 Node 1 发送 mget 请求。 Node 1 为每个分片构建多文档获取请求,然后并行转发这些请求到托管在每个所需的主分片或者副本分片的节点上。一旦收到所有答复, Node 1 构建响应并将其返回给客户端。 ​ 可以对 docs 数组中每个文档设置 routing 参数。​ bulk API, 允许在单个批量请求中执行多个创建、索引、删除和更新请求。​ ![1.png](https://cdn.nlark.com/yuque/0/2021/png/12610368/1623024954615-d04f5d7c-5a5b-4388-b04f-aafc82d4dd78.png#clientId=ubda5c093-cc76-4&amp;from=ui&amp;id=u9f023603&amp;margin=%5Bobject%20Object%5D&amp;name=1.png&amp;originHeight=250&amp;originWidth=547&amp;originalType=binary&amp;ratio=2&amp;size=42248&amp;status=done&amp;style=none&amp;taskId=ud39cf9c1-2196-4596-a720-f75d22a80b4) bulk API 按如下步骤顺序执行: 客户端向 Node 1 发送 bulk 请求。 Node 1 为每个节点创建一个批量请求,并将这些请求并行转发到每个包含主分片的节点主机。 主分片一个接一个按顺序执行每个操作。当每个操作成功时,主分片并行转发新文档(或删除)到副本分片,然后执行下一个操作。 一旦所有的副本分片报告所有操作成功,该节点将向协调节点报告成功,协调节点将这些响应收集整理并返回给客户端。2.6 分片原理分片是 Elasticsearch 最小的工作单元。但是究竟什么是一个分片,它是如何工作的?​ 传统的数据库每个字段存储单个值,但这对全文检索并不够。文本字段中的每个单词需要被搜索,对数据库意味着需要单个字段有索引多值的能力。最好的支持是一个字段多个值需求的数据结构是倒排索引。​ 2.6.1 倒排索引Elasticsearch 使用一种称为倒排索引的结构,它适用于快速的全文搜索。​ 见其名,知其意,有倒排索引,肯定会对应有正向索引。正向索引(forward index),反向索引(inverted index)更熟悉的名字是倒排索引。​ 所谓的正向索引,就是搜索引擎会将待搜索的文件都对应一个文件 ID,搜索时将这个ID 和搜索关键字进行对应,形成 K-V 对,然后对关键字进行统计计数。​ ![1.png](https://cdn.nlark.com/yuque/0/2021/png/12610368/1623025212673-946ce42f-d197-45d0-b729-4a4fcd0c7521.png#clientId=ubda5c093-cc76-4&amp;from=ui&amp;id=u732e4e3a&amp;margin=%5Bobject%20Object%5D&amp;name=1.png&amp;originHeight=210&amp;originWidth=483&amp;originalType=binary&amp;ratio=2&amp;size=70661&amp;status=done&amp;style=none&amp;taskId=u1bfc4bd6-0109-48e9-96b2-78323f947dd) ​ 但是互联网上收录在搜索引擎中的文档的数目是个天文数字,这样的索引结构根本无法满足实时返回排名结果的要求。所以,搜索引擎会将正向索引重新构建为倒排索引,即把文件ID 对应到关键词的映射转换为关键词到文件 ID 的映射,每个关键词都对应着一系列的文件,这些文件中都出现这个关键词。​ ![1.png](https://cdn.nlark.com/yuque/0/2021/png/12610368/1623025251544-d1171ecc-16cf-4669-9aab-22a0596d0836.png#clientId=ubda5c093-cc76-4&amp;from=ui&amp;id=u4d8a7ac1&amp;margin=%5Bobject%20Object%5D&amp;name=1.png&amp;originHeight=233&amp;originWidth=477&amp;originalType=binary&amp;ratio=2&amp;size=64846&amp;status=done&amp;style=none&amp;taskId=u8bd13f31-b65a-4284-8295-d9864474a22) 一个倒排索引由文档中所有不重复词的列表构成,对于其中每个词,有一个包含它的文档列表。例如,假设我们有两个文档,每个文档的 content 域包含如下内容: The quick brown fox jumped over the lazy dog Quick brown foxes leap over lazy dogs in summer ​ 为了创建倒排索引,我们首先将每个文档的 content 域拆分成单独的 词(我们称它为 词条或 tokens ),创建一个包含所有不重复词条的排序列表,然后列出每个词条出现在哪个文档。结果如下所示: 现在,如果我们想搜索 quick brown ,我们只需要查找包含每个词条的文档: 两个文档都匹配,但是第一个文档比第二个匹配度更高。如果我们使用仅计算匹配词条数量的简单相似性算法,那么我们可以说,对于我们查询的相关性来讲,第一个文档比第二个文档更佳。​ 目前的倒排索引有一些问题: Quick 和 quick 以独立的词条出现,然而用户可能认为它们是相同的词。 fox 和 foxes 非常相似, 就像 dog 和 dogs ;他们有相同的词根。 jumped 和 leap, 尽管没有相同的词根,但他们的意思很相近。他们是同义词。 ​ 使用前面的索引搜索 +Quick +fox 不会得到任何匹配文档。(记住,+ 前缀表明这个词必须存在。)只有同时出现 Quick 和 fox 的文档才满足这个查询条件,但是第一个文档包含quick fox ,第二个文档包含 Quick foxes 。​ 我们的用户可以合理的期望两个文档与查询匹配。我们可以做的更好。​ 如果我们将词条规范为标准模式,那么我们可以找到与用户搜索的词条不完全一致,但具有足够相关性的文档。例如: Quick 可以小写化为 quick 。 foxes 可以 词干提取 –变为词根的格式– 为 fox 。类似的, dogs 可以为提取为 dog 。 jumped 和 leap 是同义词,可以索引为相同的单词 jump 。 ​ 现在索引看上去像这样:​ ![1.png](https://cdn.nlark.com/yuque/0/2021/png/12610368/1623025513378-ce974175-0824-4575-9866-0b8a271a9d14.png#clientId=ubda5c093-cc76-4&amp;from=ui&amp;id=u95b8303d&amp;margin=%5Bobject%20Object%5D&amp;name=1.png&amp;originHeight=305&amp;originWidth=225&amp;originalType=binary&amp;ratio=2&amp;size=6630&amp;status=done&amp;style=none&amp;taskId=u68d7fdc7-4195-4e3b-9134-b1e1c92aebb) 这还远远不够。我们搜索 +Quick +fox 仍然 会失败,因为在我们的索引中,已经没有 Quick了。但是,如果我们对搜索的字符串使用与 content 域相同的标准化规则,会变成查询+quick +fox,这样两个文档都会匹配!分词和标准化的过程称为分析。 这非常重要。你只能搜索在索引中出现的词条,所以索引文本和查询字符串必须标准化为相同的格式。 2.6.2 文档搜索早期的全文检索会为整个文档集合建立一个很大的倒排索引并将其写入到磁盘。 一旦新的索引就绪,旧的就会被其替换,这样最近的变化便可以被检索到。​ 倒排索引被写入磁盘后是 不可改变 的:它永远不会修改。​ 不变性有重要的价值:​ 不需要锁。如果你从来不更新索引,你就不需要担心多进程同时修改数据的问题。 一旦索引被读入内核的文件系统缓存,便会留在哪里,由于其不变性。只要文件系统缓存中还有足够的空间,那么大部分读请求会直接请求内存,而不会命中磁盘。这提供了很大的性能提升。 其它缓存(像 filter 缓存),在索引的生命周期内始终有效。它们不需要在每次数据改变时被重建,因为数据不会变化。 写入单个大的倒排索引允许数据被压缩,减少磁盘 I/O 和 需要被缓存到内存的索引的使用量。 ​ 当然,一个不变的索引也有不好的地方。主要事实是它是不可变的! 你不能修改它。如果你需要让一个新的文档 可被搜索,你需要重建整个索引。这要么对一个索引所能包含的数据量造成了很大的限制,要么对索引可被更新的频率造成了很大的限制。​ 2.6.3 动态更新索引如何在保留不变性的前提下实现倒排索引的更新?​ 答案是: 用更多的索引。通过增加新的补充索引来反映新近的修改,而不是直接重写整个倒排索引。每一个倒排索引都会被轮流查询到,从最早的开始查询完后再对结果进行合并。​ Elasticsearch 基于 Lucene, 这个 java 库引入了按段搜索的概念。 每一 段 本身都是一个倒排索引, 但索引在 Lucene 中除表示所有段的集合外, 还增加了提交点的概念 — 一个列出了所有已知段的文件。​ ![1.png](https://cdn.nlark.com/yuque/0/2021/png/12610368/1623025848474-77e1c696-944f-45b9-bb5e-4186784ce6c6.png#clientId=ubda5c093-cc76-4&amp;from=ui&amp;id=uf66ec04b&amp;margin=%5Bobject%20Object%5D&amp;name=1.png&amp;originHeight=290&amp;originWidth=493&amp;originalType=binary&amp;ratio=2&amp;size=50707&amp;status=done&amp;style=none&amp;taskId=u924da882-dda0-4689-924a-f630496a4c7) 按段搜索会以如下流程执行: 新文档被收集到内存索引缓存 ![1.png](https://cdn.nlark.com/yuque/0/2021/png/12610368/1623025907767-dea7f0ca-574f-432b-b412-81c77d21a20f.png#clientId=ubda5c093-cc76-4&amp;from=ui&amp;id=uc7875160&amp;margin=%5Bobject%20Object%5D&amp;name=1.png&amp;originHeight=281&amp;originWidth=395&amp;originalType=binary&amp;ratio=2&amp;size=47524&amp;status=done&amp;style=none&amp;taskId=uabf22589-498a-4866-9fa4-7df353524ab) 不时地, 缓存被 提交 1）一个新的段—一个追加的倒排索引—被写入磁盘。2）一个新的包含新段名字的 提交点 被写入磁盘3）磁盘进行 同步 — 所有在文件系统缓存中等待的写入都刷新到磁盘,以确保它们被写入物理文件 新的段被开启,让它包含的文档可见以被搜索 内存缓存被清空,等待接收新的文档 ![1.png](https://cdn.nlark.com/yuque/0/2021/png/12610368/1623026080861-100a1950-51c8-445b-9180-566270e523fd.png#clientId=ubda5c093-cc76-4&amp;from=ui&amp;id=ucefb597b&amp;margin=%5Bobject%20Object%5D&amp;name=1.png&amp;originHeight=377&amp;originWidth=443&amp;originalType=binary&amp;ratio=2&amp;size=76786&amp;status=done&amp;style=none&amp;taskId=uf1f6d4c3-789a-4e6a-92d1-74b6a112fab) 当一个查询被触发,所有已知的段按顺序被查询。词项统计会对所有段的结果进行聚合,以保证每个词和每个文档的关联都被准确计算。 这种方式可以用相对较低的成本将新文档添加到索引。​ 段是不可改变的,所以既不能从把文档从旧的段中移除,也不能修改旧的段来进行反映文档的更新。 取而代之的是,每个提交点会包含一个 .del 文件,文件中会列出这些被删除文档的段信息。​ 当一个文档被 “删除” 时,它实际上只是在 .del 文件中被 标记 删除。一个被标记删除的文档仍然可以被查询匹配到, 但它会在最终结果被返回前从结果集中移除。文档更新也是类似的操作方式:当一个文档被更新时,旧版本文档被标记删除,文档的新版本被索引到一个新的段中。 可能两个版本的文档都会被一个查询匹配到,但被删除的那个旧版本文档在结果集返回前就已经被移除。​ 2.6.4 近实时搜索随着按段(per-segment)搜索的发展,一个新的文档从索引到可被搜索的延迟显著降低了。新文档在几分钟之内即可被检索,但这样还是不够快。磁盘在这里成为了瓶颈。提交(Commiting)一个新的段到磁盘需要一个 fsync 来确保段被物理性地写入磁盘,这样在断电的时候就不会丢失数据。 但是 fsync 操作代价很大; 如果每次索引一个文档都去执行一次的话会造成很大的性能问题。​ 我们需要的是一个更轻量的方式来使一个文档可被搜索,这意味着 fsync 要从整个过程中被移除。在Elasticsearch 和磁盘之间是文件系统缓存。 像之前描述的一样, 在内存索引缓冲区中的文档会被写入到一个新的段中。 但是这里新段会被先写入到文件系统缓存—这一步代价会比较低,稍后再被刷新到磁盘—这一步代价比较高。不过只要文件已经在缓存中,就可以像其它文件一样被打开和读取了。​ ![1.png](https://cdn.nlark.com/yuque/0/2021/png/12610368/1623026223455-6cd90d3d-fccb-4998-8eec-1cf20b28e79d.png#clientId=ubda5c093-cc76-4&amp;from=ui&amp;id=u9cdd96a0&amp;margin=%5Bobject%20Object%5D&amp;name=1.png&amp;originHeight=311&amp;originWidth=305&amp;originalType=binary&amp;ratio=2&amp;size=29985&amp;status=done&amp;style=none&amp;taskId=u9455fea8-0471-40c9-b10d-78952937383) Lucene 允许新段被写入和打开—使其包含的文档在未进行一次完整提交时便对搜索可见。这种方式比进行一次提交代价要小得多,并且在不影响性能的前提下可以被频繁地执行。 在 Elasticsearch 中,写入和打开一个新段的轻量的过程叫做 refresh 。 默认情况下每个分片会每秒自动刷新一次。这就是为什么我们说 Elasticsearch 是 近 实时搜索: 文档的变化并不是立即对搜索可见,但会在一秒之内变为可见。​ 这些行为可能会对新用户造成困惑: 他们索引了一个文档然后尝试搜索它,但却没有搜到。这个问题的解决办法是用 refresh API 执行一次手动刷新: /users/_refresh​ 尽管刷新是比提交轻量很多的操作,它还是会有性能开销。当写测试的时候, 手动刷新很有用,但是不要在生产环境下每次索引一个文档都去手动刷新。 相反,你的应用需要意识到 Elasticsearch 的近实时的性质,并接受它的不足。​ 并不是所有的情况都需要每秒刷新。可能你正在使用 Elasticsearch 索引大量的日志文件,你可能想优化索引速度而不是近实时搜索, 可以通过设置 refresh_interval , 降低每个索引的刷新频率。​ ​ 12345&#123; &quot;settings&quot;: &#123; &quot;refresh_interval&quot;: &quot;30s&quot; &#125;&#125; refresh_interval 可以在既存索引上进行动态更新。 在生产环境中,当你正在建立一个大的新索引时,可以先关闭自动刷新,待开始使用该索引时,再把它们调回来。​ 123456# 关闭自动刷新PUT /users/_settings&#123; &quot;refresh_interval&quot;: -1 &#125;# 每一秒刷新PUT /users/_settings&#123; &quot;refresh_interval&quot;: &quot;1s&quot; &#125; 2.6.5 持久化变更如果没有用 fsync 把数据从文件系统缓存刷(flush)到硬盘,我们不能保证数据在断电甚至是程序正常退出之后依然存在。为了保证 Elasticsearch 的可靠性,需要确保数据变化被持久化到磁盘。在 动态更新索引,我们说一次完整的提交会将段刷到磁盘,并写入一个包含所有段列表的提交点。 Elasticsearch 在启动或重新打开一个索引的过程中使用这个提交点来判断哪些段隶属于当前分片。​ 即使通过每秒刷新(refresh)实现了近实时搜索,我们仍然需要经常进行完整提交来确保能从失败中恢复。但在两次提交之间发生变化的文档怎么办?我们也不希望丢失掉这些数据。Elasticsearch 增加了一个 translog ,或者叫事务日志,在每一次对 Elasticsearch 进行操作时均进行了日志记录。​ 整个流程如下: 一个文档被索引之后,就会被添加到内存缓冲区,并且追加到了 translog ![1.png](https://cdn.nlark.com/yuque/0/2021/png/12610368/1623026691582-539f1980-35ea-4200-9431-7bf8fb562b10.png#clientId=ubda5c093-cc76-4&amp;from=ui&amp;id=u1d6c2983&amp;margin=%5Bobject%20Object%5D&amp;name=1.png&amp;originHeight=297&amp;originWidth=350&amp;originalType=binary&amp;ratio=2&amp;size=59367&amp;status=done&amp;style=none&amp;taskId=u7c9bc1cb-bcfc-4c2e-adde-d60f9beb21b) 刷新(refresh)使分片每秒被刷新(refresh)一次: 这些在内存缓冲区的文档被写入到一个新的段中,且没有进行 fsync 操作。 这个段被打开,使其可被搜索 内存缓冲区被清空 这个进程继续工作,更多的文档被添加到内存缓冲区和追加到事务日志 每隔一段时间—例如 translog 变得越来越大—索引被刷新(flush);一个新的 translog被创建,并且一个全量提交被执行 所有在内存缓冲区的文档都被写入一个新的段。 缓冲区被清空。 一个提交点被写入硬盘。 文件系统缓存通过 fsync 被刷新(flush)。 老的 translog 被删除。 ​ translog 提供所有还没有被刷到磁盘的操作的一个持久化纪录。当 Elasticsearch 启动的时候, 它会从磁盘中使用最后一个提交点去恢复已知的段,并且会重放 translog 中所有在最后一次提交后发生的变更操作。​ translog 也被用来提供实时 CRUD 。当你试着通过 ID 查询、更新、删除一个文档,它会在尝试从相应的段中检索之前, 首先检查 translog 任何最近的变更。这意味着它总是能够实时地获取到文档的最新版本。​ 执行一个提交并且截断 translog 的行为在 Elasticsearch 被称作一次 flush​ 分片每 30 分钟被自动刷新(flush),或者在 translog 太大的时候也会刷新​ 你很少需要自己手动执行 flush 操作;通常情况下,自动刷新就足够了。这就是说,在重启节点或关闭索引之前执行 flush 有益于你的索引。当 Elasticsearch 尝试恢复或重新打开一个索引, 它需要重放 translog 中所有的操作,所以如果日志越短,恢复越快。​ translog 的目的是保证操作不会丢失,在文件被 fsync 到磁盘前,被写入的文件在重启之后就会丢失。默认 translog 是每 5 秒被 fsync 刷新到硬盘, 或者在每次写请求完成之后执行(e.g. index, delete, update, bulk)。这个过程在主分片和复制分片都会发生。最终, 基本上,这意味着在整个请求被 fsync 到主分片和复制分片的 translog 之前,你的客户端不会得到一个 200 OK 响应。​ 在每次请求后都执行一个 fsync 会带来一些性能损失,尽管实践表明这种损失相对较小(特别是 bulk 导入,它在一次请求中平摊了大量文档的开销)。​ 但是对于一些大容量的偶尔丢失几秒数据问题也并不严重的集群,使用异步的 fsync还是比较有益的。比如,写入的数据被缓存到内存中,再每 5 秒执行一次 fsync 。如果你决定使用异步 translog 的话,你需要 保证 在发生 crash 时,丢失掉 sync_interval 时间段的数据也无所谓。请在决定前知晓这个特性。如果你不确定这个行为的后果,最好是使用默认的参数( “index.translog.durability”: “request” )来避免数据丢失。​ 2.6.6 段合并由于自动刷新流程每秒会创建一个新的段 ,这样会导致短时间内的段数量暴增。而段数目太多会带来较大的麻烦。 每一个段都会消耗文件句柄、内存和 cpu 运行周期。更重要的是,每个搜索请求都必须轮流检查每个段;所以段越多,搜索也就越慢。​ 段合并的时候会将那些旧的已删除文档从文件系统中清除。被删除的文档(或被更新文档的旧版本)不会被拷贝到新的大段中。​ 启动段合并不需要你做任何事。进行索引和搜索时会自动进行。​ 当索引的时候,刷新(refresh)操作会创建新的段并将段打开以供搜索使用。 合并进程选择一小部分大小相似的段,并且在后台将它们合并到更大的段中。这并不会中断索引和搜索。 一旦合并结束,老的段被删除 新的段被刷新(flush)到了磁盘。 ** 写入一个包含新段且排除旧的和较小的段的新提交点。 新的段被打开用来搜索。 老的段被删除。 ![1.png](https://cdn.nlark.com/yuque/0/2021/png/12610368/1623027512182-ad0ff1b5-52a9-4de1-926b-cd6ad7aee0ae.png#clientId=ubda5c093-cc76-4&amp;from=ui&amp;id=u7eb054a3&amp;margin=%5Bobject%20Object%5D&amp;name=1.png&amp;originHeight=248&amp;originWidth=436&amp;originalType=binary&amp;ratio=2&amp;size=49791&amp;status=done&amp;style=none&amp;taskId=u212f955f-eab0-41d6-b331-5d3a2f2a0eb) 合并大的段需要消耗大量的 I/O 和 CPU 资源,如果任其发展会影响搜索性能。Elasticsearch在默认情况下会对合并流程进行资源限制,所以搜索仍然 有足够的资源很好地执行。 2.7 文档分析分析 包含下面的过程: 将一块文本分成适合于倒排索引的独立的 词条。 将这些词条统一化为标准格式以提高它们的“可搜索性”,或者 recall。 分析器执行上面的工作。分析器实际上是将三个功能封装到了一个包里: 字符过滤器 首先,字符串按顺序通过每个 字符过滤器 。他们的任务是在分词前整理字符串。一个字符过滤器可以用来去掉 HTML,或者将 &amp; 转化成 and。 分词器 其次,字符串被 分词器 分为单个的词条。一个简单的分词器遇到空格和标点的时候,可能会将文本拆分成词条。 Token 过滤器 最后,词条按顺序通过每个 token 过滤器 。这个过程可能会改变词条(例如,小写化Quick ),删除词条(例如, 像 a, and, the 等无用词),或者增加词条(例如,像 jump和 leap 这种同义词)。 2.7.1 内置分析器Elasticsearch 还附带了可以直接使用的预包装的分析器。接下来我们会列出最重要的分析器。为了证明它们的差异,我们看看每个分析器会从下面的字符串得到哪些词条:​ &quot;Set the shape to semi-transparent by calling set_trans(5)&quot;​ 标准分析器 标准分析器是 Elasticsearch 默认使用的分析器。它是分析各种语言文本最常用的选择。它根据 Unicode 联盟 定义的 单词边界 划分文本。删除绝大部分标点。最后,将词条小写。它会产生:set, the, shape, to, semi, transparent, by, calling, set_trans, 5 简单分析器 简单分析器在任何不是字母的地方分隔文本,将词条小写。它会产生:set, the, shape, to, semi, transparent, by, calling, set, trans 空格分析器 空格分析器在空格的地方划分文本。它会产生:Set, the, shape, to, semi-transparent, by, calling, set_trans(5) 语言分析器 特定语言分析器可用于 很多语言。它们可以考虑指定语言的特点。例如, 英语 分析器附带了一组英语无用词(常用单词,例如 and 或者 the ,它们对相关性没有多少影响),它们会被删除。 由于理解英语语法的规则,这个分词器可以提取英语单词的 词干 。​ 英语 分词器会产生下面的词条:​ set, shape, semi, transpar, call, set_tran, 5​ 注意看 transparent、 calling 和 set_trans 已经变为词根格式。​ 2.7.2 分析器使用场景当我们 索引 一个文档,它的全文域被分析成词条以用来创建倒排索引。 但是,当我们在全文域 搜索 的时候,我们需要将查询字符串通过 相同的分析过程 ,以保证我们搜索的词条格式与索引中的词条格式一致。​ 全文查询,理解每个域是如何定义的,因此它们可以做正确的事: 当你查询一个 全文 域时, 会对查询字符串应用相同的分析器,以产生正确的搜索词条列表。 当你查询一个 精确值 域时,不会分析查询字符串,而是搜索你指定的精确值。 ​ 2.7.3 测试分析器有 些 时 候 很 难 理 解 分 词的 过 程 和 实 际 被 存 储 到索 引 中 的 词 条 , 特 别 是你 刚 接 触Elasticsearch。为了理解发生了什么,你可以使用 analyze API 来看文本是如何被分析的。在消息体里,指定分析器和要分析的文本。 12345GET http://localhost:9200/_analyze&#123;&quot;analyzer&quot;: &quot;standard&quot;,&quot;text&quot;: &quot;Text to analyze&quot;&#125; 结果中每个元素代表一个单独的词条: 12345678910111213141516171819202122232425&#123; &quot;tokens&quot;: [ &#123; &quot;token&quot;: &quot;text&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 4, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 1 &#125;, &#123; &quot;token&quot;: &quot;to&quot;, &quot;start_offset&quot;: 5, &quot;end_offset&quot;: 7, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 2 &#125;, &#123; &quot;token&quot;: &quot;analyze&quot;, &quot;start_offset&quot;: 8, &quot;end_offset&quot;: 15, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 3 &#125; ]&#125; ​ token 是 实 际 存 储 到 索 引 中 的 词 条 。 position 指 明 词 条 在 原 始 文 本 中 出 现 的 位 置 。start_offset 和 end_offset 指明字符在原始字符串中的位置。​ 2.7.4 指定分析器当 Elasticsearch 在你的文档中检测到一个新的字符串域,它会自动设置其为一个全文 字符串 域,使用 标准 分析器对它进行分析。你不希望总是这样。可能你想使用一个不同的分析器,适用于你的数据使用的语言。有时候你想要一个字符串域就是一个字符串域—不使用分析,直接索引你传入的精确值,例如用户 ID 或者一个内部的状态域或标签。要做到这一点,我们必须手动指定这些域的映射。​ 2.7.5 IK分词器首先我们通过 Postman 发送 GET 请求查询分词效果​ 1234# GET http://localhost:9200/_analyze&#123;&quot;text&quot;:&quot;测试单词&quot;&#125; ES 的默认分词器无法识别中文中测试、单词这样的词汇,而是简单的将每个字拆完分为一个词 1234567891011121314151617181920212223242526272829303132&#123; &quot;tokens&quot;: [ &#123; &quot;token&quot;: &quot;测&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 1, &quot;type&quot;: &quot;&lt;IDEOGRAPHIC&gt;&quot;, &quot;position&quot;: 0 &#125;, &#123; &quot;token&quot;: &quot;试&quot;, &quot;start_offset&quot;: 1, &quot;end_offset&quot;: 2, &quot;type&quot;: &quot;&lt;IDEOGRAPHIC&gt;&quot;, &quot;position&quot;: 1 &#125;, &#123; &quot;token&quot;: &quot;单&quot;, &quot;start_offset&quot;: 2, &quot;end_offset&quot;: 3, &quot;type&quot;: &quot;&lt;IDEOGRAPHIC&gt;&quot;, &quot;position&quot;: 2 &#125;, &#123; &quot;token&quot;: &quot;词&quot;, &quot;start_offset&quot;: 3, &quot;end_offset&quot;: 4, &quot;type&quot;: &quot;&lt;IDEOGRAPHIC&gt;&quot;, &quot;position&quot;: 3 &#125; ]&#125; 这样的结果显然不符合我们的使用要求,所以我们需要下载 ES 对应版本的中文分词器。​ 我们这里采用 IK 中文分词器,下载地址为:[https://github.com/medcl/elasticsearch-analysis-ik/releases/tag/v7.8.0](https://github.com/medcl/elasticsearch-analysis-ik/releases/tag/v7.8.0)​​ 将解压后的后的文件夹放入 ES 根目录下的 plugins 目录下,重启 ES 即可使用。​ 我们这次加入新的查询参数&quot;analyzer&quot;:&quot;ik_max_word&quot; 12345# GET http://localhost:9200/_analyze&#123;&quot;text&quot;:&quot;测试单词&quot;,&quot;analyzer&quot;:&quot;ik_max_word&quot;&#125; ik_max_word:会将文本做最细粒度的拆分 ik_smart:会将文本做最粗粒度的拆分 使用中文分词后的结果为: 123456789101112131415161718&#123; &quot;tokens&quot;: [ &#123; &quot;token&quot;: &quot;测试&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 2, &quot;type&quot;: &quot;CN_WORD&quot;, &quot;position&quot;: 0 &#125;, &#123; &quot;token&quot;: &quot;单词&quot;, &quot;start_offset&quot;: 2, &quot;end_offset&quot;: 4, &quot;type&quot;: &quot;CN_WORD&quot;, &quot;position&quot;: 1 &#125; ]&#125; ES 中也可以进行扩展词汇,首先查询 12345# GET http://localhost:9200/_analyze&#123;&quot;text&quot;:&quot;弗雷尔卓德&quot;,&quot;analyzer&quot;:&quot;ik_max_word&quot;&#125; 仅仅可以得到每个字的分词结果,我们需要做的就是使分词器识别到弗雷尔卓德也是一个词语 123456789101112131415161718192021222324252627282930313233343536373839&#123; &quot;tokens&quot;: [ &#123; &quot;token&quot;: &quot;弗&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 1, &quot;type&quot;: &quot;CN_CHAR&quot;, &quot;position&quot;: 0 &#125;, &#123; &quot;token&quot;: &quot;雷&quot;, &quot;start_offset&quot;: 1, &quot;end_offset&quot;: 2, &quot;type&quot;: &quot;CN_CHAR&quot;, &quot;position&quot;: 1 &#125;, &#123; &quot;token&quot;: &quot;尔&quot;, &quot;start_offset&quot;: 2, &quot;end_offset&quot;: 3, &quot;type&quot;: &quot;CN_CHAR&quot;, &quot;position&quot;: 2 &#125;, &#123; &quot;token&quot;: &quot;卓&quot;, &quot;start_offset&quot;: 3, &quot;end_offset&quot;: 4, &quot;type&quot;: &quot;CN_CHAR&quot;, &quot;position&quot;: 3 &#125;, &#123; &quot;token&quot;: &quot;德&quot;, &quot;start_offset&quot;: 4, &quot;end_offset&quot;: 5, &quot;type&quot;: &quot;CN_CHAR&quot;, &quot;position&quot;: 4 &#125; ]&#125; 首先进入 ES 根目录中的 plugins 文件夹下的 ik 文件夹,进入 config 目录,创建 custom.dic文件,写入弗雷尔卓德。同时打开 IKAnalyzer.cfg.xml 文件,将新建的 custom.dic 配置其中,重启 ES 服务器。​ 2.7.6 自定义分析器虽然 Elasticsearch 带有一些现成的分析器,然而在分析器上 Elasticsearch 真正的强大之处在于,你可以通过在一个适合你的特定数据的设置之中组合字符过滤器、分词器、词汇单元过滤器来创建自定义的分析器。在 分析与分析器 我们说过,一个 分析器 就是在一个包里面组合了三种函数的一个包装器, 三种函数按照顺序被执行: 字符过滤器 字符过滤器 用来 整理 一个尚未被分词的字符串。例如,如果我们的文本是 HTML 格式的,它会包含像 或者 这样的 HTML 标签,这些标签是我们不想索引的。我们可以使用 html 清除 字符过滤器 来移除掉所有的 HTML 标签,并且像把 &Aacute; 转换为相对应的 Unicode 字符 Á 这样,转换 HTML 实体。一个分析器可能有 0 个或者多个字符过滤器。 分词器 一个分析器 必须 有一个唯一的分词器。 分词器把字符串分解成单个词条或者词汇单元。 标准 分析器里使用的 标准 分词器 把一个字符串根据单词边界分解成单个词条,并且移除掉大部分的标点符号,然而还有其他不同行为的分词器存在。例如, 关键词 分词器 完整地输出 接收到的同样的字符串,并不做任何分词。 空格 分词器 只根据空格分割文本 。 正则 分词器 根据匹配正则表达式来分割文本 。 词单元过滤器 经过分词,作为结果的 词单元流 会按照指定的顺序通过指定的词单元过滤器 。词单元过滤器可以修改、添加或者移除词单元。我们已经提到过 lowercase 和 stop 词过滤器 ,但是在 Elasticsearch 里面还有很多可供选择的词单元过滤器。 词干过滤器 把单词 遏制 为 词干。 ascii_folding 过滤器移除变音符,把一个像 “très” 这样的词转换为 “tres” 。ngram 和 edge_ngram 词单元过滤器 可以产生 适合用于部分匹配或者自动补全的词单元。接下来,我们看看如何创建自定义的分析器:​ 123456789101112131415161718192021222324252627282930313233343536373839# PUT http: //localhost:9200/my_index&#123; &quot;settings&quot;: &#123; &quot;analysis&quot;: &#123; &quot;char_filter&quot;: &#123; &quot;&amp;_to_and&quot;: &#123; &quot;type&quot;: &quot;mapping&quot;, &quot;mappings&quot;: [ &quot;&amp;=&gt; and &quot; ] &#125; &#125;, &quot;filter&quot;: &#123; &quot;my_stopwords&quot;: &#123; &quot;type&quot;: &quot;stop&quot;, &quot;stopwords&quot;: [ &quot;the&quot;, &quot;a&quot; ] &#125; &#125;, &quot;analyzer&quot;: &#123; &quot;my_analyzer&quot;: &#123; &quot;type&quot;: &quot;custom&quot;, &quot;char_filter&quot;: [ &quot;html_strip&quot;, &quot;&amp;_to_and&quot; ], &quot;tokenizer&quot;: &quot;standard&quot;, &quot;filter&quot;: [ &quot;lowercase&quot;, &quot;my_stopwords&quot; ] &#125; &#125; &#125; &#125;&#125; 索引被创建以后,使用 analyze API 来 测试这个新的分析器 12345# GET http://127.0.0.1:9200/my_index/_analyze&#123;&quot;text&quot;:&quot;The quick &amp; brown fox&quot;,&quot;analyzer&quot;: &quot;my_analyzer&quot;&#125; 下面的缩略结果展示出我们的分析器正在正确地运行 1234567891011121314151617181920212223242526272829303132&#123; &quot;tokens&quot;: [ &#123; &quot;token&quot;: &quot;quick&quot;, &quot;start_offset&quot;: 4, &quot;end_offset&quot;: 9, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 1 &#125;, &#123; &quot;token&quot;: &quot;and&quot;, &quot;start_offset&quot;: 10, &quot;end_offset&quot;: 11, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 2 &#125;, &#123; &quot;token&quot;: &quot;brown&quot;, &quot;start_offset&quot;: 12, &quot;end_offset&quot;: 17, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 3 &#125;, &#123; &quot;token&quot;: &quot;fox&quot;, &quot;start_offset&quot;: 18, &quot;end_offset&quot;: 21, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 4 &#125; ]&#125; 2.8 文档冲突2.8.1 文档冲突当我们使用 index API 更新文档 ,可以一次性读取原始文档,做我们的修改,然后重新索引 整个文档 。 最近的索引请求将获胜:无论最后哪一个文档被索引,都将被唯一存储在 Elasticsearch 中。如果其他人同时更改这个文档,他们的更改将丢失。​ 很多时候这是没有问题的。也许我们的主数据存储是一个关系型数据库,我们只是将数据复制到 Elasticsearch 中并使其可被搜索。 也许两个人同时更改相同的文档的几率很小。或者对于我们的业务来说偶尔丢失更改并不是很严重的问题。​ 但有时丢失了一个变更就是 非常严重的 。试想我们使用 Elasticsearch 存储我们网上商城商品库存的数量, 每次我们卖一个商品的时候,我们在 Elasticsearch 中将库存数量减少。有一天,管理层决定做一次促销。突然地,我们一秒要卖好几个商品。 假设有两个 web程序并行运行,每一个都同时处理所有商品的销售 web_1 对 stock_count 所做的更改已经丢失,因为 web_2 不知道它的 stock_count 的拷贝已经过期。 结果我们会认为有超过商品的实际数量的库存,因为卖给顾客的库存商品并不存在,我们将让他们非常失望。​ 变更越频繁,读数据和更新数据的间隙越长,也就越可能丢失变更。​ 在数据库领域中,有两种方法通常被用来确保并发更新时变更不会丢失:​ 悲观并发控制 这种方法被关系型数据库广泛使用,它假定有变更冲突可能发生,因此阻塞访问资源以防止冲突。 一个典型的例子是读取一行数据之前先将其锁住,确保只有放置锁的线程能够对这行数据进行修改。 乐观并发控制 Elasticsearch 中使用的这种方法假定冲突是不可能发生的,并且不会阻塞正在尝试的操作。 然而,如果源数据在读写当中被修改,更新将会失败。应用程序接下来将决定该如何解决冲突。 例如,可以重试更新、使用新的数据、或者将相关情况报告给用户。 2.8.2 乐观并发控制Elasticsearch 是分布式的。当文档创建、更新或删除时, 新版本的文档必须复制到集群中的其他节点。Elasticsearch 也是异步和并发的,这意味着这些复制请求被并行发送,并且到达目的地时也许 顺序是乱的 。 Elasticsearch 需要一种方法确保文档的旧版本不会覆盖新的版本。​ 当我们之前讨论 index ,GET 和 delete 请求时,我们指出每个文档都有一个 _version(版本)号,当文档被修改时版本号递增。 Elasticsearch 使用这个 version 号来确保变更以正确顺序得到执行。如果旧版本的文档在新版本之后到达,它可以被简单的忽略。​ 我们可以利用 version 号来确保 应用中相互冲突的变更不会导致数据丢失。我们通过指定想要修改文档的 version 号来达到这个目的。 如果该版本不是当前版本号,我们的请求将会失败。​ 老的版本 es 使用 version,但是新版本不支持了,会报下面的错误,提示我们用 if_seq_no和 if_primary_term​ 1234567891011121314&#123; &quot;error&quot;: &#123; &quot;root_cause&quot;: [ &#123; &quot;type&quot;: &quot;action_request_validation_exception&quot;, &quot;reason&quot;: &quot;Validation Failed: 1: internal versioning can not be used for optimistic concurrency control.Please use`if_seq_no`and`if_primary_term`instead;&quot; &#125; ], &quot;type&quot;: &quot;action_request_validation_exception&quot;, &quot;reason&quot;: &quot;Validation Failed: 1: internal versioning can not be used for optimistic concurrency control. Please use`if_seq_no`and`if_primary_term`instead ;&quot; &#125;, &quot;status&quot;: 400&#125; 2.8.3 外部系统版本控制一个常见的设置是使用其它数据库作为主要的数据存储,使用 Elasticsearch 做数据检索, 这意味着主数据库的所有更改发生时都需要被复制到 Elasticsearch ,如果多个进程负责这一数据同步,你可能遇到类似于之前描述的并发问题。​ 如果你的主数据库已经有了版本号 — 或一个能作为版本号的字段值比如 timestamp —那么你就可以在 Elasticsearch 中通过增加 version_type=external 到查询字符串的方式重用这些相同的版本号, 版本号必须是大于零的整数, 且小于 9.2E+18 — 一个 Java 中 long类型的正值。​ 外部版本号的处理方式和我们之前讨论的内部版本号的处理方式有些不同,Elasticsearch 不是检查当前 _version 和请求中指定的版本号是否相同, 而是检查当前_version 是否 小于 指定的版本号。 如果请求成功,外部的版本号作为文档的新 _version进行存储。外部版本号不仅在索引和删除请求是可以指定,而且在 创建 新文档时也可以指定。 2.5 KibanaKibana 是一个免费且开放的用户界面,能够让你对 Elasticsearch 数据进行可视化,并让你在 Elastic Stack 中进行导航。你可以进行各种操作,从跟踪查询负载,到理解请求如何流经你的整个应用,都能轻松完成。下载地址: 解压缩下载的 zip 文件 修改 config/kibana.yml 文件 12345678# 默认端口server.port: 5601# ES 服务器的地址elasticsearch.hosts: [&quot;http://localhost:9200&quot;]# 索引名kibana.index: &quot;.kibana&quot;# 支持中文i18n.locale: &quot;zh-CN&quot; 通过浏览器访问 :","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"https://yinhuidong.github.io/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"https://yinhuidong.github.io/tags/Elasticsearch/"}]},{"title":"Elasticsearch使用篇","slug":"ElasticSearch/ElasticSearch基础入门篇","date":"2022-01-12T00:19:55.239Z","updated":"2022-01-12T00:19:55.240Z","comments":true,"path":"2022/01/12/ElasticSearch/ElasticSearch基础入门篇/","link":"","permalink":"https://yinhuidong.github.io/2022/01/12/ElasticSearch/ElasticSearch%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E7%AF%87/","excerpt":"","text":"第一章，ES概述1.ES是什么ElasticSearch是一个分布式，RESTFUL风格的搜索和数据分析引擎，能够解决不断涌现出的各种用例。作为ElasticStack的核心，它集中存储您的数据，帮助您发现意料之中以及意料之外的情况。​ Elaticsearch,简称为 ES, ES 是一个开源的高扩展的分布式全文搜索引擎,是整个 ElasticStack 技术栈的核心。它可以近乎实时的存储、检索数据;本身扩展性很好,可以扩展到上百台服务器,处理 PB 级别的数据。 2.全文搜索引擎对于这些非结构化的数据文本,关系型数据库搜索不是能很好的支持。一般传统数据库,全文检索都实现的很鸡肋,因为一般也没人用数据库存文本字段。进行全文检索需要扫描整个表,如果数据量大的话即使对 SQL 的语法优化,也收效甚微。建立了索引,但是维护起来也很麻烦,对于 insert 和 update 操作都会重新构建索引。​ 基于以上原因可以分析得出,在一些生产环境中,使用常规的搜索方式,性能是非常差的:​ 搜索的数据对象是大量的非结构化的文本数据。 文件记录量达到数十万或数百万个甚至更多。 支持大量基于交互式文本的查询。 需求非常灵活的全文搜索查询。 对高度相关的搜索结果的有特殊需求,但是没有可用的关系数据库可以满足。 对不同记录类型、非文本数据操作或安全事务处理的需求相对较少的情况。 ​ 为了解决结构化数据搜索和非结构化数据搜索性能问题,我们就需要专业,健壮,强大的全文搜索引擎。​ 这里说到的全文搜索引擎指的是目前广泛应用的主流搜索引擎。它的工作原理是计算机索引程序通过扫描文章中的每一个词,对每一个词建立一个索引,指明该词在文章中出现的次数和位置,当用户查询时,检索程序就根据事先建立的索引进行查找,并将查找的结果反馈给用户的检索方式。这个过程类似于通过字典中的检索字表查字的过程。 3.Elasticsearch And SolrLucene 是 Apache 软件基金会 Jakarta 项目组的一个子项目,提供了一个简单却强大的应用程式接口,能够做全文索引和搜寻。在 Java 开发环境里 Lucene 是一个成熟的免费开源工具。就其本身而言,Lucene 是当前以及最近几年最受欢迎的免费 Java 信息检索程序库。但 Lucene 只是一个提供全文搜索功能类库的核心工具包,而真正使用它还需要一个完善的服务框架搭建起来进行应用。​ 目前市面上流行的搜索引擎软件,主流的就两款:Elasticsearch 和 Solr,这两款都是基于 Lucene 搭建的,可以独立部署启动的搜索引擎服务软件。由于内核相同,所以两者除了服务器安装、部署、管理、集群以外,对于数据的操作 修改、添加、保存、查询等等都十分类似。​ 第二章，Elasticsearch 基本操作1.ES安装Elasticsearch 的官方地址9300 端口为 Elasticsearch 集群间组件的通信端口,9200 端口为浏览器访问的 http协议 RESTful 端口。 2. 数据格式Elasticsearch 是面向文档型数据库,一条数据在这里就是一个文档。​ 对比MySQL：ES 里的 Index 可以看做一个库,而 Types 相当于表,Documents 则相当于表的行。这里 Types 的概念已经被逐渐弱化,Elasticsearch 6.X 中,一个 index 下已经只能包含一个type,Elasticsearch 7.X 中, Type 的概念已经被删除了。 3. HTTP操作3.1 索引操作1）创建索引对比关系型数据库,创建索引就等同于创建数据库。​ 在 Postman 中,向 ES 服务器发 PUT 请求 :[http://127.0.0.1:9200/shopping](http://127.0.0.1:9200/shopping) 12345&#123; &quot;acknowledged&quot;: true, # 响应结果 &quot;shards_acknowledged&quot;: true, # 分片操作成功 &quot;index&quot;: &quot;shopping&quot; # 索引名称&#125; 如果重复添加索引,会返回错误信息​ 2）查看所有索引在 Postman 中,向 ES 服务器发 GET 请求 :[http://127.0.0.1:9200/_cat/indices?v](http://127.0.0.1:9200/_cat/indices?v)​ 这里请求路径中的_cat 表示查看的意思,indices 表示索引,所以整体含义就是查看当前 ES服务器中的所有索引,就好像 MySQL 中的 show tables 的感觉,服务器响应结果如下： 12health status index uuid pri rep docs.count docs.deleted store.size pri.store.sizeyellow open shopping 4Ml2ix63QZel3aKFP527Jw 1 1 0 0 208b 208b ​ 表头 含义 health 当前服务器健康状态:green(集群完整) yellow(单点正常、集群不完整) red(单点不正常) ​ status 索引打开、关闭状态 index 索引名 uuid 索引统一编号 pri 主分片数量 rep 副本数量 docs.count 可用文档数量 docs.deleted 文档删除状态(逻辑删除) store.size 主分片和副分片整体占空间大小 pri.store.size 主分片占空间大小 3)查看单个索引在 Postman 中,向 ES 服务器发 GET 请求 :[http://127.0.0.1:9200/shopping](http://127.0.0.1:9200/shopping)​ 查看索引向 ES 服务器发送的请求路径和创建索引是一致的。但是 HTTP 方法不一致。​ 12345678910111213141516171819202122232425&#123; &quot;shopping&quot;: &#123; # 索引名 &quot;aliases&quot;: &#123;&#125;, # 别名 &quot;mappings&quot;: &#123;&#125;, # 映射 &quot;settings&quot;: &#123; # 设置 &quot;index&quot;: &#123; # 设置 - 索引 &quot;routing&quot;: &#123; &quot;allocation&quot;: &#123; &quot;include&quot;: &#123; &quot;_tier_preference&quot;: &quot;data_content&quot; &#125; &#125; &#125;, &quot;number_of_shards&quot;: &quot;1&quot;, # 主分片数 &quot;provided_name&quot;: &quot;shopping&quot;,# 索引名称 &quot;creation_date&quot;: &quot;1622879195164&quot;, # 设置 - 索引 -创建时间 &quot;number_of_replicas&quot;: &quot;1&quot;, # 设置 - 索引 - 副分片数 &quot;uuid&quot;: &quot;4Ml2ix63QZel3aKFP527Jw&quot;, # 设置 - 索引 -唯一标示 &quot;version&quot;: &#123; # 设置 - 索引 - 版本 &quot;created&quot;: &quot;7130199&quot; &#125; &#125; &#125; &#125;&#125; 4）删除索引在 Postman 中,向 ES 服务器发 DELETE 请求 :[http://127.0.0.1:9200/shopping](http://127.0.0.1:9200/shopping)​​ 重新访问索引时,服务器返回响应:索引不存在​ 123456789101112131415161718192021&#123; &quot;error&quot;: &#123; &quot;root_cause&quot;: [ &#123; &quot;type&quot;: &quot;index_not_found_exception&quot;, &quot;reason&quot;: &quot;no such index [shopping]&quot;, &quot;resource.type&quot;: &quot;index_or_alias&quot;, &quot;resource.id&quot;: &quot;shopping&quot;, &quot;index_uuid&quot;: &quot;_na_&quot;, &quot;index&quot;: &quot;shopping&quot; &#125; ], &quot;type&quot;: &quot;index_not_found_exception&quot;, &quot;reason&quot;: &quot;no such index [shopping]&quot;, &quot;resource.type&quot;: &quot;index_or_alias&quot;, &quot;resource.id&quot;: &quot;shopping&quot;, &quot;index_uuid&quot;: &quot;_na_&quot;, &quot;index&quot;: &quot;shopping&quot; &#125;, &quot;status&quot;: 404&#125; 3.2 文档操作1）创建文档索引已经创建好了,接下来我们来创建文档,并添加数据。这里的文档可以类比为关系型数据库中的表数据,添加的数据格式为 JSON 格式。​ 在 Postman 中,向 ES 服务器发 POST 请求 : [http://127.0.0.1:9200/shopping/_doc](http://127.0.0.1:9200/shopping/_doc)​​ 请求体内容为:​ 123456&#123; &quot;name&quot;:&quot;张三&quot;, &quot;nick_name&quot;:&quot;法外狂徒&quot;, &quot;age&quot;:20, &quot;high&quot;:184&#125; 此处发送请求的方式必须为 POST,不能是 PUT,否则会发生错误 1234567891011121314&#123; &quot;_index&quot;: &quot;shopping&quot;, #索引 &quot;_type&quot;: &quot;_doc&quot;, #类型 文档 &quot;_id&quot;: &quot;KpBM23kBKAGPzhDtl_KA&quot;, # 唯一标示 &quot;_version&quot;: 1, # 版本 &quot;result&quot;: &quot;created&quot;, # 结果 created标识创建成功 &quot;_shards&quot;: &#123; # 分片 &quot;total&quot;: 2, # 分片 总数 &quot;successful&quot;: 1, # 分片 成功 &quot;failed&quot;: 0 # 分片 失败 &#125;, &quot;_seq_no&quot;: 0, &quot;_primary_term&quot;: 1&#125; 上面的数据创建后,由于没有指定数据唯一性标识(ID),默认情况下,ES 服务器会随机生成一个。​ 如果想要自定义唯一性标识,需要在创建时指定:[http://127.0.0.1:9200/shopping/_doc/1](http://127.0.0.1:9200/shopping/_doc/1)​ 此处需要注意:如果增加数据时明确数据主键,那么请求方式也可以为 PUT。 2）查看文档查看文档时,需要指明文档的唯一性标识,类似于 MySQL 中数据的主键查询​ 在 Postman 中,向 ES 服务器发 GET 请求 :[http://127.0.0.1:9200/shopping/_doc/1](http://127.0.0.1:9200/shopping/_doc/1)​​ 123456789101112131415&#123; &quot;_index&quot;: &quot;shopping&quot;, &quot;_type&quot;: &quot;_doc&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 1, &quot;_seq_no&quot;: 1, &quot;_primary_term&quot;: 1, &quot;found&quot;: true, # 查询结果 &quot;_source&quot;: &#123; # 文档源信息 &quot;name&quot;: &quot;张三&quot;, &quot;nick_name&quot;: &quot;法外狂徒&quot;, &quot;age&quot;: &quot;20&quot;, &quot;high&quot;: 184 &#125;&#125; 3）修改文档和新增文档一样,输入相同的 URL 地址请求,如果请求体变化,会将原有的数据内容覆盖​ 在 Postman 中,向 ES 服务器发 **POST **请求 : 123456&#123; &quot;name&quot;:&quot;李四&quot;, &quot;nick_name&quot;:&quot;法外狂徒&quot;, &quot;age&quot;:20, &quot;high&quot;:184&#125; 4) 修改字段修改数据时,也可以只修改某一给条数据的局部信息​ 在 Postman 中,向 ES 服务器发 POST 请求 :[http://127.0.0.1:9200/shopping/_update/1](http://127.0.0.1:9200/shopping/_update/1)​​ 12345&#123; &quot;doc&quot;:&#123; &quot;name&quot;:&quot;王五&quot; &#125;&#125; 5) 删除文档删除一个文档不会立即从磁盘上移除,它只是被标记成已删除(逻辑删除)。​ 在 Postman 中,向 ES 服务器发 DELETE 请求 :[http://127.0.0.1:9200/shopping/_doc/1](http://127.0.0.1:9200/shopping/_doc/1)​ 1234567891011121314&#123; &quot;_index&quot;: &quot;shopping&quot;, &quot;_type&quot;: &quot;_doc&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 5, &quot;result&quot;: &quot;deleted&quot;, # 表示数据被标记为删除 &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;_seq_no&quot;: 5, &quot;_primary_term&quot;: 1&#125; 6) 条件删除文档一般删除数据都是根据文档的唯一性标识进行删除,实际操作时,也可以根据条件对多条数据进行删除。​ 先添加两条数据： 12345678910111213&#123; &quot;name&quot;:&quot;二十&quot;, &quot;nick_name&quot;:&quot;法外狂徒&quot;, &quot;age&quot;:20, &quot;high&quot;:184&#125;&#123; &quot;name&quot;:&quot;辣条&quot;, &quot;nick_name&quot;:&quot;法外狂徒&quot;, &quot;age&quot;:20, &quot;high&quot;:184&#125; 向 ES 服务器发 POST 请求 : [http://127.0.0.1:9200/shopping/_delete_by_query](http://127.0.0.1:9200/shopping/_delete_by_query)​​ 请求体内容为:​ 1234567891011121314151617&#123; &quot;took&quot;: 548, # 耗时 &quot;timed_out&quot;: false, # 是否超时 &quot;total&quot;: 3, #总数 &quot;deleted&quot;: 3, # 删除数量 &quot;batches&quot;: 1, &quot;version_conflicts&quot;: 0, &quot;noops&quot;: 0, &quot;retries&quot;: &#123; &quot;bulk&quot;: 0, &quot;search&quot;: 0 &#125;, &quot;throttled_millis&quot;: 0, &quot;requests_per_second&quot;: -1.0, &quot;throttled_until_millis&quot;: 0, &quot;failures&quot;: []&#125; 3.3 映射操作有了索引库,等于有了数据库中的 database。​ 接下来就需要建索引库(index)中的映射了,类似于数据库(database)中的表结构(table)。创建数据库表需要设置字段名称,类型,长度,约束等;索引库也一样,需要知道这个类型下有哪些字段,每个字段有哪些约束信息,这就叫做映射(mapping)。 1) 创建映射在 Postman 中,向 ES 服务器发 PUT 请求 :[http://127.0.0.1:9200/student/_mapping](http://127.0.0.1:9200/student/_mapping)​ 请求体内容为:​ 12345678910111213141516&#123; &quot;properties&quot;:&#123; &quot;name&quot;:&#123; &quot;type&quot;:&quot;text&quot;, &quot;index&quot;:&quot;true&quot; &#125; , &quot;sex&quot;:&#123; &quot;type&quot;:&quot;text&quot;, &quot;index&quot;:&quot;false&quot; &#125; , &quot;age&quot;:&#123; &quot;type&quot;:&quot;long&quot;, &quot;index&quot;:&quot;false&quot; &#125; &#125;&#125; 映射数据说明： 字段名:任意填写,下面指定许多属性,例如:title、subtitle、images、price type:类型,Elasticsearch 中支持的数据类型非常丰富 123456789String 类型,又分两种: text:可分词 keyword:不可分词,数据会作为完整字段进行匹配Numerical:数值类型,分两类 基本数据类型:long、integer、short、byte、double、float、half_float 浮点数的高精度类型:scaled_floatDate:日期类型Array:数组类型Object:对象 index:是否索引,默认为 true,也就是说你不进行任何配置,所有字段都会被索引。 12true:字段会被索引,则可以用来进行搜索false:字段不会被索引,不能用来搜索 store:是否将数据进行独立存储,默认为 false ​ 原始的文本会存储在_source 里面,默认情况下其他提取出来的字段都不是独立存储的,是从_source 里面提取出来的。当然你也可以独立的存储某个字段,只要设置”store”: true 即可,获取独立存储的字段要比从_source 中解析快得多,但是也会占用更多的空间,所以要根据实际业务需求来设置。 analyzer:分词器,这里的 ik_max_word 即使用 ik 分词器2）查看映射在 Postman 中,向 ES 服务器发 GET 请求 :[http://127.0.0.1:9200/student/_mapping](http://127.0.0.1:9200/student/_mapping)​ 12345678910111213141516171819&#123; &quot;stu&quot;: &#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;age&quot;: &#123; &quot;type&quot;: &quot;long&quot;, &quot;index&quot;: false &#125;, &quot;name&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;sex&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;index&quot;: false &#125; &#125; &#125; &#125;&#125; 3) 索引映射关联在 Postman 中,向 ES 服务器发 PUT 请求 :[http://127.0.0.1:9200/stu](http://127.0.0.1:9200/student1)1 3.4 高级查询Elasticsearch 提供了基于 JSON 提供完整的查询 DSL 来定义查询​ 定义数据 : /student/_doc/1001 123456&#123; &quot;name&quot;:&quot;zhangsan&quot;, &quot;nickname&quot;:&quot;zhangsan&quot;, &quot;sex&quot;:&quot;男&quot;, &quot;age&quot;:&quot;30&quot;&#125; /student/_doc/1002 123456&#123; &quot;name&quot;:&quot;lisi&quot;, &quot;nickname&quot;:&quot;lisi&quot;, &quot;sex&quot;:&quot;男&quot;, &quot;age&quot;:&quot;20&quot;&#125; /student/_doc/1003 123456&#123; &quot;name&quot;:&quot;wangwu&quot;, &quot;nickname&quot;:&quot;wangwu&quot;, &quot;sex&quot;:&quot;女&quot;, &quot;age&quot;:&quot;40&quot;&#125; /student/_doc/1004 123456&#123; &quot;name&quot;:&quot;zhaoliu&quot;, &quot;nickname&quot;:&quot;zhaoliu&quot;, &quot;sex&quot;:&quot;女&quot;, &quot;age&quot;:&quot;50&quot;&#125; /student/_doc/1005 123456&#123; &quot;name&quot;:&quot;zhangsan1&quot;, &quot;nickname&quot;:&quot;zhangsan1&quot;, &quot;sex&quot;:&quot;男&quot;, &quot;age&quot;:&quot;50&quot;&#125; /student/_doc/1006 123456&#123; &quot;name&quot;:&quot;zhangsan2&quot;, &quot;nickname&quot;:&quot;zhangsan2&quot;, &quot;sex&quot;:&quot;女&quot;, &quot;age&quot;:&quot;30&quot;&#125; 1) 查询所有文档在 Postman 中,向 ES 服务器发 GET 请求 : [http://127.0.0.1:9200/student/_search](http://127.0.0.1:9200/student/_search)​ 12345678&#123; &quot;query&quot;:&#123; &quot;match_all&quot;:&#123;&#125; &#125;&#125;# query 这里的query代表一个查询对象，里面可以有不同的查询属性# match_all 查询类型，例如：match_all(代表查询所有)，match，term，range 等等# &#123;查询条件&#125;：查询条件会根据类型的不同，写法也有差异 响应结果： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103&#123; &quot;took&quot;: 100, # 耗时 &quot;timed_out&quot;: false, #是否超时 &quot;_shards&quot;: &#123; # 分片信息 &quot;total&quot;: 1, # 总数 &quot;successful&quot;: 1, # 成功 &quot;skipped&quot;: 0, # 忽略 &quot;failed&quot;: 0 # 失败 &#125;, &quot;hits&quot;: &#123; # 搜索命中结果 &quot;total&quot;: &#123; # 搜索条件匹配的文档总数 &quot;value&quot;: 7, # 总命中计数的值 &quot;relation&quot;: &quot;eq&quot; # 计数规则 eq：标识计数准确 gte：计数不准确 &#125;, &quot;max_score&quot;: 1.0, #匹配度分值 &quot;hits&quot;: [ # 命中结果集合 &#123; &quot;_index&quot;: &quot;student&quot;, &quot;_type&quot;: &quot;1001&quot;, &quot;_id&quot;: &quot;xnuC3HkB5T_yiydta8QV&quot;, &quot;_score&quot;: 1.0, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;zhangsan&quot;, &quot;nickname&quot;: &quot;zhangsan&quot;, &quot;sex&quot;: &quot;男&quot;, &quot;age&quot;: &quot;30&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;student&quot;, &quot;_type&quot;: &quot;1001&quot;, &quot;_id&quot;: &quot;1001&quot;, &quot;_score&quot;: 1.0, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;zhangsan&quot;, &quot;nickname&quot;: &quot;zhangsan&quot;, &quot;sex&quot;: &quot;男&quot;, &quot;age&quot;: &quot;30&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;student&quot;, &quot;_type&quot;: &quot;1001&quot;, &quot;_id&quot;: &quot;1002&quot;, &quot;_score&quot;: 1.0, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;lisi&quot;, &quot;nickname&quot;: &quot;lisi&quot;, &quot;sex&quot;: &quot;男&quot;, &quot;age&quot;: &quot;20&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;student&quot;, &quot;_type&quot;: &quot;1001&quot;, &quot;_id&quot;: &quot;1003&quot;, &quot;_score&quot;: 1.0, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;wangwu&quot;, &quot;nickname&quot;: &quot;wangwu&quot;, &quot;sex&quot;: &quot;女&quot;, &quot;age&quot;: &quot;40&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;student&quot;, &quot;_type&quot;: &quot;1001&quot;, &quot;_id&quot;: &quot;1004&quot;, &quot;_score&quot;: 1.0, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;zhaoliu&quot;, &quot;nickname&quot;: &quot;zhaoliu&quot;, &quot;sex&quot;: &quot;女&quot;, &quot;age&quot;: &quot;50&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;student&quot;, &quot;_type&quot;: &quot;1001&quot;, &quot;_id&quot;: &quot;1005&quot;, &quot;_score&quot;: 1.0, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;zhangsan1&quot;, &quot;nickname&quot;: &quot;zhangsan1&quot;, &quot;sex&quot;: &quot;男&quot;, &quot;age&quot;: &quot;50&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;student&quot;, &quot;_type&quot;: &quot;1001&quot;, &quot;_id&quot;: &quot;1006&quot;, &quot;_score&quot;: 1.0, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;zhangsan2&quot;, &quot;nickname&quot;: &quot;zhangsan2&quot;, &quot;sex&quot;: &quot;女&quot;, &quot;age&quot;: &quot;30&quot; &#125; &#125; ] &#125;&#125; 2） 匹配查询match 匹配类型查询,会把查询条件进行分词,然后进行查询,多个词条之间是 or 的关系​ 在 Postman 中,向 ES 服务器发 GET 请求 :[http://127.0.0.1:9200/student/_search](http://127.0.0.1:9200/student/_search) 1234567&#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;name&quot;:&quot;zhangsan&quot; &#125; &#125;&#125; 响应结果： 12345678910111213141516171819202122232425262728293031323334353637383940414243&#123; &quot;took&quot;: 8, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 1, &quot;successful&quot;: 1, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: &#123; &quot;value&quot;: 2, &quot;relation&quot;: &quot;eq&quot; &#125;, &quot;max_score&quot;: 1.1631508, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;student&quot;, &quot;_type&quot;: &quot;1001&quot;, &quot;_id&quot;: &quot;xnuC3HkB5T_yiydta8QV&quot;, &quot;_score&quot;: 1.1631508, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;zhangsan&quot;, &quot;nickname&quot;: &quot;zhangsan&quot;, &quot;sex&quot;: &quot;男&quot;, &quot;age&quot;: &quot;30&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;student&quot;, &quot;_type&quot;: &quot;1001&quot;, &quot;_id&quot;: &quot;1001&quot;, &quot;_score&quot;: 1.1631508, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;zhangsan&quot;, &quot;nickname&quot;: &quot;zhangsan&quot;, &quot;sex&quot;: &quot;男&quot;, &quot;age&quot;: &quot;30&quot; &#125; &#125; ] &#125;&#125; 3）字段匹配查询multi_match 与 match 类似,不同的是它可以在多个字段中查询。​ 在 Postman 中,向 ES 服务器发 GET 请求 :[http://127.0.0.1:9200/student/_search](http://127.0.0.1:9200/student/_search) 12345678&#123; &quot;query&quot;:&#123; &quot;multi_match&quot;:&#123; &quot;query&quot;:&quot;zhangsan&quot;, &quot;fields&quot;:[&quot;name&quot;,&quot;nickname&quot;] &#125; &#125;&#125; 响应结果 12345678910111213141516171819202122232425262728293031323334353637383940414243&#123; &quot;took&quot;: 7, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 1, &quot;successful&quot;: 1, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: &#123; &quot;value&quot;: 2, &quot;relation&quot;: &quot;eq&quot; &#125;, &quot;max_score&quot;: 1.1631508, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;student&quot;, &quot;_type&quot;: &quot;1001&quot;, &quot;_id&quot;: &quot;xnuC3HkB5T_yiydta8QV&quot;, &quot;_score&quot;: 1.1631508, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;zhangsan&quot;, &quot;nickname&quot;: &quot;zhangsan&quot;, &quot;sex&quot;: &quot;男&quot;, &quot;age&quot;: &quot;30&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;student&quot;, &quot;_type&quot;: &quot;1001&quot;, &quot;_id&quot;: &quot;1001&quot;, &quot;_score&quot;: 1.1631508, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;zhangsan&quot;, &quot;nickname&quot;: &quot;zhangsan&quot;, &quot;sex&quot;: &quot;男&quot;, &quot;age&quot;: &quot;30&quot; &#125; &#125; ] &#125;&#125; 4）关键字精确查询term 查询,精确的关键词匹配查询,不对查询条件进行分词。​ 在 Postman 中,向 ES 服务器发 GET 请求 : [http://127.0.0.1:9200/student/_search](http://127.0.0.1:9200/student/_search)​ 123456789&#123; &quot;query&quot;:&#123; &quot;term&quot;:&#123; &quot;name&quot;:&#123; &quot;value&quot;:&quot;zhangsan&quot; &#125; &#125; &#125;&#125; 响应结果 12345678910111213141516171819202122232425262728293031323334353637383940414243&#123; &quot;took&quot;: 2, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 1, &quot;successful&quot;: 1, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: &#123; &quot;value&quot;: 2, &quot;relation&quot;: &quot;eq&quot; &#125;, &quot;max_score&quot;: 1.1631508, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;student&quot;, &quot;_type&quot;: &quot;1001&quot;, &quot;_id&quot;: &quot;xnuC3HkB5T_yiydta8QV&quot;, &quot;_score&quot;: 1.1631508, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;zhangsan&quot;, &quot;nickname&quot;: &quot;zhangsan&quot;, &quot;sex&quot;: &quot;男&quot;, &quot;age&quot;: &quot;30&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;student&quot;, &quot;_type&quot;: &quot;1001&quot;, &quot;_id&quot;: &quot;1001&quot;, &quot;_score&quot;: 1.1631508, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;zhangsan&quot;, &quot;nickname&quot;: &quot;zhangsan&quot;, &quot;sex&quot;: &quot;男&quot;, &quot;age&quot;: &quot;30&quot; &#125; &#125; ] &#125;&#125; 5）多关键字精确查询terms 查询和 term 查询一样,但它允许你指定多值进行匹配。如果这个字段包含了指定值中的任何一个值,那么这个文档满足条件,类似于 mysql 的 in​ 在 Postman 中,向 ES 服务器发 GET 请求 :[http://127.0.0.1:9200/student/_search](http://127.0.0.1:9200/student/_search) 1234567&#123; &quot;query&quot;:&#123; &quot;terms&quot;:&#123; &quot;name&quot;:[&quot;zhangsan&quot;,&quot;lisi&quot;] &#125; &#125;&#125; 响应结果 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455&#123; &quot;took&quot;: 9, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 1, &quot;successful&quot;: 1, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: &#123; &quot;value&quot;: 3, &quot;relation&quot;: &quot;eq&quot; &#125;, &quot;max_score&quot;: 1.0, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;student&quot;, &quot;_type&quot;: &quot;1001&quot;, &quot;_id&quot;: &quot;xnuC3HkB5T_yiydta8QV&quot;, &quot;_score&quot;: 1.0, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;zhangsan&quot;, &quot;nickname&quot;: &quot;zhangsan&quot;, &quot;sex&quot;: &quot;男&quot;, &quot;age&quot;: &quot;30&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;student&quot;, &quot;_type&quot;: &quot;1001&quot;, &quot;_id&quot;: &quot;1001&quot;, &quot;_score&quot;: 1.0, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;zhangsan&quot;, &quot;nickname&quot;: &quot;zhangsan&quot;, &quot;sex&quot;: &quot;男&quot;, &quot;age&quot;: &quot;30&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;student&quot;, &quot;_type&quot;: &quot;1001&quot;, &quot;_id&quot;: &quot;1002&quot;, &quot;_score&quot;: 1.0, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;lisi&quot;, &quot;nickname&quot;: &quot;lisi&quot;, &quot;sex&quot;: &quot;男&quot;, &quot;age&quot;: &quot;20&quot; &#125; &#125; ] &#125;&#125; 6）指定查询字段默认情况下,Elasticsearch 在搜索的结果中,会把文档中保存在_source 的所有字段都返回。如果我们只想获取其中的部分字段,我们可以添加_source 的过滤。​ 在 Postman 中,向 ES 服务器发 GET 请求 :[http://127.0.0.1:9200/student/_search](http://127.0.0.1:9200/student/_search)​ 12345678&#123; &quot;_source&quot;:[&quot;name&quot;,&quot;nickname&quot;], &quot;query&quot;:&#123; &quot;terms&quot;:&#123; &quot;nickname&quot;:[&quot;zhangsan&quot;] &#125; &#125;&#125; 响应结果 123456789101112131415161718192021222324252627282930313233343536373839&#123; &quot;took&quot;: 3, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 1, &quot;successful&quot;: 1, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: &#123; &quot;value&quot;: 2, &quot;relation&quot;: &quot;eq&quot; &#125;, &quot;max_score&quot;: 1.0, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;student&quot;, &quot;_type&quot;: &quot;1001&quot;, &quot;_id&quot;: &quot;xnuC3HkB5T_yiydta8QV&quot;, &quot;_score&quot;: 1.0, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;zhangsan&quot;, &quot;nickname&quot;: &quot;zhangsan&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;student&quot;, &quot;_type&quot;: &quot;1001&quot;, &quot;_id&quot;: &quot;1001&quot;, &quot;_score&quot;: 1.0, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;zhangsan&quot;, &quot;nickname&quot;: &quot;zhangsan&quot; &#125; &#125; ] &#125;&#125; 7）过滤字段可以通过: includes:来指定想要显示的字段 excludes:来指定不想要显示的字段 在 Postman 中,向 ES 服务器发 GET 请求 :[http://127.0.0.1:9200/student/_search](http://127.0.0.1:9200/student/_search) 12345678910&#123; &quot;_source&quot;:&#123; &quot;includes&quot;:[&quot;name&quot;,&quot;nickname&quot;] &#125;, &quot;query&quot;:&#123; &quot;terms&quot;:&#123; &quot;nickname&quot;:[&quot;zhangsan&quot;] &#125; &#125;&#125; 在 Postman 中,向 ES 服务器发 GET 请求 :[http://127.0.0.1:9200/student/_search](http://127.0.0.1:9200/student/_search) 12345678910&#123; &quot;_source&quot;:&#123; &quot;excludes&quot;:[&quot;name&quot;,&quot;nickname&quot;] &#125;, &quot;query&quot;:&#123; &quot;terms&quot;:&#123; &quot;nickname&quot;:[&quot;zhangsan&quot;] &#125; &#125;&#125; 8) 组合查询bool把各种其它查询通过must(必须式进行组合)、must_not(必须不)、should(应该)的方​ 在 Postman 中,向 ES 服务器发 GET 请求 :[http://127.0.0.1:9200/student/_search](http://127.0.0.1:9200/student/_search)​ 123456789101112131415161718192021222324252627&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;name&quot;: &quot;zhangsan&quot; &#125; &#125; ], &quot;must_not&quot;: [ &#123; &quot;match&quot;: &#123; &quot;age&quot;: &quot;40&quot; &#125; &#125; ], &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;sex&quot;: &quot;男&quot; &#125; &#125; ] &#125; &#125;&#125; 9）范围查询range 查询找出那些落在指定区间内的数字或者时间。range 查询允许以下字符 操作符 说明 gt 大于 gte 大于等于 it 小于 ite 小于等于 在 Postman 中,向 ES 服务器发 GET 请求 ：[http://127.0.0.1:9200/student/_search](http://127.0.0.1:9200/student/_search)​ 12345678910&#123; &quot;query&quot;: &#123; &quot;range&quot;: &#123; &quot;age&quot;: &#123; &quot;gte&quot;: 30, &quot;lte&quot;: 35 &#125; &#125; &#125;&#125; 10） 模糊查询返回包含与搜索字词相似的字词的文档。​ 编辑距离是将一个术语转换为另一个术语所需的一个字符更改的次数。这些更改可以包括: 更改字符(box → fox) 删除字符(black → lack) 插入字符(sic → sick) 转置两个相邻字符(act → cat) 为了找到相似的术语,fuzzy 查询会在指定的编辑距离内创建一组搜索词的所有可能的变体或扩展。然后查询返回每个扩展的完全匹配。​ 通过 fuzziness 修改编辑距离。一般使用默认值 AUTO,根据术语的长度生成编辑距离。​ 在 Postman 中,向 ES 服务器发 GET 请求 :[http://127.0.0.1:9200/student/_search](http://127.0.0.1:9200/student/_search)​ 123456789&#123; &quot;query&quot;: &#123; &quot;fuzzy&quot;: &#123; &quot;title&quot;: &#123; &quot;value&quot;: &quot;zhangsan&quot; &#125; &#125; &#125;&#125; 在 Postman 中,向 ES 服务器发 GET 请求 :[http://127.0.0.1:9200/student/_search](http://127.0.0.1:9200/student/_search) 12345678910&#123; &quot;query&quot;: &#123; &quot;fuzzy&quot;: &#123; &quot;title&quot;: &#123; &quot;value&quot;: &quot;zhangsan&quot;, &quot;fuzziness&quot;: 2 &#125; &#125; &#125;&#125; 11）单字段排序sort 可以让我们按照不同的字段进行排序,并且通过 order 指定排序的方式。desc 降序,asc升序。​ 在 Postman 中,向 ES 服务器发 GET 请求 :[http://127.0.0.1:9200/student/_search](http://127.0.0.1:9200/student/_search) 1234567891011121314&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;name&quot;: &quot;zhangsan&quot; &#125; &#125;, &quot;sort&quot;: [ &#123; &quot;age&quot;: &#123; &quot;order&quot;: &quot;desc&quot; &#125; &#125; ]&#125; 12）多字段排序假定我们想要结合使用 age 和 _score 进行查询,并且匹配的结果首先按照年龄排序,然后按照相关性得分排序​ 在 Postman 中,向 ES 服务器发 GET 请求 :[http://127.0.0.1:9200/student/_search](http://127.0.0.1:9200/student/_search)​ 1234567891011121314151617&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;sort&quot;: [ &#123; &quot;age&quot;: &#123; &quot;order&quot;: &quot;desc&quot; &#125; &#125;, &#123; &quot;_score&quot;: &#123; &quot;order&quot;: &quot;desc&quot; &#125; &#125; ]&#125; 13）高亮查询在进行关键字搜索时,搜索出的内容中的关键字会显示不同的颜色,称之为高亮。Elasticsearch 可以对查询内容中的关键字部分,进行标签和样式(高亮)的设置。​ 在使用 match 查询的同时,加上一个 highlight 属性: pre_tags:前置标签 post_tags:后置标签 fields:需要高亮的字段 title:这里声明 title 字段需要高亮,后面可以为这个字段设置特有配置,也可以空 在 Postman 中,向 ES 服务器发 GET 请求 :[http://127.0.0.1:9200/student/_search](http://127.0.0.1:9200/student/_search) 1234567891011121314&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;name&quot;: &quot;zhangsan&quot; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;pre_tags&quot;: &quot;&lt;font color=&#x27;red&#x27;&gt;&quot;, &quot;post_tags&quot;: &quot;&lt;/font&gt;&quot;, &quot;fields&quot;: &#123; &quot;name&quot;: &#123;&#125; &#125; &#125;&#125; 14）分页查询from:当前页的起始索引,默认从 0 开始。 from = (pageNum - 1) * size size:每页显示多少条​ 在 Postman 中,向 ES 服务器发 GET 请求 :[http://127.0.0.1:9200/student/_search](http://127.0.0.1:9200/student/_search) 1234567891011121314&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;sort&quot;: [ &#123; &quot;age&quot;: &#123; &quot;order&quot;: &quot;desc&quot; &#125; &#125; ], &quot;from&quot;: 0, &quot;size&quot;: 2&#125; 15）聚合查询聚合允许使用者对 es 文档进行统计分析,类似与关系型数据库中的 group by,当然还有很多其他的聚合,例如取最大值、平均值等等。​ 对某个字段取最大值 max ​ 在 Postman 中,向 ES 服务器发 GET 请求 :[http://127.0.0.1:9200/student/_search](http://127.0.0.1:9200/student/_search) 12345678&#123; &quot;aggs&quot;:&#123; &quot;max_age&quot;:&#123; &quot;max&quot;:&#123;&quot;field&quot;:&quot;age&quot;&#125; &#125; &#125;, &quot;size&quot;:0&#125; 对某个字段取最小值 min ​ 在 Postman 中,向 ES 服务器发 GET 请求 :[http://127.0.0.1:9200/student/_search](http://127.0.0.1:9200/student/_search) 12345678&#123; &quot;aggs&quot;:&#123; &quot;min_age&quot;:&#123; &quot;min&quot;:&#123;&quot;field&quot;:&quot;age&quot;&#125; &#125; &#125;, &quot;size&quot;:0&#125; 对某个字段求和 sum 在 Postman 中,向 ES 服务器发 GET 请求 :[http://127.0.0.1:9200/student/_search](http://127.0.0.1:9200/student/_search) 12345678&#123; &quot;aggs&quot;:&#123; &quot;sum_age&quot;:&#123; &quot;sum&quot;:&#123;&quot;field&quot;:&quot;age&quot;&#125; &#125; &#125;, &quot;size&quot;:0&#125; 对某个字段取平均值 avg 在 Postman 中,向 ES 服务器发 GET 请求 :[http://127.0.0.1:9200/student/_search](http://127.0.0.1:9200/student/_search) 12345678910&#123; &quot;aggs&quot;: &#123; &quot;avg_age&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;age&quot; &#125; &#125; &#125;, &quot;size&quot;: 0&#125; 对某个字段的值进行去重之后再取总数 在 Postman 中,向 ES 服务器发 GET 请求 :[http://127.0.0.1:9200/student/_search](http://127.0.0.1:9200/student/_search) 12345678910&#123; &quot;aggs&quot;: &#123; &quot;distinct_age&quot;: &#123; &quot;cardinality&quot;: &#123; &quot;field&quot;: &quot;age&quot; &#125; &#125; &#125;, &quot;size&quot;: 0&#125; State 聚合 ​ stats 聚合,对某个字段一次性返回 count,max,min,avg 和 sum 五个指标在 Postman 中,向 ES 服务器发 GET 请求 :[http://127.0.0.1:9200/student/_search](http://127.0.0.1:9200/student/_search)​ 12345678910&#123; &quot;aggs&quot;: &#123; &quot;stats_age&quot;: &#123; &quot;stats&quot;: &#123; &quot;field&quot;: &quot;age&quot; &#125; &#125; &#125;, &quot;size&quot;: 0&#125; 16）桶聚合查询桶聚和相当于 sql 中的 group by 语句 terms 聚合,分组统计 ​ 在 Postman 中,向 ES 服务器发 GET 请求 :[http://127.0.0.1:9200/student/_search](http://127.0.0.1:9200/student/_search) 12345678910&#123; &quot;aggs&quot;: &#123; &quot;age_groupby&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;age&quot; &#125; &#125; &#125;, &quot;size&quot;: 0&#125; 在 terms 分组下再进行聚合 在 Postman 中,向 ES 服务器发 GET 请求 :[http://127.0.0.1:9200/student/_search](http://127.0.0.1:9200/student/_search) 12345678910&#123; &quot;aggs&quot;: &#123; &quot;age_groupby&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;age&quot; &#125; &#125; &#125;, &quot;size&quot;: 0&#125; 4.Java API 操作Elasticsearch 软件是由 Java 语言开发的,所以也可以通过 Java API 的方式对 Elasticsearch服务进行访问。​ 4.1 创建maven项目我们在 IDEA 开发工具中创建 Maven 项目(模块也可)ES​ 修改 pom 文件,增加 Maven 依赖关系。 12345678910111213141516171819202122232425262728293031323334&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt; &lt;version&gt;7.13.1&lt;/version&gt; &lt;/dependency&gt; &lt;!-- elasticsearch 的客户端 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-rest-high-level-client&lt;/artifactId&gt; &lt;version&gt;7.13.1&lt;/version&gt; &lt;/dependency&gt; &lt;!-- elasticsearch 依赖 2.x 的 log4j --&gt;&lt;!-- &lt;dependency&gt;--&gt;&lt;!-- &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;--&gt;&lt;!-- &lt;artifactId&gt;log4j-api&lt;/artifactId&gt;--&gt;&lt;!-- &lt;version&gt;2.8.2&lt;/version&gt;--&gt;&lt;!-- &lt;/dependency&gt;--&gt;&lt;!-- &lt;dependency&gt;--&gt;&lt;!-- &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;--&gt;&lt;!-- &lt;artifactId&gt;log4j-core&lt;/artifactId&gt;--&gt;&lt;!-- &lt;version&gt;2.8.2&lt;/version&gt;--&gt;&lt;!-- &lt;/dependency&gt;--&gt;&lt;!-- &lt;dependency&gt;--&gt;&lt;!-- &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;--&gt;&lt;!-- &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt;--&gt;&lt;!-- &lt;version&gt;2.9.9&lt;/version&gt;--&gt;&lt;!-- &lt;/dependency&gt;--&gt; &lt;!-- junit 单元测试 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 4.2 客户端对象1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162@SpringBootTestclass BootApplicationTests &#123; private RestHighLevelClient client; //注意:9200 端口为 Elasticsearch 的 Web 通信端口,localhost 为启动 ES 服务的主机名 @BeforeEach public void init() &#123; client = new RestHighLevelClient( RestClient.builder(new HttpHost(&quot;localhost&quot;, 9200, &quot;http&quot;)) ); System.out.println(&quot;restHighLevelClient 初始化完成&quot;); &#125; @AfterEach public void close() &#123; try &#123; client.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; /** * 测试能否正常获取到连接 */ @Test void contextLoads() &#123; System.out.println(client); &#125;&#125;class User &#123; private String name; private Integer age; private String sex; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public Integer getAge() &#123; return age; &#125; public void setAge(Integer age) &#123; this.age = age; &#125; public String getSex() &#123; return sex; &#125; public void setSex(String sex) &#123; this.sex = sex; &#125;&#125; 4.3 索引操作4.3.1 创建索引12345678910@Testvoid testCreateIndex() throws IOException &#123; //创建索引请求对象 CreateIndexRequest request = new CreateIndexRequest(&quot;aaa&quot;); //发送请求获取响应 CreateIndexResponse response = client.indices().create(request, RequestOptions.DEFAULT); //获取响应结果 boolean acknowledged = response.isAcknowledged(); System.out.println(&quot;acknowledged = &quot; + acknowledged);&#125; 4.3.2 查看索引12345678@Testvoid testFindIndex() throws Exception &#123; GetIndexRequest request = new GetIndexRequest(&quot;aaa&quot;); GetIndexResponse response = client.indices().get(request, RequestOptions.DEFAULT); System.out.println(&quot;aliases:&quot; + response.getAliases()); System.out.println(&quot;mappings:&quot; + response.getMappings()); System.out.println(&quot;settings:&quot; + response.getSettings());&#125; 4.3.3 删除索引1234567891011@Testvoid testDeleteIndex() throws Exception &#123; // 删除索引 - 请求对象 DeleteIndexRequest request = new DeleteIndexRequest(&quot;user&quot;); // 发送请求,获取响应 AcknowledgedResponse response = client.indices().delete(request, RequestOptions.DEFAULT); // 操作结果 System.out.println(&quot;操作结果 : &quot; + response.isAcknowledged());&#125; 4.4 文档操作4.4.1 新增文档1234567891011121314151617181920212223242526@Testvoid testCreateDocuMent() throws Exception &#123; //新增文档请求对象 IndexRequest request = new IndexRequest(); //设置索引及唯一性标识 request.index(&quot;user&quot;).id(&quot;1001&quot;); //创建数据对象 User user = new User(); user.setName(&quot;二十&quot;); user.setAge(22); user.setSex(&quot;男&quot;); Gson gson = new Gson(); String s = gson.toJson(user); //添加文档数据，数据格式为JSON request.source(s); //客户端发送请求，获取响应对象 IndexResponse response = client.index(request, RequestOptions.DEFAULT); //打印结果 String index = response.getIndex(); String id = response.getId(); DocWriteResponse.Result result = response.getResult(); System.out.println(&quot;index = &quot; + index); System.out.println(&quot;id = &quot; + id); System.out.println(&quot;result = &quot; + result);&#125; 4.4.2 修改文档123456789101112131415161718@Testpublic void testUpdateDocument() throws Exception &#123; //修改文档 请求对象 UpdateRequest request = new UpdateRequest(); //配置修改参数 request.index(&quot;user&quot;).id(&quot;1001&quot;); //设置请求体，对数据进行修改 request.doc(XContentType.JSON, &quot;sex&quot;, &quot;女&quot;); //客户端发送请求，获取响应对象 UpdateResponse response = client.update(request, RequestOptions.DEFAULT); //打印结果 String index = response.getIndex(); String id = response.getId(); DocWriteResponse.Result result = response.getResult(); System.out.println(&quot;index = &quot; + index); System.out.println(&quot;id = &quot; + id); System.out.println(&quot;result = &quot; + result);&#125; 4.4.3 查询文档1234567891011121314@Testpublic void testSelectDocuments() throws Exception &#123; //1.创建请求对象 GetRequest request = new GetRequest().index(&quot;user&quot;).id(&quot;1001&quot;); //2.客户端发送请求,获取响应对象 GetResponse response = client.get(request, RequestOptions.DEFAULT); //3.打印结果信息 System.out.println(&quot;_index:&quot; + response.getIndex()); System.out.println(&quot;_type:&quot; + response.getType()); System.out.println(&quot;_id:&quot; + response.getId()); //这时得到的source是json，需要转化为对象 System.out.println(&quot;source:&quot; + response.getSourceAsString());&#125; 4.4.4 删除文档12345678910@Testpublic void testDeleteDoc() throws Exception &#123; //创建请求对象 DeleteRequest request = new DeleteRequest().index(&quot;user&quot;).id(&quot;1&quot;); //客户端发送请求,获取响应对象 DeleteResponse response = client.delete(request, RequestOptions.DEFAULT); //打印信息 System.out.println(response.toString());&#125; 4.5 批量操作4.5.1 添加1234567891011121314@Testpublic void testBatch() throws Exception &#123; //创建批量新增请求对象 BulkRequest request = new BulkRequest(); request.add(new IndexRequest().index(&quot;user&quot;).id(&quot;1001&quot;).source(XContentType.JSON, &quot;name&quot;, &quot;zhangsan&quot;)); request.add(new IndexRequest().index(&quot;user&quot;).id(&quot;1002&quot;).source(XContentType.JSON, &quot;name&quot;, &quot;lisi&quot;)); request.add(new IndexRequest().index(&quot;user&quot;).id(&quot;1003&quot;).source(XContentType.JSON, &quot;name&quot;, &quot;wangwu&quot;)); //客户端发送请求,获取响应对象 BulkResponse responses = client.bulk(request, RequestOptions.DEFAULT); //打印结果信息 System.out.println(&quot;took:&quot; + responses.getTook()); System.out.println(&quot;items:&quot; + responses.getItems());&#125; 4.5.2 删除1234567891011121314@Testpublic void testBatchDel() throws Exception &#123; //创建批量删除请求对象 BulkRequest request = new BulkRequest(); request.add(new DeleteRequest().index(&quot;user&quot;).id(&quot;1001&quot;)); request.add(new DeleteRequest().index(&quot;user&quot;).id(&quot;1002&quot;)); request.add(new DeleteRequest().index(&quot;user&quot;).id(&quot;1003&quot;)); //客户端发送请求,获取响应对象 BulkResponse responses = client.bulk(request, RequestOptions.DEFAULT); //打印结果信息 System.out.println(&quot;took:&quot; + responses.getTook()); System.out.println(&quot;items:&quot; + responses.getItems());&#125; 4.6 高级查询4.6.1 请求体查询12345678910111213141516171819202122232425262728/** * 请求体查询 - 查询所有索引数据 */@Testpublic void testQueryIndexs() throws Exception &#123; // 创建搜索请求对象 SearchRequest request = new SearchRequest(); request.indices(&quot;student&quot;); // 构建查询的请求体 SearchSourceBuilder sourceBuilder = new SearchSourceBuilder(); // 查询所有数据 sourceBuilder.query(QueryBuilders.matchAllQuery()); request.source(sourceBuilder); SearchResponse response = client.search(request, RequestOptions.DEFAULT); // 查询匹配 SearchHits hits = response.getHits(); System.out.println(&quot;took:&quot; + response.getTook()); System.out.println(&quot;timeout:&quot; + response.isTimedOut()); System.out.println(&quot;total:&quot; + hits.getTotalHits()); System.out.println(&quot;MaxScore:&quot; + hits.getMaxScore()); System.out.println(&quot;hits========&gt;&gt;&quot;); for (SearchHit hit : hits) &#123; //输出每条查询的结果信息 System.out.println(hit.getSourceAsString()); &#125; System.out.println(&quot;&lt;&lt;========&quot;);&#125; 4.6.2 term查询123456789101112131415161718192021222324252627/** * term 查询,查询条件为关键字 */@Testpublic void testQueryTerm() throws Exception &#123; // 创建搜索请求对象 SearchRequest request = new SearchRequest(); request.indices(&quot;student&quot;); // 构建查询的请求体 SearchSourceBuilder sourceBuilder = new SearchSourceBuilder(); sourceBuilder.query(QueryBuilders.termQuery(&quot;age&quot;, &quot;30&quot;)); request.source(sourceBuilder); SearchResponse response = client.search(request, RequestOptions.DEFAULT); // 查询匹配 SearchHits hits = response.getHits(); System.out.println(&quot;took:&quot; + response.getTook()); System.out.println(&quot;timeout:&quot; + response.isTimedOut()); System.out.println(&quot;total:&quot; + hits.getTotalHits()); System.out.println(&quot;MaxScore:&quot; + hits.getMaxScore()); System.out.println(&quot;hits========&gt;&gt;&quot;); for (SearchHit hit : hits) &#123; //输出每条查询的结果信息 System.out.println(hit.getSourceAsString()); &#125; System.out.println(&quot;&lt;&lt;========&quot;);&#125; 4.6.3 分页查询1234567891011121314151617181920212223242526272829@Testpublic void testQueryLimit() throws Exception &#123; // 创建搜索请求对象 SearchRequest request = new SearchRequest(); request.indices(&quot;student&quot;); // 构建查询的请求体 SearchSourceBuilder sourceBuilder = new SearchSourceBuilder(); sourceBuilder.query(QueryBuilders.matchAllQuery()); // 分页查询 // 当前页其实索引(第一条数据的顺序号),from sourceBuilder.from(0); // 每页显示多少条 size sourceBuilder.size(2); request.source(sourceBuilder); SearchResponse response = client.search(request, RequestOptions.DEFAULT); // 查询匹配 SearchHits hits = response.getHits(); System.out.println(&quot;took:&quot; + response.getTook()); System.out.println(&quot;timeout:&quot; + response.isTimedOut()); System.out.println(&quot;total:&quot; + hits.getTotalHits()); System.out.println(&quot;MaxScore:&quot; + hits.getMaxScore()); System.out.println(&quot;hits========&gt;&gt;&quot;); for (SearchHit hit : hits) &#123; //输出每条查询的结果信息 System.out.println(hit.getSourceAsString()); &#125; System.out.println(&quot;&lt;&lt;========&quot;);&#125; 4.6.4 数据排序1234567891011121314151617181920212223242526@Testpublic void testQueryOrder() throws Exception &#123; // 创建搜索请求对象 SearchRequest request = new SearchRequest(); request.indices(&quot;student&quot;); // 构建查询的请求体 SearchSourceBuilder sourceBuilder = new SearchSourceBuilder(); sourceBuilder.query(QueryBuilders.matchAllQuery()); // 排序 sourceBuilder.sort(&quot;age&quot;, SortOrder.ASC); request.source(sourceBuilder); SearchResponse response = client.search(request, RequestOptions.DEFAULT); // 查询匹配 SearchHits hits = response.getHits(); System.out.println(&quot;took:&quot; + response.getTook()); System.out.println(&quot;timeout:&quot; + response.isTimedOut()); System.out.println(&quot;total:&quot; + hits.getTotalHits()); System.out.println(&quot;MaxScore:&quot; + hits.getMaxScore()); System.out.println(&quot;hits========&gt;&gt;&quot;); for (SearchHit hit : hits) &#123; //输出每条查询的结果信息 System.out.println(hit.getSourceAsString()); &#125; System.out.println(&quot;&lt;&lt;========&quot;);&#125; 4.6.5 过滤字段12345678910111213141516171819202122232425262728@Testpublic void testQueryFilter() throws Exception &#123; // 创建搜索请求对象 SearchRequest request = new SearchRequest(); request.indices(&quot;student&quot;); // 构建查询的请求体 SearchSourceBuilder sourceBuilder = new SearchSourceBuilder(); sourceBuilder.query(QueryBuilders.matchAllQuery()); //查询字段过滤 String[] excludes = &#123;&#125;; String[] includes = &#123;&quot;name&quot;, &quot;age&quot;&#125;; sourceBuilder.fetchSource(includes, excludes); request.source(sourceBuilder); SearchResponse response = client.search(request, RequestOptions.DEFAULT); // 查询匹配 SearchHits hits = response.getHits(); System.out.println(&quot;took:&quot; + response.getTook()); System.out.println(&quot;timeout:&quot; + response.isTimedOut()); System.out.println(&quot;total:&quot; + hits.getTotalHits()); System.out.println(&quot;MaxScore:&quot; + hits.getMaxScore()); System.out.println(&quot;hits========&gt;&gt;&quot;); for (SearchHit hit : hits) &#123; //输出每条查询的结果信息 System.out.println(hit.getSourceAsString()); &#125; System.out.println(&quot;&lt;&lt;========&quot;);&#125; 4.6.6 Bool查询12345678910111213141516171819202122232425262728293031@Testpublic void testQueryBool() throws Exception &#123; // 创建搜索请求对象 SearchRequest request = new SearchRequest(); request.indices(&quot;student&quot;); // 构建查询的请求体 SearchSourceBuilder sourceBuilder = new SearchSourceBuilder(); BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery(); // 必须包含 boolQueryBuilder.must(QueryBuilders.matchQuery(&quot;age&quot;, &quot;30&quot;)); // 一定不含 boolQueryBuilder.mustNot(QueryBuilders.matchQuery(&quot;name&quot;, &quot;zhangsan&quot;)); // 可能包含 boolQueryBuilder.should(QueryBuilders.matchQuery(&quot;sex&quot;, &quot;男&quot;)); sourceBuilder.query(boolQueryBuilder); request.source(sourceBuilder); SearchResponse response = client.search(request, RequestOptions.DEFAULT); // 查询匹配 SearchHits hits = response.getHits(); System.out.println(&quot;took:&quot; + response.getTook()); System.out.println(&quot;timeout:&quot; + response.isTimedOut()); System.out.println(&quot;total:&quot; + hits.getTotalHits()); System.out.println(&quot;MaxScore:&quot; + hits.getMaxScore()); System.out.println(&quot;hits========&gt;&gt;&quot;); for (SearchHit hit : hits) &#123; //输出每条查询的结果信息 System.out.println(hit.getSourceAsString()); &#125; System.out.println(&quot;&lt;&lt;========&quot;);&#125; 4.6.7 范围查询1234567891011121314151617181920212223242526272829@Testpublic void testQueryRange() throws Exception &#123; // 创建搜索请求对象 SearchRequest request = new SearchRequest(); request.indices(&quot;student&quot;); // 构建查询的请求体 SearchSourceBuilder sourceBuilder = new SearchSourceBuilder(); RangeQueryBuilder rangeQuery = QueryBuilders.rangeQuery(&quot;age&quot;); // 大于等于 rangeQuery.gte(&quot;30&quot;); // 小于等于 rangeQuery.lte(&quot;40&quot;); sourceBuilder.query(rangeQuery); request.source(sourceBuilder); SearchResponse response = client.search(request, RequestOptions.DEFAULT); // 查询匹配 SearchHits hits = response.getHits(); System.out.println(&quot;took:&quot; + response.getTook()); System.out.println(&quot;timeout:&quot; + response.isTimedOut()); System.out.println(&quot;total:&quot; + hits.getTotalHits()); System.out.println(&quot;MaxScore:&quot; + hits.getMaxScore()); System.out.println(&quot;hits========&gt;&gt;&quot;); for (SearchHit hit : hits) &#123; //输出每条查询的结果信息 System.out.println(hit.getSourceAsString()); &#125; System.out.println(&quot;&lt;&lt;========&quot;);&#125; 4.6.8 模糊查询12345678910111213141516171819202122232425@Testpublic void testQueryLike() throws Exception &#123; // 创建搜索请求对象 SearchRequest request = new SearchRequest(); request.indices(&quot;student&quot;); // 构建查询的请求体 SearchSourceBuilder sourceBuilder = new SearchSourceBuilder(); sourceBuilder.query(QueryBuilders.fuzzyQuery(&quot;name&quot;, &quot;zhangsan&quot;).fuzziness(Fuzziness.ONE)); request.source(sourceBuilder); SearchResponse response = client.search(request, RequestOptions.DEFAULT); // 查询匹配 SearchHits hits = response.getHits(); System.out.println(&quot;took:&quot; + response.getTook()); System.out.println(&quot;timeout:&quot; + response.isTimedOut()); System.out.println(&quot;total:&quot; + hits.getTotalHits()); System.out.println(&quot;MaxScore:&quot; + hits.getMaxScore()); System.out.println(&quot;hits========&gt;&gt;&quot;); for (SearchHit hit : hits) &#123; //输出每条查询的结果信息 System.out.println(hit.getSourceAsString()); &#125; System.out.println(&quot;&lt;&lt;========&quot;);&#125; 4.6.9 高亮查询12345678910111213141516171819202122232425262728293031323334353637383940@Testpublic void testQueryHighLeight() throws Exception &#123; // 高亮查询 SearchRequest request = new SearchRequest().indices(&quot;student&quot;); //2.创建查询请求体构建器 SearchSourceBuilder sourceBuilder = new SearchSourceBuilder(); //构建查询方式:高亮查询 TermsQueryBuilder termsQueryBuilder = QueryBuilders.termsQuery(&quot;name&quot;, &quot;zhangsan&quot;); //设置查询方式 sourceBuilder.query(termsQueryBuilder); //构建高亮字段 HighlightBuilder highlightBuilder = new HighlightBuilder(); highlightBuilder.preTags(&quot;&lt;font color=&#x27;red&#x27;&gt;&quot;);//设置标签前缀 highlightBuilder.postTags(&quot;&lt;/font&gt;&quot;);//设置标签后缀 highlightBuilder.field(&quot;name&quot;);//设置高亮字段 //设置高亮构建对象 sourceBuilder.highlighter(highlightBuilder); //设置请求体 request.source(sourceBuilder); //3.客户端发送请求,获取响应对象 SearchResponse response = client.search(request, RequestOptions.DEFAULT); //4.打印响应结果 SearchHits hits = response.getHits(); System.out.println(&quot;took::&quot; + response.getTook()); System.out.println(&quot;time_out::&quot; + response.isTimedOut()); System.out.println(&quot;total::&quot; + hits.getTotalHits()); System.out.println(&quot;max_score::&quot; + hits.getMaxScore()); System.out.println(&quot;hits::::&gt;&gt;&quot;); for (SearchHit hit : hits) &#123; String sourceAsString = hit.getSourceAsString(); System.out.println(sourceAsString); //打印高亮结果 Map&lt;String, HighlightField&gt; highlightFields = hit.getHighlightFields(); System.out.println(highlightFields); &#125; System.out.println(&quot;&lt;&lt;::::&quot;);&#125; 4.6.10 聚合查询-最大年龄123456789101112131415@Testpublic void testMaxAge() throws Exception &#123; // 高亮查询 SearchRequest request = new SearchRequest().indices(&quot;student&quot;); SearchSourceBuilder sourceBuilder = new SearchSourceBuilder(); sourceBuilder.aggregation(AggregationBuilders.max(&quot;maxAge&quot;).field(&quot;age&quot;)); //设置请求体 request.source(sourceBuilder); //3.客户端发送请求,获取响应对象 SearchResponse response = client.search(request, RequestOptions.DEFAULT); //4.打印响应结果 SearchHits hits = response.getHits(); System.out.println(response);&#125; 4.6.11 聚合查询-分组统计123456789101112131415@Testpublic void testGroupCount() throws Exception &#123; // 高亮查询 SearchRequest request = new SearchRequest().indices(&quot;student&quot;); SearchSourceBuilder sourceBuilder = new SearchSourceBuilder(); sourceBuilder.aggregation(AggregationBuilders.terms(&quot;age_groupby&quot;).field(&quot;age&quot;)); //设置请求体 request.source(sourceBuilder); //3.客户端发送请求,获取响应对象 SearchResponse response = client.search(request, RequestOptions.DEFAULT); //4.打印响应结果 SearchHits hits = response.getHits(); System.out.println(hits);&#125;","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"https://yinhuidong.github.io/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"https://yinhuidong.github.io/tags/Elasticsearch/"}]},{"title":"MySQL[十二]InnoDB之BufferPool","slug":"MySQL/MySQL[十二]InnoDB之BufferPool","date":"2022-01-11T16:00:00.000Z","updated":"2022-01-12T00:42:38.236Z","comments":true,"path":"2022/01/12/MySQL/MySQL[十二]InnoDB之BufferPool/","link":"","permalink":"https://yinhuidong.github.io/2022/01/12/MySQL/MySQL[%E5%8D%81%E4%BA%8C]InnoDB%E4%B9%8BBufferPool/","excerpt":"","text":"一，缓存的重要性对于使用InnoDB作为存储引擎的表来说，不管是用于存储用户数据的索引（包括聚簇索引和二级索引），还是各种系统数据，都是以页的形式存放在表空间中的，而所谓的表空间只不过是InnoDB对文件系统上一个或几个实际文件的抽象，也就是说我们的数据说到底还是存储在磁盘上的。但是磁盘太慢了，所以InnoDB存储引擎在处理客户端的请求时，当需要访问某个页的数据时，就会把完整的页的数据全部加载到内存中，也就是说即使我们只需要访问一个页的一条记录，那也需要先把整个页的数据加载到内存中。将整个页加载到内存中后就可以进行读写访问了，在进行完读写访问之后并不着急把该页对应的内存空间释放掉，而是将其缓存起来，这样将来有请求再次访问该页面时，就可以省去磁盘IO的开销了。 二，InnoDB的Buffer Pool 1.啥是个Buffer Pool为了缓存磁盘中的页，在MySQL服务器启动的时候就向操作系统申请了一片连续的内存，叫做Buffer Pool（中文名是缓冲池）。那它有多大呢？这个其实看我们机器的配置，默认情况下Buffer Pool只有128M大小，但是可以在启动服务器的时候配置innodb_buffer_pool_size参数的值，它表示Buffer Pool的大小，就像这样： 12[server]innodb_buffer_pool_size = 268435456 其中，268435456的单位是字节，也就是我指定Buffer Pool的大小为256M。需要注意的是，Buffer Pool也不能太小，最小值为5M(当小于该值时会自动设置成5M)。 2.Buffer Pool内部组成Buffer Pool中默认的缓存页大小和在磁盘上默认的页大小是一样的，都是16KB。为了更好的管理这些在Buffer Pool中的缓存页，InnoDB为每一个缓存页都创建了一些所谓的控制信息，这些控制信息包括该页所属的表空间编号、页号、缓存页在Buffer Pool中的地址、链表节点信息、一些锁信息以及LSN信息（锁和LSN先忽略），当然还有一些别的控制信息，暂时省略。 每个缓存页对应的控制信息占用的内存大小是相同的，我们就把每个页对应的控制信息占用的一块内存称为一个控制块，控制块和缓存页是一一对应的，它们都被存放到 Buffer Pool 中，其中控制块被存放到 Buffer Pool 的前边，缓存页被存放到 Buffer Pool 后边。 每一个控制块都对应一个缓存页，那在分配足够多的控制块和缓存页后，可能剩余的那点儿空间不够一对控制块和缓存页的大小，这个用不到的那点儿内存空间就被称为碎片。当然，如果把Buffer Pool的大小设置的刚刚好的话，也可能不会产生碎片。 每个控制块大约占用缓存页大小的5%，在MySQL5.7.21这个版本中，每个控制块占用的大小是808字节。而我们设置的innodb_buffer_pool_size并不包含这部分控制块占用的内存空间大小，也就是说InnoDB在为Buffer Pool向操作系统申请连续的内存空间时，这片连续的内存空间一般会比innodb_buffer_pool_size的值大5%左右。 3.free链表的管理当我们最初启动MySQL服务器的时候，需要完成对Buffer Pool的初始化过程，就是先向操作系统申请Buffer Pool的内存空间，然后把它划分成若干对控制块和缓存页。但是此时并没有真实的磁盘页被缓存到Buffer Pool中（因为还没有用到），之后随着程序的运行，会不断的有磁盘上的页被缓存到Buffer Pool中。那么问题来了，从磁盘上读取一个页到Buffer Pool中的时候该放到哪个缓存页的位置呢？或者说怎么区分Buffer Pool中哪些缓存页是空闲的，哪些已经被使用了呢？我们最好在某个地方记录一下Buffer Pool中哪些缓存页是可用的，我们可以把所有空闲的缓存页对应的控制块作为一个节点放到一个链表中，这个链表也可以被称作free链表（或者说空闲链表）。刚刚完成初始化的Buffer Pool中所有的缓存页都是空闲的，所以每一个缓存页对应的控制块都会被加入到free链表中。 为了管理好这个free链表，特意为这个链表定义了一个基节点，里边儿包含着链表的头节点地址，尾节点地址，以及当前链表中节点的数量等信息。这里需要注意的是，链表的基节点占用的内存空间并不包含在为Buffer Pool申请的一大片连续内存空间之内，而是单独申请的一块内存空间。 链表基节点占用的内存空间并不大，在MySQL5.7.21这个版本里，每个基节点只占用40字节大小。后边我们即将介绍许多不同的链表，它们的基节点和free链表的基节点的内存分配方式是一样的，都是单独申请的一块40字节大小的内存空间，并不包含在为Buffer Pool申请的一大片连续内存空间之内。 有了这个free链表之后，每当需要从磁盘中加载一个页到Buffer Pool中时，就从free链表中取一个空闲的缓存页，并且把该缓存页对应的控制块的信息填上（就是该页所在的表空间、页号之类的信息），然后把该缓存页对应的free链表节点从链表中移除，表示该缓存页已经被使用了。 4.缓存页的哈希处理当我们需要访问某个页中的数据时，就会把该页从磁盘加载到Buffer Pool中，如果该页已经在Buffer Pool中的话直接使用就可以了。那么问题也就来了，我们怎么知道该页在不在Buffer Pool中呢？难不成需要依次遍历Buffer Pool中各个缓存页么？ 我们其实是根据表空间号 + 页号来定位一个页的，也就相当于表空间号 + 页号是一个key，缓存页就是对应的value，怎么通过一个key来快速找着一个value呢？那肯定是哈希表。 所以我们可以用表空间号 + 页号作为key，缓存页作为value创建一个哈希表，在需要访问某个页的数据时，先从哈希表中根据表空间号 + 页号看看有没有对应的缓存页，如果有，直接使用该缓存页就好，如果没有，那就从free链表中选一个空闲的缓存页，然后把磁盘中对应的页加载到该缓存页的位置。 5.flush链表的管理如果我们修改了Buffer Pool中某个缓存页的数据，那它就和磁盘上的页不一致了，这样的缓存页也被称为脏页（英文名：dirty page）。最简单的做法就是每发生一次修改就立即同步到磁盘上对应的页上，但是频繁的往磁盘中写数据会严重的影响程序的性能。所以每次修改缓存页后，我们一般一般异步同步磁盘。 但是如果不立即同步到磁盘的话，那之后再同步的时候我们怎么知道Buffer Pool中哪些页是脏页，哪些页从来没被修改过呢？创建一个存储脏页的链表，凡是修改过的缓存页对应的控制块都会作为一个节点加入到一个链表中，因为这个链表节点对应的缓存页都是需要被刷新到磁盘上的，所以也叫flush链表。链表的构造和free链表差不多。 6.LRU链表的管理6.1 缓存不够的窘境Buffer Pool对应的内存大小毕竟是有限的，如果需要缓存的页占用的内存大小超过了Buffer Pool大小，也就是free链表中已经没有多余的空闲缓存页咋办？当然是把某些旧的缓存页从Buffer Pool中移除，然后再把新的页放进来， 那么移除哪些缓存页呢？ 我们设立Buffer Pool的初衷就是想减少和磁盘的IO交互，最好每次在访问某个页的时候它都已经被缓存到Buffer Pool中了。假设我们一共访问了n次页，那么被访问的页已经在缓存中的次数除以n就是所谓的缓存命中率，我们的期望就是让缓存命中率越高越好。所以是留下最近很频繁使用的。 6.2简单的LRU链表管理Buffer Pool的缓存页其实也是这个道理，当Buffer Pool中不再有空闲的缓存页时，就需要淘汰掉部分最近很少使用的缓存页。怎么知道哪些缓存页最近频繁使用，哪些最近很少使用呢？我们可以再创建一个链表，由于这个链表是为了按照最近最少使用的原则去淘汰缓存页的，所以这个链表可以被称为LRU链表（LRU的英文全称：Least Recently Used）。当我们需要访问某个页时，可以这样处理LRU链表： 如果该页不在Buffer Pool中，在把该页从磁盘加载到Buffer Pool中的缓存页时，就把该缓存页对应的控制块作为节点塞到链表的头部。 如果该页已经缓存在Buffer Pool中，则直接把该页对应的控制块移动到LRU链表的头部。 也就是说：只要我们使用到某个缓存页，就把该缓存页调整到LRU链表的头部，这样LRU链表尾部就是最近最少使用的缓存页了。 所以当Buffer Pool中的空闲缓存页使用完时，到LRU链表的尾部找些缓存页淘汰。 6.3划分区域的LRU链表上边的这个简单的LRU链表有问题，因为存在这两种比较尴尬的情况： 情况一：InnoDB提供了一个服务——预读（英文名：read ahead）。所谓预读，就是InnoDB认为执行当前的请求可能之后会读取某些页面，就预先把它们加载到Buffer Pool中。根据触发方式的不同，预读又可以细分为下边两种： 线性预读InnoDB提供了一个系统变量innodb_read_ahead_threshold，如果顺序访问了某个区（extent）的页面超过这个系统变量的值，就会触发一次异步读取下一个区中全部的页面到Buffer Pool的请求，异步读取意味着从磁盘中加载这些被预读的页面并不会影响到当前工作线程的正常执行。这个innodb_read_ahead_threshold系统变量的值默认是56，我们可以在服务器启动时通过启动参数或者服务器运行过程中直接调整该系统变量的值，不过它是一个全局变量，注意使用SET GLOBAL命令来修改。 InnoDB是怎么实现异步读取的呢？在Windows或者Linux平台上，可能是直接调用操作系统内核提供的AIO接口，在其它类Unix操作系统中，使用了一种模拟AIO接口的方式来实现异步读取，其实就是让别的线程去读取需要预读的页面。 随机预读如果Buffer Pool中已经缓存了某个区的13个连续的页面，不论这些页面是不是顺序读取的，都会触发一次异步读取本区中所有其的页面到Buffer Pool的请求。InnoDB同时提供了innodb_random_read_ahead系统变量，它的默认值为OFF，也就意味着InnoDB并不会默认开启随机预读的功能，如果我们想开启该功能，可以通过修改启动参数或者直接使用SET GLOBAL命令把该变量的值设置为ON。 如果预读到Buffer Pool中的页成功的被使用到，那就可以极大的提高语句执行的效率。可是如果用不到呢？这些预读的页都会放到LRU链表的头部，但是如果此时Buffer Pool的容量不太大而且很多预读的页面都没有用到的话，这就会导致处在LRU链表尾部的一些缓存页会很快的被淘汰掉，也就是所谓的劣币驱逐良币，会大大降低缓存命中率。 情况二：有一些扫描全表的查询语句（比如没有建立合适的索引或者压根儿没有WHERE子句的查询）。扫描全表意味着将访问到该表所在的所有页！假设这个表中记录非常多的话，那该表会占用特别多的页，当需要访问这些页时，会把它们统统都加载到Buffer Pool中，这也就意味着Buffer Pool中的所有页都被换了一次，其他查询语句在执行时又得执行一次从磁盘加载到Buffer Pool的操作。而这种全表扫描的语句执行的频率也不高，每次执行都要把Buffer Pool中的缓存页换一次，这严重的影响到其他查询对 Buffer Pool的使用，从而大大降低了缓存命中率。 总结一下上边说的可能降低Buffer Pool的两种情况： 加载到Buffer Pool中的页不一定被用到。 如果非常多的使用频率偏低的页被同时加载到Buffer Pool时，可能会把那些使用频率非常高的页从Buffer Pool中淘汰掉。 因为有这两种情况的存在，所以InnoDB把这个LRU链表按照一定比例分成两截，分别是： 一部分存储使用频率非常高的缓存页，所以这一部分链表也叫做热数据，或者称young区域。 另一部分存储使用频率不是很高的缓存页，所以这一部分链表也叫做冷数据，或者称old区域。 我们是按照某个比例将LRU链表分成两半的，不是某些节点固定是young区域的，某些节点固定是old区域的，随着程序的运行，某个节点所属的区域也可能发生变化。那这个划分成两截的比例怎么确定呢？对于InnoDB存储引擎来说，我们可以通过查看系统变量innodb_old_blocks_pct的值来确定old区域在LRU链表中所占的比例，比方说这样： 1234567mysql&gt; SHOW VARIABLES LIKE &#x27;innodb_old_blocks_pct&#x27;;+-----------------------+-------+| Variable_name | Value |+-----------------------+-------+| innodb_old_blocks_pct | 37 |+-----------------------+-------+1 row in set (0.01 sec) 从结果可以看出来，默认情况下，old区域在LRU链表中所占的比例是37%，也就是说old区域大约占LRU链表的3/8。这个比例我们是可以设置的，我们可以在启动时修改innodb_old_blocks_pct参数来控制old区域在LRU链表中所占的比例，比方说这样修改配置文件： 12[server]innodb_old_blocks_pct = 40 这样我们在启动服务器后，old区域占LRU链表的比例就是40%。当然，如果在服务器运行期间，我们也可以修改这个系统变量的值，不过需要注意的是，这个系统变量属于全局变量，一经修改，会对所有客户端生效，所以我们只能这样修改： 1SET GLOBAL innodb_old_blocks_pct = 40; 有了这个被划分成young和old区域的LRU链表之后，InnoDB就可以针对我们上边提到的两种可能降低缓存命中率的情况进行优化： 针对预读的页面可能不进行后续访问情况的优化InnoDB规定，当磁盘上的某个页面在初次加载到Buffer Pool中的某个缓存页时，该缓存页对应的控制块会被放到old区域的头部。这样针对预读到Buffer Pool却不进行后续访问的页面就会被逐渐从old区域逐出，而不会影响young区域中被使用比较频繁的缓存页。 针对全表扫描时，短时间内访问大量使用频率非常低的页面情况的优化在进行全表扫描时，虽然首次被加载到Buffer Pool的页被放到了old区域的头部，但是后续会被马上访问到，每次进行访问的时候又会把该页放到young区域的头部，这样仍然会把那些使用频率比较高的页面给顶下去。全表扫描有一个特点，那就是它的执行频率非常低，而且在执行全表扫描的过程中，即使某个页面中有很多条记录，也就是去多次访问这个页面所花费的时间也是非常少的。所以我们只需要规定，在对某个处在old区域的缓存页进行第一次访问时就在它对应的控制块中记录下来这个访问时间，如果后续的访问时间与第一次访问的时间在某个时间间隔内，那么该页面就不会被从old区域移动到young区域的头部，否则将它移动到young区域的头部。上述的这个间隔时间是由系统变量innodb_old_blocks_time控制的： 1234567mysql&gt; SHOW VARIABLES LIKE &#x27;innodb_old_blocks_time&#x27;;+------------------------+-------+| Variable_name | Value |+------------------------+-------+| innodb_old_blocks_time | 1000 |+------------------------+-------+1 row in set (0.01 sec) 这个innodb_old_blocks_time的默认值是1000，它的单位是毫秒，也就意味着对于从磁盘上被加载到LRU链表的old区域的某个页来说，如果第一次和最后一次访问该页面的时间间隔小于1s（很明显在一次全表扫描的过程中，多次访问一个页面中的时间不会超过1s），那么该页是不会被加入到young区域的。 当然，像innodb_old_blocks_pct一样，我们也可以在服务器启动或运行时设置innodb_old_blocks_time的值。 这里需要注意的是，如果我们把innodb_old_blocks_time的值设置为0，那么每次我们访问一个页面时就会把该页面放到young区域的头部。 综上所述，正是因为将LRU链表划分为young和old区域这两个部分，又添加了innodb_old_blocks_time这个系统变量，才使得预读机制和全表扫描造成的缓存命中率降低的问题得到了遏制，因为用不到的预读页面以及全表扫描的页面都只会被放到old区域，而不影响young区域中的缓存页。 6.4 更进一步优化LRU链表对于young区域的缓存页来说，我们每次访问一个缓存页就要把它移动到LRU链表的头部，这样开销太大了，毕竟在young区域的缓存页都是热点数据，也就是可能被经常访问的，这样频繁的对LRU链表进行节点移动操作不太好，为了解决这个问题其实我们还可以提出一些优化策略，比如只有被访问的缓存页位于young区域的1/4的后边，才会被移动到LRU链表头部，这样就可以降低调整LRU链表的频率，从而提升性能（也就是说如果某个缓存页对应的节点在young区域的1/4中，再次访问该缓存页时也不会将其移动到LRU链表头部）。 介绍随机预读的时候曾说，如果Buffer Pool中有某个区的13个连续页面就会触发随机预读，这其实是不严谨的（但是MySQL文档就是这么说的），其实还要求这13个页面是非常热的页面，所谓的非常热，指的是这些页面在整个young区域的头1/4处。 还有针对LRU链表的优化措施，核心就是尽量高效的提高 Buffer Pool 的缓存命中率。 7.其他的一些链表为了更好的管理Buffer Pool中的缓存页，除了我们上边提到的一些措施，InnoDB还引进了其他的一些链表，比如unzip LRU链表用于管理解压页，zip clean链表用于管理没有被解压的压缩页，zip free数组中每一个元素都代表一个链表，它们组成所谓的伙伴系统来为压缩页提供内存空间等等，为了更好的管理这个Buffer Pool引入了各种链表或其他数据结构。 8.刷新脏页到磁盘后台有专门的线程每隔一段时间负责把脏页刷新到磁盘，这样可以不影响用户线程处理正常的请求。主要有两种刷新路径： 从LRU链表的冷数据中刷新一部分页面到磁盘。后台线程会定时从LRU链表尾部开始扫描一些页面，扫描的页面数量可以通过系统变量innodb_lru_scan_depth来指定，如果从里边儿发现脏页，会把它们刷新到磁盘。这种刷新页面的方式被称之为BUF_FLUSH_LRU。 从flush链表中刷新一部分页面到磁盘。后台线程也会定时从flush链表中刷新一部分页面到磁盘，刷新的速率取决于当时系统是不是很繁忙。这种刷新页面的方式被称之为BUF_FLUSH_LIST。 有时候后台线程刷新脏页的进度比较慢，导致用户线程在准备加载一个磁盘页到Buffer Pool时没有可用的缓存页，这时就会尝试看看LRU链表尾部有没有可以直接释放掉的未修改页面，如果没有的话会不得不将LRU链表尾部的一个脏页同步刷新到磁盘（和磁盘交互是很慢的，这会降低处理用户请求的速度）。这种刷新单个页面到磁盘中的刷新方式被称之为BUF_FLUSH_SINGLE_PAGE。 当然，有时候系统特别繁忙时，也可能出现用户线程批量的从flush链表中刷新脏页的情况，很显然在处理用户请求过程中去刷新脏页是一种严重降低处理速度的行为，这属于一种迫不得已的情况。 9.多个Buffer Pool实例Buffer Pool本质是InnoDB向操作系统申请的一块连续的内存空间，在多线程环境下，访问Buffer Pool中的各种链表都需要加锁处理，在Buffer Pool特别大而且多线程并发访问特别高的情况下，单一的Buffer Pool可能会影响请求的处理速度。所以在Buffer Pool特别大的时候，我们可以把它们拆分成若干个小的Buffer Pool，每个Buffer Pool都称为一个实例，它们都是独立的，独立的去申请内存空间，独立的管理各种链表，所以在多线程并发访问时并不会相互影响，从而提高并发处理能力。我们可以在服务器启动的时候通过设置innodb_buffer_pool_instances的值来修改Buffer Pool实例的个数，比方说这样： 12[server]innodb_buffer_pool_instances = 2 这样就表明我们要创建2个Buffer Pool实例。 每个Buffer Pool实例实际占多少内存空间呢？其实使用这个公式算出来的： 1innodb_buffer_pool_size/innodb_buffer_pool_instances 也就是总共的大小除以实例的个数，结果就是每个Buffer Pool实例占用的大小。 不过也不是说Buffer Pool实例创建的越多越好，分别管理各个Buffer Pool也是需要性能开销的，InnoDB规定：当innodb_buffer_pool_size的值小于1G的时候设置多个实例是无效的，InnoDB会默认把innodb_buffer_pool_instances 的值修改为1。而MySQL希望在Buffer Pool大于或等于1G的时候设置多个Buffer Pool实例。 10.innodb_buffer_pool_chunk_size在MySQL 5.7.5之前，Buffer Pool的大小只能在服务器启动时通过配置innodb_buffer_pool_size启动参数来调整大小，在服务器运行过程中是不允许调整该值的。不过MySQL在5.7.5以及之后的版本中支持了在服务器运行过程中调整Buffer Pool大小的功能，但是有一个问题，就是每次当我们要重新调整Buffer Pool大小时，都需要重新向操作系统申请一块连续的内存空间，然后将旧的Buffer Pool中的内容复制到这一块新空间，这是极其耗时的。所以MySQL决定不再一次性为某个Buffer Pool实例向操作系统申请一大片连续的内存空间，而是以一个所谓的chunk为单位向操作系统申请空间。也就是说一个Buffer Pool实例其实是由若干个chunk组成的，一个chunk就代表一片连续的内存空间，里边儿包含了若干缓存页与其对应的控制块。 正是因为发明了这个chunk的概念，我们在服务器运行期间调整Buffer Pool的大小时就是以chunk为单位增加或者删除内存空间，而不需要重新向操作系统申请一片大的内存，然后进行缓存页的复制。这个所谓的chunk的大小是我们在启动操作MySQL服务器时通过innodb_buffer_pool_chunk_size启动参数指定的，它的默认值是134217728，也就是128M。不过需要注意的是，innodb_buffer_pool_chunk_size的值只能在服务器启动时指定，在服务器运行过程中是不可以修改的。 为什么不允许在服务器运行过程中修改innodb_buffer_pool_chunk_size的值？因为innodb_buffer_pool_chunk_size的值代表InnoDB向操作系统申请的一片连续的内存空间的大小，如果你在服务器运行过程中修改了该值，就意味着要重新向操作系统申请连续的内存空间并且将原先的缓存页和它们对应的控制块复制到这个新的内存空间中，这是十分耗时的操作！ 另外，这个innodb_buffer_pool_chunk_size的值并不包含缓存页对应的控制块的内存空间大小，所以实际上InnoDB向操作系统申请连续内存空间时，每个chunk的大小要比innodb_buffer_pool_chunk_size的值大一些，约5%。 11.配置Buffer Pool时的注意事项 innodb_buffer_pool_size必须是innodb_buffer_pool_chunk_size × innodb_buffer_pool_instances的倍数（这主要是想保证每一个Buffer Pool实例中包含的chunk数量相同）。假设我们指定的innodb_buffer_pool_chunk_size的值是128M，innodb_buffer_pool_instances的值是16，那么这两个值的乘积就是2G，也就是说innodb_buffer_pool_size的值必须是2G或者2G的整数倍。比方说我们在启动MySQL服务器是这样指定启动参数的：默认的innodb_buffer_pool_chunk_size值是128M，指定的innodb_buffer_pool_instances的值是16，所以innodb_buffer_pool_size的值必须是2G或者2G的整数倍，上边例子中指定的innodb_buffer_pool_size的值是8G，符合规定，所以在服务器启动完成之后我们查看一下该变量的值就是我们指定的8G（8589934592字节）：如果我们指定的innodb_buffer_pool_size大于2G并且不是2G的整数倍，那么服务器会自动的把innodb_buffer_pool_size的值调整为2G的整数倍，比方说我们在启动服务器时指定的innodb_buffer_pool_size的值是9G：那么服务器会自动把innodb_buffer_pool_size的值调整为10G，10737418240字节。 1mysqld --innodb-buffer-pool-size=8G --innodb-buffer-pool-instances=16 1234567mysql&gt; show variables like &#x27;innodb_buffer_pool_size&#x27;;+-------------------------+------------+| Variable_name | Value |+-------------------------+------------+| innodb_buffer_pool_size | 8589934592 |+-------------------------+------------+1 row in set (0.00 sec) 1mysqld --innodb-buffer-pool-size=9G --innodb-buffer-pool-instances=16 1234567mysql&gt; show variables like &#x27;innodb_buffer_pool_size&#x27;;+-------------------------+-------------+| Variable_name | Value |+-------------------------+-------------+| innodb_buffer_pool_size | 10737418240 |+-------------------------+-------------+1 row in set (0.01 sec) 如果在服务器启动时，innodb_buffer_pool_chunk_size × innodb_buffer_pool_instances的值已经大于innodb_buffer_pool_size的值，那么innodb_buffer_pool_chunk_size的值会被服务器自动设置为innodb_buffer_pool_size/innodb_buffer_pool_instances的值。比方说我们在启动服务器时指定的innodb_buffer_pool_size的值为2G，innodb_buffer_pool_instances的值为16，innodb_buffer_pool_chunk_size的值为256M：由于256M × 16 = 4G，而4G &gt; 2G，所以innodb_buffer_pool_chunk_size值会被服务器改写为innodb_buffer_pool_size/innodb_buffer_pool_instances的值，也就是：2G/16 = 128M（134217728字节）。 1mysqld --innodb-buffer-pool-size=2G --innodb-buffer-pool-instances=16 --innodb-buffer-pool-chunk-size=256M 123456789101112131415mysql&gt; show variables like &#x27;innodb_buffer_pool_size&#x27;;+-------------------------+------------+| Variable_name | Value |+-------------------------+------------+| innodb_buffer_pool_size | 2147483648 |+-------------------------+------------+1 row in set (0.01 sec)mysql&gt; show variables like &#x27;innodb_buffer_pool_chunk_size&#x27;;+-------------------------------+-----------+| Variable_name | Value |+-------------------------------+-----------+| innodb_buffer_pool_chunk_size | 134217728 |+-------------------------------+-----------+1 row in set (0.00 sec) 12.Buffer Pool中存储的其它信息Buffer Pool的缓存页除了用来缓存磁盘上的页面以外，还可以存储锁信息、自适应哈希索引等信息。 13.查看Buffer Pool的状态信息MySQL给我们提供了SHOW ENGINE INNODB STATUS语句来查看关于InnoDB存储引擎运行过程中的一些状态信息，其中就包括Buffer Pool的一些信息，我们看一下（为了突出重点，我们只把输出中关于Buffer Pool的部分提取了出来）： 123456789101112131415161718192021222324252627mysql&gt; SHOW ENGINE INNODB STATUS\\G(...省略前边的许多状态)----------------------BUFFER POOL AND MEMORY----------------------Total memory allocated 13218349056;Dictionary memory allocated 4014231Buffer pool size 786432Free buffers 8174Database pages 710576Old database pages 262143Modified db pages 124941Pending reads 0Pending writes: LRU 0, flush list 0, single page 0Pages made young 6195930012, not young 78247510485108.18 youngs/s, 226.15 non-youngs/sPages read 2748866728, created 29217873, written 4845680877160.77 reads/s, 3.80 creates/s, 190.16 writes/sBuffer pool hit rate 956 / 1000, young-making rate 30 / 1000 not 605 / 1000Pages read ahead 0.00/s, evicted without access 0.00/s, Random read ahead 0.00/sLRU len: 710576, unzip_LRU len: 118I/O sum[134264]:cur[144], unzip sum[16]:cur[0]--------------(...省略后边的许多状态)mysql&gt; 我们来详细看一下这里边的每个值都代表什么意思： Total memory allocated：代表Buffer Pool向操作系统申请的连续内存空间大小，包括全部控制块、缓存页、以及碎片的大小。 Dictionary memory allocated：为数据字典信息分配的内存空间大小，注意这个内存空间和Buffer Pool没啥关系，不包括在Total memory allocated中。 Buffer pool size：代表该Buffer Pool可以容纳多少缓存页，注意，单位是页！ Free buffers：代表当前Buffer Pool还有多少空闲缓存页，也就是free链表中还有多少个节点。 Database pages：代表LRU链表中的页的数量，包含young和old两个区域的节点数量。 Old database pages：代表LRU链表old区域的节点数量。 Modified db pages：代表脏页数量，也就是flush链表中节点的数量。 Pending reads：正在等待从磁盘上加载到Buffer Pool中的页面数量。当准备从磁盘中加载某个页面时，会先为这个页面在Buffer Pool中分配一个缓存页以及它对应的控制块，然后把这个控制块添加到LRU的old区域的头部，但是这个时候真正的磁盘页并没有被加载进来，Pending reads的值会跟着加1。 Pending writes LRU：即将从LRU链表中刷新到磁盘中的页面数量。 Pending writes flush list：即将从flush链表中刷新到磁盘中的页面数量。 Pending writes single page：即将以单个页面的形式刷新到磁盘中的页面数量。 Pages made young：代表LRU链表中曾经从old区域移动到young区域头部的节点数量。这里需要注意，一个节点每次只有从old区域移动到young区域头部时才会将Pages made young的值加1，也就是说如果该节点本来就在young区域，由于它符合在young区域1/4后边的要求，下一次访问这个页面时也会将它移动到young区域头部，但这个过程并不会导致Pages made young的值加1。 Page made not young：在将innodb_old_blocks_time设置的值大于0时，首次访问或者后续访问某个处在old区域的节点时由于不符合时间间隔的限制而不能将其移动到young区域头部时，Page made not young的值会加1。这里需要注意，对于处在young区域的节点，如果由于它在young区域的1/4处而导致它没有被移动到young区域头部，这样的访问并不会将Page made not young的值加1。 youngs/s：代表每秒从old区域被移动到young区域头部的节点数量。 non-youngs/s：代表每秒由于不满足时间限制而不能从old区域移动到young区域头部的节点数量。 Pages read、created、written：代表读取，创建，写入了多少页。后边跟着读取、创建、写入的速率。 Buffer pool hit rate：表示在过去某段时间，平均访问1000次页面，有多少次该页面已经被缓存到Buffer Pool了。 young-making rate：表示在过去某段时间，平均访问1000次页面，有多少次访问使页面移动到young区域的头部了。 需要注意的一点是，这里统计的将页面移动到**young**区域的头部次数不仅仅包含从**old**区域移动到**young**区域头部的次数，还包括从**young**区域移动到**young**区域头部的次数（访问某个**young**区域的节点，只要该节点在**young**区域的1/4处往后，就会把它移动到**young**区域的头部）。 not (young-making rate)：表示在过去某段时间，平均访问1000次页面，有多少次访问没有使页面移动到young区域的头部。 需要注意的一点是，这里统计的没有将页面移动到**young**区域的头部次数不仅仅包含因为设置了**innodb_old_blocks_time**系统变量而导致访问了**old**区域中的节点但没把它们移动到**young**区域的次数，还包含因为该节点在**young**区域的前1/4处而没有被移动到**young**区域头部的次数。 LRU len：代表LRU链表中节点的数量。 unzip_LRU：代表unzip_LRU链表中节点的数量。 I/O sum：最近50s读取磁盘页的总数。 I/O cur：现在正在读取的磁盘页数量。 I/O unzip sum：最近50s解压的页面数量。 I/O unzip cur：正在解压的页面数量。 三，总结 磁盘太慢，用内存作为缓存很有必要。 Buffer Pool本质上是InnoDB向操作系统申请的一段连续的内存空间，可以通过innodb_buffer_pool_size来调整它的大小。 Buffer Pool向操作系统申请的连续内存由控制块和缓存页组成，每个控制块和缓存页都是一一对应的，在填充足够多的控制块和缓存页的组合后，Buffer Pool剩余的空间可能产生不够填充一组控制块和缓存页，这部分空间不能被使用，也被称为碎片。 InnoDB使用了许多链表来管理Buffer Pool。 free链表中每一个节点都代表一个空闲的缓存页，在将磁盘中的页加载到Buffer Pool时，会从free链表中寻找空闲的缓存页。 为了快速定位某个页是否被加载到Buffer Pool，使用表空间号 + 页号作为key，缓存页作为value，建立哈希表。 在Buffer Pool中被修改的页称为脏页，脏页并不是立即刷新，而是被加入到flush链表中，待之后的某个时刻同步到磁盘上。 LRU链表分为young和old两个区域，可以通过innodb_old_blocks_pct来调节old区域所占的比例。首次从磁盘上加载到Buffer Pool的页会被放到old区域的头部，在innodb_old_blocks_time间隔时间内访问该页不会把它移动到young区域头部。在Buffer Pool没有可用的空闲缓存页时，会首先淘汰掉old区域的一些页。 我们可以通过指定innodb_buffer_pool_instances来控制Buffer Pool实例的个数，每个Buffer Pool实例中都有各自独立的链表，互不干扰。 自MySQL 5.7.5版本之后，可以在服务器运行过程中调整Buffer Pool大小。每个Buffer Pool实例由若干个chunk组成，每个chunk的大小可以在服务器启动时通过启动参数调整。 可以用下边的命令查看Buffer Pool的状态信息： 1SHOW ENGINE INNODB STATUS\\G","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/tags/MySQL/"}]},{"title":"软件设计概述","slug":"设计模式/软件设计概述","date":"2022-01-11T12:02:25.638Z","updated":"2022-01-11T12:28:53.706Z","comments":true,"path":"2022/01/11/设计模式/软件设计概述/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%BD%AF%E4%BB%B6%E8%AE%BE%E8%AE%A1%E6%A6%82%E8%BF%B0/","excerpt":"","text":"一，软件设计概述1.软件设计的概念软件设计模式（或者说是设计模式），是一套被反复使用，多数人知晓的，经过系统整理的代码设计经验的总结。 它描述了在软件设计过程中的一些不断重复发生的问题，以及该问题的解决方案。 它是解决特定问题的一系列套路，是代码设计经验的总结，具有一定的普遍性，可以反复使用。 其目的是为了提高代码的可重用性、代码的可读性和代码的可靠性。 2.学习设计模式的意义设计模式的本质是面向对象设计原则的实际运用，是对类的封装性、继承性和多态性以及类的关联关系和组合关系的充分理解。 可以提高程序员的思维能力、编程能力和设计能力。 使程序设计更加标准化、代码编制更加工程化，使软件开发效率大大提高，从而缩短软件的开发周期。 使设计的代码可重用性高、可读性强、可靠性高、灵活性好、可维护性强。 软件设计模式只是一个引导。在具体的软件幵发中，必须根据设计的应用系统的特点和要求来恰当选择。对于简单的程序开发，苛能写一个简单的算法要比引入某种设计模式更加容易。但对大项目的开发或者框架设计，用设计模式来组织代码显然更好。 3.软件设计模式的基本要素 名称：可以根据模式的问题、特点、解决方案、功能和效果来命名。 问题：描述了该模式的应用环境，即何时使用该模式。它解释了设计问题和问题存在的前因后果，以及必须满足的一系列先决条件。 方案：包括设计的组成成分、它们之间的相互关系及各自的职责和协作方式。因为模式就像一个模板，可应用于多种不同场合，所以解决方案并不描述一个特定而具体的设计或实现，而是提供设计问题的抽象描述和怎样用一个具有一般意义的元素组合（类或对象的 组合）来解决这个问题。 效果：模式的优缺点。主要是对时间和空间的衡量，以及该模式对系统的灵活性、扩充性、可移植性的影响，也考虑其实现问题。 二，设计模式的分类与功能1.根据目的来分根据模式是用来完成什么工作来划分，这种方式可分为创建型模式、结构型模式和行为型模式 3 种。 创建型模式：用于描述“怎样创建对象”，它的主要特点是“将对象的创建与使用分离”。GoF 中提供了单例、原型、工厂方法、抽象工厂、建造者等 5 种创建型模式。 结构型模式：用于描述如何将类或对象按某种布局组成更大的结构，GoF 中提供了代理、适配器、桥接、装饰、外观、享元、组合等 7 种结构型模式。 行为型模式：用于描述类或对象之间怎样相互协作共同完成单个对象都无法单独完成的任务，以及怎样分配职责。GoF 中提供了模板方法、策略、命令、职责链、状态、观察者、中介者、迭代器、访问者、备忘录、解释器等 11 种行为型模式。 2.根据作用范围来分根据模式是主要用于类上还是主要用于对象上来分，这种方式可分为类模式和对象模式两种。 类模式：用于处理类与子类之间的关系，这些关系通过继承来建立，是静态的，在编译时刻便确定下来了。GoF中的工厂方法、（类）适配器、模板方法、解释器属于该模式。 对象模式：用于处理对象之间的关系，这些关系可以通过组合或聚合来实现，在运行时刻是可以变化的，更具动态性。GoF 中除了以上 4 种，其他的都是对象模式。 范围/目的 创建型模式 结构型模式 行为型模式 类模式 工厂方法 (类）适配器 模板方法、解释器 对象模式 单例 原型 抽象工厂 建造者 代理 (对象）适配器 桥接 装饰 外观 享元 组合 策略 命令 职责链 状态 观察者 中介者 迭代器 访问者 备忘录 3.设计模式的功能 单例（Singleton）模式：某个类只能生成一个实例，该类提供了一个全局访问点供外部获取该实例，其拓展是有限多例模式。 原型（Prototype）模式：将一个对象作为原型，通过对其进行复制而克隆出多个和原型类似的新实例。 工厂方法（Factory Method）模式：定义一个用于创建产品的接口，由子类决定生产什么产品。 抽象工厂（AbstractFactory）模式：提供一个创建产品族的接口，其每个子类可以生产一系列相关的产品。 建造者（Builder）模式：将一个复杂对象分解成多个相对简单的部分，然后根据不同需要分别创建它们，最后构建成该复杂对象。 代理（Proxy）模式：为某对象提供一种代理以控制对该对象的访问。即客户端通过代理间接地访问该对象，从而限制、增强或修改该对象的一些特性。 适配器（Adapter）模式：将一个类的接口转换成客户希望的另外一个接口，使得原本由于接口不兼容而不能一起工作的那些类能一起工作。 桥接（Bridge）模式：将抽象与实现分离，使它们可以独立变化。它是用组合关系代替继承关系来实现，从而降低了抽象和实现这两个可变维度的耦合度。 装饰（Decorator）模式：动态的给对象增加一些职责，即增加其额外的功能。 外观（Facade）模式：为多个复杂的子系统提供一个一致的接口，使这些子系统更加容易被访问。 享元（Flyweight）模式：运用共享技术来有效地支持大量细粒度对象的复用。 组合（Composite）模式：将对象组合成树状层次结构，使用户对单个对象和组合对象具有一致的访问性。 模板方法（TemplateMethod）模式：定义一个操作中的算法骨架，而将算法的一些步骤延迟到子类中，使得子类可以不改变该算法结构的情况下重定义该算法的某些特定步骤。 策略（Strategy）模式：定义了一系列算法，并将每个算法封装起来，使它们可以相互替换，且算法的改变不会影响使用算法的客户。 命令（Command）模式：将一个请求封装为一个对象，使发出请求的责任和执行请求的责任分割开。 职责链（Chain of Responsibility）模式：把请求从链中的一个对象传到下一个对象，直到请求被响应为止。通过这种方式去除对象之间的耦合。 状态（State）模式：允许一个对象在其内部状态发生改变时改变其行为能力。 观察者（Observer）模式：多个对象间存在一对多关系，当一个对象发生改变时，把这种改变通知给其他多个对象，从而影响其他对象的行为。 中介者（Mediator）模式：定义一个中介对象来简化原有对象之间的交互关系，降低系统中对象间的耦合度，使原有对象之间不必相互了解。 迭代器（Iterator）模式：提供一种方法来顺序访问聚合对象中的一系列数据，而不暴露聚合对象的内部表示。 访问者（Visitor）模式：在不改变集合元素的前提下，为一个集合中的每个元素提供多种访问方式，即每个元素有多个访问者对象访问。 备忘录（Memento）模式：在不破坏封装性的前提下，获取并保存一个对象的内部状态，以便以后恢复它。 解释器（Interpreter）模式：提供如何定义语言的文法，以及对语言句子的解释方法，即解释器。 三，UML统一建模语言UML（Unified Modeling Language，统一建模语言）是用来设计软件蓝图的可视化建模语言，是一种为面向对象系统的产品进行说明、可视化和编制文档的标准语言，独立于任何一种具体的程序设计语言。 它的特点是简单、统一、图形化、能表达软件设计中的动态与静态信息。 1.应用场景UML 能为软件开发的所有阶段提供模型化和可视化支持。而且融入了软件工程领域的新思想、新方法和新技术，使软件设计人员沟通更简明，进一步缩短了设计时间，减少开发成本。 UML 具有很宽的应用领域。其中最常用的是建立软件系统的模型，但它同样可以用于描述非软件领域的系统，如机械系统、企业机构或业务过程，以及处理复杂数据的信息系统、具有实时要求的工业系统或工业过程等。总之，UML 可以对任何具有静态结构和动态行为的系统进行建模，而且使用于从需求规格描述直至系统完成后的测试和维护等系统开发的各个阶段。 UML 模型大多以图表的方式表现出来，一份典型的建模图表通常包含几个块或框、连接线和作为模型附加信息的文本。这些虽简单却非常重要，在 UML 规则中相互联系和扩展。 2.基本构件UML 建模的核心是模型，模型是现实的简化、真实系统的抽象。UML 提供了系统的设计蓝图。当给软件系统建模时，需要采用通用的符号语言，这种描述模型所使用的语言被称为建模语言。在 UML 中，所有的描述由事物、关系和图这些构件组成。下图完整地描述了所有构件的关系。 下面对具体构件进行说明。 3.事物事物是抽象化的最终结果，分为结构事物、行为事物、分组事物和注释事物。 1）结构事物结构事物是模型中的静态部分，用以呈现概念或实体的表现元素。 事物 解释 图例 类（Class） 具有相同属性、方法、关系和语义的对象集合 接口（Interface） 指一个类或构件的一个服务的操作集合，它仅仅定义了一组操作的规范，并没有给出这组操作的具体实现 用例（User Case） 指对一组动作序列的描述，系统执行这些动作将产生一个对特定的参与者（Actor）有价值且可观察的结果 协作（Collaboration） 定义元素之间的相互作用 组件（Component） 描述物理系统的一部分 活动类（Active Class） 指对象有一个或多个进程或线程。活动类和类很相象，只是它的对象代表的元素的行为和其他元素是同时存在的 节点（Node） 定义为运行时存在的物理元素 2）行为事物行为事物指 UML 模型中的动态部分。 事物 解释 用例 交互（Interaction） 包括一组元素之间的消息交换 状态机（State Machine） 由一系列对象的状态组成 3）分组事物目前只有一种分组事物，即包。包纯碎是概念上的，只存在于开发阶段，结构事物、行为事物甚至分组事物都有可能放在一个包中。 事物 解释 用例 包（Package） UML中唯一的组织机制 4）注释事物注释事物是解释 UML 模型元素的部分。 事物 解释 用例 注释（Note） 用于解析说明 UML 元素 4.图UML2.0 一共有 13 种图（UML1.5 定义了 9 种，UML2.0 增加了 4 种），分别是类图、对象图、构件图、部署图、活动图、状态图、用例图、时序图、协作图 9 种，以及包图、组合结构图、时间图、交互概览图 4 种。 图名称 解释 类图（Class Diagrams） 用于定义系统中的类 对象图（Object Diagrams） 类图的一个实例，描述了系统在具体时间点上所包含的对象及各个对象之间的关系 构件图（Component Diagrams） 一种特殊的 UML 图，描述系统的静态实现视图 部署图（Deployment Diagrams） 定义系统中软硬件的物理体系结构 活动图（Activity Diagrams） 用来描述满足用例要求所要进行的活动及活动间的约束关系 状态图（State Chart Diagrams） 用来描述类的对象的所有可能的状态和时间发生时，状态的转移条件 用例图（Usecase Diagrams） 用来描述用户的需求，从用户的角度描述系统的功能，并指出各功能的执行者，强调谁在使用系统、系统为执行者完成哪些功能 时序图（Sequence Diagrams） 描述对象之间的交互顺序，着重体现对象间消息传递的时间顺序，强调对象之间消息的发送顺序，同时显示对象之间的交互过程 协作图（Collaboration Diagrams） 描述对象之间的合作关系，更侧重向用户对象说明哪些对象有消息的传递 包图（Package Diagrams） 对构成系统的模型元素进行分组整理的图 组合结构图（Composite Structure Diagrams） 表示类或者构建内部结构的图 时间图（Timing Diagrams） 用来显示随时间变化，一个或多个元素的值或状态的更改，也显示时间控制事件之间的交互及管理它们的时间和期限约束 交互概览图（Interaction Overview Diagrams） 用活动图来表示多个交互之间的控制关系的图 四，UML类图及类图之间的关系类图是一种模型类型，确切地说，是一种静态模型类型。类图表示类、接口和它们之间的协作关系，用于系统设计阶段。 1.类，接口，类图1）类类（Class）是指具有相同属性、方法和关系的对象的抽象，它封装了数据和行为，是面向对象程序设计（OOP）的基础，具有封装性、继承性和多态性等三大特性。在 UML 中，类使用包含类名、属性和操作且带有分隔线的矩形来表示。 (1) 类名（Name）是一个字符串，例如，Student。 (2) 属性（Attribute）是指类的特性，即类的成员变量。UML 按以下格式表示： 1[可见性]属性名:类型[=默认值] 例如：-name:String 注意：“可见性”表示该属性对类外的元素是否可见，包括公有（Public）、私有（Private）、受保护（Protected）和朋友（Friendly）4 种，在类图中分别用符号+、-、#、~表示。 (3) 操作（Operations）是类的任意一个实例对象都可以使用的行为，是类的成员方法。UML 按以下格式表示： 1[可见性]名称(参数列表)[:返回类型] 例如：+display():void。 如下所示是学生类的 UML 表示。 类图中，需注意以下几点： 抽象类或抽象方法用斜体表示 如果是接口，则在类名上方加 &lt;&gt; 字段和方法返回值的数据类型非必需 静态类或静态方法加下划线 2）接口接口（Interface）是一种特殊的类，它具有类的结构但不可被实例化，只可以被子类实现。它包含抽象操作，但不包含属性。它描述了类或组件对外可见的动作。在 UML 中，接口使用一个带有名称的小圆圈来进行表示。 如下所示是图形类接口的 UML 表示。 3)类图类图（ClassDiagram）是用来显示系统中的类、接口、协作以及它们之间的静态结构和关系的一种静态模型。它主要用于描述软件系统的结构化设计，帮助人们简化对软件系统的理解，它是系统分析与设计阶段的重要产物，也是系统编码与测试的重要模型依据。 类图中的类可以通过某种编程语言直接实现。类图在软件系统开发的整个生命周期都是有效的，它是面向对象系统的建模中最常见的图。如下所示是“计算长方形和圆形的周长与面积”的类图，图形接口有计算面积和周长的抽象方法，长方形和圆形实现这两个方法供访问类调用。 计算长方形与圆形的周长与面积 2.类之间的关系UML 将事物之间的联系归纳为 6 种，并用对应的图形类表示。下面根据类与类之间的耦合度从弱到强排列。UML 中的类图有以下几种关系：依赖关系、关联关系、聚合关系、组合关系、泛化关系和实现关系。其中泛化和实现的耦合度相等，它们是最强的。 1）依赖关系依赖（Dependency）关系是一种使用关系，它是对象之间耦合度最弱的一种关联方式，是临时性的关联。在代码中，某个类的方法通过局部变量、方法的参数或者对静态方法的调用来访问另一个类（被依赖类）中的某些方法来完成一些职责。 在 UML 类图中，依赖关系使用带箭头的虚线来表示，箭头从使用类指向被依赖的类。如下是人与手机的关系图，人通过手机的语音传送方法打电话。 依赖关系的实例 2）关联关系关联（Association）关系是对象之间的一种引用关系，用于表示一类对象与另一类对象之间的联系，如老师和学生、师傅和徒弟、丈夫和妻子等。关联关系是类与类之间最常用的一种关系，分为一般关联关系、聚合关系和组合关系。我们先介绍一般关联。 关联可以是双向的，也可以是单向的。在 UML 类图中，双向的关联可以用带两个箭头或者没有箭头的实线来表示，单向的关联用带一个箭头的实线来表示，箭头从使用类指向被关联的类。也可以在关联线的两端标注角色名，代表两种不同的角色。 在代码中通常将一个类的对象作为另一个类的成员变量来实现关联关系。如下是老师和学生的关系图，每个老师可以教多个学生，每个学生也可向多个老师学，他们是双向关联。 3）聚合关系聚合（Aggregation）关系是关联关系的一种，是强关联关系，是整体和部分之间的关系，是 has-a 的关系。 聚合关系也是通过成员对象来实现的，其中成员对象是整体对象的一部分，但是成员对象可以脱离整体对象而独立存在。例如，学校与老师的关系，学校包含老师，但如果学校停办了，老师依然存在。 在 UML 类图中，聚合关系可以用带空心菱形的实线来表示，菱形指向整体。如下是大学和教师的关系图。 4）组合关系组合（Composition）关系也是关联关系的一种，也表示类之间的整体与部分的关系，但它是一种更强烈的聚合关系，是 cxmtains-a 关系。 在组合关系中，整体对象可以控制部分对象的生命周期，一旦整体对象不存在，部分对象也将不存在，部分对象不能脱离整体对象而存在。例如，头和嘴的关系，没有了头，嘴也就不存在了。 在 UML 类图中，组合关系用带实心菱形的实线来表示，菱形指向整体。如下是头和嘴的关系图。 5）泛化关系泛化（Generalization）关系是对象之间耦合度最大的一种关系，表示一般与特殊的关系，是父类与子类之间的关系，是一种继承关系，是 is-a 的关系。 在 UML 类图中，泛化关系用带空心三角箭头的实线来表示，箭头从子类指向父类。在代码实现时，使用面向对象的继承机制来实现泛化关系。例如，Student 类和 Teacher 类都是 Person 类的子类，其类图如下所示。 6）实现关系实现（Realization）关系是接口与实现类之间的关系。在这种关系中，类实现了接口，类中的操作实现了接口中所声明的所有的抽象操作。 在 UML 类图中，实现关系使用带空心三角箭头的虚线来表示，箭头从实现类指向接口。例如，汽车和船实现了交通工具，其类图如下所示。 3.类关系记忆技巧 分类 箭头特征 记忆技巧 箭头方向 从子类指向父类 定义子类需要通过 extends 关键字指定父类 子类一定是知道父类定义的，但父类并不知道子类的定义 只有知道对方信息时才能指向对方 箭头的方向是从子类指向父类 继承/实现 用线条连接两个类； 空心三角箭头表示继承或实现 实现表示继承，是is-a的关系，表示扩展，不虚，很结实 虚线表示实现，虚线代表“虚”无实体 关联/依赖 用线条连接两个类； 普通箭头表示关联或依赖 虚线表示依赖关系：临时用一下，若即若离，虚无缥缈，若有若无 表示一种使用关系，一个类需要借助另一个类来实现功能 一般一个类将另一个类作为参数使用，或作为返回值 实线表示关联关系：关系稳定，实打实的关系 表示一个类对象和另一个类对象有关联 通常一个类中有另一个类对象作为属性 组合/聚合 用菱形表示：像一个盛东西的器皿（如盘子） 聚合：空心菱形，代表空器皿里可以放很多相同的东西，聚集在一起（箭头方向所指的类） 整体和局部的关系，两者有独立的生命周期，是 has-a 的关系 弱关系，消极的词：弱-空 组合：实心菱形，代表器皿里已经有实体结构的存在，生死与共 整体与局部的关系，和聚合关系对比，关系更加强烈，两者具有相同的生命周期，contains-a 的关系 强关系，积极的词；强-满 | UML 的标准类关系图中，没有实心箭头。有些 Java 编程的 IDE 自带类生成工具可能出现实心箭头，主要目的是降低理解难度。 下图是对动物衍生关系描述的类图。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"软件架构","slug":"软件架构","permalink":"https://yinhuidong.github.io/tags/%E8%BD%AF%E4%BB%B6%E6%9E%B6%E6%9E%84/"}]},{"title":"软件架构的基本原则","slug":"设计模式/软件架构的基本原则","date":"2022-01-11T12:02:16.665Z","updated":"2022-01-11T12:28:39.120Z","comments":true,"path":"2022/01/11/设计模式/软件架构的基本原则/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%BD%AF%E4%BB%B6%E6%9E%B6%E6%9E%84%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%8E%9F%E5%88%99/","excerpt":"","text":"一，软件架构的两个要点1.代码复用无论是开发哪种软件产品，成本和时间都是最重要的。较少的开发时间意味着可以比竞争对手更早进入市场。较低的开发成本意味着能够留出更多的营销资金，覆盖更广泛的潜在客户。 代码复用是减少开发成本最常用的方式之一，其目的非常明显，即：与其反复从头开发，不如在新对象中重用已有的代码。 使用设计模式是增加软件组件灵活性并使其易于复用的方式之一。但是，这可能也会让组件变得更加复杂。 一般情况下，复用可以分为三个层次。在最底层，可以复用类、类库、容器，也许还有一些类的“团体（例如容器和迭代器）”。 框架位于最高层。它们能帮助你精简自己的设计，可以明确解决问题所需的抽象概念，然后用类来表示这些概念并定义其关系。例如，JUnit 是一个小型框架，也是框架的“Hello, world”，其中定义了 Test、TestCase 和 TestSuite 这几个类及其关系。框架通常比单个类的颗粒度要大。你可以通过在某处构建子类来与框架建立联系。这些子类信奉“别给我们打电话，我们会给你打电话的。” 还有一个中间层次。这是我觉得设计模式所处的位置。设计模式比框架更小且更抽象。它们实际上是对一组类的关系及其互动方式的描述。当你从类转向模式，并最终到达框架的过程中，复用程度会不断增加。 中间层次的优点在于模式提供的复用方式要比框架的风险小。创建框架是一项投入重大且风险很高的工作，模式则能让你独立于具体代码来复用设计思想和理念。 2.扩展性需求变化是程序员生命中唯一不变的事情。比如以下几种场景： 你在 Windows 平台上发布了一款游戏，现在人们想要 Mac OS 的版本。 你创建了一个使用方形按钮的 GUI 框架，但几个月后开始流行原型按钮。 你设计了一款优秀的电子商务网站，但仅仅几个月后，客户就要求新增电话订单的功能。 首先，在完成了第一版的程序后，我们就应该做好了从头开始优化重写代码的准备，因为现在你已经能在很多方面更好的理解问题了，同时在专业水平上也有所提高，所以之前的代码现在看上去可能会显得很糟糕。 其次，可能是在你掌控之外的某些事情发生了变化，这也是导致许多开发团队转变最初想法的原因。比如，每位在网络应用中使用 Flash 的开发者都必须重新开发或移植代码，因为不断地有浏览器停止对 Flash 格式地支持。 最后，可能是需求的改变，之前你的客户对当前版本的程序感到满意，但是现在希望对程序进行 11 个“小小”的改动，使其可完成原始计划阶段中完全没有提到的功能，新增或改变功能。 当然这也有好的一面，如果有人要求你对程序进行修改，至少说明还有人关心它。因此在设计程序架构时，有经验的开发者都会尽量选择支持未来任何可能变更的方式。 二，正确使用设计模式设计模式不是为每个人准备的，而是基于业务来选择设计模式，需要时就能想到它。要明白一点，技术永远为业务服务，技术只是满足业务需要的一个工具。我们需要掌握每种设计模式的应用场景、特征、优缺点，以及每种设计模式的关联关系，这样就能够很好地满足日常业务的需要。 设计模式不是为了特定场景而生的，而是为了让人可以更好和更快地开发。 设计模式只是实现了七大设计原则的具体方式，套用太多设计模式只会陷入模式套路陷阱，最后代码写的凌乱不堪。 不能为了使用设计模式而去做架构，而是有了做架构的需求后，发现它符合某一类设计模式的结构，在将两者结合。 想要游刃有余地使用设计模式，需要打下牢固的程序设计语言基础、夯实自己的编程思想、积累大量的时间经验、提高开发能力。目的都是让程序低耦合，高复用，高内聚，易扩展，易维护。 1.需求驱动不仅仅是功能性需求，需求驱动还包括性能和运行时的需求，如软件的可维护性和可复用性等方面。设计模式是针对软件设计的，而软件设计是针对需求的，一定不要为了使用设计模式而使用设计模式，否则可能会使设计变得复杂，使软件难以调试和维护。 2.分析已经存在的项目对现有的应用实例进行分析是一个很好的学习途径，应当注意学习已有的项目，而不仅是学习设计模式如何实现，更重要的是注意在什么场合使用设计模式。 3.掌握当前开发平台设计模式大部分都是针对面向对象的软件设计，因此在理论上适合任何面向对象的语言，但随着技术的发展和编程环境的改善，设计模式的实现方式会有很大的差别。在一些平台下，某些设计模式是自然实现的。 4.开发中领悟软件开发是一项实践工作，最直接的方法就是编程。没有从来不下棋却熟悉定式的围棋高手，也没有不会编程就能成为架构设计师的先例。掌握设计模式是水到渠成的事情，除了理论只是和实践积累，可能会“渐悟”或者“顿悟”。 5.避免过度设计设计模式解决的是设计不足的问题，但同时也要避免设计过度。一定要牢记简洁原则，要知道设计模式是为了使设计简单，而不是更复杂。如果引入设计模式使得设计变得复杂，只能说我们把简单问题复杂化了，问题本身不需要设计模式。 这里需要把握的是需求变化的程度，一定要区分需求的稳定部分和可变部分。一个软件必然有稳定部分，这个部分就是核心业务逻辑。如果核心业务逻辑发生变化，软件就没有存在的必要，核心业务逻辑是我们需要固化的。对于可变的部分，需要判断可能发生变化的程度来确定设计策略和设计风险。要知道，设计过度与设计不足同样对项目有害。 设计模式从来都不是单个设计模式独立使用的。在实际应用中，通常多个设计模式混合使用，你中有我，我中有你。 三，开闭原则1.定义软件实体应当对扩展开放，对修改关闭。 何为软件实体？ 项目中划分出的模块 类与接口 方法 开闭原则的含义是：当应用的需求改变时，在不修改软件实体的源代码或者二进制代码的前提下，可以扩展模块的功能，使其满足新的需求。 2.作用开闭原则是面向对象程序设计的终极目标，它使软件实体拥有一定的适应性和灵活性的同时具备稳定性和延续性。 软件测试：软件遵守开闭原则的话，软件测试时只需要对扩展的代码进行测试就可以了，因为原有的测试代码仍然能够正常运行。 提高代码复用性：粒度越小，被复用的可能性就越大；在面向对象的程序设计中，根据原子和抽象编程可以提高代码的可复用性。 提高软件可维护性：遵守开闭原则的软件，其稳定性高和延续性强，从而易于扩展和维护。 3.实现方法可以通过“抽象约束、封装变化”来实现开闭原则，即通过接口或者抽象类为软件实体定义一个相对稳定的抽象层，而将相同的可变因素封装在相同的具体实现类中。 因为抽象灵活性好，适应性广，只要抽象的合理，可以基本保持软件架构的稳定。而软件中易变的细节可以从抽象派生来的实现类来进行扩展，当软件需要发生变化时，只需要根据需求重新派生一个实现类来扩展就可以了。 Windows 的桌面主题设计 Windows 的主题是桌面背景图片、窗口颜色和声音等元素的组合。用户可以根据自己的喜爱更换自己的桌面主题，也可以从网上下载新的主题。这些主题有共同的特点，可以为其定义一个抽象类（Abstract Subject），而每个具体的主题（Specific Subject）是其子类。用户窗体可以根据需要选择或者增加新的主题，而不需要修改原代码，所以它是满足开闭原则的。 四，里氏替换原则1.定义继承必须确保超类所拥有的性质在子类中仍然成立 里氏替换原则主要阐述了有关继承的一些原则，也就是什么时候应该使用继承，什么时候不应该使用继承，以及其中蕴含的原理。里氏替换原是继承复用的基础，它反映了基类与子类之间的关系，是对开闭原则的补充，是对实现抽象化的具体步骤的规范。 2.作用 里氏替换原则是实现开闭原则的重要方式之一 它克服了继承中重写父类造成的可复用性变差的缺点 它是动作正确性的保证。即类的扩展不会给已有的系统引入新的错误，降低了代码出错的可能性 加强程序的健壮性，同时变更时可以做到非常好的兼容性，提高程序的维护性、可扩展性，降低需求变更时引入的风险 3.实现方法里氏替换原则通俗来讲就是：子类可以扩展父类的功能，但不能改变父类原有的功能。也就是说：子类继承父类时，除添加新的方法完成新增功能外，尽量不要重写父类的方法。 子类可以实现父类的抽象方法，但不能覆盖父类的非抽象方法 子类中可以增加自己特有的方法 当子类的方法重载父类的方法时，方法的前置条件（即方法的输入参数）要比父类的方法更宽松 当子类的方法实现父类的方法时（重写/重载或实现抽象方法），方法的后置条件（即方法的的输出/返回值）要比父类的方法更严格或相等 通过重写父类的方法来完成新的功能写起来虽然简单，但是整个继承体系的可复用性会比较差，特别是运用多态比较频繁时，程序运行出错的概率会非常大。 如果程序违背了里氏替换原则，则继承类的对象在基类出现的地方会出现运行错误。这时其修正方法是：取消原来的继承关系，重新设计它们之间的关系。 里氏替换原则在“几维鸟不是鸟”实例中的应用。 分析：鸟一般都会飞行，如燕子的飞行速度大概是每小时 120 千米。但是新西兰的几维鸟由于翅膀退化无法飞行。假如要设计一个实例，计算这两种鸟飞行 300 千米要花费的时间。显然，拿燕子来测试这段代码，结果正确，能计算出所需要的时间；但拿几维鸟来测试，结果会发生“除零异常”或是“无穷大”，明显不符合预期。 正确的做法是：取消几维鸟原来的继承关系，定义鸟和几维鸟的更一般的父类，如动物类，它们都有奔跑的能力。几维鸟的飞行速度虽然为 0，但奔跑速度不为 0，可以计算出其奔跑 300 千米所要花费的时间。 五，依赖倒置原则1.定义高层模块不应该依赖低层模块，两者都应该依赖其抽象；抽象不应该依赖细节，细节应该依赖抽象 核心思想是：要面向接口编程，不要面向实现编程。 依赖倒置原则是实现开闭原则的重要途径之一，它降低了客户与实现模块之间的耦合。 由于在软件设计中，细节具有多变性，而抽象层则相对稳定，因此以抽象为基础搭建起来的架构要比以细节为基础搭建起来的架构要稳定得多。这里的抽象指的是接口或者抽象类，而细节是指具体的实现类。 使用接口或者抽象类的目的是制定好规范和契约，而不去涉及任何具体的操作，把展现细节的任务交给它们的实现类去完成。 2.作用 依赖倒置原则可以降低类间的耦合性。 依赖倒置原则可以提高系统的稳定性。 依赖倒置原则可以减少并行开发引起的风险。 依赖倒置原则可以提高代码的可读性和可维护性。 3.实现方法依赖倒置原则的目的是通过要面向接口的编程来降低类间的耦合性，所以我们在实际编程中只要遵循以下4点，就能在项目中满足这个规则。 每个类尽量提供接口或抽象类，或者两者都具备。 变量的声明类型尽量是接口或者是抽象类。 任何类都不应该从具体类派生 使用继承时尽量遵循里氏替换原则 依赖倒置原则在“顾客购物程序”中的应用。 六，单一职责原则1.定义一个类应该有且仅有一个引起它变化的原因，否则类应该被拆分。 该原则提出对象不应该承担太多职责，如果一个对象承担了太多的职责，至少存在以下两个缺点： 一个职责的变化可能会削弱或者抑制这个类实现其他职责的能力； 当客户端需要该对象的某一个职责时，不得不将其他不需要的职责全都包含进来，从而造成冗余代码或代码的浪费。 2.优点单一职责原则的核心就是控制类的粒度大小、将对象解耦、提高其内聚性。如果遵循单一职责原则将有以下优点。 降低类的复杂度。一个类只负责一项职责，其逻辑肯定要比负责多项职责简单得多。 提高类的可读性。复杂性降低，自然其可读性会提高。 提高系统的可维护性。可读性提高，那自然更容易维护了。 变更引起的风险降低。变更是必然的，如果单一职责原则遵守得好，当修改一个功能时，可以显著降低对其他功能的影响。 3.实现方法单一职责原则是最简单但又最难运用的原则，需要设计人员发现类的不同职责并将其分离，再封装到不同的类或模块中。而发现类的多重职责需要设计人员具有较强的分析设计能力和相关重构经验。 大学学生工作管理程序 分析：大学学生工作主要包括学生生活辅导和学生学业指导两个方面的工作，其中生活辅导主要包括班委建设、出勤统计、心理辅导、费用催缴、班级管理等工作，学业指导主要包括专业引导、学习辅导、科研指导、学习总结等工作。如果将这些工作交给一位老师负责显然不合理，正确的做 法是生活辅导由辅导员负责，学业指导由学业导师负责。 单一职责同样也适用于方法。一个方法应该尽可能做好一件事情。如果一个方法处理的事情太多，其颗粒度会变得很粗，不利于重用。 七，接口隔离原则1.定义单一职责同样也适用于方法。一个方法应该尽可能做好一件事情。如果一个方法处理的事情太多，其颗粒度会变得很粗，不利于重用。 要为各个类建立它们需要的专用接口，而不要试图去建立一个很庞大的接口供所有依赖它的类去调用。 接口隔离原则和单一职责都是为了提高类的内聚性、降低它们之间的耦合性，体现了封装的思想，但两者是不同的： 单一职责原则注重的是职责，而接口隔离原则注重的是对接口依赖的隔离。 单一职责原则主要是约束类，它针对的是程序中的实现和细节；接口隔离原则主要约束接口，主要针对抽象和程序整体框架的构建。 2.优点接口隔离原则是为了约束接口、降低类对接口的依赖性，遵循接口隔离原则有以下 5 个优点。 将臃肿庞大的接口分解为多个粒度小的接口，可以预防外来变更的扩散，提高系统的灵活性和可维护性。 接口隔离提高了系统的内聚性，减少了对外交互，降低了系统的耦合性。 如果接口的粒度大小定义合理，能够保证系统的稳定性；但是，如果定义过小，则会造成接口数量过多，使设计复杂化；如果定义太大，灵活性降低，无法提供定制服务，给整体项目带来无法预料的风险。 使用多个专门的接口还能够体现对象的层次，因为可以通过接口的继承，实现对总接口的定义。 能减少项目工程中的代码冗余。过大的大接口里面通常放置许多不用的方法，当实现这个接口的时候，被迫设计冗余的代码。 3.实现方法在具体应用接口隔离原则时，应该根据以下几个规则来衡量。 接口尽量小，但是要有限度。一个接口只服务于一个子模块或业务逻辑。 为依赖接口的类定制服务。只提供调用者需要的方法，屏蔽不需要的方法。 了解环境，拒绝盲从。每个项目或产品都有选定的环境因素，环境不同，接口拆分的标准就不同深入了解业务逻辑。 提高内聚，减少对外交互。使接口用最少的方法去完成最多的事情。 学生成绩管理程序 分析：学生成绩管理程序一般包含插入成绩、删除成绩、修改成绩、计算总分、计算均分、打印成绩信息、査询成绩信息等功能，如果将这些功能全部放到一个接口中显然不太合理，正确的做法是将它们分别放在输入模块、统计模块和打印模块等 3 个模块中。 八，迪米特法则1.定义如果两个软件实体无须直接通信，那么就不应当发生直接的相互调用，可以通过第三方转发该调用。其目的是降低类之间的耦合度，提高模块的相对独立性。 迪米特法则中的“朋友”是指：当前对象本身、当前对象的成员对象、当前对象所创建的对象、当前对象的方法参数等，这些对象同当前对象存在关联、聚合或组合关系，可以直接访问这些对象的方法。 2.优点迪米特法则要求限制软件实体之间通信的宽度和深度，正确使用迪米特法则将有以下两个优点。 降低了类之间的耦合度，提高了模块的相对独立性。 由于亲合度降低，从而提高了类的可复用率和系统的扩展性。 但是，过度使用迪米特法则会使系统产生大量的中介类，从而增加系统的复杂性，使模块之间的通信效率降低。所以，在釆用迪米特法则时需要反复权衡，确保高内聚和低耦合的同时，保证系统的结构清晰。 3.实现方法从迪米特法则的定义和特点可知，它强调以下两点： 从依赖者的角度来说，只依赖应该依赖的对象。 从被依赖者的角度说，只暴露应该暴露的方法。 所以，在运用迪米特法则时要注意以下 6 点。 在类的划分上，应该创建弱耦合的类。类与类之间的耦合越弱，就越有利于实现可复用的目标。 在类的结构设计上，尽量降低类成员的访问权限。 在类的设计上，优先考虑将一个类设置成不变类。 在对其他类的引用上，将引用其他对象的次数降到最低。 不暴露类的属性成员，而应该提供相应的访问器（set 和 get 方法）。 谨慎使用序列化（Serializable）功能。 明星与经纪人的关系实例 分析：明星由于全身心投入艺术，所以许多日常事务由经纪人负责处理，如与粉丝的见面会，与媒体公司的业务洽淡等。这里的经纪人是明星的朋友，而粉丝和媒体公司是陌生人，所以适合使用迪米特法则。 九，合成复用原则1.定义它要求在软件复用时，要尽量先使用组合或者聚合等关联关系来实现，其次才考虑使用继承关系来实现。 如果要使用继承关系，则必须严格遵循里氏替换原则。合成复用原则同里氏替换原则相辅相成的，两者都是开闭原则的具体实现规范。 2.优点通常类的复用分为继承复用和合成复用两种，继承复用虽然有简单和易实现的优点，但它也存在以下缺点。 继承复用破坏了类的封装性。因为继承会将父类的实现细节暴露给子类，父类对子类是透明的，所以这种复用又称为“白箱”复用。 子类与父类的耦合度高。父类的实现的任何改变都会导致子类的实现发生变化，这不利于类的扩展与维护。 它限制了复用的灵活性。从父类继承而来的实现是静态的，在编译时已经定义，所以在运行时不可能发生变化。 采用组合或聚合复用时，可以将已有对象纳入新对象中，使之成为新对象的一部分，新对象可以调用已有对象的功能，它有以下优点。 它维持了类的封装性。因为成分对象的内部细节是新对象看不见的，所以这种复用又称为“黑箱”复用。 新旧类之间的耦合度低。这种复用所需的依赖较少，新对象存取成分对象的唯一方法是通过成分对象的接口。 复用的灵活性高。这种复用可以在运行时动态进行，新对象可以动态地引用与成分对象类型相同的对象。 3.实现方法合成复用原则是通过将已有的对象纳入新对象中，作为新对象的成员对象来实现的，新对象可以调用已有对象的功能，从而达到复用。 汽车分类管理程序 分析：汽车按“动力源”划分可分为汽油汽车、电动汽车等；按“颜色”划分可分为白色汽车、黑色汽车和红色汽车等。如果同时考虑这两种分类，其组合就很多。图 1 所示是用继承关系实现的汽车分类的类图。 十，总结7 种设计原则，它们分别为开闭原则、里氏替换原则、依赖倒置原则、单一职责原则、接口隔离原则、迪米特法则和合成复用原则。 这 7 种设计原则是软件设计模式必须尽量遵循的原则，是设计模式的基础。在实际开发过程中，并不是一定要求所有代码都遵循设计原则，而是要综合考虑人力、时间、成本、质量，不刻意追求完美，要在适当的场景遵循设计原则。这体现的是一种平衡取舍，可以帮助我们设计出更加优雅的代码结构。 设计原则 一句话归纳 目的 开闭原则 对扩展开放，对修改关闭 降低维护带来的新风险 依赖倒置原则 高层不应该依赖低层，要面向接口编程 更利于代码结构的升级扩展 单一职责原则 一个类只干一件事，实现类要单一 便于理解，提高代码的可读性 接口隔离原则 一个接口只干一件事，接口要精简单一 功能解耦，高聚合、低耦合 迪米特法则 不该知道的不要知道，一个类应该保持对其它对象最少的了解，降低耦合度 只和朋友交流，不和陌生人说话，减少代码臃肿 里氏替换原则 不要破坏继承体系，子类重写方法功能发生改变，不应该影响父类方法的含义 防止继承泛滥 合成复用原则 尽量使用组合或者聚合关系实现代码复用，少使用继承 降低代码耦合 实际上，这些原则的目的只有一个：降低对象之间的耦合，增加程序的可复用性、可扩展性和可维护性。 在程序设计时，我们应该将程序功能最小化，每个类只干一件事。若有类似功能基础之上添加新功能，则要合理使用继承。对于多方法的调用，要会运用接口，同时合理设置接口功能与数量。最后类与类之间做到低耦合高内聚。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"软件架构","slug":"软件架构","permalink":"https://yinhuidong.github.io/tags/%E8%BD%AF%E4%BB%B6%E6%9E%B6%E6%9E%84/"}]},{"title":"设计模式概览","slug":"设计模式/三种类型设计模式的特点","date":"2022-01-11T12:02:08.137Z","updated":"2022-01-11T12:13:24.895Z","comments":true,"path":"2022/01/11/设计模式/三种类型设计模式的特点/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E4%B8%89%E7%A7%8D%E7%B1%BB%E5%9E%8B%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E7%9A%84%E7%89%B9%E7%82%B9/","excerpt":"","text":"一，创建型模式的特点和分类创建型模式的主要关注点是如何创建对象？，它的主要特点是将对象的创建与使用分离。这样可以降低系统的耦合度，使用者不需要关注对象的创建细节，对象的创建由相关的工厂来完成。就像去商场购物，不需要知道商品是怎么生产出来的，因为他们由专门的厂商生产。 创建型模式分为以下几种 单例模式：某各类只能生成一个实例，该类提供了一个全局访问点供外部获取该实例，其拓展是有限多例模式。 原型模式：将一个对象作为原型，通过对其进行复制而克隆出多个和原型类似的新实例。 工厂方法：定义一个用于创建产品的接口，由子类决定生产什么产品。 抽象工厂：提供一个创建产品族的接口，其每个子类可以生产一系列相关的产品。 建造者模式：将一个复杂对象分解成多个相对简单部分，然后根据不同需要分别创建他们，最后构建成该复杂对象。 以上5种创建型模式，除了工厂方法模式属于类创建型模式，其他全部属于对象创建型模式，我们将在之后的教程中详细的介绍他们的特点木结构与应用。 二，结构型模式概述结构型模式描述如何将类或者对象按某种布局组成更大的结构。它分为类结构型模式和对象结构型模式，前者采用继承机制来组织接口和类，后者采用组合或聚合来组合对象。 由于组合关系或聚合关系比继承关系耦合度低，满足合成复用原则，所以对象结构型模式比类结构型模式具有更大的灵活性。 结构型模式分为以下7种 代理模式：为某对象提供一种代理以控制该对象的访问，即客户端通过代理间接的访问该对象，从而限制，增强或者修改该对象的一些特性。 适配器模式：将一个类的接口转换成客户希望的另一个接口，使得原本由于接口不兼容而不能一起工作的那些类能一起工作。 桥接模式：将抽象与实现分离，使他们可以独立变化。他是用组合关系来代替继承关系来实现的，从而降低了抽象和实现这两个可变维度的耦合性。 装饰着模式：动态的给对象增加一些职责，即增加额外的功能。 外观模式：将多个复杂的子系统提供一个一致的接口，使这些子系统更加容易被访问。 享元模式：运用共享技术来有效的支持大量细粒度对象的复用 组合模式：将对象组合成树状层次结构，使用户对单个对象和组合对象具有一致访问性 以上7种结构型模式，除了适配器模式分为类结构模式和对象结构型模式两种，其他的全部属于对象结构型模式。 三，行为型模式概述行为型模式用于描述程序在运行时复杂的流程控制，即描述多个类或对象之间怎样相互协作共同完成单个对象都无法单独完成的任务，它涉及算法与对象间职责的分配。 行为型模式分为类行为模式和对象行为模式，前者采用继承机制来在类间分派行为，后者采用组合或聚合在对象间分配行为。由于组合关系或聚合关系比继承关系耦合度低，满足“合成复用原则”，所以对象行为模式比类行为模式具有更大的灵活性。 行为型模式包含11种设计模式 模板方法（Template Method）模式：定义一个操作中的算法骨架，将算法的一些步骤延迟到子类中，使得子类在可以不改变该算法结构的情况下重定义该算法的某些特定步骤。 策略（Strategy）模式：定义了一系列算法，并将每个算法封装起来，使它们可以相互替换，且算法的改变不会影响使用算法的客户。 命令（Command）模式：将一个请求封装为一个对象，使发出请求的责任和执行请求的责任分割开。 职责链（Chain of Responsibility）模式：把请求从链中的一个对象传到下一个对象，直到请求被响应为止。通过这种方式去除对象之间的耦合。 状态（State）模式：允许一个对象在其内部状态发生改变时改变其行为能力。 观察者（Observer）模式：多个对象间存在一对多关系，当一个对象发生改变时，把这种改变通知给其他多个对象，从而影响其他对象的行为。 中介者（Mediator）模式：定义一个中介对象来简化原有对象之间的交互关系，降低系统中对象间的耦合度，使原有对象之间不必相互了解。 迭代器（Iterator）模式：提供一种方法来顺序访问聚合对象中的一系列数据，而不暴露聚合对象的内部表示。 访问者（Visitor）模式：在不改变集合元素的前提下，为一个集合中的每个元素提供多种访问方式，即每个元素有多个访问者对象访问。 备忘录（Memento）模式：在不破坏封装性的前提下，获取并保存一个对象的内部状态，以便以后恢复它。 解释器（Interpreter）模式：提供如何定义语言的文法，以及对语言句子的解释方法，即解释器。 以上11种行为型模式，除了模板方法模式和解释器模式是类行为模式，其他的全部属于对象行为型模式。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"单例模式","slug":"设计模式/创建型模式之单例模式","date":"2022-01-11T12:02:00.546Z","updated":"2022-01-11T12:06:59.633Z","comments":true,"path":"2022/01/11/设计模式/创建型模式之单例模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E5%88%9B%E5%BB%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"在一些系统中，为了节省内存资源，保证数据一致性，对某些类要求只能创建一个实例，这就是所谓的单例模式。 一，单例模式的定义与特点单例模式的定义：指一个类只有一个实例，且该类能自行创建这个实例的一种模式 单例模式的特点： 单例类只有一个实例对象 该单例对象必须由单例类自行创建 单例类对外提供一个访问该单例的全局访问点 二，单例模式的优点和缺点优点 单例模式可以保证内存中只有一个实例，减少了内存的开销 可以避免对资源的多重占用 单例模式设置全局访问点，可以优化和共享资源的访问 缺点 单例模式一般没有接口，扩展困难，除非修改源代码，违反开闭原则。 在并发测试中，单例模式不利于代码调试，在调试过程中，如果单例中的代码没有执行完，也不能模拟生成一个新的对象。 单例模式的功能代码通常写在一个类，如果功能设计不合理，则很容易违背单一职责原则。 三，单例模式的应用场景对于Java来说，单例模式可以保证在一个JVM中只存在单一实例。单例模式的应用场景主要有以下几个方面。 需要频繁创建的一些类，使用单例可以降低系统的内存压力，减少 GC。 某类只要求生成一个对象的时候，如一个班中的班长、每个人的身份证号等。 某些类创建实例时占用资源较多，或实例化耗时较长，且经常使用。 某类需要频繁实例化，而创建的对象又频繁被销毁的时候，如多线程的线程池、网络连接池等。 频繁访问数据库或文件的对象。 对于一些控制硬件级别的操作，或者从系统上来讲应当是单一控制逻辑的操作，如果有多个实例，则系统会完全乱套。 当对象需要被共享的场合。由于单例模式只允许创建一个对象，共享该对象可以节省内存，并加快对象访问速度。如 Web 中的配置对象、数据库的连接池等。 四，单例模式的结构与实现单例模式是设计模式中最简单的模式之一。通常，普通类的构造函数是公有的，外部类可以通过“new 构造函数()”来生成多个实例。但是，如果将类的构造函数设为私有的，外部类就无法调用该构造函数，也就无法生成多个实例。这时该类自身必须定义一个静态私有实例，并向外提供一个静态的公有函数用于创建或获取该静态私有实例。 1.单例模式的结构单例模式的主要结构包括： 单例类：包含一个实例且能自行创建这个实例的类 访问类：使用单例的类 2.单例模式的实现1）懒汉式该模式的特点是类加载时没有生成单例，只有当第一次调用 getlnstance 方法时才去创建这个单例。 12345678910111213141516171819public class LazySinglenton &#123; //内存可见性 private static volatile LazySinglenton instance ; private LazySinglenton()&#123; &#125; public static LazySinglenton getInstance()&#123; if (instance == null)&#123; synchronized (LazySinglenton.class)&#123; if (instance == null)&#123; instance = new LazySinglenton(); &#125; &#125; &#125; return instance; &#125;&#125; 注意：如果编写的是多线程程序，则不要删除上例代码中的关键字 volatile 和 synchronized，否则将存在线程非安全的问题。如果不删除这两个关键字就能保证线程安全，但是每次访问时都要同步，会影响性能，且消耗更多的资源，这是懒汉式单例的缺点。 2)饿汉式该模式的特点是类一旦加载就创建一个单例，保证在调用 getInstance 方法之前单例已经存在了。 123456789101112public class HungrySingleton &#123; private static final HungrySingleton instance = new HungrySingleton(); private HungrySingleton()&#123; &#125; public static HungrySingleton getInstance()&#123; return instance; &#125;&#125; 饿汉式单例在类创建的同时就已经创建好一个静态的对象供系统使用，以后不再改变，所以是线程安全的，可以直接用于多线程而不会出现问题。 五，单例模式的应用实例1.用懒汉式单例模式模拟产生美国当今总统对象分析：在每一届任期内，美国的总统只有一人，所以本实例适合用单例模式实现，图 2 所示是用懒汉式单例实现的结构图。 1234567891011121314151617181920212223242526272829303132333435363738public class SingletonLazy &#123; public static void main(String[] args) &#123; President zt1 = President.getInstance(); zt1.getName(); //输出总统的名字 President zt2 = President.getInstance(); zt2.getName(); //输出总统的名字 if (zt1 == zt2) &#123; System.out.println(&quot;他们是同一人！&quot;); &#125; else &#123; System.out.println(&quot;他们不是同一人！&quot;); &#125; &#125;&#125;class President &#123; private static volatile President instance = null; //保证instance在所有线程中同步 //private避免类在外部被实例化 private President() &#123; System.out.println(&quot;产生一个总统！&quot;); &#125; public static synchronized President getInstance() &#123; //在getInstance方法上加同步 if (instance == null) &#123; synchronized (President.class) &#123; if (instance == null) &#123; instance = new President(); &#125; &#125; &#125; return instance; &#125; public void getName() &#123; System.out.println(&quot;我是美国总统：特朗普。&quot;); &#125;&#125; 程序运行结果如下： 1234产生一个总统！我是美国总统：特朗普。我是美国总统：特朗普。他们是同一人！ 2.用饿汉式单例模式模拟产生猪八戒对象分析：同上例类似，猪八戒也只有一个，所以本实例同样适合用单例模式实现。这里的猪八戒类是单例类，可以将其定义成面板 JPanel 的子类，里面包含了标签，用于保存猪八戒的图像，客户窗体可以获得猪八戒对象，并显示它。图 3 所示是用饿汉式单例实现的结构图。 1234567891011121314151617181920212223242526272829303132public class SingletonEager &#123; public static void main(String[] args) &#123; JFrame jf = new JFrame(&quot;饿汉单例模式测试&quot;); jf.setLayout(new GridLayout(1, 2)); Container contentPane = jf.getContentPane(); Bajie obj1 = Bajie.getInstance(); contentPane.add(obj1); Bajie obj2 = Bajie.getInstance(); contentPane.add(obj2); if (obj1 == obj2) &#123; System.out.println(&quot;他们是同一人！&quot;); &#125; else &#123; System.out.println(&quot;他们不是同一人！&quot;); &#125; jf.pack(); jf.setVisible(true); jf.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE); &#125;&#125;class Bajie extends JPanel &#123; private static Bajie instance = new Bajie(); private Bajie() &#123; JLabel l1 = new JLabel(new ImageIcon(&quot;/home/yhd/图片/4.jpg&quot;)); this.add(l1); &#125; public static Bajie getInstance() &#123; return instance; &#125;&#125; 六，单例模式的扩展单例模式可扩展为有限的多例（Multitcm）模式，这种模式可生成有限个实例并保存在 ArrayList 中，客户需要时可随机获取，其结构图如图 5 所示。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"原型模式","slug":"设计模式/创建型模式之原型模式","date":"2022-01-11T12:01:51.844Z","updated":"2022-01-11T12:07:20.406Z","comments":true,"path":"2022/01/11/设计模式/创建型模式之原型模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E5%88%9B%E5%BB%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%8E%9F%E5%9E%8B%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"在有些系统中，存在大量相同或相似对象的创建问题，如果用传统的构造函数来创建对象，会比较复杂且耗时耗资源，用原型模式生成对象就很高效，就像孙悟空拔下猴毛轻轻一吹就变出很多孙悟空一样简单。 一，原型模式的定义与特点用一个已经创建的实例作为原型，通过复制该原型对象来创建一个和原型相同或相似的新对象。在这里，原型实例指定了要创建的对象的种类。用这种方式创建对象非常高效，根本无须知道对象创建的细节。 原型模式的优点 Java 自带的原型模式基于内存二进制流的复制，在性能上比直接 new 一个对象更加优良。 可以使用深克隆方式保存对象的状态，使用原型模式将对象复制一份，并将其状态保存起来，简化了创建对象的过程，以便在需要的时候使用（例如恢复到历史某一状态），可辅助实现撤销操作。 原型模式的缺点 需要为每一个类都配置一个 clone 方法 clone 方法位于类的内部，当对已有类进行改造的时候，需要修改代码，违背了开闭原则。 当实现深克隆时，需要编写较为复杂的代码，而且当对象之间存在多重嵌套引用时，为了实现深克隆，每一层对象对应的类都必须支持深克隆，实现起来会比较麻烦。因此，深克隆、浅克隆需要运用得当。 二，原型模式的结构与实现由于 Java 提供了对象的 clone() 方法，所以用 Java 实现原型模式很简单。 1.模式的结构原型模式包含以下主要角色。 抽象原型类：规定了具体原型对象必须实现的接口。 具体原型类：实现抽象原型类的 clone() 方法，它是可被复制的对象。 访问类：使用具体原型类中的 clone() 方法来复制新的对象。 2.模式的实现原型模式的克隆分为浅克隆和深克隆。 浅克隆：创建一个新对象，新对象的属性和原来对象完全相同，对于非基本类型属性，仍指向原有属性所指向的对象的内存地址。 深克隆：创建一个新对象，属性中引用的其他对象也会被克隆，不再指向原有对象地址。 Java 中的 Object 类提供了浅克隆的 clone() 方法，具体原型类只要实现 Cloneable 接口就可实现对象的浅克隆，这里的 Cloneable 接口就是抽象原型类。其代码如下： 12345678910111213141516171819public class Realizetype implements Cloneable&#123; public Realizetype()&#123; System.out.println(&quot;具体原型创建成功！&quot;); &#125; @Override protected Object clone() throws CloneNotSupportedException &#123; System.out.println(&quot;具体原型克隆成功！&quot;); return super.clone(); &#125;&#125;class RealizetypeTest&#123; public static void main(String[]args)throws Exception&#123; Realizetype obj1 = new Realizetype(); Realizetype obj2 = (Realizetype) obj1.clone(); System.out.println(&quot;obj1==obj2? &quot;+ (obj1==obj2)); &#125;&#125; 程序运行结果 123具体原型创建成功！具体原型克隆成功！obj1==obj2? false 三，原型模式的应用实例1.用原型模式生成“三好学生”奖状分析：同一学校的“三好学生”奖状除了获奖人姓名不同，其他都相同，属于相似对象的复制，同样可以用原型模式创建，然后再做简单修改就可以了。 12345678910111213141516171819202122232425262728293031323334353637383940public class Citation implements Cloneable&#123; protected String name; protected String info; protected String college; public Citation(String name,String info,String college)&#123; this.name = name; this.info = info; this.college = college; &#125; @Override protected Object clone() throws CloneNotSupportedException &#123; return super.clone(); &#125; public void setName(String name) &#123; this.name = name; &#125; public String getName() &#123; return name; &#125; public void display()&#123; System.out.println(&quot;name:&quot;+name+&quot; info:&quot;+info+&quot; college:&quot;+college); &#125;&#125;class ProtoTypeCitation&#123; public static void main(String[] args) throws Exception&#123; Citation obj1 = new Citation(&quot;张三&quot;,&quot;男&quot;,&quot;pc&quot;); obj1.display(); Citation obj2 = (Citation) obj1.clone(); obj2.setName(&quot;李四&quot;); obj2.display(); &#125;&#125; 12name:张三 info:男 college:pcname:李四 info:男 college:pc 四，原型模式的应用场景 对象之间相同或相似，即只是个别的几个属性不同的时候。 创建对象成本较大，例如初始化时间长，占用CPU太多，或者占用网络资源太多等，需要优化资源。 创建一个对象需要繁琐的数据准备或访问权限等，需要提高性能或者提高安全性。 系统中大量使用该类对象，且各个调用者都需要给它的属性重新赋值。 在 Spring 中，原型模式应用的非常广泛，例如 scope=&#39;prototype&#39;、JSON.parseObject() 等都是原型模式的具体应用。 五，原型模式的扩展原型模式可扩展为带原型管理器的原型模式，它在原型模式的基础上增加了一个原型管理器 PrototypeManager 类。该类用 HashMap 保存多个复制的原型，Client 类可以通过管理器的 get(String id) 方法从中获取复制的原型。 用带原型管理器的原型模式来生成包含“圆”和“正方形”等图形的原型，并计算其面积。分析：本实例中由于存在不同的图形类，例如，“圆”和“正方形”，它们计算面积的方法不一样，所以需要用一个原型管理器来管理它们。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475import java.util.*;interface Shape extends Cloneable &#123; public Object clone(); //拷贝 public void countArea(); //计算面积&#125;class Circle implements Shape &#123; public Object clone() &#123; Circle w = null; try &#123; w = (Circle) super.clone(); &#125; catch (CloneNotSupportedException e) &#123; System.out.println(&quot;拷贝圆失败!&quot;); &#125; return w; &#125; public void countArea() &#123; int r = 0; System.out.print(&quot;这是一个圆，请输入圆的半径：&quot;); Scanner input = new Scanner(System.in); r = input.nextInt(); System.out.println(&quot;该圆的面积=&quot; + 3.1415 * r * r + &quot;\\n&quot;); &#125;&#125;class Square implements Shape &#123; public Object clone() &#123; Square b = null; try &#123; b = (Square) super.clone(); &#125; catch (CloneNotSupportedException e) &#123; System.out.println(&quot;拷贝正方形失败!&quot;); &#125; return b; &#125; public void countArea() &#123; int a = 0; System.out.print(&quot;这是一个正方形，请输入它的边长：&quot;); Scanner input = new Scanner(System.in); a = input.nextInt(); System.out.println(&quot;该正方形的面积=&quot; + a * a + &quot;\\n&quot;); &#125;&#125;class ProtoTypeManager &#123; private HashMap&lt;String, Shape&gt; ht = new HashMap&lt;String, Shape&gt;(); public ProtoTypeManager() &#123; ht.put(&quot;Circle&quot;, new Circle()); ht.put(&quot;Square&quot;, new Square()); &#125; public void addshape(String key, Shape obj) &#123; ht.put(key, obj); &#125; public Shape getShape(String key) &#123; Shape temp = ht.get(key); return (Shape) temp.clone(); &#125;&#125;public class ProtoTypeShape &#123; public static void main(String[] args) &#123; ProtoTypeManager pm = new ProtoTypeManager(); Shape obj1 = (Circle) pm.getShape(&quot;Circle&quot;); obj1.countArea(); Shape obj2 = (Shape) pm.getShape(&quot;Square&quot;); obj2.countArea(); &#125;&#125; 运行结果 12345这是一个圆，请输入圆的半径：3该圆的面积=28.2735这是一个正方形，请输入它的边长：3该正方形的面积=9","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"简单工厂模式","slug":"设计模式/创建型模式之简单工厂模式","date":"2022-01-11T12:01:41.405Z","updated":"2022-01-11T12:07:09.399Z","comments":true,"path":"2022/01/11/设计模式/创建型模式之简单工厂模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E5%88%9B%E5%BB%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E7%AE%80%E5%8D%95%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"在日常开发中，凡是需要生成复杂对象的地方，都可以尝试考虑使用工厂模式来代替。 复杂对象指的是类的构造函数参数过多等对类的构造有影响的情况，因为类的构造过于复杂，如果直接在其他业务类内使用，则两者的耦合过重，后续业务更改，就需要在任何引用该类的源代码内进行更改，光是查找所有依赖就很消耗时间了，更别说要一个一个修改了。 一，工厂模式的定义定义一个创建产品对象的工厂接口，将产品对象的实际创建工作推迟到具体子工厂类当中。这满足创建型模式中所要求的“创建与使用相分离”的特点。 按实际业务场景划分，工厂模式有 3 种不同的实现方式，分别是简单工厂模式、工厂方法模式和抽象工厂模式。 我们把被创建的对象称为“产品”，把创建产品的对象称为“工厂”。如果要创建的产品不多，只要一个工厂类就可以完成，这种模式叫“简单工厂模式”。 在简单工厂模式中创建实例的方法通常为静态（static）方法，因此简单工厂模式（Simple Factory Pattern）又叫作静态工厂方法模式（Static Factory Method Pattern）。 简单来说，简单工厂模式有一个具体的工厂类，可以生成多个不同的产品，属于创建型设计模式。简单工厂模式不在 GoF 23 种设计模式之列。 简单工厂模式每增加一个产品就要增加一个具体产品类和一个对应的具体工厂类，这增加了系统的复杂度，违背了“开闭原则”。 “工厂方法模式”是对简单工厂模式的进一步抽象化，其好处是可以使系统在不修改原来代码的情况下引进新的产品，即满足开闭原则。 二，优点和缺点优点 工厂类包含必要的逻辑判断，可以决定在什么时候创建哪一个产品的实例。客户端可以免除直接创建产品对象的职责，很方便的创建出相应的产品。工厂和产品的职责区分明确。 客户端无需知道所创建具体产品的类名，只需知道参数即可。 也可以引入配置文件，在不修改客户端代码的情况下更换和添加新的具体产品类。 缺点 简单工厂模式的工厂类单一，负责所有产品的创建，职责过重，一旦异常，整个系统将受影响。且工厂类代码会非常臃肿，违背高聚合原则。 使用简单工厂模式会增加系统中类的个数（引入新的工厂类），增加系统的复杂度和理解难度 系统扩展困难，一旦增加新产品不得不修改工厂逻辑，在产品类型较多时，可能造成逻辑过于复杂 简单工厂模式使用了 static 工厂方法，造成工厂角色无法形成基于继承的等级结构。 应用场景 对于产品种类相对较少的情况，考虑使用简单工厂模式。使用简单工厂模式的客户端只需要传入工厂类的参数，不需要关心如何创建对象的逻辑，可以很方便地创建所需产品。 三，模式的结构与实现简单工厂模式的主要角色如下： 简单工厂（SimpleFactory）：是简单工厂模式的核心，负责实现创建所有实例的内部逻辑。工厂类的创建产品类的方法可以被外界直接调用，创建所需的产品对象。 抽象产品（Product）：是简单工厂创建的所有对象的父类，负责描述所有实例共有的公共接口。 具体产品（ConcreteProduct）：是简单工厂模式的创建目标。 12345678910111213141516171819202122232425262728293031323334353637383940414243public class Client &#123; public static void main(String[] args) &#123; &#125; //抽象产品 public interface Product &#123; void show(); &#125; //具体产品：ProductA static class ConcreteProduct1 implements Product &#123; @Override public void show() &#123; System.out.println(&quot;具体产品1显示...&quot;); &#125; &#125; //具体产品：ProductB static class ConcreteProduct2 implements Product &#123; @Override public void show() &#123; System.out.println(&quot;具体产品2显示...&quot;); &#125; &#125; final class Const &#123; static final int PRODUCT_A = 0; static final int PRODUCT_B = 1; static final int PRODUCT_C = 2; &#125; static class SimpleFactory &#123; public static Product makeProduct(int kind) &#123; switch (kind) &#123; case Const.PRODUCT_A: return new ConcreteProduct1(); case Const.PRODUCT_B: return new ConcreteProduct2(); &#125; return null; &#125; &#125;&#125;","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"工厂方法模式","slug":"设计模式/创建型模式之工厂方法模式","date":"2022-01-11T12:01:34.895Z","updated":"2022-01-11T12:07:04.629Z","comments":true,"path":"2022/01/11/设计模式/创建型模式之工厂方法模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E5%88%9B%E5%BB%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%B7%A5%E5%8E%82%E6%96%B9%E6%B3%95%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"一，定义与优缺点简单工厂模式违背了开闭原则，而“工厂方法模式”是对简单工厂模式的进一步抽象化，其好处是可以使系统在不修改原来代码的情况下引进新的产品，即满足开闭原则。 优点 用户只需要知道具体工厂的名称就可得到所要的产品，无须知道产品的具体创建过程。 灵活性增强，对于新产品的创建，只需多写一个相应的工厂类。 典型的解耦框架。高层模块只需要知道产品的抽象类，无须关心其他实现类，满足迪米特法则、依赖倒置原则和里氏替换原则。 缺点 类的个数容易过多，增加复杂度 增加了系统的抽象性和理解难度 抽象产品只能生产一种产品，此弊端可使用抽象工厂模式解决。 应用场景 客户只知道创建产品的工厂名，而不知道具体的产品名。如 TCL 电视工厂、海信电视工厂等。 创建对象的任务由多个具体子工厂中的某一个完成，而抽象工厂只提供创建产品的接口。 客户不关心创建产品的细节，只关心产品的品牌 二，模式的结构与实现工厂方法模式由抽象工厂、具体工厂、抽象产品和具体产品等4个要素构成。本节来分析其基本结构和实现方法。 1.模式的结构工厂方法模式的主要角色如下。 抽象工厂（Abstract Factory）：提供了创建产品的接口，调用者通过它访问具体工厂的工厂方法 newProduct() 来创建产品。 具体工厂（ConcreteFactory）：主要是实现抽象工厂中的抽象方法，完成具体产品的创建。 抽象产品（Product）：定义了产品的规范，描述了产品的主要特性和功能。 具体产品（ConcreteProduct）：实现了抽象产品角色所定义的接口，由具体工厂来创建，它同具体工厂之间一一对应。 2.模式的实现不同的工厂生产不同的妹子，想要获取某种妹子，就要通过对应的工厂获取。 123456789101112131415161718192021222324252627282930313233343536373839404142434445public abstract class AbstractFactory &#123; private AbstractMeiZi meiZi; public abstract AbstractMeiZi getMeiZi();&#125;public class PiaoLiangFactory extends AbstractFactory &#123; @Override public AbstractMeiZi getMeiZi()&#123; return new PiaoLiangMeiZi(); &#125;&#125;public class XingGanFactory extends AbstractFactory &#123; @Override public AbstractMeiZi getMeiZi() &#123; return new XingGanMeiZi(); &#125;&#125;public abstract class AbstractMeiZi &#123; private Integer id; private String name;&#125;public class PiaoLiangMeiZi extends AbstractMeiZi &#123;&#125;public class XingGanMeiZi extends AbstractMeiZi &#123;&#125;public class MainTest &#123; @Test public void test()&#123; AbstractFactory factory = new PiaoLiangFactory(); AbstractMeiZi meiZi = factory.getMeiZi(); System.out.println(meiZi); &#125;&#125; 当需要生成的产品不多且不会增加，一个具体工厂类就可以完成任务时，可删除抽象工厂类。这时工厂方法模式将退化到简单工厂模式。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"抽象工厂模式","slug":"设计模式/创建型模式之抽象工厂模式","date":"2022-01-11T12:01:26.778Z","updated":"2022-01-11T12:06:49.176Z","comments":true,"path":"2022/01/11/设计模式/创建型模式之抽象工厂模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E5%88%9B%E5%BB%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E6%8A%BD%E8%B1%A1%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"工厂方法模式中考虑的是一类产品的生产，如畜牧场只养动物、电视机厂只生产电视机、计算机软件学院只培养计算机软件专业的学生等。 同种类称为同等级，也就是说：[工厂方法模式]只考虑生产同等级的产品，但是在现实生活中许多工厂是综合型的工厂，能生产多等级（种类） 的产品，如农场里既养动物又种植物，电器厂既生产电视机又生产洗衣机或空调，大学既有软件专业又有生物专业等。 一，模式的定义与特点抽象工厂（AbstractFactory）模式的定义：是一种为访问类提供一个创建一组相关或相互依赖对象的接口，且访问类无须指定所要产品的具体类就能得到同族的不同等级的产品的模式结构。 抽象工厂模式是工厂方法模式的升级版本，工厂方法模式只生产一个等级的产品，而抽象工厂模式可生产多个等级的产品。 使用抽象工厂模式一般要满足以下条件。 系统中有多个产品族，每个具体工厂创建同一族但属于不同等级结构的产品。 系统一次只可能消费其中某一族产品，即同族的产品一起使用。 抽象工厂模式除了具有工厂方法模式的优点外，其他主要优点如下。 可以在类的内部对产品族中相关联的多等级产品共同管理，而不必专门引入多个新的类来进行管理。 当需要产品族时，抽象工厂可以保证客户端始终只使用同一个产品的产品组。 抽象工厂增强了程序的可扩展性，当增加一个新的产品族时，不需要修改原代码，满足开闭原则。 其缺点是：当产品族中需要增加一个新的产品时，所有的工厂类都需要进行修改。增加了系统的抽象性和理解难度。 二，模式的结构与实现抽象工厂模式同工厂方法模式一样，也是由抽象工厂、具体工厂、抽象产品和具体产品等 4 个要素构成，但抽象工厂中方法个数不同，抽象产品的个数也不同。现在我们来分析其基本结构和实现方法。 1. 模式的结构抽象工厂模式的主要角色如下。 抽象工厂（Abstract Factory）：提供了创建产品的接口，它包含多个创建产品的方法 newProduct()，可以创建多个不同等级的产品。 具体工厂（Concrete Factory）：主要是实现抽象工厂中的多个抽象方法，完成具体产品的创建。 抽象产品（Product）：定义了产品的规范，描述了产品的主要特性和功能，抽象工厂模式有多个抽象产品。 具体产品（ConcreteProduct）：实现了抽象产品角色所定义的接口，由具体工厂来创建，它同具体工厂之间是多对一的关系。 2.模式的实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374public abstract class AbstractFactory &#123; public abstract AbstractMeiZi getMeiZi(); public abstract AbstractCar getCar();&#125;//==================妹子工厂====================//public abstract class AbstractMeiZiFactory &#123; public abstract AbstractMeiZi getMeiZi();&#125;public class PiaoLiangFactory extends AbstractMeiZiFactory &#123; @Override public AbstractMeiZi getMeiZi()&#123; return new PiaoLiangMeiZi(); &#125;&#125;public class XingGanFactory extends AbstractMeiZiFactory &#123; @Override public AbstractMeiZi getMeiZi() &#123; return new XingGanMeiZi(); &#125;&#125;public abstract class AbstractMeiZi &#123; private Integer id; private String name;&#125;public class XingGanMeiZi extends AbstractMeiZi&#123;&#125;public class PiaoLiangMeiZi extends AbstractMeiZi&#123;&#125;//==================汽车工厂====================//public abstract class AbstractCarFactory &#123; public abstract AbstractCar getCar();&#125;public class BsjFactory extends AbstractCarFactory &#123; @Override public AbstractCar getCar() &#123; return new Bsj(); &#125;&#125;public class LbjnFactory extends AbstractCarFactory&#123; @Override public AbstractCar getCar() &#123; return new Lbjn(); &#125;&#125;public abstract class AbstractCar &#123; private Integer id; private String name;&#125;public class Bsj extends AbstractCar&#123;&#125;public class Lbjn extends AbstractCar&#123;&#125; 三，模式的应用场景抽象工厂模式最早的应用是用于创建属于不同操作系统的视窗构件。如 [Java]的 AWT 中的 Button 和 Text 等构件在 Windows 和 UNIX 中的本地实现是不同的。 抽象工厂模式通常适用于以下场景： 当需要创建的对象是一系列相互关联或相互依赖的产品族时，如电器工厂中的电视机、洗衣机、空调等。 系统中有多个产品族，但每次只使用其中的某一族产品。如有人只喜欢穿某一个品牌的衣服和鞋。 系统中提供了产品的类库，且所有产品的接口相同，客户端不依赖产品实例的创建细节和内部结构。 四，模式的扩展抽象工厂模式的扩展有一定的“开闭原则”倾斜性： 当增加一个新的产品族时只需增加一个新的具体工厂，不需要修改原代码，满足开闭原则。 当产品族中需要增加一个新种类的产品时，则所有的工厂类都需要进行修改，不满足开闭原则。 另一方面，当系统中只存在一个等级结构的产品时，抽象工厂模式将退化到工厂方法模式。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"构建者模式","slug":"设计模式/创建型模式之建造者模式","date":"2022-01-11T12:01:18.420Z","updated":"2022-01-11T12:07:14.560Z","comments":true,"path":"2022/01/11/设计模式/创建型模式之建造者模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E5%88%9B%E5%BB%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%BB%BA%E9%80%A0%E8%80%85%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"在软件开发过程中有时需要创建一个复杂的对象，这个复杂对象通常由多个子部件按一定的步骤组合而成。例如，计算机是由 CPU、主板、内存、硬盘、显卡、机箱、显示器、键盘、鼠标等部件组装而成的，采购员不可能自己去组装计算机，而是将计算机的配置要求告诉计算机销售公司，计算机销售公司安排技术人员去组装计算机，然后再交给要买计算机的采购员。 以上所有这些产品都是由多个部件构成的，各个部件可以灵活选择，但其创建步骤都大同小异。这类产品的创建无法用工厂模式描述，只有建造者模式可以很好地描述该类产品的创建。 一，模式的定义与特点建造者（Builder）模式的定义：指将一个复杂对象的构造与它的表示分离，使同样的构建过程可以创建不同的表示，这样的设计模式被称为建造者模式。它是将一个复杂的对象分解为多个简单的对象，然后一步一步构建而成。它将变与不变相分离，即产品的组成部分是不变的，但每一部分是可以灵活选择的。 该模式的主要优点如下： 封装性好，构建和表示分离。 扩展性好，各个具体的建造者相互独立，有利于系统的解耦。 客户端不必知道产品内部组成的细节，建造者可以对创建过程逐步细化，而不对其它模块产生任何影响，便于控制细节风险。 其缺点如下： 产品的组成部分必须相同，这限制了其使用范围。 如果产品的内部变化复杂，如果产品内部发生变化，则建造者也要同步修改，后期维护成本较大。 建造者（Builder）模式和工厂模式的关注点不同：建造者模式注重零部件的组装过程，而[工厂方法模式]更注重零部件的创建过程，但两者可以结合使用。 二，模式的结构与实现建造者（Builder）模式由产品、抽象建造者、具体建造者、指挥者等 4 个要素构成，现在我们来分析其基本结构和实现方法。 1.模式的结构建造者（Builder）模式的主要角色如下。 产品角色（Product）：它是包含多个组成部件的复杂对象，由具体建造者来创建其各个零部件。 抽象建造者（Builder）：它是一个包含创建产品各个子部件的抽象方法的接口，通常还包含一个返回复杂产品的方法 getResult()。 具体建造者(Concrete Builder）：实现 Builder 接口，完成复杂产品的各个部件的具体创建方法。 指挥者（Director）：它调用建造者对象中的部件构造与装配方法完成复杂对象的创建，在指挥者中不涉及具体产品的信息。 2.模式的实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public abstract class PhoneBuilder &#123; protected Phone phone; public abstract void doCpu(); public abstract void doMemory(); public abstract void doSsd(); public Phone getPhone()&#123; doCpu(); doMemory(); doSsd(); return new Phone(); &#125;&#125;public class XiaoMiPhoneBuilder extends PhoneBuilder&#123; @Override public void doCpu() &#123; System.out.println(&quot;小米工厂生产CPU&quot;); &#125; @Override public void doMemory() &#123; System.out.println(&quot;小米工厂生产Memory&quot;); &#125; @Override public void doSsd() &#123; System.out.println(&quot;小米工厂生产SSD&quot;); &#125;&#125;public class Phone &#123; private Integer id; private String name; private String logo;&#125;public class MainTest &#123; @Test public void test() &#123; PhoneBuilder builder = new XiaoMiPhoneBuilder(); Phone phone = builder.getPhone(); &#125;&#125; 三，模式的应用场景建造者模式唯一区别于工厂模式的是针对复杂对象的创建。也就是说，如果创建简单对象，通常都是使用工厂模式进行创建，而如果创建复杂对象，就可以考虑使用建造者模式。 当需要创建的产品具备复杂创建过程时，可以抽取出共性创建过程，然后交由具体实现类自定义创建流程，使得同样的创建行为可以生产出不同的产品，分离了创建与表示，使创建产品的灵活性大大增加。 建造者模式主要适用于以下应用场景： 相同的方法，不同的执行顺序，产生不同的结果。 多个部件或零件，都可以装配到一个对象中，但是产生的结果又不相同。 产品类非常复杂，或者产品类中不同的调用顺序产生不同的作用。 初始化一个对象特别复杂，参数多，而且很多参数都具有默认值。 四，建造者和工厂模式的区别 建造者模式更加注重方法的调用顺序，工厂模式注重创建对象。 创建对象的力度不同，建造者模式创建复杂的对象，由各种复杂的部件组成，工厂模式创建出来的对象都一样 关注重点不一样，工厂模式只需要把对象创建出来就可以了，而建造者模式不仅要创建出对象，还要知道对象由哪些部件组成。 建造者模式根据建造过程中的顺序不一样，最终对象部件组成也不一样。 五，模式的扩展建造者（Builder）模式在应用过程中可以根据需要改变，如果创建的产品种类只有一种，只需要一个具体建造者，这时可以省略掉抽象建造者，甚至可以省略掉指挥者角色。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"代理模式","slug":"设计模式/结构型模式之代理模式","date":"2022-01-11T12:01:09.773Z","updated":"2022-01-11T12:07:27.650Z","comments":true,"path":"2022/01/11/设计模式/结构型模式之代理模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E7%BB%93%E6%9E%84%E5%9E%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"在有些情况下，一个客户不能或者不想直接访问另一个对象，这时需要找一个中介帮忙完成某项任务，这个中介就是代理对象。例如，购买火车票不一定要去火车站买，可以通过 12306 网站或者去火车票代售点买。又如找女朋友、找保姆、找工作等都可以通过找中介完成。 在软件设计中，使用代理模式的例子也很多，例如，要访问的远程对象比较大（如视频或大图像等），其下载要花很多时间。还有因为安全原因需要屏蔽客户端直接访问真实对象，如某单位的内部数据库等。 一，代理模式的定义与特点代理模式的定义：由于某些原因需要给某对象提供一个代理以控制对该对象的访问。这时，访问对象不适合或者不能直接引用目标对象，代理对象作为访问对象和目标对象之间的中介。 代理模式的主要优点有： 代理模式在客户端与目标对象之间起到一个中介作用和保护目标对象的作用； 代理对象可以扩展目标对象的功能； 代理模式能将客户端与目标对象分离，在一定程度上降低了系统的耦合度，增加了程序的可扩展性 其主要缺点是： 代理模式会造成系统设计中类的数量增加 在客户端和目标对象之间增加一个代理对象，会造成请求处理速度变慢； 增加了系统的复杂度； 如何解决以上提到的缺点呢？答案是可以使用动态代理方式 二，代理模式的结构与实现代理模式的结构比较简单，主要是通过定义一个继承抽象主题的代理来包含真实主题，从而实现对真实主题的访问。 1.模式的结构代理模式的主要角色如下。 抽象主题（Subject）类：通过接口或抽象类声明真实主题和代理对象实现的业务方法。 真实主题（Real Subject）类：实现了抽象主题中的具体业务，是代理对象所代表的真实对象，是最终要引用的对象。 代理（Proxy）类：提供了与真实主题相同的接口，其内部含有对真实主题的引用，它可以访问、控制或扩展真实主题的功能。 在代码中，一般代理会被理解为代码增强，实际上就是在原代码逻辑前后增加一些代码逻辑，而使调用者无感知。 根据代理的创建时期，代理模式分为静态代理和动态代理。 静态：由程序员创建代理类或特定工具自动生成源代码再对其编译，在程序运行前代理类的 .class 文件就已经存在了。 动态：在程序运行时，运用反射机制动态创建而成 2.模式的实现123456789101112131415161718192021222324252627282930313233343536373839404142package proxy;public class ProxyTest &#123; public static void main(String[] args) &#123; Proxy proxy = new Proxy(); proxy.Request(); &#125;&#125;//抽象主题interface Subject &#123; void Request();&#125;//真实主题class RealSubject implements Subject &#123; public void Request() &#123; System.out.println(&quot;访问真实主题方法...&quot;); &#125;&#125;//代理class Proxy implements Subject &#123; private RealSubject realSubject; public void Request() &#123; if (realSubject == null) &#123; realSubject = new RealSubject(); &#125; preRequest(); realSubject.Request(); postRequest(); &#125; public void preRequest() &#123; System.out.println(&quot;访问真实主题之前的预处理。&quot;); &#125; public void postRequest() &#123; System.out.println(&quot;访问真实主题之后的后续处理。&quot;); &#125;&#125; 程序运行的结果如下： 123访问真实主题之前的预处理。访问真实主题方法...访问真实主题之后的后续处理。 三，代理模式的应用实例1.jdk 123456789101112131415161718192021222324252627public interface Game &#123; void playGame(String name);&#125;public class UziProxy&lt;T&gt; &#123; public static&lt;T&gt; T getProxy(T t)&#123; return (T) Proxy.newProxyInstance(t.getClass().getClassLoader(), t.getClass().getInterfaces(), new InvocationHandler() &#123; @Override public Object invoke(Object o, Method method, Object[] objects) throws Throwable &#123; System.out.println(&quot;增强&quot;); return method.invoke(t,objects); &#125; &#125;); &#125;&#125;public class MainTest &#123; @Test public void test()&#123; Game game = UziProxy.getProxy(name -&gt; System.out.println(&quot;选vn，一打五。&quot;+name)); game.playGame(&quot;LOL&quot;); &#125;&#125; 2.cglib123456789101112131415161718192021222324252627282930313233public class ProxyFactory&lt;T&gt; &#123; public static&lt;T&gt; T getProxy(T t)&#123; return (T) Enhancer.create(t.getClass(), (MethodInterceptor) (o, method, objects, methodProxy) -&gt; &#123; System.out.println(&quot;增强&quot;); return method.invoke(t,objects); &#125;); &#125;&#125;public class Phone &#123; void call()&#123; System.out.println(&quot;打电话&quot;); &#125;&#125;public class MiPhone extends Phone&#123; @Override void call() &#123; System.out.println(&quot;打微信电话&quot;); &#125;&#125;public class MainTest &#123; public static void main(String[] args) &#123; Phone proxy = ProxyFactory.getProxy(new MiPhone()); proxy.call(); &#125;&#125; 四，代理模式的应用场景当无法或不想直接引用某个对象或访问某个对象存在困难时，可以通过代理对象来间接访问。使用代理模式主要有两个目的：一是保护目标对象，二是增强目标对象。 前面分析了代理模式的结构与特点，现在来分析以下的应用场景。 远程代理，这种方式通常是为了隐藏目标对象存在于不同地址空间的事实，方便客户端访问。例如，用户申请某些网盘空间时，会在用户的文件系统中建立一个虚拟的硬盘，用户访问虚拟硬盘时实际访问的是网盘空间。 虚拟代理，这种方式通常用于要创建的目标对象开销很大时。例如，下载一幅很大的图像需要很长时间，因某种计算比较复杂而短时间无法完成，这时可以先用小比例的虚拟代理替换真实的对象，消除用户对服务器慢的感觉。 安全代理，这种方式通常用于控制不同种类客户对真实对象的访问权限。 智能指引，主要用于调用目标对象时，代理附加一些额外的处理功能。例如，增加计算真实对象的引用次数的功能，这样当该对象没有被引用时，就可以自动释放它。 延迟加载，指为了提高系统的性能，延迟对目标的加载。例如，Hibernate 中就存在属性的延迟加载和关联表的延时加载。 五，代理模式的扩展在前面介绍的代理模式中，代理类中包含了对真实主题的引用，这种方式存在两个缺点。 真实主题与代理主题一一对应，增加真实主题也要增加代理。 设计代理以前真实主题必须事先存在，不太灵活。采用动态代理模式可以解决以上问题，如Spring AOP。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"适配器模式","slug":"设计模式/结构型模式之适配器模式","date":"2022-01-11T12:00:58.995Z","updated":"2022-01-11T12:08:00.723Z","comments":true,"path":"2022/01/11/设计模式/结构型模式之适配器模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E7%BB%93%E6%9E%84%E5%9E%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E9%80%82%E9%85%8D%E5%99%A8%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"在软件设计中也可能出现：需要开发的具有某种业务功能的组件在现有的组件库中已经存在，但它们与当前系统的接口规范不兼容，如果重新开发这些组件成本又很高，这时用适配器模式能很好地解决这些问题。 一，模式的定义与特点适配器模式（Adapter）的定义如下：将一个类的接口转换成客户希望的另外一个接口，使得原本由于接口不兼容而不能一起工作的那些类能一起工作。适配器模式分为类结构型模式和对象结构型模式两种，前者类之间的耦合度比后者高，且要求程序员了解现有组件库中的相关组件的内部结构，所以应用相对较少些。 该模式的主要优点如下。 客户端通过适配器可以透明地调用目标接口。 复用了现存的类，程序员不需要修改原有代码而重用现有的适配者类。 将目标类和适配者类解耦，解决了目标类和适配者类接口不一致的问题。 在很多业务场景中符合开闭原则。 其缺点是： 适配器编写过程需要结合业务场景全面考虑，可能会增加系统的复杂性。 增加代码阅读难度，降低代码可读性，过多使用适配器会使系统代码变得凌乱。 二，模式的结构与实现类适配器模式可采用多重继承方式实现，如 C++ 可定义一个适配器类来同时继承当前系统的业务接口和现有组件库中已经存在的组件接口；Java 不支持多继承，但可以定义一个适配器类来实现当前系统的业务接口，同时又继承现有组件库中已经存在的组件。 对象适配器模式可釆用将现有组件库中已经实现的组件引入适配器类中，该类同时实现当前系统的业务接口。现在来介绍它们的基本结构。 1.模式的结构适配器模式（Adapter）包含以下主要角色。 目标（Target）接口：当前系统业务所期待的接口，它可以是抽象类或接口。 适配者（Adaptee）类：它是被访问和适配的现存组件库中的组件接口。 适配器（Adapter）类：它是一个转换器，通过继承或引用适配者的对象，把适配者接口转换成目标接口，让客户按目标接口的格式访问适配者。 类适配器 对象适配器 2.模式的实现1）类适配器1234567891011121314151617181920212223242526272829303132package adapter;//目标接口interface Target&#123; public void request();&#125;//适配者接口class Adaptee&#123; public void specificRequest() &#123; System.out.println(&quot;适配者中的业务代码被调用！&quot;); &#125;&#125;//类适配器类class ClassAdapter extends Adaptee implements Target&#123; public void request() &#123; specificRequest(); &#125;&#125;//客户端代码public class ClassAdapterTest&#123; public static void main(String[] args) &#123; System.out.println(&quot;类适配器模式测试：&quot;); Target target = new ClassAdapter(); target.request(); &#125;&#125; 2）对象适配器12345678910111213141516171819202122232425package adapter;//对象适配器类class ObjectAdapter implements Target&#123; private Adaptee adaptee; public ObjectAdapter(Adaptee adaptee) &#123; this.adaptee=adaptee; &#125; public void request() &#123; adaptee.specificRequest(); &#125;&#125;//客户端代码public class ObjectAdapterTest&#123; public static void main(String[] args) &#123; System.out.println(&quot;对象适配器模式测试：&quot;); Adaptee adaptee = new Adaptee(); Target target = new ObjectAdapter(adaptee); target.request(); &#125;&#125; 对象适配器模式中的“目标接口”和“适配者类”的代码同类适配器模式一样，只要修改适配器类和客户端的代码即可。 三，模式的应用实例用适配器模式（Adapter）模拟新能源汽车的发动机。 分析：新能源汽车的发动机有电能发动机（Electric Motor）和光能发动机（Optical Motor）等，各种发动机的驱动方法不同，例如，电能发动机的驱动方法 electricDrive() 是用电能驱动，而光能发动机的驱动方法 opticalDrive() 是用光能驱动，它们是适配器模式中被访问的适配者。 客户端希望用统一的发动机驱动方法 drive() 访问这两种发动机，所以必须定义一个统一的目标接口 Motor，然后再定义电能适配器（Electric Adapter）和光能适配器（Optical Adapter）去适配这两种发动机。 我们把客户端想访问的新能源发动机的适配器的名称放在 XML 配置文件中（点此下载 XML 文件），客户端可以通过对象生成器类 ReadXML 去读取。这样，客户端就可以通过 Motor 接口随便使用任意一种新能源发动机去驱动汽车。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687package adapter;//目标：发动机interface Motor&#123; public void drive();&#125;//适配者1：电能发动机class ElectricMotor&#123; public void electricDrive() &#123; System.out.println(&quot;电能发动机驱动汽车！&quot;); &#125;&#125;//适配者2：光能发动机class OpticalMotor&#123; public void opticalDrive() &#123; System.out.println(&quot;光能发动机驱动汽车！&quot;); &#125;&#125;//电能适配器class ElectricAdapter implements Motor&#123; private ElectricMotor emotor; public ElectricAdapter() &#123; emotor=new ElectricMotor(); &#125; public void drive() &#123; emotor.electricDrive(); &#125;&#125;//光能适配器class OpticalAdapter implements Motor&#123; private OpticalMotor omotor; public OpticalAdapter() &#123; omotor=new OpticalMotor(); &#125; public void drive() &#123; omotor.opticalDrive(); &#125;&#125;//客户端代码public class MotorAdapterTest&#123; public static void main(String[] args) &#123; System.out.println(&quot;适配器模式测试：&quot;); Motor motor=(Motor)ReadXML.getObject(); motor.drive(); &#125;&#125;===========================================================================================package adapter;import javax.xml.parsers.*;import org.w3c.dom.*;import java.io.*;class ReadXML&#123; public static Object getObject() &#123; try &#123; DocumentBuilderFactory dFactory=DocumentBuilderFactory.newInstance(); DocumentBuilder builder=dFactory.newDocumentBuilder(); Document doc; doc=builder.parse(new File(&quot;src/adapter/config.xml&quot;)); NodeList nl=doc.getElementsByTagName(&quot;className&quot;); Node classNode=nl.item(0).getFirstChild(); String cName=&quot;adapter.&quot;+classNode.getNodeValue(); Class&lt;?&gt; c=Class.forName(cName); Object obj=c.newInstance(); return obj; &#125; catch(Exception e) &#123; e.printStackTrace(); return null; &#125; &#125;&#125; 四，模式的应用场景适配器模式（Adapter）通常适用于以下场景。 以前开发的系统存在满足新系统功能需求的类，但其接口同新系统的接口不一致。 使用第三方提供的组件，但组件接口定义和自己要求的接口定义不同。 五，模式的扩展适配器模式（Adapter）可扩展为双向适配器模式，双向适配器类既可以把适配者接口转换成目标接口，也可以把目标接口转换成适配者接口。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package adapter;//目标接口interface TwoWayTarget&#123; public void request();&#125;//适配者接口interface TwoWayAdaptee&#123; public void specificRequest();&#125;//目标实现class TargetRealize implements TwoWayTarget&#123; public void request() &#123; System.out.println(&quot;目标代码被调用！&quot;); &#125;&#125;//适配者实现class AdapteeRealize implements TwoWayAdaptee&#123; public void specificRequest() &#123; System.out.println(&quot;适配者代码被调用！&quot;); &#125;&#125;//双向适配器class TwoWayAdapter implements TwoWayTarget,TwoWayAdaptee&#123; private TwoWayTarget target; private TwoWayAdaptee adaptee; public TwoWayAdapter(TwoWayTarget target) &#123; this.target=target; &#125; public TwoWayAdapter(TwoWayAdaptee adaptee) &#123; this.adaptee=adaptee; &#125; public void request() &#123; adaptee.specificRequest(); &#125; public void specificRequest() &#123; target.request(); &#125;&#125;//客户端代码public class TwoWayAdapterTest&#123; public static void main(String[] args) &#123; System.out.println(&quot;目标通过双向适配器访问适配者：&quot;); TwoWayAdaptee adaptee=new AdapteeRealize(); TwoWayTarget target=new TwoWayAdapter(adaptee); target.request(); System.out.println(&quot;-------------------&quot;); System.out.println(&quot;适配者通过双向适配器访问目标：&quot;); target=new TargetRealize(); adaptee=new TwoWayAdapter(target); adaptee.specificRequest(); &#125;&#125; 程序的运行结果如下： 12345目标通过双向适配器访问适配者：适配者代码被调用！-------------------适配者通过双向适配器访问目标：目标代码被调用！","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"桥接模式","slug":"设计模式/结构型模式之桥接模式","date":"2022-01-11T12:00:46.212Z","updated":"2022-01-11T12:07:51.865Z","comments":true,"path":"2022/01/11/设计模式/结构型模式之桥接模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E7%BB%93%E6%9E%84%E5%9E%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E6%A1%A5%E6%8E%A5%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"在现实生活中，某些类具有两个或多个维度的变化，如图形既可按形状分，又可按颜色分。如何设计类似于 Photoshop 这样的软件，能画不同形状和不同颜色的图形呢？如果用继承方式，m 种形状和 n 种颜色的图形就有 m×n 种，不但对应的子类很多，而且扩展困难。 当然，这样的例子还有很多，如不同颜色和字体的文字、不同品牌和功率的汽车、不同性别和职业的男女、支持不同平台和不同文件格式的媒体播放器等。如果用桥接模式就能很好地解决这些问题。 一，桥接模式的定义与特点将抽象与实现分离，使它们可以独立变化。它是用组合关系代替继承关系来实现，从而降低了抽象和实现这两个可变维度的耦合度。 桥接模式遵循了里氏替换原则和依赖倒置原则，最终实现了开闭原则，对修改关闭，对扩展开放。 桥接（Bridge）模式的优点是： 抽象与实现分离，扩展能力强 符合开闭原则 符合合成复用原则 其实现细节对客户透明 缺点是：由于聚合关系建立在抽象层，要求开发者针对抽象化进行设计与编程，能正确地识别出系统中两个独立变化的维度，这增加了系统的理解与设计难度。 二，桥接模式的结构与实现可以将抽象化部分与实现化部分分开，取消二者的继承关系，改用组合关系。 1.模式的结构桥接（Bridge）模式包含以下主要角色。 抽象化（Abstraction）角色：定义抽象类，并包含一个对实现化对象的引用。 扩展抽象化（Refined Abstraction）角色：是抽象化角色的子类，实现父类中的业务方法，并通过组合关系调用实现化角色中的业务方法。 实现化（Implementor）角色：定义实现化角色的接口，供扩展抽象化角色调用。 具体实现化（Concrete Implementor）角色：给出实现化角色接口的具体实现。 2.模式的实现1234567891011121314151617181920212223242526272829303132333435363738394041424344package bridge;public class BridgeTest &#123; public static void main(String[] args) &#123; Implementor imple = new ConcreteImplementorA(); Abstraction abs = new RefinedAbstraction(imple); abs.Operation(); &#125;&#125;//实现化角色interface Implementor &#123; public void OperationImpl();&#125;//具体实现化角色class ConcreteImplementorA implements Implementor &#123; public void OperationImpl() &#123; System.out.println(&quot;具体实现化(Concrete Implementor)角色被访问&quot;); &#125;&#125;//抽象化角色abstract class Abstraction &#123; protected Implementor imple; protected Abstraction(Implementor imple) &#123; this.imple = imple; &#125; public abstract void Operation();&#125;//扩展抽象化角色class RefinedAbstraction extends Abstraction &#123; protected RefinedAbstraction(Implementor imple) &#123; super(imple); &#125; public void Operation() &#123; System.out.println(&quot;扩展抽象化(Refined Abstraction)角色被访问&quot;); imple.OperationImpl(); &#125;&#125; 三，桥接模式的应用实例模拟女士皮包的选购 分析：女士皮包有很多种，可以按用途分、按皮质分、按品牌分、按颜色分、按大小分等，存在多个维度的变化，所以采用桥接模式来实现女士皮包的选购比较合适。 本实例按用途分可选钱包（Wallet）和挎包（HandBag），按颜色分可选黄色（Yellow）和红色（Red）。可以按两个维度定义为颜色类和包类。 颜色类（Color）是一个维度，定义为实现化角色，它有两个具体实现化角色：黄色和红色，通过 getColor() 方法可以选择颜色；包类（Bag）是另一个维度，定义为抽象化角色，它有两个扩展抽象化角色：挎包和钱包，它包含了颜色类对象，通过 getName() 方法可以选择相关颜色的挎包和钱包。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667//============= Color ====================//public interface Color &#123; String getColor();&#125;public class Red implements Color&#123; @Override public String getColor() &#123; return &quot;红色&quot;; &#125;&#125;public class Yellow implements Color&#123; @Override public String getColor() &#123; return &quot;黄色&quot;; &#125;&#125;//========================== Bag ========================//public abstract class Bag &#123; Color color; public Bag(Color color) &#123; this.color = color; &#125; public void setColor(Color color) &#123; this.color = color; &#125; public abstract String getName();&#125;public class HandBag extends Bag&#123; public HandBag(Color color) &#123; super(color); &#125; @Override public String getName() &#123; return &quot;挎包&quot;; &#125;&#125;public class Wallet extends Bag&#123; public Wallet(Color color) &#123; super(color); &#125; @Override public String getName() &#123; return &quot;钱包&quot;; &#125;&#125;//=================== Test ==========================//public class MainTest &#123; public static void main(String[] args) &#123; Color color = new Red(); Bag bag = new Wallet(color); System.out.println(&quot;bag.getName() = &quot; + bag.getName()); System.out.println(&quot;color.getColor() = &quot; + color.getColor()); &#125;&#125; 四，桥接模式的应用场景当一个类内部具备两种或多种变化维度时，使用桥接模式可以解耦这些变化的维度，使高层代码架构稳定。 桥接模式通常适用于以下场景。 当一个类存在两个独立变化的维度，且这两个维度都需要进行扩展时。 当一个系统不希望使用继承或因为多层次继承导致系统类的个数急剧增加时。 当一个系统需要在构件的抽象化角色和具体化角色之间增加更多的灵活性时。 桥接模式的一个常见使用场景就是替换继承。我们知道，继承拥有很多优点，比如，抽象、封装、多态等，父类封装共性，子类实现特性。继承可以很好的实现代码复用（封装）的功能，但这也是继承的一大缺点。 因为父类拥有的方法，子类也会继承得到，无论子类需不需要，这说明继承具备强侵入性（父类代码侵入子类），同时会导致子类臃肿。因此，在设计模式中，有一个原则为优先使用组合/聚合，而不是继承。 很多时候，我们分不清该使用继承还是组合/聚合或其他方式等，其实可以从现实语义进行思考。因为软件最终还是提供给现实生活中的人使用的，是服务于人类社会的，软件是具备现实场景的。当我们从纯代码角度无法看清问题时，现实角度可能会提供更加开阔的思路。 五，桥接模式的扩展在软件开发中，有时桥接（Bridge）模式可与[适配器模式]联合使用。当桥接（Bridge）模式的实现化角色的接口与现有类的接口不一致时，可以在二者中间定义一个适配器将二者连接起来。 其实也可以和工厂模式组合。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"装饰者模式","slug":"设计模式/结构型模式之装饰者模式","date":"2022-01-11T12:00:37.880Z","updated":"2022-01-11T12:08:48.107Z","comments":true,"path":"2022/01/11/设计模式/结构型模式之装饰者模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E7%BB%93%E6%9E%84%E5%9E%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E8%A3%85%E9%A5%B0%E8%80%85%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"有些人早餐可能会吃煎饼，煎饼中可以加鸡蛋，也可以加香肠，但是不管怎么“加码”，都还是一个煎饼。在现实生活中，常常需要对现有产品增加新的功能或美化其外观，如房子装修、相片加相框等，都是装饰器模式。 在软件开发过程中，有时想用一些现存的组件。这些组件可能只是完成了一些核心功能。但在不改变其结构的情况下，可以动态地扩展其功能。所有这些都可以釆用装饰器模式来实现。 一，装饰者模式的定义与特点装饰器（Decorator）模式的定义：指在不改变现有对象结构的情况下，动态地给该对象增加一些职责（即增加其额外功能）的模式，它属于对象结构型模式。 装饰器模式的主要优点有： 装饰器是继承的有力补充，比继承灵活，在不改变原有对象的情况下，动态的给一个对象扩展功能，即插即用 通过使用不用装饰类及这些装饰类的排列组合，可以实现不同效果 装饰器模式完全遵守开闭原则 其主要缺点是：装饰器模式会增加许多子类，过度使用会增加程序得复杂性。 二，装饰者模式的结构与实现通常情况下，扩展一个类的功能会使用继承方式来实现。但继承具有静态特征，耦合度高，并且随着扩展功能的增多，子类会很膨胀。如果使用组合关系来创建一个包装对象（即装饰对象）来包裹真实对象，并在保持真实对象的类结构不变的前提下，为其提供额外的功能，这就是装饰器模式的目标。 1.模式的结构装饰器模式主要包含以下角色。 抽象构件（Component）角色：定义一个抽象接口以规范准备接收附加责任的对象。 具体构件（ConcreteComponent）角色：实现抽象构件，通过装饰角色为其添加一些职责。 抽象装饰（Decorator）角色：继承抽象构件，并包含具体构件的实例，可以通过其子类扩展具体构件的功能。 具体装饰（ConcreteDecorator）角色：实现抽象装饰的相关方法，并给具体构件对象添加附加的责任。 2.模式的实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public interface Component &#123; void operation();&#125;public class ConcreteComponent implements Component&#123; public ConcreteComponent() &#123; System.out.println(&quot;创建具体构件角色&quot;); &#125; @Override public void operation() &#123; System.out.println(&quot;调用具体构件角色的方法operation()&quot;); &#125;&#125;public abstract class Decorator implements Component&#123; private Component component; public Decorator(Component component) &#123; this.component = component; &#125; @Override public void operation() &#123; component.operation(); &#125;&#125;public class ConcreteDecorator extends Decorator&#123; public ConcreteDecorator(Component component) &#123; super(component); &#125; @Override public void operation() &#123; super.operation(); addedFunction(); &#125; public void addedFunction() &#123; System.out.println(&quot;为具体构件角色增加额外的功能addedFunction()&quot;); &#125;&#125;public class MainTest &#123; public static void main(String[] args) &#123; Component component = new ConcreteComponent(); Decorator decorator = new ConcreteDecorator(component); decorator.operation(); &#125;&#125; 三，装饰器模式的应用实例用装饰器模式实现游戏角色“莫莉卡·安斯兰”的变身。 分析：在《恶魔战士》中，游戏角色“莫莉卡·安斯兰”的原身是一个可爱少女，但当她变身时，会变成头顶及背部延伸出蝙蝠状飞翼的女妖，当然她还可以变为穿着漂亮外衣的少女。这些都可用装饰器模式来实现，在本实例中的“莫莉卡”原身有 setImage(String t) 方法决定其显示方式，而其 变身“蝙蝠状女妖”和“着装少女”可以用 setChanger() 方法来改变其外观，原身与变身后的效果用 display() 方法来显示。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475import java.awt.*;import javax.swing.*;public class MorriganAensland &#123; public static void main(String[] args) &#123; Morrigan m0 = new original(); m0.display(); Morrigan m1 = new Succubus(m0); m1.display(); Morrigan m2 = new Girl(m0); m2.display(); &#125;&#125;//抽象构件角色：莫莉卡interface Morrigan &#123; public void display();&#125;//具体构件角色：原身class original extends JFrame implements Morrigan &#123; private static final long serialVersionUID = 1L; private String t = &quot;2.jpeg&quot;; public original() &#123; super(&quot;《恶魔战士》中的莫莉卡·安斯兰&quot;); &#125; public void setImage(String t) &#123; this.t = t; &#125; @Override public void display() &#123; this.setLayout(new FlowLayout()); JLabel l1 = new JLabel(new ImageIcon(&quot;/home/yhd/图片/&quot; + t)); this.add(l1); this.pack(); this.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE); this.setVisible(true); &#125;&#125;//抽象装饰角色：变形class Changer implements Morrigan &#123; Morrigan m; public Changer(Morrigan m) &#123; this.m = m; &#125; @Override public void display() &#123; m.display(); &#125;&#125;//具体装饰角色：女妖class Succubus extends Changer &#123; public Succubus(Morrigan m) &#123; super(m); &#125; @Override public void display() &#123; setChanger(); super.display(); &#125; public void setChanger() &#123; ((original) super.m).setImage(&quot;3.jpeg&quot;); &#125;&#125;//具体装饰角色：少女class Girl extends Changer &#123; public Girl(Morrigan m) &#123; super(m); &#125; @Override public void display() &#123; setChanger(); super.display(); &#125; public void setChanger() &#123; ((original) super.m).setImage(&quot;4.jpeg&quot;); &#125;&#125; 四，装饰器模式的应用场景 当需要给一个现有类添加附加职责，而又不能采用生成子类的方法进行扩充时。例如，该类被隐藏或者该类是终极类或者采用继承方式会产生大量的子类。 当需要通过对现有的一组基本功能进行排列组合而产生非常多的功能时，采用继承关系很难实现，而采用装饰器模式却很好实现。 当对象的功能要求可以动态地添加，也可以再动态地撤销时。 Java I/O 标准库的设计 例如，InputStream 的子类 FilterInputStream，OutputStream 的子类 FilterOutputStream，Reader 的子类 BufferedReader 以及 FilterReader，还有 Writer 的子类 BufferedWriter、FilterWriter 以及 PrintWriter 等，它们都是抽象装饰类。 五，装饰器模式的扩展装饰器模式所包含的 4 个角色不是任何时候都要存在的，在有些应用环境下模式是可以简化的，如以下两种情况。 1.如果只有一个具体构件而没有抽象构件时，可以让抽象装饰继承具体构件。 2.如果只有一个具体装饰时，可以将抽象装饰和具体装饰合并。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"外观模式","slug":"设计模式/结构型模式之外观模式","date":"2022-01-11T12:00:30.269Z","updated":"2022-01-11T12:08:09.351Z","comments":true,"path":"2022/01/11/设计模式/结构型模式之外观模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E7%BB%93%E6%9E%84%E5%9E%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%A4%96%E8%A7%82%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"在现实生活中，常常存在办事较复杂的例子，如办房产证或注册一家公司，有时要同多个部门联系，这时要是有一个综合部门能解决一切手续问题就好了。 软件设计也是这样，当一个系统的功能越来越强，子系统会越来越多，客户对系统的访问也变得越来越复杂。这时如果系统内部发生改变，客户端也要跟着改变，这违背了“开闭原则”，也违背了“迪米特法则”，所以有必要为多个子系统提供一个统一的接口，从而降低系统的耦合度，这就是外观模式的目标。 客户去当地房产局办理房产证过户要遇到的相关部门 一，外观模式的定义与特点外观（Facade）模式又叫作门面模式，是一种通过为多个复杂的子系统提供一个一致的接口，而使这些子系统更加容易被访问的模式。该模式对外有一个统一接口，外部应用程序不用关心内部子系统的具体细节，这样会大大降低应用程序的复杂度，提高了程序的可维护性。 在日常编码工作中，我们都在有意无意的大量使用外观模式。只要是高层模块需要调度多个子系统（2个以上的类对象），我们都会自觉地创建一个新的类封装这些子系统，提供精简的接口，让高层模块可以更加容易地间接调用这些子系统的功能。尤其是现阶段各种第三方SDK、开源类库，很大概率都会使用外观模式。 外观（Facade）模式是“迪米特法则”的典型应用，它有以下主要优点。 降低了子系统与客户端之间的耦合度，使得子系统的变化不会影响调用它的客户类。 对客户屏蔽了子系统组件，减少了客户处理的对象数目，并使得子系统使用起来更加容易。 降低了大型软件系统中的编译依赖性，简化了系统在不同平台之间的移植过程，因为编译一个子系统不会影响其他的子系统，也不会影响外观对象。 外观（Facade）模式的主要缺点如下。 不能很好地限制客户使用子系统类，很容易带来未知风险。 增加新的子系统可能需要修改外观类或客户端的源代码，违背了“开闭原则”。 二，外观模式的结构与实现外观（Facade）模式的结构比较简单，主要是定义了一个高层接口。它包含了对各个子系统的引用，客户端可以通过它访问各个子系统的功能。现在来分析其基本结构和实现方法。 1.模式的结构外观（Facade）模式包含以下主要角色。 外观（Facade）角色：为多个子系统对外提供一个共同的接口。 子系统（Sub System）角色：实现系统的部分功能，客户可以通过外观角色访问它。 客户（Client）角色：通过一个外观角色访问各个子系统的功能。 2.模式的实现1234567891011121314151617181920212223242526272829303132333435public class FacadePattern &#123; public static void main(String[] args) &#123; Facade f = new Facade(); f.method(); &#125;&#125;//外观角色class Facade &#123; private SubSystem01 obj1 = new SubSystem01(); private SubSystem02 obj2 = new SubSystem02(); private SubSystem03 obj3 = new SubSystem03(); public void method() &#123; obj1.method1(); obj2.method2(); obj3.method3(); &#125;&#125;//子系统角色class SubSystem01 &#123; public void method1() &#123; System.out.println(&quot;子系统01的method1()被调用！&quot;); &#125;&#125;//子系统角色class SubSystem02 &#123; public void method2() &#123; System.out.println(&quot;子系统02的method2()被调用！&quot;); &#125;&#125;//子系统角色class SubSystem03 &#123; public void method3() &#123; System.out.println(&quot;子系统03的method3()被调用！&quot;); &#125;&#125; 三，外观模式的应用实例用门面模式实现开保时捷一晚上约三个妹子 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public class BaoShiJie &#123; public void happy()&#123; GirlOne one = new GirlOne(); GirlTwo two = new GirlTwo(); GirlThree three = new GirlThree(); one.papapa(&quot;张三&quot;); two.papa(&quot;张三&quot;); three.kuaile(&quot;张三&quot;); &#125;&#125;// =============== 妹子类 ========================== //public class GirlOne &#123; public void papapa(String name )&#123; System.out.println(name+&quot;与美女一号一起鼓掌&quot;); &#125;&#125;public class GirlTwo &#123; public void papa(String name)&#123; System.out.println(name+&quot;与美女二号一起起床&quot;); &#125;&#125;public class GirlThree &#123; public void kuaile(String name)&#123; System.out.println(name+&quot;与美女三号一度春宵&quot;); &#125;&#125;public class MainTest &#123; /** * 设计模式之门面模式 * 需求：一晚上约三个妹子 * 一个一个约 * 门面模式：一窝端 */ @Test public void test()&#123; GirlOne one = new GirlOne(); GirlTwo two = new GirlTwo(); GirlThree three = new GirlThree(); one.papapa(&quot;张三&quot;); two.papa(&quot;张三&quot;); three.kuaile(&quot;张三&quot;); &#125; @Test public void testA()&#123; BaoShiJie panameila = new BaoShiJie(); panameila.happy(); &#125;&#125; 四，外观模式的应用场景通常在以下情况下可以考虑使用外观模式。 对分层结构系统构建时，使用外观模式定义子系统中每层的入口点可以简化子系统之间的依赖关系。 当一个复杂系统的子系统很多时，外观模式可以为系统设计一个简单的接口供外界访问。 当客户端与多个子系统之间存在很大的联系时，引入外观模式可将它们分离，从而提高子系统的独立性和可移植性。 五，外观模式的扩展在外观模式中，当增加或移除子系统时需要修改外观类，这违背了“开闭原则”。如果引入抽象外观类，则在一定程度上解决了该问题。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"享元模式","slug":"设计模式/结构型模式之享元模式","date":"2022-01-11T12:00:19.054Z","updated":"2022-01-11T12:08:36.283Z","comments":true,"path":"2022/01/11/设计模式/结构型模式之享元模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E7%BB%93%E6%9E%84%E5%9E%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E4%BA%AB%E5%85%83%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"在面向对象程序设计过程中，有时会面临要创建大量相同或相似对象实例的问题。创建那么多的对象将会耗费很多的系统资源，它是系统性能提高的一个瓶颈。 例如，围棋和五子棋中的黑白棋子，图像中的坐标点或颜色，局域网中的路由器、交换机和集线器，教室里的桌子和凳子等。这些对象有很多相似的地方，如果能把它们相同的部分提取出来共享，则能节省大量的系统资源，这就是享元模式的产生背景。 一，享元模式的定义与特点享元（Flyweight）模式的定义：运用共享技术来有效地支持大量细粒度对象的复用。它通过共享已经存在的对象来大幅度减少需要创建的对象数量、避免大量相似类的开销，从而提高系统资源的利用率。 享元模式的主要优点是：相同对象只要保存一份，这降低了系统中对象的数量，从而降低了系统中细粒度对象给内存带来的压力。 其主要缺点是： 为了使对象可以共享，需要将一些不能共享的状态外部化，这将增加程序的复杂性。 读取享元模式的外部状态会使得运行时间稍微变长。 时间换空间的思想 二，享元模式的结构与实现享元模式的定义提出了两个要求，细粒度和共享对象。因为要求细粒度，所以不可避免地会使对象数量多且性质相近，此时我们就将这些对象的信息分为两个部分：内部状态和外部状态。 内部状态指对象共享出来的信息，存储在享元信息内部，并且不回随环境的改变而改变； 外部状态指对象得以依赖的一个标记，随环境的改变而改变，不可共享。 比如，连接池中的连接对象，保存在连接对象中的用户名、密码、连接URL等信息，在创建对象的时候就设置好了，不会随环境的改变而改变，这些为内部状态。而当每个连接要被回收利用时，我们需要将它标记为可用状态，这些为外部状态。 享元模式的本质是缓存共享对象，降低内存消耗。 1.模式的结构享元模式的主要角色有如下。 抽象享元角色（Flyweight）：是所有的具体享元类的基类，为具体享元规范需要实现的公共接口，非享元的外部状态以参数的形式通过方法传入。 具体享元（Concrete Flyweight）角色：实现抽象享元角色中所规定的接口。 非享元（Unsharable Flyweight)角色：是不可以共享的外部状态，它以参数的形式注入具体享元的相关方法中。 享元工厂（Flyweight Factory）角色：负责创建和管理享元角色。当客户对象请求一个享元对象时，享元工厂检査系统中是否存在符合要求的享元对象，如果存在则提供给客户；如果不存在的话，则创建一个新的享元对象。 享元模式的结构图，其中： UnsharedConcreteFlyweight 是非享元角色，里面包含了非共享的外部状态信息 info； Flyweight 是抽象享元角色，里面包含了享元方法 operation(UnsharedConcreteFlyweight state)，非享元的外部状态以参数的形式通过该方法传入； ConcreteFlyweight 是具体享元角色，包含了关键字 key，它实现了抽象享元接口； FlyweightFactory 是享元工厂角色，它是关键字 key 来管理具体享元； 客户角色通过享元工厂获取具体享元，并访问具体享元的相关方法。 三，享元模式的应用实例享元模式在五子棋游戏中的应用 分析：五子棋同围棋一样，包含多个“黑”或“白”颜色的棋子，所以用享元模式比较好。 本实例中: 棋子（ChessPieces）类是抽象享元角色，它包含了一个落子的 DownPieces(Graphics g,Point pt) 方法； 白子（WhitePieces）和黑子（BlackPieces）类是具体享元角色，它实现了落子方法； Point 是非享元角色，它指定了落子的位置； WeiqiFactory 是享元工厂角色，它通过 ArrayList 来管理棋子，并且提供了获取白子或者黑子的 getChessPieces(String type) 方法； 客户类（Chessboard）利用 Graphics 组件在框架窗体中绘制一个棋盘，并实现 mouseClicked(MouseEvent e) 事件处理方法，该方法根据用户的选择从享元工厂中获取白子或者黑子并落在棋盘上。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113public class WzqGame &#123; public static void main(String[] args) &#123; new Chessboard(); &#125;&#125;//棋盘class Chessboard extends MouseAdapter &#123; WeiqiFactory wf; JFrame f; Graphics g; JRadioButton wz; JRadioButton bz; private final int x = 50; private final int y = 50; private final int w = 40; //小方格宽度和高度 private final int rw = 400; //棋盘宽度和高度 Chessboard() &#123; wf = new WeiqiFactory(); f = new JFrame(&quot;享元模式在五子棋游戏中的应用&quot;); f.setBounds(100, 100, 500, 550); f.setVisible(true); f.setResizable(false); f.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE); JPanel SouthJP = new JPanel(); f.add(&quot;South&quot;, SouthJP); wz = new JRadioButton(&quot;白子&quot;); bz = new JRadioButton(&quot;黑子&quot;, true); ButtonGroup group = new ButtonGroup(); group.add(wz); group.add(bz); SouthJP.add(wz); SouthJP.add(bz); JPanel CenterJP = new JPanel(); CenterJP.setLayout(null); CenterJP.setSize(500, 500); CenterJP.addMouseListener(this); f.add(&quot;Center&quot;, CenterJP); try &#123; Thread.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; g = CenterJP.getGraphics(); g.setColor(Color.BLUE); g.drawRect(x, y, rw, rw); for (int i = 1; i &lt; 10; i++) &#123; //绘制第i条竖直线 g.drawLine(x + (i * w), y, x + (i * w), y + rw); //绘制第i条水平线 g.drawLine(x, y + (i * w), x + rw, y + (i * w)); &#125; &#125; @Override public void mouseClicked(MouseEvent e) &#123; Point pt = new Point(e.getX() - 15, e.getY() - 15); if (wz.isSelected()) &#123; ChessPieces c1 = wf.getChessPieces(&quot;w&quot;); c1.DownPieces(g, pt); &#125; else if (bz.isSelected()) &#123; ChessPieces c2 = wf.getChessPieces(&quot;b&quot;); c2.DownPieces(g, pt); &#125; &#125;&#125;//抽象享元角色：棋子interface ChessPieces &#123; public void DownPieces(Graphics g, Point pt); //下子&#125;//具体享元角色：白子class WhitePieces implements ChessPieces &#123; @Override public void DownPieces(Graphics g, Point pt) &#123; g.setColor(Color.WHITE); g.fillOval(pt.x, pt.y, 30, 30); &#125;&#125;//具体享元角色：黑子class BlackPieces implements ChessPieces &#123; @Override public void DownPieces(Graphics g, Point pt) &#123; g.setColor(Color.BLACK); g.fillOval(pt.x, pt.y, 30, 30); &#125;&#125;//享元工厂角色class WeiqiFactory &#123; private ArrayList&lt;ChessPieces&gt; qz; public WeiqiFactory() &#123; qz = new ArrayList&lt;ChessPieces&gt;(); ChessPieces w = new WhitePieces(); qz.add(w); ChessPieces b = new BlackPieces(); qz.add(b); &#125; public ChessPieces getChessPieces(String type) &#123; if (type.equalsIgnoreCase(&quot;w&quot;)) &#123; return (ChessPieces) qz.get(0); &#125; else if (type.equalsIgnoreCase(&quot;b&quot;)) &#123; return (ChessPieces) qz.get(1); &#125; else &#123; return null; &#125; &#125;&#125; 四，享元模式的应用场景当系统中多处需要同一组信息时，可以把这些信息封装到一个对象中，然后对该对象进行缓存，这样，一个对象就可以提供给多出需要使用的地方，避免大量同一对象的多次创建，降低大量内存空间的消耗。 享元模式其实是[工厂方法模式]的一个改进机制，享元模式同样要求创建一个或一组对象，并且就是通过工厂方法模式生成对象的，只不过享元模式为工厂方法模式增加了缓存这一功能。 前面分析了享元模式的结构与特点，下面分析它适用的应用场景。享元模式是通过减少内存中对象的数量来节省内存空间的，所以以下几种情形适合采用享元模式。 系统中存在大量相同或相似的对象，这些对象耗费大量的内存资源。 大部分的对象可以按照内部状态进行分组，且可将不同部分外部化，这样每一个组只需保存一个内部状态。 由于享元模式需要额外维护一个保存享元的[数据结构]，所以应当在有足够多的享元实例时才值得使用享元模式。 五，享元模式的扩展在前面介绍的享元模式中，其结构图通常包含可以共享的部分和不可以共享的部分。在实际使用过程中，有时候会稍加改变，即存在两种特殊的享元模式：单纯享元模式和复合享元模式，下面分别对它们进行简单介绍。 单纯享元模式，这种享元模式中的所有的具体享元类都是可以共享的，不存在非共享的具体享元类。 2)复合享元模式，这种享元模式中的有些享元对象是由一些单纯享元对象组合而成的，它们就是复合享元对象。虽然复合享元对象本身不能共享，但它们可以分解成单纯享元对象再被共享。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"组合模式","slug":"设计模式/结构型模式之组合模式","date":"2022-01-11T11:59:59.908Z","updated":"2022-01-11T12:08:58.455Z","comments":true,"path":"2022/01/11/设计模式/结构型模式之组合模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E7%BB%93%E6%9E%84%E5%9E%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E7%BB%84%E5%90%88%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"在现实生活中，存在很多“部分-整体”的关系，例如，大学中的部门与学院、总公司中的部门与分公司、学习用品中的书与书包、生活用品中的衣服与衣柜、以及厨房中的锅碗瓢盆等。在软件开发中也是这样，例如，文件系统中的文件与文件夹、窗体程序中的简单控件与容器控件等。对这些简单对象与复合对象的处理，如果用组合模式来实现会很方便。 一，组合模式的定义与特点它是一种将对象组合成树状的层次结构的模式，用来表示“整体-部分”的关系，使用户对单个对象和组合对象具有一致的访问性，属于结构型[设计模式]。 组合模式一般用来描述整体与部分的关系，它将对象组织到树形结构中，顶层的节点被称为根节点，根节点下面可以包含树枝节点和叶子节点，树枝节点下面又可以包含树枝节点和叶子节点，树形结构图如下。 由上图可以看出，其实根节点和树枝节点本质上属于同一种数据类型，可以作为容器使用；而叶子节点与树枝节点在语义上不属于用一种类型。但是在组合模式中，会把树枝节点和叶子节点看作属于同一种数据类型（用统一接口定义），让它们具备一致行为。 这样，在组合模式中，整个树形结构中的对象都属于同一种类型，带来的好处就是用户不需要辨别是树枝节点还是叶子节点，可以直接进行操作，给用户的使用带来极大的便利。 组合模式的主要优点有： 组合模式使得客户端代码可以一致地处理单个对象和组合对象，无须关心自己处理的是单个对象，还是组合对象，这简化了客户端代码； 更容易在组合体内加入新的对象，客户端不会因为加入了新的对象而更改源代码，满足“开闭原则”； 其主要缺点是： 设计较复杂，客户端需要花更多时间理清类之间的层次关系； 不容易限制容器中的构件； 不容易用继承的方法来增加构件的新功能； 二，组合模式的结构与实现组合模式的结构不是很复杂，下面对它的结构和实现进行分析。 1.模式的结构组合模式包含以下主要角色。 抽象构件（Component）角色：它的主要作用是为树叶构件和树枝构件声明公共接口，并实现它们的默认行为。在透明式的组合模式中抽象构件还声明访问和管理子类的接口；在安全式的组合模式中不声明访问和管理子类的接口，管理工作由树枝构件完成。（总的抽象类或接口，定义一些通用的方法，比如新增、删除） 树叶构件（Leaf）角色：是组合中的叶节点对象，它没有子节点，用于继承或实现抽象构件。 树枝构件（Composite）角色 / 中间构件：是组合中的分支节点对象，它有子节点，用于继承和实现抽象构件。它的主要作用是存储和管理子部件，通常包含 Add()、Remove()、GetChild() 等方法。 组合模式分为透明式的组合模式和安全式的组合模式。 1）透明方式在该方式中，由于抽象构件声明了所有子类中的全部方法，所以客户端无须区别树叶对象和树枝对象，对客户端来说是透明的。但其缺点是：树叶构件本来没有 Add()、Remove() 及 GetChild() 方法，却要实现它们（空实现或抛异常），这样会带来一些安全性问题。 2）安全方式在该方式中，将管理子构件的方法移到树枝构件中，抽象构件和树叶构件没有对子对象的管理方法，这样就避免了上一种方式的安全性问题，但由于叶子和分支有不同的接口，客户端在调用时要知道树叶对象和树枝对象的存在，所以失去了透明性。 2.模式的实现假如要访问集合 c0=&#123;leaf1,&#123;leaf2,leaf3&#125;&#125; 中的元素，其对应的树状图如图所示。 1)透明组合模式1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071class Leaf implements Component &#123; String name; public Leaf(String name) &#123; this.name = name; &#125; @Override public void add(Component c) &#123; &#125; @Override public void remove(Component c) &#123; &#125; @Override public Component getChild(int i) &#123; return null; &#125; @Override public void operation() &#123; System.out.println(&quot;树叶&quot; + name + &quot;被访问！&quot;); &#125;&#125;class Composite implements Component &#123; List&lt;Component&gt; children = new ArrayList&lt;&gt;(); @Override public void add(Component c) &#123; children.add(c); &#125; @Override public void remove(Component c) &#123; children.remove(c); &#125; @Override public Component getChild(int i) &#123; return children.get(i); &#125; @Override public void operation() &#123; if(children!=null &amp;&amp; children.size()&gt;0)&#123; children.forEach(Component::operation); &#125; &#125;&#125;class MainTest&#123; public static void main(String[] args) &#123; Component c0 = new Composite(); Component c1 = new Composite(); Component l1 = new Leaf(&quot;leaf1&quot;); Component l2 = new Leaf(&quot;leaf2&quot;); Component l3 = new Leaf(&quot;leaf3&quot;); c0.add(l1); c0.add(c1); c1.add(l2); c1.add(l3); c0.operation(); &#125;&#125; 2)安全组合模式安全式的组合模式与透明式组合模式的实现代码类似，只要对其做简单修改就可以了，代码如下。 123456789101112131415161718192021public interface Component &#123; void operation();&#125;class MainTest&#123; public static void main(String[] args) &#123; Composite c0 = new Composite(); Composite c1 = new Composite(); Component l1 = new Leaf(&quot;leaf1&quot;); Component l2 = new Leaf(&quot;leaf2&quot;); Component l3 = new Leaf(&quot;leaf3&quot;); c0.add(l1); c0.add(c1); c1.add(l2); c1.add(l3); c0.operation(); &#125;&#125; 三，组合模式的应用实例用组合模式实现当用户在商店购物后，显示其所选商品信息，并计算所选商品总价的功能。 说明：假如李先生到韶关“天街e角”生活用品店购物，用 1 个红色小袋子装了 2 包婺源特产（单价 7.9 元）、1 张婺源地图（单价 9.9 元）；用 1 个白色小袋子装了 2 包韶关香藉（单价 68 元）和 3 包韶关红茶（单价 180 元）；用 1 个中袋子装了前面的红色小袋子和 1 个景德镇瓷器（单价 380 元）；用 1 个大袋子装了前面的中袋子、白色小袋子和 1 双李宁牌运动鞋（单价 198 元）。 最后“大袋子”中的内容有：{1 双李宁牌运动鞋（单价 198 元）、白色小袋子{2 包韶关香菇（单价 68 元）、3 包韶关红茶（单价 180 元）}、中袋子{1 个景德镇瓷器（单价 380 元）、红色小袋子{2 包婺源特产（单价 7.9 元）、1 张婺源地图（单价 9.9 元）}}}，现在要求编程显示李先生放在大袋子中的所有商品信息并计算要支付的总价。 本实例可按安全组合模式设计。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public interface Dad &#123; float calculation(); //计算 void show();&#125;public class Bag implements Dad &#123; private String name; List&lt;Dad&gt; list = new ArrayList&lt;&gt;(); public Bag(String name) &#123; this.name = name; &#125; @Override public float calculation() &#123; float total = 0; for (Dad dad : list) &#123; total += dad.calculation(); &#125; return total; &#125; @Override public void show() &#123; list.forEach(Dad::show); &#125; public void add(Dad c) &#123; list.add(c); &#125; public void remove(Dad c) &#123; list.remove(c); &#125; public Dad getChild(int i) &#123; return list.get(i); &#125;&#125;class Product implements Dad &#123; private String name; //名字 private int quantity; //数量 private float unitPrice; //单价 public Product(String name, int quantity, float unitPrice) &#123; this.name = name; this.quantity = quantity; this.unitPrice = unitPrice; &#125; @Override public float calculation() &#123; return quantity * unitPrice; &#125; @Override public void show() &#123; System.out.println(name + &quot;(数量：&quot; + quantity + &quot;，单价：&quot; + unitPrice + &quot;元)&quot;); &#125;&#125; 四，组合模式的应用场景 在需要表示一个对象整体与部分的层次结构的场合。 要求对用户隐藏组合对象与单个对象的不同，用户可以用统一的接口使用组合结构中的所有对象的场合。 五，组合模式的扩展如果对前面介绍的组合模式中的树叶节点和树枝节点进行抽象，也就是说树叶节点和树枝节点还有子节点，这时组合模式就扩展成复杂的组合模式了，如 [Java] AWT/[Swing] 中的简单组件 JTextComponent 有子类 JTextField、JTextArea，容器组件 Container 也有子类 Window、Panel。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"模板方法模式","slug":"设计模式/行为型模式之模板方法模式","date":"2022-01-11T11:59:38.496Z","updated":"2022-01-11T12:11:00.871Z","comments":true,"path":"2022/01/11/设计模式/行为型模式之模板方法模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%A1%8C%E4%B8%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E6%A8%A1%E6%9D%BF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"在面向对象程序设计过程中，程序员常常会遇到这种情况：设计一个系统时知道了算法所需的关键步骤，而且确定了这些步骤的执行顺序，但某些步骤的具体实现还未知，或者说某些步骤的实现与具体的环境相关。 例如，去银行办理业务一般要经过以下4个流程：取号、排队、办理具体业务、对银行工作人员进行评分等，其中取号、排队和对银行工作人员进行评分的业务对每个客户是一样的，可以在父类中实现，但是办理具体业务却因人而异，它可能是存款、取款或者转账等，可以延迟到子类中实现。 这样的例子在生活中还有很多，例如，一个人每天会起床、吃饭、做事、睡觉等，其中“做事”的内容每天可能不同。我们把这些规定了流程或格式的实例定义成模板，允许使用者根据自己的需求去更新它，例如，简历模板、论文模板、Word 中模板文件等。 一，模式的定义与特点定义一个操作中的算法骨架，而将算法的一些步骤延迟到子类中，使得子类可以不改变该算法结构的情况下重定义该算法的某些特定步骤。它是一种类行为型模式。 该模式的主要优点如下。 它封装了不变部分，扩展可变部分。它把认为是不变部分的算法封装到父类中实现，而把可变部分算法由子类继承实现，便于子类继续扩展。 它在父类中提取了公共的部分代码，便于代码复用。 部分方法是由子类实现的，因此子类可以通过扩展方式增加相应的功能，符合开闭原则。 该模式的主要缺点如下。 对每个不同的实现都需要定义一个子类，这会导致类的个数增加，系统更加庞大，设计也更加抽象，间接地增加了系统实现的复杂度。 父类中的抽象方法由子类实现，子类执行的结果会影响父类的结果，这导致一种反向的控制结构，它提高了代码阅读的难度。 由于继承关系自身的缺点，如果父类添加新的抽象方法，则所有子类都要改一遍。 二，模式的结构与实现模板方法模式需要注意抽象类与具体子类之间的协作。它用到了虚函数的多态性技术以及“不用调用我，让我来调用你”的反向控制技术。现在来介绍它们的基本结构。 1.模式的结构模板方法模式包含以下主要角色： 1）抽象类/抽象模板抽象模板类，负责给出一个算法的轮廓和骨架。它由一个模板方法和若干个基本方法构成。这些方法的定义如下。 ① 模板方法：定义了算法的骨架，按某种顺序调用其包含的基本方法。 ② 基本方法：是整个算法中的一个步骤，包含以下几种类型。 抽象方法：在抽象类中声明，由具体子类实现。 具体方法：在抽象类中已经实现，在具体子类中可以继承或重写它。 钩子方法：在抽象类中已经实现，包括用于判断的逻辑方法和需要子类重写的空方法两种。 2）具体子类/具体实现具体实现类，实现抽象类中所定义的抽象方法和钩子方法，它们是一个顶级逻辑的一个组成步骤。 1234567891011121314151617181920212223242526272829303132333435363738public class TemplateMethodPattern &#123; public static void main(String[] args) &#123; AbstractClass tm = new ConcreteClass(); tm.TemplateMethod(); &#125;&#125;//抽象类abstract class AbstractClass &#123; //模板方法 public void TemplateMethod() &#123; SpecificMethod(); abstractMethod1(); abstractMethod2(); &#125; //具体方法 public void SpecificMethod() &#123; System.out.println(&quot;抽象类中的具体方法被调用...&quot;); &#125; //抽象方法1 public abstract void abstractMethod1(); //抽象方法2 public abstract void abstractMethod2();&#125;//具体子类class ConcreteClass extends AbstractClass &#123; public void abstractMethod1() &#123; System.out.println(&quot;抽象方法1的实现被调用...&quot;); &#125; public void abstractMethod2() &#123; System.out.println(&quot;抽象方法2的实现被调用...&quot;); &#125;&#125; 三，模式的应用实例泡一个妹子分为多个步骤 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public abstract class PaoMei &#123; protected void success(String name)&#123; getWeChat(); chat(); yuehui(); kfc(); &#125; protected void getWeChat()&#123; System.out.println(&quot;得到微信&quot;); &#125; protected void chat()&#123; System.out.println(&quot;聊天，约出来&quot;); &#125; protected void yuehui()&#123; &#125; protected void kfc()&#123; System.out.println(&quot;一起起床&quot;); &#125;&#125;public class Shl extends PaoMei&#123; @Override protected void yuehui() &#123; System.out.println(&quot;开保时捷带她兜风&quot;); &#125;&#125;public class LaTiao extends PaoMei&#123; @Override protected void yuehui() &#123; System.out.println(&quot;舔狗&quot;); &#125;&#125;public class MainTest &#123; @Test public void test()&#123; PaoMei shl= new Shl(); shl.success(&quot;lxm&quot;); PaoMei lt =new LaTiao(); lt.success(&quot;lxm&quot;); &#125;&#125; 四，模式的应用场景模板方法模式通常适用于以下场景。 算法的整体步骤很固定，但其中个别部分易变时，这时候可以使用模板方法模式，将容易变的部分抽象出来，供子类实现。 当多个子类存在公共的行为时，可以将其提取出来并集中到一个公共父类中以避免代码重复。首先，要识别现有代码中的不同之处，并且将不同之处分离为新的操作。最后，用一个调用这些新的操作的模板方法来替换这些不同的代码。 当需要控制子类的扩展时，模板方法只在特定点调用钩子操作，这样就只允许在这些点进行扩展。 五，模式的扩展在模板方法模式中，基本方法包含：抽象方法、具体方法和钩子方法，正确使用“钩子方法”可以使得子类控制父类的行为。如下面例子中，可以通过在具体子类中重写钩子方法 HookMethod1() 和 HookMethod2() 来改变抽象父类中的运行结果。 如果钩子方法 HookMethod1() 和钩子方法 HookMethod2() 的代码改变，则程序的运行结果也会改变。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"策略模式","slug":"设计模式/行为型模式之策略模式","date":"2022-01-11T11:59:27.773Z","updated":"2022-01-11T12:09:53.174Z","comments":true,"path":"2022/01/11/设计模式/行为型模式之策略模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%A1%8C%E4%B8%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E7%AD%96%E7%95%A5%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"在现实生活中常常遇到实现某种目标存在多种策略可供选择的情况，例如，出行旅游可以乘坐飞机、乘坐火车、骑自行车或自己开私家车等，超市促销可以釆用打折、送商品、送积分等方法。 在软件开发中也常常遇到类似的情况，当实现某一个功能存在多种算法或者策略，我们可以根据环境或者条件的不同选择不同的算法或者策略来完成该功能，如数据排序策略有冒泡排序、选择排序、插入排序、二叉树排序等。 如果使用多重条件转移语句实现（即硬编码），不但使条件语句变得很复杂，而且增加、删除或更换算法要修改原代码，不易维护，违背开闭原则。如果采用策略模式就能很好解决该问题。 一，策略模式的定义与特点该模式定义了一系列算法，并将每个算法封装起来，使它们可以相互替换，且算法的变化不会影响使用算法的客户。策略模式属于对象行为模式，它通过对算法进行封装，把使用算法的责任和算法的实现分割开来，并委派给不同的对象对这些算法进行管理。 策略模式的主要优点如下。 多重条件语句不易维护，而使用策略模式可以避免使用多重条件语句，如 if…else 语句、switch…case 语句。 策略模式提供了一系列的可供重用的算法族，恰当使用继承可以把算法族的公共代码转移到父类里面，从而避免重复的代码。 策略模式可以提供相同行为的不同实现，客户可以根据不同时间或空间要求选择不同的。 策略模式提供了对开闭原则的完美支持，可以在不修改原代码的情况下，灵活增加新算法。 策略模式把算法的使用放到环境类中，而算法的实现移到具体策略类中，实现了二者的分离。 其主要缺点如下。 客户端必须理解所有策略算法的区别，以便适时选择恰当的算法类。 策略模式造成很多的策略类，增加维护难度。 二，策略模式的结构与实现策略模式是准备一组算法，并将这组算法封装到一系列的策略类里面，作为一个抽象策略类的子类。策略模式的重心不是如何实现算法，而是如何组织这些算法，从而让程序结构更加灵活，具有更好的维护性和扩展性。 1.模式的结构策略模式的主要角色如下。 抽象策略（Strategy）类：定义了一个公共接口，各种不同的算法以不同的方式实现这个接口，环境角色使用这个接口调用不同的算法，一般使用接口或抽象类实现。 具体策略（Concrete Strategy）类：实现了抽象策略定义的接口，提供具体的算法实现。 环境（Context）类：持有一个策略类的引用，最终给客户端调用。 2.模式的实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class StrategyPattern &#123; public static void main(String[] args) &#123; Context c = new Context(); Strategy s = new ConcreteStrategyA(); c.setStrategy(s); c.strategyMethod(); System.out.println(&quot;-----------------&quot;); s = new ConcreteStrategyB(); c.setStrategy(s); c.strategyMethod(); &#125;&#125;//抽象策略类interface Strategy &#123; public void strategyMethod(); //策略方法&#125;//具体策略类Aclass ConcreteStrategyA implements Strategy &#123; public void strategyMethod() &#123; System.out.println(&quot;具体策略A的策略方法被访问！&quot;); &#125;&#125;//具体策略类Bclass ConcreteStrategyB implements Strategy &#123; public void strategyMethod() &#123; System.out.println(&quot;具体策略B的策略方法被访问！&quot;); &#125;&#125;//环境类class Context &#123; private Strategy strategy; public Strategy getStrategy() &#123; return strategy; &#125; public void setStrategy(Strategy strategy) &#123; this.strategy = strategy; &#125; public void strategyMethod() &#123; strategy.strategyMethod(); &#125;&#125; 三，策略模式的应用实例1234567891011121314151617181920212223242526272829public abstract class GoToSchool &#123; public abstract void gotoSchool();&#125;@Data@AllArgsConstructor@NoArgsConstructorpublic class XiaoMing &#123; private GoToSchool goToSchool; public void gotoSchool()&#123; goToSchool.gotoSchool(); &#125;&#125;public class Car extends GoToSchool&#123; @Override public void gotoSchool() &#123; System.out.println(&quot;有钱，自己开车去&quot;); &#125;&#125;public class Walk extends GoToSchool&#123; @Override public void gotoSchool() &#123; System.out.println(&quot;穷逼，走着走去&quot;); &#125;&#125; 四，策略模式的应用场景策略模式在很多地方用到，如 [Java] SE 中的容器布局管理就是一个典型的实例，Java SE 中的每个容器都存在多种布局供用户选择。在程序设计中，通常在以下几种情况中使用策略模式较多。 一个系统需要动态地在几种算法中选择一种时，可将每个算法封装到策略类中。 一个类定义了多种行为，并且这些行为在这个类的操作中以多个条件语句的形式出现，可将每个条件分支移入它们各自的策略类中以代替这些条件语句。 系统中各算法彼此完全独立，且要求对客户隐藏具体算法的实现细节时。 系统要求使用算法的客户不应该知道其操作的数据时，可使用策略模式来隐藏与算法相关的[数据结构]。 多个类只区别在表现行为不同，可以使用策略模式，在运行时动态选择具体要执行的行为。 五，策略模式的扩展在一个使用策略模式的系统中，当存在的策略很多时，客户端管理所有策略算法将变得很复杂，如果在环境类中使用策略工厂模式来管理这些策略类将大大减少客户端的工作复杂度。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"命令模式","slug":"设计模式/行为型模式之命令模式","date":"2022-01-11T11:59:16.871Z","updated":"2022-01-11T12:10:50.755Z","comments":true,"path":"2022/01/11/设计模式/行为型模式之命令模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%A1%8C%E4%B8%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%91%BD%E4%BB%A4%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"在软件开发系统中，“方法的请求者”与“方法的实现者”之间经常存在紧密的耦合关系，这不利于软件功能的扩展与维护。例如，想对方法进行“撤销、重做、记录”等处理都很不方便，因此“如何将方法的请求者与实现者解耦？”变得很重要，命令模式就能很好地解决这个问题。 在现实生活中，命令模式的例子也很多。比如看电视时，我们只需要轻轻一按遥控器就能完成频道的切换，这就是命令模式，将换台请求和换台处理完全解耦了。电视机遥控器（命令发送者）通过按钮（具体命令）来遥控电视机（命令接收者）。 再比如，我们去餐厅吃饭，菜单不是等到客人来了之后才定制的，而是已经预先配置好的。这样，客人来了就只需要点菜，而不是任由客人临时定制。餐厅提供的菜单就相当于把请求和处理进行了解耦，这就是命令模式的体现。 一，命令/委派模式的定义与特点将一个请求封装为一个对象，使发出请求的责任和执行请求的责任分割开。这样两者之间通过命令对象进行沟通，这样方便将命令对象进行储存、传递、调用、增加与管理。 命令模式的主要优点如下。 通过引入中间件（抽象接口）降低系统的耦合度。 扩展性良好，增加或删除命令非常方便。采用命令模式增加与删除命令不会影响其他类，且满足“开闭原则”。 可以实现宏命令。命令模式可以与[组合模式]结合，将多个命令装配成一个组合命令，即宏命令。 方便实现 Undo 和 Redo 操作。命令模式可以和[备忘录模式](结合，实现命令的撤销与恢复。 可以在现有命令的基础上，增加额外功能。比如日志记录，结合装饰器模式会更加灵活。 其缺点是： 可能产生大量具体的命令类。因为每一个具体操作都需要设计一个具体命令类，这会增加系统的复杂性。 命令模式的结果其实就是接收方的执行结果，但是为了以命令的形式进行架构、解耦请求与实现，引入了额外类型结构（引入了请求方与抽象命令接口），增加了理解上的困难。不过这也是[设计模式]的通病，抽象必然会额外增加类的数量，代码抽离肯定比代码聚合更加难理解。 二，命令模式的结构与实现可以将系统中的相关操作抽象成命令，使调用者与实现者相关分离，其结构如下。 1.模式的结构命令模式包含以下主要角色。 抽象命令类（Command）角色：声明执行命令的接口，拥有执行命令的抽象方法 execute()。 具体命令类（Concrete Command）角色：是抽象命令类的具体实现类，它拥有接收者对象，并通过调用接收者的功能来完成命令要执行的操作。 实现者/接收者（Receiver）角色：执行命令功能的相关操作，是具体命令对象业务的真正实现者。 调用者/请求者（Invoker）角色：是请求的发送者，它通常拥有很多的命令对象，并通过访问命令对象来执行相关请求，它不直接访问接收者。 2.模式的实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package command;public class CommandPattern &#123; public static void main(String[] args) &#123; Command cmd = new ConcreteCommand(); Invoker ir = new Invoker(cmd); System.out.println(&quot;客户访问调用者的call()方法...&quot;); ir.call(); &#125;&#125;//调用者class Invoker &#123; private Command command; public Invoker(Command command) &#123; this.command = command; &#125; public void setCommand(Command command) &#123; this.command = command; &#125; public void call() &#123; System.out.println(&quot;调用者执行命令command...&quot;); command.execute(); &#125;&#125;//抽象命令interface Command &#123; public abstract void execute();&#125;//具体命令class ConcreteCommand implements Command &#123; private Receiver receiver; ConcreteCommand() &#123; receiver = new Receiver(); &#125; public void execute() &#123; receiver.action(); &#125;&#125;//接收者class Receiver &#123; public void action() &#123; System.out.println(&quot;接收者的action()方法被调用...&quot;); &#125;&#125; 三，命令模式的应用实例历总与妹子 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public interface Option &#123; void todoOption();&#125;public class CryOption implements Option&#123; @Override public void todoOption() &#123; System.out.println(&quot;papapa&quot;); &#125;&#125;public class SimleOption implements Option&#123; @Override public void todoOption() &#123; System.out.println(&quot;hahha&quot;); &#125;&#125;public class MeiZi &#123; Option option; public MeiZi(Option option) &#123; this.option = option; &#125; public void doOption()&#123; option.todoOption(); &#125;&#125;public class LiZong &#123; MeiZi meiZi; public LiZong(MeiZi meiZi) &#123; this.meiZi = meiZi; &#125; public void happy()&#123; meiZi.doOption(); &#125;&#125;class MainTest&#123; public static void main(String[] args) &#123; Option option = new CryOption(); new LiZong(new MeiZi(option)).happy(); &#125;&#125; 四，命令模式的应用场景当系统的某项操作具备命令语义，且命令实现不稳定（变化）时，可以通过命令模式解耦请求与实现。使用抽象命令接口使请求方的代码架构稳定，封装接收方具体命令的实现细节。接收方与抽象命令呈现弱耦合（内部方法无需一致），具备良好的扩展性。 命令模式通常适用于以下场景。 请求调用者需要与请求接收者解耦时，命令模式可以使调用者和接收者不直接交互。 系统随机请求命令或经常增加、删除命令时，命令模式可以方便地实现这些功能。 当系统需要执行一组操作时，命令模式可以定义宏命令来实现该功能。 当系统需要支持命令的撤销（Undo）操作和恢复（Redo）操作时，可以将命令对象存储起来，采用备忘录模式来实现。 五，命令模式的扩展在软件开发中，有时将命令模式与前面学的组合模式联合使用，这就构成了宏命令模式，也叫组合命令模式。宏命令包含了一组命令，它充当了具体命令与调用者的双重角色，执行它时将递归调用它所包含的所有命令。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"责任链模式","slug":"设计模式/行为型模式之责任链模式","date":"2022-01-11T11:59:06.959Z","updated":"2022-01-11T12:11:11.154Z","comments":true,"path":"2022/01/11/设计模式/行为型模式之责任链模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%A1%8C%E4%B8%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E8%B4%A3%E4%BB%BB%E9%93%BE%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"在现实生活中，一个事件需要经过多个对象处理是很常见的场景。例如，采购审批流程、请假流程等。公司员工请假，可批假的领导有部门负责人、副总经理、总经理等，但每个领导能批准的天数不同，员工必须根据需要请假的天数去找不同的领导签名，也就是说员工必须记住每个领导的姓名、电话和地址等信息，这无疑增加了难度。 在计算机软硬件中也有相关例子，如总线网中数据报传送，每台计算机根据目标地址是否同自己的地址相同来决定是否接收；还有异常处理中，处理程序根据异常的类型决定自己是否处理该异常。 一，模式的定义与特点责任链（Chain of Responsibility）模式的定义：为了避免请求发送者与多个请求处理者耦合在一起，于是将所有请求的处理者通过前一对象记住其下一个对象的引用而连成一条链；当有请求发生时，可将请求沿着这条链传递，直到有对象处理它为止。 在责任链模式中，客户只需要将请求发送到责任链上即可，无须关心请求的处理细节和请求的传递过程，请求会自动进行传递。所以责任链将请求的发送者和请求的处理者解耦了。 责任链模式是一种对象行为型模式，其主要优点如下。 降低了对象之间的耦合度。该模式使得一个对象无须知道到底是哪一个对象处理其请求以及链的结构，发送者和接收者也无须拥有对方的明确信息。 增强了系统的可扩展性。可以根据需要增加新的请求处理类，满足开闭原则。 增强了给对象指派职责的灵活性。当工作流程发生变化，可以动态地改变链内的成员或者调动它们的次序，也可动态地新增或者删除责任。 责任链简化了对象之间的连接。每个对象只需保持一个指向其后继者的引用，不需保持其他所有处理者的引用，这避免了使用众多的 if 或者 if···else 语句。 责任分担。每个类只需要处理自己该处理的工作，不该处理的传递给下一个对象完成，明确各类的责任范围，符合类的单一职责原则。 其主要缺点如下。 不能保证每个请求一定被处理。由于一个请求没有明确的接收者，所以不能保证它一定会被处理，该请求可能一直传到链的末端都得不到处理。 对比较长的职责链，请求的处理可能涉及多个处理对象，系统性能将受到一定影响。 职责链建立的合理性要靠客户端来保证，增加了客户端的复杂性，可能会由于职责链的错误设置而导致系统出错，如可能会造成循环调用。 二，模式的结构与实现1.模式的结构职责链模式主要包含以下角色。 抽象处理者（Handler）角色：定义一个处理请求的接口，包含抽象处理方法和一个后继连接。 具体处理者（Concrete Handler）角色：实现抽象处理者的处理方法，判断能否处理本次请求，如果可以处理请求则处理，否则将该请求转给它的后继者。 客户类（Client）角色：创建处理链，并向链头的具体处理者对象提交请求，它不关心处理细节和请求的传递过程。 责任链模式的本质是解耦请求与处理，让请求在处理链中能进行传递与被处理；理解责任链模式应当理解其模式，而不是其具体实现。责任链模式的独到之处是将其节点处理者组合成了链式结构，并允许节点自身决定是否进行请求处理或转发，相当于让请求流动起来。 2.模式的实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public abstract class Handler &#123; private Handler next; public void setNext(Handler next) &#123; this.next = next; &#125; public Handler getNext() &#123; return next; &#125; public abstract void handleRequest(String request);&#125;class Handler1 extends Handler&#123; @Override public void handleRequest(String request) &#123; if (request.equals(&quot;one&quot;))&#123; System.out.println(&quot;handler1&quot;); &#125; if (getNext()!=null)&#123; getNext().handleRequest(request); &#125; &#125;&#125;class Handler2 extends Handler&#123; @Override public void handleRequest(String request) &#123; if (request.equals(&quot;one&quot;))&#123; System.out.println(&quot;handler2&quot;); &#125; if (getNext()!=null)&#123; getNext().handleRequest(request); &#125; &#125;&#125;class MainTest&#123; public static void main(String[] args) &#123; Handler h1 = new Handler1(); Handler h2 = new Handler2(); h1.setNext(h2); h1.handleRequest(&quot;one&quot;); &#125;&#125; 在上面代码中，我们把消息硬编码为 String 类型，而在真实业务中，消息是具备多样性的，可以是 int、String 或者自定义类型。因此，在上面代码的基础上，可以对消息类型进行抽象 Request，增强了消息的兼容性。 三，模式的应用实例用责任链设计一个请假条审批模块 分析：假如规定学生请假小于或等于 2 天，班主任可以批准；小于或等于 7 天，系主任可以批准；小于或等于 10 天，院长可以批准；其他情况不予批准；这个实例适合使用职责链模式实现。 首先，定义一个领导类（Leader），它是抽象处理者，包含了一个指向下一位领导的指针 next 和一个处理假条的抽象处理方法 handleRequest(int LeaveDays)；然后，定义班主任类（ClassAdviser）、系主任类（DepartmentHead）和院长类（Dean），它们是抽象处理者的子类，是具体处理者，必须根据自己的权力去实现父类的 handleRequest(int LeaveDays) 方法，如果无权处理就将假条交给下一位具体处理者，直到最后；客户类负责创建处理链，并将假条交给链头的具体处理者（班主任）。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101public abstract class Leader &#123; private Leader next; //0 未处理 1 通过 2 驳回 public int status; public void setNext(Leader next) &#123; this.next = next; &#125; public Leader getNext() &#123; return next; &#125; public abstract Boolean doRequest(int day);&#125;class Teacher extends Leader &#123; @Override public Boolean doRequest(int day) &#123; if (day &lt;= 2) &#123; status = 1; System.out.println(&quot;老师处理，通过&quot;); return true; &#125; else &#123; if (getNext() != null) &#123; System.out.println(&quot;老师流转，通过&quot;); return getNext().doRequest(day); &#125; &#125; return false; &#125;&#125;class XiZhuRen extends Leader &#123; @Override public Boolean doRequest(int day) &#123; if (day &lt;= 7) &#123; status = 1; System.out.println(&quot;系主任处理，通过&quot;); return true; &#125; else &#123; if (getNext() != null) &#123; System.out.println(&quot;系主任流转，通过&quot;); return getNext().doRequest(day); &#125; &#125; return false; &#125;&#125;class YuanZhang extends Leader &#123; @Override public Boolean doRequest(int day) &#123; if (day &lt;= 10) &#123; status = 1; System.out.println(&quot;院长处理，通过&quot;); return true; &#125; else &#123; if (getNext() != null) &#123; System.out.println(&quot;院长流转，通过&quot;); return getNext().doRequest(day); &#125; &#125; status = 2; System.out.println(&quot;院长处理，NO通过&quot;); return false; &#125;&#125;class Student &#123; Leader leader; public Student(Leader leader) &#123; this.leader = leader; &#125; public Boolean doRequest(int day) &#123; return leader.doRequest(day); &#125;&#125;class MainTest &#123; public static void main(String[] args) &#123; Leader tea = new Teacher(); Leader xi = new XiZhuRen(); Leader yuan = new YuanZhang(); tea.setNext(xi); xi.setNext(yuan); Student stu = new Student(tea); stu.doRequest(11); &#125;&#125; 四，模式的应用场景 多个对象可以处理一个请求，但具体由哪个对象处理该请求在运行时自动确定。 可动态指定一组对象处理请求，或添加新的处理者。 需要在不明确指定请求处理者的情况下，向多个处理者中的一个提交请求。 五，模式的扩展职责链模式存在以下两种情况。 纯的职责链模式：一个请求必须被某一个处理者对象所接收，且一个具体处理者对某个请求的处理只能采用以下两种行为之一：自己处理（承担责任）；把责任推给下家处理。 不纯的职责链模式：允许出现某一个具体处理者对象在承担了请求的一部分责任后又将剩余的责任传给下家的情况，且一个请求可以最终不被任何接收端对象所接收。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"状态模式","slug":"设计模式/行为型模式之状态模式","date":"2022-01-11T11:58:57.230Z","updated":"2022-01-11T12:11:33.423Z","comments":true,"path":"2022/01/11/设计模式/行为型模式之状态模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%A1%8C%E4%B8%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E7%8A%B6%E6%80%81%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"在软件开发过程中，应用程序中的部分对象可能会根据不同的情况做出不同的行为，我们把这种对象称为有状态的对象，而把影响对象行为的一个或多个动态变化的属性称为状态。当有状态的对象与外部事件产生互动时，其内部状态就会发生改变，从而使其行为也发生改变。如人都有高兴和伤心的时候，不同的情绪有不同的行为，当然外界也会影响其情绪变化。 对这种有状态的对象编程，传统的解决方案是：将这些所有可能发生的情况全都考虑到，然后使用 if-else 或 switch-case 语句来做状态判断，再进行不同情况的处理。但是显然这种做法对复杂的状态判断存在天然弊端，条件判断语句会过于臃肿，可读性差，且不具备扩展性，维护难度也大。且增加新的状态时要添加新的 if-else 语句，这违背了“开闭原则”，不利于程序的扩展。 以上问题如果采用“状态模式”就能很好地得到解决。状态模式的解决思想是：当控制一个对象状态转换的条件表达式过于复杂时，把相关“判断逻辑”提取出来，用各个不同的类进行表示，系统处于哪种情况，直接使用相应的状态类对象进行处理，这样能把原来复杂的逻辑判断简单化，消除了 if-else、switch-case 等冗余语句，代码更有层次性，并且具备良好的扩展力。 一，状态模式的定义与特点状态（State）模式的定义：对有状态的对象，把复杂的“判断逻辑”提取到不同的状态对象中，允许状态对象在其内部状态发生改变时改变其行为。 状态模式是一种对象行为型模式，其主要优点如下。 结构清晰，状态模式将与特定状态相关的行为局部化到一个状态中，并且将不同状态的行为分割开来，满足“单一职责原则”。 将状态转换显示化，减少对象间的相互依赖。将不同的状态引入独立的对象中会使得状态转换变得更加明确，且减少对象间的相互依赖。 状态类职责明确，有利于程序的扩展。通过定义新的子类很容易地增加新的状态和转换。 状态模式的主要缺点如下。 状态模式的使用必然会增加系统的类与对象的个数。 状态模式的结构与实现都较为复杂，如果使用不当会导致程序结构和代码的混乱。 状态模式对开闭原则的支持并不太好，对于可以切换状态的状态模式，增加新的状态类需要修改那些负责状态转换的源码，否则无法切换到新增状态，而且修改某个状态类的行为也需要修改对应类的源码。 二，状态模式的结构与实现状态模式把受环境改变的对象行为包装在不同的状态对象里，其意图是让一个对象在其内部状态改变的时候，其行为也随之改变。 1.模式的结构状态模式包含以下主要角色。 环境类（Context）角色：也称为上下文，它定义了客户端需要的接口，内部维护一个当前状态，并负责具体状态的切换。 抽象状态（State）角色：定义一个接口，用以封装环境对象中的特定状态所对应的行为，可以有一个或多个行为。 具体状态（Concrete State）角色：实现抽象状态所对应的行为，并且在需要的情况下进行状态切换。 2.模式的实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public class StatePatternClient &#123; public static void main(String[] args) &#123; Context context = new Context(); //创建环境 context.Handle(); //处理请求 context.Handle(); context.Handle(); context.Handle(); &#125;&#125;//环境类class Context &#123; private State state; //定义环境类的初始状态 public Context() &#123; this.state = new ConcreteStateA(); &#125; //设置新状态 public void setState(State state) &#123; this.state = state; &#125; //读取状态 public State getState() &#123; return (state); &#125; //对请求做处理 public void Handle() &#123; state.Handle(this); &#125;&#125;//抽象状态类abstract class State &#123; public abstract void Handle(Context context);&#125;//具体状态A类class ConcreteStateA extends State &#123; public void Handle(Context context) &#123; System.out.println(&quot;当前状态是 A.&quot;); context.setState(new ConcreteStateB()); &#125;&#125;//具体状态B类class ConcreteStateB extends State &#123; public void Handle(Context context) &#123; System.out.println(&quot;当前状态是 B.&quot;); context.setState(new ConcreteStateA()); &#125;&#125; 三，状态模式的应用实例由状态决定yh结果 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273public abstract class State &#123; public void happy() &#123; &#125; public State next() &#123; throw new RuntimeException(); &#125;&#125;public class Bad extends State&#123; @Override public void happy() &#123; System.out.println(&quot;状态不好 回家睡觉&quot;); &#125; @Override public State next() &#123; return new Good(); &#125;&#125;public class Good extends State&#123; @Override public void happy() &#123; System.out.println(&quot;状态很好 一起起床&quot;); &#125; @Override public State next() &#123; return new Bad(); &#125;&#125;@Data@NoArgsConstructor@AllArgsConstructor@Accessors(chain = true)public class YueHui &#123; private State state; public void happy()&#123; state.happy(); &#125; public void next()&#123; this.state=state.next(); &#125;&#125;/** * @author yhd * @email yinhuidong1@xiaomi.com * @description 跟妹子出去约会 * 状态很好 一起起床 * 状态很差 回家睡觉 * @since 2021/5/22 上午12:52 */public class MainTest &#123; @Test public void test() &#123; YueHui event = new YueHui(new Good()); event.happy(); event.next(); event.happy(); &#125;&#125; 设计一个多线程的状态转换程序 分析：多线程存在 5 种状态，分别为新建状态、就绪状态、运行状态、阻塞状态和死亡状态，各个状态当遇到相关方法调用或事件触发时会转换到其他状态。 现在先定义一个抽象状态类（TheadState），然后为每个状态设计一个具体状态类，它们是新建状态（New）、就绪状态（Runnable ）、运行状态（Running）、阻塞状态（Blocked）和死亡状态（Dead），每个状态中有触发它们转变状态的方法，环境类（ThreadContext）中先生成一个初始状态（New），并提供相关触发方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138public class ScoreStateTest &#123; public static void main(String[] args) &#123; ThreadContext context = new ThreadContext(); context.start(); context.getCPU(); context.suspend(); context.resume(); context.getCPU(); context.stop(); &#125;&#125;//环境类class ThreadContext &#123; private ThreadState state; ThreadContext() &#123; state = new New(); &#125; public void setState(ThreadState state) &#123; this.state = state; &#125; public ThreadState getState() &#123; return state; &#125; public void start() &#123; ((New) state).start(this); &#125; public void getCPU() &#123; ((Runnable) state).getCPU(this); &#125; public void suspend() &#123; ((Running) state).suspend(this); &#125; public void stop() &#123; ((Running) state).stop(this); &#125; public void resume() &#123; ((Blocked) state).resume(this); &#125;&#125;//抽象状态类：线程状态abstract class ThreadState &#123; protected String stateName; //状态名&#125;//具体状态类：新建状态class New extends ThreadState &#123; public New() &#123; stateName = &quot;新建状态&quot;; System.out.println(&quot;当前线程处于：新建状态.&quot;); &#125; public void start(ThreadContext hj) &#123; System.out.print(&quot;调用start()方法--&gt;&quot;); if (stateName.equals(&quot;新建状态&quot;)) &#123; hj.setState(new Runnable()); &#125; else &#123; System.out.println(&quot;当前线程不是新建状态，不能调用start()方法.&quot;); &#125; &#125;&#125;//具体状态类：就绪状态class Runnable extends ThreadState &#123; public Runnable() &#123; stateName = &quot;就绪状态&quot;; System.out.println(&quot;当前线程处于：就绪状态.&quot;); &#125; public void getCPU(ThreadContext hj) &#123; System.out.print(&quot;获得CPU时间--&gt;&quot;); if (stateName.equals(&quot;就绪状态&quot;)) &#123; hj.setState(new Running()); &#125; else &#123; System.out.println(&quot;当前线程不是就绪状态，不能获取CPU.&quot;); &#125; &#125;&#125;//具体状态类：运行状态class Running extends ThreadState &#123; public Running() &#123; stateName = &quot;运行状态&quot;; System.out.println(&quot;当前线程处于：运行状态.&quot;); &#125; public void suspend(ThreadContext hj) &#123; System.out.print(&quot;调用suspend()方法--&gt;&quot;); if (stateName.equals(&quot;运行状态&quot;)) &#123; hj.setState(new Blocked()); &#125; else &#123; System.out.println(&quot;当前线程不是运行状态，不能调用suspend()方法.&quot;); &#125; &#125; public void stop(ThreadContext hj) &#123; System.out.print(&quot;调用stop()方法--&gt;&quot;); if (stateName.equals(&quot;运行状态&quot;)) &#123; hj.setState(new Dead()); &#125; else &#123; System.out.println(&quot;当前线程不是运行状态，不能调用stop()方法.&quot;); &#125; &#125;&#125;//具体状态类：阻塞状态class Blocked extends ThreadState &#123; public Blocked() &#123; stateName = &quot;阻塞状态&quot;; System.out.println(&quot;当前线程处于：阻塞状态.&quot;); &#125; public void resume(ThreadContext hj) &#123; System.out.print(&quot;调用resume()方法--&gt;&quot;); if (stateName.equals(&quot;阻塞状态&quot;)) &#123; hj.setState(new Runnable()); &#125; else &#123; System.out.println(&quot;当前线程不是阻塞状态，不能调用resume()方法.&quot;); &#125; &#125;&#125;//具体状态类：死亡状态class Dead extends ThreadState &#123; public Dead() &#123; stateName = &quot;死亡状态&quot;; System.out.println(&quot;当前线程处于：死亡状态.&quot;); &#125;&#125; 四，状态模式的应用场景通常在以下情况下可以考虑使用状态模式。 当一个对象的行为取决于它的状态，并且它必须在运行时根据状态改变它的行为时，就可以考虑使用状态模式。 一个操作中含有庞大的分支结构，并且这些分支决定于对象的状态时。 五，状态模式的扩展在有些情况下，可能有多个环境对象需要共享一组状态，这时需要引入享元模式，将这些具体状态对象放在集合中供程序共享。 分析：共享状态模式的不同之处是在环境类中增加了一个 HashMap 来保存相关状态，当需要某种状态时可以从中获取，其程序代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364package state;import java.util.HashMap;public class FlyweightStatePattern &#123; public static void main(String[] args) &#123; ShareContext context = new ShareContext(); //创建环境 context.Handle(); //处理请求 context.Handle(); context.Handle(); context.Handle(); &#125;&#125;//环境类class ShareContext &#123; private ShareState state; private HashMap&lt;String, ShareState&gt; stateSet = new HashMap&lt;String, ShareState&gt;(); public ShareContext() &#123; state = new ConcreteState1(); stateSet.put(&quot;1&quot;, state); state = new ConcreteState2(); stateSet.put(&quot;2&quot;, state); state = getState(&quot;1&quot;); &#125; //设置新状态 public void setState(ShareState state) &#123; this.state = state; &#125; //读取状态 public ShareState getState(String key) &#123; ShareState s = (ShareState) stateSet.get(key); return s; &#125; //对请求做处理 public void Handle() &#123; state.Handle(this); &#125;&#125;//抽象状态类abstract class ShareState &#123; public abstract void Handle(ShareContext context);&#125;//具体状态1类class ConcreteState1 extends ShareState &#123; public void Handle(ShareContext context) &#123; System.out.println(&quot;当前状态是： 状态1&quot;); context.setState(context.getState(&quot;2&quot;)); &#125;&#125;//具体状态2类class ConcreteState2 extends ShareState &#123; public void Handle(ShareContext context) &#123; System.out.println(&quot;当前状态是： 状态2&quot;); context.setState(context.getState(&quot;1&quot;)); &#125;&#125; 六，扩展状态模式与责任链模式的区别 状态模式和责任链模式都能消除 if-else 分支过多的问题。但在某些情况下，状态模式中的状态可以理解为责任，那么在这种情况下，两种模式都可以使用。 从定义来看，状态模式强调的是一个对象内在状态的改变，而责任链模式强调的是外部节点对象间的改变。 从代码实现上来看，两者最大的区别就是状态模式的各个状态对象知道自己要进入的下一个状态对象，而责任链模式并不清楚其下一个节点处理对象，因为链式组装由客户端负责。 状态模式与策略模式的区别 状态模式和策略模式的 UML 类图架构几乎完全一样，但两者的应用场景是不一样的。策略模式的多种算法行为择其一都能满足，彼此之间是独立的，用户可自行更换策略算法，而状态模式的各个状态间存在相互关系，彼此之间在一定条件下存在自动切换状态的效果，并且用户无法指定状态，只能设置初始状态。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"观察者模式","slug":"设计模式/行为型模式之观察者模式","date":"2022-01-11T11:58:43.800Z","updated":"2022-01-11T12:10:31.385Z","comments":true,"path":"2022/01/11/设计模式/行为型模式之观察者模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%A1%8C%E4%B8%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E8%A7%82%E5%AF%9F%E8%80%85%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"在现实世界中，许多对象并不是独立存在的，其中一个对象的行为发生改变可能会导致一个或者多个其他对象的行为也发生改变。例如，某种商品的物价上涨时会导致部分商家高兴，而消费者伤心；还有，当我们开车到交叉路口时，遇到红灯会停，遇到绿灯会行。这样的例子还有很多，例如，股票价格与股民、微信公众号与微信用户、气象局的天气预报与听众、小偷与警察等。 在软件世界也是这样，例如，Excel 中的数据与折线图、饼状图、柱状图之间的关系；MVC 模式中的模型与视图的关系；事件模型中的事件源与事件处理者。所有这些，如果用观察者模式来实现就非常方便。 一，模式的定义与特点指多个对象间存在一对多的依赖关系，当一个对象的状态发生改变时，所有依赖于它的对象都得到通知并被自动更新。这种模式有时又称作发布-订阅模式、模型-视图模式，它是对象行为型模式。 观察者模式是一种对象行为型模式，其主要优点如下。 降低了目标与观察者之间的耦合关系，两者之间是抽象耦合关系。符合依赖倒置原则。 目标与观察者之间建立了一套触发机制。 它的主要缺点如下。 目标与观察者之间的依赖关系并没有完全解除，而且有可能出现循环引用。 当观察者对象很多时，通知的发布会花费很多时间，影响程序的效率。 二，模式的结构与实现实现观察者模式时要注意具体目标对象和具体观察者对象之间不能直接调用，否则将使两者之间紧密耦合起来，这违反了面向对象的设计原则。 1.模式的结构观察者模式的主要角色如下。 抽象主题（Subject）角色：也叫抽象目标类，它提供了一个用于保存观察者对象的聚集类和增加、删除观察者对象的方法，以及通知所有观察者的抽象方法。 具体主题（Concrete Subject）角色：也叫具体目标类，它实现抽象目标中的通知方法，当具体主题的内部状态发生改变时，通知所有注册过的观察者对象。 抽象观察者（Observer）角色：它是一个抽象类或接口，它包含了一个更新自己的抽象方法，当接到具体主题的更改通知时被调用。 具体观察者（Concrete Observer）角色：实现抽象观察者中定义的抽象方法，以便在得到目标的更改通知时更新自身的状态。 2.模式的实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public abstract class Subject &#123; protected List&lt;Observer&gt; observers = new ArrayList(); public void add(Observer obs) &#123; observers.add(obs); &#125; public void remove(Observer obs) &#123; observers.remove(obs); &#125; public abstract void notifyObserver(); //通知观察者方法&#125;//抽象观察者interface Observer &#123; void response(); //反应&#125;//具体目标class ConcreteSubject extends Subject &#123; @Override public void notifyObserver() &#123; observers.forEach(Observer::response); &#125;&#125;//具体观察者1class ConcreteObserver1 implements Observer &#123; @Override public void response() &#123; System.out.println(&quot;具体观察者1作出反应！&quot;); &#125;&#125;//具体观察者1class ConcreteObserver2 implements Observer &#123; @Override public void response() &#123; System.out.println(&quot;具体观察者2作出反应！&quot;); &#125;&#125;class MainTest &#123; public static void main(String[] args) &#123; Subject subject = new ConcreteSubject(); subject.add(new ConcreteObserver1()); subject.add(new ConcreteObserver2()); subject.notifyObserver(); &#125;&#125; 三，模式的应用实例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455@Data@AllArgsConstructor@NoArgsConstructorpublic class Mm &#123; private List&lt;TianGou&gt; tianGous; public void sendEvent()&#123; for (TianGou gou : tianGous) &#123; gou.acceptEvent(&quot;今天休息，不出去了&quot;); &#125; &#125;&#125;public abstract class TianGou &#123; abstract void acceptEvent(String event);&#125;@Data@NoArgsConstructorpublic class TianGouOne extends TianGou&#123; @Override public void acceptEvent(String event) &#123; System.out.println(&quot;TianGouOne ====&quot;+event); &#125;&#125;@Data@NoArgsConstructorpublic class TianGouTwo extends TianGou&#123; @Override public void acceptEvent(String event) &#123; System.out.println(&quot;TianGouTwo ====&quot;+event); &#125;&#125;public class MainTest &#123; @Test public void test() &#123; TianGou one = new TianGouOne(); TianGou two = new TianGouTwo(); List&lt;TianGou&gt; tianGous = new ArrayList&lt;&gt;(); tianGous.add(one); tianGous.add(two); Mm mm = new Mm(tianGous); mm.sendEvent(); &#125;&#125; 四，模式的应用场景在软件系统中，当系统一方行为依赖另一方行为的变动时，可使用观察者模式松耦合联动双方，使得一方的变动可以通知到感兴趣的另一方对象，从而让另一方对象对此做出响应。 通过前面的分析与应用实例可知观察者模式适合以下几种情形。 对象间存在一对多关系，一个对象的状态发生改变会影响其他对象。 当一个抽象模型有两个方面，其中一个方面依赖于另一方面时，可将这二者封装在独立的对象中以使它们可以各自独立地改变和复用。 实现类似广播机制的功能，不需要知道具体收听者，只需分发广播，系统中感兴趣的对象会自动接收该广播。 多层级嵌套使用，形成一种链式触发机制，使得事件具备跨域（跨越两种观察者类型）通知。 五，模式的扩展在 [Java] 中，通过 java.util.Observable 类和 java.util.Observer 接口定义了观察者模式，只要实现它们的子类就可以编写观察者模式实例。 1. Observable类Observable 类是抽象目标类，它有一个 Vector 向量，用于保存所有要通知的观察者对象，下面来介绍它最重要的 3 个方法。 void addObserver(Observer o) 方法：用于将新的观察者对象添加到向量中。 void notifyObservers(Object arg) 方法：调用向量中的所有观察者对象的 update() 方法，通知它们数据发生改变。通常越晚加入向量的观察者越先得到通知。 void setChange() 方法：用来设置一个 boolean 类型的内部标志位，注明目标对象发生了变化。当它为真时，notifyObservers() 才会通知观察者。 2. Observer 接口Observer 接口是抽象观察者，它监视目标对象的变化，当目标对象发生变化时，观察者得到通知，并调用 void update(Observable o,Object arg) 方法，进行相应的工作。 12345678910111213141516171819202122232425262728293031public class PublishAdapter extends Observable &#123; public void publishMessage(String message) &#123; setChanged(); notifyObservers(message); &#125;&#125;@Datapublic class ListenerAdapter implements Observer &#123; @Override public void update(Observable o, Object arg) &#123; PublishAdapter publishAdapter= (PublishAdapter) o; String message = (String) arg; System.out.println(&quot;o = &quot; + o); System.out.println(&quot;arg = &quot; + arg); &#125;&#125;public static void main(String[] args) &#123; PublishAdapter publish = new PublishAdapter(); ListenerAdapter listenerAdapter = new ListenerAdapter(); publish.addObserver(listenerAdapter); publish.publishMessage(&quot;HAHAH&quot;);&#125; 3.基于 Guava API 轻松落地观察者模式12345&lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;29.0-jre&lt;/version&gt;&lt;/dependency&gt; 1234567891011121314151617public class GuavaEvent &#123; @Subscribe public void subscribe(String str) &#123; //业务逻辑 System.out.println(&quot;执行 subscribe 方法,传入的参数是:&quot; + str); &#125;&#125;public class GuavaEventTest &#123; public static void main(String[] args) &#123; EventBus eventbus = new EventBus(); GuavaEvent guavaEvent = new GuavaEvent(); eventbus.register(guavaEvent); eventbus.post(&quot;Tom&quot;); &#125;&#125;","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"中介者模式","slug":"设计模式/行为型模式之中介者模式","date":"2022-01-11T11:58:32.749Z","updated":"2022-01-11T12:11:22.263Z","comments":true,"path":"2022/01/11/设计模式/行为型模式之中介者模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%A1%8C%E4%B8%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E4%B8%AD%E4%BB%8B%E8%80%85%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"在现实生活中，常常会出现好多对象之间存在复杂的交互关系，这种交互关系常常是“网状结构”，它要求每个对象都必须知道它需要交互的对象。例如，每个人必须记住他（她）所有朋友的电话；而且，朋友中如果有人的电话修改了，他（她）必须让其他所有的朋友一起修改，这叫作“牵一发而动全身”，非常复杂。 如果把这种“网状结构”改为“星形结构”的话，将大大降低它们之间的“耦合性”，这时只要找一个“中介者”就可以了。如前面所说的“每个人必须记住所有朋友电话”的问题，只要在网上建立一个每个朋友都可以访问的“通信录”就解决了。这样的例子还有很多，例如，你刚刚参加工作想租房，可以找“房屋中介”；或者，自己刚刚到一个陌生城市找工作，可以找“人才交流中心”帮忙。 在软件的开发过程中，这样的例子也很多，例如，在 MVC 框架中，控制器（C）就是模型（M）和视图（V）的中介者；还有大家常用的 QQ 聊天程序的“中介者”是 QQ 服务器。所有这些，都可以采用“中介者模式”来实现，它将大大降低对象之间的耦合性，提高系统的灵活性。 一，模式的定义与特点中介者（Mediator）模式的定义：定义一个中介对象来封装一系列对象之间的交互，使原有对象之间的耦合松散，且可以独立地改变它们之间的交互。中介者模式又叫调停模式，它是迪米特法则的典型应用。 中介者模式是一种对象行为型模式，其主要优点如下。 类之间各司其职，符合迪米特法则。 降低了对象之间的耦合性，使得对象易于独立地被复用。 将对象间的一对多关联转变为一对一的关联，提高系统的灵活性，使得系统易于维护和扩展。 其主要缺点是：中介者模式将原本多个对象直接的相互依赖变成了中介者和多个同事类的依赖关系。当同事类越多时，中介者就会越臃肿，变得复杂且难以维护。 二，模式的结构与实现中介者模式实现的关键是找出“中介者” 1.模式的结构中介者模式包含以下主要角色。 抽象中介者（Mediator）角色：它是中介者的接口，提供了同事对象注册与转发同事对象信息的抽象方法。 具体中介者（Concrete Mediator）角色：实现中介者接口，定义一个 List 来管理同事对象，协调各个同事角色之间的交互关系，因此它依赖于同事角色。 抽象同事类（Colleague）角色：定义同事类的接口，保存中介者对象，提供同事对象交互的抽象方法，实现所有相互影响的同事类的公共功能。 具体同事类（Concrete Colleague）角色：是抽象同事类的实现者，当需要与其他同事对象交互时，由中介者对象负责后续的交互。 2.模式的实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677public class MediatorPattern &#123; public static void main(String[] args) &#123; Mediator md = new ConcreteMediator(); Colleague c1, c2; c1 = new ConcreteColleague1(); c2 = new ConcreteColleague2(); md.register(c1); md.register(c2); c1.send(); System.out.println(&quot;-------------&quot;); c2.send(); &#125;&#125;//抽象中介者abstract class Mediator &#123; public abstract void register(Colleague colleague); public abstract void relay(Colleague cl); //转发&#125;//具体中介者class ConcreteMediator extends Mediator &#123; private List&lt;Colleague&gt; colleagues = new ArrayList&lt;Colleague&gt;(); public void register(Colleague colleague) &#123; if (!colleagues.contains(colleague)) &#123; colleagues.add(colleague); colleague.setMedium(this); &#125; &#125; public void relay(Colleague cl) &#123; for (Colleague ob : colleagues) &#123; if (!ob.equals(cl)) &#123; ((Colleague) ob).receive(); &#125; &#125; &#125;&#125;//抽象同事类abstract class Colleague &#123; protected Mediator mediator; public void setMedium(Mediator mediator) &#123; this.mediator = mediator; &#125; public abstract void receive(); public abstract void send();&#125;//具体同事类class ConcreteColleague1 extends Colleague &#123; public void receive() &#123; System.out.println(&quot;具体同事类1收到请求。&quot;); &#125; public void send() &#123; System.out.println(&quot;具体同事类1发出请求。&quot;); mediator.relay(this); //请中介者转发 &#125;&#125;//具体同事类class ConcreteColleague2 extends Colleague &#123; public void receive() &#123; System.out.println(&quot;具体同事类2收到请求。&quot;); &#125; public void send() &#123; System.out.println(&quot;具体同事类2发出请求。&quot;); mediator.relay(this); //请中介者转发 &#125;&#125; 三，模式的应用实例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126import javax.swing.*;import java.awt.*;import java.awt.event.ActionEvent;import java.awt.event.ActionListener;import java.util.ArrayList;import java.util.List;public class DatingPlatform &#123; public static void main(String[] args) &#123; Medium md = new EstateMedium(); //房产中介 Customer member1, member2; member1 = new Seller(&quot;张三(卖方)&quot;); member2 = new Buyer(&quot;李四(买方)&quot;); md.register(member1); //客户注册 md.register(member2); &#125;&#125;//抽象中介者：中介公司interface Medium &#123; void register(Customer member); //客户注册 void relay(String from, String ad); //转发&#125;//具体中介者：房地产中介class EstateMedium implements Medium &#123; private List&lt;Customer&gt; members = new ArrayList&lt;Customer&gt;(); public void register(Customer member) &#123; if (!members.contains(member)) &#123; members.add(member); member.setMedium(this); &#125; &#125; public void relay(String from, String ad) &#123; for (Customer ob : members) &#123; String name = ob.getName(); if (!name.equals(from)) &#123; ((Customer) ob).receive(from, ad); &#125; &#125; &#125;&#125;//抽象同事类：客户abstract class Customer extends JFrame implements ActionListener &#123; private static final long serialVersionUID = -7219939540794786080L; protected Medium medium; protected String name; JTextField SentText; JTextArea ReceiveArea; public Customer(String name) &#123; super(name); this.name = name; &#125; void ClientWindow(int x, int y) &#123; Container cp; JScrollPane sp; JPanel p1, p2; cp = this.getContentPane(); SentText = new JTextField(18); ReceiveArea = new JTextArea(10, 18); ReceiveArea.setEditable(false); p1 = new JPanel(); p1.setBorder(BorderFactory.createTitledBorder(&quot;接收内容：&quot;)); p1.add(ReceiveArea); sp = new JScrollPane(p1); cp.add(sp, BorderLayout.NORTH); p2 = new JPanel(); p2.setBorder(BorderFactory.createTitledBorder(&quot;发送内容：&quot;)); p2.add(SentText); cp.add(p2, BorderLayout.SOUTH); SentText.addActionListener(this); this.setLocation(x, y); this.setSize(250, 330); this.setResizable(false); //窗口大小不可调整 this.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE); this.setVisible(true); &#125; public void actionPerformed(ActionEvent e) &#123; String tempInfo = SentText.getText().trim(); SentText.setText(&quot;&quot;); this.send(tempInfo); &#125; public String getName() &#123; return name; &#125; public void setMedium(Medium medium) &#123; this.medium = medium; &#125; public abstract void send(String ad); public abstract void receive(String from, String ad);&#125;//具体同事类：卖方class Seller extends Customer &#123; private static final long serialVersionUID = -1443076716629516027L; public Seller(String name) &#123; super(name); ClientWindow(50, 100); &#125; public void send(String ad) &#123; ReceiveArea.append(&quot;我(卖方)说: &quot; + ad + &quot;\\n&quot;); //使滚动条滚动到最底端 ReceiveArea.setCaretPosition(ReceiveArea.getText().length()); medium.relay(name, ad); &#125; public void receive(String from, String ad) &#123; ReceiveArea.append(from + &quot;说: &quot; + ad + &quot;\\n&quot;); //使滚动条滚动到最底端 ReceiveArea.setCaretPosition(ReceiveArea.getText().length()); &#125;&#125;//具体同事类：买方class Buyer extends Customer &#123; private static final long serialVersionUID = -474879276076308825L; public Buyer(String name) &#123; super(name); ClientWindow(350, 100); &#125; public void send(String ad) &#123; ReceiveArea.append(&quot;我(买方)说: &quot; + ad + &quot;\\n&quot;); //使滚动条滚动到最底端 ReceiveArea.setCaretPosition(ReceiveArea.getText().length()); medium.relay(name, ad); &#125; public void receive(String from, String ad) &#123; ReceiveArea.append(from + &quot;说: &quot; + ad + &quot;\\n&quot;); //使滚动条滚动到最底端 ReceiveArea.setCaretPosition(ReceiveArea.getText().length()); &#125;&#125; 四，模式的应用场景 当对象之间存在复杂的网状结构关系而导致依赖关系混乱且难以复用时。 当想创建一个运行于多个类之间的对象，又不想生成新的子类时。 五，模式的扩展在实际开发中，通常采用以下两种方法来简化中介者模式，使开发变得更简单。 不定义中介者接口，把具体中介者对象实现成为单例。 同时对象不持有中介者，而是在需要的时候直接获取中介者对象并调用。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081public class SimpleMediatorPattern &#123; public static void main(String[] args) &#123; SimpleColleague c1, c2; c1 = new SimpleConcreteColleague1(); c2 = new SimpleConcreteColleague2(); c1.send(); System.out.println(&quot;-----------------&quot;); c2.send(); &#125;&#125;//简单单例中介者class SimpleMediator &#123; private static SimpleMediator smd = new SimpleMediator(); private List&lt;SimpleColleague&gt; colleagues = new ArrayList&lt;SimpleColleague&gt;(); private SimpleMediator() &#123; &#125; public static SimpleMediator getMedium() &#123; return (smd); &#125; public void register(SimpleColleague colleague) &#123; if (!colleagues.contains(colleague)) &#123; colleagues.add(colleague); &#125; &#125; public void relay(SimpleColleague scl) &#123; for (SimpleColleague ob : colleagues) &#123; if (!ob.equals(scl)) &#123; ((SimpleColleague) ob).receive(); &#125; &#125; &#125;&#125;//抽象同事类interface SimpleColleague &#123; void receive(); void send();&#125;//具体同事类class SimpleConcreteColleague1 implements SimpleColleague &#123; SimpleConcreteColleague1() &#123; SimpleMediator smd = SimpleMediator.getMedium(); smd.register(this); &#125; public void receive() &#123; System.out.println(&quot;具体同事类1：收到请求。&quot;); &#125; public void send() &#123; SimpleMediator smd = SimpleMediator.getMedium(); System.out.println(&quot;具体同事类1：发出请求...&quot;); smd.relay(this); //请中介者转发 &#125;&#125;//具体同事类class SimpleConcreteColleague2 implements SimpleColleague &#123; SimpleConcreteColleague2() &#123; SimpleMediator smd = SimpleMediator.getMedium(); smd.register(this); &#125; public void receive() &#123; System.out.println(&quot;具体同事类2：收到请求。&quot;); &#125; public void send() &#123; SimpleMediator smd = SimpleMediator.getMedium(); System.out.println(&quot;具体同事类2：发出请求...&quot;); smd.relay(this); //请中介者转发 &#125;&#125;","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"迭代器模式","slug":"设计模式/行为型模式之迭代器模式","date":"2022-01-11T11:58:16.449Z","updated":"2022-01-11T12:10:11.981Z","comments":true,"path":"2022/01/11/设计模式/行为型模式之迭代器模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%A1%8C%E4%B8%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E8%BF%AD%E4%BB%A3%E5%99%A8%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"现实生活以及程序设计中，经常要访问一个聚合对象中的各个元素，如“数据结构”中的链表遍历，通常的做法是将链表的创建和遍历都放在同一个类中，但这种方式不利于程序的扩展，如果要更换遍历方法就必须修改程序源代码，这违背了 “开闭原则”。 既然将遍历方法封装在聚合类中不可取，那么聚合类中不提供遍历方法，将遍历方法由用户自己实现是否可行呢？答案是同样不可取，因为这种方式会存在两个缺点： 暴露了聚合类的内部表示，使其数据不安全； 增加了客户的负担。 “迭代器模式”能较好地克服以上缺点，它在客户访问类与聚合类之间插入一个迭代器，这分离了聚合对象与其遍历行为，对客户也隐藏了其内部细节，且满足“单一职责原则”和“开闭原则”，如 [Java] 中的 Collection、List、Set、Map 等都包含了迭代器。 迭代器模式在生活中应用的比较广泛，比如：物流系统中的传送带，不管传送的是什么物品，都会被打包成一个个箱子，并且有一个统一的二维码。这样我们不需要关心箱子里是什么，在分发时只需要一个个检查发送的目的地即可。再比如，我们平时乘坐交通工具，都是统一刷卡或者刷脸进站，而不需要关心是男性还是女性、是残疾人还是正常人等信息。 一，模式的定义与特点迭代器（Iterator）模式的定义：提供一个对象来顺序访问聚合对象中的一系列数据，而不暴露聚合对象的内部表示。迭代器模式是一种对象行为型模式，其主要优点如下。 访问一个聚合对象的内容而无须暴露它的内部表示。 遍历任务交由迭代器完成，这简化了聚合类。 它支持以不同方式遍历一个聚合，甚至可以自定义迭代器的子类以支持新的遍历。 增加新的聚合类和迭代器类都很方便，无须修改原有代码。 封装性良好，为遍历不同的聚合结构提供一个统一的接口。 其主要缺点是：增加了类的个数，这在一定程度上增加了系统的复杂性。 在日常开发中，我们几乎不会自己写迭代器。除非需要定制一个自己实现的数据结构对应的迭代器，否则，开源框架提供的 API 完全够用。 二，模式的结构与实现迭代器模式是通过将聚合对象的遍历行为分离出来，抽象成迭代器类来实现的，其目的是在不暴露聚合对象的内部结构的情况下，让外部代码透明地访问聚合的内部数据。 1.模式的结构迭代器模式主要包含以下角色。 抽象聚合（Aggregate）角色：定义存储、添加、删除聚合对象以及创建迭代器对象的接口。 具体聚合（ConcreteAggregate）角色：实现抽象聚合类，返回一个具体迭代器的实例。 抽象迭代器（Iterator）角色：定义访问和遍历聚合元素的接口，通常包含 hasNext()、first()、next() 等方法。 具体迭代器（Concretelterator）角色：实现抽象迭代器接口中所定义的方法，完成对聚合对象的遍历，记录遍历的当前位置。 2.模式的实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869public class IteratorPattern &#123; public static void main(String[] args) &#123; Aggregate ag = new ConcreteAggregate(); ag.add(&quot;中山大学&quot;); ag.add(&quot;华南理工&quot;); ag.add(&quot;韶关学院&quot;); System.out.print(&quot;聚合的内容有：&quot;); Iterator it = ag.getIterator(); while (it.hasNext()) &#123; Object ob = it.next(); System.out.print(ob.toString() + &quot;\\t&quot;); &#125; Object ob = it.first(); System.out.println(&quot;\\nFirst：&quot; + ob.toString()); &#125;&#125;//抽象聚合interface Aggregate &#123; public void add(Object obj); public void remove(Object obj); public Iterator getIterator();&#125;//具体聚合class ConcreteAggregate implements Aggregate &#123; private List&lt;Object&gt; list = new ArrayList&lt;Object&gt;(); public void add(Object obj) &#123; list.add(obj); &#125; public void remove(Object obj) &#123; list.remove(obj); &#125; public Iterator getIterator() &#123; return (new ConcreteIterator(list)); &#125;&#125;//抽象迭代器interface Iterator &#123; Object first(); Object next(); boolean hasNext();&#125;//具体迭代器class ConcreteIterator implements Iterator &#123; private List&lt;Object&gt; list = null; private int index = -1; public ConcreteIterator(List&lt;Object&gt; list) &#123; this.list = list; &#125; public boolean hasNext() &#123; if (index &lt; list.size() - 1) &#123; return true; &#125; else &#123; return false; &#125; &#125; public Object first() &#123; index = 0; Object obj = list.get(index); ; return obj; &#125; public Object next() &#123; Object obj = null; if (this.hasNext()) &#123; obj = list.get(++index); &#125; return obj; &#125;&#125; 三，模式的应用场景前面介绍了关于迭代器模式的结构与特点，下面介绍其应用场景，迭代器模式通常在以下几种情况使用。 当需要为聚合对象提供多种遍历方式时。 当需要为遍历不同的聚合结构提供一个统一的接口时。 当访问一个聚合对象的内容而无须暴露其内部细节的表示时。 由于聚合与迭代器的关系非常密切，所以大多数语言在实现聚合类时都提供了迭代器类，因此大数情况下使用语言中已有的聚合类的迭代器就已经够了。 四，模式的扩展迭代器模式常常与[组合模式]结合起来使用，在对组合模式中的容器构件进行访问时，经常将迭代器潜藏在组合模式的容器构成类中。当然，也可以构造一个外部迭代器来对容器构件进行访问。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"访问者模式","slug":"设计模式/行为型模式之访问者模式","date":"2022-01-11T11:58:06.865Z","updated":"2022-01-11T12:10:20.708Z","comments":true,"path":"2022/01/11/设计模式/行为型模式之访问者模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%A1%8C%E4%B8%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E8%AE%BF%E9%97%AE%E8%80%85%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"在现实生活中，有些集合对象存在多种不同的元素，且每种元素也存在多种不同的访问者和处理方式。例如，公园中存在多个景点，也存在多个游客，不同的游客对同一个景点的评价可能不同；医院医生开的处方单中包含多种药元素，査看它的划价员和药房工作人员对它的处理方式也不同，划价员根据处方单上面的药品名和数量进行划价，药房工作人员根据处方单的内容进行抓药。 这样的例子还有很多，例如，电影或电视剧中的人物角色，不同的观众对他们的评价也不同；还有顾客在商场购物时放在“购物车”中的商品，顾客主要关心所选商品的性价比，而收银员关心的是商品的价格和数量。 这些被处理的数据元素相对稳定而访问方式多种多样的[数据结构]，如果用“访问者模式”来处理比较方便。访问者模式能把处理方法从数据结构中分离出来，并可以根据需要增加新的处理方法，且不用修改原来的程序代码与数据结构，这提高了程序的扩展性和灵活性。 一，模式的定义与特点访问者（Visitor）模式的定义：将作用于某种数据结构中的各元素的操作分离出来封装成独立的类，使其在不改变数据结构的前提下可以添加作用于这些元素的新的操作，为数据结构中的每个元素提供多种访问方式。它将对数据的操作与数据结构进行分离，是行为类模式中最复杂的一种模式。 访问者（Visitor）模式是一种对象行为型模式，其主要优点如下。 扩展性好。能够在不修改对象结构中的元素的情况下，为对象结构中的元素添加新的功能。 复用性好。可以通过访问者来定义整个对象结构通用的功能，从而提高系统的复用程度。 灵活性好。访问者模式将数据结构与作用于结构上的操作解耦，使得操作集合可相对自由地演化而不影响系统的数据结构。 符合单一职责原则。访问者模式把相关的行为封装在一起，构成一个访问者，使每一个访问者的功能都比较单一。 访问者（Visitor）模式的主要缺点如下。 增加新的元素类很困难。在访问者模式中，每增加一个新的元素类，都要在每一个具体访问者类中增加相应的具体操作，这违背了“开闭原则”。 破坏封装。访问者模式中具体元素对访问者公布细节，这破坏了对象的封装性。 违反了依赖倒置原则。访问者模式依赖了具体类，而没有依赖抽象类。 二，模式的结构与实现访问者（Visitor）模式实现的关键是如何将作用于元素的操作分离出来封装成独立的类。 1.模式的结构 2.模式的实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273public class VisitorPattern &#123; public static void main(String[] args) &#123; ObjectStructure os = new ObjectStructure(); os.add(new ConcreteElementA()); os.add(new ConcreteElementB()); Visitor visitor = new ConcreteVisitorA(); os.accept(visitor); System.out.println(&quot;------------------------&quot;); visitor = new ConcreteVisitorB(); os.accept(visitor); &#125;&#125;//抽象访问者interface Visitor &#123; void visit(ConcreteElementA element); void visit(ConcreteElementB element);&#125;//具体访问者A类class ConcreteVisitorA implements Visitor &#123; public void visit(ConcreteElementA element) &#123; System.out.println(&quot;具体访问者A访问--&gt;&quot; + element.operationA()); &#125; public void visit(ConcreteElementB element) &#123; System.out.println(&quot;具体访问者A访问--&gt;&quot; + element.operationB()); &#125;&#125;//具体访问者B类class ConcreteVisitorB implements Visitor &#123; public void visit(ConcreteElementA element) &#123; System.out.println(&quot;具体访问者B访问--&gt;&quot; + element.operationA()); &#125; public void visit(ConcreteElementB element) &#123; System.out.println(&quot;具体访问者B访问--&gt;&quot; + element.operationB()); &#125;&#125;//抽象元素类interface Element &#123; void accept(Visitor visitor);&#125;//具体元素A类class ConcreteElementA implements Element &#123; public void accept(Visitor visitor) &#123; visitor.visit(this); &#125; public String operationA() &#123; return &quot;具体元素A的操作。&quot;; &#125;&#125;//具体元素B类class ConcreteElementB implements Element &#123; public void accept(Visitor visitor) &#123; visitor.visit(this); &#125; public String operationB() &#123; return &quot;具体元素B的操作。&quot;; &#125;&#125;//对象结构角色class ObjectStructure &#123; private List&lt;Element&gt; list = new ArrayList&lt;Element&gt;(); public void accept(Visitor visitor) &#123; Iterator&lt;Element&gt; i = list.iterator(); while (i.hasNext()) &#123; ((Element) i.next()).accept(visitor); &#125; &#125; public void add(Element element) &#123; list.add(element); &#125; public void remove(Element element) &#123; list.remove(element); &#125;&#125; 三，模式的应用场景当系统中存在类型数量稳定（固定）的一类数据结构时，可以使用访问者模式方便地实现对该类型所有数据结构的不同操作，而又不会对数据产生任何副作用（脏数据）。 简而言之，就是当对集合中的不同类型数据（类型数量稳定）进行多种操作时，使用访问者模式。 通常在以下情况可以考虑使用访问者（Visitor）模式。 对象结构相对稳定，但其操作算法经常变化的程序。 对象结构中的对象需要提供多种不同且不相关的操作，而且要避免让这些操作的变化影响对象的结构。 对象结构包含很多类型的对象，希望对这些对象实施一些依赖于其具体类型的操作。 四，模式的扩展访问者（Visitor）模式是使用频率较高的一种[设计模式]，它常常同以下两种设计模式联用。 (1)与“[迭代器模式]”联用。因为访问者模式中的“对象结构”是一个包含元素角色的容器，当访问者遍历容器中的所有元素时，常常要用迭代器。 (2)访问者（Visitor）模式同“[组合模式]”联用。因为访问者（Visitor）模式中的“元素对象”可能是叶子对象或者是容器对象，如果元素对象包含容器对象，就必须用到[组合模式]。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"备忘录模式","slug":"设计模式/行为型模式之备忘录模式","date":"2022-01-11T11:57:59.752Z","updated":"2022-01-11T12:09:22.827Z","comments":true,"path":"2022/01/11/设计模式/行为型模式之备忘录模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%A1%8C%E4%B8%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%A4%87%E5%BF%98%E5%BD%95%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"一，模式的定义与特点在不破坏封装性的前提下，捕获一个对象的内部状态，并在该对象之外保存这个状态，以便以后当需要时能将该对象恢复到原先保存的状态。该模式又叫快照模式。 备忘录模式是一种对象行为型模式，其主要优点如下。 提供了一种可以恢复状态的机制。当用户需要时能够比较方便地将数据恢复到某个历史的状态。 实现了内部状态的封装。除了创建它的发起人之外，其他对象都不能够访问这些状态信息。 简化了发起人类。发起人不需要管理和保存其内部状态的各个备份，所有状态信息都保存在备忘录中，并由管理者进行管理，这符合单一职责原则。 其主要缺点是：资源消耗大。如果要保存的内部状态信息过多或者特别频繁，将会占用比较大的内存资源。 二，模式的结构与实现备忘录模式的核心是设计备忘录类以及用于管理备忘录的管理者类。 1.模式的结构备忘录模式的主要角色如下。 发起人（Originator）角色：记录当前时刻的内部状态信息，提供创建备忘录和恢复备忘录数据的功能，实现其他业务功能，它可以访问备忘录里的所有信息。 备忘录（Memento）角色：负责存储发起人的内部状态，在需要的时候提供这些内部状态给发起人。 管理者（Caretaker）角色：对备忘录进行管理，提供保存与获取备忘录的功能，但其不能对备忘录的内容进行访问与修改。 2.模式的实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class MementoPattern &#123; public static void main(String[] args) &#123; Originator or = new Originator(); Caretaker cr = new Caretaker(); or.setState(&quot;S0&quot;); System.out.println(&quot;初始状态:&quot; + or.getState()); cr.setMemento(or.createMemento()); //保存状态 or.setState(&quot;S1&quot;); System.out.println(&quot;新的状态:&quot; + or.getState()); or.restoreMemento(cr.getMemento()); //恢复状态 System.out.println(&quot;恢复状态:&quot; + or.getState()); &#125;&#125;//备忘录class Memento &#123; private String state; public Memento(String state) &#123; this.state = state; &#125; public void setState(String state) &#123; this.state = state; &#125; public String getState() &#123; return state; &#125;&#125;//发起人class Originator &#123; private String state; public void setState(String state) &#123; this.state = state; &#125; public String getState() &#123; return state; &#125; public Memento createMemento() &#123; return new Memento(state); &#125; public void restoreMemento(Memento m) &#123; this.setState(m.getState()); &#125;&#125;//管理者class Caretaker &#123; private Memento memento; public void setMemento(Memento m) &#123; memento = m; &#125; public Memento getMemento() &#123; return memento; &#125;&#125; 三，模式的应用场景 需要保存与恢复数据的场景，如玩游戏时的中间结果的存档功能。 需要提供一个可回滚操作的场景，如 Word、记事本、Photoshop，Eclipse 等软件在编辑时按 Ctrl+Z 组合键，还有数据库中事务操作。 四，模式的扩展在备忘录模式中，通过定义“备忘录”来备份“发起人”的信息，而原型模式的 clone() 方法具有自备份功能，所以，如果让发起人实现 Cloneable 接口就有备份自己的功能，这时可以删除备忘录类。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class PrototypeMemento &#123; public static void main(String[] args) &#123; OriginatorPrototype or = new OriginatorPrototype(); PrototypeCaretaker cr = new PrototypeCaretaker(); or.setState(&quot;S0&quot;); System.out.println(&quot;初始状态:&quot; + or.getState()); cr.setMemento(or.createMemento()); //保存状态 or.setState(&quot;S1&quot;); System.out.println(&quot;新的状态:&quot; + or.getState()); or.restoreMemento(cr.getMemento()); //恢复状态 System.out.println(&quot;恢复状态:&quot; + or.getState()); &#125;&#125;//发起人原型class OriginatorPrototype implements Cloneable &#123; private String state; public void setState(String state) &#123; this.state = state; &#125; public String getState() &#123; return state; &#125; public OriginatorPrototype createMemento() &#123; return this.clone(); &#125; public void restoreMemento(OriginatorPrototype opt) &#123; this.setState(opt.getState()); &#125; public OriginatorPrototype clone() &#123; try &#123; return (OriginatorPrototype) super.clone(); &#125; catch (CloneNotSupportedException e) &#123; e.printStackTrace(); &#125; return null; &#125;&#125;//原型管理者class PrototypeCaretaker &#123; private OriginatorPrototype opt; public void setMemento(OriginatorPrototype opt) &#123; this.opt = opt; &#125; public OriginatorPrototype getMemento() &#123; return opt; &#125;&#125;","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"解释器模式","slug":"设计模式/行为型模式之解释器模式","date":"2022-01-11T11:57:49.640Z","updated":"2022-01-11T12:10:40.428Z","comments":true,"path":"2022/01/11/设计模式/行为型模式之解释器模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%A1%8C%E4%B8%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E8%A7%A3%E9%87%8A%E5%99%A8%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"在软件开发中，会遇到有些问题多次重复出现，而且有一定的相似性和规律性。如果将它们归纳成一种简单的语言，那么这些问题实例将是该语言的一些句子，这样就可以用“编译原理”中的解释器模式来实现了。 一，模式的定义与特点解释器（Interpreter）模式的定义：给分析对象定义一个语言，并定义该语言的文法表示，再设计一个解析器来解释语言中的句子。也就是说，用编译语言的方式来分析应用中的实例。这种模式实现了文法表达式处理的接口，该接口解释一个特定的上下文。 这里提到的文法和句子的概念同编译原理中的描述相同，“文法”指语言的语法规则，而“句子”是语言集中的元素。例如，汉语中的句子有很多，“我是中国人”是其中的一个句子，可以用一棵语法树来直观地描述语言中的句子。 解释器模式是一种类行为型模式，其主要优点如下。 扩展性好。由于在解释器模式中使用类来表示语言的文法规则，因此可以通过继承等机制来改变或扩展文法。 容易实现。在语法树中的每个表达式节点类都是相似的，所以实现其文法较为容易。 解释器模式的主要缺点如下。 执行效率较低。解释器模式中通常使用大量的循环和递归调用，当要解释的句子较复杂时，其运行速度很慢，且代码的调试过程也比较麻烦。 会引起类膨胀。解释器模式中的每条规则至少需要定义一个类，当包含的文法规则很多时，类的个数将急剧增加，导致系统难以管理与维护。 可应用的场景比较少。在软件开发中，需要定义语言文法的应用实例非常少，所以这种模式很少被使用到。 二，模式的结构与实现解释器模式常用于对简单语言的编译或分析实例中，为了掌握好它的结构与实现，必须先了解编译原理中的“文法、句子、语法树”等相关概念。 文法 文法是用于描述语言的语法结构的形式规则。没有规矩不成方圆，例如，有些人认为完美爱情的准则是“相互吸引、感情专一、任何一方都没有恋爱经历”，虽然最后一条准则较苛刻，但任何事情都要有规则，语言也一样，不管它是机器语言还是自然语言，都有它自己的文法规则。例如，中文中的“句子”的文法如下。 1234567〈句子〉::=〈主语〉〈谓语〉〈宾语〉〈主语〉::=〈代词〉|〈名词〉〈谓语〉::=〈动词〉〈宾语〉::=〈代词〉|〈名词〉〈代词〉你|我|他〈名词〉7大学生I筱霞I英语〈动词〉::=是|学习 注：这里的符号“::=”表示“定义为”的意思，用“〈”和“〉”括住的是非终结符，没有括住的是终结符。 句子 句子是语言的基本单位，是语言集中的一个元素，它由终结符构成，能由“文法”推导出。例如，上述文法可以推出“我是大学生”，所以它是句子。 语法树 语法树是句子结构的一种树型表示，它代表了句子的推导结果，它有利于理解句子语法结构的层次。图 1 所示是“我是大学生”的语法树。 有了以上基础知识，现在来介绍解释器模式的结构就简单了。解释器模式的结构与组合模式相似，不过其包含的组成元素比组合模式多，而且组合模式是对象结构型模式，而解释器模式是类行为型模式。 1.模式的结构解释器模式包含以下主要角色。 抽象表达式（Abstract Expression）角色：定义解释器的接口，约定解释器的解释操作，主要包含解释方法 interpret()。 终结符表达式（Terminal Expression）角色：是抽象表达式的子类，用来实现文法中与终结符相关的操作，文法中的每一个终结符都有一个具体终结表达式与之相对应。 非终结符表达式（Nonterminal Expression）角色：也是抽象表达式的子类，用来实现文法中与非终结符相关的操作，文法中的每条规则都对应于一个非终结符表达式。 环境（Context）角色：通常包含各个解释器需要的数据或是公共的功能，一般用来传递被所有解释器共享的数据，后面的解释器可以从这里获取这些值。 客户端（Client）：主要任务是将需要分析的句子或表达式转换成使用解释器对象描述的抽象语法树，然后调用解释器的解释方法，当然也可以通过环境角色间接访问解释器的解释方法。 2.模式的实现解释器模式实现的关键是定义文法规则、设计终结符类与非终结符类、画出结构图，必要时构建语法树。 12345678910111213141516171819202122232425262728//抽象表达式类interface AbstractExpression &#123; public void interpret(String info); //解释方法&#125;//终结符表达式类class TerminalExpression implements AbstractExpression &#123; public void interpret(String info) &#123; //对终结符表达式的处理 &#125;&#125;//非终结符表达式类class NonterminalExpression implements AbstractExpression &#123; private AbstractExpression exp1; private AbstractExpression exp2; public void interpret(String info) &#123; //非对终结符表达式的处理 &#125;&#125;//环境类class Context &#123; private AbstractExpression exp; public Context() &#123; //数据初始化 &#125; public void operation(String info) &#123; //调用相关表达式类的解释方法 &#125;&#125; 三，模式的应用场景前面介绍了解释器模式的结构与特点，下面分析它的应用场景。 当语言的文法较为简单，且执行效率不是关键问题时。 当问题重复出现，且可以用一种简单的语言来进行表达时。 当一个语言需要解释执行，并且语言中的句子可以表示为一个抽象语法树的时候，如 XML 文档解释。 注意：解释器模式在实际的软件开发中使用比较少，因为它会引起效率、性能以及维护等问题。如果碰到对表达式的解释，在 Java 中可以用 Expression4J 或 Jep 等来设计。 四，模式的扩展在项目开发中，如果要对数据表达式进行分析与计算，无须再用解释器模式进行设计了，Java 提供了以下强大的数学公式解析器：Expression4J、MESP(Math Expression String Parser) 和 Jep 等，它们可以解释一些复杂的文法，功能强大，使用简单。 现在以 Jep 为例来介绍该工具包的使用方法。Jep 是 Java expression parser 的简称，即 Java 表达式分析器，它是一个用来转换和计算数学表达式的 Java 库。通过这个程序库，用户可以以字符串的形式输入一个任意的公式，然后快速地计算出其结果。而且 Jep 支持用户自定义变量、常量和函数，它包括许多常用的数学函数和常量。 使用前先下载 Jep 压缩包，解压后，将 jep-x.x.x.jar 文件移到选择的目录中，在 Eclipse 的“Java 构建路径”对话框的“库”选项卡中选择“添加外部 JAR(X)…”，将该 Jep 包添加项目中后即可使用其中的类库。 下面以计算存款利息为例来介绍。存款利息的计算公式是：本金x利率x时间=利息，其相关代码如下： 123456789101112131415import com.singularsys.jep.*;public class JepDemo &#123; public static void main(String[] args) throws JepException &#123; Jep jep = new Jep(); //定义要计算的数据表达式 String 存款利息 = &quot;本金*利率*时间&quot;; //给相关变量赋值 jep.addVariable(&quot;本金&quot;, 10000); jep.addVariable(&quot;利率&quot;, 0.038); jep.addVariable(&quot;时间&quot;, 2); jep.parse(存款利息); //解析表达式 Object accrual = jep.evaluate(); //计算 System.out.println(&quot;存款利息：&quot; + accrual); &#125;&#125;","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"ClickHouse","slug":"clickhouse/clickhouse","date":"2022-01-11T11:50:20.738Z","updated":"2022-01-11T11:55:06.481Z","comments":true,"path":"2022/01/11/clickhouse/clickhouse/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/clickhouse/clickhouse/","excerpt":"","text":"一，clickhouse特点1.列式存储行存储 好处是想查某个人所有的属性时，可以通过一次磁盘查找加顺序读取就可以。但是当想查所有人的年龄时，需要不停的查找，或者全表扫描才行，遍历的很多数据都是不需要的。 id 姓名 年龄 1 张三 18 2 李四 19 3 王五 20 列存储 列存储的好处 1 对于列的聚合，计数，求和等统计操作要优于行式存储。 2 由于某一列的数据类型都是相同的，针对于数据存储更容易进行数据压缩，每一列选择更优的数据压缩算法，大大提高了数据的压缩比重。 3 由于数据压缩比更好，一方面节省了磁盘空间，另一方面对于cache也有了更大的发挥空间。 id 1 2 3 姓名 张三 李四 王五 年龄 18 19 20 2.DBMS的功能 几乎覆盖了标准SQL的大部分语法，包括 DDL和 DML ,以及配套的各种函数。 用户管理及权限管理 数据的备份与恢复 3.多样化引擎clickhouse和mysql类似，把表级的存储引擎插件化，根据表的不同需求可以设定不同的存储引擎。目前包括合并树、日志、接口和其他四大类20多种引擎。 4.高吞吐写入能力ClickHouse采用类LSM Tree的结构，数据写入后定期在后台Compaction。通过类LSM tree的结构，ClickHouse在数据导入时全部是顺序append写，写入后数据段不可更改，在后台compaction时也是多个段merge sort后顺序写回磁盘。顺序写的特性，充分利用了磁盘的吞吐能力，即便在HDD上也有着优异的写入性能。 官方公开benchmark测试显示能够达到50MB-200MB/s的写入吞吐能力，按照每行100Byte估算，大约相当于50W-200W条/s的写入速度。 5.数据分区与线程级并行ClickHouse将数据划分为多个partition，每个partition再进一步划分为多个index granularity，然后通过多个CPU核心分别处理其中的一部分来实现并行数据处理。 在这种设计下，单条Query就能利用整机所有CPU。极致的并行处理能力，极大的降低了查询延时。 所以，clickhouse即使对于大量数据的查询也能够化整为零平行处理。但是有一个弊端就是对于单条查询使用多cpu，就不利于同时并发多条查询。所以对于高qps的查询业务，clickhouse并不是强项。 6.关联查询clickhouse像很多OLAP数据库一样，单表查询速度由于关联查询，而且clickhouse的两者差距更为明显。 二，clickhouse安装1.取消打开文件数限制在/etc/security/limits.conf、/etc/security/limits.d/90-nproc.conf这2个文件的末尾加入以下内容： 1234* soft nofile 65536 * hard nofile 65536 * soft nproc 131072 * hard nproc 131072 2.取消SELINUX修改/etc/selinux/config中的SELINUX=disabled后重启。 3.开放端口HTTP：8123 TCP：9000 4.安装123456yum install -y libtoolyum install -y *unixODBC*yum install yum-utilsrpm --import https://repo.clickhouse.tech/CLICKHOUSE-KEY.GPGyum-config-manager --add-repo https://repo.clickhouse.tech/rpm/clickhouse.repoyum install clickhouse-server clickhouse-client 5.修改配置文件1vim /etc/clickhouse-server/config.xml 把 &lt;listen_host&gt;::&lt;/listen_host&gt;的注解打开，这样的话才能让clickhouse被除本机以外的服务器访问。 6.启动ClickServer1systemctl start clickhouse-server 7.使用client连接server1clickhouse-client -m 三，数据类型1.整型固定长度的整型，包括有符号整型或无符号整型。 12345678910整型范围（-2n-1~2n-1-1）：Int8 - [-128 : 127]Int16 - [-32768 : 32767]Int32 - [-2147483648 : 2147483647]Int64 - [-9223372036854775808 : 9223372036854775807]无符号整型范围（0~2n-1）：UInt8 - [0 : 255]UInt16 - [0 : 65535]UInt32 - [0 : 4294967295]UInt64 - [0 : 18446744073709551615] 适用场景：个数，数量，存储id等。 2.浮点型12Float32 - floatFloat64 – double 建议尽可能以整数形式存储数据。例如，将固定精度的数字转换为整数值，如时间用毫秒为单位表示，因为浮点型进行计算时可能引起四舍五入的误差。 适用场景：一般数据值比较小，不涉及大量的统计计算，精度要求不高的时候。比如保存商品的重量。 3.布尔型没有单独的类型来存储布尔值。可以使用 UInt8 类型，取值限制为 0 或 1。 4.Decimal型有符号的浮点数，可在加、减和乘法运算过程中保持精度。对于除法，最低有效数字会被丢弃（不舍入）。 有三种声明： 123Decimal32(s)，相当于Decimal(9-s,s)Decimal64(s)，相当于Decimal(18-s,s)Decimal128(s)，相当于Decimal(38-s,s) 适用场景：一般金额字段，汇率，利率等字段为了保证小数点精度，都是用Decimal进行存储。 5.字符串1）String字符串可以任意长度的。它可以包含任意的字节集，包含空字节。 2）FixedString(N)固定长度 N 的字符串，N 必须是严格的正自然数。当服务端读取长度小于 N 的字符串时候，通过在字符串末尾添加空字节来达到 N 字节长度。 当服务端读取长度大于 N 的字符串时候，将返回错误消息。 与String相比，极少会使用FixedString，因为使用起来不是很方便。 适用场景：名称，文字描述，字符型编码。固定长度的可以保存一些定长的内容，比如一些编码，性别等，但是考虑到一定的变化风险，带来收益不够明显，所以定长字符串使用意义有限。 6.枚举类型包括 Enum8 和 Enum16 类型。Enum 保存&#39;string&#39;= integer 的对应关系。 Enum8 用 &#39;String&#39;= Int8 对描述。 Enum16 用&#39;String&#39;= Int16对描述。 用法演示： 创建一个带有一个枚举 Enum8(&#39;hello&#39; = 1, &#39;world&#39; = 2) 类型的列： 12345CREATE TABLE t_enum( x Enum8(&#x27;hello&#x27; = 1, &#x27;world&#x27; = 2))ENGINE = TinyLog 这个 x 列只能存储类型定义中列出的值：&#39;hello&#39;或&#39;world&#39;。如果尝试保存任何其他值，ClickHouse 抛出异常。 从表中查询数据时，ClickHouse 从 Enum 中输出字符串值。 1234567SELECT * FROM t_enum┌─x─────┐│ hello ││ world ││ hello │└───────┘ 如果需要看到对应行的数值，则必须将 Enum 值转换为整数类型。 1234567SELECT CAST(x, &#x27;Int8&#x27;) FROM t_enum┌─CAST(x, &#x27;Int8&#x27;)────┐│ 1 ││ 2 ││ 1 │└────────────────────┘ 适用场景：对于一些状态，类型的字段算是一种空间优化，也算是一种数据约束。但是实际使用中往往因为一些数据内容的变化增加一定的维护成本，甚至是数据丢失的问题。所以谨慎使用。 7.时间类型目前clickhouse 有三种时间类型 Date 接受 年-月-日 的字符串比如 ‘2019-12-16’ Datetime 接受 年-月-日 时:分:秒 的字符串比如 ‘2019-12-16 20:50:10’ Datetime64 接受 年-月-日 时:分:秒.亚秒 的字符串比如 ‘2019-12-16 20:50:10.66’ 日期类型，用两个字节存储，表示从 1970-01-01 (无符号) 到当前的日期值。 还有很多数据结构，可以参考官方文档：https://clickhouse.yandex/docs/zh/data_types/ 8.数组**Array(T)**：由 T 类型元素组成的数组。 T 可以是任意类型，包含数组类型。 但不推荐使用多维数组，ClickHouse 对多维数组的支持有限。例如，不能在 MergeTree 表中存储多维数组。 可以使用array函数来创建数组，也可以使用方括号：[]。 创建数组案例： 1234567891011121314151617181920SELECT array(1, 2) AS x, toTypeName(x)SELECT [1, 2] AS x, toTypeName(x)┌─x─────┬─toTypeName(array(1, 2))─┐│ [1,2] │ Array(UInt8) │└───────┴─────────────────────────┘1 rows in set. Elapsed: 0.002 sec.:) SELECT [1, 2] AS x, toTypeName(x)┌─x─────┬─toTypeName([1, 2])─┐│ [1,2] │ Array(UInt8) │└───────┴────────────────────┘1 rows in set. Elapsed: 0.002 sec. 四，表引擎1.表引擎的使用表引擎是clickhouse的一大特色。可以说， 表引擎决定了如何存储表的数据。包括： 1）数据的存储方式和位置，写到哪里以及从哪里读取数据。 2）支持哪些查询以及如何支持。 3）并发数据访问。 4）索引的使用（如果存在）。 5）是否可以执行多线程请求。 6）数据复制参数。 表引擎的使用方式就是必须显形在创建表时定义该表使用的引擎，以及引擎使用的相关参数。如： 1create table t_tinylog ( id String, name String) engine=TinyLog; 引擎的名称大小写敏感。 2.TinyLog以列文件的形式保存在磁盘上，不支持索引，没有并发控制。一般保存少量数据的小表，生产环境上作用有限。可以用于平时练习测试用。 3.Memory内存引擎，数据以未压缩的原始形式直接保存在内存当中，服务器重启数据就会消失。读写操作不会相互阻塞，不支持索引。简单查询下有非常非常高的性能表现（超过10G/s）。 一般用到它的地方不多，除了用来测试，就是在需要非常高的性能，同时数据量又不太大（上限大概 1 亿行）的场景。 4.MergeTreeClickhouse 中最强大的表引擎当属 MergeTree （合并树）引擎及该系列（MergeTree）中的其他引擎。地位可以相当于innodb之于Mysql。 而且基于MergeTree，还衍生出了很多小弟，也是非常有特色的引擎。 建表语句 1234567891011121314151617create table t_order_mt( id UInt32, sku_id String, total_amount Decimal(16,2), create_time Datetime ) engine =MergeTree partition by toYYYYMMDD(create_time) primary key (id) order by (id,sku_id) insert into t_order_mtvalues(101,&#x27;sku_001&#x27;,1000.00,&#x27;2020-06-01 12:00:00&#x27;) ,(102,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 11:00:00&#x27;),(102,&#x27;sku_004&#x27;,2500.00,&#x27;2020-06-01 12:00:00&#x27;),(102,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 13:00:00&#x27;)(102,&#x27;sku_002&#x27;,12000.00,&#x27;2020-06-01 13:00:00&#x27;)(102,&#x27;sku_002&#x27;,600.00,&#x27;2020-06-02 12:00:00&#x27;) MergeTree其实还有很多参数(绝大多数用默认值即可)，但是三个参数是更加重要的，也涉及了关于MergeTree的很多概念。 1）partition by 分区（可选项）作用： 学过hive的应该都不陌生，分区的目的主要是降低扫描的范围，优化查询速度。 如果不填： 只会使用一个分区。 分区目录： MergeTree 是以列文件+索引文件+表定义文件组成的，但是如果设定了分区那么这些文件就会保存到不同的分区目录中。 并行：分区后，面对涉及跨分区的查询统计，clickhouse会以分区为单位并行处理。 数据写入与分区合并： 任何一个批次的数据写入都会产生一个临时分区，不会纳入任何一个已有的分区。写入后的某个时刻（大概10-15分钟后），clickhouse会自动执行合并操作（等不及也可以手动通过optimize执行），把临时分区的数据，合并到已有分区中。 1optimize table xxxx [final] 2) primary key主键(可选)clickhouse中的主键，和其他数据库不太一样，它只提供了数据的一级索引，但是却不是唯一约束。这就意味着是可以存在相同primary key的数据的。 主键的设定主要依据是查询语句中的 where 条件。 根据条件通过对主键进行某种形式的二分查找，能够定位到对应的index granularity,避免了全表扫描。 index granularity： 直接翻译的话就是索引粒度，指在稀疏索引中两个相邻索引对应数据的间隔。clickhouse中的MergeTree默认是8192。官方不建议修改这个值，除非该列存在大量重复值，比如在一个分区中几万行才有一个不同数据。 稀疏索引： 稀疏索引的好处就是可以用很少的索引数据，定位更多的数据，代价就是只能定位到索引粒度的第一行，然后再进行进行一点扫描。 3）order by(必选)order by 设定了分区内的数据按照哪些字段顺序进行有序保存。 order by是MergeTree中唯一一个必填项，甚至比primary key 还重要，因为当用户不设置主键的情况，很多处理会依照order by的字段进行处理（比如去重和汇总）。 要求：主键必须是order by字段的前缀字段。 比如order by 字段是 (id,sku_id) 那么主键必须是id 或者(id,sku_id) 4)二级索引目前在clickhouse的官网上二级索引的功能是被标注为实验性的。 所以使用二级索引前需要增加设置。 1set allow_experimental_data_skipping_indices=1; 12345678910create table t_order_mt2( id UInt32, sku_id String, total_amount Decimal(16,2), create_time Datetime, INDEX a total_amount TYPE minmax GRANULARITY 5 ) engine =MergeTree partition by toYYYYMMDD(create_time) primary key (id) order by (id, sku_id) 其中GRANULARITY N 是设定二级索引对于一级索引粒度的粒度。 那么在使用下面语句进行测试，可以看出二级索引能够为非主键字段的查询发挥作用。 1&#x27;select * from test1.t_order_mt where total_amount &gt; toDecimal32(900., 2)&#x27; 5)数据TTLTTL即Time To Live，MergeTree提供了可以管理数据或者列的生命周期的功能。 ①列级别TTL123456789 create table t_order_mt3( id UInt32, sku_id String, total_amount Decimal(16,2) TTL create_time+interval 10 SECOND, create_time Datetime ) engine =MergeTreepartition by toYYYYMMDD(create_time) primary key (id) order by (id, sku_id) 插入数据 1234insert into t_order_mt3values(106,&#x27;sku_001&#x27;,1000.00,&#x27;2020-06-12 22:52:30&#x27;) ,(107,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-12 22:52:30&#x27;),(110,&#x27;sku_003&#x27;,600.00,&#x27;2020-06-13 12:00:00&#x27;) ②表级TTL针对整张表，下面的这条语句是数据会在create_time之后10秒丢失。 1alter table t_order_mt3 MODIFY TTL create_time + INTERVAL 10 SECOND; 涉及判断的字段必须是Date或者Datetime类型，推荐使用分区的日期字段。 能够使用的时间周期： 12345678- SECOND- MINUTE- HOUR- DAY- WEEK- MONTH- QUARTER- YEAR 5.ReplacingMergeTreeReplacingMergeTree是MergeTree的一个变种，它存储特性完全继承MergeTree，只是多了一个去重的功能。 尽管MergeTree可以设置主键，但是primary key其实没有唯一约束的功能。如果你想处理掉重复的数据，可以借助这个ReplacingMergeTree。 去重时机：数据的去重只会在合并的过程中出现。合并会在未知的时间在后台进行，所以你无法预先作出计划。有一些数据可能仍未被处理。 去重范围：如果表经过了分区，去重只会在分区内部进行去重，不能执行跨分区的去重。 所以ReplacingMergeTree能力有限， ReplacingMergeTree 适用于在后台清除重复的数据以节省空间，但是它不保证没有重复的数据出现。 123456789 create table t_order_rmt( id UInt32, sku_id String, total_amount Decimal(16,2) , create_time Datetime ) engine =ReplacingMergeTree(create_time)partition by toYYYYMMDD(create_time) primary key (id) order by (id, sku_id) ReplacingMergeTree()填入的参数为版本字段，重复数据保留版本字段值最大的。 如果不填版本字段，默认保留最后一条。 1234567insert into t_order_rmtvalues(101,&#x27;sku_001&#x27;,1000.00,&#x27;2020-06-01 12:00:00&#x27;) ,(102,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 11:00:00&#x27;),(102,&#x27;sku_004&#x27;,2500.00,&#x27;2020-06-01 12:00:00&#x27;),(102,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 13:00:00&#x27;)(102,&#x27;sku_002&#x27;,12000.00,&#x27;2020-06-01 13:00:00&#x27;)(102,&#x27;sku_002&#x27;,600.00,&#x27;2020-06-02 12:00:00&#x27;) 1SELECT * FROM t_order_rmt 1OPTIMIZE TABLE t_order_rmt FINAL 1SELECT * FROM t_order_rmt 通过测试得到结论： 实际上是使用order by 字段作为唯一键。 去重不能跨分区。 只有合并分区才会进行去重。 认定重复的数据保留，版本字段值最大的。 如果版本字段相同则保留最后一条。 6.SummingMergeTree对于不查询明细，只关心以维度进行汇总聚合结果的场景。如果只使用普通的MergeTree的话，无论是存储空间的开销，还是查询时临时聚合的开销都比较大。 Clickhouse 为了这种场景，提供了一种能够“预聚合”的引擎，SummingMergeTree. 表定义 123456789create table t_order_smt( id UInt32, sku_id String, total_amount Decimal(16,2) , create_time Datetime ) engine =SummingMergeTree(total_amount) partition by toYYYYMMDD(create_time) primary key (id) order by (id,sku_id ) 插入数据 1234567insert into t_order_smtvalues(101,&#x27;sku_001&#x27;,1000.00,&#x27;2020-06-01 12:00:00&#x27;) ,(102,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 11:00:00&#x27;),(102,&#x27;sku_004&#x27;,2500.00,&#x27;2020-06-01 12:00:00&#x27;),(102,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 13:00:00&#x27;)(102,&#x27;sku_002&#x27;,12000.00,&#x27;2020-06-01 13:00:00&#x27;)(102,&#x27;sku_002&#x27;,600.00,&#x27;2020-06-02 12:00:00&#x27;) 1optimize table t_order_smt final; 通过结果可以得到以下结论： 以SummingMergeTree（）中指定的列作为汇总数据列。可以填写多列必须数字列，如果不填，以所有非维度列且为数字列的字段为汇总数据列。 以order by 的列为准，作为维度列。 其他的列保留第一行。 不在一个分区的数据不会被聚合。 设计聚合表的话，唯一键值、流水号可以去掉，所有字段全部是维度、度量或者时间戳。 能不能直接 select total_amount from province_name=’’ and create_date=’xxx’ 来得到汇总值？ 不行，可能会包含一些还没来得及聚合的临时明细 select sum(total_amount) from province_name=’’ and create_date=’xxx’ 五，SQL操作基本上来说传统关系型数据库（以MySQL为例）的SQL语句，基本支持但是也有不一样的地方。这里不会从头讲解SQL语法只介绍Clickhouse与标准SQL（MySQL）不一致的地方。 1.insert基本与标准SQL（MySQL）基本一致 包括标准 insert into [table_name] values(…),(….) 以及从表到表的插入 1insert into [table_name] select a,b,c from [table_name_2] 2.update和deleteClickHouse提供了Delete 和Update的能力，这类操作被称为Mutation查询，它可以看做Alter 的一种。 虽然可以实现修改和删除，但是和一般的OLTP数据库不一样，Mutation语句是一种很“重”的操作，而且不支持事务。 “重”的原因主要是每次修改或者删除都会导致放弃目标数据的原有分区，重建新分区。所以尽量做批量的变更，不要进行频繁小数据的操作。 删除操作 1alter table t_order_smt delete where sku_id =&#x27;sku_001&#x27;; 修改操作 12alter table t_order_smt update total_amount=toDecimal32(2000.00,2) where id =102; 由于操作比较“重”，所以 Mutation语句分两步执行，同步执行的部分其实只是进行新增数据新增分区和并把旧分区打上逻辑上的失效标记。直到触发分区合并的时候，才会删除旧数据释放磁盘空间。 3.查询操作clickhouse基本上与标准SQL 差别不大。 支持子查询 支持CTE(with 子句) 支持各种JOIN， 但是JOIN操作无法使用缓存，所以即使是两次相同的JOIN语句，Clickhouse也会视为两条新SQL。 不支持窗口函数。 不支持自定义函数。 GROUP BY 操作增加了 with rollup\\with cube\\with total 用来计算小计和总计。 模拟数据 12345678910111213insert into t_order_mtvalues(101,&#x27;sku_001&#x27;,1000.00,&#x27;2020-06-01 12:00:00&#x27;) ,(102,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 12:00:00&#x27;),(103,&#x27;sku_004&#x27;,2500.00,&#x27;2020-06-01 12:00:00&#x27;),(104,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 12:00:00&#x27;)(105,&#x27;sku_003&#x27;,600.00,&#x27;2020-06-02 12:00:00&#x27;),(106,&#x27;sku_001&#x27;,1000.00,&#x27;2020-06-04 12:00:00&#x27;),(107,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-04 12:00:00&#x27;),(108,&#x27;sku_004&#x27;,2500.00,&#x27;2020-06-04 12:00:00&#x27;),(109,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-04 12:00:00&#x27;),(110,&#x27;sku_003&#x27;,600.00,&#x27;2020-06-01 12:00:00&#x27;)select id , sku_id,sum(total_amount) from t_order_mt group by id,sku_id with rollup; with rollup : 从右至左去掉维度进行小计。 1select id , sku_id,sum(total_amount) from t_order_mt group by id,sku_id with cube; with cube : 从右至左去掉维度进行小计，再从左至右去掉维度进行小计。 1select id , sku_id,sum(total_amount) from t_order_mt group by id,sku_id with totals; with totals: 只计算合计。 4.alter操作同mysql的修改字段基本一致。 新增字段 1alter table tableName add column newcolname String after col1 修改字段类型 1alter table tableName modify column newcolname String ； 删除字段 1alter table tableName drop column newcolname ; 5.导出数据1&quot;select toHour(create_time) hr ,count(*) from test1.order_wide where dt=&#x27;2020-06-23&#x27; group by hr&quot; --format CSVWithNames&gt; ~/rs1.csv 支持格式的地址 六，副本副本的目的主要是保障数据的高可用性，即使一台clickhouse节点宕机，那么也可以从其他服务器获得相同的数据。 1.副本写入流程 2.配置这时需要启动zookeeper集群 和另外一台clickhouse 服务器。 另外一台clickhouse服务器的安装完全和第一台一直即可。 在两台服务器的/etc/clickhouse-server/config.d目录下创建一个名为metrika.xml的配置文件： 123456789101112131415161718&lt;?xml version=&quot;1.0&quot;?&gt;&lt;yandex&gt; &lt;zookeeper-servers&gt; &lt;node index=&quot;1&quot;&gt; &lt;host&gt;hdp1&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;node index=&quot;2&quot;&gt; &lt;host&gt;hdp2&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt;&lt;node index=&quot;3&quot;&gt; &lt;host&gt;hdp3&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;/zookeeper-servers&gt;&lt;/yandex&gt; 在 /etc/clickhouse-server/config.xml中增加 1&lt;include_from&gt;/etc/clickhouse-server/config.d/metrika.xml&lt;/include_from&gt; 在两台电脑上分别建表 A机器 123456789create table rep_t_order_mt_0105 ( id UInt32, sku_id String, total_amount Decimal(16,2), create_time Datetime ) engine =ReplicatedMergeTree(&#x27;/clickhouse/tables/01/rep_t_order_mt_0105&#x27;,&#x27;rep_hdp1&#x27;) partition by toYYYYMMDD(create_time) primary key (id) order by (id,sku_id); B机器 123456789create table rep_t_order_mt_0105 ( id UInt32, sku_id String, total_amount Decimal(16,2), create_time Datetime) engine =ReplicatedMergeTree(&#x27;/clickhouse/tables/01/rep_t_order_mt_0105&#x27;,&#x27;rep_hdp2&#x27;)partition by toYYYYMMDD(create_time) primary key (id) order by (id,sku_id); 3.参数解释ReplicatedMergeTree 中， 第一参数是分片的zk_path，一般按照： /clickhouse/table/&#123;shard&#125;/&#123;table_name&#125; 的格式写，如果只有一个分片就写01即可。 第二个参数是副本名称，相同的分片副本名称不能相同。 insert语句 123456 insert into rep_t_order_mt_0105 values(101,&#x27;sku_001&#x27;,1000.00,&#x27;2020-06-01 12:00:00&#x27;) ,(102,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 12:00:00&#x27;),(103,&#x27;sku_004&#x27;,2500.00,&#x27;2020-06-01 12:00:00&#x27;),(104,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 12:00:00&#x27;)(105,&#x27;sku_003&#x27;,600.00,&#x27;2020-06-02 12:00:00&#x27;) 七，分片集群副本虽然能够提高数据的可用性，降低丢失风险，但是对数据的横向扩容没有解决。每台机子实际上必须容纳全量数据。 要解决数据水平切分的问题，需要引入分片的概念。通过分片把一份完整的数据进行切分，不同的分片分布到不同的节点上。在通过Distributed表引擎把数据拼接起来一同使用。 Distributed表引擎本身不存储数据，有点类似于MyCat之于MySql，成为一种中间件，通过分布式逻辑表来写入、分发、路由来操作多台节点不同分片的分布式数据。 1.配置配置的位置还是在之前的metrika.xml，配置分片如下的结构 1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;yandex&gt;&lt;clickhouse_remote_servers&gt;&lt;gmall_cluster&gt; &lt;!-- 集群名称--&gt; &lt;shard&gt; &lt;!--集群的第一个分片--&gt;&lt;internal_replication&gt;true&lt;/internal_replication&gt; &lt;replica&gt; &lt;!—该分片的第一个副本--&gt; &lt;host&gt;hdp1&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;replica&gt; &lt;!—该分片的第二个副本--&gt; &lt;host&gt;hdp2&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;shard&gt; &lt;!--集群的第二个分片--&gt; &lt;internal_replication&gt;true&lt;/internal_replication&gt; &lt;replica&gt; &lt;!—该分片的第一个副本--&gt; &lt;host&gt;hdp3&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;replica&gt; &lt;!—该分片的第二个副本--&gt; &lt;host&gt;hdp4&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;shard&gt; &lt;!--集群的第三个分片--&gt; &lt;internal_replication&gt;true&lt;/internal_replication&gt; &lt;replica&gt; &lt;!—该分片的第一个副本--&gt; &lt;host&gt;hdp5&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;replica&gt; &lt;!—该分片的第二个副本--&gt; &lt;host&gt;hdp6&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;/shard&gt;&lt;/gmall_cluster&gt;&lt;/clickhouse_remote_servers&gt;&lt;/yandex&gt; 2.读写原理 3.三节点版本配置metrika.xml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&lt;yandex&gt;&lt;clickhouse_remote_servers&gt;&lt;gmall_cluster&gt; &lt;!-- 集群名称--&gt; &lt;shard&gt; &lt;!--集群的第一个分片--&gt;&lt;internal_replication&gt;true&lt;/internal_replication&gt; &lt;replica&gt; &lt;!—该分片的第一个副本--&gt; &lt;host&gt;hdp1&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;replica&gt; &lt;!—该分片的第二个副本--&gt; &lt;host&gt;hdp2&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;shard&gt; &lt;!--集群的第二个分片--&gt; &lt;internal_replication&gt;true&lt;/internal_replication&gt; &lt;replica&gt; &lt;!—该分片的第一个副本--&gt; &lt;host&gt;hdp3&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt;&lt;/shard&gt;&lt;/gmall_cluster&gt;&lt;/clickhouse_remote_servers&gt;&lt;zookeeper-servers&gt; &lt;node index=&quot;1&quot;&gt; &lt;host&gt;hadoop102&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;node index=&quot;2&quot;&gt; &lt;host&gt;hadoop103&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;node index=&quot;3&quot;&gt; &lt;host&gt;hadoop104&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt;&lt;/zookeeper-servers&gt;&lt;macros&gt;&lt;shard&gt;01&lt;/shard&gt; &lt;!—不同机器放的分片数不一样--&gt;&lt;replica&gt;rep_1_1&lt;/replica&gt; &lt;!—不同机器放的副本数不一样--&gt;&lt;/macros&gt;&lt;/yandex&gt; hdp1 hdp2 hdp3 01 rep_1_1 01 rep_1_2 02 rep_2_1 123456789 create table st_order_mt_0105 on cluster gmall_cluster ( id UInt32, sku_id String, total_amount Decimal(16,2), create_time Datetime) engine =ReplicatedMergeTree(&#x27;/clickhouse/tables/&#123;shard&#125;/st_order_mt_0105&#x27;,&#x27;&#123;replica&#125;&#x27;)partition by toYYYYMMDD(create_time) primary key (id) order by (id,sku_id); 4.Distribute 分布式表1234567create table st_order_mt_0105_all on cluster gmall_cluster( id UInt32, sku_id String, total_amount Decimal(16,2), create_time Datetime)engine = Distributed(gmall_cluster,test0105, st_order_mt_0105,hiveHash(sku_id)) 其中参数： Distributed( 集群名称，库名，本地表名，分片键) 分片键必须是整型数字 也可以rand() 插入数据 123456 insert into st_order_mt_0105_all values(201,&#x27;sku_001&#x27;,1000.00,&#x27;2020-06-01 12:00:00&#x27;) ,(202,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 12:00:00&#x27;),(203,&#x27;sku_004&#x27;,2500.00,&#x27;2020-06-01 12:00:00&#x27;),(204,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 12:00:00&#x27;)(205,&#x27;sku_003&#x27;,600.00,&#x27;2020-06-02 12:00:00&#x27;) 通过查询分布式表语句 1SELECT * FROM st_order_mt_all 和 本地表 1select * from st_order_mt; 来观察数据的分布是否正确。 八，java操作clickHouse1.依赖12345678910111213&lt;!-- 官方驱动,默认连接为HTTP协议，8123端口 --&gt;&lt;!-- &lt;dependency&gt;--&gt;&lt;!-- &lt;groupId&gt;ru.yandex.clickhouse&lt;/groupId&gt;--&gt;&lt;!-- &lt;artifactId&gt;clickhouse-jdbc&lt;/artifactId&gt;--&gt;&lt;!-- &lt;version&gt;0.1.52&lt;/version&gt;--&gt;&lt;!-- &lt;/dependency&gt;--&gt;&lt;!--两者不可共用--&gt;&lt;!-- 三方提供的驱动，默认连接协议为TCP，端口为9000 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.housepower&lt;/groupId&gt; &lt;artifactId&gt;clickhouse-native-jdbc&lt;/artifactId&gt; &lt;version&gt;1.6-stable&lt;/version&gt; &lt;/dependency&gt; 2.基本操作123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899/** * @author yhd * @since 2021/3/31 15:25 * @email yinhuidong1@xiaomi.com * @description 测试 java 连接 clickhouse 的基础操作 * @params * @return */@SpringBootTestclass ClickhouseApplicationTests &#123; /** * @return * @author yhd * @email yinhuidong1@xiaomi.com * @description 尝试获取连接并在default数据库创建一张表 * @params * @since 2021/3/31 15:20 */ @Test void contextLoads() throws Exception &#123; Class.forName(&quot;com.github.housepower.jdbc.ClickHouseDriver&quot;); Connection connection = DriverManager.getConnection(&quot;jdbc:clickhouse://121.199.31.160:9000&quot;); Statement statement = connection.createStatement(); statement.executeQuery(&quot;create table default.jdbc_example(day Date, name String, age UInt8) Engine=Log&quot;); &#125; /** * @return * @author yhd * @email yinhuidong1@xiaomi.com * @description 批量插入10条数据 * @params * @since 2021/3/31 15:20 */ @Test public void test() throws Exception &#123; Class.forName(&quot;com.github.housepower.jdbc.ClickHouseDriver&quot;); Connection connection = DriverManager.getConnection(&quot;jdbc:clickhouse://121.199.31.160:9000&quot;); PreparedStatement pstmt = connection.prepareStatement(&quot;insert into default.jdbc_example values(?, ?, ?)&quot;); // insert 10 records for (int i = 0; i &lt; 10; i++) &#123; pstmt.setDate(1, new Date(System.currentTimeMillis())); pstmt.setString(2, &quot;panda_&quot; + (i + 1)); pstmt.setInt(3, 18); pstmt.addBatch(); &#125; pstmt.executeBatch(); &#125; /** * @return * @author yhd * @email yinhuidong1@xiaomi.com * @description 查询 * @params * @since 2021/3/31 15:22 */ @Test public void test2() throws Exception &#123; Class.forName(&quot;com.github.housepower.jdbc.ClickHouseDriver&quot;); Connection connection = DriverManager.getConnection(&quot;jdbc:clickhouse://121.199.31.160:9000&quot;); Statement statement = connection.createStatement(); String sql = &quot;select * from default.jdbc_example&quot;; ResultSet rs = statement.executeQuery(sql); while (rs.next()) &#123; // ResultSet 的下标值从 1 开始，不可使用 0，否则越界，报 ArrayIndexOutOfBoundsException 异常 System.out.println(rs.getDate(1) + &quot;, &quot; + rs.getString(2) + &quot;, &quot; + rs.getInt(3)); &#125; &#125; /** * @author yhd * @since 2021/3/31 15:24 * @email yinhuidong1@xiaomi.com * @description 删除表操作 * @params * @return */ @Test public void test3()throws Exception &#123; Class.forName(&quot;com.github.housepower.jdbc.ClickHouseDriver&quot;); Connection connection = DriverManager.getConnection(&quot;jdbc:clickhouse://121.199.31.160:9000&quot;); Statement statement = connection.createStatement(); statement.executeQuery(&quot;drop table default.jdbc_example&quot;); &#125;&#125; 九，SpringBoot整合ClickHouse案例基于：Druid连接池和mybatis进行整合。Druid 1.1.10 版本 SQL Parser对clickhouse的开始提供支持。 1.依赖1234567891011121314151617181920 &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.2.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.1.4&lt;/version&gt; &lt;/dependency&gt;&lt;!-- 官方驱动,默认连接为HTTP协议，8123端口 --&gt; &lt;dependency&gt; &lt;groupId&gt;ru.yandex.clickhouse&lt;/groupId&gt; &lt;artifactId&gt;clickhouse-jdbc&lt;/artifactId&gt; &lt;version&gt;0.1.52&lt;/version&gt; &lt;/dependency&gt; 2.配置数据源1234567spring.datasource.type=com.alibaba.druid.pool.DruidDataSourcespring.datasource.click.driverClassName=ru.yandex.clickhouse.ClickHouseDriverspring.datasource.click.url=jdbc:clickhouse://121.199.31.160:8123/defaultspring.datasource.click.initialSize=10spring.datasource.click.maxActive=100spring.datasource.click.minIdle=10spring.datasource.click.maxWait=6000 3.代码配置123456789101112@Data@Component@ConfigurationProperties(prefix = &quot;spring.datasource.click&quot;)public class ClickHouseProperties &#123; private String driverClassName ; private String url ; private Integer initialSize ; private Integer maxActive ; private Integer minIdle ; private Integer maxWait ;&#125; 12345678910111213141516@SpringBootConfigurationpublic class DruidConfig &#123; @Resource private ClickHouseProperties clickHouseProperties ; @Bean public DataSource dataSource() &#123; DruidDataSource datasource = new DruidDataSource(); datasource.setUrl(clickHouseProperties.getUrl()); datasource.setDriverClassName(clickHouseProperties.getDriverClassName()); datasource.setInitialSize(clickHouseProperties.getInitialSize()); datasource.setMinIdle(clickHouseProperties.getMinIdle()); datasource.setMaxActive(clickHouseProperties.getMaxActive()); datasource.setMaxWait(clickHouseProperties.getMaxWait()); return datasource; &#125;&#125;","categories":[{"name":"ClickHouse","slug":"ClickHouse","permalink":"https://yinhuidong.github.io/categories/ClickHouse/"}],"tags":[{"name":"ClickHouse","slug":"ClickHouse","permalink":"https://yinhuidong.github.io/tags/ClickHouse/"}]},{"title":"ThreadLocal","slug":"JUC/ThreadLocal","date":"2022-01-11T11:08:04.200Z","updated":"2022-01-11T11:31:15.889Z","comments":true,"path":"2022/01/11/JUC/ThreadLocal/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/JUC/ThreadLocal/","excerpt":"","text":"一，TL的基本使用与原理为线程创建独一份的副本数据。 1.基本使用12345678910111213141516171819202122232425262728293031323334/** * @author 二十 * @since 2021/8/28 11:19 下午 */public class TlTest &#123; private static AtomicInteger id = new AtomicInteger(0); private static ThreadLocal&lt;Integer&gt; tl = ThreadLocal.withInitial(()-&gt;id.getAndIncrement()); private static CountDownLatch count = new CountDownLatch(3); public static void main(String[] args)throws Exception &#123; new Thread(()-&gt;&#123; System.out.println(tl.get()+&quot; &quot;+Thread.currentThread().getName()); tl.remove(); count.countDown(); &#125;,&quot;A&quot;).start(); new Thread(()-&gt;&#123; System.out.println(tl.get()+&quot; &quot;+Thread.currentThread().getName()); tl.remove(); count.countDown(); &#125;,&quot;B&quot;).start(); new Thread(()-&gt;&#123; System.out.println(tl.get()+&quot; &quot;+Thread.currentThread().getName()); tl.remove(); count.countDown(); &#125;,&quot;C&quot;).start(); count.await(); &#125;&#125; 2.原理分析​ 里面维护一个ThreadLocalMap结构，每一个元素对应一个桶位。 ​ 使用ThreadLocal定义的变量，将指向当前线程本地的一个LocalMap空间。 ​ ThreadLocal变量作为key，其内容作为value，保存在本地。 ​ 多线程对ThreadLocal对象进行操作，实际上是对各自的本地变量进行操作，不存在线程安全问题。 ​ 假设一个类里面定义了三个threadlocal，三个线程来访问这个类，每个线程本地会维护一个threadlocalmap，每一个map里面会有三个entry，key是threadlocal对象，value是threadlocal里面set的值。 二，TL源码1.属性1234567891011121314151617181920212223242526 /** * 线程获取Threadlocal.get()时，如果是第一次在某个threadlocal对象上get，会给当前线程分配一个value， * 这个value和当前的threadlocal对象被包装成一个entry，其中key=threadlocal对象， * value=threadlocal对象给当前线程生成的value。这个entry存放到哪个位置与这个value有关。 */private final int threadLocalHashCode = nextHashCode();//创建threadlocal对象时会使用到，每创建一个threadlocal对象就会使用它分配一个hash值给对象。private static AtomicInteger nextHashCode = new AtomicInteger();//每创建一个threadlocal对象，这个nextHashCode就会增长0x61c88647。private static final int HASH_INCREMENT = 0x61c88647;//创建新的threadlocal对象的时候，给当前对象分配hash的时候用到。private static int nextHashCode() &#123; return nextHashCode.getAndAdd(HASH_INCREMENT);&#125;//留给子类重写扩展的protected T initialValue() &#123; return null;&#125;//带初始化值得threadlocalpublic static &lt;S&gt; ThreadLocal&lt;S&gt; withInitial(Supplier&lt;? extends S&gt; supplier) &#123; return new SuppliedThreadLocal&lt;&gt;(supplier);&#125;public ThreadLocal() &#123;&#125; 2.get()1234567891011121314151617public T get() &#123; //获取当前线程 Thread t = Thread.currentThread(); //根据当前线程获取对应的map ThreadLocalMap map = getMap(t); if (map != null) &#123; //已经初始化 //根据当前threadlocal对象获取entry节点 ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) &#123; //节点初始化过 //获取entry的value并返回 T result = (T)e.value; return result; &#125; &#125; //走到这里说明map尚未初始化获取entry尚未初始化 return setInitialValue();&#125; 2.1 setInitialValue()1234567891011121314151617private T setInitialValue() &#123; //获取初始值，留给子类重写 T value = initialValue(); //获取当前线程 Thread t = Thread.currentThread(); //获取当前线程对应的map ThreadLocalMap map = getMap(t); //如果map初始化过 if (map != null) //map里面放入当前对象和value map.set(this, value); else //map尚未初始化过 //初始化map--直接new一个并放入当前对象和value createMap(t, value); //返回value return value;&#125; 2.2 getMap（）1234//返回当前线程的threadLocalsThreadLocalMap getMap(Thread t) &#123; return t.threadLocals;&#125; 2.3 createMap（）1234//利用构造器初始化threadLocals并将当前线程和线程对应的value设置进去void createMap(Thread t, T firstValue) &#123; t.threadLocals = new ThreadLocalMap(this, firstValue);&#125; 3.set()12345678public void set(T value) &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) //当前线程对应的map已经初始化 map.set(this, value); //map放入值 else //map未初始化 createMap(t, value); //初始化map&#125; 4.remove()12345public void remove() &#123; ThreadLocalMap m = getMap(Thread.currentThread()); if (m != null) //map已经初始化 m.remove(this); //调用map的remove移除掉当前对象对应的entry&#125; 5.内部类ThreadLocalMap1234567891011121314151617181920212223242526272829303132333435363738394041424344454647//threadlocalmap里面的key是弱引用 ，key=threadlocal对象//value是强引用，value保存的是threadlocal对象与当前线程关联的value//这样设计的好处是为了防止内存泄漏static class Entry extends WeakReference&lt;ThreadLocal&lt;?&gt;&gt; &#123; Object value; Entry(ThreadLocal&lt;?&gt; k, Object v) &#123; super(k); value = v; &#125;&#125;//map的初始化容量为16private static final int INITIAL_CAPACITY = 16;//map里面的entry桶位列表private Entry[] table;//列表容量private int size = 0;/** * 扩容阈值 当前数组长度的三分之二 */private int threshold; // Default to 0//将扩容阈值设置为当前数组长度的三分之二private void setThreshold(int len) &#123; threshold = len * 2 / 3;&#125;//获取下一个位置private static int nextIndex(int i, int len) &#123; return ((i + 1 &lt; len) ? i + 1 : 0);&#125;//获取下一个位置private static int prevIndex(int i, int len) &#123; return ((i - 1 &gt;= 0) ? i - 1 : len - 1);&#125;//其实从上层的api可以发现这里其实是延迟初始化，只有线程第一次调用threadlocal的//get或者set的时候才会初始化。ThreadLocalMap(ThreadLocal&lt;?&gt; firstKey, Object firstValue) &#123; //初始化散列表，长度为16 table = new Entry[INITIAL_CAPACITY]; //计算entry的存储位置 int i = firstKey.threadLocalHashCode &amp; (INITIAL_CAPACITY - 1); //创建新的entry table[i] = new Entry(firstKey, firstValue); //占用量设置为1 size = 1; //修改扩容阈值为初始化长度 setThreshold(INITIAL_CAPACITY);&#125; 5.1 getEntry()123456789private Entry getEntry(ThreadLocal&lt;?&gt; key) &#123; //根据当前线程的threadlocal对象获取entry的存储位置 int i = key.threadLocalHashCode &amp; (table.length - 1); Entry e = table[i]; if (e != null &amp;&amp; e.get() == key) //校验entry是不是已经丢了，或者已经被覆盖 return e; else //执行打这里说明entry已经丢了或者被发生了hash冲突，继续向后寻找 return getEntryAfterMiss(key, i, e);&#125; 5.2 getEntryAfterMiss()123456789101112131415161718192021222324private Entry getEntryAfterMiss(ThreadLocal&lt;?&gt; key, int i, Entry e) &#123; //获取散列表 Entry[] tab = table; //获取散列表的长度 int len = tab.length; //如果entry不为空，那就说明entryhash冲突了 while (e != null) &#123; //获取entry对应的threadlocal对象 ThreadLocal&lt;?&gt; k = e.get(); //说明key对应的threadlocal对象已经被回收了，当前entry属于脏数据 if (k == key) //直接返回 return e; //如果key==null，说明key对应的threadlocal对象已经被回收了，当前entry属于脏数据 if (k == null) //做一次探测式过期清理 expungeStaleEntry(i); else //执行到这里说明发生了hash冲突，继续从当前位置往后寻找 i = nextIndex(i, len); e = tab[i]; &#125; //说明entry过期了，直接返回null return null;&#125; 5.3 expungeStaleEntry() 探测式过期清理123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051private int expungeStaleEntry(int staleSlot) &#123; //获取散列表 Entry[] tab = table; //获取散列表的长度 int len = tab.length; //因为此处threadlocal对象已经被回收，所以直接将value设置为null，help GC tab[staleSlot].value = null; //再讲当前桶位设置为空 tab[staleSlot] = null; /** * 为什么这里要分两次设置为null？ * 因为key本身是弱引用，但是value是强引用，如果直接回收桶位，value无法直接被回收 */ //散列表的占用长度-1 size--; Entry e; int i; //从当前节点所在位置的下一个位置直到最后循环， for (i = nextIndex(staleSlot, len); //停止条件是当前索引对应桶位=null (e = tab[i]) != null; //循环条件是每次索引+1 i = nextIndex(i, len)) &#123; //获取entry的threadlocal对象 ThreadLocal&lt;?&gt; k = e.get(); if (k == null) &#123;//如果对象为空，说明已经过期了，entry是脏数据 //回收 e.value = null; tab[i] = null; size--; &#125; else &#123;//此时说明entry不是脏数据 //计算threadlocal对象在散列表的新索引，为啥重新计算？ //因为当前get到了脏数据，刚刚从散列表移除，所以散列表的占用量已经发生了变化 int h = k.threadLocalHashCode &amp; (len - 1); //如果没有发生hash冲突 if (h != i) &#123; //将原来的桶位释放 tab[i] = null; //寻找存放位置，直到所在桶位为空，因为可能计算出的位置发生了hash冲突， //这个时候，就要索引下推到下一桶位 while (tab[h] != null) h = nextIndex(h, len); //将entry放到新的桶位 tab[h] = e; &#125; &#125; &#125; //返回最后处理的索引处 return i;&#125; 5.4 set()1234567891011121314151617181920212223242526272829private void set(ThreadLocal&lt;?&gt; key, Object value) &#123; Entry[] tab = table; int len = tab.length; int i = key.threadLocalHashCode &amp; (len-1); for (Entry e = tab[i];//当前threadlocal对象所对应的节点 e != null; //终止条件是entry为空，说明这个桶位能存放entry e = tab[i = nextIndex(i, len)]) &#123; //桶位下推 ThreadLocal&lt;?&gt; k = e.get(); //如果当前对象所对应的桶位有值，且当前桶位的key是当前对象， //说明这是一次值重置，直接覆盖旧的值即可 if (k == key) &#123; e.value = value; return; &#125; //如果k==null，说明当前位置对应的entry是过期的， if (k == null) &#123; replaceStaleEntry(key, value, i); return; &#125; &#125; //来到这里的条件：这次操作不是一次对已经有的值得覆盖，或者已经找到了应该存放当前entry的桶位 tab[i] = new Entry(key, value); int sz = ++size; //如果达到了扩容的条件，进行扩容操作 if (!cleanSomeSlots(i, sz) &amp;&amp; sz &gt;= threshold) rehash();&#125; 5.5 replaceStaleEntry()替换过期entry123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051//替换过期entryprivate void replaceStaleEntry(ThreadLocal&lt;?&gt; key, Object value, int staleSlot) &#123; Entry[] tab = table; int len = tab.length; Entry e; //进入这个方法的条件说明：当前位置的节点其实是过期的，但是还没来得及回收 int slotToExpunge = staleSlot; //当前桶位的索引 //从当前位置向前清理 for (int i = prevIndex(staleSlot, len); //i=当前索引的前一个索引 (e = tab[i]) != null; //终止条件是索引所在的桶位有数据 i = prevIndex(i, len)) //循环条件是每次往前一个桶位 //说明是过期的，那就继续往前清理 if (e.get() == null) slotToExpunge = i; //从当前位置向后清理 for (int i = nextIndex(staleSlot, len); (e = tab[i]) != null; i = nextIndex(i, len)) &#123; ThreadLocal&lt;?&gt; k = e.get(); //如果当前位置的key和当前threadlocal对象一致 if (k == key) &#123; //进行值覆盖操作 e.value = value; //将过期数据放到当前循环到的table[i] tab[i] = tab[staleSlot]; //这里的逻辑其实就是进行一下位置优化 tab[staleSlot] = e; //说明上面的循环并没有找到过期数据 if (slotToExpunge == staleSlot) //吧探测的开始位置改成当前位置 slotToExpunge = i; //进行探测式过期清理 cleanSomeSlots(expungeStaleEntry(slotToExpunge), len); return; &#125; //当前遍历entry是一个过期数据 &amp;&amp; 往前找过期数据没找到 if (k == null &amp;&amp; slotToExpunge == staleSlot) //更新探测位置为当前位置 slotToExpunge = i; &#125; //将新的值放入当前节点 tab[staleSlot].value = null; tab[staleSlot] = new Entry(key, value); //如果两个索引不相等，就继续清理 if (slotToExpunge != staleSlot) cleanSomeSlots(expungeStaleEntry(slotToExpunge), len);&#125; 5.6 cleanSomeSlots()启发式清理工作123456789101112131415161718192021//启发式清理工作 i 开始清理位置 n 结束条件，数组长度private boolean cleanSomeSlots(int i, int n) &#123; boolean removed = false; Entry[] tab = table; int len = tab.length; do &#123; //获取当前i的下一个下标 i = nextIndex(i, len); //获取当前下标为I的元素 Entry e = tab[i]; //断定为过期元素 if (e != null &amp;&amp; e.get() == null) &#123; n = len;//更新数组长度 removed = true; //从当前过期位置开始一次谈测试清理工作 i = expungeStaleEntry(i); &#125; &#125; while ( (n &gt;&gt;&gt;= 1) != 0);//假设table.length=16 return removed;&#125; 5.7 rehash()12345678private void rehash() &#123; //遍历，探测式清理，干掉所有过期数据 expungeStaleEntries(); //仍然达到扩容条件 if (size &gt;= threshold - threshold / 4) //扩容 resize();&#125; 5.8 resize()123456789101112131415161718192021222324252627private void resize() &#123; Entry[] oldTab = table; int oldLen = oldTab.length; int newLen = oldLen * 2; //扩容为原来的2倍 Entry[] newTab = new Entry[newLen]; int count = 0; for (int j = 0; j &lt; oldLen; ++j) &#123; Entry e = oldTab[j]; //访问old表指定位置的data if (e != null) &#123; //data存在 ThreadLocal&lt;?&gt; k = e.get(); if (k == null) &#123; //过期数据 e.value = null; // Help the GC &#125; else &#123; int h = k.threadLocalHashCode &amp; (newLen - 1);//重新计算hash值 while (newTab[h] != null) //获取到一个最近的，可以使用的位置 h = nextIndex(h, newLen); newTab[h] = e; //数据迁移 count++; &#125; &#125; &#125; setThreshold(newLen);//设置下一次扩容的指标 size = count; table = newTab;&#125; 5.9 remove()12345678910111213141516private void remove(ThreadLocal&lt;?&gt; key) &#123; Entry[] tab = table; int len = tab.length; int i = key.threadLocalHashCode &amp; (len-1); for (Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) &#123; //从当前位置开始，如果桶位是空，就去下一个 //如果不为空的桶位与当前线程的threadlocal对象一致 if (e.get() == key) &#123; e.clear(); //干掉key的引用 expungeStaleEntry(i); //探测式过期清理 return; &#125; &#125;&#125;","categories":[{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"阻塞队列","slug":"JUC/阻塞队列","date":"2022-01-11T11:07:55.302Z","updated":"2022-01-11T11:25:05.613Z","comments":true,"path":"2022/01/11/JUC/阻塞队列/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/JUC/%E9%98%BB%E5%A1%9E%E9%98%9F%E5%88%97/","excerpt":"","text":"把多线程环境比作是分布式的话，那么线程与线程之间是不是也可以使用这种消息队列的方式进行数据通信和解耦呢？​ 一，阻塞队列使用案例1.注册成功后增加积分假如模拟一个场景，就是用户注册的时候，在注册成功以后发放积分。这个场景在一般来说，会这么去实现：但是实际上，我们需要考虑两个问题： 性能，在注册这个环节里面，假如添加用户需要花费 1 秒钟，增加积分需要花费 1 秒钟，那么整个注册结果的返回就可能需要大于 2 秒，虽然影响不是很大，但是在量比较大的时候，我们也需要做一些优化。 耦合，添加用户和增加积分，可以认为是两个领域，也就是说，增加积分并不是注册必须要具备的功能，但是一旦增加积分这个逻辑出现异常，就会导致注册失败。这种耦合在程序设计的时候是一定要规避的。 因此我们可以通过异步的方式来实现。​ 2.改造之前的代码逻辑12345678910111213141516171819202122232425262728293031323334353637public class UserService &#123; public boolean register() &#123; User user = new User(); user.setName(&quot;Mic&quot;); addUser(user); sendPoints(user); return true; &#125; public static void main(String[] args) &#123; new UserService().register(); &#125; private void addUser(User user) &#123; System.out.println(&quot; 添加用户：&quot; + user); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; private void sendPoints(User user) &#123; System.out.println(&quot; 发 送 积 分 给 指 定 用 户:&quot; + user); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125;@Dataclass User &#123; private String name;&#125; 3.改造之后的逻辑123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public class UserService &#123; private final ExecutorService single = Executors.newSingleThreadExecutor(); private volatile boolean isRunning = true; ArrayBlockingQueue arrayBlockingQueue = new ArrayBlockingQueue(10); &#123; init(); &#125; public void init() &#123; single.execute(() -&gt; &#123; while (isRunning) &#123; try &#123; User user = (User) arrayBlockingQueue.take();// 阻塞的方式获取队列中的数据 sendPoints(user); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; public boolean register() &#123; User user = new User(); user.setName(&quot;Mic&quot;); addUser(user); arrayBlockingQueue.add(user);// 添加到异步队列 return true; &#125; public static void main(String[] args) &#123; new UserService().register(); &#125; private void addUser(User user) &#123; System.out.println(&quot; 添加用户：&quot; + user); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; private void sendPoints(User user) &#123; System.out.println(&quot; 发 送 积 分 给 指 定 用 户:&quot; + user); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125;@Dataclass User &#123; private String name;&#125; 优化以后，整个流程就变成了这样我们使用了 ArrayBlockingQueue 基于数组的阻塞队列，来优化代码的执行逻辑。 4.阻塞队列的应用场景阻塞队列这块的应用场景，比较多的仍然是对于生产者消费者场景的应用，但是由于分布式架构的普及，更多的关注在分布式消息队列上。所以其实如果把阻塞队列比作成分布式消息队列的话，那么所谓的生产者和消费者其实就是基于阻塞队列的解耦。 另外，阻塞队列是一个 fifo 的队列，所以对于希望在线程级别需要实现对目标服务的顺序访问的场景中，也可以使用。 二,J.U.C 中的阻塞队列1.JUC中提供的阻塞队列在 Java8 中，提供了 7 个阻塞队列。 ArrayBlockingQueue 数组实现的有界阻塞队列, 此队列按照先进先出（FIFO）的原则对元素进行排序。 LinkedBlockingQueue 链表实现的有界阻塞队列, 此队列的默认和最大长度为Integer.MAX_VALUE。此队列按照先进先出的原则对元素进行排序 PriorityBlockingQueue 支持优先级排序的无界阻塞队列, 默认情况下元素采取自然顺序升序排列。也可以自定义类实现 compareTo()方法来指定元素排序规则，或者初始化 PriorityBlockingQueue 时，指定构造参数 Comparator 来对元素进行排序。 DelayQueue 优先级队列实现的无界阻塞队列 SynchronousQueue 不存储元素的阻塞队列, 每一个 put 操作必须等待一个 take 操作，否则不能继续添加元素。 LinkedTransferQueue 链表实现的无界阻塞队列 LinkedBlockingDeque 链表实现的双向阻塞队列 2.阻塞队列的操作方法在阻塞队列中，提供了四种处理方式 2.1插入操作add(e) ：添加元素到队列中，如果队列满了，继续插入元素会报错，IllegalStateException。 offer(e) : 添加元素到队列，同时会返回元素是否插入成功的状态，如果成功则返回 true。 put(e) ：当阻塞队列满了以后，生产者继续通过 put添加元素，队列会一直阻塞生产者线程，直到队列可用。 offer(e,time,unit) ：当阻塞队列满了以后继续添加元素，生产者线程会被阻塞指定时间，如果超时，则线程直接退出。 2.2移除操作remove()：当队列为空时，调用 remove 会返回 false，如果元素移除成功，则返回 true。 poll(): 当队列中存在元素，则从队列中取出一个元素，如果队列为空，则直接返回 null。 take()：基于阻塞的方式获取队列中的元素，如果队列为空，则 take 方法会一直阻塞，直到队列中有新的数据可以消费。 poll(time,unit)：带超时机制的获取数据，如果队列为空，则会等待指定的时间再去获取元素返回。​ 三，ArrayBlockingQueue源码1.构造方法 ArrayBlockingQueue 提供了三个构造方法，分别如下。 capacity： 表示数组的长度，也就是队列的长度。 fair：表示是否为公平的阻塞队列，默认情况下构造的是非公平的阻塞队列。 ​ 1234567891011121314151617181920212223242526272829303132333435public ArrayBlockingQueue(int capacity) &#123; this(capacity, false);&#125;public ArrayBlockingQueue(int capacity, boolean fair) &#123; if (capacity &lt;= 0) throw new IllegalArgumentException(); this.items = new Object[capacity]; lock = new ReentrantLock(fair);//重入锁，出队和入队持有这一把锁 notEmpty = lock.newCondition();//初始化非空等待队列 notFull = lock.newCondition();//初始化非满等待队列&#125;public ArrayBlockingQueue(int capacity, boolean fair, Collection&lt;? extends E&gt; c) &#123; this(capacity, fair); final ReentrantLock lock = this.lock; lock.lock(); // Lock only for visibility, not mutual exclusion try &#123; int i = 0; try &#123; for (E e : c) &#123; checkNotNull(e); items[i++] = e; &#125; &#125; catch (ArrayIndexOutOfBoundsException ex) &#123; throw new IllegalArgumentException(); &#125; count = i; putIndex = (i == capacity) ? 0 : i; &#125; finally &#123; lock.unlock(); &#125;&#125; items 构造以后，大概是一个这样的数组结构：​ 2.add以 add 方法作为入口，在 add 方法中会调用父类的 add 方法，也就是 AbstractQueue.​ 12345678910public boolean add(E e) &#123; return super.add(e);&#125;======================================================public boolean add(E e) &#123; if (offer(e)) return true; else throw new IllegalStateException(&quot;Queue full&quot;);&#125; 从父类的 add 方法可以看到，这里做了一个队列是否满了的判断，如果队列满了直接抛出一个异常。​ 3.offer​ 123456789101112131415161718public boolean offer(E e) &#123; //校验放入队列的元素如果为null，抛出空指针异常 checkNotNull(e); final ReentrantLock lock = this.lock; lock.lock(); try &#123; //如果队列已经满了，返回false if (count == items.length) return false; else &#123; //否则，执行入队逻辑 enqueue(e); return true; &#125; &#125; finally &#123; lock.unlock(); &#125;&#125; 4.checkNotNull1234private static void checkNotNull(Object v) &#123; if (v == null) throw new NullPointerException();&#125; 5.enqueue1234567891011121314private void enqueue(E x) &#123; //当前队列的引用 final Object[] items = this.items; //元素入队 items[putIndex] = x; //如果下一个元素存放位置大于队列长度，把队列长度置为0 if (++putIndex == items.length) putIndex = 0; //元素个数+1 count++; //唤醒处于等待状态下的线程，表示当前队列中的元素不为空,如果存在消费者线程阻塞，就可以开始取出元素 notEmpty.signal();&#125; putIndex 为什么会在等于数组长度的时候重新设置为 0？​ 因为 ArrayBlockingQueue 是一个 FIFO 的队列，队列添加元素时，是从队尾获取 putIndex 来存储元素，当 putIndex等于数组长度时，下次就需要从数组头部开始添加了。 下面这个图模拟了添加到不同长度的元素时，putIndex 的变化，当 putIndex 等于数组长度时，不可能让 putIndex 继续累加，否则会超出数组初始化的容量大小。 当元素满了以后是无法继续添加的，因为会报错。 队列中的元素肯定会有一个消费者线程通过 take或者其他方法来获取数据，而获取数据的同时元素也会从队列中移除。 ​ 6.putput 方法和 add 方法功能一样，差异是 put 方法如果队列满了，会阻塞。 1234567891011121314public void put(E e) throws InterruptedException &#123; checkNotNull(e); final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try &#123; //元素个数如果等于数组的长度，阻塞当前线程 while (count == items.length) notFull.await(); //否则，入队逻辑 enqueue(e); &#125; finally &#123; lock.unlock(); &#125;&#125; 7.taketake 方法是一种阻塞获取队列中元素的方法。​ 它的实现原理很简单，有就删除没有就阻塞，注意这个阻塞是可以中断的，如果队列没有数据那么就加入 notEmpty条件队列等待(有数据就直接取走，方法结束)，如果有新的put 线程添加了数据，那么 put 操作将会唤醒 take 线程，执行 take 操作。​ 12345678910111213public E take() throws InterruptedException &#123; final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try &#123; //队列元素个数==0 阻塞 while (count == 0) notEmpty.await(); //否则执行出队逻辑 return dequeue(); &#125; finally &#123; lock.unlock(); &#125;&#125; 如果队列中添加了元素，那么这个时候，会在 enqueue 中调用 notempty.signal 唤醒 take 线程来获得元素 8.dequeue这个是出队列的方法，主要是删除队列头部的元素并发返回给客户端。 123456789101112131415161718private E dequeue() &#123; //获取队列引用 final Object[] items = this.items; //拿出来一个元素 E x = (E) items[takeIndex]; //将拿出的元素的索引位置设置为null items[takeIndex] = null; //如果到了数组长度 从零开始 if (++takeIndex == items.length) takeIndex = 0; //队列元素个数-1 count--; if (itrs != null) itrs.elementDequeued();//更新迭代器中的元素个数 //触发 因为队列满了以后导致的被阻塞的线程 notFull.signal(); return x;&#125; 9.elementDequeued​ ArrayBlockingQueue 中，实现了迭代器的功能，也就是可以通过迭代器来遍历阻塞队列中的元素。 所以 itrs.elementDequeued() 是用来更新迭代器中的元素数据的。 takeIndex 的索引变化图如下，同时随着数据的移除，会唤醒处于 put 阻塞状态下的线程来继续添加数据。 10.removeremove 方法是移除一个指定元素。看看它的实现代码： 1234567891011121314151617181920212223242526272829303132public boolean remove(Object o) &#123; //判空 if (o == null) return false; //获取队列的引用 final Object[] items = this.items; final ReentrantLock lock = this.lock; lock.lock(); try &#123; //如果队列中元素个数大于0 if (count &gt; 0) &#123; //获取上一次放入的元素的索引位置 final int putIndex = this.putIndex; //获取上一次取出的元素的索引的位置 int i = takeIndex; do &#123; //遍历每一个元素，如果找到了，就移除元素 if (o.equals(items[i])) &#123; removeAt(i); return true; &#125; //这个的逻辑有啥用？ //可能存在这样一种情况： // 1 null 2 3 4 if (++i == items.length) i = 0; &#125; while (i != putIndex); &#125; return false; &#125; finally &#123; lock.unlock(); &#125;&#125; 11.removeAt123456789101112131415161718192021222324252627282930313233343536373839404142void removeAt(final int removeIndex) &#123; //获取队列的引用 final Object[] items = this.items; //如果要删除的节点恰好是下一个获取元素要拿的索引位置，直接干掉就中 if (removeIndex == takeIndex) &#123; // removing front item; just advance items[takeIndex] = null; if (++takeIndex == items.length) takeIndex = 0; count--; if (itrs != null) itrs.elementDequeued(); &#125; else &#123; //这个时候就需要轮训删除 final int putIndex = this.putIndex; //自旋 for (int i = removeIndex;;) &#123; //删除索引的下一个索引 int next = i + 1; //如果到头了 ，意思就是 ， 那就得 从头再来 if (next == items.length) next = 0; // 断流了 得跳过去 if (next != putIndex) &#123; items[i] = items[next]; i = next; &#125; else &#123; //找到了 删除 退出 items[i] = null; this.putIndex = i; break; &#125; &#125; //元素个数-- count--; //处理迭代器的逻辑 if (itrs != null) itrs.removedAt(removeIndex); &#125; //将往队列放元素，但是因为队列满了阻塞的线程唤醒一个，当然了，也可能队列没有线程 notFull.signal();&#125;","categories":[{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"ThreadPoolExecutor","slug":"JUC/ThreadPoolExecutor","date":"2022-01-11T11:07:47.466Z","updated":"2022-01-11T11:31:41.419Z","comments":true,"path":"2022/01/11/JUC/ThreadPoolExecutor/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/JUC/ThreadPoolExecutor/","excerpt":"","text":"一，线程池的基本使用1.线程池的介绍线程复用，控制最大并发数，管理线程。​ 优点：提高响应速度，避免每次都去创建线程。便于管理，降低资源消耗。​ 2.常用的线程池 Executors.newFixedThreadPool();执行长期任务性能好，创建一个线程池，一池有N个固定的线程，有固定的线程数的线程 Executors.newSingleThreadExecutor();一个任务一个任务的执行，一池一线程 Executors.newCachedThreadPool();执行很多短期异步任务，线程池根据需要创建新线程，但在先前构建的线程可用时将重用它们。可扩容，遇强则强。 ​ 线程池的核心其实都是同一个类ThreadPoolExecutor。​ 注意​ 线程池不允许使用Executors去创建，而是通过ThreadPoolExecutor的方式，规避资源消耗。 说明：Executors返回的线程池对象的弊端如下： FixedThreadPool和SingleThreadPool；允许的请求队列长度为Integer的最大值，可能会堆积大量的请求，从而导致OOM。 CachedThreadPool和ScheduledThreadPool允许的创建线程数量为Integer最大值，可能会创建大量的线程，从而导致OOM。 123456789101112131415161718public static void main(String[] args) &#123; ExecutorService pool1 = Executors.newFixedThreadPool(5);//一个银行网点5个受理业务窗口 ExecutorService pool2 = Executors.newSingleThreadExecutor();//一个银行网点1个受理业务窗口 ExecutorService pool3 = Executors.newCachedThreadPool();//一个银行网点n个受理业务窗口 //3个顾客 try &#123; for (int i = 0; i &lt; 30; i++) &#123; pool3.execute(()-&gt;&#123; System.out.println(Thread.currentThread().getName()+&quot;\\t线程办理业务！&quot;); &#125;); //pool1.submit(()-&gt;&#123;&#125;); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; pool3.shutdown(); &#125;&#125; 3.线程池的7大参数 int corePoolSize, 线程池中的常驻核心线程数 ​ int maximumPoolSize, 能够容纳同时执行的最大线程数，必须大于1 ​ long keepAliveTime, 多余空闲线程存活时间 ​ TimeUnit unit, 上个参数的单位 ​ BlockingQueue workQueue, 任务队列，被提交但是尚未执行的任务 ​ ThreadFactory threadFactory, 表示生成线程池中工作线程的线程工厂，用于创建线程，一般默认 ​ RejectedExecutionHandler handler，拒绝策略，表示当队列满了，并且工作线程大于等于线程池的最大连接数时，如何来拒绝请求执行的runnable的策略。3.1 生产中线程池参数如何设置​ 为什么IO 密集型的设置为 2n, 计算密集型设置为 n+1不对？ 因为核心线程数设置多少要具体情况具体分析,使用线程池的业务场景不同,解决方案自然是不一样的。​ 场景假设​ 假设现在要给 100w 用户发放优惠券,通过线程池异步发送 ​ 假设某线程池执行发优惠券的任务共耗时 50ms,其中 45ms 在io, 5ms 在进行计算(真正的 io 耗时 计算耗时可以通过 记录log 判断时间差值计算出来 取平均值即可 ) ​ 如何设置线程池的参数快速的将这 100w 张券发完？ ​ 核心线程数 = CPU核数 * ((Io耗时 / 计算耗时) + 1) 核心线程数 = 8C * ((45ms / 5ms) +1 ) = 80个 45ms / 5ms 是什么意思？​ CPU 在等待 IO 返回时完全可以将 CPU 时间片拿出来去做其他的计算,45ms 可以多处理 9 个计算任务,再加上原本就有一个 5ms 在计算,也就是说: 一个CPU 核在执行这个 50ms 发券任务时,可以并发的起10个线程去处理任务！那8C CPU 最多同时可以有 8个核心并行的处理任务, 8 _ 10 = 80，一秒钟一个线程可以处理 1000ms / 50ms = 20个任务可以算出线程池执行任务的峰值 qps = 20 _ 80 = 1600，发完100w 张券所需时间: 100w / 1600 = 625S,也就是说大概 10分钟左右就能发完 100w 张券。​ 不太正确的结论: 核心线程数在处理这个任务的情况下可以设置为 80 用来极限的压榨机器CPU 的性能。​ 核心线程数设置为 80,这几乎吃完了所有的 CPU 时间片, CPU 的负载将会达到 100% ; 试想一下生产环境如果你的机器 CPU 负载是 100% , 慌不慌？(CPU 负载打满机器不会宕机, 但没有 CPU 资源来处理用户的请求,表现为服务假死/机器请求半天无反应)​ 设置线程池核心线程数要考虑 CPU 的使用要素​ 每台机器操作系统需要消耗一些 CPU 资源; 假设用了 2% 的CPU 资源; ​ 如果是面向用户的服务,处理用户的请求也是要消耗CPU 资源的,可以通过一些监控系统,看看平时 CPU 在繁忙时间段的负载是多少; 假设用了 10% 的资源; ​ 如果除了发券任务的线程池还有其他线程池在运行,就得把其他线程池消耗的CPU资源也算上,假设用了 13% 的资源; ​ 实际情况一些中间件框架也会用线程池,也会吃一些CPU 资源。 ​ 为什么用线程池没考虑上下文的切换？​ 1ms = 1000us, 一次上下文的切换大概是 1us, 上下文切换的时间跟执行任务的时间比起来可以忽略不计。​ 结论 : CPU核数 * ((Io耗时 / 计算耗时) + 1)​ 这是机器 CPU 负载 100% 时极限的值, 乘以期望的 CPU 负载百分比即可算出实际情况最佳的线程数。​ 3.2 8C16G 的机器需要几台可以抗起 3W 的qps？假设一个 用户领券系统的 qps 在3w左右大部分服务通常的部署在 Tomcat 上, Tomcat 内部也是通过线程来处理用户的请求,Tomcat 也是通过线程池来管理线程, 实际上算出 Tomcat 实际的并发和理想状态能支持的的并发就好了。​ 上个问题分析出来发券接口 50ms 耗时, 8C 的CPU 占用 100%, 不考虑内存 磁盘 网络等其他开销, 线程池极限的QPS 是1600, 这里也不考虑有没有其他线程池或者七七八八的东西消耗 CPU 资源了。假设 CPU 只能维持在 70% 左右的负载；单台机器的 qps 就只能有 1600 * 70% = 1120,就算 1100，3w / 1100 = 27.27 向上取整 大概需要 28 台机器。作为一个有经验的开发人员实际部署的时候绝对要多扩容几台服务器来兜底, 推荐部署 32 - 36 台机器分两个集群部署。​ 3.3 线程池可以先启动最大线程数再将任务放到阻塞队列里么？​ 启动最大线程数再将任务放到阻塞队列的诀窍就在 workQueue 的 offer 方法;我们可以用自己实现的阻塞队列在重写 offer 方法; 在 offer 方法中判断 当前线程数是否大于等于最大线程数，如果不大于就返回 false, 这样就跳过了 execute 方法的第二步, 来到了第三步的创建最大线程数的逻辑。dubbo就是这么干的。​ 4.线程池的工作原理 在创建了线程池后，线程池中的线程数为零。 ​ 当调用execute ()方法添加一个请求任务时，线程池会做出如下判断: ​ 如果正在运行的线程数量小于corePoolSize,那么马上创建线程运行这个任务; 如果正在运行的线程数量大于或等于corePoolSize,那么将这个任务放入队列; 如果这个时候队列满了且正在运行的线程数量还小于maximumPoolSize，那么还是要创建非核心线程立刻运行这个任务; 如果队列满了且正在运行的线程数量大于或等于max imumPoolSize,那么线程池会启动饱和拒绝策略来执行。 ​ 当一个线程完成任务时，它会从队列中取下一个任务来执行。 ​ 当一个线程无事可做超过一定的时间(keepAliveTime) 时，线程会判断: ​ 如果当前运行的线程数大于corePoolSize.那么这个线程就被停掉。所以线程池的所有任务完成后，它最终会收缩到corePoolSize的大小。 ​ 4.1 线程池的状态 4.2 线程池的拒绝策略 AbortPolicy 抛出异常 CallerRunsPolicy 调用者线程执行 DiscardOldestPolicy 丢弃队列中最老的任务，再次尝试提交 DiscardPolicy 直接丢弃 ​ 5. 自定义线程池1234567891011121314151617181920212223242526public static void main(String[] args) &#123; ExecutorService pool = new ThreadPoolExecutor( 2, 5, 3l, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;&gt;(3), Executors.defaultThreadFactory(), //new ThreadPoolExecutor.AbortPolicy() //抛异常 //new ThreadPoolExecutor.CallerRunsPolicy() //main 线程办理业务！ //new ThreadPoolExecutor.DiscardOldestPolicy() //就处理能处理的，剩下的老的直接丢了。 new ThreadPoolExecutor.DiscardPolicy() //如果新来的处理不了，直接就扔了。 ); try &#123; for (int i = 0; i &lt; 30; i++) &#123; pool.execute(() -&gt; &#123; System.out.println(Thread.currentThread().getName() + &quot;\\t线程办理业务！&quot;); &#125;); //pool1.submit(()-&gt;&#123;&#125;); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; pool.shutdown(); &#125;&#125; 二，线程池源码 1. 成员属性/静态属性/构造方法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108//高三位：表示当前线程池运行状态 除去高三位之后的低位：表示当前线程池中所拥有的的线程数量 private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0));//表示在ctl中，低COUNT_BITS位适用于存放当前线程数量的位。 private static final int COUNT_BITS = Integer.SIZE - 3;//线程池所能存放的最大容量 private static final int CAPACITY = (1 &lt;&lt; COUNT_BITS) - 1; //-1左移29位 负数 111 private static final int RUNNING = -1 &lt;&lt; COUNT_BITS;// 0 000 private static final int SHUTDOWN = 0 &lt;&lt; COUNT_BITS;//001 private static final int STOP = 1 &lt;&lt; COUNT_BITS;//010 private static final int TIDYING = 2 &lt;&lt; COUNT_BITS;//011 private static final int TERMINATED = 3 &lt;&lt; COUNT_BITS; // 获取当前线程池运行状态 private static int runStateOf(int c) &#123; return c &amp; ~COUNT_MASK; &#125;//获取当前线程池线程数量 private static int workerCountOf(int c) &#123; return c &amp; COUNT_MASK; &#125;//用在重置当前线程ctl值时，会用到 rs表示线程池状态 wc表示线程池worker数量 private static int ctlOf(int rs, int wc) &#123; return rs | wc; &#125;//cas的方式让ctl值+1 private boolean compareAndIncrementWorkerCount(int expect) &#123; return ctl.compareAndSet(expect, expect + 1); &#125;//cas的方式让ctl-1 private boolean compareAndDecrementWorkerCount(int expect) &#123; return ctl.compareAndSet(expect, expect - 1); &#125;//将ctl值-1 private void decrementWorkerCount() &#123; ctl.addAndGet(-1); &#125;//当线程池中的线程达到核心线程数时，在提交任务，就会提交到任务队列 private final BlockingQueue&lt;Runnable&gt; workQueue; //线程池中的全局锁，增加worker，减少worker，修改线程池运行状态 private final ReentrantLock mainLock = new ReentrantLock(); //真正存放worker-&gt;thread的地方 private final HashSet&lt;Worker&gt; workers = new HashSet&lt;&gt;(); /* 线程池中提供了一个对外方法，awaitTermination(long time,TimeUnit),该方法调用会被阻塞， 并且在以下几种情况任意一种发生时都会导致该方法的执行: 即shutdown方法被调用之后， 或者参数中定义的timeout时间到达或者当前线程被打断，这几种情况任意一个发生了都会导致该方法在所有任务完成之后才执行。 第一个参数是long类型的超时时间，第二个参数可以为该时间指定单位。*/ private final Condition termination = mainLock.newCondition(); //记录线程池生命周期内，线程最大值 private int largestPoolSize; //记录线程池所完成的任务总数，当worker退出时，会将worker完成的任务累积到这里 private long completedTaskCount; //创建线程池会使用到线程工厂，当我们使用Executor.newFix ... /newCache ... 使用的DefaultThreadFactory，//生成的线程名不容易分析执行的是哪里的业务//一般不建议使用使用自带的，推荐自己实现这个接口 private volatile ThreadFactory threadFactory; //拒绝策略，默认是采用抛出异常 private volatile RejectedExecutionHandler handler; //空闲线程存活时间 ：allowCoreThreadTimeOut=false时，会维护核心线程数量内的线程存活，超出部分会超时。//allowCoreThreadTimeOut=true时，核心数量内的线程也会被回收。 private volatile long keepAliveTime; //控制核心线程是否可以被回收，true可以，false不可以。 private volatile boolean allowCoreThreadTimeOut; //核心线程数限制 private volatile int corePoolSize; //最大线程数限制 private volatile int maximumPoolSize; //默认的拒绝策略，抛异常的方式 private static final RejectedExecutionHandler defaultHandler = new AbortPolicy(); public ThreadPoolExecutor(int corePoolSize, //核心线程数 int maximumPoolSize, //最大线程数 long keepAliveTime, //空闲等待时间 TimeUnit unit, //时间单位 BlockingQueue&lt;Runnable&gt; workQueue, //任务队列 ThreadFactory threadFactory, //线程工厂 RejectedExecutionHandler handler //拒绝策略 ) &#123; if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler; &#125; 2.内部类worker线程池中的线程实际上都封装成了一个个worker来执行。​ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667 private final class Worker extends AbstractQueuedSynchronizer implements Runnable &#123; private static final long serialVersionUID = 6138294804551838833L; //worker内部封装的工作线程 @SuppressWarnings(&quot;serial&quot;) final Thread thread; //假设firstTask不为空，当worker启动后（worker内部的线程启动后）会优先执行firstTask，//当执行完firstTask后，会去queue中去获取下一个任务 @SuppressWarnings(&quot;serial&quot;) Runnable firstTask; //记录当前worker所完成的任务数量 volatile long completedTasks; //firstTask可以为空，启动后会到queue中获取 Worker(Runnable firstTask) &#123; setState(-1); // 设置aqs独占模式为 初始化中状态，不能被抢占锁 this.firstTask = firstTask; this.thread = getThreadFactory().newThread(this); //使用线程工厂创建了一个线程，并且将当前worker指定为 //runnable，并且说当前thread启动的时候，会以worker.run()为入口。 &#125; /** */ public void run() &#123; //调用了threadPoolExecutor的方法 runWorker(this); &#125; //当前worker的独占锁是否被独占 protected boolean isHeldExclusively() &#123; return getState() != 0; &#125;//尝试去占用worker的独占锁 protected boolean tryAcquire(int unused) &#123; if (compareAndSetState(0, 1)) &#123; setExclusiveOwnerThread(Thread.currentThread()); return true; &#125; return false; &#125;//尝试去释放worker的独占锁 protected boolean tryRelease(int unused) &#123; setExclusiveOwnerThread(null); setState(0); return true; &#125; public void lock() &#123; acquire(1); &#125; public boolean tryLock() &#123; return tryAcquire(1); &#125; public void unlock() &#123; release(1); &#125; public boolean isLocked() &#123; return isHeldExclusively(); &#125; void interruptIfStarted() &#123; Thread t; if (getState() &gt;= 0 &amp;&amp; (t = thread) != null &amp;&amp; !t.isInterrupted()) &#123; try &#123; t.interrupt(); &#125; catch (SecurityException ignore) &#123; &#125; &#125; &#125; &#125; 3.execute当调用execute()来提交一个任务的时候，首先会判断如果当前线程数量小于核心线程数，此次提交任务会创建一个核心线程执行任务。如果提交失败，会继续判断：如果当前线程池处于RUNNING状态，尝试将task放入到workQueue中。如果还是失败，会继续判断：达到了最大线程数，所以执行拒绝策略。 123456789101112131415161718192021222324252627282930313233343536373839404142434445public void execute(Runnable command) &#123; //如果任务为空，抛出空指针异常 if (command == null) throw new NullPointerException(); //获取ctl的最新值，高三位表示此线程池的状态，低位表示当前线程池的线程数量 int c = ctl.get(); //如果当前线程数小于核心线程数，此任务会交给一个核心线程去执行 if (workerCountOf(c) &lt; corePoolSize) &#123; //创建worker成功以后，会把这个任务交给worker的firstTask，由worker包装的核心线程执行 if (addWorker(command, true)) return; //获取ctl的最新值，高三位表示此线程池的状态，低位表示当前线程池的线程数量 c = ctl.get(); //执行到这里，说明addworker失败 /** * 1.存在并发 * 2.线程池的状态非running，shutdown状态下也会创建成功，前提是firsttask==null,队列！=null */ &#125; /** * 执行到这里，说明，此时线程池已经达到核心线程数 addworker失败 */ //如果线程池没有被shutdown，也就是正常运行，说明此时存在并发，那就把这个任务放到队列 if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; //执行到这里说明任务提交到队列成功 //再次获取ctl 线程池的状态 和 工作线程数 int recheck = ctl.get(); //如果此时的线程池状态不在是running 需要删除刚刚提交的任务 //删除成功说明此时任务尚未被执行，删除失败代表此时已经有核心线程正在执行任务 if (! isRunning(recheck) &amp;&amp; remove(command)) //进入这里说明线程池的状态发生改变，并且任务删除成功，启动拒绝策略 reject(command); //当前是running状态 或者当前是非running状态，并且任务已经在被核心线程执行 else if (workerCountOf(recheck) == 0) //线程池是running状态但是线程池的工作线程数==0 addWorker(null, false); &#125; /** * 执行到这里有几种情况： * 1.任务入队失败，说明队列满了 * 2.当前线程池是非running状态，这个时候commond！=null，addworker一定返回false */ else if (!addWorker(command, false)) //达到了最大线程数，执行拒绝策略 reject(command);&#125; 4.addWorker自旋去申请一个工作线程，申请成功以后，将线程包装成一个worker，加入到worker队列中，并启动任务。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465//firstTask可以为空，自动取queue拿任务， core：采用的线程数限制，true，核心线程数，false，非核心线程 private boolean addWorker(Runnable firstTask, boolean core) &#123; retry: for (int c = ctl.get();;) &#123;//获取当前ctl保存到c if (runStateAtLeast(c, SHUTDOWN)//当前线程池不是RUNNING &amp;&amp; (runStateAtLeast(c, STOP) || firstTask != null || workQueue.isEmpty())) //当前线程池状态为SHUTDOWN || 任务不为空 || 队列为空 return false; //自旋：来到这里说明线程池的状态允许执行任务 for (;;) &#123; //如果当前线程池的数量大于等于最大线程数 &amp;&amp; 工作线程数达到5亿 if (workerCountOf(c) &gt;= ((core ? corePoolSize : maximumPoolSize) &amp; COUNT_MASK)) return false; //如果cas使工作线程数+1成功，相当于申请到了一个工作线程 if (compareAndIncrementWorkerCount(c)) break retry; c = ctl.get(); //来到这里说明： //1.并发导致申请工作线程失败 //2.线程池状态改变 if (runStateAtLeast(c, SHUTDOWN))//如果线程池状态没问题，再次尝试获取工作线程 continue retry; &#125; &#125; boolean workerStarted = false;//任务是否开始执行 boolean workerAdded = false;//任务是否添加到池子中 Worker w = null; try &#123; w = new Worker(firstTask); //包装成一个worker final Thread t = w.thread; if (t != null) &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; int c = ctl.get(); //如果当前线程池状态是RUNNING，任务不为空 if (isRunning(c) || (runStateLessThan(c, STOP) &amp;&amp; firstTask == null)) &#123; //如果线程状态不是就绪 if (t.getState() != Thread.State.NEW) throw new IllegalThreadStateException(); workers.add(w);//加入到队列 workerAdded = true; int s = workers.size(); if (s &gt; largestPoolSize) largestPoolSize = s; &#125; &#125; finally &#123; mainLock.unlock(); &#125; if (workerAdded) &#123; t.start();//启动任务 workerStarted = true; &#125; &#125; &#125; finally &#123; if (! workerStarted) //如果启动失败，释放掉刚才自旋获取的工作线程 addWorkerFailed(w); &#125; return workerStarted; &#125; 5.runworker执行任务的线程去获取独占锁执行任务，执行完成后将线程完成的任务书+1并释放锁，然后执行退出逻辑。 1234567891011121314151617181920212223242526272829303132333435363738394041 final void runWorker(Worker w) &#123; Thread wt = Thread.currentThread(); Runnable task = w.firstTask; w.firstTask = null; w.unlock(); //为什么先调用unlock()?为了强制刷新当前持有锁的线程为空。 boolean completedAbruptly = true;//是否突然退出？true表示发生异常，当前线程突然退出的，false表示正常退出。 try &#123;//任务不为空 || 在queue中获取任务成功 while (task != null || (task = getTask()) != null) &#123; w.lock();//设置独占锁为当前线程 //为什么要设置独占锁？防止shutdown时会判断当前work状态 //如果线程池状态为STOP || //（获取当前线程中断状态并给当前线程设置中断信号未false &amp;&amp; 线程池状态为STOP）&amp;&amp; 线程未设置中断状态 if ((runStateAtLeast(ctl.get(), STOP) || (Thread.interrupted() &amp;&amp; runStateAtLeast(ctl.get(), STOP))) &amp;&amp; !wt.isInterrupted()) wt.interrupt(); try &#123; //钩子方法 beforeExecute(wt, task); try &#123; //执行任务 task.run(); afterExecute(task, null);//钩子方法 &#125; catch (Throwable ex) &#123; afterExecute(task, ex);//钩子方法 throw ex; &#125; &#125; finally &#123; task = null; w.completedTasks++;//将线程完成的任务数+1 w.unlock();//释放锁 &#125; &#125; completedAbruptly = false;//异常打断标记设置为false &#125; finally &#123;//执行退出逻辑 processWorkerExit(w, completedAbruptly); &#125; &#125; 6.getTask自旋的方式去队列获取任务 123456789101112131415161718192021222324252627282930313233343536373839404142private Runnable getTask() &#123; boolean timedOut = false; for (;;) &#123; int c = ctl.get(); //如果当前线程池是非RUNNING &amp;&amp; 当前状态大于等于 STOP || queue为空 if (runStateAtLeast(c, SHUTDOWN) &amp;&amp; (runStateAtLeast(c, STOP) || workQueue.isEmpty())) &#123; decrementWorkerCount(); //将获取到的工作线程还回去 return null; &#125; /* 能够来到这里的情况： 1.RUNNING 2.SHUTDOWN但是队列不为空 */ int wc = workerCountOf(c); //获取到worker数量 //如果是核心线程就不需要超时时间 boolean timed = allowCoreThreadTimeOut || wc &gt; corePoolSize; //条件1说明 当前工作线程数已经大于最大线程数 //条件2说明 当前线程可以被回收 ||wc==1且任务队列已经空了，当前线程可以放心退出。 if ((wc &gt; maximumPoolSize || (timed &amp;&amp; timedOut)) &amp;&amp; (wc &gt; 1 || workQueue.isEmpty())) &#123; if (compareAndDecrementWorkerCount(c))//如果成功减少worker数量 return null; continue; &#125; try &#123; Runnable r = timed ? workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) : workQueue.take();//去队列获取任务 if (r != null)//任务不为空，返回任务 return r; timedOut = true;//否则标记为超时 &#125; catch (InterruptedException retry) &#123; timedOut = false; &#125; &#125;&#125; 7.shutdown他会先尝试去停止所有空闲的线程，然后执行tryTerminate()，tryTerminate()会通过自旋的方式判断，当队列里面的任务都执行完了，销毁掉执行完任务的线程，当最后一个线程退出的时候，将线程池状态修改为TIDYING，最终修改为TERMINATED。 12345678910111213public void shutdown() &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock();//获取全局锁 try &#123; checkShutdownAccess();//权限判断 advanceRunState(SHUTDOWN);//自旋设置线程池状态为SHUTDOWN interruptIdleWorkers();//遍历所有的线程，中断空闲线程 onShutdown(); // 空方法，子类可以扩展 &#125; finally &#123; mainLock.unlock(); &#125; tryTerminate();&#125; 8.shutdownNow自旋设置线程池状态为STOP，遍历中断（interrupt的方式）线程池中所有线程，导出所有未处理的任务，然后执行tryTerminate()。 123456789101112131415public List&lt;Runnable&gt; shutdownNow() &#123; List&lt;Runnable&gt; tasks; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; checkShutdownAccess(); advanceRunState(STOP);//自旋设置线程池状态为STOP interruptWorkers();//遍历中断线程池中所有线程 tasks = drainQueue();//导出所有未处理的任务 &#125; finally &#123; mainLock.unlock(); &#125; tryTerminate(); return tasks;&#125; 9.tryTerminate通过自旋的方式，当非空闲线程执行完任务，就会来到这里被中断，最后一个退出的线程，设置线程池状态为TIDYING，最终设置为设置为TERMINATED，并调用termination.signalAll();唤醒调用awaitTermination()的线程继续执行程序。 123456789101112131415161718192021222324252627282930313233343536373839404142434445final void tryTerminate() &#123; for (;;) &#123;//自旋 int c = ctl.get(); /* 线程池RUNNING ||线程池大于等于TIDYING(说明已经有线程在执行这个方法) || 线程池大于STOP状态 且 队列不为空 */ if (isRunning(c) || runStateAtLeast(c, TIDYING) || (runStateLessThan(c, STOP) &amp;&amp; ! workQueue.isEmpty())) return; /* 什么时候执行到这里？ 1.线程池状态大于等于STOP 2.线程池状态大于等于STOP并且队列为空 当前线程池中的线程数量&gt;0 */ if (workerCountOf(c) != 0) &#123; //非空闲线程当执行完任务，最终也会执行到这里 interruptIdleWorkers(ONLY_ONE);//中断一个空闲线程 return; &#125; //最后一个退出的线程会来到这里 final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; //强制设置线程池状态为TIDYING if (ctl.compareAndSet(c, ctlOf(TIDYING, 0))) &#123; try &#123; terminated(); //钩子方法 &#125; finally &#123; //设置为TERMINATED ctl.set(ctlOf(TERMINATED, 0)); //唤醒调用awaitTermination()的线程 termination.signalAll(); &#125; return; &#125; &#125; finally &#123; mainLock.unlock(); &#125; // else retry on failed CAS &#125;&#125;","categories":[{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"CyclicBarrier","slug":"JUC/CyclicBarrier","date":"2022-01-11T11:07:39.796Z","updated":"2022-01-11T11:28:00.513Z","comments":true,"path":"2022/01/11/JUC/CyclicBarrier/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/JUC/CyclicBarrier/","excerpt":"","text":"一，使用CyclicBarrier 的字面意思是可循环使用（Cyclic）的屏障（Barrier）。它要做的事情是，让一组线程到达一个屏障（也可以叫同步点）时被阻塞，直到最后一个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程才会继续工作。CyclicBarrier 默认的构造方法是 CyclicBarrier(int parties)，其参数表示屏障拦截的线程数量，每个线程调用 await 方法告诉 CyclicBarrier 当前线程已经到达了屏障，然后当前线程被阻塞。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class CyclicBarrierTest01 &#123; /** * 案例： * 模拟过气游戏 “王者荣耀” 游戏开始逻辑 */ public static void main(String[] args) &#123; //第一步：定义玩家，定义5个 String[] heros = &#123;&quot;安琪拉&quot;,&quot;亚瑟&quot;,&quot;马超&quot;,&quot;张飞&quot;, &quot;刘备&quot;&#125;; //第二步：创建固定线程数量的线程池，线程数量为5 ExecutorService service = Executors.newFixedThreadPool(5); //第三步：创建barrier，parties 设置为5 CyclicBarrier barrier = new CyclicBarrier(5); //第四步：通过for循环开启5任务，模拟开始游戏，传递给每个任务 英雄名称和barrier for(int i = 0; i &lt; 5; i++) &#123; service.execute(new Player(heros[i], barrier)); &#125; service.shutdown(); &#125; static class Player implements Runnable &#123; private String hero; private CyclicBarrier barrier; public Player(String hero, CyclicBarrier barrier) &#123; this.hero = hero; this.barrier = barrier; &#125; @Override public void run() &#123; try &#123; //每个玩家加载进度不一样，这里使用随机数来模拟！ TimeUnit.SECONDS.sleep(new Random().nextInt(10)); System.out.println(hero + &quot;：加载进度100%，等待其他玩家加载完成中...&quot;); barrier.await(); System.out.println(hero + &quot;：发现所有英雄加载完成，开始战斗吧！&quot;); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (BrokenBarrierException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 对于指定计数值 parties，若由于某种原因，没有足够的线程调用 CyclicBarrier 的 await，则所有调用 await 的线程都会被阻塞； 同样的 CyclicBarrier 也可以调用 await(timeout, unit)，设置超时时间，在设定时间内，如果没有足够线程到达，则解除阻塞状态，继续工作； 通过 reset 重置计数，会使得进入 await 的线程出现BrokenBarrierException； 如 果 采 用 是 CyclicBarrier(int parties, RunnablebarrierAction) 构造方法，执行 barrierAction 操作的是最后一个到达的线程。 二，源码 2.1 成员变量 属性 内部类1234567891011121314151617181920212223/** * 每个barrier都表示为一个generation实例。当barrier触发trip条件或重置时generation随之改变。 * 使用barrier时有很多generation与线程关联-由于不确定性的方式，锁可能分配给等待的线程。 * 但是在同一时间只有一个是活跃的generation(通过count变量确定)，并且其余的要么被销毁，要么被trip条件等待。 * 如果有一个中断，但没有随后的重置，就不需要有活跃的generation。 */private static class Generation &#123; boolean broken = false;&#125;//因为barrier实现是依赖于Condition条件队列的，condition条件队列必须依赖lock才能使用。private final ReentrantLock lock = new ReentrantLock();//线程挂起实现使用的 condition 队列。条件：当前代所有线程到位，这个条件队列内的线程 才会被唤醒。private final Condition trip = lock.newCondition();//Barrier需要参与进来的线程数量private final int parties;//当前代 最后一个到位的线程 需要执行的事件private final Runnable barrierCommand;//表示barrier对象 当前 “代”private Generation generation = new Generation();//表示当前“代”还有多少个线程 未到位。//初始值为partiesprivate int count; 2.2 构造器123456789101112//构造函数，指定参与线程数，并在所有线程到达barrier之后执行给定的barrierAction逻辑public CyclicBarrier(int parties, Runnable barrierAction) &#123; if (parties &lt;= 0) throw new IllegalArgumentException(); this.parties = parties; this.count = parties; this.barrierCommand = barrierAction;&#125;//构造函数，指定参与线程数public CyclicBarrier(int parties) &#123; this(parties, null);&#125; 2.3 await12345678//等待所有的参与者到达barrierpublic int await() throws InterruptedException, BrokenBarrierException &#123; try &#123; return dowait(false, 0L); &#125; catch (TimeoutException toe) &#123; throw new Error(toe); // cannot happen &#125;&#125; 2.4 dowait123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133/** * Main barrier code, covering the various policies. * timed：表示当前调用await方法的线程是否指定了 超时时长，如果true 表示 线程是响应超时的 * nanos：线程等待超时时长 纳秒，如果timed == false ,nanos == 0 */private int dowait(boolean timed, long nanos) throws InterruptedException, BrokenBarrierException,TimeoutException &#123; //获取barrier全局锁对象 final ReentrantLock lock = this.lock; //加锁 //为什么要加锁呢？ //因为 barrier的挂起 和 唤醒 依赖的组件是 condition。 lock.lock(); try &#123; //获取barrier当前的 “代” final Generation g = generation; //如果当前代是已经被打破状态，则当前调用await方法的线程，直接抛出Broken异常 if (g.broken) throw new BrokenBarrierException(); //如果当前线程的中断标记位 为 true，则打破当前代，然后当前线程抛出 中断异常 if (Thread.interrupted()) &#123; //1.设置当前代的状态为broken状态 2.唤醒在trip 条件队列内的线程 breakBarrier(); throw new InterruptedException(); &#125; //执行到这里，说明 当前线程中断状态是正常的 false， 当前代的broken为 false（未打破状态） //正常逻辑... //假设 parties 给的是 5，那么index对应的值为 4,3,2,1,0 int index = --count; //条件成立：说明当前线程是最后一个到达barrier的线程，此时需要做什么呢？ if (index == 0) &#123; // tripped //标记：true表示 最后一个线程 执行cmd时未抛异常。 false，表示最后一个线程执行cmd时抛出异常了. //cmd就是创建 barrier对象时 指定的第二个 Runnable接口实现，这个可以为null boolean ranAction = false; try &#123; final Runnable command = barrierCommand; //条件成立：说明创建barrier对象时 指定 Runnable接口了，这个时候最后一个到达的线程 就需要执行这个接口 if (command != null) command.run(); //command.run()未抛出异常的话，那么线程会执行到这里。 ranAction = true; //开启新的一代 //1.唤醒trip条件队列内挂起的线程，被唤醒的线程 会依次 获取到lock，然后依次退出await方法。 //2.重置count 为 parties //3.创建一个新的generation对象，表示新的一代 nextGeneration(); //返回0，因为当前线程是此 代 最后一个到达的线程，所以Index == 0 return 0; &#125; finally &#123; if (!ranAction) //如果command.run()执行抛出异常的话，会进入到这里。 breakBarrier(); &#125; &#125; //执行到这里，说明当前线程 并不是最后一个到达Barrier的线程..此时需要进入一个自旋中. // loop until tripped, broken, interrupted, or timed out //自旋，一直到 条件满足、当前代被打破、线程被中断，等待超时 for (;;) &#123; try &#123; //条件成立：说明当前线程是不指定超时时间的 if (!timed) //当前线程 会 释放掉lock，然后进入到trip条件队列的尾部，然后挂起自己，等待被唤醒。 trip.await(); else if (nanos &gt; 0L) //说明当前线程调用await方法时 是指定了 超时时间的！ nanos = trip.awaitNanos(nanos); &#125; catch (InterruptedException ie) &#123; //抛出中断异常，会进来这里。 //什么时候会抛出InterruptedException异常呢？ //Node节点在 条件队列内 时 收到中断信号时 会抛出中断异常！ //条件一：g == generation 成立，说明当前代并没有变化。 //条件二：! g.broken 当前代如果没有被打破，那么当前线程就去打破，并且抛出异常.. if (g == generation &amp;&amp; ! g.broken) &#123; breakBarrier(); throw ie; &#125; else &#123; //执行到else有几种情况？ //1.代发生了变化，这个时候就不需要抛出中断异常了，因为 代已经更新了，这里唤醒后就走正常逻辑了..只不过设置下 中断标记。 //2.代没有发生变化，但是代被打破了，此时也不用返回中断异常，执行到下面的时候会抛出 brokenBarrier异常。也记录下中断标记位。 // We&#x27;re about to finish waiting even if we had not // been interrupted, so this interrupt is deemed to // &quot;belong&quot; to subsequent execution. Thread.currentThread().interrupt(); &#125; &#125; //唤醒后，执行到这里，有几种情况？ //1.正常情况，当前barrier开启了新的一代（trip.signalAll()） //2.当前Generation被打破，此时也会唤醒所有在trip上挂起的线程 //3.当前线程trip中等待超时，然后主动转移到 阻塞队列 然后获取到锁 唤醒。 //条件成立：当前代已经被打破 if (g.broken) //线程唤醒后依次抛出BrokenBarrier异常。 throw new BrokenBarrierException(); //唤醒后，执行到这里，有几种情况？ //1.正常情况，当前barrier开启了新的一代（trip.signalAll()） //3.当前线程trip中等待超时，然后主动转移到 阻塞队列 然后获取到锁 唤醒。 //条件成立：说明当前线程挂起期间，最后一个线程到位了，然后触发了开启新的一代的逻辑，此时唤醒trip条件队列内的线程。 if (g != generation) //返回当前线程的index。 return index; //唤醒后，执行到这里，有几种情况？ //3.当前线程trip中等待超时，然后主动转移到 阻塞队列 然后获取到锁 唤醒。 if (timed &amp;&amp; nanos &lt;= 0L) &#123; //打破barrier breakBarrier(); //抛出超时异常. throw new TimeoutException(); &#125; &#125; &#125; finally &#123; lock.unlock(); &#125;&#125; 2.5 breakBarrier打破barrier屏障，再屏障内的线程 都会抛出异常。 123456789private void breakBarrier() &#123; //将代中的broken设置为true，表示这一代是被打破了的，再来到这一代的线程，直接抛出异常. generation.broken = true; //重置count为parties count = parties; //将在trip条件队列内挂起的线程 全部唤醒，唤醒后的线程 会检查当前代 是否是打破的， //如果是打破的话，接下来的逻辑和 开启下一代 唤醒的逻辑不一样. trip.signalAll();&#125; 2.6 nextGeneration开启下一代，当这一代所有的线程到位后（假设barrierCommand不为空，还需要最后一个线程执行完事件），会调用nextGeneration开启新的一代。 123456789101112private void nextGeneration() &#123; //将在trip条件队列内挂起的线程 全部唤醒 // signal completion of last generation trip.signalAll(); //重置count为parties // set up next generation count = parties; //开启新的一代..使用一个新的 generation对象，表示新的一代，新的一代和上一代 没有任何关系。 generation = new Generation();&#125;","categories":[{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"Semaphore","slug":"JUC/Semaphore","date":"2022-01-11T11:07:31.422Z","updated":"2022-01-11T11:32:49.088Z","comments":true,"path":"2022/01/11/JUC/Semaphore/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/JUC/Semaphore/","excerpt":"","text":"一，使用12345678910111213141516171819202122232425262728293031323334353637383940414243@Testpublic void test3()throws InterruptedException&#123; final Semaphore semaphore = new Semaphore(2,true); new Thread(()-&gt;&#123; try &#123; semaphore.acquire(); System.out.println(&quot;线程A获取通行证成功！&quot;); TimeUnit.SECONDS.sleep(10); &#125;catch (Exception e)&#123; &#125;finally &#123; semaphore.release(); &#125; &#125;).start(); TimeUnit.MICROSECONDS.sleep(200); new Thread(()-&gt;&#123; try &#123; semaphore.acquire(); System.out.println(&quot;线程B获取通行证成功！&quot;); TimeUnit.SECONDS.sleep(10); &#125;catch (Exception e)&#123; &#125;finally &#123; semaphore.release(); &#125; &#125;).start(); TimeUnit.MILLISECONDS.sleep(200); new Thread(() -&gt;&#123; try &#123; semaphore.acquire(); System.out.println(&quot;线程C获取通行证成功!&quot;); &#125; catch (InterruptedException e) &#123; &#125;finally &#123; semaphore.release(); &#125; &#125;).start(); while (true)&#123;&#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public class Pool &#123; /** 可同时访问资源的最大线程数*/ private static final int MAX_AVAILABLE = 100; /** 信号量 表示：可获取的对象通行证*/ private final Semaphore available = new Semaphore(MAX_AVAILABLE, true); /** 共享资源，可以想象成 items 数组内存储的都是Connection对象 模拟是链接池*/ protected Object[] items = new Object[MAX_AVAILABLE]; /** 共享资源占用情况，与items数组一一对应，比如：items[0]对象被外部线程占用，那么 used[0] == true，否则used[0] == false*/ protected boolean[] used = new boolean[MAX_AVAILABLE]; /** * 获取一个空闲对象 * 如果当前池中无空闲对象，则等待..直到有空闲对象为止 */ public Object getItem() throws InterruptedException &#123; available.acquire(); return getNextAvailableItem(); &#125; /** * 归还对象到池中 */ public void putItem(Object x) &#123; if (markAsUnused(x)) available.release(); &#125; /** * 获取池内一个空闲对象，获取成功则返回Object，失败返回Null * 成功后将对应的 used[i] = true */ private synchronized Object getNextAvailableItem() &#123; for (int i = 0; i &lt; MAX_AVAILABLE; ++i) &#123; if (!used[i]) &#123; used[i] = true; return items[i]; &#125; &#125; return null; &#125; /** * 归还对象到池中，归还成功返回true * 归还失败： * 1.池中不存在该对象引用，返回false * 2.池中存在该对象引用，但该对象目前状态为空闲状态，也返回false */ private synchronized boolean markAsUnused(Object item) &#123; for (int i = 0; i &lt; MAX_AVAILABLE; ++i) &#123; if (item == items[i]) &#123; if (used[i]) &#123; used[i] = false; return true; &#125; else return false; &#125; &#125; return false; &#125;&#125; 二，源码 1.构造器1234567891011121314//默认是非公平锁public Semaphore(int permits) &#123; sync = new NonfairSync(permits);&#125;//可以指定为公平锁的构造器public Semaphore(int permits, boolean fair) &#123; sync = fair ? new FairSync(permits) : new NonfairSync(permits);&#125;//=======================//Sync(int permits) &#123; setState(permits);&#125; 2.acquire1234public void acquire() throws InterruptedException &#123; //去抢占锁，默认传1，如果想要传其他参数，可以手动指定 sync.acquireSharedInterruptibly(1);&#125; 3.AQS.acquireSharedInterruptibly1234567891011public final void acquireSharedInterruptibly(int arg) throws InterruptedException &#123; //如果当前线程已经被中断 抛出中断异常 if (Thread.interrupted()) throw new InterruptedException(); //小于0 的情况有两种： //锁没有被持有 或者 持有锁的线程不是当前线程 //当前剩下的证书不足以支持当前线程本次获取 if (tryAcquireShared(arg) &lt; 0) doAcquireSharedInterruptibly(arg);&#125; 4.Semaphore.FairSync.tryAcquireShared123456789101112131415161718protected int tryAcquireShared(int acquires) &#123; for (;;) &#123; // 当前队列还有元素，头节点的下一个节点不是空节点&amp;&amp;当前线程的节点不是头节点的下一个节点 if (hasQueuedPredecessors()) //返回-1 return -1; //获取当前最新的state值 int available = getState(); //用state-传入的值 int remaining = available - acquires; //条件1成立 ：当前剩下的证书不足以支持当前线程获取 //条件2成立：前置条件：当前剩下的证书足够支持当前线程持有。 //如果cas去设置新的state成功，返回 if (remaining &lt; 0 || compareAndSetState(available, remaining)) return remaining; &#125;&#125; 5.AQS.hasQueuedPredecessors判断当前AQS阻塞队列里面是否有等待的线程 123456789101112131415public final boolean hasQueuedPredecessors() &#123; Node t = tail; // Read fields in reverse initialization order Node h = head; Node s; /* 1.h!=t:头节点不等于尾结点，说明当前队列还有元素 2.头节点的下一个节点是空 || 头节点的下一个节点不是空节点&amp;&amp;当前线程的节点不是头节点的下一个节点 总结一下：如果返回true： 当前队列还有元素，头节点的下一个节点不是空节点&amp;&amp;当前线程的节点不是头节点的下一个节点 */ return h != t &amp;&amp; ((s = h.next) == null || s.thread != Thread.currentThread());&#125; 6.AQS.doAcquireSharedInterruptibly123456789101112131415161718192021222324252627282930313233private void doAcquireSharedInterruptibly(int arg) throws InterruptedException &#123; //将当前线程构建成一个共享节点入队 final Node node = addWaiter(Node.SHARED); boolean failed = true; try &#123; for (;;) &#123; //获取当前节点的前置节点 final Node p = node.predecessor(); //如果前置节点是头节点 if (p == head) &#123; //尝试去获取锁 int r = tryAcquireShared(arg); //大于0说明成功拿到了锁 if (r &gt;= 0) &#123; //设置头节点并向后传播唤醒 setHeadAndPropagate(node, r); //原头节点出队 p.next = null; // help GC failed = false; return; &#125; &#125; //如果当前节点的前驱节点不是头节点，给当前线程找一个好爸爸 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) throw new InterruptedException(); &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; 7.AQS.setHeadAndPropagate1234567891011121314private void setHeadAndPropagate(Node node, int propagate) &#123; Node h = head; // Record old head for check below //设置node为头节点 setHead(node); if (propagate &gt; 0 || h == null || h.waitStatus &lt; 0 || (h = head) == null || h.waitStatus &lt; 0) &#123; Node s = node.next; //如果当前节点的下一个节点为空 || 当前节点的下一个节点不为空且当前节点的下一个节点是共享节点 if (s == null || s.isShared()) //执行向后唤醒逻辑 doReleaseShared(); &#125;&#125; 8.release123public void release() &#123; sync.releaseShared(1);&#125; 9.releaseShared123456789public final boolean releaseShared(int arg) &#123; //如果尝试释放锁成功 if (tryReleaseShared(arg)) &#123; //执行向后唤醒逻辑 doReleaseShared(); return true; &#125; return false;&#125; 10.tryReleaseShared12345678910111213protected final boolean tryReleaseShared(int releases) &#123; for (;;) &#123; //获取当前最新的state int current = getState(); //将当前的state 和释放的许可证个数相加 int next = current + releases; if (next &lt; current) // overflow throw new Error(&quot;Maximum permit count exceeded&quot;); //cas成功 返回true if (compareAndSetState(current, next)) return true; &#125;&#125; 11.doReleaseShared1234567891011121314151617181920212223242526272829private void doReleaseShared() &#123; for (;;) &#123; //获取头节点 Node h = head; //头节点不为空 &amp;&amp; 头节点不是尾结点 if (h != null &amp;&amp; h != tail) &#123; int ws = h.waitStatus; //如果头结点的等待状态 是 -1 if (ws == Node.SIGNAL) &#123; //如果cas 设置头节点 -1 ， 0 失败 //为啥会失败？其他线程获取到锁以后执行向后唤醒逻辑了 if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) continue; // 唤醒节点的线程 unparkSuccessor(h); &#125; //如果等待状态 ==0 且cas 设置 头节点的等待状态为向后传播失败 else if (ws == 0 &amp;&amp; !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) continue; // loop on failed CAS &#125; //如果头节点没变 也就是说 ，唤醒的后面的节点 还没来得及将自己设置为头节点 ，跳出循环 。 //为什么可以直接跳出？不怕向后唤醒中断么？ 不怕 ，首先 ，极端情况已经都判断完了 if (h == head) // loop if head changed break; &#125;&#125;","categories":[{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"CountDownLatch","slug":"JUC/CountDownLatch","date":"2022-01-11T11:07:22.214Z","updated":"2022-01-11T11:27:37.985Z","comments":true,"path":"2022/01/11/JUC/CountDownLatch/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/JUC/CountDownLatch/","excerpt":"","text":"一，源码​ 对于 CountDownLatch，我们仅仅需要关心两个方法，一个是 countDown() 方法，另一个是 await() 方法。countDown() 方法每次调用都会将 state 减 1，直到state 的值为 0；而 await 是一个阻塞方法，当 state 减为 0 的时候，await 方法才会返回。await 可以被多个线程调用，所有调用了await 方法的线程阻塞在 AQS 的阻塞队列中，等待条件满足（state == 0），将线程从队列中一个个唤醒过来。 1.内部类12345678910111213141516171819202122232425262728293031private static final class Sync extends AbstractQueuedSynchronizer &#123; private static final long serialVersionUID = 4982264981922014374L; //CountDownLatch中的计数其实就是AQS的state Sync(int count) &#123; setState(count); &#125; int getCount() &#123; return getState(); &#125; protected int tryAcquireShared(int acquires) &#123; //如果state =0 返回1 否则返回 0 return (getState() == 0) ? 1 : -1; &#125; protected boolean tryReleaseShared(int releases) &#123; for (;;) &#123; //获取最新的state int c = getState(); //如果state==0 返回false if (c == 0) return false; int nextc = c-1; //如果cas成功，且c=1，返回true if (compareAndSetState(c, nextc)) return nextc == 0; &#125; &#125;&#125; 2.构造函数123456789private final Sync sync;//构造函数public CountDownLatch(int count) &#123; //边界值判断 if (count &lt; 0) throw new IllegalArgumentException(&quot;count &lt; 0&quot;); //初始化Sync this.sync = new Sync(count);&#125; 3.await使当前线程挂起，直到计数器减为0或者当前线程被中断。 1234public void await() throws InterruptedException &#123; //执行aqs.acquireSharedInterruptibly() sync.acquireSharedInterruptibly(1);&#125; 4.AQS.acquireSharedInterruptiblycountdownlatch 也用到了 AQS，在 CountDownLatch 内部写了一个 Sync 并且继承了AQS 这个抽象类重写了 AQS中的共享锁方法。首先看到下面这个代码，这块代码主要是 判 断 当 前 线 程 是 否 获 取 到 了 共 享 锁 ; （ 在CountDownLatch 中 ， 使 用 的是 共 享 锁 机 制 ， 因 为CountDownLatch 并不需要实现互斥的特性）。 12345678910public final void acquireSharedInterruptibly(long arg) throws InterruptedException &#123; //如果当前线程被中断，抛出中断异常 if (Thread.interrupted()) throw new InterruptedException(); //条件成立：说明此时state&gt;0将线程入队，然后等待唤醒 //条件不成立：说明此时state=0，说明此时阻塞已经放开，当前线程不会被阻塞 if (tryAcquireShared(arg) &lt; 0) //将当前线程加入到共享锁队列 doAcquireSharedInterruptibly(arg);&#125; 5.tryAcquireShared判断state状态. 1234protected int tryAcquireShared(int acquires) &#123; //如果state =0 返回1 否则返回 0 return (getState() == 0) ? 1 : -1;&#125; 6.AQS.doAcquireSharedInterruptibly addWaiter 设置为 shared 模式。 ​ tryAcquire 和 tryAcquireShared 的返回值不同，因此会多出一个判断过程。 ​ 在 判 断 前 驱 节 点 是 头 节 点 后 ， 调 用 了setHeadAndPropagate 方法，而不是简单的更新一下头节点。 ​ 123456789101112131415161718192021222324252627282930313233343536private void doAcquireSharedInterruptibly(long arg) throws InterruptedException &#123; //将当前线程封装成节点入队，共享节点，使用的是state的高16位运算 final Node node = addWaiter(Node.SHARED); boolean failed = true; try &#123; //自旋 for (;;) &#123; //获取当前节点的前驱 final Node p = node.predecessor(); //如果前驱节点是头节点 if (p == head) &#123; //当前节点就可以尝试去抢锁 long r = tryAcquireShared(arg); //此时说明抢到锁了 if (r &gt;= 0) &#123; //修改头节点的值 setHeadAndPropagate(node, r); //头节点出队 p.next = null; // help GC //代表抢锁成功 failed = false; return; &#125; &#125; //否则的话，线程在这里park，如果线程中断信号=true，就会抛出中断异常 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) throw new InterruptedException(); &#125; &#125; finally &#123; //如果抢锁失败了，就走取消竞争锁的逻辑 if (failed) cancelAcquire(node); &#125;&#125; 假如这个时候有 3 个线程调用了 await 方法，由于这个时候 state 的值还不为 0，所以这三个线程都会加入到 AQS队列中。并且三个线程都处于阻塞状态。 7.countDown递减锁计数，如果锁计数为0，释放所有阻塞线程。 123public void countDown() &#123; sync.releaseShared(1);&#125; 8.AQS.releaseShared由于线程被 await 方法阻塞了，所以只有等到countdown 方法使得 state=0 的时候才会被唤醒。 只有当 state 减为 0 的时候，tryReleaseShared 才返回 true, 否则只是简单的 state = state - 1。 ​ 如果 state=0, 则调用 doReleaseShared唤醒处于 await 状态下的线程。 ​ 12345678910public final boolean releaseShared(int arg) &#123; //执行子类重写的方法，state=0的时候，执行doReleaseShared //条件成立：说明当前调用latch.countDown()方法的线程，正好是state-1 == 0 的这个线程，需要做触发唤醒await状态的线程。 if (tryReleaseShared(arg)) &#123; //调用countDown()方法的线程，只有一个线程会进入到这个if块里面，执行下面的方法 doReleaseShared(); return true; &#125; return false;&#125; 9.tryReleaseShared自旋释放锁，释放完了返回true，否则返回false。 12345678910111213protected boolean tryReleaseShared(int releases) &#123; for (;;) &#123; //获取最新的state int c = getState(); //如果state==0 返回false if (c == 0) return false; int nextc = c-1; //如果cas成功，且c=1，返回true if (compareAndSetState(c, nextc)) return nextc == 0; &#125;&#125; 10.AQS.doReleaseShared共享锁的释放和独占锁的释放有一定的差别​ 前面唤醒锁的逻辑和独占锁是一样，先判断头结点是不是SIGNAL 状态，如果是，则修改为 0，并且唤醒头结点的下一个节点。​ PROPAGATE ： 标识为 PROPAGATE 状态的节点，是共享锁模式下的节点状态，处于这个状态下的节点，会对线程的唤醒进行传播 123456789101112131415161718192021222324252627282930313233343536373839404142434445private void doReleaseShared() &#123; //自旋 for (;;) &#123; //获取头节点的引用 Node h = head; //如果头节点不为空 &amp;&amp; 头节点不等于尾结点 //条件一成立：说明阻塞队列不为空 //什么时候不成立？latch创建出来以后，没有任何线程调用过await方法之前，就有线程调用countDown操作，并且触发了唤醒阻塞节点的逻辑 //条件二成立：说明当前队列除了头节点还有其他节点 //什么时候不成立？ //1.正常唤醒情况：依次获取共享锁，当前线程执行到这里的时候是tail节点 //2.第一个调用await的线程与调用countDown的线程并发了 if (h != null &amp;&amp; h != tail) &#123; int ws = h.waitStatus; //如果头结点的转态=-1 if (ws == Node.SIGNAL) &#123; //cas设置头节点的状态失败 if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) continue; //cas成功，唤醒头节点的下一个节点 unparkSuccessor(h); &#125; //cas失败走到这里， //执行到这里的时候，刚好有一个节点入队，入队会将这个 ws 设置为 -1 else if (ws == 0 &amp;&amp; !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) continue; &#125; /* 条件成立： 1.说明刚刚唤醒的后继节点，还没将自己设置为头节点，没执行到呢.... 这个时候，当前线程直接跳出去结束了 此时并不需要担心 唤醒逻辑 在这里断开 ，因为被唤醒的线程，早晚会执行到doReleaseShared方法 2.head==null latch创建出来以后，没有任何线程调用过await方法之前，就有线程调用countDown操作，并且触发了唤醒阻塞节点的逻辑 3.h==tail break 条件不成立： 条件成立1的相反情况，此时唤醒他的节点 执行 h == head 不成立，此时 原头节点不会跳出，会继续唤醒新的头节点的后继节点。 */ if (h == head) break; &#125;&#125; 11.AQS.doAcquireSharedInterruptibly一旦 ThreadA 被唤醒，代码又会继续回到doAcquireSharedInterruptibly 中来执行。如果当前 state满足=0 的条件，则会执行 setHeadAndPropagate 方法。 123456789101112131415161718192021222324252627private void doAcquireSharedInterruptibly(int arg) throws InterruptedException &#123; final Node node = addWaiter(Node.SHARED); //创建一个共享模式的节点添加到队列中 boolean failed = true; try &#123; for (;;) &#123;//被唤醒的线程进入下一次循环继续判断 final Node p = node.predecessor(); if (p == head) &#123; int r = tryAcquireShared(arg);//就判断尝试获取锁 if (r &gt;= 0) &#123;//r&gt;=0 表示获取到了执行权限，这个时候因为 state!=0，所以不会执行这段代码 setHeadAndPropagate(node, r); p.next = null; // help GC 把当前节点移除 aqs 队列 failed = false; return; &#125; &#125; //阻塞线程 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) throw new InterruptedException(); &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; 12.setHeadAndPropagate这个方法的主要作用是把被唤醒的节点，设置成 head 节点。 然后继续唤醒队列中的其他线程。由于现在队列中有 3 个线程处于阻塞状态，一旦 ThreadA被唤醒，并且设置为 head 之后，会继续唤醒后续的ThreadB。​ 123456789101112131415private void setHeadAndPropagate(Node node, int propagate) &#123; Node h = head; // Record old head for check below //将当前节点设置为头节点 setHead(node); //1&gt;0 if (propagate &gt; 0 || h == null || h.waitStatus &lt; 0 || (h = head) == null || h.waitStatus &lt; 0) &#123; Node s = node.next; //条件一：s==null 什么时候成立呢？ 当前node节点已经是tail节点了， //条件二的前置条件：s!=null 要求s的模式是共享模式 if (s == null || s.isShared()) //继续向后唤醒 doReleaseShared(); &#125;&#125; 13.流程图 二，使用countdownlatch 是一个同步工具类，它允许一个或多个线程一直等待，直到其他线程的操作执行完毕再执行。从命名可以解读到 countdown 是倒数的意思，类似于倒计时的概念。​ countdownlatch 提供了两个方法，一个是 countDown，一个是 await， countdownlatch 初始化的时候需要传入一个整数，在这个整数倒数到 0 之前，调用了 await 方法的程序都必须要等待，然后通过 countDown 来倒数。​ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687/** * @author 二十 * @since 2021/9/6 2:00 下午 */public class DemoA &#123; private static CountDownLatch c = new CountDownLatch(6); private static ThreadPoolExecutor executor = new ThreadPoolExecutor( 6, 6, 1, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;&gt;(1), new MyDefaultFactory(), new ThreadPoolExecutor.AbortPolicy() ); public static void main(String[] args)throws Exception &#123; for (int i = 0; i &lt; 6; i++) executor.submit(()-&gt;&#123; System.out.println(Thread.currentThread().getName() + &quot;国被灭！&quot;); c.countDown(); &#125;); c.await(); if (Thread.currentThread().getName().equals(&quot;main&quot;)) System.out.println(&quot;main线程执行结束:&quot; + Thread.currentThread().getName() ); &#125; private static class MyDefaultFactory implements ThreadFactory&#123; private static Queue&lt;String&gt; queue = new LinkedList(); static &#123; for (int i = 1; i &lt;= 6; i++) queue.add(Objects.requireNonNull(Message.foreach_CountryEnum(i)).message); &#125; @Override public Thread newThread(Runnable r) &#123; return new Thread(r,&quot;thread-&quot;+queue.poll() +&quot;-er_shi&quot;); &#125; &#125; enum Message &#123; ONE(1, &quot;齐&quot;), TWO(2, &quot;楚&quot;), THREE(3, &quot;燕&quot;), FOUR(4, &quot;赵&quot;), FIVE(5, &quot;魏&quot;), SIX(6, &quot;韩&quot;); private int code; private String message; Message(int code, String message) &#123; this.code = code; this.message = message; &#125; public int getCode() &#123; return code; &#125; public void setCode(int code) &#123; this.code = code; &#125; public String getMessage() &#123; return message; &#125; public void setMessage(String message) &#123; this.message = message; &#125; public static Message foreach_CountryEnum(int index) &#123; Message[] countryEnums = Message.values(); for (Message countryEnum : countryEnums) &#123; if (countryEnum.getCode() == index) &#123; return countryEnum; &#125; &#125; return null; &#125; &#125;&#125;","categories":[{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"ReentrantReadWriteLock","slug":"JUC/ReentrantReadWriteLock","date":"2022-01-11T11:06:57.592Z","updated":"2022-01-11T11:29:11.743Z","comments":true,"path":"2022/01/11/JUC/ReentrantReadWriteLock/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/JUC/ReentrantReadWriteLock/","excerpt":"","text":"读写锁：读锁和写锁都会发生死锁。读和读之间允许共享 ，读和写之间独占，写和写之间独占。​ 一，读写锁的使用123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public class Demo7 &#123; public static void main(String[] args)throws Exception &#123; MyCache myCache=new MyCache(); for (int i = 1; i &lt;=5; i++) &#123; int num = i ; new Thread(()-&gt;&#123; myCache.put(String.valueOf(num),String.valueOf(num)); &#125;,String.valueOf(i)).start(); &#125; TimeUnit.SECONDS.sleep(3); for (int i = 1; i &lt;=5; i++) &#123; int num = i ; new Thread(()-&gt;&#123; myCache.get(String.valueOf(num)); &#125;,String.valueOf(i)).start(); &#125; &#125;&#125;class MyCache&#123; //volatile:表示经常变化的 private volatile Map&lt;String,Object&gt; map=new HashMap&lt;&gt;(); private ReadWriteLock lock=new ReentrantReadWriteLock(); public void put(String key,Object val)&#123; try &#123; lock.writeLock().lock(); System.out.println(Thread.currentThread().getName()+&quot;\\t 开始写入数据&quot;+key+&quot;!!!!!!!!!&quot;); TimeUnit.SECONDS.sleep(1); map.put(key,val); System.out.println(Thread.currentThread().getName()+&quot;\\t 完成写入数据&quot;+key+&quot;----------&quot;); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;finally &#123; lock.writeLock().unlock(); &#125; &#125; public void get(String key)&#123; try &#123; lock.readLock().lock(); System.out.println(Thread.currentThread().getName()+&quot;\\t 开始读取数据&quot;+key+&quot;!!!!!!!!!&quot;); TimeUnit.SECONDS.sleep(1); Object result = map.get(key); System.out.println(Thread.currentThread().getName()+&quot;\\t 完成读取数据&quot;+result+&quot;----------&quot;); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;finally &#123; lock.readLock().unlock(); &#125; &#125;&#125; 二，读写锁的原理读写锁用的是同一个 Sycn 同步器，因此等待队列、state 等也是同一个。​ 1.线程1写锁加锁线程1成功上锁，流程与ReentrantLock加锁相比没有特殊之处，不同的是写锁状态占了state的低16位，而读锁使用的是state的高16位。​ 2.线程2读锁加锁t2 执行 r.lock，这时进入读锁的 sync.acquireShared(1) 流程，首先会进入 tryAcquireShared 流程。如果有写锁占据，那么 tryAcquireShared 返回 -1 表示失败。​ tryAcquireShared 返回值表示: -1 表示失败 0 表示成功，但后继节点不会继续唤醒 正数表示成功，而且数值是还有几个后继节点需要唤醒，读写锁返回 1。 ​ 这时会进入 sync.doAcquireShared(1) 流程，首先也是调用 addWaiter 添加节点。​ t2 会看看自己的节点是不是老二，如果是，还会再次调用 tryAcquireShared(1) 来尝试获取锁。​ 如果没有成功，在 doAcquireShared 内 for (; ; ) 循环一次，把前驱节点的 waitStatus 改为 -1，再 for (;; ) 循环一次尝试 tryAcquireShared(1) 如果还不成功，那么在 parkAndCheckInterrupt() 处 park。​ 3.线程3读锁加锁，线程4写锁加锁这种状态下，假设又有 t3 加读锁和 t4 加写锁，这期间 t1 仍然持有锁，就变成了下面的样子。​ 4.线程1写锁释放锁这时会走到写锁的 sync.release(1) 流程，调用 sync.tryRelease(1) 成功，变成下面的样子​ 接下来执行唤醒流程 sync.unparkSuccessor，即让老二恢复运行，这时 t2 在 doAcquireShared 内parkAndCheckInterrupt() 处恢复运行。​ 这回再来一次 for (;;) 执行 tryAcquireShared 成功则让读锁计数加一。​ 这时 t2 已经恢复运行，接下来 t2 调用 setHeadAndPropagate(node, 1)，它原本所在节点被置为头节点。​ 事情还没完，在 setHeadAndPropagate 方法内还会检查下一个节点是否是 shared，如果是则调用doReleaseShared() 将 head 的状态从 -1 改为 0 并唤醒老二，这时 t3 在 doAcquireShared 内parkAndCheckInterrupt() 处恢复运行。​ 这回再来一次 for (;;) 执行 tryAcquireShared 成功则让读锁计数加一。​ 这时 t3 已经恢复运行，接下来 t3 调用 setHeadAndPropagate(node, 1)，它原本所在节点被置为头节点。​ 下一个节点不是 shared 了，因此不会继续唤醒 t4 所在节点。​ 5.线程2读锁释放锁，线程3读锁释放锁t2 进入 sync.releaseShared(1) 中，调用 tryReleaseShared(1) 让计数减一，但由于计数还不为零。​ t3 进入 sync.releaseShared(1) 中，调用 tryReleaseShared(1) 让计数减一，这回计数为零了，进入doReleaseShared() 将头节点从 -1 改为 0 并唤醒老二，即：​ 之后 t4 在 acquireQueued 中 parkAndCheckInterrupt 处恢复运行，再次 for (;;) 这次自己是老二，并且没有其他竞争，tryAcquire(1) 成功，修改头结点，流程结束。​ 三，源码分析1.写锁上锁流程1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162static final class NonfairSync extends Sync &#123; // ... 省略无关代码 // 外部类 WriteLock 方法, 方便阅读, 放在此处 public void lock() &#123; sync.acquire(1); &#125; // AQS 继承过来的方法, 方便阅读, 放在此处 public final void acquire(int arg) &#123; if ( // 尝试获得写锁失败 !tryAcquire(arg) &amp;&amp; // 将当前线程关联到一个 Node 对象上, 模式为独占模式 // 进入 AQS 队列阻塞 acquireQueued(addWaiter(Node.EXCLUSIVE), arg) ) &#123; selfInterrupt(); &#125; &#125; // Sync 继承过来的方法, 方便阅读, 放在此处 protected final boolean tryAcquire(int acquires) &#123; // 获得低 16 位, 代表写锁的 state 计数 Thread current = Thread.currentThread(); int c = getState(); int w = exclusiveCount(c); if (c != 0) &#123; if ( // c != 0 and w == 0 表示有读锁, 或者 w == 0 || // 如果 exclusiveOwnerThread 不是自己 current != getExclusiveOwnerThread() ) &#123; // 获得锁失败 return false; &#125; // 写锁计数超过低 16 位, 报异常 if (w + exclusiveCount(acquires) &gt; MAX_COUNT) throw new Error(&quot;Maximum lock count exceeded&quot;); // 写锁重入, 获得锁成功 setState(c + acquires); return true; &#125; if ( // 判断写锁是否该阻塞, 或者 writerShouldBlock() || // 尝试更改计数失败 !compareAndSetState(c, c + acquires) ) &#123; // 获得锁失败 return false; &#125; // 获得锁成功 setExclusiveOwnerThread(current); return true; &#125; // 非公平锁 writerShouldBlock 总是返回 false, 无需阻塞 final boolean writerShouldBlock() &#123; return false; &#125;&#125; 2.写锁释放锁的流程12345678910111213141516171819202122232425262728293031323334static final class NonfairSync extends Sync &#123; // ... 省略无关代码 // WriteLock 方法, 方便阅读, 放在此处 public void unlock() &#123; sync.release(1); &#125; // AQS 继承过来的方法, 方便阅读, 放在此处 public final boolean release(int arg) &#123; // 尝试释放写锁成功 if (tryRelease(arg)) &#123; // unpark AQS 中等待的线程 Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; &#125; return false; &#125; // Sync 继承过来的方法, 方便阅读, 放在此处 protected final boolean tryRelease(int releases) &#123; if (!isHeldExclusively()) throw new IllegalMonitorStateException(); int nextc = getState() - releases; // 因为可重入的原因, 写锁计数为 0, 才算释放成功 boolean free = exclusiveCount(nextc) == 0; if (free) &#123; setExclusiveOwnerThread(null); &#125; setState(nextc); return free; &#125;&#125; 3.读锁上锁流程123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151static final class NonfairSync extends Sync &#123; // ReadLock 方法, 方便阅读, 放在此处 public void lock() &#123; sync.acquireShared(1); &#125; // AQS 继承过来的方法, 方便阅读, 放在此处 public final void acquireShared(int arg) &#123; // tryAcquireShared 返回负数, 表示获取读锁失败 if (tryAcquireShared(arg) &lt; 0) &#123; doAcquireShared(arg); &#125; &#125; // Sync 继承过来的方法, 方便阅读, 放在此处 protected final int tryAcquireShared(int unused) &#123; Thread current = Thread.currentThread(); int c = getState(); // 如果是其它线程持有写锁, 获取读锁失败 if ( exclusiveCount(c) != 0 &amp;&amp; getExclusiveOwnerThread() != current ) &#123; return -1; &#125; int r = sharedCount(c); if ( // 读锁不该阻塞(如果老二是写锁，读锁该阻塞), 并且 !readerShouldBlock() &amp;&amp; // 小于读锁计数, 并且 r &lt; MAX_COUNT &amp;&amp; // 尝试增加计数成功 compareAndSetState(c, c + SHARED_UNIT) ) &#123; // ... 省略不重要的代码 return 1; &#125; return fullTryAcquireShared(current); &#125; // 非公平锁 readerShouldBlock 看 AQS 队列中第一个节点是否是写锁 // true 则该阻塞, false 则不阻塞 final boolean readerShouldBlock() &#123; return apparentlyFirstQueuedIsExclusive(); &#125; // AQS 继承过来的方法, 方便阅读, 放在此处 // 与 tryAcquireShared 功能类似, 但会不断尝试 for (;;) 获取读锁, 执行过程中无阻塞 final int fullTryAcquireShared(Thread current) &#123; HoldCounter rh = null; for (; ; ) &#123; int c = getState(); if (exclusiveCount(c) != 0) &#123; if (getExclusiveOwnerThread() != current) return -1; &#125; else if (readerShouldBlock()) &#123; // ... 省略不重要的代码 &#125; if (sharedCount(c) == MAX_COUNT) throw new Error(&quot;Maximum lock count exceeded&quot;); if (compareAndSetState(c, c + SHARED_UNIT)) &#123; // ... 省略不重要的代码 return 1; &#125; &#125; &#125; // AQS 继承过来的方法, 方便阅读, 放在此处 private void doAcquireShared(int arg) &#123; // 将当前线程关联到一个 Node 对象上, 模式为共享模式 final Node node = addWaiter(Node.SHARED); boolean failed = true; try &#123; boolean interrupted = false; for (; ; ) &#123; final Node p = node.predecessor(); if (p == head) &#123; // 再一次尝试获取读锁 int r = tryAcquireShared(arg); // 成功 if (r &gt;= 0) &#123; // ㈠ // r 表示可用资源数, 在这里总是 1 允许传播 //（唤醒 AQS 中下一个 Share 节点） setHeadAndPropagate(node, r); p.next = null; // help GC if (interrupted) selfInterrupt(); failed = false; return; &#125; &#125; if ( // 是否在获取读锁失败时阻塞（前一个阶段 waitStatus == Node.SIGNAL） shouldParkAfterFailedAcquire(p, node) &amp;&amp; // park 当前线程 parkAndCheckInterrupt() ) &#123; interrupted = true; &#125; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125; &#125; // ㈠ AQS 继承过来的方法, 方便阅读, 放在此处 private void setHeadAndPropagate(Node node, int propagate) &#123; Node h = head; // Record old head for check below // 设置自己为 head setHead(node); // propagate 表示有共享资源（例如共享读锁或信号量） // 原 head waitStatus == Node.SIGNAL 或 Node.PROPAGATE // 现在 head waitStatus == Node.SIGNAL 或 Node.PROPAGATE if (propagate &gt; 0 || h == null || h.waitStatus &lt; 0 || (h = head) == null || h.waitStatus &lt; 0) &#123; Node s = node.next; // 如果是最后一个节点或者是等待共享读锁的节点 if (s == null || s.isShared()) &#123; // 进入 ㈡ doReleaseShared(); &#125; &#125; &#125; // ㈡ AQS 继承过来的方法, 方便阅读, 放在此处 private void doReleaseShared() &#123; // 如果 head.waitStatus == Node.SIGNAL ==&gt; 0 成功, 下一个节点 unpark // 如果 head.waitStatus == 0 ==&gt; Node.PROPAGATE, 为了解决 bug, 见后面分析 for (; ; ) &#123; Node h = head; // 队列还有节点 if (h != null &amp;&amp; h != tail) &#123; int ws = h.waitStatus; if (ws == Node.SIGNAL) &#123; if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) continue; // loop to recheck cases // 下一个节点 unpark 如果成功获取读锁 // 并且下下个节点还是 shared, 继续 doReleaseShared unparkSuccessor(h); &#125; else if (ws == 0 &amp;&amp; !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) continue; // loop on failed CAS &#125; if (h == head) // loop if head changed break; &#125; &#125;&#125; 4.读锁释放锁的流程123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354static final class NonfairSync extends Sync &#123; // ReadLock 方法, 方便阅读, 放在此处 public void unlock() &#123; sync.releaseShared(1); &#125; // AQS 继承过来的方法, 方便阅读, 放在此处 public final boolean releaseShared(int arg) &#123; if (tryReleaseShared(arg)) &#123; doReleaseShared(); return true; &#125; return false; &#125; // Sync 继承过来的方法, 方便阅读, 放在此处 protected final boolean tryReleaseShared(int unused) &#123; // ... 省略不重要的代码 for (; ; ) &#123; int c = getState(); int nextc = c - SHARED_UNIT; if (compareAndSetState(c, nextc)) &#123; // 读锁的计数不会影响其它获取读锁线程, 但会影响其它获取写锁线程 // 计数为 0 才是真正释放 return nextc == 0; &#125; &#125; &#125; // AQS 继承过来的方法, 方便阅读, 放在此处 private void doReleaseShared() &#123; // 如果 head.waitStatus == Node.SIGNAL ==&gt; 0 成功, 下一个节点 unpark // 如果 head.waitStatus == 0 ==&gt; Node.PROPAGATE for (; ; ) &#123; Node h = head; if (h != null &amp;&amp; h != tail) &#123; int ws = h.waitStatus; // 如果有其它线程也在释放读锁，那么需要将 waitStatus 先改为 0 // 防止 unparkSuccessor 被多次执行 if (ws == Node.SIGNAL) &#123; if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) continue; // loop to recheck cases unparkSuccessor(h); &#125; // 如果已经是 0 了，改为 -3，用来解决传播性，见后文信号量 bug 分析 else if (ws == 0 &amp;&amp; !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) continue; // loop on failed CAS &#125; if (h == head) // loop if head changed break; &#125; &#125;&#125;","categories":[{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"AQS","slug":"JUC/AQS","date":"2022-01-11T11:06:49.064Z","updated":"2022-01-11T11:24:53.300Z","comments":true,"path":"2022/01/11/JUC/AQS/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/JUC/AQS/","excerpt":"","text":"一，LockLock在juc中是最核心的组件​ 1.Lock的实现Lock 本质上是一个接口，它定义了释放锁和获得锁的抽象方法，定义成接口就意味着它定义了锁的一个标准规范，也同时意味着锁的不同实现。实现 Lock 接口的类有很多，以下为几个常见的锁实现。​ ReentrantLock：表示重入锁，它是唯一一个实现了 Lock 接口的类。重入锁指的是线程在获得锁之后，再次获取该锁不需要阻塞，而是直接关联一次计数器增加重入次数。​ ReentrantReadWriteLock：重入读写锁，它实现了 ReadWriteLock 接口，在这个类中维护了两个锁，一个是 ReadLock，一个是 WriteLock，他们都分别实现了 Lock接口。读写锁是一种适合读多写少的场景下解决线程安全问题的工具，基本原则是： 读和读不互斥、读和写互斥、写和写互斥。也就是说涉及到影响数据变化的操作都会存在互斥。​ StampedLock： stampedLock 是 JDK8 引入的新的锁机制，可以简单认为是读写锁的一个改进版本，读写锁虽然通过分离读和写的功能使得读和读之间可以完全并发，但是读和写是有冲突的，如果大量的读线程存在，可能会引起写线程的饥饿。stampedLock 是一种乐观的读策略，使得乐观锁完全不会阻塞写线程。​ 2.Lock的类关系图Lock有很多的实现，但是直观的实现是ReentrantLock重入锁。​ 12345void lock() // 如果锁可用就获得锁，如果锁不可用就阻塞直到锁释放void lockInterruptibly() // 和lock()方法相似, 但阻塞的线程 可 中 断 ， 抛 出java.lang.InterruptedException 异常boolean tryLock() // 非阻塞获取锁;尝试获取锁，如果成功返回 trueboolean tryLock(long timeout, TimeUnit timeUnit) //带有超时时间的获取锁方法void unlock() // 释放锁 二，ReentrantLock重入锁，表示支持重新进入的锁，也就是说，如果当前线程 t1 通过调用 lock 方法获取了锁之后，再次调用 lock，是不会再阻塞去获取锁的，直接增加重试次数就行了。synchronized 和 ReentrantLock 都是可重入锁。​ 1.重入锁的设计目的比如调用 demo 方法获得了当前的对象锁，然后在这个方法中再去调用demo2，demo2 中的存在同一个实例锁，这个时候当前线程会因为无法获得demo2 的对象锁而阻塞，就会产生死锁。重入锁的设计目的是避免线程的死锁。​ 1234567891011121314151617public class ReentrantDemo &#123; public synchronized void demo() &#123; System.out.println(&quot;begin:demo&quot;); demo2(); &#125; public void demo2() &#123; System.out.println(&quot;begin:demo1&quot;); synchronized (this) &#123; &#125; &#125; public static void main(String[] args) &#123; ReentrantDemo rd = new ReentrantDemo(); new Thread(rd::demo).start(); &#125;&#125; 2.ReentrantLock使用123456789101112131415161718192021222324public class AtomicDemo &#123; private static int count = 0; static Lock lock = new ReentrantLock(); public static void inc() &#123; lock.lock(); try &#123; Thread.sleep(1); count++; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;finally &#123; lock.unlock(); &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; for (int i = 0; i &lt; 1000; i++) &#123; new Thread(() -&gt; AtomicDemo.inc()).start(); &#125; Thread.sleep(3000); System.out.println(&quot;result:&quot; + count); &#125;&#125; 3.ReentrantReadWriteLock​ 以前理解的锁，基本都是排他锁，也就是这些锁在同一时刻只允许一个线程进行访问，而读写所在同一时刻可以允许多个线程访问，但是在写线程访问时，所有的读线程和其他写线程都会被阻塞。读写锁维护了一对锁，一个读锁、一个写锁;一般情况下，读写锁的性能都会比排它锁好，因为大多数场景读是多于写的。在读多于写的情况下，读写锁能够提供比排它锁更好的并发性和吞吐量。​ 12345678910111213141516171819202122232425262728public class LockDemo &#123; static Map&lt;String, Object&gt; cacheMap = new HashMap&lt;&gt;(); static ReentrantReadWriteLock rwl = new ReentrantReadWriteLock(); static Lock read = rwl.readLock(); static Lock write = rwl.writeLock(); public static final Object get(String key) &#123; System.out.println(&quot; 开始读取数据&quot;); read.lock(); // 读锁 try &#123; return cacheMap.get(key); &#125; finally &#123; read.unlock(); &#125; &#125; public static final Object put(String key, Object value) &#123; System.out.println(&quot; 开始写数据&quot;); write.lock(); try &#123; return cacheMap.put(key, value); &#125; finally &#123; write.unlock(); &#125; &#125;&#125; 在这个案例中，通过 hashmap 来模拟了一个内存缓存，然后使用读写锁来保证这个内存缓存的线程安全性。当执行读操作的时候，需要获取读锁，在并发访问的时候，读锁不会被阻塞，因为读操作不会影响执行结果。​ 在执行写操作时，线程必须要获取写锁，当已经有线程持有写锁的情况下，当前线程会被阻塞，只有当写锁释放以后，其他读写操作才能继续执行。使用读写锁提升读操作的并发性，也保证每次写操作对所有的读写操作的可见性。​ 读锁与读锁可以共享​ 读锁与写锁不可以共享（排他） 写锁与写锁不可以共享（排他）​ 三，ReentrantLock 的实现原理​ 锁的基本原理是，基于将多线程并行任务通过某一种机制实现线程的串行执行，从而达到线程安全性的目的。在 ReentrantLock 中，在多线程竞争重入锁时，竞争失败的线程是如何实现阻塞以及被唤醒的呢？​ 1.AQS是什么？在 Lock 中，用到了一个同步队列 AQS，全称 AbstractQueuedSynchronizer，它是一个同步工具也是 Lock 用来实现线程同步的核心组件。​ 2.AQS的两种功能使用层面上来说，AQS的功能分为两种:独占和共享。​ 独占锁：每次只能有一个线程持有锁。​ 共享锁：允许多线程同时获取锁，并发访问共享资源。​ 3.AQS的内部实现AQS队列内部维护的是一个FIFO的双向链表，这种结构的特点是每个数据结构都有两个指针，分别指向直接的后继节点和直接的前驱结点。所以双向链表可以从任意一个结点开始很方便的访问前驱和后继。每个Node其实是由线程封装，当线程争抢锁失败后会封装成Node加入到AQS队列中去，当获取锁的线程释放锁之后，会从队列中唤醒一个阻塞的节点（线程）。​ 4.Node的组成1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950static final class Node &#123; /** Marker to indicate a node is waiting in shared mode */ static final Node SHARED = new Node(); /** Marker to indicate a node is waiting in exclusive mode */ static final Node EXCLUSIVE = null; static final int CANCELLED = 1; static final int SIGNAL = -1; static final int CONDITION = -2; static final int PROPAGATE = -3; volatile int waitStatus; volatile Node prev; //前驱结点 volatile Node next; //后继节点 volatile Thread thread; //当前线程 Node nextWaiter; //存储在condition队列中的后继节点 //是否为共享锁 final boolean isShared() &#123; return nextWaiter == SHARED; &#125; final Node predecessor() throws NullPointerException &#123; Node p = prev; if (p == null) throw new NullPointerException(); else return p; &#125; Node() &#123; // Used to establish initial head or SHARED marker &#125; Node(Thread thread, Node mode) &#123; // Used by addWaiter this.nextWaiter = mode; this.thread = thread; &#125; Node(Thread thread, int waitStatus) &#123; // Used by Condition this.waitStatus = waitStatus; this.thread = thread; &#125;&#125; 5.释放锁以及添加线程对于队列的变化当出现锁竞争以及释放锁的时候，AQS 同步队列中的节点会发生变化，首先看一下添加节点的场景。​ 这里会涉及到两个变化：​ 新的线程封装成Node节点追加到同步队列中，设置prev节点以及修改当前节点的前驱节点的next节点指向自己。 通过CAS的方式将tail重新指向新的尾部节点。 ​ head节点表示获取锁成功的节点，当头结点在释放同步状态的时候，会唤醒后继节点，如果后继节点获得锁成功，会把自己设置为头节点，节点的变化过程如下：​ ​ 这个过程也是涉及到两个变化：​ 修改head节点指向下一个获得锁的节点 新的获得锁的节点，将prev的指针指向null ​ 设置head节点不需要用CAS，原因是设置head节点是由获得锁的线程来完成的，而同步锁只能由一个线程来获得，所以不需要CAS保证，只需要把head节点设置为原首节点的后继节点，并且断开原head节点的next引用即可。 四，ReentrantLock 的源码分析1.加锁逻辑ReentrantLock.lock123public void lock() &#123; sync.lock();&#125; ​ sync 实际上是一个抽象的静态内部类，它继承了 AQS 来实现重入锁的逻辑，前面说过 AQS 是一个同步队列，它能够实现线程的阻塞以及唤醒，但它并不具备业务功能，所以在不同的同步场景中，会继承 AQS 来实现对应场景的功能。​ Sync 有两个具体的实现类，分别是： NofairSync：表示可以存在抢占锁的功能，也就是说不管当前队列上是否存在其他线程等待，新线程都有机会抢占锁。 FailSync: 表示所有线程严格按照 FIFO 来获取锁。​ NonfairSync.lock 非公平锁和公平锁最大的区别在于：非公平锁中抢占锁的逻辑是，不管有没有线程排队，先上来cas去抢占一下 cas成功，就表示成功获得了锁 cas失败，调用acquire(1)走锁竞争逻辑 ​ state 是 AQS 中的一个属性，它在不同的实现中所表达的含义不一样，对于重入锁的实现来说，表示一个同步状态。它有两个含义的表示：​ 当 state=0 时，表示无锁状态。 ​ 当 state&gt;0 时，表示已经有线程获得了锁，也就是 state=1，但是因为ReentrantLock 允许重入，所以同一个线程多次获得同步锁的时候，state 会递增，比如重入 5 次，那么 state=5。 而在释放锁的时候，同样需要释放 5 次直到 state=0其他线程才有资格获得锁。 ​ 1234567final void lock() &#123; //如果cas的方式修改state状态成功，说明获取到了锁，直接设置当前持有锁的线程是当前线程 if (compareAndSetState(0, 1)) setExclusiveOwnerThread(Thread.currentThread()); else //cas失败，走抢占锁的逻辑 acquire(1);&#125; \u0000 AQS.acquireacquire是AQS中的方法，如果cas操作未能成功，说明state已经不为0，此时继续acquire(1)操作。​ 这个方法的主要逻辑：​ 通过 tryAcquire 尝试获取独占锁，如果成功返回 true，失败返回 false. 如果 tryAcquire 失败，则会通过 addWaiter 方法将当前线程封装成 Node 添加到 AQS 队列尾部. acquireQueued，将 Node 作为参数，通过自旋去尝试获取锁。12345678public final void acquire(int arg) &#123; //如果尝试去抢占锁失败 就讲当前线尾插进入阻塞队列，让当前现成的node自旋的方式去尝试获取锁 if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) //响应中断逻辑，表示如果当前线程在 acquireQueued 中被中断过，则需要产生一个中断请求， //原因是线程在调用 acquireQueued 方法的时候是不会响应中断请求的。 selfInterrupt();&#125; AQS.selfInterrupt 表示如果当前线程在 acquireQueued 中被中断过，则需要产生一个中断请求，原因是线程在调用 acquireQueued 方法的时候是不会响应中断请求的。 1234//执行中断逻辑static void selfInterrupt() &#123; Thread.currentThread().interrupt();&#125; AQS.tryAcquire这里采用模板模式，具体的逻辑交给继承该类的子类实现，直接看非公平锁的逻辑。 123protected boolean tryAcquire(int arg) &#123; throw new UnsupportedOperationException();&#125; NonfairSync.tryAcquire这个方法的作用是尝试获取锁，如果成功返回true，不成功返回false。​ 它是重写AQS类中的tryAcquire方法，AQS中的tryAcquire()的定义，并没有实现，而是抛出异常。 123protected final boolean tryAcquire(int acquires) &#123; return nonfairTryAcquire(acquires);&#125; ReentrantLock.nofairTryAcquire 获取当前线程，判断当前的锁的状态 如果state=0 表示当前是无锁状态，通过cas更新state状态的值 当前线程是属于重入，则增加重入次数12345678910111213141516171819202122232425262728final boolean nonfairTryAcquire(int acquires) &#123; //获取当前线程 final Thread current = Thread.currentThread(); //获取当前的state值 int c = getState(); //如果此时没有线程持有锁 if (c == 0) &#123; //cas去获取一次锁 if (compareAndSetState(0, acquires)) &#123; //走到这里说明cas的方式获取到了锁，直接设置当前锁的线程是当前线程，并返回true setExclusiveOwnerThread(current); return true; &#125; &#125; //走到这里说明 当前锁被其他线程持有 或者 当前cas失败，存在竞争 else if (current == getExclusiveOwnerThread()) &#123; //如果持有锁的线程是当前线程自己 //执行到这里说明是一次重入锁的逻辑，直接在原有状态上+就可以 int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; &#125; //什么情况下会走到这里？ //cas失败，锁被其他线程持有， //state》0 return false;&#125; AQS.addWaiter 当 tryAcquire 方法获取锁失败以后，则会先调用 addWaiter 将当前线程封装成Node. 入参 mode 表示当前节点的状态，传递的参数是 Node.EXCLUSIVE，表示独占状态。意味着重入锁用到了 AQS 的独占锁功能。​ 将当前线程封装成 Node ​ 当前链表中的 tail 节点是否为空，如果不为空，则通过 cas 操作把当前线程的node 添加到 AQS 队列 ​ 如果为空或者 cas 失败，调用 enq 将节点添加到 AQS 队列1234567891011121314151617181920private Node addWaiter(Node mode) &#123; //构建一个新的节点 Node node = new Node(Thread.currentThread(), mode); // Try the fast path of enq; backup to full enq on failure Node pred = tail; //尾结点不为空，说明队列已经初始化过了 if (pred != null) &#123; //将当前节点的前驱节点指向为尾结点 node.prev = pred; //cas的方式设置尾结点 if (compareAndSetTail(pred, node)) &#123; //cas成功，让尾结点的后继指针指向node pred.next = node; return node; &#125; &#125; //执行到这里 说明队列尚未初始化， 或者cas的方式设置尾结点失败，说明有其他线程先一步入队了。 enq(node); return node;&#125; AQS.enq enq 就是通过自旋操作把当前节点加入到队列中。 123456789101112131415161718192021private Node enq(final Node node) &#123; //自旋： for (;;) &#123; Node t = tail; if (t == null) &#123; // 尾结点为空，说明需要初始化 if (compareAndSetHead(new Node())) tail = head; //注意，这里初始化完之后并没有直接退出，而是继续往下走，为啥呢？ //因为当前执行的是初始化操作，那就说明，当前这个抢占锁失败的线程需要给当前持有锁的线程补充一个头节点 &#125; else &#123;//尾结点不为空 //让当前节点的前驱指针指向尾结点 node.prev = t; //cas的方式设置尾结点 if (compareAndSetTail(t, node)) &#123; //让尾结点的后继指针指向当前节点 t.next = node; return t; &#125; &#125; &#125;&#125; AQS .acquireQueued通过 addWaiter 方法把线程添加到链表后，会接着把 Node 作为参数传递给acquireQueued 方法，去竞争锁​ 获取当前节点的 prev 节点 如果 prev 节点为 head 节点，那么它就有资格去争抢锁，调用 tryAcquire 抢占锁 ​ 抢占锁成功以后，把获得锁的节点设置为 head，并且移除原来的初始化 head节点 ​ 如果获得锁失败，则根据 waitStatus 决定是否需要挂起线程 最后，通过 cancelAcquire 取消获得锁的操作 123456789101112131415161718192021222324252627282930final boolean acquireQueued(final Node node, int arg) &#123; boolean failed = true; try &#123; boolean interrupted = false; for (;;) &#123; //获取当前节点的前驱节点 final Node p = node.predecessor(); //如果当前节点的前驱节点是头节点，说明当前节点拥有了竞争锁的权利，尝试cas的方式去获取一次锁 if (p == head &amp;&amp; tryAcquire(arg)) &#123; //执行到这里说名cas获取到了锁 //设置头节点为当前节点，这里为啥不用加锁，因为设置头节点的线程是持有锁的线程，独占模式下，只有一个线程持有锁，所以设置头节点不存在线程安全问题 setHead(node); //让p的后继指针指向null 帮助垃圾回收 p.next = null; // help GC //声明获取锁成功 failed = false; //当前线程不需要被中断 return interrupted; &#125; //当前持有锁的线程可能还没有释放锁，所以当前线程尝试获取锁会失败，这个时候当前线程应该park if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) //这里比较友好，仅仅是把中断标识位改成true，如果是响应中断逻辑，直接这里抛异常了，然后走cancelAcquire(node)的逻辑。 interrupted = true; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; AQS.shouldParkAfterFailedAcquire 如果 ThreadA 的锁还没有释放的情况下，ThreadB 和 ThreadC 来争抢锁肯定是会失败，那么失败以后会调用 shouldParkAfterFailedAcquire 方法​ Node 有 5 种状态，分别是：CANCELLED（1），SIGNAL（-1）、CONDITION（-2）、PROPAGATE(-3)、默认状态(0) CANCELLED: 在同步队列中等待的线程等待超时或被中断，需要从同步队列中取消该 Node 的结点, 其结点的 waitStatus 为 CANCELLED，即结束状态，进入该状态后的结点将不会再变化 ​ SIGNAL: 只要前置节点释放锁，就会通知标识为 SIGNAL 状态的后续节点的线程 ​ CONDITION： 和 Condition 有关系 ​ PROPAGATE：共享模式下，PROPAGATE 状态的线程处于可运行状态 ​ 0:初始状态 ​ 这个方法的主要作用是，通过 Node 的状态来判断，ThreadA 竞争锁失败以后是否应该被挂起。​ 如果 ThreadA 的 pred 节点状态为 SIGNAL，那就表示可以放心挂起当前线程 ​ 通过循环扫描链表把 CANCELLED 状态的节点移除 ​ 修改 pred 节点的状态为 SIGNAL,返回 false. ​ 返回 false 时，也就是不需要挂起，返回 true，则需要调用 parkAndCheckInterrupt挂起当前线程 1234567891011121314151617181920private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) &#123; //前驱节点的等待状态 int ws = pred.waitStatus; //如果前置节点为 SIGNAL，意味着只需要等待其他前置节点的线程被释放 if (ws == Node.SIGNAL) return true; //ws 大于 0，意味着 prev 节点取消了排队，直接移除这个节点就行 if (ws &gt; 0) &#123; do &#123;//相当于: pred=pred.prev; node.prev=pred; node.prev = pred = pred.prev; &#125; while (pred.waitStatus &gt; 0);//这里采用循环，从双向列表中移除 CANCELLED 的节点 pred.next = node; &#125; else &#123; //利用 cas 设置 prev 节点的状态为 SIGNAL(-1) compareAndSetWaitStatus(pred, ws, Node.SIGNAL); &#125; return false;&#125; AQS.parkAndCheckInterrupt​ 使用 LockSupport.park 挂起当前线程变成 WATING 状态​ Thread.interrupted，返回当前线程是否被其他线程触发过中断请求，也就是thread.interrupt(); 如果有触发过中断请求，那么这个方法会返回当前的中断标识true，并且对中断标识进行复位标识已经响应过了中断请求。如果返回 true，意味着在 acquire 方法中会执行 selfInterrupt()。 1234private final boolean parkAndCheckInterrupt() &#123; LockSupport.park(this); return Thread.interrupted();&#125; AQS.cancelAcquire123456789101112131415161718192021222324252627282930313233343536373839404142434445464748private void cancelAcquire(Node node) &#123; //如果节点不存在，直接返回 if (node == null) return; //将当前节点的线程设置为空 node.thread = null; //获取当前节点的前驱节点 Node pred = node.prev; //循环判断：如果前驱节点是一个取消状态，那就继续往前找 while (pred.waitStatus &gt; 0) node.prev = pred = pred.prev; //可能是当前节点，可能是ws&gt;0的节点 Node predNext = pred.next; //设置当前节点的waitStatus是取消状态 node.waitStatus = Node.CANCELLED; /** * 当前取消排队的node所在 队列的位置不同，执行的出队策略是不一样的，一共分为三种情况： * 1.当前node是队尾 tail -&gt; node * 2.当前node 不是 head.next 节点，也不是 tail * 3.当前node 是 head.next节点。 */ //尾结点且cas tail成功 if (node == tail &amp;&amp; compareAndSetTail(node, pred)) &#123; //pred.next = null node出队 compareAndSetNext(pred, predNext, null); &#125; else &#123; int ws; /* node不是head.next node 不是tail node的前驱节点是取消状态 或 假设前驱状态是 &lt;= 0 则设置前驱状态为 Signal状态..表示要唤醒后继节点。 if里面要做的就是让node 的prev 跨过node 直接指向 node.next */ if (pred != head &amp;&amp; ((ws = pred.waitStatus) == Node.SIGNAL || (ws &lt;= 0 &amp;&amp; compareAndSetWaitStatus(pred, ws, Node.SIGNAL))) &amp;&amp; pred.thread != null) &#123; Node next = node.next; if (next != null &amp;&amp; next.waitStatus &lt;= 0) compareAndSetNext(pred, predNext, next); &#125; else &#123; //node是head.next &amp;&amp; node !=tail //那就跨过node head.next = node.next node.next.prev=head unparkSuccessor(node); &#125; node.next = node; // help GC &#125;&#125; 2.释放锁的逻辑如果这个时候 ThreadA 释放锁了，那么来看锁被释放后会产生什么效果？​ ReentrantLock.unlock1234public void unlock() &#123; //执行sync的release方法，默认是非公平锁，所以看非公平锁的逻辑 sync.release(1);&#125; AQS.release123456789101112public final boolean release(int arg) &#123; //如果尝试释放锁成功 if (tryRelease(arg)) &#123; //如果头结点不为空，且头节点状态不等于0 Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) //唤醒后继线程 unparkSuccessor(h); return true; &#125; return false;&#125; ReentrantLock.tryRelease这个方法可以认为是一个设置锁状态的操作，通过将 state 状态减掉传入的参数值（参数是 1），如果结果状态为 0，就将排它锁的 Owner 设置为 null，以使得其它的线程有机会进行执行。​ 在排它锁中，加锁的时候状态会增加 1（当然可以自己修改这个值），在解锁的时候减掉 1，同一个锁，在可以重入后，可能会被叠加为 2、3、4 这些值，只有 unlock()的次数与 lock()的次数对应才会将 Owner 线程设置为空，而且也只有这种情况下才会返回 true。 1234567891011121314protected final boolean tryRelease(int releases) &#123; //将当前值与传入的值进行相减 int c = getState() - releases; //如果当前线程不是线程拥有者，抛异常 if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; if (c == 0) &#123;//重入锁已经完全释放 free = true; setExclusiveOwnerThread(null);//设置当前线程的拥有者为null &#125; setState(c);//修改state状态 return free;&#125; AQS.unparkSuccessor1234567891011121314151617181920private void unparkSuccessor(Node node) &#123; //获取头节点的状态 int ws = node.waitStatus; if (ws &lt; 0)//如果头结点的状态小于0 //修改头节点的状态为0 compareAndSetWaitStatus(node, ws, 0); //获取头节点的下一个节点 Node s = node.next; /如果下一个节点为 null 或者 status&gt;0 表示 cancelled 状态 if (s == null || s.waitStatus &gt; 0) &#123; s = null; //通过从尾部节点开始扫描，找到距离 head 最近的一个waitStatus&lt;=0 的节点 for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) if (t.waitStatus &lt;= 0) s = t; &#125; if (s != null)//唤醒这个节点 LockSupport.unpark(s.thread);&#125; 通过锁的释放，原本的结构就发生了一些变化。head 节点的 waitStatus 变成了 0，ThreadB 被唤醒。​ 为什么释放锁的时候是从尾结点开始扫描？​ 再回到 enq那个方法。看一个新的节点是如何加入到链表中的​ 将新的节点的 prev 指向 tail。 通过 cas 将 tail 设置为新的节点，因为 cas 是原子操作所以能够保证线程安全性。 ​ t.next=node；设置原 tail 的 next 节点指向新的节点。 ​ 在 cas 操作之后，t.next=node 操作之前。 存在其他线程调用 unlock 方法从 head开始往后遍历，由于 t.next=node 还没执行意味着链表的关系还没有建立完整。就会导致遍历到 t 节点的时候被中断。所以从后往前遍历，一定不会存在这个问题。​ 原本被挂起的线程在被唤醒后继续执行​ 通过 ReentrantLock.unlock，原本挂起的线程被唤醒以后继续执行，原来被挂起的线程是在 acquireQueued 方法中，所以被唤醒以后继续从这个方法开始执行。​ 由于 ThreadB 的 prev 节点指向的是 head，并且 ThreadA 已经释放了锁。所以这个时候调用 tryAcquire 方法时，可以顺利获取到锁。​ 把 ThreadB 节点当成 head。 ​ 把原 head 节点的 next 节点指向为 null。 ​ 3.公平锁和非公平锁的区别锁的公平性是相对于获取锁的顺序而言的，如果是一个公平锁，那么锁的获取顺序就应该符合请求的绝对时间顺序，也就是 FIFO。 在上面分析的例子来说，只要CAS 设置同步状态成功，则表示当前线程获取了锁，而公平锁则不一样，差异点有两个： FairSync.tryAcquire123final void lock() &#123; acquire(1); &#125; 非公平锁在获取锁的时候，会先通过 CAS 进行抢占，而公平锁则不会。​ FairSync .tryAcquire123456789101112131415161718192021222324252627282930protected final boolean tryAcquire(int acquires) &#123; //获取当前线程 final Thread current = Thread.currentThread(); //获取当前state状态 int c = getState(); if (c == 0) &#123;//状态为0，说明此时没有现成持有锁 //如果队列里面没有节点 且cas的方式获取锁成功 if (!hasQueuedPredecessors() &amp;&amp; compareAndSetState(0, acquires)) &#123; //设置持有锁的线程我当前线程 setExclusiveOwnerThread(current); return true; &#125; &#125; //如果当前线程是获取锁的线程 else if (current == getExclusiveOwnerThread()) &#123; //锁重入逻辑 int nextc = c + acquires; if (nextc &lt; 0) throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; &#125; /* 返回false的情况： 1. 当前锁被其他线程持有 2. 队列里面有节点或cas争抢锁失败 */ return false;&#125; 这个方法与 nonfairTryAcquire(int acquires)比较，不同的地方在于判断条件多了hasQueuedPredecessors()方法，也就是加入了[同步队列中当前节点是否有前驱节点]的判断，如果该方法返回 true，则表示有线程比当前线程更早地请求获取锁，因此需要等待前驱线程获取并释放锁之后才能继续获取锁。 hasQueuedPredecessors1234567891011121314//首先什么时候返回false？返回false代表当前队列没有线程或者当前队列头节点的下一个线程就是当前线程//1.队列为空//2.头节点的下一个节点的线程是当前线程public final boolean hasQueuedPredecessors() &#123; Node t = tail; Node h = head; Node s; //h!=t:头节点不是尾结点，说明队列里面还有其他的节点 //(头节点的下一个节点为空||头节点的后继节点对应的线程不是当前线程) //头节点的下一个节点的线程正在拿到锁 或者 头节点的下一个节点的线程不是当前线程 return h != t &amp;&amp; ((s = h.next) == null || s.thread != Thread.currentThread()); &#125; 五，Condition 条件队列对比synchronized管程模型：Condition原理图：​ 1.AQS.ConditionObject12345678public class ConditionObject implements Condition, java.io.Serializable &#123; private static final long serialVersionUID = 1173984872572414699L; //第一个等待的节点 private transient Node firstWaiter; //最后一个等待的节点 private transient Node lastWaiter; public ConditionObject() &#123; &#125; 2.Condition接口12345678910111213141516public interface Condition &#123; void await() throws InterruptedException; void awaitUninterruptibly(); long awaitNanos(long nanosTimeout) throws InterruptedException; boolean await(long time, TimeUnit unit) throws InterruptedException; boolean awaitUntil(Date deadline) throws InterruptedException; void signal(); void signalAll();&#125; 3.newCondition()创建一个等待条件 123public Condition newCondition() &#123; return sync.newCondition();&#125; 123final ConditionObject newCondition() &#123; return new ConditionObject();&#125; 实际上这里new了一个AQS的ConditionObject。​ 4.AQS.await这个方法的逻辑就是先进行边界判断，如果当前线程已经被中断，就抛出中断异常。否则将当前线程封装成节点加入到条件队列，并释放全部锁资源。接下来判断是否当前持有锁的线程执行了当前线程的signal方法： 如果没有执行：当前线程就还是在条件队列，此时应当将当前线程挂起 如果执行了：当前线程就迁移到了阻塞队列，就走竞争锁的逻辑…. 1234567891011121314151617181920212223242526272829303132333435363738public final void await() throws InterruptedException &#123; //如果当前线程已经被挂起，直接抛出中断异常 if (Thread.interrupted()) throw new InterruptedException(); //将当前线程封装成节点加入到条件队列 Node node = addConditionWaiter(); //释放线程的全部锁资源 //为什么这里要释放掉全部的锁？ //如果此时线程不释放锁资源，其他线程没办法拿到锁触发当前线程的唤醒机制 int savedState = fullyRelease(node); //中断状态： //-1:当前线程在条件队列接收到了中断信号 //0:当前线程在条件队列没有接收到中断信号 //1:当前线程在条件队列没有接受到中断信号，但是在阻塞队列接收到了中断信号 int interruptMode = 0; //如果当前线程不在阻塞队列 while (!isOnSyncQueue(node)) &#123; //挂起当前线程 LockSupport.park(this); //判断当前线程在等待期间是否被中断过，无论是否发生中断，最终都会进入到阻塞队列 //什么时候会被唤醒？都有几种情况？ //1.常规：外部线程获取到lock之后，调用了signal方法，转移条件队列的头节点到阻塞队列，当这个节点获取到锁之后，会唤醒 //2.转移至阻塞队列后，发现阻塞队列的前驱节点状态是取消状态，此时会唤醒当前节点 //3.当前节点挂起期间，被外部线程使用中断唤醒... if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) break; &#125; //来到这里说明当前线程已经被其他线程调用了signal方法，加入到了阻塞队列 //如果当前线程在竞争锁的过程中被中断过 &amp;&amp; 中断标识位不是 throw_ie if (acquireQueued(node, savedState) &amp;&amp; interruptMode != THROW_IE) //将当前线程的中断标识位设置为重新中断 interruptMode = REINTERRUPT; if (node.nextWaiter != null) unlinkCancelledWaiters(); if (interruptMode != 0) reportInterruptAfterWait(interruptMode);&#125; 5.Condition.addConditionWaiter线程进入条件队列挂起逻辑： 如果队列有元素，但是元素的状态不对，做一次全队的清理，将所有取消状态的节点干掉，并重新获取尾结点 将线程封装成node节点，判断当前条件队列是否有元素 如果没有元素，就将当前node设置为头节点 如果有元素，就将当前尾结点的next指针指向node 最终设置当前节点为尾节点，并返回当前线程的node。 1234567891011121314151617181920212223private Node addConditionWaiter() &#123; //获取尾结点的引用 Node t = lastWaiter; //条件一：尾结点！=空 //条件二：尾结点的状态！=-2，说明被中断了 if (t != null &amp;&amp; t.waitStatus != Node.CONDITION) &#123; //做一次全队列的垃圾节点清理 unlinkCancelledWaiters(); //重新获取尾结点的引用，因为在刚才的清理中，尾结点可能换成新的了 t = lastWaiter; &#125; //封装成node Node node = new Node(Thread.currentThread(), Node.CONDITION); //如果尾结点为空，说明队列为空，当前节点是进入队列的第一个元素 if (t == null) firstWaiter = node; //队列不为空 else t.nextWaiter = node; lastWaiter = node; //返回当前线程封装成的节点 return node;&#125; 6.AQS.fullyRelease释放当前节点线程的全部锁资源 正常情况下是会释放成功的，但是可能存在未持有锁的线程调用await方法，错误写法 这个时候，就会将当前线程对应的node状态修改为取消状态，后继线程在假如队列的时候，就会把取消状态的线程清理出去 如果成功释放了锁，会返回当前线程释放的state值，为什么要返回？ 因为当前节点迁移到阻塞队列以后，再次被唤醒，并且当前节点在队列中是头节点的下一个节点而且state状态=0表示无锁，这个时候说明当前节点要去竞争锁，此时要将node的state设置为savedStated 12345678910111213141516171819202122232425262728293031final int fullyRelease(Node node) &#123; /** * 完全释放锁是否成功 * 当failed失败的时候，说明当前线程是未持有锁调用 await 方法的线程... 错误写法 * 假设失败，在finally代码块中 会将刚刚加入到条件队列的 当前线程对应的node状态 修改为 取消状态 * 后继线程就会将取消状态的节点给清理出去... */ boolean failed = true; try &#123; //获取当前线程持有的state值总数 int savedState = getState(); //绝大部分情况下：release这里会返回true if (release(savedState)) &#123; //失败标记设置为false failed = false; /** * 返回当前线程释放的state值 * 为什么要返回？ * 因为当节点被迁移到阻塞队列以后，再次被唤醒，且当前node在阻塞队列中是head.next 而且 * 当前lock状态是state=0 的情况下 当前node 可以获取到锁 此时需要将state 设置为 savedState */ return savedState; &#125; else &#123; throw new IllegalMonitorStateException(); &#125; &#125; finally &#123; //如果失败了，就把当前节点状态改成取消 if (failed) node.waitStatus = Node.CANCELLED; &#125;&#125; 此时，同步队列会触发锁的释放和重新竞争。ThreadB 获得了锁。 7.isOnSyncQueue判断节点是否在阻塞队列 第一个if判断： 节点状态==-2说明当前节点一定在条件队列 当前节点的前驱节点为空：首先分析前置条件 node 的状态肯定不是 -2 ，可能=0或者1，等于1就说明当前节点是未持有锁的await，最终状态会被设置为取消，等于0就说明当前节点已经被持有锁的线程唤醒，但是唤醒之后是先修改状态在入队，所以前驱节点为空，说明此时是当前节点已经被唤醒，但是还未进入到阻塞队列。 执行到第二个if，当前节点的下一个节点不为空，说明一定在同步队列，为啥那？ 执行到这里，可以排除掉取消状态的节点，也就是说node的状态一定不等于1，为啥？取消状态的节点不会被加入到阻塞队列 入队的两个条件：当前节点的前驱节点设置为tail，然后cas设置当前节点为tail节点，两个条件都成功了，才是真正进入到了阻塞队列 所以说，就算前驱节点不等于空，也不是一定就是进入了阻塞队列。可能在过程中。 剩下最后一种情况就是当前这个node节点可能进入到阻塞队列了，但是还不是尾结点，这个时候就需要从后往前便利找到他才行，找到了就返回true，找不到说明当前这个节点正在迁移中，返回false。 12345678910111213141516171819202122232425262728//判断节点是否在阻塞队列final boolean isOnSyncQueue(Node node) &#123; /** * 条件1：node.waitStatus==node.condition * 条件成立说明当前node一定是在条件队列，因为signal方法迁移节点到阻塞队列前，会将node的状态设置为0 * 条件2：前置条件：node.waitStatus ！= Node.CONDITION ====&gt; * node.waitStatus ==0 : 表示当前节点已经被signal * node.waitStatus ==1 : 当前线程是未持有锁调用await 最终这个节点将被取消 * node.waitStatus ==0 为什么还要判断 node.prev == null ？ * 因为signal方法是先修改状态在迁移 */ if (node.waitStatus == Node.CONDITION || node.prev == null) return false; /** * 执行到这里， * node.waitStatus !=Condition 且 node.prev!=null ====&gt; 可以排除掉 node.waitStatus ==1 取消状态 * 为什么可以排除掉取消状态？因为signal方法是不会吧取消状态的节点迁移走的 * 设置prev引用的逻辑是迁移阻塞队列 逻辑的设置的 eng方法 * 入队的逻辑：1.设置node.prev =tail * 2.cas当前node为 阻塞队列的tail节点成功才算是真正的进入到阻塞队列 * 可以推算出，就算是prev不是null，也不能说明当前node已经成功入队到阻塞队列了。 */ if (node.next != null) return true; //执行到这里从阻塞队列的尾巴开始向前遍历查找node，找到了就返回true，找不到返回false， //当前node有可能正在迁移中，还未完成。 return findNodeFromTail(node);&#125; 8.signal 首先当前线程持有的必须是独占锁，否则没有必要进入到阻塞队列，所以如果不是独占锁，会抛出异常。 获取到条件队列的头节点，如果头节点不为空，就说明条件队列里面有节点，去唤醒条件队列的第一个节点。 12345678910public final void signal() &#123; //判断当前线程是不是持有的独占锁，如果不是，直接抛出异常 if (!isHeldExclusively()) throw new IllegalMonitorStateException(); //获取条件队列的首节点 Node first = firstWaiter; //如果首节点不为空 if (first != null) doSignal(first);//唤醒条件队列的首节点线程&#125; 9.doSignal 如果头节点的下一个节点为空，说明头节点出队之后，头节点，尾结点都会是空，也就是条件队列没有元素了。 断开当前头节点的next 调用transferForSignal方法去迁移头节点到阻塞队列，如果成功返回true，失败返回false。 循环的条件是：如果当前头节点迁移失败了，那就将头节点设置为当前头节点的下一个节点继续尝试，直到迁移成功或者头节点为空。 12345678910111213141516private void doSignal(Node first) &#123; do &#123; //当前first马上要出队了，所以更新firstWaiter为当前节点的下一个节点 //如果当前节点的下一个节点是null，说明条件队列只有当前一个节点了...当前出队后，整个队列就空了 //所以需要更新lastWaiter==null if((firstWaiter=first.nextWaiter)==null) lastWaiter=null; //当前节点即将出队列，断开和下一个节点的关系 first.nextWaiter = null; /** * transferForSignal(first) 返回true 当前first节点迁移到阻塞队列成功 false 迁移失败 * while循环：(first =firstWaiter)!=null 当前first 迁移失败 ，则将first更新为first.next 继续尝试迁移 * 直至迁移某个节点成功，或者条件队列为 null 为止。 */ &#125; while (!transferForSignal(first)&amp;&amp;(first =firstWaiter)!=null);&#125; 10.transferForSignal接收到signal信号以后，把节点转入等待队列 首先上来先cas将node的状态从-2设置为0，设置成功才继续往下，那么什么时候会设置失败呢？ 当前节点是取消状态，或者当前节点的线程在挂起期间被其他线程使用中断信号唤醒过，这个时候节点会进入到阻塞队列，这个时候节点的状态也会修改成0 使用enq方法添加当前节点到阻塞队列，并返回当前节点的前驱节点 如果前驱节点的状态大于0就说明前驱结点的状态是取消状态， 第二个条件的前置条件就是前驱节点状态小于0，cas设置前驱节点为signal， 如果条件一或者条件二成立一个，那就唤醒当前节点的线程 12345678910111213141516171819202122232425262728293031//接收到signal信号后，把节点转入等待队列final boolean transferForSignal(Node node) &#123; /** * cas修改当前节点的状态，修改为0，因为当前节点马上要迁移到阻塞队列了 * 成功：当前节点在条件队列中状态正常 * 失败： * 1.取消状态 （线程await时，未持有锁，最终线程对应的node将会被设置为取消状态） * 2.node对应的线程 挂起期间，被其他线程使用中断信号唤醒过...（就会主动进入到阻塞队列，这时 * 也会修改状态为0） */ if (!compareAndSetWaitStatus(node, Node.CONDITION, 0)) return false; //添加节点到等待队列，并返回节点的前继节点(prev) Node p = enq(node); //ws:前驱节点的状态 int ws = p.waitStatus; /** * 条件1：ws&gt;0成立：说明前驱节点的状态在阻塞队列中是取消状态，唤醒当前节点 * 条件2：前置条件 ws&lt;=0 * cas 返回true表示设置前驱节点状态为signal状态成功 * cas返回false，什么时候返回false？ * 当前驱node对应的线程是lockInterrupt入队的node时，是会响应中断的，外部线程给前驱线程中断信号之后 * 前驱node会将状态修改为 取消状态，并且执行出队逻辑... * 前驱节点状态只要不是0 或者 -1 ，那么就唤醒当前线程 */ if (ws &gt; 0 || !compareAndSetWaitStatus(p, ws, Node.SIGNAL)) //如果前节点被取消，说明当前为最后一个等待线程，unpark唤醒当前线程 LockSupport.unpark(node.thread); //如果 node 的 prev 节点已经是signal 状态，那么被阻塞的 ThreadA 的唤醒工作由 AQS 队列来完成 return true;&#125; 执行完 doSignal 以后，会把 condition 队列中的节点转移到 aqs 队列上，逻辑结构图如下，这个时候会判断 ThreadA 的 prev 节点也就是 head 节点的 waitStatus，如果大于 0 或者设置 SIGNAL 失败，表示节点被设置成了 CANCELLED 状态。这个时候会唤醒ThreadA 这个线程。否则就基于 AQS 队列的机制来唤醒，也就是等到 ThreadB 释放锁之后来唤醒 ThreadA。 再往下就是被阻塞的线程唤醒之后的逻辑。​ 11.AQS.await​ 前面在分析 await 方法时，线程会被阻塞。而通过 signal被唤醒之后又继续回到上次执行的逻辑中。​ checkInterruptWhileWaiting 这个方法是干嘛呢？其实从名字就可以看出来，就是 ThreadA 在 condition 队列被阻塞的过程中，有没有被其他线程触发过中断请求。​ 123456789101112131415161718192021222324252627public final void await() throws InterruptedException &#123; if (Thread.interrupted())//表示 await 允许被中断 throw new InterruptedException(); Node node = addConditionWaiter();//创建一个新的节点，节点状态为 condition，采用的数据结构仍然是链表 int savedState = fullyRelease(node); //释放当前的锁，得到锁的状态，并唤醒 AQS 队列中的一个线程 int interruptMode = 0; //如果当前节点没有在同步队列上，即还没有被 signal，则将当前线程阻塞 while (!isOnSyncQueue(node)) &#123;//判断这个节点是否在 AQS 队列上，第一次判断的是 false，因为前面已经释放锁了 LockSupport.park(this);//通过 park 挂起当前线程 if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) break; &#125; // 当这个线程醒来,会尝试拿锁, 当 acquireQueued返回 false 就是拿到锁了. // interruptMode != THROW_IE -&gt; 表示这个线程没有成功将 node 入队,但 signal 执行了 enq 方法让其入队了. // 将这个变量设置成 REINTERRUPT if (acquireQueued(node, savedState) &amp;&amp; interruptMode != THROW_IE) interruptMode = REINTERRUPT; // 如果 node 的下一个等待者不是 null, 则进行清理,清理 Condition 队列上的节点. // 如果是 null ,就没有什么好清理的了. if (node.nextWaiter != null) // clean up if cancelled unlinkCancelledWaiters(); // 如果线程被中断了,需要抛出异常.或者什么都不做 if (interruptMode != 0) reportInterruptAfterWait(interruptMode);&#125; 12.checkInterruptWhileWaiting​ 如果当前线程被中断，则调用transferAfterCancelledWait 方法判断后续的处理应该是抛出 InterruptedException 还是重新中断。​ 这里需要注意的地方是，如果第一次 CAS 失败了，则不能判断当前线程是先进行了中断还是先进行了 signal 方法的调用，可能是先执行了 signal 然后中断，也可能是先执行了中断，后执行了 signal，当然，这两个操作肯定是发生在 CAS 之前。这时需要做的就是等待当前线程的 node被添加到 AQS 队列后，也就是 enq 方法返回后，返回false 告诉 checkInterruptWhileWaiting 方法返回REINTERRUPT(1)，后续进行重新中断。​ 简单来说，该方法的返回值代表当前线程是否在 park 的时候被中断唤醒，如果为 true 表示中断在 signal 调用之前，signal 还未执行，那么这个时候会根据 await 的语义，在 await 时遇到中断需要抛出interruptedException，返回 true 就是告诉checkInterruptWhileWaiting 返回 THROW_IE(-1)。如果返回 false，否则表示 signal 已经执行过了，只需要重新响应中断即可。​ 12345private int checkInterruptWhileWaiting(Node node) &#123; return Thread.interrupted() ? (transferAfterCancelledWait(node) ? THROW_IE : REINTERRUPT) : 0;&#125; 12345678910111213final boolean transferAfterCancelledWait(Node node) &#123; //使用 cas 修改节点状态，如果还能修改成功，说明线程被中断时，signal 还没有被调用。// 这里有一个知识点，就是线程被唤醒，并不一定是在 java 层面执行了locksupport.unpark，也可能是调用了线程的 interrupt()方法，这个方法会更新一个中断标识，并且会唤醒处于阻塞状态下的线程。 if (compareAndSetWaitStatus(node, Node.CONDITION, 0)) &#123; enq(node);//如果 cas 成功，则把node 添加到 AQS 队列 return true; &#125;// 如果 cas 失败，则判断当前 node 是否已经在 AQS 队列上，如果不在，则让给其他线程执行// 当 node 被触发了 signal 方法时， node 就会被加到 aqs 队列上 while (!isOnSyncQueue(node))//循环检测 node 是否已经成功添加到 AQS 队列中。如果没有，则通过 yield Thread.yield();//使当前线程由执行状态，变成为就绪状态，让出cpu时间，在下一个线程执行时候，此线程有可能被执行，也有可能没有被执行。 return false;&#125; 12.AQS.acquireQueued​ 这个方法是当前被唤醒的节点ThreadA 去抢占同步锁。并且要恢复到原本的重入次数状态。调用完这个方法之后，AQS 队列的状态如下：​ 将 head 节点的 waitStatus 设置为-1，Signal 状态。​ 12.reportInterruptAfterWait根据 checkInterruptWhileWaiting 方法返回的中断标识来进行中断上报。 如果是 THROW_IE，则抛出中断异常。​ 如果是 REINTERRUPT，则重新响应中断。​ 1234567private void reportInterruptAfterWait(int interruptMode) throws InterruptedException &#123; if (interruptMode == THROW_IE) throw new InterruptedException(); else if (interruptMode == REINTERRUPT) selfInterrupt();&#125; 13.总结与梳理线程 awaitThread 先通过 lock.lock()方法获取锁成功后调用了 condition.await 方法进入等待队列，而另一个线程 signalThread 通过 lock.lock()方法获取锁成功后调用了 condition.signal 或者 signalAll 方法，使得线程awaitThread 能够有机会移入到同步队列中，当其他线程释放 lock 后使得线程 awaitThread 能够有机会获取lock，从而使得线程 awaitThread 能够从 await 方法中退出执行后续操作。如果 awaitThread 获取 lock 失败会直接进入到同步队列。​ ​ 阻塞：await()方法中，在线程释放锁资源之后，如果节点不在 AQS 等待队列，则阻塞当前线程，如果在等待队列，则自旋等待尝试获取锁。​ 释放：signal()后，节点会从 condition 队列移动到 AQS等待队列，则进入正常锁的获取流程。​","categories":[{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"CompletableFuture","slug":"JUC/CompletableFuture","date":"2022-01-11T11:06:39.043Z","updated":"2022-01-11T11:26:51.313Z","comments":true,"path":"2022/01/11/JUC/CompletableFuture/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/JUC/CompletableFuture/","excerpt":"","text":"一，前置知识1，用户线程和守护线程1.所有用户线程执行完毕，程序就会退出，不再等待守护线程。123public static void main(String[] args) &#123; new Thread(()-&gt; System.out.println(&quot;当前线程&quot;+Thread.currentThread().getName()+&quot;是&quot;+(Thread.currentThread().isDaemon()?&quot;守护线程&quot;:&quot;用户线程&quot;))).start();&#125; 2.用户线程不结束，程序就不会退出123456789101112public class DemoA &#123; public static void main(String[] args) &#123; Thread thread = new Thread(DemoA::run,&quot;线程1&quot;); thread.start(); &#125; private static void run() &#123; System.out.println(&quot;当前线程&quot; + Thread.currentThread().getName() + &quot;是&quot; + (Thread.currentThread().isDaemon() ? &quot;守护线程&quot; : &quot;用户线程&quot;)); while (true) ; &#125;&#125; 3.不管守护线程结束没有，只要用户线程结束，程序就会退出1234567891011121314public class DemoA &#123; public static void main(String[] args) &#123; Thread thread = new Thread(DemoA::run,&quot;线程1&quot;); //将线程1设置为守护线程 thread.setDaemon(true); thread.start(); &#125; private static void run() &#123; System.out.println(&quot;当前线程&quot; + Thread.currentThread().getName() + &quot;是&quot; + (Thread.currentThread().isDaemon() ? &quot;守护线程&quot; : &quot;用户线程&quot;)); while (true) ; &#125;&#125; 2.回首FutureTask1.FutureTask存在的问题123456789101112131415161718192021222324252627public class Demob &#123; public static void main(String[] args)throws Exception &#123; FutureTask&lt;String&gt; futureTask = new FutureTask&lt;&gt;(Demob::call); new Thread(futureTask,&quot;t1&quot;).start(); //异步结果集的展现 ---- 会阻塞 //System.out.println(futureTask.get()); //轮询的方式去问 --- 消耗cpu资源 while (true) &#123; if (futureTask.isDone())&#123; System.out.println(futureTask.get()); break; &#125; &#125; System.out.println(&quot;main结束&quot;); &#125; private static String call() &#123; try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return &quot;yhd&quot;; &#125;&#125; 从上面的代码可以看出FutureTask存在的问题： get()异步结果集的展现 —- 会阻塞 futureTask.isDone()轮询的方式去问 — 消耗cpu资源 2.想完成一些复杂的任务 应对Future的完成时间，完成了可以告诉我，也就是我们的回调通知。 将两个异步计算合成一个异步计算，这两个异步计算互相独立，同时第二个又依赖第一个的结果。 当Future集合中某个任务最快结束时，返回结果。 等待Future集合中的所有任务都完成。 二，CompletableFuture1.介绍 异步函数式编程，Future接口的扩展与增强版。 可以通过回调的方式处理计算结果，并且提供了转换和组合CompletableFuture的方法。 完成通知回调 类似Linux管道符的形式，可以异步的计算，上一个计算结果可以给下一个，结合lambda表达式和函数式编程串起来转换和组合获得结果。 2.实例化如果不指定线程池，默认所有的线程都是守护线程。 123456789101112public class DemoC &#123; private static ThreadPoolExecutor pool=new ThreadPoolExecutor(1,5,1, TimeUnit.SECONDS,new ArrayBlockingQueue&lt;&gt;(1)); public static void main(String[] args) throws Exception&#123; //创建一个有返回结果的实例 CompletableFuture&lt;String&gt; supplyAsync = CompletableFuture.supplyAsync(() -&gt; &quot;yhd&quot;, pool); //创建一个没有返回结果的实例 CompletableFuture&lt;Void&gt; runAsync = CompletableFuture.runAsync(System.out::println); pool.shutdown(); &#125;&#125; 3.异步+回调whenComplete 和 join 的区别： 一个是执行完返回结果，一个是主动获取结果。 whenComplete：是执行当前任务的线程执行继续执行 whenComplete 的任务。whenCompleteAsync：是执行把 whenCompleteAsync 这个任务继续提交给线程池来进行执行。方法不以Async结尾，意味着Action使用相同的线程执行，而Async可能会使用其他线程执行（如果是使用相同的线程池，也可能会被同一个线程选中执行） 1234567891011121314public class DemoC &#123; private static ThreadPoolExecutor pool=new ThreadPoolExecutor(1,5,1, TimeUnit.SECONDS,new ArrayBlockingQueue&lt;&gt;(1)); public static void main(String[] args) throws Exception&#123; //创建一个有返回结果的实例 计算完成时回调 发生异常时执行 CompletableFuture.supplyAsync(() -&gt; &quot;yhd&quot;, pool).whenComplete(DemoC::accept).exceptionally(Throwable::toString); pool.shutdown(); &#125; private static void accept(String result, Throwable e) &#123; System.out.println(result); &#125;&#125; 4.结果处理handle 是执行任务完成时对结果的处理。handle 是在任务完成后再执行，还可以处理异常的任务。 12345678910111213141516/** * @author yhd * @createtime 2020/11/15 23:10 */public class DemoB &#123; public static void main(String[] args) &#123; CompletableFuture.supplyAsync(()-&gt;&quot;尹会东&quot;).handle(DemoB::apply).whenCompleteAsync(((s, throwable) -&gt; System.out.println(s))); &#125; private static String apply(String s, Throwable throwable) &#123; if (null != throwable) return throwable.getMessage(); return s + &quot; 牛逼！&quot;; &#125;&#125; 5.线程串行化thenApply 方法：当一个线程依赖另一个线程时，获取上一个任务返回的结果，并返回当前任务的返回值。thenAccept方法：消费处理结果。接收任务的处理结果，并消费处理，无返回结果。thenRun方法：只要上面的任务执行完成，就开始执行thenRun，只是处理完任务后，执行 thenRun的后续操作带有Async默认是异步执行的。这里所谓的异步指的是不在当前线程内执行。 123456789public class DemoE &#123; public static void main(String[] args) &#123; CompletableFuture&lt;Integer&gt; thenApply = CompletableFuture.supplyAsync(() -&gt; 1).thenApply(integer -&gt; 1); CompletableFuture&lt;Void&gt; thenAccept = CompletableFuture.supplyAsync(() -&gt; 1).thenAccept(System.out::println); CompletableFuture&lt;Void&gt; thenRun = CompletableFuture.supplyAsync(() -&gt; 1).thenRun(System.out::println); System.out.println(thenApply.join()); &#125;&#125; 6.任务组合1.两任务组合 - 都要完成两个任务必须都完成，触发该任务。 thenCombine：组合两个future，获取两个future任务的返回结果，并返回当前任务的返回值thenAcceptBoth：组合两个future，获取两个future任务的返回结果，然后处理任务，没有返回值。runAfterBoth：组合两个future，不需要获取future的结果，只需两个future处理完任务后，处理该任务。 12345678910111213141516public class DemoF &#123; public static void main(String[] args) &#123; CompletableFuture .supplyAsync(() -&gt; &quot;hello&quot;) .thenApplyAsync(t -&gt; t + &quot; world!&quot;) .thenCombineAsync( CompletableFuture .completedFuture(&quot; CompletableFuture&quot;), (t, u) -&gt; t + u) .whenComplete(DemoF::accept); &#125; private static void accept(String t, Throwable u) &#123; System.out.println(t); &#125;&#125; 2.两任务组合 - 一个完成当两个任务中，任意一个future任务完成的时候，执行任务。applyToEither：两个任务有一个执行完成，获取它的返回值，处理任务并有新的返回值。acceptEither：两个任务有一个执行完成，获取它的返回值，处理任务，没有新的返回值。runAfterEither：两个任务有一个执行完成，不需要获取future的结果，处理任务，也没有返回值 3.多任务组合allOf：等待所有任务完成anyOf：只要有一个任务完成 7.计算接口性能1.一淘123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566/** * @author yhd * @createtime 2020/11/15 15:11 * 查询多个电商网站同一个商品的价格 * 同步和异步两种方式 */public class DemoD &#123; static List&lt;Gmall&gt; gmalls= Arrays.asList( new Gmall(&quot;京东&quot;), new Gmall(&quot;拼多多&quot;), new Gmall(&quot;淘宝&quot;), new Gmall(&quot;唯品会&quot;), new Gmall(&quot;分期乐&quot;)); /** * 同步 * @param gmalls * @param productName * @return */ public static List&lt;String&gt; getPriceSync(List&lt;Gmall&gt; gmalls,String productName)&#123; return gmalls.stream().map(gmall-&gt;String.format(productName+&quot;in %s price is %d &quot;,gmall.getGmallName(),gmall.getPriceByProductName(productName))).collect(Collectors.toList()); &#125; /** * 异步 * @param gmalls * @param productName * @return */ public static List&lt;String&gt; getPriceAsync(List&lt;Gmall&gt; gmalls,String productName)&#123; return gmalls.stream().map(gmall-&gt;CompletableFuture.supplyAsync(()-&gt;String.format(productName+&quot;in %s price is %d &quot;,gmall.getGmallName(),gmall.getPriceByProductName(productName)))).collect(Collectors.toList()).stream().map(CompletableFuture::join).collect(Collectors.toList()); &#125; public static void main(String[] args) &#123; long startTime = System.currentTimeMillis(); getPriceSync(gmalls,&quot;金鳞岂是池中物&quot;).forEach(System.out::println); long endTime = System.currentTimeMillis(); System.out.println(&quot;同步&quot;+ (endTime-startTime)); System.out.println(&quot;------------------------&quot;); long s = System.currentTimeMillis(); getPriceAsync(gmalls,&quot;金鳞岂是池中物&quot;).forEach(System.out::println); long e = System.currentTimeMillis(); System.out.println(&quot;异步&quot;+ (e-s)); &#125;&#125;@Data@NoArgsConstructor@AllArgsConstructorclass Gmall&#123; private String gmallName; public Integer getPriceByProductName(String productName)&#123; try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return (int)(Math.random()*100)+1; &#125;&#125; 结果： 123456789101112131415=================================金鳞岂是池中物in 京东 price is 100 金鳞岂是池中物in 拼多多 price is 12 金鳞岂是池中物in 淘宝 price is 25 金鳞岂是池中物in 唯品会 price is 35 金鳞岂是池中物in 分期乐 price is 60 同步5081------------------------金鳞岂是池中物in 京东 price is 57 金鳞岂是池中物in 拼多多 price is 68 金鳞岂是池中物in 淘宝 price is 84 金鳞岂是池中物in 唯品会 price is 16 金鳞岂是池中物in 分期乐 price is 20 异步1387================================= 2.检索123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566/** * @author yhd * @createtime 2020/11/15 23:40 */public class DemoC &#123; //模拟数据库 private static Map&lt;String,Company&gt; db=new HashMap&lt;&gt;(); //模拟查询条件 private static List&lt;String&gt; persons; //初始化数据 static &#123; persons=Arrays.asList(&quot;马云&quot;,&quot;马化腾&quot;,&quot;李彦宏&quot;,&quot;张朝阳&quot;,&quot;刘强东&quot;,&quot;王兴&quot;); db.put(&quot;马云&quot;,new Company(&quot;马云&quot;,&quot;阿里巴巴&quot;)); db.put(&quot;马化腾&quot;,new Company(&quot;马化腾&quot;,&quot;腾讯&quot;)); db.put(&quot;李彦宏&quot;,new Company(&quot;李彦宏&quot;,&quot;百度&quot;)); db.put(&quot;张朝阳&quot;,new Company(&quot;张朝阳&quot;,&quot;搜狐&quot;)); db.put(&quot;刘强东&quot;,new Company(&quot;刘强东&quot;,&quot;京东&quot;)); db.put(&quot;王兴&quot;,new Company(&quot;王兴&quot;,&quot;美团&quot;)); &#125; //模拟去数据库查询 public static Company selectDB(String key)&#123; try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return db.get(key); &#125; public static void main(String[] args) &#123; Sync(); System.out.println(); Async(); System.out.println(); System.out.println(result()); &#125; //同步查询 public static void Sync()&#123; Long startTime=System.currentTimeMillis(); persons.stream().map(key-&gt;selectDB(key).getWorks()).collect(Collectors.toList()).forEach(System.out::println); Long endTime=System.currentTimeMillis(); System.out.println(endTime-startTime); &#125; //异步查询 public static void Async()&#123; Long startTime=System.currentTimeMillis(); persons.stream().map(key-&gt;CompletableFuture.supplyAsync(()-&gt;selectDB(key).getWorks())).collect(Collectors.toList()).stream().map(CompletableFuture::join).collect(Collectors.toList()).forEach(System.out::println); Long endTime=System.currentTimeMillis(); System.out.println(endTime-startTime); &#125; //转换成map返回 public static Map&lt;String,String&gt; result()&#123; return persons.stream().map(key -&gt; CompletableFuture.supplyAsync(() -&gt; selectDB(key))).collect(Collectors.toList()).stream().map(CompletableFuture::join).collect(Collectors.toMap(Company::getPerson,Company::getWorks)); &#125;&#125;@Data@NoArgsConstructor@AllArgsConstructorclass Company&#123; private String person; private String works;&#125; 结果 1234567891011121314151617阿里巴巴腾讯百度搜狐京东美团6053阿里巴巴腾讯百度搜狐京东美团1406&#123;张朝阳=搜狐, 马云=阿里巴巴, 王兴=美团, 李彦宏=百度, 刘强东=京东, 马化腾=腾讯&#125; 3.商品详情1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889/** * @author 二十 * @since 2021/8/28 7:54 下午 */public class CompletableFutureTest &#123; private static ThreadPoolExecutor pool = new ThreadPoolExecutor(1, 5, 1, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;&gt;(1)); public static void main(String[] args) &#123; Map&lt;String, Result&gt; result = new HashMap&lt;&gt;(4); Long s = System.currentTimeMillis(); CompletableFuture&lt;Result&gt; skuInfoFuture = CompletableFuture.supplyAsync(() -&gt; &#123; Result skuInfo = getSkuInfo(1); result.put(skuInfo.getName(), skuInfo); return skuInfo; &#125;, pool); CompletableFuture&lt;Void&gt; skuPriceFuture = CompletableFuture.runAsync(() -&gt; &#123; Result skuPrice = getSkuPrice(1); result.put(skuPrice.getName(), skuPrice); &#125;, pool); CompletableFuture&lt;Void&gt; skuImgsFuture = skuInfoFuture.thenAcceptAsync(r -&gt; &#123; Result skuImgs = getSkuImgs(r.getId()); result.put(skuImgs.getName(), skuImgs); &#125;, pool); CompletableFuture&lt;Void&gt; spuInfoFuture = skuInfoFuture.thenAcceptAsync(r -&gt; &#123; Result spuInfo = getSpuInfo(r.getId()); result.put(spuInfo.getName(), spuInfo); &#125;, pool); CompletableFuture.allOf(skuInfoFuture, skuPriceFuture, skuImgsFuture, spuInfoFuture).join(); System.out.println(result); Long e = System.currentTimeMillis(); System.out.println(&quot;查詢總計耗時：&quot; + (e - s)); pool.shutdown(); &#125; /** * 查询sku基本信息 */ public static Result getSkuInfo(Integer key) &#123; sleep(); return new Result(1, &quot;skuInfo&quot;); &#125; /** * 查询sku图片信息 */ public static Result getSkuImgs(Integer key) &#123; sleep(); return new Result(1, &quot;skuImgs&quot;); &#125; /** * 查询spu销售属性 */ public static Result getSpuInfo(Integer key) &#123; sleep(); return new Result(1, &quot;spuInfo&quot;); &#125; /** * 查询sku价格 */ public static Result getSkuPrice(Integer key) &#123; sleep(); return new Result(1, &quot;skuPrice&quot;); &#125; public static void sleep() &#123; try &#123; TimeUnit.SECONDS.sleep(2); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; @Data @NoArgsConstructor @AllArgsConstructor private static class Result &#123; private Integer id; private String name; &#125;&#125; 哪有什么岁月静好，不过是在你看不见的地方负重前行。","categories":[{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"FutureTask","slug":"JUC/FutureTask","date":"2022-01-11T11:06:27.504Z","updated":"2022-01-11T11:28:34.940Z","comments":true,"path":"2022/01/11/JUC/FutureTask/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/JUC/FutureTask/","excerpt":"","text":"一，Future和FutureTask的关系1.Future12345678910111213/** * @author 二十 * @since 2021/8/28 3:50 下午 */public class FutureTaskTest &#123; public static void main(String[] args) &#123; ThreadPoolExecutor executor = new ThreadPoolExecutor(5, 5, 5, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;&gt;(1)); Future&lt;String&gt; future = executor.submit(() -&gt; &quot;callable&quot;); Future&lt;?&gt; future1 = executor.submit(() -&gt; System.out.println(&quot;runnable&quot;)); FutureTask&lt;String&gt; futureTask = new FutureTask&lt;&gt;(() -&gt; &quot;hahah&quot;); &#125;&#125; 当我们调用线程池的submit()方法的时候，会给我们返回一个Future类型的对象。那么这个Future又是什么？ 12345678910111213141516public interface Future&lt;V&gt; &#123; //取消任务 boolean cancel(boolean mayInterruptIfRunning); //是否取消了任务 boolean isCancelled(); //是否执行完了任务 boolean isDone(); //获取线程的执行结果 V get() throws InterruptedException, ExecutionException; //获取结果超时 V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException;&#125; 这个接口主要的功能就是可以获取到线程执行完成后的结果。​ 2.两者的关系 FutureTask可以看做是Future 和 Runnable 两个接口的实现类。​ 二，FutureTask源码1.属性12345678910111213141516171819202122232425//表示当前任务的状态private volatile int state;//任务尚未执行private static final int NEW = 0;//任务还未结束private static final int COMPLETING = 1;//任务正常执行结束private static final int NORMAL = 2;//任务发生了异常，由callable.call()向上抛出private static final int EXCEPTIONAL = 3;//任务被取消private static final int CANCELLED = 4;//任务正在中断private static final int INTERRUPTING = 5;//任务已经被中断private static final int INTERRUPTED = 6;//submit(runnable/callable) 使用装饰者模式将runnable装饰成callableprivate Callable&lt;V&gt; callable;//正常情况下：用来保存call的执行结果，异常情况下：用来保存call抛出的异常private Object outcome; // non-volatile, protected by state reads/writes//执行当前任务的线程private volatile Thread runner;//因为会有很多线程去get当前任务的结果，所以 这里使用了一种数据结构 stack 头插 头取 的一个队列。private volatile WaitNode waiters; 2.WaitNode内部类这个类里面封装着执行任务的线程和指向下一个线程的指针。​ 12345static final class WaitNode &#123; volatile Thread thread; volatile WaitNode next; WaitNode() &#123; thread = Thread.currentThread(); &#125;&#125; 3.构造器当我们传入一个runnable接口的时候： 1234public FutureTask(Runnable runnable, V result) &#123; this.callable = Executors.callable(runnable, result); this.state = NEW; // ensure visibility of callable&#125; 调用了Executors.callable(runnable, result); 12345public static &lt;T&gt; Callable&lt;T&gt; callable(Runnable task, T result) &#123; if (task == null) throw new NullPointerException(); return new RunnableAdapter&lt;T&gt;(task, result);&#125; 当任务不为空，实际上返回的是一个RunnableAdapter对象。 123456789101112static final class RunnableAdapter&lt;T&gt; implements Callable&lt;T&gt; &#123; final Runnable task; final T result; RunnableAdapter(Runnable task, T result) &#123; this.task = task; this.result = result; &#125; public T call() &#123; task.run(); return result; &#125;&#125; 在这里，将runnable装饰成了callable。​ 4.run()1234567891011121314151617181920212223242526272829303132public void run() &#123; if(任务不是刚创建||任务没有获取到执行权)&#123; return &#125; try&#123; if(如果任务不为空&amp;&amp;任务没有被其他线程执行)&#123; 声明 result 声明 ran = true try&#123; 调用call方法执行任务，并将结果赋值给result ran = true &#125;catch(Exception e)&#123; result=null ran =false 调用setException(ex) &#125; if(ran)&#123; 执行set(result) &#125; &#125; &#125;catch(Exception e)&#123; &#125;finally&#123; 释放任务执行者 if(任务状态为被打断中或者已经被打断)&#123; 执行 handlePossibleCancellationInterrupt(s) &#125; &#125;&#125; 5.set()12345678protected void set(V v)&#123; if(cas的方式设置当前任务的状态为完成中成功)&#123; 将任务结果赋值给outcome 设置当前任务状态为正常结束 调用finishCompletion() &#125;&#125; 6.setException()1234567protected void setException(Throwable t) &#123; if (cas的方式设置当前任务的状态为完成中成功) &#123; 将异常赋值给outcome 设置当前任务状态为异常 finishCompletion(); &#125;&#125; 7.handlePossibleCancellationInterrupt()12345678private void handlePossibleCancellationInterrupt(int s) &#123; if (如果当前任务状态为被打断) while (当前任务状态为被打断) Thread.yield(); // 释放当前线程cpu的执行权&#125; 8.finishCompletion()123456789101112131415161718192021222324private void finishCompletion() &#123; for循环让q指向当前链表的头节点 //这里的操作是为了可能任务还没开始执行就被其他线程取消了，这个时候将等待结果的线程全部唤醒，小概率事件 if(使用cas设置waiters为null成功)&#123; for(;;)&#123; 获取q包装的线程 if(当前节点包装的线程不为空)&#123; 不让q在继续包装这个线程 唤醒q之前包装的线程 &#125; q指向链表的下一个节点 if(下一个节点为空)&#123; break &#125; &#125; break &#125; done() 将callable设置为null帮助gc&#125; 9.done()1protected void done() &#123; &#125; 10.get()123456public V get() throws InterruptedException, ExecutionException &#123; 获取当前任务执行状态 if (当前任务尚未执行完) s = awaitDone(false, 0L); return report(s);&#125; 11.awaitDone()123456789101112131415161718192021222324252627282930//参数说明：是否设置了超时时间 超时时间private int awaitDone(boolean timed, long nanos) throws InterruptedException &#123; final long deadline = timed ? System.nanoTime() + nanos : 0L; 声明 当前线程是否进入获取结果的等待队列 =false for (;;) &#123; if (如果当前线程被其他线程用中断的方式唤醒) &#123; 当前节点出队列 并抛出中断异常 &#125; //假设当前线程是被其他线程以unpark的方式正常唤醒，那么就会走下面的逻辑 获取当前任务状态 //大于完成中有可能正常结束，也有可能异常结束 if (如果当前任务状态大于完成中) &#123; if (如果当前线程创建过node) 释放node return 当前任务状态; &#125; else if (当前任务状态是完成中) 释放线程的cpu执行权，接着等 else if (等待节点为空，说明是第一次自旋，还没有创建节点) 创建一个新的节点 else if (创建好了对象但是还没有入队) cas的方式加入队列 else if (有超时时间) &#123; 走超时逻辑 &#125; else 阻塞当前线程 &#125;&#125; 12.report(int s)123456789//参数说明：当前任务状态private V report(int s) throws ExecutionException &#123; if (任务正常结束) return 任务结果; if (任务被取消或中断) 抛异常 否则就是任务发生异常，将异常向上抛出&#125; 13.cancel()1234567891011121314151617181920public boolean cancel(boolean mayInterruptIfRunning) &#123; //说明此时任务不能取消了，直接返回false if (!(任务状态==刚创建 &amp;&amp; cas修改任务状态为取消或中断成功))) return false; try &#123; if (尝试打断成功) &#123; try &#123; 获取执行任务的线程 if (执行任务的线程！=null) 给执行任务的线程设置中断标识 &#125; finally &#123; cas的方式设置任务的状态为被打断 &#125; &#125; &#125; finally &#123; finishCompletion(); &#125; return true;&#125; 三，流程梳理​ 首先明确一点，不管我们传入的是Runnable还是Callable接口，他最终用的都是Callable，Runnable接口会被他通过装饰者模式封装为Callable。 ​ 一个任务执行的入口其实就是run方法。 ​ 进入run方法，首先他会判断当前任务是否已经被执行过或者当前线程通过cas的方式并没有抢到执行当前任务的机会。如果是的话，说明已经有线程正在执行或者执行完了当前的任务，直接返回即可。 接下来他会判断当前任务是否为空或者当前任务的状态，其实判断状态就是为了判断当前任务是不是被取消了。如果任务不为空并且没有被取消，他会执行call方法（实际上就是我们自己的业务代码）并将结果设置到result将任务的是否被执行状态改成true表示顺利执行完。 ​ call方法执行过程中如果发生异常了，会将结果设置为null，是否被顺利执行的状态为设置为false，表示执行过程发生了异常。 ​ 接下来，它会将异常信息封装到outcome，然后设置但该你任务的状态为异常结束，并自旋的方式唤醒所有等待队列中等待获取结果的线程。 ​ 如果call正常执行完了，他会用cas的方式设置当前任务的完成状态为完成中，如果设置成功，outcome来接收任务的结果，设置当前任务的状态为正常完成状态，并且唤醒所有等待结果的线程。 ​ 最终它会将执行当前任务的线程设置为null，并判断，如果当前任务的状态是被打断中或者已经被打断，他就会自旋判断如果当前线程的状态为被打断，让执行当前任务的线程释放cpu。 ​ get方法是获取当前任务的结果。首先他会获取当前任务的状态，如果状态小于等于未完成首先他会通过自旋的方式，第一次自旋尚未给当前线程创建waitNode对象，此时就需要位当前线程创建waitNode对象。第二次自旋，创建好了对象还没有入队，cas的方式入队。第三次自旋，判断是否设置超时时间，如果没设置超时时间，当前get操作的线程就会被park了。 线程状态会变为 WAITING状态，相当于休眠了..除非有其它线程将你唤醒 或者 将当前线程 中断。他会获取当前线程的任务状态，如果任务还没有完成，释放cpu接着等。如果任务已经完成，此时需要将node设置位null help GC，直接返回当前的状态。除此之外还要判断，如果当前线程被其他线程用中断的方式唤醒，这种唤醒方式会将Thread的中断标记位设置为false，当前线程出队，get方法抛出中断异常。 ​ 如果状态表示已经有结果，会执行report方法。 ​ report方法，如果任务正常执行结束，返回结果，如果任务被取消或者中断了，抛出异常，如果任务执行过程中发生异常结束了，返回异常。 ​ 最后就是任务的取消方法 cancel。他会先判断state == NEW 成立 表示当前任务处于运行中 或者 处于线程池 任务队列中..并且cas修改状态成功，他就会尝试取打断。 ​ 如果尝试打断成功，给runner线程一个中断信号..如果你的程序是响应中断 会走中断逻辑..假设你程序不是响应中断的..啥也不会发生。最后，设置线程状态为中断，唤醒获取结果的线程。","categories":[{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"ConcurrentHashMap","slug":"JUC/ConcurrentHashMap","date":"2022-01-11T11:06:18.507Z","updated":"2022-01-11T11:27:21.431Z","comments":true,"path":"2022/01/11/JUC/ConcurrentHashMap/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/JUC/ConcurrentHashMap/","excerpt":"","text":"一，使用ConcurrentHashMap 是 J.U.C 包里面提供的一个线程安全并且高效的 HashMap，所以ConcurrentHashMap 在并发编程的场景中使用的频率比较高。 ConcurrentHashMap 是 Map 的派生类，所以 api 基本和 Hashmap 是类似，主要就是 put、get 这些方法，接下来基于 ConcurrentHashMap 的 put 和 get 这两个方法作为切入点来分析 ConcurrentHashMap 的源码实现。 二，源码 1.成员变量123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147//散列表数组的最大限制private static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;//散列表默认值private static final int DEFAULT_CAPACITY = 16;static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8;//并发级别：jdk7历史遗留问题，仅仅在初始化的时候使用到，并不是真正的代表并发级别private static final int DEFAULT_CONCURRENCY_LEVEL = 16;//负载因子，JDK1.8中 ConcurrentHashMap 是固定值private static final float LOAD_FACTOR = 0.75f;//树化阈值，指定桶位 链表长度达到8的话，有可能发生树化操作。static final int TREEIFY_THRESHOLD = 8;//红黑树转化为链表的阈值static final int UNTREEIFY_THRESHOLD = 6;//联合TREEIFY_THRESHOLD控制桶位是否树化，只有当table数组长度达到64且 某个桶位 中的链表长度达到8，才会真正树化static final int MIN_TREEIFY_CAPACITY = 64;//线程迁移数据最小步长，控制线程迁移任务最小区间一个值private static final int MIN_TRANSFER_STRIDE = 16;//计算扩容时候生成的一个 标识戳private static int RESIZE_STAMP_BITS = 16;//结果是65535 表示并发扩容最多线程数private static final int MAX_RESIZERS = (1 &lt;&lt; (32 - RESIZE_STAMP_BITS)) - 1;//扩容相关private static final int RESIZE_STAMP_SHIFT = 32 - RESIZE_STAMP_BITS;//当node节点hash=-1 表示当前节点已经被迁移了 ，fwd节点static final int MOVED = -1; // hash for forwarding nodes//node hash=-2 表示当前节点已经树化 且 当前节点为treebin对象 ，代理操作红黑树static final int TREEBIN = -2; // hash for roots of treesstatic final int RESERVED = -3; // hash for transient reservations//转化成二进制实际上是 31个 1 可以将一个负数通过位移运算得到一个正数static final int HASH_BITS = 0x7fffffff; // usable bits of normal node hash//当前系统的cpu数量static final int NCPU = Runtime.getRuntime().availableProcessors();//为了兼容7版本的chp保存的，核心代码并没有使用到private static final ObjectStreamField[] serialPersistentFields = &#123; new ObjectStreamField(&quot;segments&quot;, Segment[].class), new ObjectStreamField(&quot;segmentMask&quot;, Integer.TYPE), new ObjectStreamField(&quot;segmentShift&quot;, Integer.TYPE)&#125;;//散列表，长度一定是2次方数transient volatile Node&lt;K,V&gt;[] table;//扩容过程中，会将扩容中的新table 赋值给nextTable 保持引用，扩容结束之后，这里会被设置为Nullprivate transient volatile Node&lt;K,V&gt;[] nextTable;//LongAdder 中的 baseCount 未发生竞争时 或者 当前LongAdder处于加锁状态时，增量累到到baseCount中private transient volatile long baseCount;/** * sizeCtl &lt; 0 * 1. -1 表示当前table正在初始化（有线程在创建table数组），当前线程需要自旋等待.. * 2.表示当前table数组正在进行扩容 ,高16位表示：扩容的标识戳 低16位表示：（1 + nThread） 当前参与并发扩容的线程数量 * * sizeCtl = 0，表示创建table数组时 使用DEFAULT_CAPACITY为大小 * * sizeCtl &gt; 0 * * 1. 如果table未初始化，表示初始化大小 * 2. 如果table已经初始化，表示下次扩容时的 触发条件（阈值） */private transient volatile int sizeCtl;/** * * 扩容过程中，记录当前进度。所有线程都需要从transferIndex中分配区间任务，去执行自己的任务。 */private transient volatile int transferIndex;/** * LongAdder中的cellsBuzy 0表示当前LongAdder对象无锁状态，1表示当前LongAdder对象加锁状态 */private transient volatile int cellsBusy;/** * LongAdder中的cells数组，当baseCount发生竞争后，会创建cells数组， * 线程会通过计算hash值 取到 自己的cell ，将增量累加到指定cell中 * 总数 = sum(cells) + baseCount */private transient volatile CounterCell[] counterCells;// Unsafe mechanicsprivate static final sun.misc.Unsafe U;/**表示sizeCtl属性在ConcurrentHashMap中内存偏移地址*/private static final long SIZECTL;/**表示transferIndex属性在ConcurrentHashMap中内存偏移地址*/private static final long TRANSFERINDEX;/**表示baseCount属性在ConcurrentHashMap中内存偏移地址*/private static final long BASECOUNT;/**表示cellsBusy属性在ConcurrentHashMap中内存偏移地址*/private static final long CELLSBUSY;/**表示cellValue属性在CounterCell中内存偏移地址*/private static final long CELLVALUE;/**表示数组第一个元素的偏移地址*/private static final long ABASE;private static final int ASHIFT;static &#123; try &#123; U = sun.misc.Unsafe.getUnsafe(); Class&lt;?&gt; k = ConcurrentHashMap.class; SIZECTL = U.objectFieldOffset (k.getDeclaredField(&quot;sizeCtl&quot;)); TRANSFERINDEX = U.objectFieldOffset (k.getDeclaredField(&quot;transferIndex&quot;)); BASECOUNT = U.objectFieldOffset (k.getDeclaredField(&quot;baseCount&quot;)); CELLSBUSY = U.objectFieldOffset (k.getDeclaredField(&quot;cellsBusy&quot;)); Class&lt;?&gt; ck = CounterCell.class; CELLVALUE = U.objectFieldOffset (ck.getDeclaredField(&quot;value&quot;)); Class&lt;?&gt; ak = Node[].class; ABASE = U.arrayBaseOffset(ak); //表示数组单元所占用空间大小,scale 表示Node[]数组中每一个单元所占用空间大小 int scale = U.arrayIndexScale(ak); //1 0000 &amp; 0 1111 = 0 if ((scale &amp; (scale - 1)) != 0) throw new Error(&quot;data type scale not a power of two&quot;); //numberOfLeadingZeros() 这个方法是返回当前数值转换为二进制后，从高位到低位开始统计，看有多少个0连续在一块。 //8 =&gt; 1000 numberOfLeadingZeros(8) = 28 //4 =&gt; 100 numberOfLeadingZeros(4) = 29 //ASHIFT = 31 - 29 = 2 ？？ //ABASE + （5 &lt;&lt; ASHIFT） ASHIFT = 31 - Integer.numberOfLeadingZeros(scale); &#125; catch (Exception e) &#123; throw new Error(e); &#125;&#125; 2.基础方法2.1 spread高位运算 123static final int spread(int h) &#123; return (h ^ (h &gt;&gt;&gt; 16)) &amp; HASH_BITS;&#125; 2.2 tabAt该方法获取对象中offset偏移地址对应的对象field的值。实际上这段代码的含义等价于tab[i],但是为什么不直接使用 tab[i]来计算呢？ getObjectVolatile，一旦看到 volatile 关键字，就表示可见性。因为对 volatile 写操作 happen-before 于 volatile 读操作，因此其他线程对 table 的修改均对 get 读取可见； 虽然 table 数组本身是增加了 volatile 属性，但是“volatile 的数组只针对数组的引用具有volatile 的语义，而不是它的元素”。 所以如果有其他线程对这个数组的元素进行写操作，那么当前线程来读的时候不一定能读到最新的值。出于性能考虑，Doug Lea 直接通过 Unsafe 类来对 table 进行操作。 123static final &lt;K,V&gt; Node&lt;K,V&gt; tabAt(Node&lt;K,V&gt;[] tab, int i) &#123; return (Node&lt;K,V&gt;)U.getObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE);&#125; 2.3 casTabAtcas设置当前节点为桶位的头节点 1234static final &lt;K,V&gt; boolean casTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; c, Node&lt;K,V&gt; v) &#123; return U.compareAndSwapObject(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, c, v);&#125; 2.4 setTabAt123static final &lt;K,V&gt; void setTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; v) &#123; U.putObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, v);&#125; 2.5 resizeStampresizeStamp 用来生成一个和扩容有关的扩容戳，具体有什么作用呢？ 123static final int resizeStamp(int n) &#123; return Integer.numberOfLeadingZeros(n) | (1 &lt;&lt; (RESIZE_STAMP_BITS - 1));&#125; Integer.numberOfLeadingZeros 这个方法是返回无符号整数 n 最高位非 0 位前面的 0 的个数。 比如 10 的二进制是 0000 0000 0000 0000 0000 0000 0000 1010，那么这个方法返回的值就是 28。 根据 resizeStamp 的运算逻辑，我们来推演一下，假如 n=16，那么 resizeStamp(16)=32796转化为二进制是[0000 0000 0000 0000 1000 0000 0001 1100]​ 接着再来看,当第一个线程尝试进行扩容的时候，会执行下面这段代码： 1U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2) rs 左移 16 位，相当于原本的二进制低位变成了高位 1000 0000 0001 1100 0000 0000 00000000 然后再+2 =1000 0000 0001 1100 0000 0000 0000 0000+10=1000 0000 0001 1100 0000 00000000 0010 高 16 位代表扩容的标记、低 16 位代表并行扩容的线程数 这样来存储有什么好处呢？ 1，首先在 CHM 中是支持并发扩容的，也就是说如果当前的数组需要进行扩容操作，可以由多个线程来共同负责 2，可以保证每次扩容都生成唯一的生成戳，每次新的扩容，都有一个不同的 n，这个生成戳就是根据 n 来计算出来的一个数字，n 不同，这个数字也不同 第一个线程尝试扩容的时候，为什么是+2 因为 1 表示初始化，2 表示一个线程在执行扩容，而且对 sizeCtl 的操作都是基于位运算的，所以不会关心它本身的数值是多少，只关心它在二进制上的数值，而 sc + 1 会在低 16 位上加 1。 2.6 tableSizeFor经过多次位移返回大于等于c的最小的二次方数 1234567891011121314151617181920/** * Returns a power of two table size for the given desired capacity. * See Hackers Delight, sec 3.2 * 返回&gt;=c的最小的2的次方数 * c=28 * n=27 =&gt; 0b 11011 * 11011 | 01101 =&gt; 11111 * 11111 | 00111 =&gt; 11111 * .... * =&gt; 11111 + 1 =100000 = 32 */private static final int tableSizeFor(int c) &#123; int n = c - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125; 3. 构造方法1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public ConcurrentHashMap() &#123;&#125;public ConcurrentHashMap(int initialCapacity) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException(); //如果指定的容量超过允许的最大值，设置为最大值 int cap = ((initialCapacity &gt;= (MAXIMUM_CAPACITY &gt;&gt;&gt; 1)) ? MAXIMUM_CAPACITY : tableSizeFor(initialCapacity + (initialCapacity &gt;&gt;&gt; 1) + 1)); /** * sizeCtl &gt; 0 * 当目前table未初始化时，sizeCtl表示初始化容量 */ this.sizeCtl = cap;&#125;public ConcurrentHashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; this.sizeCtl = DEFAULT_CAPACITY; putAll(m);&#125;public ConcurrentHashMap(int initialCapacity, float loadFactor) &#123; this(initialCapacity, loadFactor, 1);&#125;public ConcurrentHashMap(int initialCapacity, float loadFactor, int concurrencyLevel) &#123; //参数校验 if (!(loadFactor &gt; 0.0f) || initialCapacity &lt; 0 || concurrencyLevel &lt;= 0) throw new IllegalArgumentException(); //如果初始容量小于并发级别，那就设置初始容量为并发级别 if (initialCapacity &lt; concurrencyLevel) initialCapacity = concurrencyLevel; //16/0.75 +1 = 22 long size = (long)(1.0 + (long)initialCapacity / loadFactor); // 22 - &gt; 32 int cap = (size &gt;= (long)MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : tableSizeFor((int)size); /** * sizeCtl &gt; 0 * 当目前table未初始化时，sizeCtl表示初始化容量 */ this.sizeCtl = cap;&#125; 4.put1234public V put(K key, V value) &#123; //如果key已经存在，是否覆盖，默认是false return putVal(key, value, false);&#125; 5 putVal123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129final V putVal(K key, V value, boolean onlyIfAbsent) &#123; //控制k 和 v 不能为null if (key == null || value == null) throw new NullPointerException(); //通过spread方法，可以让高位也能参与进寻址运算。 int hash = spread(key.hashCode()); //binCount表示当前k-v 封装成node后插入到指定桶位后，在桶位中的所属链表的下标位置 //0 表示当前桶位为null，node可以直接放着 //2 表示当前桶位已经可能是红黑树 int binCount = 0; //tab 引用map对象的table //自旋 for (Node&lt;K,V&gt;[] tab = table;;) &#123; //f 表示桶位的头结点 //n 表示散列表数组的长度 //i 表示key通过寻址计算后，得到的桶位下标 //fh 表示桶位头结点的hash值 Node&lt;K,V&gt; f; int n, i, fh; //CASE1：成立，表示当前map中的table尚未初始化.. if (tab == null || (n = tab.length) == 0) //最终当前线程都会获取到最新的map.table引用。 tab = initTable(); //CASE2：i 表示key使用路由寻址算法得到 key对应 table数组的下标位置，tabAt 获取指定桶位的头结点 f else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123; //进入到CASE2代码块 前置条件 当前table数组i桶位是Null时。 //使用CAS方式 设置 指定数组i桶位 为 new Node&lt;K,V&gt;(hash, key, value, null),并且期望值是null //cas操作成功 表示ok，直接break for循环即可 //cas操作失败，表示在当前线程之前，有其它线程先你一步向指定i桶位设置值了。 //当前线程只能再次自旋，去走其它逻辑。 if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin &#125; //CASE3：前置条件，桶位的头结点一定不是null。 //条件成立表示当前桶位的头结点 为 FWD结点，表示目前map正处于扩容过程中.. else if ((fh = f.hash) == MOVED) //看到fwd节点后，当前节点有义务帮助当前map对象完成迁移数据的工作 //帮助扩容 tab = helpTransfer(tab, f); //CASE4：当前桶位 可能是 链表 也可能是 红黑树代理结点TreeBin else &#123; //当插入key存在时，会将旧值赋值给oldVal，返回给put方法调用处.. V oldVal = null; //使用sync 加锁“头节点”，理论上是“头结点” synchronized (f) &#123; //为什么又要对比一下，看看当前桶位的头节点 是否为 之前获取的头结点？ //为了避免其它线程将该桶位的头结点修改掉，导致当前线程从sync 加锁 就有问题了。之后所有操作都不用在做了。 if (tabAt(tab, i) == f) &#123;//条件成立，说明咱们 加锁 的对象没有问题，可以进来造了！ //条件成立，说明当前桶位就是普通链表桶位。 if (fh &gt;= 0) &#123; //1.当前插入key与链表当中所有元素的key都不一致时，当前的插入操作是追加到链表的末尾，binCount表示链表长度 //2.当前插入key与链表当中的某个元素的key一致时，当前插入操作可能就是替换了。binCount表示冲突位置（binCount - 1） binCount = 1; //迭代循环当前桶位的链表，e是每次循环处理节点。 for (Node&lt;K,V&gt; e = f;; ++binCount) &#123; //当前循环节点 key K ek; //条件一：e.hash == hash 成立 表示循环的当前元素的hash值与插入节点的hash值一致，需要进一步判断 //条件二：((ek = e.key) == key ||(ek != null &amp;&amp; key.equals(ek))) // 成立：说明循环的当前节点与插入节点的key一致，发生冲突了 if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; //将当前循环的元素的 值 赋值给oldVal oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; &#125; //当前元素 与 插入元素的key不一致 时，会走下面程序。 //1.更新循环处理节点为 当前节点的下一个节点 //2.判断下一个节点是否为null，如果是null，说明当前节点已经是队尾了，插入数据需要追加到队尾节点的后面。 Node&lt;K,V&gt; pred = e; if ((e = e.next) == null) &#123; pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; &#125; &#125; &#125; //前置条件，该桶位一定不是链表 //条件成立，表示当前桶位是 红黑树代理结点TreeBin else if (f instanceof TreeBin) &#123; //p 表示红黑树中如果与你插入节点的key 有冲突节点的话 ，则putTreeVal 方法 会返回冲突节点的引用。 Node&lt;K,V&gt; p; //强制设置binCount为2，因为binCount &lt;= 1 时有其它含义，所以这里设置为了2 binCount = 2; //条件一：成立，说明当前插入节点的key与红黑树中的某个节点的key一致，冲突了 if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) &#123; //将冲突节点的值 赋值给 oldVal oldVal = p.val; if (!onlyIfAbsent) p.val = value; &#125; &#125; &#125; &#125; //说明当前桶位不为null，可能是红黑树 也可能是链表 if (binCount != 0) &#123; //如果binCount&gt;=8 表示处理的桶位一定是链表 if (binCount &gt;= TREEIFY_THRESHOLD) //调用转化链表为红黑树的方法 treeifyBin(tab, i); //说明当前线程插入的数据key，与原有k-v发生冲突，需要将原数据v返回给调用者。 if (oldVal != null) return oldVal; break; &#125; &#125; &#125; //1.统计当前table一共有多少数据 //2.判断是否达到扩容阈值标准，触发扩容。 addCount(1L, binCount); return null;&#125; 6 initTable数组初始化方法，这个方法比较简单，就是初始化一个合适大小的数组。 sizeCtl ：这个标志是在 Node 数组初始化或者扩容的时候的一个控制位标识，负数代表正在进行初始化或者扩容操作。 -1 代表正在初始化 -N 代表有 N-1 个线程正在进行扩容操作，这里不是简单的理解成 n 个线程，sizeCtl 就是-N 0 标识 Node 数组还没有被初始化，正数代表初始化或者下一次扩容的大小 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/** * Initializes table, using the size recorded in sizeCtl. * * sizeCtl &lt; 0 * * 1. -1 表示当前table正在初始化（有线程在创建table数组），当前线程需要自旋等待.. * * 2.表示当前table数组正在进行扩容 ,高16位表示：扩容的标识戳 低16位表示：（1 + nThread） 当前参与并发扩容的线程数量 * * * * sizeCtl = 0，表示创建table数组时 使用DEFAULT_CAPACITY为大小 * * * * sizeCtl &gt; 0 * * * * 1. 如果table未初始化，表示初始化大小 * * 2. 如果table已经初始化，表示下次扩容时的 触发条件（阈值） */private final Node&lt;K,V&gt;[] initTable() &#123; //tab 引用map.table //sc sizeCtl的临时值 Node&lt;K,V&gt;[] tab; int sc; //自旋 条件：map.table 尚未初始化 while ((tab = table) == null || tab.length == 0) &#123; if ((sc = sizeCtl) &lt; 0) //大概率就是-1，表示其它线程正在进行创建table的过程，当前线程没有竞争到初始化table的锁。 Thread.yield(); // lost initialization race; just spin //1.sizeCtl = 0，表示创建table数组时 使用DEFAULT_CAPACITY为大小 //2.如果table未初始化，表示初始化大小 //3.如果table已经初始化，表示下次扩容时的 触发条件（阈值） else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) &#123; try &#123; //这里为什么又要判断呢？ 防止其它线程已经初始化完毕了，然后当前线程再次初始化..导致丢失数据。 //条件成立，说明其它线程都没有进入过这个if块，当前线程就是具备初始化table权利了。 if ((tab = table) == null || tab.length == 0) &#123; //sc大于0 创建table时 使用 sc为指定大小，否则使用 16 默认值. int n = (sc &gt; 0) ? sc : DEFAULT_CAPACITY; @SuppressWarnings(&quot;unchecked&quot;) Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; //最终赋值给 map.table table = tab = nt; //n &gt;&gt;&gt; 2 =&gt; 等于 1/4 n n - (1/4)n = 3/4 n =&gt; 0.75 * n //sc 0.75 n 表示下一次扩容时的触发条件。 sc = n - (n &gt;&gt;&gt; 2); &#125; &#125; finally &#123; //1.如果当前线程是第一次创建map.table的线程话，sc表示的是 下一次扩容的阈值 //2.表示当前线程 并不是第一次创建map.table的线程，当前线程进入到else if 块 时，将 //sizeCtl 设置为了-1 ，那么这时需要将其修改为 进入时的值。 sizeCtl = sc; &#125; break; &#125; &#125; return tab;&#125; 7 addCount123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124private final void addCount(long x, int check) &#123; //as 表示 LongAdder.cells //b 表示LongAdder.base //s 表示当前map.table中元素的数量 CounterCell[] as; long b, s; //条件一：true-&gt;表示cells已经初始化了，当前线程应该去使用hash寻址找到合适的cell 去累加数据 // false-&gt;表示当前线程应该将数据累加到 base //条件二：false-&gt;表示写base成功，数据累加到base中了，当前竞争不激烈，不需要创建cells // true-&gt;表示写base失败，与其他线程在base上发生了竞争，当前线程应该去尝试创建cells。 if ((as = counterCells) != null || !U.compareAndSwapLong(this, BASECOUNT, b = baseCount, s = b + x)) &#123; //有几种情况进入到if块中？ //1.true-&gt;表示cells已经初始化了，当前线程应该去使用hash寻址找到合适的cell 去累加数据 //2.true-&gt;表示写base失败，与其他线程在base上发生了竞争，当前线程应该去尝试创建cells。 //a 表示当前线程hash寻址命中的cell CounterCell a; //v 表示当前线程写cell时的期望值 long v; //m 表示当前cells数组的长度 int m; //true -&gt; 未竞争 false-&gt;发生竞争 boolean uncontended = true; //条件一：as == null || (m = as.length - 1) &lt; 0 //true-&gt; 表示当前线程是通过 写base竞争失败 然后进入的if块，就需要调用fullAddCount方法去扩容 或者 重试.. LongAdder.longAccumulate //条件二：a = as[ThreadLocalRandom.getProbe() &amp; m]) == null 前置条件：cells已经初始化了 //true-&gt;表示当前线程命中的cell表格是个空，需要当前线程进入fullAddCount方法去初始化 cell，放入当前位置. //条件三：!(uncontended = U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x) // false-&gt;取反得到false，表示当前线程使用cas方式更新当前命中的cell成功 // true-&gt;取反得到true,表示当前线程使用cas方式更新当前命中的cell失败，需要进入fullAddCount进行重试 或者 扩容 cells。 if (as == null || (m = as.length - 1) &lt; 0 || (a = as[ThreadLocalRandom.getProbe() &amp; m]) == null || !(uncontended = U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x)) ) &#123; fullAddCount(x, uncontended); //考虑到fullAddCount里面的事情比较累，就让当前线程 不参与到 扩容相关的逻辑了，直接返回到调用点。 return; &#125; if (check &lt;= 1) return; //获取当前散列表元素个数，这是一个期望值 s = sumCount(); &#125; //表示一定是一个put操作调用的addCount if (check &gt;= 0) &#123; //tab 表示map.table //nt 表示map.nextTable //n 表示map.table数组的长度 //sc 表示sizeCtl的临时值 Node&lt;K,V&gt;[] tab, nt; int n, sc; /** * sizeCtl &lt; 0 * 1. -1 表示当前table正在初始化（有线程在创建table数组），当前线程需要自旋等待.. * 2.表示当前table数组正在进行扩容 ,高16位表示：扩容的标识戳 低16位表示：（1 + nThread） 当前参与并发扩容的线程数量 * * sizeCtl = 0，表示创建table数组时 使用DEFAULT_CAPACITY为大小 * * sizeCtl &gt; 0 * * 1. 如果table未初始化，表示初始化大小 * 2. 如果table已经初始化，表示下次扩容时的 触发条件（阈值） */ //自旋 //条件一：s &gt;= (long)(sc = sizeCtl) // true-&gt; 1.当前sizeCtl为一个负数 表示正在扩容中.. // 2.当前sizeCtl是一个正数，表示扩容阈值 // false-&gt; 表示当前table尚未达到扩容条件 //条件二：(tab = table) != null // 恒成立 true //条件三：(n = tab.length) &lt; MAXIMUM_CAPACITY // true-&gt;当前table长度小于最大值限制，则可以进行扩容。 while (s &gt;= (long)(sc = sizeCtl) &amp;&amp; (tab = table) != null &amp;&amp; (n = tab.length) &lt; MAXIMUM_CAPACITY) &#123; //扩容批次唯一标识戳 //16 -&gt; 32 扩容 标识为：1000 0000 0001 1011 int rs = resizeStamp(n); //条件成立：表示当前table正在扩容 // 当前线程理论上应该协助table完成扩容 if (sc &lt; 0) &#123; //条件一：(sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs // true-&gt;说明当前线程获取到的扩容唯一标识戳 非 本批次扩容 // false-&gt;说明当前线程获取到的扩容唯一标识戳 是 本批次扩容 //条件二： JDK1.8 中有bug jira已经提出来了 其实想表达的是 = sc == (rs &lt;&lt; 16 ) + 1 // true-&gt; 表示扩容完毕，当前线程不需要再参与进来了 // false-&gt;扩容还在进行中，当前线程可以参与 //条件三：JDK1.8 中有bug jira已经提出来了 其实想表达的是 = sc == (rs&lt;&lt;16) + MAX_RESIZERS // true-&gt; 表示当前参与并发扩容的线程达到了最大值 65535 - 1 // false-&gt;表示当前线程可以参与进来 //条件四：(nt = nextTable) == null // true-&gt;表示本次扩容结束 // false-&gt;扩容正在进行中 if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex &lt;= 0) break; //前置条件：当前table正在执行扩容中.. 当前线程有机会参与进扩容。 //条件成立：说明当前线程成功参与到扩容任务中，并且将sc低16位值加1，表示多了一个线程参与工作 //条件失败：1.当前有很多线程都在此处尝试修改sizeCtl，有其它一个线程修改成功了，导致你的sc期望值与内存中的值不一致 修改失败 // 2.transfer 任务内部的线程也修改了sizeCtl。 if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) //协助扩容线程，持有nextTable参数 transfer(tab, nt); &#125; //1000 0000 0001 1011 0000 0000 0000 0000 +2 =&gt; 1000 0000 0001 1011 0000 0000 0000 0010 //条件成立，说明当前线程是触发扩容的第一个线程，在transfer方法需要做一些扩容准备工作 else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) //触发扩容条件的线程 不持有nextTable transfer(tab, null); s = sumCount(); &#125; &#125;&#125; 8. transferConcurrentHashMap 支持并发扩容，实现方式是，把 Node 数组进行拆分，让每个线程处理自己的区域，假设 table 数组总长度是 64，默认情况下，那么每个线程可以分到 16 个 bucket。然后每个线程处理的范围，按照倒序来做迁移。 通过 for 自循环处理每个槽位中的链表元素，默认 advace 为真，通过 CAS 设置 transferIndex属性值，并初始化 i 和 bound 值，i 指当前处理的槽位序号，bound 指需要处理的槽位边界，先处理槽位 31 的节点； （bound,i） =(16,31) 从 31 的位置往前推动。​ 每存在一个线程执行完扩容操作，就通过 cas 执行 sc-1。 接着判断(sc-2) !=resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT ; 如果相等，表示当前为整个扩容操作的 最后一个线程，那么意味着整个扩容操作就结束了；如果不相等，说明还得继续。 这么做的目的，一方面是防止不同扩容之间出现相同的 sizeCtl，另外一方面，还可以避免sizeCtl 的 ABA 问题导致的扩容重叠的情况。​ 扩容图解lastRun机制判断是否需要扩容，也就是当更新后的键值对总数 baseCount &gt;= 阈值 sizeCtl 时，进行rehash，这里面会有两个逻辑。 如果当前正在处于扩容阶段，则当前线程会加入并且协助扩容。 如果当前没有在扩容，则直接触发扩容操作。 ​ 扩容操作的核心在于数据的转移，在单线程环境下数据的转移很简单，无非就是把旧数组中的数据迁移到新的数组。但是这在多线程环境下，在扩容的时候其他线程也可能正在添加元素，这时又触发了扩容怎么办？可能大家想到的第一个解决方案是加互斥锁，把转移过程锁住，虽然是可行的解决方案，但是会带来较大的性能开销。因为互斥锁会导致所有访问临界区的线程陷入到阻塞状态，持有锁的线程耗时越长，其他竞争线程就会一直被阻塞，导致吞吐量较低。而且还可能导致死锁。 而 ConcurrentHashMap 并没有直接加锁，而是采用 CAS 实现无锁的并发同步策略，最精华的部分是它可以利用多线程来进行协同扩容。 它把 Node 数组当作多个线程之间共享的任务队列，然后通过维护一个指针来划分每个线程锁负责的区间，每个线程通过区间逆向遍历来实现扩容，一个已经迁移完的bucket会被替换为一个ForwardingNode节点，标记当前bucket已经被其他线程迁移完了。接下来分析一下它的源码实现。 fwd:这个类是个标识类，用于指向新表用的，其他线程遇到这个类会主动跳过这个类，因为这个类要么就是扩容迁移正在进行，要么就是已经完成扩容迁移，也就是这个类要保证线程安全，再进行操作。 advance:这个变量是用于提示代码是否进行推进处理，也就是当前桶处理完，处理下一个桶的标识。 finishing:这个变量用于提示扩容是否结束用的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237private final void transfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab) &#123; //n 表示扩容之前table数组的长度 //stride 表示分配给线程任务的步长 int n = tab.length, stride; // stride 固定为 16 if ((stride = (NCPU &gt; 1) ? (n &gt;&gt;&gt; 3) / NCPU : n) &lt; MIN_TRANSFER_STRIDE) stride = MIN_TRANSFER_STRIDE; // subdivide range //条件成立：表示当前线程为触发本次扩容的线程，需要做一些扩容准备工作 //条件不成立：表示当前线程是协助扩容的线程.. if (nextTab == null) &#123; // initiating try &#123; //创建了一个比扩容之前大一倍的table @SuppressWarnings(&quot;unchecked&quot;) Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n &lt;&lt; 1]; nextTab = nt; &#125; catch (Throwable ex) &#123; // try to cope with OOME sizeCtl = Integer.MAX_VALUE; return; &#125; //赋值给对象属性 nextTable ，方便协助扩容线程 拿到新表 nextTable = nextTab; //记录迁移数据整体位置的一个标记。index计数是从1开始计算的。 transferIndex = n; &#125; //表示新数组的长度 int nextn = nextTab.length; //fwd 节点，当某个桶位数据处理完毕后，将此桶位设置为fwd节点，其它写线程 或读线程看到后，会有不同逻辑。 ForwardingNode&lt;K,V&gt; fwd = new ForwardingNode&lt;K,V&gt;(nextTab); //推进标记 boolean advance = true; //完成标记 boolean finishing = false; // to ensure sweep before committing nextTab //i 表示分配给当前线程任务，执行到的桶位 //bound 表示分配给当前线程任务的下界限制 int i = 0, bound = 0; //自旋 for (;;) &#123; //f 桶位的头结点 //fh 头结点的hash Node&lt;K,V&gt; f; int fh; /** * 1.给当前线程分配任务区间 * 2.维护当前线程任务进度（i 表示当前处理的桶位） * 3.维护map对象全局范围内的进度 */ while (advance) &#123; //分配任务的开始下标 //分配任务的结束下标 int nextIndex, nextBound; //CASE1: //条件一：--i &gt;= bound //成立：表示当前线程的任务尚未完成，还有相应的区间的桶位要处理，--i 就让当前线程处理下一个 桶位. //不成立：表示当前线程任务已完成 或 者未分配 if (--i &gt;= bound || finishing) advance = false; //CASE2: //前置条件：当前线程任务已完成 或 者未分配 //条件成立：表示对象全局范围内的桶位都分配完毕了，没有区间可分配了，设置当前线程的i变量为-1 跳出循环后，执行退出迁移任务相关的程序 //条件不成立：表示对象全局范围内的桶位尚未分配完毕，还有区间可分配 else if ((nextIndex = transferIndex) &lt;= 0) &#123; i = -1; advance = false; &#125; //CASE3: //前置条件：1、当前线程需要分配任务区间 2.全局范围内还有桶位尚未迁移 //条件成立：说明给当前线程分配任务成功 //条件失败：说明分配给当前线程失败，应该是和其它线程发生了竞争吧 else if (U.compareAndSwapInt (this, TRANSFERINDEX, nextIndex, nextBound = (nextIndex &gt; stride ? nextIndex - stride : 0))) &#123; bound = nextBound; i = nextIndex - 1; advance = false; &#125; &#125; //CASE1： //条件一：i &lt; 0 //成立：表示当前线程未分配到任务 if (i &lt; 0 || i &gt;= n || i + n &gt;= nextn) &#123; //保存sizeCtl 的变量 int sc; if (finishing) &#123; nextTable = null; table = nextTab; sizeCtl = (n &lt;&lt; 1) - (n &gt;&gt;&gt; 1); return; &#125; //条件成立：说明设置sizeCtl 低16位 -1 成功，当前线程可以正常退出 if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) &#123; //1000 0000 0001 1011 0000 0000 0000 0000 //条件成立：说明当前线程不是最后一个退出transfer任务的线程 if ((sc - 2) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) //正常退出 return; finishing = advance = true; i = n; // recheck before commit &#125; &#125; //前置条件：【CASE2~CASE4】 当前线程任务尚未处理完，正在进行中 //CASE2: //条件成立：说明当前桶位未存放数据，只需要将此处设置为fwd节点即可。 else if ((f = tabAt(tab, i)) == null) advance = casTabAt(tab, i, null, fwd); //CASE3: //条件成立：说明当前桶位已经迁移过了，当前线程不用再处理了，直接再次更新当前线程任务索引，再次处理下一个桶位 或者 其它操作 else if ((fh = f.hash) == MOVED) advance = true; // already processed //CASE4: //前置条件：当前桶位有数据，而且node节点 不是 fwd节点，说明这些数据需要迁移。 else &#123; //sync 加锁当前桶位的头结点 synchronized (f) &#123; //防止在你加锁头对象之前，当前桶位的头对象被其它写线程修改过，导致你目前加锁对象错误... if (tabAt(tab, i) == f) &#123; //ln 表示低位链表引用 //hn 表示高位链表引用 Node&lt;K,V&gt; ln, hn; //条件成立：表示当前桶位是链表桶位 if (fh &gt;= 0) &#123; //lastRun //可以获取出 当前链表 末尾连续高位不变的 node int runBit = fh &amp; n; Node&lt;K,V&gt; lastRun = f; for (Node&lt;K,V&gt; p = f.next; p != null; p = p.next) &#123; int b = p.hash &amp; n; if (b != runBit) &#123; runBit = b; lastRun = p; &#125; &#125; //条件成立：说明lastRun引用的链表为 低位链表，那么就让 ln 指向 低位链表 if (runBit == 0) &#123; ln = lastRun; hn = null; &#125; //否则，说明lastRun引用的链表为 高位链表，就让 hn 指向 高位链表 else &#123; hn = lastRun; ln = null; &#125; for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) &#123; int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); else hn = new Node&lt;K,V&gt;(ph, pk, pv, hn); &#125; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; &#125; //条件成立：表示当前桶位是 红黑树 代理结点TreeBin else if (f instanceof TreeBin) &#123; //转换头结点为 treeBin引用 t TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; //低位双向链表 lo 指向低位链表的头 loTail 指向低位链表的尾巴 TreeNode&lt;K,V&gt; lo = null, loTail = null; //高位双向链表 lo 指向高位链表的头 loTail 指向高位链表的尾巴 TreeNode&lt;K,V&gt; hi = null, hiTail = null; //lc 表示低位链表元素数量 //hc 表示高位链表元素数量 int lc = 0, hc = 0; //迭代TreeBin中的双向链表，从头结点 至 尾节点 for (Node&lt;K,V&gt; e = t.first; e != null; e = e.next) &#123; // h 表示循环处理当前元素的 hash int h = e.hash; //使用当前节点 构建出来的 新的 TreeNode TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt; (h, e.key, e.val, null, null); //条件成立：表示当前循环节点 属于低位链 节点 if ((h &amp; n) == 0) &#123; //条件成立：说明当前低位链表 还没有数据 if ((p.prev = loTail) == null) lo = p; //说明 低位链表已经有数据了，此时当前元素 追加到 低位链表的末尾就行了 else loTail.next = p; //将低位链表尾指针指向 p 节点 loTail = p; ++lc; &#125; //当前节点 属于 高位链 节点 else &#123; if ((p.prev = hiTail) == null) hi = p; else hiTail.next = p; hiTail = p; ++hc; &#125; &#125; ln = (lc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(lo) : (hc != 0) ? new TreeBin&lt;K,V&gt;(lo) : t; hn = (hc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(hi) : (lc != 0) ? new TreeBin&lt;K,V&gt;(hi) : t; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; &#125; &#125; &#125; &#125; &#125;&#125; 链表迁移原理 1）高低位原理分析ConcurrentHashMap 在做链表迁移时，会用高低位来实现，这里有两个问题要分析一下 1，如何实现高低位链表的区分 假如有这样一个队列 第 14 个槽位插入新节点之后，链表元素个数已经达到了 8，且数组长度为 16，优先通过扩容来缓解链表过长的问题 假如当前线程正在处理槽位为 14 的节点，它是一个链表结构，在代码中，首先定义两个变量节点 ln 和 hn，实际就是 lowNode 和 HighNode，分别保存 hash 值的第 x 位为 0 和不等于0 的节点 通过 fn&amp;n 可以把这个链表中的元素分为两类，A 类是 hash 值的第 X 位为 0，B 类是 hash 值的第 x 位为不等于 0（至于为什么要这么区分，稍后分析），并且通过 lastRun 记录最后要处理的节点。最终要达到的目的是，A 类的链表保持位置不动，B 类的链表为 14+16(扩容增加的长度)=30 把 14 槽位的链表单独伶出来，用蓝色表示 fn&amp;n=0 的节点，假如链表的分类是这样 1234567for (Node&lt;K,V&gt; p = f.next; p != null; p = p.next) &#123; int b = p.hash &amp; n; if (b != runBit) &#123; runBit = b; lastRun = p; &#125;&#125; 通过上面这段代码遍历，会记录 runBit 以及 lastRun，按照上面这个结构，那么 runBit 应该是蓝色节点，lastRun 应该是第 6 个节点接着，再通过这段代码进行遍历，生成 ln 链以及 hn 链 1234567for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) &#123; int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); else hn = new Node&lt;K,V&gt;(ph, pk, pv, hn);&#125; 接着，通过 CAS 操作，把 hn 链放在 i+n 也就是 14+16 的位置，ln 链保持原来的位置不动。并且设置当前节点为 fwd，表示已经被当前线程迁移完了。 123setTabAt(nextTab, i, ln);setTabAt(nextTab, i + n, hn);setTabAt(tab, i, fwd); 迁移完成以后的数据分布如下 2）为什么要做高低位的划分要想了解这么设计的目的，我们需要从 ConcurrentHashMap 的根据下标获取对象的算法来看，在 putVal 方法中 1018 行： 1(f = tabAt(tab, i = (n - 1) &amp; hash)) == null 通过(n-1) &amp; hash 来获得在 table 中的数组下标来获取节点数据，【&amp;运算是二进制运算符，1&amp; 1=1，其他都为 0】。 9.helpTransfer如果对应的节点存在，判断这个节点的 hash 是不是等于 MOVED(-1)，说明当前节点是ForwardingNode 节点，意味着有其他线程正在进行扩容，那么当前现在直接帮助它进行扩容，因此调用 helpTransfer方法。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455final Node&lt;K,V&gt;[] helpTransfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt; f) &#123; //nextTab 引用的是 fwd.nextTable == map.nextTable 理论上是这样。 //sc 保存map.sizeCtl Node&lt;K,V&gt;[] nextTab; int sc; //条件一：tab != null 恒成立 true //条件二：(f instanceof ForwardingNode) 恒成立 true //条件三：((ForwardingNode&lt;K,V&gt;)f).nextTable) != null 恒成立 true if (tab != null &amp;&amp; (f instanceof ForwardingNode) &amp;&amp; (nextTab = ((ForwardingNode&lt;K,V&gt;)f).nextTable) != null) &#123; //拿当前标的长度 获取 扩容标识戳 假设 16 -&gt; 32 扩容：1000 0000 0001 1011 int rs = resizeStamp(tab.length); //条件一：nextTab == nextTable //成立：表示当前扩容正在进行中 //不成立：1.nextTable被设置为Null 了，扩容完毕后，会被设为Null // 2.再次出发扩容了...咱们拿到的nextTab 也已经过期了... //条件二：table == tab //成立：说明 扩容正在进行中，还未完成 //不成立：说明扩容已经结束了，扩容结束之后，最后退出的线程 会设置 nextTable 为 table //条件三：(sc = sizeCtl) &lt; 0 //成立：说明扩容正在进行中 //不成立：说明sizeCtl当前是一个大于0的数，此时代表下次扩容的阈值，当前扩容已经结束。 while (nextTab == nextTable &amp;&amp; table == tab &amp;&amp; (sc = sizeCtl) &lt; 0) &#123; //条件一：(sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs // true-&gt;说明当前线程获取到的扩容唯一标识戳 非 本批次扩容 // false-&gt;说明当前线程获取到的扩容唯一标识戳 是 本批次扩容 //条件二： JDK1.8 中有bug jira已经提出来了 其实想表达的是 = sc == (rs &lt;&lt; 16 ) + 1 // true-&gt; 表示扩容完毕，当前线程不需要再参与进来了 // false-&gt;扩容还在进行中，当前线程可以参与 //条件三：JDK1.8 中有bug jira已经提出来了 其实想表达的是 = sc == (rs&lt;&lt;16) + MAX_RESIZERS // true-&gt; 表示当前参与并发扩容的线程达到了最大值 65535 - 1 // false-&gt;表示当前线程可以参与进来 //条件四：transferIndex &lt;= 0 // true-&gt;说明map对象全局范围内的任务已经分配完了，当前线程进去也没活干.. // false-&gt;还有任务可以分配。 if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || transferIndex &lt;= 0) break; if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) &#123; transfer(tab, nextTab); break; &#125; &#125; return nextTab; &#125; return table;&#125; 10.get1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public V get(Object key) &#123; //tab 引用map.table //e 当前元素 //p 目标节点 //n table数组长度 //eh 当前元素hash //ek 当前元素key Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; e, p; int n, eh; K ek; //扰动运算后得到 更散列的hash值 int h = spread(key.hashCode()); //条件一：(tab = table) != null //true-&gt;表示已经put过数据，并且map内部的table也已经初始化完毕 //false-&gt;表示创建完map后，并没有put过数据，map内部的table是延迟初始化的，只有第一次写数据时会触发创建逻辑。 //条件二：(n = tab.length) &gt; 0 true-&gt;表示table已经初始化 //条件三：(e = tabAt(tab, (n - 1) &amp; h)) != null //true-&gt;当前key寻址的桶位 有值 //false-&gt;当前key寻址的桶位中是null，是null直接返回null if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (e = tabAt(tab, (n - 1) &amp; h)) != null) &#123; //前置条件：当前桶位有数据 //对比头结点hash与查询key的hash是否一致 //条件成立：说明头结点与查询Key的hash值 完全一致 if ((eh = e.hash) == h) &#123; //完全比对 查询key 和 头结点的key //条件成立：说明头结点就是查询数据 if ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) return e.val; &#125; //条件成立： //1.-1 fwd 说明当前table正在扩容，且当前查询的这个桶位的数据 已经被迁移走了 //2.-2 TreeBin节点，需要使用TreeBin 提供的find 方法查询。 else if (eh &lt; 0) return (p = e.find(h, key)) != null ? p.val : null; //当前桶位已经形成链表的这种情况 while ((e = e.next) != null) &#123; if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) return e.val; &#125; &#125; return null;&#125; 11.remove123public V remove(Object key) &#123; return replaceNode(key, null, null);&#125; 12.replaceNode123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143final V replaceNode(Object key, V value, Object cv) &#123; //计算key经过扰动运算后的hash int hash = spread(key.hashCode()); //自旋 for (Node&lt;K,V&gt;[] tab = table;;) &#123; //f表示桶位头结点 //n表示当前table数组长度 //i表示hash命中桶位下标 //fh表示桶位头结点 hash Node&lt;K,V&gt; f; int n, i, fh; //CASE1： //条件一：tab == null true-&gt;表示当前map.table尚未初始化.. false-&gt;已经初始化 //条件二：(n = tab.length) == 0 true-&gt;表示当前map.table尚未初始化.. false-&gt;已经初始化 //条件三：(f = tabAt(tab, i = (n - 1) &amp; hash)) == null true -&gt; 表示命中桶位中为null，直接break， 会返回 if (tab == null || (n = tab.length) == 0 || (f = tabAt(tab, i = (n - 1) &amp; hash)) == null) break; //CASE2： //前置条件CASE2 ~ CASE3：当前桶位不是null //条件成立：说明当前table正在扩容中，当前是个写操作，所以当前线程需要协助table完成扩容。 else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); //CASE3: //前置条件CASE2 ~ CASE3：当前桶位不是null //当前桶位 可能是 &quot;链表&quot; 也可能 是 &quot;红黑树&quot; TreeBin else &#123; //保留替换之前的数据引用 V oldVal = null; //校验标记 boolean validated = false; //加锁当前桶位 头结点，加锁成功之后会进入 代码块。 synchronized (f) &#123; //判断sync加锁是否为当前桶位 头节点，防止其它线程，在当前线程加锁成功之前，修改过 桶位 的头结点。 //条件成立：当前桶位头结点 仍然为f，其它线程没修改过。 if (tabAt(tab, i) == f) &#123; //条件成立：说明桶位 为 链表 或者 单个 node if (fh &gt;= 0) &#123; validated = true; //e 表示当前循环处理元素 //pred 表示当前循环节点的上一个节点 Node&lt;K,V&gt; e = f, pred = null; for (;;) &#123; //当前节点key K ek; //条件一：e.hash == hash true-&gt;说明当前节点的hash与查找节点hash一致 //条件二：((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) //if 条件成立，说明key 与查询的key完全一致。 if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; //当前节点的value V ev = e.val; //条件一：cv == null true-&gt;替换的值为null 那么就是一个删除操作 //条件二：cv == ev || (ev != null &amp;&amp; cv.equals(ev)) 那么是一个替换操作 if (cv == null || cv == ev || (ev != null &amp;&amp; cv.equals(ev))) &#123; //删除 或者 替换 //将当前节点的值 赋值给 oldVal 后续返回会用到 oldVal = ev; //条件成立：说明当前是一个替换操作 if (value != null) //直接替换 e.val = value; //条件成立：说明当前节点非头结点 else if (pred != null) //当前节点的上一个节点，指向当前节点的下一个节点。 pred.next = e.next; else //说明当前节点即为 头结点，只需要将 桶位设置为头结点的下一个节点。 setTabAt(tab, i, e.next); &#125; break; &#125; pred = e; if ((e = e.next) == null) break; &#125; &#125; //条件成立：TreeBin节点。 else if (f instanceof TreeBin) &#123; validated = true; //转换为实际类型 TreeBin t TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; //r 表示 红黑树 根节点 //p 表示 红黑树中查找到对应key 一致的node TreeNode&lt;K,V&gt; r, p; //条件一：(r = t.root) != null 理论上是成立 //条件二：TreeNode.findTreeNode 以当前节点为入口，向下查找key（包括本身节点） // true-&gt;说明查找到相应key 对应的node节点。会赋值给p if ((r = t.root) != null &amp;&amp; (p = r.findTreeNode(hash, key, null)) != null) &#123; //保存p.val 到pv V pv = p.val; //条件一：cv == null 成立：不必对value，就做替换或者删除操作 //条件二：cv == pv ||(pv != null &amp;&amp; cv.equals(pv)) 成立：说明“对比值”与当前p节点的值 一致 if (cv == null || cv == pv || (pv != null &amp;&amp; cv.equals(pv))) &#123; //替换或者删除操作 oldVal = pv; //条件成立：替换操作 if (value != null) p.val = value; //删除操作 else if (t.removeTreeNode(p)) //这里没做判断，直接搞了...很疑惑 setTabAt(tab, i, untreeify(t.first)); &#125; &#125; &#125; &#125; &#125; //当其他线程修改过桶位 头结点时，当前线程 sync 头结点 锁错对象时，validated 为false，会进入下次for 自旋 if (validated) &#123; if (oldVal != null) &#123; //替换的值 为null，说明当前是一次删除操作，oldVal ！=null 成立，说明删除成功，更新当前元素个数计数器。 if (value == null) addCount(-1L, -1); return oldVal; &#125; break; &#125; &#125; &#125; return null;&#125; 13.TreeBin13.1 属性1234567891011121314151617//红黑树 根节点 TreeNode&lt;K,V&gt; root;//链表的头节点volatile TreeNode&lt;K,V&gt; first;//等待者线程（当前lockState是读锁状态）volatile Thread waiter;/** * 1.写锁状态 写是独占状态，以散列表来看，真正进入到TreeBin中的写线程 同一时刻 只有一个线程。 1 * 2.读锁状态 读锁是共享，同一时刻可以有多个线程 同时进入到 TreeBin对象中获取数据。 每一个线程 都会给 lockStat + 4 * 3.等待者状态（写线程在等待），当TreeBin中有读线程目前正在读取数据时，写线程无法修改数据，那么就将lockState的最低2位 设置为 0b 10 */volatile int lockState;// values for lockStatestatic final int WRITER = 1; // set while holding write lockstatic final int WAITER = 2; // set when waiting for write lockstatic final int READER = 4; // increment value for setting read lock 13.2 构造器1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586TreeBin(TreeNode&lt;K,V&gt; b) &#123; //设置节点hash为-2 表示此节点是TreeBin节点 super(TREEBIN, null, null, null); //使用first 引用 treeNode链表 this.first = b; //r 红黑树的根节点引用 TreeNode&lt;K,V&gt; r = null; //x表示遍历的当前节点 for (TreeNode&lt;K,V&gt; x = b, next; x != null; x = next) &#123; next = (TreeNode&lt;K,V&gt;)x.next; //强制设置当前插入节点的左右子树为null x.left = x.right = null; //条件成立：说明当前红黑树 是一个空树，那么设置插入元素 为根节点 if (r == null) &#123; //根节点的父节点 一定为 null x.parent = null; //颜色改为黑色 x.red = false; //让r引用x所指向的对象。 r = x; &#125; else &#123; //非第一次循环，都会来带else分支，此时红黑树已经有数据了 //k 表示 插入节点的key K k = x.key; //h 表示 插入节点的hash int h = x.hash; //kc 表示 插入节点key的class类型 Class&lt;?&gt; kc = null; //p 表示 为查找插入节点的父节点的一个临时节点 TreeNode&lt;K,V&gt; p = r; for (;;) &#123; //dir (-1, 1) //-1 表示插入节点的hash值大于 当前p节点的hash //1 表示插入节点的hash值 小于 当前p节点的hash //ph p表示 为查找插入节点的父节点的一个临时节点的hash int dir, ph; //临时节点 key K pk = p.key; //插入节点的hash值 小于 当前节点 if ((ph = p.hash) &gt; h) //插入节点可能需要插入到当前节点的左子节点 或者 继续在左子树上查找 dir = -1; //插入节点的hash值 大于 当前节点 else if (ph &lt; h) //插入节点可能需要插入到当前节点的右子节点 或者 继续在右子树上查找 dir = 1; //如果执行到 CASE3，说明当前插入节点的hash 与 当前节点的hash一致，会在case3 做出最终排序。最终 //拿到的dir 一定不是0，（-1， 1） else if ((kc == null &amp;&amp; (kc = comparableClassFor(k)) == null) || (dir = compareComparables(kc, k, pk)) == 0) dir = tieBreakOrder(k, pk); //xp 想要表示的是 插入节点的 父节点 TreeNode&lt;K,V&gt; xp = p; //条件成立：说明当前p节点 即为插入节点的父节点 //条件不成立：说明p节点 底下还有层次，需要将p指向 p的左子节点 或者 右子节点，表示继续向下搜索。 if ((p = (dir &lt;= 0) ? p.left : p.right) == null) &#123; //设置插入节点的父节点 为 当前节点 x.parent = xp; //小于P节点，需要插入到P节点的左子节点 if (dir &lt;= 0) xp.left = x; //大于P节点，需要插入到P节点的右子节点 else xp.right = x; //插入节点后，红黑树性质 可能会被破坏，所以需要调用 平衡方法 r = balanceInsertion(r, x); break; &#125; &#125; &#125; &#125; //将r 赋值给 TreeBin对象的 root引用。 this.root = r; assert checkInvariants(root);&#125; 13.3 putTreeVal12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970final TreeNode&lt;K,V&gt; putTreeVal(int h, K k, V v) &#123; Class&lt;?&gt; kc = null; boolean searched = false; for (TreeNode&lt;K,V&gt; p = root;;) &#123; int dir, ph; K pk; if (p == null) &#123; first = root = new TreeNode&lt;K,V&gt;(h, k, v, null, null); break; &#125; else if ((ph = p.hash) &gt; h) dir = -1; else if (ph &lt; h) dir = 1; else if ((pk = p.key) == k || (pk != null &amp;&amp; k.equals(pk))) return p; else if ((kc == null &amp;&amp; (kc = comparableClassFor(k)) == null) || (dir = compareComparables(kc, k, pk)) == 0) &#123; if (!searched) &#123; TreeNode&lt;K,V&gt; q, ch; searched = true; if (((ch = p.left) != null &amp;&amp; (q = ch.findTreeNode(h, k, kc)) != null) || ((ch = p.right) != null &amp;&amp; (q = ch.findTreeNode(h, k, kc)) != null)) return q; &#125; dir = tieBreakOrder(k, pk); &#125; TreeNode&lt;K,V&gt; xp = p; if ((p = (dir &lt;= 0) ? p.left : p.right) == null) &#123; //当前循环节点xp 即为 x 节点的爸爸 //x 表示插入节点 //f 老的头结点 TreeNode&lt;K,V&gt; x, f = first; first = x = new TreeNode&lt;K,V&gt;(h, k, v, f, xp); //条件成立：说明链表有数据 if (f != null) //设置老的头结点的前置引用为 当前的头结点。 f.prev = x; if (dir &lt;= 0) xp.left = x; else xp.right = x; if (!xp.red) x.red = true; else &#123; //表示 当前新插入节点后，新插入节点 与 父节点 形成 “红红相连” lockRoot(); try &#123; //平衡红黑树，使其再次符合规范。 root = balanceInsertion(root, x); &#125; finally &#123; unlockRoot(); &#125; &#125; break; &#125; &#125; assert checkInvariants(root); return null;&#125; 13.4 find123456789101112131415161718192021222324252627282930313233343536373839404142434445464748final Node&lt;K,V&gt; find(int h, Object k) &#123; if (k != null) &#123; //e 表示循环迭代的当前节点 迭代的是first引用的链表 for (Node&lt;K,V&gt; e = first; e != null; ) &#123; //s 保存的是lock临时状态 //ek 链表当前节点 的key int s; K ek; //(WAITER|WRITER) =&gt; 0010 | 0001 =&gt; 0011 //lockState &amp; 0011 != 0 条件成立：说明当前TreeBin 有等待者线程 或者 目前有写操作线程正在加锁 if (((s = lockState) &amp; (WAITER|WRITER)) != 0) &#123; if (e.hash == h &amp;&amp; ((ek = e.key) == k || (ek != null &amp;&amp; k.equals(ek)))) return e; e = e.next; &#125; //前置条件：当前TreeBin中 等待者线程 或者 写线程 都没有 //条件成立：说明添加读锁成功 else if (U.compareAndSwapInt(this, LOCKSTATE, s, s + READER)) &#123; TreeNode&lt;K,V&gt; r, p; try &#123; //查询操作 p = ((r = root) == null ? null : r.findTreeNode(h, k, null)); &#125; finally &#123; //w 表示等待者线程 Thread w; //U.getAndAddInt(this, LOCKSTATE, -READER) == (READER|WAITER) //1.当前线程查询红黑树结束，释放当前线程的读锁 就是让 lockstate 值 - 4 //(READER|WAITER) = 0110 =&gt; 表示当前只有一个线程在读，且“有一个线程在等待” //当前读线程为 TreeBin中的最后一个读线程。 //2.(w = waiter) != null 说明有一个写线程在等待读操作全部结束。 if (U.getAndAddInt(this, LOCKSTATE, -READER) == (READER|WAITER) &amp;&amp; (w = waiter) != null) //使用unpark 让 写线程 恢复运行状态。 LockSupport.unpark(w); &#125; return p; &#125; &#125; &#125; return null;&#125; 三，总结在java8中，ConcurrentHashMap使用数组+链表+红黑树的组合方式，利用cas和synchronized保证并发写的安全。​ 引入红黑树的原因：链表查询的时间复杂度为On，但是红黑树的查询时间复杂度为O(log(n)),所以在节点比较多的情况下，使用红黑树可以大大提升性能。 ​ 链式桶是一个由node节点组成的链表。树状桶是一颗由TreeNode节点组成的红黑树。输的根节点为TreeBin类型。​ 当链表长度大于8整个hash表长度大于64的时候，就会转化为TreeBin。TreeBin作为根节点，其实就是红黑树对象。在ConcurrentHashMap的table数组中，存放的就是TreeBin对象，而不是TreeNoe对象。​ 数组table是懒加载的，只有第一次添加元素的时候才会初始化，所以initTable()存在线程安全问题。​ 重要的属性就是sizeCtl，用来控制table的初始化和扩容操作的过程：​ -1代表table正在初始化，其他线程直接join等待。 -N代表有N-1个线程正在进行扩容操作，严格来说，当其为负数的时候，只用到了低16位，如果低16位为M，此时有M-1个线程进行扩容。 大于0有两种情况：如果table没有初始化，她就表示table初始化的大小，如果table初始化完了，就表示table的容量，默认是table大小的四分之三。 ​ Transfer()扩容​ table数据转移到nextTable。扩容操作的核心在于数据的转移，把旧数组中的数据迁移到新的数组。ConcurrentHashMap精华的部分是它可以利用多线程来进行协同扩容，简单来说，它把table数组当作多个线程之间共享的任务队列，然后通过维护一个指针来划分每个线程所负责的区间，每个线程通过区间逆向遍历来实现扩容，一个已经迁移完的 Bucket会被替换为一个Forwarding节点，标记当前Bucket已经被其他线程迁移完了。​ helpTransfer()帮助扩容​ ConcurrentHashMap并发添加元素时，如果正在扩容，其他线程会帮助扩容，也就是多线程扩容。​ 第一次添加元素时，默认初始长度为16，当往table中继续添加元素时，通过Hash值跟数组长度取余来决定放在数组的哪个Bucket位置，如果出现放在同一个位置，就优先以链表的形式存放，在同一个位置的个数达到了8个以上，如果数组的长度还小于64，就会扩容数组。如果数组的长度大于等于64，就会将该节点的链表转换成树。​ 通过扩容数组的方式来把这些节点分散开。然后将这些元素复制到扩容后的新数组中，同一个Bucket中的元素通过Hash值的数组长度位来重新确定位置，可能还是放在原来的位置，也可能放到新的位置。而且，在扩容完成之后，如果之前某个节点是树，但是现在该节点的“Key-Value对”数又小于等于6个，就会将该树转为链表。​ put()​ JDK1.8在使用CAS自旋完成桶的设置时，使用synchronized内置锁保证桶内并发操作的线程安全。尽管对同一个Map操作的线程争用会非常激烈，但是在同一个桶内的线程争用通常不会很激烈，所以使用CAS自旋、synchronized不会降低ConcurrentHashMap的性能。为什么不用ReentrantLock显式锁呢?如果为每 个桶都创建一个ReentrantLock实 例，就会带来大量的内存消耗，反过来，使用CAS自旋、synchronized，内存消耗的增加更小。​ get()​ get()通过UnSafe的getObjectVolatile()来读取数组中的元素。为什么要这样做?虽然HashEntry数组的引用是volatile类型，但是数组内元素的 用不是volatile类型，因此多线程对 数组元素的修改是不安全的，可能会在数组中读取到尚未构造完成的元素对象。get()方法通过UnSafe的getObjectVolatile方法来保证元素的读取安全，调用getObjectVolatile()去读取数组元素需要先获得元素在数组中的偏移量，在这里，get()方法根据哈希码计算出偏移量为u，然后通过偏移量u来尝试读取数值。​ ​ ​ ​ ​ ​","categories":[{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"LongAdder","slug":"JUC/LongAdder","date":"2022-01-11T11:06:11.172Z","updated":"2022-01-11T11:28:53.952Z","comments":true,"path":"2022/01/11/JUC/LongAdder/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/JUC/LongAdder/","excerpt":"","text":"一，为什么要用LongAdder 【参考】volatile 解决多线程内存不可见问题。对于一写多读，是可以解决变量同步问题，但是如果多写，同样无法解决线程安全问题。说明：如果是 count++ 操作，使用如下类实现：AtomicInteger count = new AtomicInteger(); count.addAndGet(1); 如果是 JDK8，推荐使用 LongAdder 对象，比 AtomicLong 性能更好（减少乐观 锁的重试次数）。 以上内容来自阿里《Java开发手册》。​ 里面提到了多线程读写环境下，LongAdder相对于Atomic原子类拥有更好的效率。​ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283public class LongAdderAndAtomicTest &#123; private static AtomicInteger a = new AtomicInteger(0); private static LongAdder b = new LongAdder(); public static void main(String[] args) throws Exception &#123; test(1, 10000000); test(10, 10000000); test(20, 10000000); test(50, 10000000); test(100, 10000000); &#125; /** * 测试LongAdder和Atomic的效率 * * @param threadNum 线程数 * @param times 执行时间 */ public static void test(Integer threadNum, Integer times) throws Exception &#123; System.out.println(&quot;线程数为：&quot; + threadNum); testAtomic(threadNum, times); testLongAdder(threadNum, times); &#125; /** * 测试Atomic的效率 * * @param threadNum * @param times */ public static void testAtomic(Integer threadNum, Integer times) throws InterruptedException &#123; //开始时间 long start = System.currentTimeMillis(); CountDownLatch countDownLatch = new CountDownLatch(threadNum); for (int i = 0; i &lt; threadNum; i++) &#123; new Thread(() -&gt; &#123; for (int j = 0; j &lt; times; j++) &#123; a.incrementAndGet(); &#125; countDownLatch.countDown(); &#125;).start(); &#125; countDownLatch.await(); //结束时间 long end = System.currentTimeMillis(); System.out.println(&quot;Atomic 消耗时间：&quot; + (end - start)); &#125; /** * 测试LongAdder的效率 * * @param threadNum * @param times */ public static void testLongAdder(Integer threadNum, Integer times) throws InterruptedException &#123; //开始时间 long start = System.currentTimeMillis(); CountDownLatch countDownLatch = new CountDownLatch(threadNum); for (int i = 0; i &lt; threadNum; i++) &#123; new Thread(() -&gt; &#123; for (int j = 0; j &lt; times; j++) &#123; b.increment(); &#125; countDownLatch.countDown(); &#125;).start(); &#125; countDownLatch.await(); //结束时间 long end = System.currentTimeMillis(); System.out.println(&quot;LongAdder 消耗时间：&quot; + (end - start)); &#125;&#125; 由图可以看到，线程数越多，LongAdder的效率型对于Atomic越高，由此可以看出，LongAdder更适合于高并发情况下。​ 二，LongAdder源码阅读看LongAdder类的继承关系： 1public class LongAdder extends Striped64 implements Serializable LongAdder这个类继承自Striped64，Striped64里面声明了一个内部类Cell。 1234567891011121314151617181920212223242526@sun.misc.Contended static final class Cell &#123; //拥有内存可见性的value volatile long value; //带参数的构造器 Cell(long x) &#123; value = x; &#125; //调用unsafe类的cas对cmp和bal进行比较交换，返回是否成功 final boolean cas(long cmp, long val) &#123; return UNSAFE.compareAndSwapLong(this, valueOffset, cmp, val); &#125; //声明unsafe类 private static final sun.misc.Unsafe UNSAFE; //声明cell成员属性的内存偏移量 private static final long valueOffset; //初始化 static &#123; try &#123; UNSAFE = sun.misc.Unsafe.getUnsafe(); Class&lt;?&gt; ak = Cell.class; valueOffset = UNSAFE.objectFieldOffset (ak.getDeclaredField(&quot;value&quot;)); &#125; catch (Exception e) &#123; throw new Error(e); &#125; &#125;&#125; 1234567891011//获取当前系统的cpu数 控制cells数组长度的一个关键条件static final int NCPU = Runtime.getRuntime().availableProcessors();//数组的长度，只要数组不为空，一定是2的倍数，这样-1转化为二进制的时候一定是一大堆1transient volatile Cell[] cells;//没有发生过竞争时，数据会累加到 base上 | 当cells扩容时，需要将数据写到base中transient volatile long base;//初始化cells或者扩容cells都需要获取锁，0 表示无锁状态，1 表示其他线程已经持有锁了transient volatile int cellsBusy; LongAdder里面使用的这两个属性实际上是继承自他的父类的。​ 12345678910111213141516171819202122232425262728293031323334353637383940public void add(long x) &#123; /** * as: 表示cells数组的引用 * b： 表示获取的base值 * v: 表示期望值 * m: 表示cells数组的长度 * a: 表示当前线程命中的cell单元格 */ Cell[] as; long b, v; int m; Cell a; /** * 条件一：true-&gt;表示cells已经初始化过，当前线程应该将数据写入到对应的cell中 * false-&gt;表示cells未初始化，当前所有线程应该将数据写入到base中 * 条件二：false-&gt;表示当前线程cas替换数据成功 * true-&gt;表示发生竞争了，可能需要重试或者扩容 * 进入if的条件：数组已经初始化 或者 cas交换数据失败，表示有竞争 * */ if ((as = cells) != null || !casBase(b = base, b + x)) &#123; /** * uncontended: true -&gt; 未竞争 false-&gt;发生竞争 * 条件一：true-&gt;数组没有初始化 * false-&gt;数组已经初始化 * 条件二：true-&gt;数组没有初始化 * false-&gt;数组已经初始化 * * getProbe()：获取当前线程的hash值 * 条件三：true-&gt; 当前线程对应的cell并没有初始化 * false-&gt;当前线程对应的cell已经初始化 * 条件四：true-&gt;cas交换失败，表示有竞争 * false-&gt;cas交换成功 * 进入if的条件： * cells未初始化， 或者 当前线程对应的cell未初始化， 或者 cas交换失败 */ boolean uncontended = true; if (as == null || (m = as.length - 1) &lt; 0 || (a = as[getProbe() &amp; m]) == null || !(uncontended = a.cas(v = a.value, v + x))) longAccumulate(x, null, uncontended); &#125;&#125; 接下来看longAccumulate（）。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178/** * 首先：哪些情况会进入当前方法？ * cells未初始化， 或者 当前线程对应的cell未初始化， 或者 cas交换失败 * @param x 新值 * @param fn 没用上。一个扩展接口 * @param wasUncontended 只有cells初始化之后，并且当前线程 竞争修改失败，才会是false */final void longAccumulate(long x, LongBinaryOperator fn, boolean wasUncontended) &#123; //h：代表当前线程hash值 int h; /** * 如果当前线程hash值等于0，条件成立 * 给当前线程分配hash值 * 将当前线程的hash值重新赋值给h * 设置为未竞争或者竞争修改成功状态。 * 为什么？ * 因为默认所有线程进来操做的都是cells[0]的位置，所以不把它当作一次真正的竞争。 */ if ((h = getProbe()) == 0) &#123; //给当前线程分配hash值 ThreadLocalRandom.current(); // force initialization //将当前线程的hash值重新赋值给h h = getProbe(); wasUncontended = true; &#125; //表示扩容意向 false 一定不会扩容，true 可能会扩容。 boolean collide = false; //自旋 for (;;) &#123; /** * as 代表cells的引用 * a 当前线程对应的cell * n cells的长度 * v 期望值 */ Cell[] as; Cell a; int n; long v; /** * case1： * cells已经初始化 * * case1.1： * if(当前线程对应的cell还没有初始化 &amp;&amp; 当前处于无锁状态)&#123; * 创建一个新的cell对象 r * * if（当前锁状态未0并且获取到了锁）&#123; * created : 标记是否创建成功 * if（cells已经被初始化 &amp;&amp; 当前线程对应的cell为空）&#123; * 将当前线程对应位置的cell初始化为新创建的cell r * create=true 表示创建成功，最终在释放锁。 * &#125; * &#125; * 将扩容意向改成false * &#125; * * case1.2: * if(如果当前线程竞争修改失败)&#123; * 状态改为true； * //默认所有线程一开始都在cell[0]的位置，所以一定会发生竞争， * //这次竞争就不当作一次真正的竞争。 * &#125; * * case1.3： * if(当前线程rehash过hash值 &amp;&amp; 新命中的cell不为空)&#123; * 尝试cas一次 * &#125; * * case1.4： * if(如果cells的长度&gt;cpu数 || cells和as不一致)&#123; * //cells和as不一致 说明其他线程已经扩容过了，当前线程只需要rehash重试即可 * 扩容意向强制改为false。 * &#125; * * case1.5： * //!collide = true 设置扩容意向 为true 但是不一定真的发生扩容 * * case1.6： * if(锁状态为0 &amp;&amp; 获取到了锁)&#123; * //第二层判断为了防止当前线程在对第一层id的条件判断一半的时候，又进来一个线程，将所有业务已经执行一遍了。 * //只有当cells==as才能说明，当前线程在第一层if执行条件的过程中，没有其他线程进来破坏。 * if(cells==as)&#123; * 扩容为原来的二倍 * 重置当前线程Hash值 * &#125; * &#125; */ if ((as = cells) != null &amp;&amp; (n = as.length) &gt; 0) &#123; if ((a = as[(n - 1) &amp; h]) == null) &#123; if (cellsBusy == 0) &#123; // Try to attach new Cell Cell r = new Cell(x); // Optimistically create if (cellsBusy == 0 &amp;&amp; casCellsBusy()) &#123; //标记是否创建成功 boolean created = false; try &#123; /** * rs：cells的引用 * m：cells的长度 * j：当前线程对应的cells下标 */ Cell[] rs; int m, j; if ((rs = cells) != null &amp;&amp; (m = rs.length) &gt; 0 &amp;&amp; rs[j = (m - 1) &amp; h] == null) &#123; rs[j] = r; created = true; &#125; &#125; finally &#123; cellsBusy = 0; &#125; if (created) break; continue; // Slot is now non-empty &#125; &#125; collide = false; &#125; else if (!wasUncontended) // CAS already known to fail wasUncontended = true; // Continue after rehash else if (a.cas(v = a.value, ((fn == null) ? v + x : fn.applyAsLong(v, x)))) break; else if (n &gt;= NCPU || cells != as) collide = false; // At max size or stale else if (!collide) collide = true; else if (cellsBusy == 0 &amp;&amp; casCellsBusy()) &#123; try &#123; if (cells == as) &#123; // Expand table unless stale Cell[] rs = new Cell[n &lt;&lt; 1]; for (int i = 0; i &lt; n; ++i) rs[i] = as[i]; cells = rs; &#125; &#125; finally &#123; cellsBusy = 0; &#125; collide = false; continue; // Retry with expanded table &#125; //重置当前线程Hash值 h = advanceProbe(h); &#125; /** * case2： * cells并未初始化 * 锁状态为0 * cells==as ？ 因为其它线程可能会在你给as赋值之后修改了 cells * 获取锁成功 * 里面再次判断cells==as是因为防止其他线程在判断第一层if的中间被其他线程先进来修改了一次 * 初始化cells */ else if (cellsBusy == 0 &amp;&amp; cells == as &amp;&amp; casCellsBusy()) &#123; boolean init = false; try &#123; // Initialize table if (cells == as) &#123; Cell[] rs = new Cell[2]; rs[h &amp; 1] = new Cell(x); cells = rs; init = true; &#125; &#125; finally &#123; cellsBusy = 0; &#125; if (init) break; &#125; /** * case3： * cellsBusy处于加锁状态，表示其他线程正在初始化cells， * 那么当前线程就应该将数据累加到base */ else if (casBase(v = base, ((fn == null) ? v + x : fn.applyAsLong(v, x)))) break; // Fall back on using base &#125;&#125; getProbe()获取当前线程hash值。 123static final int getProbe() &#123; return UNSAFE.getInt(Thread.currentThread(), PROBE);&#125; casCellsBusy()cas的方式获取锁。 123final boolean casCellsBusy() &#123; return UNSAFE.compareAndSwapInt(this, CELLSBUSY, 0, 1);&#125; advanceProbe(h)重置当前线程hash值。 1234567static final int advanceProbe(int probe) &#123; probe ^= probe &lt;&lt; 13; // xorshift probe ^= probe &gt;&gt;&gt; 17; probe ^= probe &lt;&lt; 5; UNSAFE.putInt(Thread.currentThread(), PROBE, probe); return probe;&#125; casBase()cas的方式改变base值。 123final boolean casBase(long cmp, long val) &#123; return UNSAFE.compareAndSwapLong(this, BASE, cmp, val);&#125; 三，LongAdder执行流程​ LongAdder的执行流程实际上就是：​ 当没有线程竞争的时候，线程会直接操做base里面的值。 当有线程竞争的时候，会将base的值拷贝成一个cells数组，每个线程都来操作一个cell数组中的桶位，最终将cells各个桶位和base求和，就可以得到LongAdder的最终值。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192public void add(long x) &#123; if(数组未初始化||cas的方式把值写入base失败)&#123; if(数组未初始化||当前线程对应的单元格未初始化||或cas的方式在当前线程对应的单元格交换数据失败，说明有竞争)&#123; longAccumulate(x, null, uncontended); &#125; &#125;&#125;final void longAccumulate(long x, LongBinaryOperator fn,boolean wasUncontended) &#123; if(当前线程hash值==0)&#123; 重置当前线程hash值 是否发生竞争改为true &#125; 声明扩容意向=false for(;;)&#123; if(数组已经初始化过)&#123; if(当前线程命中的单元格未初始化)&#123; if(如果当前数组不是正在扩容)&#123; 创建一个新的Cell(x) if(当前并未有其他线程对数组进行扩容&amp;&amp;且当前线程竞争扩容数组的权利成功)&#123; 声明 创建完成 =false； if(数组不为空&amp;&amp;当前没有线程正在操作数组&amp;&amp;且当前线程命中的单元格为空)&#123; 将当前线程对应的单元格初始化为刚才创建的cell 创建完成 =true； 释放 数组扩容的权利 &#125; if(数组已经创建完成)&#123; break &#125; continue; &#125; &#125; 扩容意向 =false &#125; else if(如果当前线程竞争修改失败)&#123; 状态设置为true //默认所有线程1开始都在cell[0]的位置，所以一定会发生竞争，这次竞争就不当做一次真正的竞争 &#125; else if(cas的方式修改当前线程命中的单元格成功)&#123; break &#125; else if(数组长度大于cpu数||cells长度 和当前数组长度不一致)&#123; 扩容意向 =false //cells长度 和当前数组长度不一致：说明已经有其他线程扩容了，当前线程只要rehash重试即可。 &#125; else if(扩容意向==false)&#123; 扩容意向 = true //但是不一定真的发生扩容 &#125; else if(当前没有线程正在在扩容数组 &amp;&amp; 并且当前线程竞争到了扩容数组的资格)&#123; if(在这个过程中没有线程把数组扩容了,也就是啥也没发生)&#123; 数组扩容一倍，将原数组的值进行一个拷贝，将新数组的地址指向原数组 &#125; 释放数组扩容的权利 扩容意向改成false &#125; 把当前线程的hash值rehash &#125; //当前数组没有被初始化 else if(当前没有线程正在初始化数组，也没有线程已经初始化完了，并且当前线程竞争到了初始化的权利)&#123; 声明 初始化 == false if(当前数组还未初始化，说明没有线程已经初始化完了)&#123; 创建一个长度为2 的数组 并把当前线程命中的单元格初始化 初始化 == true &#125; 释放数组扩容权利 if(初始化==true)&#123; break &#125; &#125; //如果有线程正在初始化数组，那么当前线程就应该将数据累加到base else if(cas的方式修改base值成功)&#123; break &#125; &#125;&#125; 1.为什么比cas的效率高？​ cas是多线程竞争，只有拿到锁的线程才能去操做资源，其他线程不断的自旋重试，相当于线程排队。 LongAdder是多线程竞争的时候，他会将共享资源拷贝多份，采用分支合并的思想，提升效率。","categories":[{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"Atomic原子类","slug":"JUC/Atomic原子类","date":"2022-01-11T11:06:04.400Z","updated":"2022-01-11T11:24:37.487Z","comments":true,"path":"2022/01/11/JUC/Atomic原子类/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/JUC/Atomic%E5%8E%9F%E5%AD%90%E7%B1%BB/","excerpt":"","text":"所谓的原子性表示一个或者多个操作，要么全部执行完，要么一个也不执行。不能出现成功一部分失败一部分的情况。​ 在多线程中，如果多个线程同时更新一个共享变量，可能会得到一个意料之外的值。比如 i=1 。A 线程更新 i+1 、B 线程也更新 i+1。​ 通过两个线程并行操作之后可能 i 的值不等于 3。而可能等于 2。因为 A 和 B 在更新变量 i 的时候拿到的 i 可能都是 1这就是一个典型的原子性问题。​ 从 JDK1.5 开始，在 J.U.C 包中提供了 Atomic 包，提供了对于常用数据结构的原子操作。它提供了简单、高效、以及线程安全的更新一个变量的方式。​ 1.juc中的原子操作类由于变量类型的关系，在 J.U.C 中提供了 12 个原子操作的类。这 12 个类可以分为四大类：​ 原子更新基本类型：AtomicBoolean、AtomicInteger、AtomicLong ​ 原子更新数组：AtomicIntegerArray 、 AtomicLongArray 、AtomicReferenceArray ​ 原子更新引用：AtomicReference 、 AtomicReferenceFieldUpdater 、AtomicMarkableReference（更新带有标记位的引用类型） ​ 原子更新字段：AtomicIntegerFieldUpdater、AtomicLongFieldUpdater、AtomicStampedReference ​ 2.AtomicInterger源码分析2.1属性&amp;构造器12345678910111213141516171819202122232425private static final long serialVersionUID = 6214790243416807050L;//获取到Unsafe类private static final Unsafe unsafe = Unsafe.getUnsafe();//value的内存偏移量private static final long valueOffset;//初始化的时候获取value的偏移量static &#123; try &#123; valueOffset = unsafe.objectFieldOffset (AtomicInteger.class.getDeclaredField(&quot;value&quot;)); &#125; catch (Exception ex) &#123; throw new Error(ex); &#125;&#125;//具体存放的value值，通过volatile保证内存可见性private volatile int value;//创建一个带有初始值的原子类对象public AtomicInteger(int initialValue) &#123; value = initialValue;&#125;//创建一个默认0值的原子类对象public AtomicInteger() &#123;&#125; 2.2 get()&amp;set()&amp;lazySet()1234567891011121314151617181920212223//获取当前最新值//get 方法只需要直接返回 value 的值就行，这里的 value 是通过 Volatile 修饰的，用来保证可见性。 public final int get() &#123; return value; &#125; //将当前值设置为指定的值 public final void set(int newValue) &#123; value = newValue; &#125; /** * lazyset调用了unsafe的putOrderedInt方法 * 对比一下set和lazySet： * 对比汇编指令差异，可以看到set操作加了lock锁，lazySet没有。 * lock锁的含义是什么呢？ * lazySet提供一个store store屏障（在当代系统中是很低成本的操作，或者说没有操作）， * 但是没有store load屏障（这通常是volatile写入动作最昂贵的部分）。 * @param newValue */ public final void lazySet(int newValue) &#123; unsafe.putOrderedInt(this, valueOffset, newValue); &#125; 2.3 getAndIncrement()​ getAndIncrement 实际上是调用 unsafe 这个类里面提供的方法，这个类相当于是一个后门，使得 Java 可以像 C 语言的指针一样直接操作内存空间。当然也会带来一些弊端，就是指针的问题。​ 实际上这个类在很多方面都有使用，除了 J.U.C 这个包以外，还有 Netty、kafka 等等，这个类提供了很多功能，包括多线程同步(monitorEnter)、CAS 操 作 (compareAndSwap) 、 线 程 的 挂 起 和 恢 复(park/unpark)、内存屏(loadFence/storeFence)内存管理（内存分配、释放内存、获取内存地址等.）​ 123public final int getAndIncrement() &#123; return unsafe.getAndAddInt(this, valueOffset, 1);&#125; 2.4 getAndAddInt()通过 do/while 循环，基于 CAS 乐观锁来做原子递增。实际上前面的 valueOffset 的作用就是从主内存中获得当前value 的值和预期值做一个比较，如果相等，对 value 做递增并结束循环。 123public final int getAndIncrement() &#123; return unsafe.getAndAddInt(this, valueOffset, 1);&#125; 2.5 compareAndSet()它 提 供 了 compareAndSet ， 允 许 客 户 端 基 于AtomicInteger 来实现乐观锁的操作。 123public final boolean compareAndSet(int expect, int update) &#123; return unsafe.compareAndSwapInt(this, valueOffset, expect, update);&#125; 3.AtomicStampReference​ \u0000解决cas的ABA问题锁提供的一个类，解决方法是对数据加版本号。​ 3.1 属性&amp;构造器123456//用volatile来修饰Pair对象保证其内存可见性private volatile Pair&lt;V&gt; pair;//通过传入的初始值和版本号构建pair对象public AtomicStampedReference(V initialRef, int initialStamp) &#123; pair = Pair.of(initialRef, initialStamp);&#125; 3.2 内部类123456789101112private static class Pair&lt;T&gt; &#123; final T reference; //引用数据 final int stamp; //版本号 private Pair(T reference, int stamp) &#123; this.reference = reference; this.stamp = stamp; &#125; //通过泛型方法构造对象 static &lt;T&gt; Pair&lt;T&gt; of(T reference, int stamp) &#123; return new Pair&lt;T&gt;(reference, stamp); &#125;&#125; 3.3 compareAndSet()比较并交换 123456789101112131415161718192021//参数：期望值，新值，期望版本，新版本public boolean compareAndSet(V expectedReference, V newReference, int expectedStamp, int newStamp) &#123; Pair&lt;V&gt; current = pair; return //期望值和当前值相等 expectedReference == current.reference &amp;&amp; //期望版本和新版本相等 expectedStamp == current.stamp &amp;&amp; ( ( //表示版本号对应上的同时，值也对应上，就没有必要创建新的pair对象了 newReference == current.reference &amp;&amp; newStamp == current.stamp ) || //创建新的pair对象 casPair(current, Pair.of(newReference, newStamp)) );&#125; 3.4 casPair()实际上调用的还是unsafe类里面的compareAndSwapObject（）。 12345private boolean casPair(Pair&lt;V&gt; cmp, Pair&lt;V&gt; val) &#123; return UNSAFE.compareAndSwapObject(this, pairOffset, cmp, val);&#125;","categories":[{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"CAS原理","slug":"JUC/CAS原理","date":"2022-01-11T11:05:54.708Z","updated":"2022-01-11T11:24:20.232Z","comments":true,"path":"2022/01/11/JUC/CAS原理/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/JUC/CAS%E5%8E%9F%E7%90%86/","excerpt":"","text":"1.保证i++在多线程下的安全并发对一个数进行++操作会导致结果不准确，因为++操作本身并不是原子性的，所以在多线程下会出现问题。 1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * @author 二十 * @since 2021/8/26 8:53 上午 */public class Demo &#123; private static volatile int count = 0; private static int threadSize = 100; private static CountDownLatch countDownLatch = new CountDownLatch(threadSize); private static void request() &#123; try &#123; TimeUnit.MICROSECONDS.sleep(5); /** * 出现问题的原因：count++分为三步操作， * count-&gt;栈 A * B=A+1 * count=B */ count++; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; for (int i = 0; i &lt; 100; i++) &#123; new Thread(() -&gt; &#123; for (int j = 0; j &lt; 10; j++) &#123; request(); &#125; countDownLatch.countDown(); &#125;).start(); &#125; countDownLatch.await(); System.out.println(&quot;count:&quot; + count); &#125;&#125; 通过加锁的方式可以保证数据的准确性，但是如果直接在request()加锁，效率会很低，因为这样锁的粒度比较大。​ 分析一下count++操作：​ count的值赋值给操作数栈内的a 将操作数栈内的a+1并赋值给b 将操作数栈内b的值赋值给count 并发条件下出现错误的原因就是因为假如a线程在操作一半的过程中（1-3之间），线程b来获取count的值进行++操作，就会获取到不准确的count值。可以控制在第三步加锁，在线程将自己操作数栈内的结果赋值给count之前，先比较当前的最新count和当前线程操作数栈内拷贝的count副本值是否一致，如果一致，则当前线程可以将自己操作数栈的结果赋值给count，否则重新获取count值进行操作，直到成功。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/** * @author 二十 * @since 2021/8/26 8:53 上午 */public class Demo &#123; private static volatile int count = 0; private static int threadSize = 100; private static CountDownLatch countDownLatch = new CountDownLatch(threadSize); private static void request() &#123; try &#123; TimeUnit.MICROSECONDS.sleep(5); /** * 出现问题的原因：count++分为三步操作， * count-&gt;栈 A * B=A+1 * count=B */// count++; int e; while (!compareAndSwap(e = getCount(), e + 1)) &#123; &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; private static int getCount() &#123; return count; &#125; private static synchronized boolean compareAndSwap(int e, int n) &#123; if (getCount() == e) &#123; count = n; return true; &#125; return false; &#125; public static void main(String[] args) throws InterruptedException &#123; for (int i = 0; i &lt; 100; i++) &#123; new Thread(() -&gt; &#123; for (int j = 0; j &lt; 10; j++) &#123; request(); &#125; countDownLatch.countDown(); &#125;).start(); &#125; countDownLatch.await(); System.out.println(&quot;count:&quot; + count); &#125;&#125; 2.Java对cas的支持2.1 cas概念CAS 全称“CompareAndSwap”，中文翻译过来为“比较并替换”​ CAS操作包含三个操作数————内存位置（V）、期望值（A）和新值（B）。 如果内存位置的值与期望值匹配，那么处理器会自动将该位置值更新为新值。 否则，处理器不作任何操作。 无论哪种情况，它都会在CAS指令之前返回该位置的值。（CAS在一些特殊情况下仅返回CAS是否成功，而不提取当前值） CAS有效的说明了“我认为位置V应该包含值A；如果包含该值，则将B放到这个位置；否则，不要更改。 ​ 2.2 jdk中对cas提供的支持java中提供了对CAS操作的支持，具体在sun.misc.unsafe类中，声明如下：​ 123public final native boolean compareAndSwapObject(Object var1, long var2, Object var4, Object var5);public final native boolean compareAndSwapInt(Object var1, long var2, int var4, int var5);public final native boolean compareAndSwapLong(Object var1, long var2, long var4, long var6); 参数var1 表示要操作的对象 参数var2 表示要操作对象中属性地址的偏移量 参数var4 表示需要修改数据的期望的值 参数var5 表示需要修改为的新值 2.3 cas实现原理CAS通过调用JNI的代码实现，JNI：java Native Interface，允许java调用其它语言。而compareAndSwapxxx系列的方法就是借助“C语言”来调用cpu底层指令实现的。以常用的Intel x86平台来说，最终映射到的cpu的指令为“cmpxchg”，这是一个原子指令，cpu执行此命令时，实现比较并替换的操作！​ 2.4 cmpxchg怎么保证多核心下的线程安全？系统底层进行CAS操作的时候，会判断当前系统是否为多核心系统，如果是就给“总线”加锁，只有一个线程会对总线加锁成功，加锁成功之后会执行CAS操作，也就是说CAS的原子性是平台级别的！​ 2.5 cas存在的问题1)ABA问题​ 线程1准备用CAS将变量的值由A替换为B，在此之前，线程2将变量的值由A替换为C，又由C替换为A，然后线程1执行CAS时发现变量的值仍然为A，所以CAS成功。但实际上这时的现场已经和最初不同了，尽管CAS成功，但可能存在潜藏的问题。举例：一个小偷，把别人家的钱偷了之后又还了回来，还是原来的钱吗，你老婆出轨之后又回来，还是原来的老婆吗？ABA问题也一样，如果不好好解决就会带来大量的问题。最常见的就是资金问题，也就是别人如果挪用了你的钱，在你发现之前又还了回来。但是别人却已经触犯了法律。​ 2)循环时间长开销大​ 3)只能保证一个共享变量的原子操作2.6 ABA问题演示123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * @author 二十 * @since 2021/8/26 2:15 下午 */public class Aba &#123; static CountDownLatch countDownLatch = new CountDownLatch(2); static AtomicInteger i = new AtomicInteger(1); public static void main(String[] args) &#123; new Thread(() -&gt; &#123; int e = Aba.i.get(); int n = e + 1; try &#123; TimeUnit.SECONDS.sleep(2); &#125; catch (InterruptedException ex) &#123; ex.printStackTrace(); &#125; boolean flag = i.compareAndSet(e, n); System.out.println(&quot;flag = &quot; + flag); countDownLatch.countDown(); &#125;).start(); try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; new Thread(() -&gt; &#123; i.incrementAndGet(); i.decrementAndGet(); countDownLatch.countDown(); &#125;).start(); try &#123; countDownLatch.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 2.7 jdk如何解决ABA问题解决ABA问题其实很简单，只需要加一个版本号，每次操作，版本号都会加1，每次比较交换的时候，把版本号也比较一下，这样就能确保，这个数据是正确的。​ AtomicStampReference主要包含一个对象引用及一个可以自动更新的整数“stamp”的pair对象来解决ABA问题。​ 123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * @author 二十 * @since 2021/8/26 2:29 下午 */public class AtomicStampReferenceTest &#123; static CountDownLatch countDownLatch = new CountDownLatch(2); static AtomicStampedReference&lt;Integer&gt; i = new AtomicStampedReference(1, 1); public static void main(String[] args) &#123; new Thread(() -&gt; &#123; int e = Aba.i.get(); int n = e + 1; int ev = i.getStamp(); try &#123; TimeUnit.SECONDS.sleep(2); &#125; catch (InterruptedException ex) &#123; ex.printStackTrace(); &#125; boolean flag = i.compareAndSet(e, n, ev, ev + 1); System.out.println(&quot;flag = &quot; + flag); countDownLatch.countDown(); &#125;).start(); try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; new Thread(() -&gt; &#123; i.set(i.getReference() + 1, i.getStamp() + 1); i.set(i.getReference() - 1, i.getStamp() + 1); countDownLatch.countDown(); &#125;).start(); try &#123; countDownLatch.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; ​","categories":[{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"Volatile","slug":"JUC/Volatile","date":"2022-01-11T11:05:46.558Z","updated":"2022-01-11T11:30:33.567Z","comments":true,"path":"2022/01/11/JUC/Volatile/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/JUC/Volatile/","excerpt":"","text":"一，内存模型的相关概念计算机在执行程序时，每条指令都是在CPU中执行的，而执行指令过程中，势必涉及到数据的读取和写入。由于程序运行过程中的临时数据是存放在主存（物理内存）当中的，这时就存在一个问题，由于CPU执行速度很快，而从内存读取数据和向内存写入数据的过程跟CPU执行指令的速度比起来要慢的多，因此如果任何时候对数据的操作都要通过和内存的交互来进行，会大大降低指令执行的速度。因此在CPU里面就有了高速缓存。​ 也就是，当程序在运行过程中，会将运算需要的数据从主存复制一份到CPU的高速缓存当中，那么CPU进行计算时就可以直接从它的高速缓存读取数据和向其中写入数据，当运算结束之后，再将高速缓存中的数据刷新到主存当中。举个简单的例子，比如下面的这段代码：​ 1i=i+1 当线程执行这个语句时，会先从主存当中读取i的值，然后复制一份到高速缓存当中，然后CPU执行指令对i进行加1操作，然后将数据写入高速缓存，最后将高速缓存中i最新的值刷新到主存当中。​ 这个代码在单线程中运行是没有任何问题的，但是在多线程中运行就会有问题了。在多核CPU中，每条线程可能运行于不同的CPU中，因此每个线程运行时有自己的高速缓存（对单核CPU来说，其实也会出现这种问题，只不过是以线程调度的形式来分别执行的）。本文我们以多核CPU为例。​ 比如同时有2个线程执行这段代码，假如初始时i的值为0，那么我们希望两个线程执行完之后i的值变为2。但是事实会是这样吗？​ 可能存在下面一种情况：初始时，两个线程分别读取i的值存入各自所在的CPU的高速缓存当中，然后线程1进行加1操作，然后把i的最新值1写入到内存。此时线程2的高速缓存当中i的值还是0，进行加1操作之后，i的值为1，然后线程2把i的值写入内存。​ 最终结果i的值是1，而不是2。这就是著名的缓存一致性问题。通常称这种被多个线程访问的变量为共享变量。​ 也就是说，如果一个变量在多个CPU中都存在缓存（一般在多线程编程时才会出现），那么就可能存在缓存不一致的问题。​ 为了解决缓存不一致性问题，通常来说有以下2种解决方法：​ 通过总线加Lock锁的方式 通过缓存一致性协议 ​ 这两种方式都是在硬件层面上提供的方式。​ 在早期的CPU当中，是通过在总线上加LOCK锁的形式来解决缓存不一致的问题（相当于让CPU串行，效率低）。因为CPU和其他部件进行通信都是通过总线来进行的，如果对总线加LOCK锁的话，也就是说阻塞了其他CPU对其他部件访问（如内存），从而使得只能有一个CPU能使用这个变量的内存。比如上面例子中 如果一个线程在执行 i = i +1，如果在执行这段代码的过程中，在总线上发出了LCOK锁的信号，那么只有等待这段代码完全执行完毕之后，其他CPU才能从变量i所在的内存读取变量，然后进行相应的操作。这样就解决了缓存不一致的问题。​ 但是上面的方式会有一个问题，由于在锁住总线期间，其他CPU无法访问内存，导致效率低下。​ 所以就出现了缓存一致性协议。最出名的就是Intel 的MESI协议，MESI协议保证了每个缓存中使用的共享变量的副本是一致的（改动则通知的方式）。它核心的思想是：当CPU写数据时，如果发现操作的变量是共享变量，即在其他CPU中也存在该变量的副本，会发出信号通知其他CPU将该变量的缓存行置为无效状态，因此当其他CPU需要读取这个变量时，发现自己缓存中缓存该变量的缓存行是无效的，那么它就会从内存重新读取。 嗅探是实现缓存一致性的常见机制。​ 注意，缓存的一致性问题，不是多处理器导致，而是多缓存导致的。 ​ 嗅探机制工作原理：每个处理器通过监听在总线上传播的数据来检查自己的缓存值是不是过期了，如果处理器发现自己缓存行对应的内存地址修改，就会将当前处理器的缓存行设置无效状态，当处理器对这个数据进行修改操作的时候，会重新从主内存中把数据读到处理器缓存中。​ 注意：基于 CPU 缓存一致性协议，JVM 实现了 volatile 的可见性，但由于总线嗅探机制，会不断的监听总线，如果大量使用 volatile 会引起总线风暴。所以，volatile 的使用要适合具体场景。 ​ 二，并发编程中的三个概念在并发编程中，通常会关心以下三个问题：原子性问题，可见性问题，有序性问题。​ 1.原子性原子性：即一个操作或者多个操作 要么全部执行并且执行的过程不会被任何因素打断，要么就都不执行。​ 一个很经典的例子就是银行账户转账问题：​ 比如从账户A向账户B转1000元，那么必然包括2个操作：从账户A减去1000元，往账户B加上1000元。​ 如果这2个操作不具备原子性，会造成什么样的后果。假如从账户A减去1000元之后，操作突然中止。然后又从B取出了500元，取出500元之后，再执行 往账户B加上1000元 的操作。这样就会导致账户A虽然减去了1000元，但是账户B没有收到这个转过来的1000元。​ 所以这2个操作必须要具备原子性才能保证不出现一些意外的问题。​ 同样地反映到并发编程中会出现什么结果呢？​ 假如为一个32位的变量赋值过程不具备原子性的话，会发生什么后果？​ 1i=9； ​ 假若一个线程执行到这个语句时，暂且假设为一个32位的变量赋值包括两个过程：为低16位赋值，为高16位赋值。​ 那么就可能发生一种情况：当将低16位数值写入之后，突然被中断，而此时又有一个线程去读取i的值，那么读取到的就是错误的数据。​ 2.可见性可见性是指当多个线程访问同一个变量时，一个线程修改了这个变量的值，其他线程能够立即看得到修改的值。​ 123456//线程1执行的代码int i = 0;i = 10; //线程2执行的代码j = i; ​ 假若执行线程1的是CPU1，执行线程2的是CPU2。由上面的分析可知，当线程1执行 i =10这句时，会先把i的初始值加载到CPU1的高速缓存中，然后赋值为10，那么在CPU1的高速缓存当中i的值变为10了，却没有立即写入到主存当中。​ 此时线程2执行 j = i，它会先去主存读取i的值并加载到CPU2的缓存当中，注意此时内存当中i的值还是0，那么就会使得j的值为0，而不是10.​ 这就是可见性问题，线程1对变量i修改了之后，线程2没有立即看到线程1修改的值。​ 3.有序性有序性：即程序执行的顺序按照代码的先后顺序执行。​ 1234int i = 0; boolean flag = false;i = 1; //语句1 flag = true; //语句2 ​ 上面代码定义了一个int型变量，定义了一个boolean类型变量，然后分别对两个变量进行赋值操作。从代码顺序上看，语句1是在语句2前面的，那么JVM在真正执行这段代码的时候会保证语句1一定会在语句2前面执行吗？不一定，为什么呢？这里可能会发生指令重排序（Instruction Reorder）。​ 下面解释一下什么是指令重排序，一般来说，处理器为了提高程序运行效率，可能会对输入代码进行优化，它不保证程序中各个语句的执行先后顺序同代码中的顺序一致，但是它会保证程序最终执行结果和代码顺序执行的结果是一致的。​ 比如上面的代码中，语句1和语句2谁先执行对最终的程序结果并没有影响，那么就有可能在执行过程中，语句2先执行而语句1后执行。​ 但是要注意，虽然处理器会对指令进行重排序，但是它会保证程序最终结果会和代码顺序执行结果相同，那么它靠什么保证的呢？​ 1234int a = 10; //语句1int r = 2; //语句2a = a + 3; //语句3r = a*a; //语句4 这段代码有4个语句，那么可能的一个执行顺序是：​ 那么可不可能是这个执行顺序呢： 语句2 语句1 语句4 语句3​ 不可能，因为处理器在进行重排序时是会考虑指令之间的数据依赖性，如果一个指令Instruction 2必须用到Instruction 1的结果，那么处理器会保证Instruction 1会在Instruction 2之前执行。​ 虽然重排序不会影响单个线程内程序执行的结果，但是多线程呢？​ 123456789//线程1:context = loadContext(); //语句1inited = true; //语句2 //线程2:while(!inited )&#123; sleep()&#125;doSomethingwithconfig(context); 上面代码中，由于语句1和语句2没有数据依赖性，因此可能会被重排序。假如发生了重排序，在线程1执行过程中先执行语句2，而此是线程2会以为初始化工作已经完成，那么就会跳出while循环，去执行doSomethingwithconfig(context)方法，而此时context并没有被初始化，就会导致程序出错。​ 从上面可以看出，指令重排序不会影响单个线程的执行，但是会影响到线程并发执行的正确性。​ 也就是说，要想并发程序正确地执行，必须要保证原子性、可见性以及有序性。只要有一个没有被保证，就有可能会导致程序运行不正确。​ 三，Java内存模型在前面谈到了一些关于内存模型以及并发编程中可能会出现的一些问题。下面来看一下Java内存模型，研究一下Java内存模型提供了哪些保证以及在java中提供了哪些方法和机制来让我们在进行多线程编程时能够保证程序执行的正确性。​ 在Java虚拟机规范中试图定义一种Java内存模型（Java Memory Model，JMM）来屏蔽各个硬件平台和操作系统的内存访问差异，以实现让Java程序在各种平台下都能达到一致的内存访问效果。那么Java内存模型规定了哪些东西呢，它定义了程序中变量的访问规则，往大一点说是定义了程序执行的次序。注意，为了获得较好的执行性能，Java内存模型并没有限制执行引擎使用处理器的寄存器或者高速缓存来提升指令执行速度，也没有限制编译器对指令进行重排序。也就是说，在java内存模型中，也会存在缓存一致性问题和指令重排序的问题。​ JMM 定义了线程和主内存之间的抽象关系：线程之间的共享变量存储在主内存中，每个线程都有一个私有的本地内存，本地内存中存储了该线程以读/写共享变量的副本。​ JMM 的规定： 所有的共享变量都存储于主内存。这里所说的变量指的是实例变量和类变量，不包含局部变量，因为局部变量是线程私有的，因此不存在竞争问题。 每一个线程还存在自己的工作内存，线程的工作内存，保留了被线程使用的变量的工作副本。 线程对变量的所有的操作（读，取）都必须在工作内存中完成，而不能直接读写主内存中的变量。 不同线程之间也不能直接访问对方工作内存中的变量，线程间变量的值的传递需要通过主内存中转来完成。 ​ 1i = 10; ​ 执行线程必须先在自己的工作线程中对变量i所在的缓存行进行赋值操作，然后再写入主存当中。而不是直接将数值10写入主存当中。​ 那么Java语言 本身对 原子性、可见性以及有序性提供了哪些保证呢？​ 1.原子性在Java中，对基本数据类型的变量的读取和赋值操作是原子性操作，即这些操作是不可被中断的，要么执行，要么不执行。​ 1234x = 10; //语句1y = x; //语句2x++; //语句3x = x + 1; //语句4 ​ 只有语句1是原子性操作，其他三个语句都不是原子性操作。​ 语句1是直接将数值10赋值给x，也就是说线程执行这个语句的会直接将数值10写入到工作内存中。​ 语句2实际上包含2个操作，它先要去读取x的值，再将x的值写入工作内存，虽然读取x的值以及 将x的值写入工作内存 这2个操作都是原子性操作，但是合起来就不是原子性操作了。​ 同样的，x++和 x = x+1包括3个操作：读取x的值，进行加1操作，写入新的值。​ 所以上面4个语句只有语句1的操作具备原子性。​ 也就是说，只有简单的读取、赋值（而且必须是将数字赋值给某个变量，变量之间的相互赋值不是原子操作）才是原子操作。​ 不过这里有一点需要注意：在32位平台下，对64位数据的读取和赋值是需要通过两个操作来完成的，不能保证其原子性。但是好像在最新的JDK中，JVM已经保证对64位数据的读取和赋值也是原子性操作了。​ 从上面可以看出，Java内存模型只保证了基本读取和赋值是原子性操作，如果要实现更大范围操作的原子性，可以通过synchronized和Lock来实现。由于synchronized和Lock能够保证任一时刻只有一个线程执行该代码块，那么自然就不存在原子性问题了，从而保证了原子性。​ 2.可见性对于可见性，Java提供了volatile关键字来保证可见性。​ 当一个共享变量被volatile修饰时，它会保证修改的值会立即被更新到主存，当有其他线程需要读取时，它会去内存中读取新值。​ 而普通的共享变量不能保证可见性，因为普通共享变量被修改之后，什么时候被写入主存是不确定的，当其他线程去读取时，此时内存中可能还是原来的旧值，因此无法保证可见性。​ 另外，通过synchronized和Lock也能够保证可见性，synchronized和Lock能保证同一时刻只有一个线程获取锁然后执行同步代码，并且在释放锁之前会将对变量的修改刷新到主存当中。因此可以保证可见性。​ 3.有序性在Java内存模型中，允许编译器和处理器对指令进行重排序，但是重排序过程不会影响到单线程程序的执行，却会影响到多线程并发执行的正确性。 在Java里面，可以通过volatile关键字来保证一定的“有序性”（具体原理在下一节讲述）。另外可以通过synchronized和Lock来保证有序性，很显然，synchronized和Lock保证每个时刻是有一个线程执行同步代码，相当于是让线程顺序执行同步代码，自然就保证了有序性。 另外，Java内存模型具备一些先天的“有序性”，即不需要通过任何手段就能够得到保证的有序性，这个通常也称为 happens-before 原则。如果两个操作的执行次序无法从happens-before原则推导出来，那么它们就不能保证它们的有序性，虚拟机可以随意地对它们进行重排序。 下面就来具体介绍下happens-before原则（先行发生原则）： 程序次序规则：一个线程内，按照代码顺序，书写在前面的操作先行发生于书写在后面的操作 锁定规则：一个unLock操作先行发生于后面对同一个锁额lock操作 volatile变量规则：对一个变量的写操作先行发生于后面对这个变量的读操作 传递规则：如果操作A先行发生于操作B，而操作B又先行发生于操作C，则可以得出操作A先行发生于操作C 线程启动规则：Thread对象的start()方法先行发生于此线程的每个一个动作 线程中断规则：对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生 线程终结规则：线程中所有的操作都先行发生于线程的终止检测，我们可以通过Thread.join()方法结束、Thread.isAlive()的返回值手段检测到线程已经终止执行 对象终结规则：一个对象的初始化完成先行发生于他的finalize()方法的开始 这8条原则摘自《深入理解Java虚拟机》。 对于程序次序规则来说，我的理解就是一段程序代码的执行在单个线程中看起来是有序的。注意，虽然这条规则中提到“书写在前面的操作先行发生于书写在后面的操作”，这个应该是程序看起来执行的顺序是按照代码顺序执行的，因为虚拟机可能会对程序代码进行指令重排序。虽然进行重排序，但是最终执行的结果是与程序顺序执行的结果一致的，它只会对不存在数据依赖性的指令进行重排序。因此，在单个线程中，程序执行看起来是有序执行的，这一点要注意理解。事实上，这个规则是用来保证程序在单线程中执行结果的正确性，但无法保证程序在多线程中执行的正确性。​ 第二条规则也比较容易理解，也就是说无论在单线程中还是多线程中，同一个锁如果出于被锁定的状态，那么必须先对锁进行了释放操作，后面才能继续进行lock操作。​ 第三条规则是一条比较重要的规则，也是后文将要重点讲述的内容。直观地解释就是，如果一个线程先去写一个变量，然后一个线程去进行读取，那么写入操作肯定会先行发生于读操作。​ 第四条规则实际上就是体现happens-before原则具备传递性。​ 后4条规则都是显而易见的。​ 四，深入理解volatile关键字1.volatile关键的两层语义一旦一个共享变量（类的成员变量、类的静态成员变量）被volatile修饰之后，那么就具备了两层语义： 保证了不同线程对这个变量进行操作时的可见性，即一个线程修改了某个变量的值，这新值对其他线程来说是立即可见的。 禁止进行指令重排序。 看一段代码，假如线程1先执行，线程2后执行：​ 12345678//线程1boolean stop = false;while(!stop)&#123; doSomething();&#125; //线程2stop = true; ​ 这段代码是很典型的一段代码，很多人在中断线程时可能都会采用这种标记办法。但是事实上，这段代码会完全运行正确么？即一定会将线程中断么？不一定，也许在大多数时候，这个代码能够把线程中断，但是也有可能会导致无法中断线程（虽然这个可能性很小，但是只要一旦发生这种情况就会造成死循环了）。 下面解释一下这段代码为何有可能导致无法中断线程。在前面已经解释过，每个线程在运行过程中都有自己的工作内存，那么线程1在运行的时候，会将stop变量的值拷贝一份放在自己的工作内存当中。 那么当线程2更改了stop变量的值之后，但是还没来得及写入主存当中，线程2转去做其他事情了，那么线程1由于不知道线程2对stop变量的更改，因此还会一直循环下去。 但是用volatile修饰之后就变得不一样了： 使用volatile关键字会强制将修改的值立即写入主存； 使用volatile关键字的话，当线程2进行修改时，会导致线程1的工作内存中缓存变量stop的缓存行无效（反映到硬件层的话，就是CPU的L1或者L2缓存中对应的缓存行无效）； 由于线程1的工作内存中缓存变量stop的缓存行无效，所以线程1再次读取变量stop的值时会去主存读取。 那么在线程2修改stop值时（当然这里包括2个操作，修改线程2工作内存中的值，然后将修改后的值写入内存），会使得线程1的工作内存中缓存变量stop的缓存行无效，然后线程1读取时，发现自己的缓存行无效，它会等待缓存行对应的主存地址被更新之后，然后去对应的主存读取最新的值。 那么线程1读取到的就是最新的正确的值。​ 2.volatile不保证原子性从上面知道volatile关键字保证了操作的可见性，但是volatile能保证对变量的操作是原子性吗？​ 12345678910111213141516171819202122232425public class Test &#123; public volatile int inc = 0; public void increase() &#123; inc++; &#125; public static void main(String[] args) &#123; final Test test = new Test(); for (int i = 0; i &lt; 10; i++) &#123; new Thread() &#123; public void run() &#123; for (int j = 0; j &lt; 1000; j++) test.increase(); &#125; ; &#125;.start(); &#125; while (Thread.activeCount() &gt; 1) //保证前面的线程都执行完 Thread.yield(); System.out.println(test.inc); &#125;&#125; 这段程序的输出结果是多少？事实上运行它会发现每次运行结果都不一致，都是一个小于10000的数字。 可能有疑问，不对啊，上面是对变量inc进行自增操作，由于volatile保证了可见性，那么在每个线程中对inc自增完之后，在其他线程中都能看到修改后的值啊，所以有10个线程分别进行了1000次操作，那么最终inc的值应该是1000*10=10000。 这里面就有一个误区了，volatile关键字能保证可见性没有错，但是上面的程序错在没能保证原子性。可见性只能保证每次读取的是最新的值，但是volatile没办法保证对变量的操作的原子性。 在前面已经提到过，自增操作是不具备原子性的，它包括读取变量的原始值、进行加1操作、写入工作内存。那么就是说自增操作的三个子操作可能会分割开执行，就有可能导致下面这种情况出现： 假如某个时刻变量inc的值为10， 线程1对变量进行自增操作，线程1先读取了变量inc的原始值，然后线程1被阻塞了； 然后线程2对变量进行自增操作，线程2也去读取变量inc的原始值，由于线程1只是对变量inc进行读取操作，而没有对变量进行修改操作，所以不会导致线程2的工作内存中缓存变量inc的缓存行无效，所以线程2会直接去主存读取inc的值，发现inc的值时10，然后进行加1操作，并把11写入工作内存，最后写入主存。 然后线程1接着进行加1操作，由于已经读取了inc的值，注意此时在线程1的工作内存中inc的值仍然为10，所以线程1对inc进行加1操作后inc的值为11，然后将11写入工作内存，最后写入主存。 那么两个线程分别进行了一次自增操作后，inc只增加了1。 虽然一个变量在修改volatile变量时，会让缓存行无效，然后其他线程去读就会读到新的值。这个就是上面的happens-before规则中的volatile变量规则，但是要注意，线程1对变量进行读取操作之后，被阻塞了的话，并没有对inc值进行修改。然后虽然volatile能保证线程2对变量inc的值读取是从内存中读取的，但是线程1没有进行修改，所以线程2根本就不会看到修改的值。 根源就在这里，自增操作不是原子性操作，而且volatile也无法保证对变量的任何操作都是原子性的。 把上面的代码改改就可以。​ 采用synchronized：​ 1234567891011121314151617181920212223public class Test &#123; public int inc = 0; public synchronized void increase() &#123; inc++; &#125; public static void main(String[] args) &#123; final Test test = new Test(); for(int i=0;i&lt;10;i++)&#123; new Thread()&#123; public void run() &#123; for(int j=0;j&lt;1000;j++) test.increase(); &#125;; &#125;.start(); &#125; while(Thread.activeCount()&gt;1) //保证前面的线程都执行完 Thread.yield(); System.out.println(test.inc); &#125;&#125; 采用Lock：​ 1234567891011121314151617181920212223242526272829public class Test &#123; public int inc = 0; Lock lock = new ReentrantLock(); public void increase() &#123; lock.lock(); try &#123; inc++; &#125; finally&#123; lock.unlock(); &#125; &#125; public static void main(String[] args) &#123; final Test test = new Test(); for(int i=0;i&lt;10;i++)&#123; new Thread()&#123; public void run() &#123; for(int j=0;j&lt;1000;j++) test.increase(); &#125;; &#125;.start(); &#125; while(Thread.activeCount()&gt;1) //保证前面的线程都执行完 Thread.yield(); System.out.println(test.inc); &#125;&#125; 采用AtomicInteger：​ 1234567891011121314151617181920212223public class Test &#123; public AtomicInteger inc = new AtomicInteger(); public void increase() &#123; inc.getAndIncrement(); &#125; public static void main(String[] args) &#123; final Test test = new Test(); for(int i=0;i&lt;10;i++)&#123; new Thread()&#123; public void run() &#123; for(int j=0;j&lt;1000;j++) test.increase(); &#125;; &#125;.start(); &#125; while(Thread.activeCount()&gt;1) //保证前面的线程都执行完 Thread.yield(); System.out.println(test.inc); &#125;&#125; ​ 在java 1.5的java.util.concurrent.atomic包下提供了一些原子操作类，即对基本数据类型的 自增（加1操作），自减（减1操作）、以及加法操作（加一个数），减法操作（减一个数）进行了封装，保证这些操作是原子性操作。atomic是利用CAS来实现原子性操作的（Compare And Swap），CAS实际上是利用处理器提供的CMPXCHG指令实现的，而处理器执行CMPXCHG指令是一个原子性操作。​ 3.volatile保证有序性在前面提到volatile关键字能禁止指令重排序，所以volatile能在一定程度上保证有序性。 volatile关键字禁止指令重排序有两层意思： 当程序执行到volatile变量的读操作或者写操作时，在其前面的操作的更改肯定全部已经进行，且结果已经对后面的操作可见；在其后面的操作肯定还没有进行； 在进行指令优化时，不能将在对volatile变量访问的语句放在其后面执行，也不能把volatile变量后面的语句放到其前面执行。 12345678//x、y为非volatile变量//flag为volatile变量 x = 2; //语句1y = 0; //语句2flag = true; //语句3x = 4; //语句4y = -1; //语句5 由于flag变量为volatile变量，那么在进行指令重排序的过程的时候，不会将语句3放到语句1、语句2前面，也不会讲语句3放到语句4、语句5后面。但是要注意语句1和语句2的顺序、语句4和语句5的顺序是不作任何保证的。​ 并且volatile关键字能保证，执行到语句3时，语句1和语句2必定是执行完毕了的，且语句1和语句2的执行结果对语句3、语句4、语句5是可见的。​ 回到前面举的一个例子：​ 123456789//线程1:context = loadContext(); //语句1inited = true; //语句2 //线程2:while(!inited )&#123; sleep()&#125;doSomethingwithconfig(context); ​ 前面举这个例子的时候，提到有可能语句2会在语句1之前执行，那么久可能导致context还没被初始化，而线程2中就使用未初始化的context去进行操作，导致程序出错。​ 这里如果用volatile关键字对inited变量进行修饰，就不会出现这种问题了，因为当执行到语句2时，必定能保证context已经初始化完毕。​ 4.volatile的原理和实现机制volatile到底如何保证可见性和禁止指令重排序的？​ 下面这段话摘自《深入理解Java虚拟机》：​ 观察加入volatile关键字和没有加入volatile关键字时所生成的汇编代码发现，加入volatile关键字时，会多出一个lock前缀指令​ lock前缀指令实际上相当于一个内存屏障（也成内存栅栏），内存屏障会提供3个功能：​ 它确保指令重排序时不会把其后面的指令排到内存屏障之前的位置，也不会把前面的指令排到内存屏障的后面；即在执行到内存屏障这句指令时，在它前面的操作已经全部完成； 它会强制将对缓存的修改操作立即写入主存； 如果是写操作，它会导致其他CPU中对应的缓存行无效。 ​ 5.禁止指令重排序什么是重排序？​ 为了提高性能，在遵守 as-if-serial 语义（即不管怎么重排序，单线程下程序的执行结果不能被改变。编译器，runtime 和处理器都必须遵守。）的情况下，编译器和处理器常常会对指令做重排序。​ 一般重排序可以分为如下三种类型：​ 编译器优化重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。 指令级并行重排序。现代处理器采用了指令级并行技术来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。 内存系统重排序。由于处理器使用缓存和读 / 写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。 数据依赖性：如果两个操作访问同一个变量，且这两个操作中有一个为写操作，此时这两个操作之间就存在数据依赖性。这里所说的数据依赖性仅针对单个处理器中执行的指令序列和单个线程中执行的操作，不同处理器之间和不同线程之间的数据依赖性不被编译器和处理器考虑。 从 Java 源代码到最终执行的指令序列，会分别经历下面三种重排序： 为了更好地理解重排序，请看下面的部分示例代码： 123456789int a = 0;// 线程 Aa = 1; // 1flag = true; // 2// 线程 Bif (flag) &#123; // 3 int i = a; // 4&#125; ​ 单看上面的程序好像没有问题，最后 i 的值是 1。但是为了提高性能，编译器和处理器常常会在不改变数据依赖的情况下对指令做重排序。假设线程 A 在执行时被重排序成先执行代码 2，再执行代码 1；而线程 B 在线程 A 执行完代码 2 后，读取了 flag 变量。由于条件判断为真，线程 B 将读取变量 a。此时，变量 a 还根本没有被线程 A 写入，那么 i 最后的值是 0，导致执行结果不正确。那么如何程序执行结果正确呢？这里仍然可以使用 volatile 关键字。​ 这个例子中， 使用 volatile 不仅保证了变量的内存可见性，还禁止了指令的重排序，即保证了 volatile 修饰的变量编译后的顺序与程序的执行顺序一样。那么使用 volatile 修饰 flag 变量后，在线程 A 中，保证了代码 1 的执行顺序一定在代码 2 之前。​ 那么， volatile 是如何禁止指令重排序的呢？这里将引出一个概念：内存屏障指令​ 内存屏障指令​ 为了实现 volatile 内存语义（即内存可见性），JMM 会限制特定类型的编译器和处理器重排序。为此，JMM 针对编译器制定了 volatile 重排序规则表，如下所示：​ ​ 使用 volatile 修饰变量时，根据 volatile 重排序规则表，Java 编译器在生成字节码时，会在指令序列中插入内存屏障指令来禁止特定类型的处理器重排序。​ 内存屏障是一组处理器指令，它的作用是禁止指令重排序和解决内存可见性的问题。​ JMM 把内存屏障指令分为下列四类：​ ​ StoreLoad 屏障是一个全能型的屏障，它同时具有其他三个屏障的效果。所以执行该屏障开销会很大，因为它使处理器要把缓存中的数据全部刷新到内存中。 volatile 读 / 写时是如何插入内存屏障的，见下图：​ 从上图，可以知道 volatile 读 / 写插入内存屏障规则： 在每个 volatile 读操作的后面插入 LoadLoad 屏障和 LoadStore 屏障。 在每个 volatile 写操作的前后分别插入一个 StoreStore 屏障和一个 StoreLoad 屏障。 ​ 也就是说，编译器不会对 volatile 读与 volatile 读后面的任意内存操作重排序；编译器不会对 volatile 写与 volatile 写前面的任意内存操作重排序。​ 五，使用volatile的场景synchronized关键字是防止多个线程同时执行一段代码，那么就会很影响程序执行效率，而volatile关键字在某些情况下性能要优于synchronized，但是要注意volatile关键字是无法替代synchronized关键字的，因为volatile关键字无法保证操作的原子性。通常来说，使用volatile必须具备以下2个条件：​ 对变量的写操作不依赖于当前值 该变量没有包含在具有其他变量的不变式中 ​ 实际上，这些条件表明，可以被写入 volatile 变量的这些有效值独立于任何程序的状态，包括变量的当前状态。​ 事实上，我的理解就是上面的2个条件需要保证操作是原子性操作，才能保证使用volatile关键字的程序在并发时能够正确执行。​ 1.状态标记位​ 123456789volatile boolean flag = false; while(!flag)&#123; doSomething();&#125; public void setFlag() &#123; flag = true;&#125; 12345678910volatile boolean inited = false;//线程1:context = loadContext(); inited = true; //线程2:while(!inited )&#123;sleep()&#125;doSomethingwithconfig(context); 2.double check1234567891011121314151617class Singleton&#123; private volatile static Singleton instance = null; private Singleton() &#123; &#125; public static Singleton getInstance() &#123; if(instance==null) &#123; synchronized (Singleton.class) &#123; if(instance==null) instance = new Singleton(); &#125; &#125; return instance; &#125;&#125;","categories":[{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"Synchronized","slug":"JUC/Synchronized","date":"2022-01-11T11:05:39.534Z","updated":"2022-01-11T11:32:10.744Z","comments":true,"path":"2022/01/11/JUC/Synchronized/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/JUC/Synchronized/","excerpt":"","text":"一，线程安全问题线程的合理使用能够提升程序的处理性能，主要有两个方面，第一个是能够利用多核CPU以及超线程技术来实现线程的并行执行；第二个是线程的异步化执行相比于同步来说，异步执行能够很好的优化程序的处理性能提升并发吞吐量。 1.线程安全一个变量i，假如一个线程去访问这个变量进行修改，这个时候对于数据的修改和访问没有任何问题。但是如果更多的线程对于着同一个变量进行修改，就会存在一个数据安全性问题。一个对象是否是线程安全的，取决于他是否会被多个线程访问，以及程序中是如何去使用这个对象的。所以，如果多个线程访问同一个共享对象，在不需要额外的同步以及调用端代码不用做其他协调的情况下，这个共享对象的状态依然是正确的（正确性意味着这个对象的结果与我们预期规定的结果保持一致），那说明这个对象是线程安全的。 2.如何保证线程并行的数据安全能够有一种方法使得线程的并行变成串行。加锁什么是锁？他是处理并发的一种同步手段，如果达到前面的目的，那么这个锁一定需要实现互斥的特性。Java提供加锁的方法就是synchronized关键字。 二，synchronized实现原理多线程并发编程中synchronized一直是元老级的，很多人都会称呼为重量级锁。但是随着jdk6对synchronized进行各种优化之后，有些情况下他就并不是那么重量级了。java6中为了减少获得锁和释放锁带来的性能消耗而引入的偏向锁和轻量级锁。​ synchronized实现同步的基础：Java中每一个对象都可以作为锁。具体表现为三种情况： 普通同步方法，锁是当前实例对象 静态同步方法，锁是当前类的Class对象 同步方法块，锁是sync括号里面配置的对象 ​ 当一个线程试图访问同步代码块时，他首先必须得到锁，退出或者抛出异常时必须释放锁。那么锁到底存储在哪里呢？锁里面会存储什么信息？​ 从JVM规范中可以看到synchronized在jvm里面的实现原理，jvm基于进入和退出monitor对象来实现方法同步和代码块的同步，但是两者的实现细节不一样。代码块同步使用monitorenter和monitorexit指令实现的，而方法的同步是使用另外一种方式实现的，细节在JVM规范里并没有详细说明。但是方法的同步同样可以使用这两个指令来实现。​ monitorenter指令是在编译后插入到同步代码块的开始位置，而monitorexit是插入到方法结束处和异常处，JVM要保证每个monitorenter必须有对应的monitorexit与之配对。任何对象都有一个monitor与之关联，当且一个monitor被持有后，它将处于锁定状态。线程执行到monitorenter指令时，将会尝试获取对象所对应的monitor的所有权，即尝试获得对象的锁。​ 12345678910111213141516/** * @author 二十 * @since 2021/8/26 4:33 下午 */public class SyncTest &#123; &#123; synchronized (Object.class)&#123; try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 执行javap -v SyncTest 命令​ 三，锁的存储​ 分析：要实现多线程的互斥状态，那这把锁需要哪些因素？ 锁需要有一个东西来表示，比如获得锁是什么状态，无锁状态是什么状态？ 这个状态需要对多个线程共享 ​ 观察synchronized的整个语法发现，synchronized(lock)是基于lock这个对象的生命周期来控制锁粒度的，那是不是锁的存储和这个lock对象有关系呢？以对象在jvm内存中是如何存储的作为切入点，分析对象里面有什么特性能够实现锁。 1.对象在内存中的布局在 Hotspot 虚拟机中，对象在内存中的存储布局，可以分为三个区域:对象头(Header)、实例数据(Instance Data)、对齐填充(Padding)。 2.JVM源码实现当我们在java代码里面使用new创建一个对象实例的时候，jvm层面实际上会创建一个instanceOopDesc对象。​ Hotspot虚拟机采用OOP-Klass用来描述对象实例Java对象实例，OOP指的是普通对象指针，Klass用来描述对象实例的具体类型。Hotspot采用instanceOopDesc和arrayOopDesc对象用来描述数组类型。​ instanceOopDesc的定义在Hotspot源码中的instanceOop.hpp文件中，另外，arrayOopDesc的定义对应arrayOop.hpp。 1234567891011121314151617181920212223#ifndef SHARE_OOPS_INSTANCEOOP_HPP#define SHARE_OOPS_INSTANCEOOP_HPP#include &quot;oops/oop.hpp&quot;// An instanceOop is an instance of a Java Class// Evaluating &quot;new HashTable()&quot; will create an instanceOop.class instanceOopDesc : public oopDesc &#123; public: // aligned header size. static int header_size() &#123; return sizeof(instanceOopDesc)/HeapWordSize; &#125; // If compressed, the offset of the fields of the instance may not be aligned. static int base_offset_in_bytes() &#123; return (UseCompressedClassPointers) ? klass_gap_offset_in_bytes() : sizeof(instanceOopDesc); &#125;&#125;;#endif // SHARE_OOPS_INSTANCEOOP_HPP 从 instanceOopDesc 代码中可以看到 instanceOopDesc继承自 oopDesc，oopDesc 的定义在Hotspot 源码中的oop.hpp 文件中。在普通实例对象中，oopDesc 的定义包含两个成员，分别是 mark 和 metadata 。_mark表示对象标记、属于 markOop 类型，也就是接下来要分析的 Mark World，它记录了对象和锁有关的信息。​ _metadata 表示类元信息，类元信息存储的是对象指向它的类元数据(Klass)的首地址，其中 Klass 表示普通指针、_compressed_klass 表示压缩类指针。 3.MarkWord在Hotspot中，markOop 的定义在markOop.hpp 文件中。​ Mark word 记录了对象和锁有关的信息，当某个对象被synchronized 关键字当成同步锁时，那么围绕这个锁的一系列操作都和 Mark word 有关系。Mark Word 在 32 位虚拟机的长度是 32bit、在 64 位虚拟机的长度是 64bit。Mark Word里面存储的数据会随着锁标志位的变化而变化，Mark Word 可能变化为存储以下 5 种情况。​ 4.为什么任何对象都可以实现锁 首先，java中的每个对象都派生自Object类，而每个Java Object 在JVM 内部都有一个native 的C++对象oop/oopDesc 进行对应。 线程在获取锁的时候，实际上就是获得了一个监视器对象（monitor），可以认为他是一个同步对象，所有的java对象天生携带monitor。在hotspot源码的markWord.hpp文件中，可以看到下面这段代码： ​ 12345ObjectMonitor* monitor() const &#123; assert(has_monitor(), &quot;check&quot;); // Use xor instead of &amp;~ to provide one extra tag-bit check. return (ObjectMonitor*) (value() ^ monitor_value);&#125; 多线程访问同步代码块时相当于去争抢对象监视器修改对象中的锁标识，上面的代码中ObjectMonitor这个对象和线程争抢锁的逻辑有密切的关系。​ 5.不同锁状态下MarkWord中存储的内容 四，锁升级使用锁能够实现数据的安全性，但是会带来性能的下降。不使用锁能够基于线程并行提升程序性能，但是却不能保证线程安全性。这两者之间似乎是没有办法达到既能满足性能也能满足安全性的要求。​ hotspot 虚拟机的作者经过调查发现，大部分情况下，加锁的代码不仅仅不存在多线程竞争，而且总是由同一个线程多次获得。所以基于这样一个概率，是的 synchronized 在JDK1.6 之后做了一些优化，为了减少获得锁和释放锁带来的性能开销，引入了偏向锁、轻量级锁的概念。因此在 synchronized 中，锁存在四种状态分别是：无锁、偏向锁、轻量级锁、重量级锁； 锁的状态根据竞争激烈的程度从低到高不断升级。​ 1，偏向锁的基本原理​ 怎么理解偏向锁？ 当一个线程访问加了同步锁的代码块时，会在对象头中存储当前线程的 ID，后续这个线程进入和退出这段加了同步锁的代码块时，不需要再次加锁和释放锁。而是直接比较对象头里面是否存储了指向当前线程的偏向锁。如果相等表示偏向锁是偏向于当前线程的，就不需要再尝试获得锁了。​ 对象一旦生成了hash码，他就无法进入偏向锁状态。也就是说，只要一个对象已经计算过hash码，她就无法进入偏向锁状态。当一个对象当前正处于偏向锁状态，并且需要计算其hash码，他的偏向锁会被撤销，并且锁会膨胀为重量级锁。 偏向锁其实主要解决的是无竞争情况下，锁的性能问题。 1）偏向锁的获取和撤销逻辑​ 首先获取锁 对象的 Markword，判断是否处于可偏向状态。（biased_lock=1、且 ThreadId 为空） ​ 如果是可偏向状态，则通过 CAS 操作，把当前线程的 ID写入到 MarkWord，锁标志位改成01，偏向标志位改成1. ​ 如果 cas 成功，那么 markword 就会变成这样。表示已经获得了锁对象的偏向锁，接着执行同步代码块 如果 cas 失败，说明有其他线程已经获得了偏向锁，这种情况说明当前锁存在竞争，需要撤销已获得偏向锁的线程，并且把它持有的锁升级为轻量级锁（这个操作需要等到全局安全点，也就是没有线程在执行字节码）才能执行 ​ 如果是已偏向状态，需要检查 markword 中存储的ThreadID 是否等于当前线程的 ThreadID ​ 如果相等，不需要再次获得锁，可直接执行同步代码块 ​ 如果不相等，说明当前锁偏向于其他线程，需要撤销偏向锁并升级到轻量级锁2) 偏向锁的撤销​ 偏向锁的撤销并不是把对象恢复到无锁可偏向状态（因为偏向锁并不存在锁释放的概念），而是在获取偏向锁的过程中，发现 cas 失败也就是存在线程竞争时，直接把被偏向的锁对象升级到被加了轻量级锁的状态。​ 对原持有偏向锁的线程进行撤销时，原获得偏向锁的线程有两种情况：​ 原获得偏向锁的线程如果已经退出了临界区，也就是同步代码块执行完了，那么这个时候会把对象头设置成无锁状态并且争抢锁的线程可以基于 CAS 重新偏向当前线程 ​ 如果原获得偏向锁的线程的同步代码块还没执行完，处于临界区之内，这个时候会把原获得偏向锁的线程升级为轻量级锁后继续执行同步代码块 ​ 在我们的应用开发中，绝大部分情况下一定会存在 2 个以上的线程竞争，那么如果开启偏向锁，反而会提升获取锁的资源消耗。所以可以通过 jvm 参数UseBiasedLocking 来设置开启或关闭偏向锁。​ 3）流程图分析 2，轻量级锁的基本原理1）轻量级锁的加锁和解锁逻辑​ 锁升级为轻量级锁之后，对象的 Markword 也会进行相应的变化。升级为轻量级锁的过程：​ 线程在自己的栈桢中创建锁记录 LockRecord。 ​ 将锁对象的对象头中的MarkWord复制到线程的刚刚创建的锁记录中。 ​ 将锁记录中的 Owner 指针指向锁对象。 ​ 将锁对象的对象头的MarkWord替换为指向锁记录的指针。2）自旋锁​ 轻量级锁在加锁过程中，用到了自旋锁。​ 所谓自旋，就是指当有另外一个线程来竞争锁时，这个线程会在原地循环等待，而不是把该线程给阻塞，直到那个获得锁的线程释放锁之后，这个线程就可以马上获得锁的。注意，锁在原地循环的时候，是会消耗 cpu 的，就相当于在执行一个啥也没有的 for 循环。​ 所以，轻量级锁适用于那些同步代码块执行的很快的场景，这样，线程原地等待很短的时间就能够获得锁了。​ 自旋锁的使用，其实也是有一定的概率背景，在大部分同步代码块执行的时间都是很短的。所以通过看似无异议的循环反而能提升锁的性能。​ 但是自旋必须要有一定的条件控制，否则如果一个线程执行同步代码块的时间很长，那么这个线程不断的循环反而会消耗 CPU 资源。默认情况下自旋的次数是 10 次，可以通过 preBlockSpin 来修改。​ 在 JDK1.6 之后，引入了自适应自旋锁，自适应意味着自旋的次数不是固定不变的，而是根据前一次在同一个锁上自旋的时间以及锁的拥有者的状态来决定。​ 如果在同一个锁对象上，自旋等待刚刚成功获得过锁，并且持有锁的线程正在运行中，那么虚拟机就会认为这次自旋也是很有可能再次成功，进而它将允许自旋等待持续相对更长的时间。如果对于某个锁，自旋很少成功获得过，那在以后尝试获取这个锁时将可能省略掉自旋过程，直接阻塞线程，避免浪费处理器资源。 为什么要将内置锁对象的MarkWord复制到锁记录里面？因为内置锁对象的MarkWord的结构会有所变化，MarkWord将会出现一个指向锁记录的指针，而不再存着无锁状态下锁对象哈希码等信息，所以必须将这些信息暂存起来，供后面在锁释放的时候使用。 JDK1.6使用的轻量级锁是普通自旋锁，且需要手动指定jvm参数开启。JDK1.7之后，轻量级锁使用自适应自旋锁，jvm启动的时候自动开启，且自旋时间由jvm自动控制。​ 轻量级锁也被称为非阻塞同步锁，乐观锁，因为这个过程并没有把线程阻塞挂起，而是让线程空循环等待。 3）轻量级锁的解锁​ 轻量级锁的锁释放逻辑其实就是获得锁的逆向逻辑，通过CAS 操作把线程栈帧中的 LockRecord 替换回到锁对象的MarkWord 中，如果成功表示没有竞争。如果失败，表示当前锁存在竞争，那么轻量级锁就会膨胀成为重量级锁。​ 4）流程图分析 3，重量级锁的基本原理​ 当轻量级锁膨胀到重量级锁之后，意味着线程只能被挂起阻塞来等待被唤醒了。​ 1）重量级锁的 monitor123456789public class SyncTest &#123; public static void main(String[] args) &#123; synchronized (SyncTest.class)&#123; &#125; test(); &#125; public static synchronized void test()&#123; &#125;&#125; 加 了 同 步 代 码 块 以 后 ， 在 字 节 码 中 会 看 到 一 个monitorenter 和 monitorexit。​ 每一个 JAVA 对象都会与一个监视器 monitor 关联，我们可以把它理解成为一把锁，当一个线程想要执行一段被synchronized 修饰的同步方法或者代码块时，该线程得先获取到 synchronized 修饰的对象对应的 monitor。monitorenter 表示去获得一个对象监视器。monitorexit 表示释放 monitor 监视器的所有权，使得其他被阻塞的线程可以尝试去获得这个监视器。​ monitor 依赖操作系统的 MutexLock(互斥锁)来实现的, 线程被阻塞后便进入内核（Linux）调度状态，这个会导致系统在用户态与内核态之间来回切换，严重影响锁的性能。​ 2）重量级锁的加锁的基本流程​ 任意线程对 Object（Object 由 synchronized 保护）的访问，首先要获得 Object 的监视器。如果获取失败，线程进入同步队列，线程状态变为 BLOCKED。当访问 Object 的前驱（获得了锁的线程）释放了锁，则该释放操作唤醒阻塞在同步队列中的线程，使其重新尝试对监视器的获取。​ 3）重量级锁核心原理jvm中每一个对象都会有一个监视器，监视器和对象一起创建，销毁。监视器相当于一个用来监视这些线程进入的特殊房间，其义务是保证同一时间只有一个线程可以访问被保护的临界区代码块。​ 本质上，监视器是一种同步工具，也可以说是一种同步机制，主要特点是：​ 同步：监视器所保护的临界区代码是互斥的执行的。一个监视器是一个运行许可，任一线程进入临界区代码都需要获得这个许可，离开时把许可归还。 协作：监视器提供Signal机制，允许正持有许可的线程暂时放弃许可进入阻塞等待状态，等待其他线程发送Signal去唤醒；其他拥有许可的线程可以发送Signal，唤醒正在阻塞等待的线程，让他可以重新获得许可并启动执行。 ​ 在HotSpot虚拟机中，监视器是由C++类ObjectMonitor实现的，ObjectMonitor类定义在ObjectMonitor.hpp文件中。​ ObjectMonitor的Owner，WaitSet，Cxq，EntryList这几个属性比较关键。​ ObjectMonitor的WaitSet，Cxq，EntryList这几个队列存放抢夺重量级锁的线程，而ObjectMonitor的Owner所指向的线程即为获得锁的线程。​ Cxq:竞争队列，所有请求锁的线程首先被放到这个竞争队列。 EntryList:Cxq中那些有资格成为候选资源的线程被移动到EntryList中。 WaitSet：某个拥有ObjectMonitor的线程在调用Object.wait()方法之后将被阻塞，然后该线程将会被放到WaitSet链表中。 ​ CxqCxq并不是一个真正的队列，只是一个虚拟队列，原因在于Cxq是由Node及其next指针逻辑构成的，并不存在一个队列的数据结构。每次新加入Node会在Cxq的队头进行，通过cas改变第一个节点的指针为新增节点，同时设置新增节点的next指向后续的节点；从Cxq取得元素时，会从队尾获取。显然，Cxq结构是一个无锁的结构。​ 因为只有Owner线程才能从队尾取元素，即线程出列操作无争用，当然就避免了cas的aba问题。​ 在线程进入Cxq前，抢锁线程会先尝试通过cas自旋锁获取锁，如果获取不到，就进入Cxq队列，这明显对于已经进入Cxq队列的线程是不公平的。所以，synchronized同步块所使用的重量级锁是不公平锁。​ EntryList​ EntryList与Cxq在逻辑上都属于等待队列。Cxq会被线程并发访问，为了降低对Cxq队尾的争用，而建立EntryList。在Owner线程释放锁时，jvm会从Cxq中迁移线程到EntryList，并会指定EntryList中的某个线程为OnDeck Thread。EntryList中的线程作为候选竞争线程而存在。​ OnDeck Thread &amp; Owner Threadjvm不直接把锁传递给Owner Thread，而是把锁竞争的权利交给OnDeck Thread，OnDeck需要重新竞争锁。这样虽然牺牲了一些公平性，但是能极大的提升系统的吞吐量，在jvm中，也把这种选择行为称为竞争切换。​ OnDeck Thread 获取到锁资源后会变成Owner Thread。无法获得锁的OnDeck Thread则会依然留在EntryList中，考虑到公平性，OnDeck Thread在EntryList中的位置不发生变化。​ 在OnDeck Thread成为Owner的过程中，还有一个不公平的事情，就是后来的新抢锁线程可能直接通过cas自旋成为Owner而抢到锁。​ WaitSet如果Owner线程被Object.wait()阻塞，就转移到WaitSet队列中，直到某个时刻通过Object.notify()唤醒，该线程就会重新进入EntryList中。​ 4）重量级锁的开销处于ContentionList，EntryList，WaitSet中的线程都处于阻塞状态，线程的阻塞或者唤醒都需要操作系统来帮忙，Linux内核下采用pthread_mutex_lock系统调用实现，进程需要从用户态切换到内核态。​ Linux系统体系架构分为用户态和内核态。​ Linux系统的内核是一组特殊的软件程序，负责控制计算机的硬件资源，例如协调cpu资源，分配内存资源，并且提供稳定的环境供应用程序运行。应用程序活动的空间为用户空间，应用程序的执行必须依托于内核提供的资源，包括cpu资源，存储资源，io资源等。​ 用户态与内核态有各自专用的内存空间，专用的寄存器等，进程从用户态切换到内核态需要传递许多变量，参数给内核，内核也需要保护好用户态在切换时的一些寄存器值，变量等，以便内核态调用结束后切换回用户态继续工作。​ 用户态的进程能够访问的资源受到了极大的控制，而运行在内核态的进程可以“为所欲为”。一个进程可以运行在用户态，也可以运行在内核态，那么肯定存在用户态和内核态切换的过程。进程从用户态到内核态切换主要包括以下三种方式:​ (1)硬件中断。硬件中断也称为外设中断，当外设完成用户的请求时会向CPU发送中断信号。​ (2)系统调用。其实系统调用本身就是中断，只不过是软件中断，跟硬件中断不同。​ (3)异常。如果当前进程运行在用户态，这个时候发生了异常事件(例如缺页异常)，就会触发切换。​ 用户态是应用程序运行的空间，为了能访问到内核管理的资源(例如CPU、内存、I/0)，可以通过内核态所提供的访问接口实现，这些接口就叫系统调用。pthredmutexlock系统调用是内核态为用户态进程提供的Linux内核态下互斥锁的访问机制，所以使用pthread_mutex_lock系统调用时，进程需要从用户态切换到内核态，而这种切换是需要消耗很多时间的，有可能比用户执行代码的时间还要长。​ 由于jvm轻量级锁使用cas进行自旋抢锁，这些cas操作都处于用户态下，进程不存在用户态和内核态之间的运行切换，因此jvm轻量级锁开销比较小。而jvm重量级锁使用了Linux内核态下的互斥锁，这是重量级锁开销很大的原因。 4，回顾线程的竞争​ 假如有这样一个同步代码块，存在 Thread#1、Thread#2 等多个线程：​ 123synchronized (lock) &#123;// do something&#125; 情况一：只有 Thread#1 会进入临界区；​ 情况二：Thread#1 和 Thread#2 交替进入临界区,竞争不激烈；​ 情况三：Thread#1/Thread#2/Thread3… 同时进入临界区，竞争激烈。​ 1）偏向锁​ 此时当 Thread#1 进入临界区时，JVM 会将 lockObject 的对象头 Mark Word 的锁标志位设为“01”，同时会用 CAS 操作把 Thread#1 的线程 ID 记录到 Mark Word 中，此时进入偏向模式。所谓“偏向”，指的是这个锁会偏向于 Thread#1，若接下来没有其他线程进入临界区，则 Thread#1 再出入临界区无需再执行任何同步操作。也就是说，若只有Thread#1 会进入临界区，实际上只有 Thread#1 初次进入临界区时需要执行 CAS 操作，以后再出入临界区都不会有同步操作带来的开销。​ 2）轻量级锁偏向锁的场景太过于理想化，更多的时候是 Thread#2 也会尝试进入临界区， 如果 Thread#2 也进入临界区但是Thread#1 还没有执行完同步代码块时，会暂停 Thread#1并且升级到轻量级锁。Thread#2 通过自旋再次尝试以轻量级锁的方式来获取锁。​ 3）重量级锁​ 如果 Thread#1 和 Thread#2 正常交替执行，那么轻量级锁基本能够满足锁的需求。但是如果 Thread#1 和 Thread#2同时进入临界区，那么轻量级锁就会膨胀为重量级锁，意味着 Thread#1 线程获得了重量级锁的情况下，Thread#2就会被阻塞。​ 五，结合wait，notify被阻塞的线程什么时候被唤醒，取决于获得锁的线程什么时候执行完同步代码块并且释放锁。那怎么做到显示控制呢？我们就需要借 助 一 个 信 号机 制 ： 在 Object 对 象 中 ，提 供 了wait/notify/notifyall，可以用于控制线程的状态 1，wait/notify/notifyall的基本概念​ wait：表示持有对象锁的线程 A 准备释放对象锁权限，释放 cpu 资源并进入等待状态。​ notify：表示持有对象锁的线程 A 准备释放对象锁权限，通知 jvm 唤醒某个竞争该对象锁的线程 X。线程 Asynchronized 代码执行结束并且释放了锁之后，线程 X 直接获得对象锁权限，其他竞争线程继续等待(即使线程 X 同步完毕，释放对象锁，其他竞争线程仍然等待，直至有新的 notify ,notifyAll 被调用)。​ notifyAll：notifyall 和 notify 的区别在于，notifyAll 会唤醒所有竞争同一个对象锁的所有线程，当已经获得锁的线程A 释放锁之后，所有被唤醒的线程都有可能获得对象锁权限。​ 三个方法都必须在 synchronized 同步关键字 所 限 定 的 作 用 域 中 调 用 ， 否 则 会 报 错java.lang.IllegalMonitorStateException ，意思是因为没有同步，所以线程对对象锁的状态是不确定的，不能调用这些方法。 通过同步机制来确保线程从 wait 方法返回时能够感知到 notify 线程对变量做出的修改 2，waity /notify 的基本使用12345678910111213141516171819202122232425262728293031323334353637public class SyncTest &#123; public static void main(String[] args) &#123; Share share = new Share(); new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) try &#123; share.add(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, &quot;AAA&quot;).start(); new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) try &#123; share.sub(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, &quot;BBB&quot;).start(); &#125;&#125;class Share &#123; private int num = 0; public synchronized void add()throws Exception&#123; //判断 if (num!=0) this.wait(); System.out.println(Thread.currentThread().getName()+&quot;执行了：&quot;+ ++num); this.notify(); &#125; public synchronized void sub()throws Exception&#123; if (num==0) this.wait(); System.out.println(Thread.currentThread().getName()+&quot;执行了：&quot;+ --num); this.notify(); &#125;&#125; 3，waity /notify 的基本原理 线程A执行MonitorEnter指令成功，获取到锁。 线程A执行Object.wait()，线程A将自己加入到等待队列并释放锁。 线程B执行MonitorEnter指令成功，获取到锁。 线程B执行Object.notify(),线程A从等待队列移动到同步队列，等到收到MonitorExit后，出队列。 ​ 六，synchronized的可重入实现原理每个锁对象拥有一个锁计数器和一个指向持有该锁的线程的指针。​ 当执行monitorenter时，如果目标锁对象的计数器为0，那么说明他没有被其他线程锁持有，Java虚拟机会将改锁对象的持有线程设置为当前线程，并且将其计数器+1.​ 在目标锁对象的计数器不为0的情况下，如果锁对象的持有线程是当前线程，那么Java虚拟机可以将其计数器+1，否则需要等待，直至持有线程释放锁。​ 当执行monitorexit时，Java虚拟机则需要将锁对象的计数器-1.计数器为0代表释放锁。​ 12345678910111213141516171819202122232425public class Demo3 &#123; private static Lock lock=new ReentrantLock(); public static void main(String[] args) &#123; m1(); &#125; public static void m1()&#123; new Thread(()-&gt;&#123; try &#123; lock.lock(); System.out.println(&quot;第一次加锁&quot;); try &#123; lock.lock(); System.out.println(&quot;第2次加锁&quot;); &#125;finally &#123; lock.unlock(); &#125; &#125;finally &#123; lock.unlock(); &#125; &#125;,&quot;m1&quot;).start(); &#125;&#125;","categories":[{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"Unsafe API详解","slug":"JUC/Unsafe API详解","date":"2022-01-11T11:05:27.570Z","updated":"2022-01-11T11:30:55.633Z","comments":true,"path":"2022/01/11/JUC/Unsafe API详解/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/JUC/Unsafe%20API%E8%AF%A6%E8%A7%A3/","excerpt":"","text":"一，Unsafe使用建议 unsafe有可能在未来几年的jdk版本中移除或者不允许java应用代码使用，这一点可能会导致使用了unsafe的应用无法运行在高版本的jdk中。 unsafe不少方法中必须提供原始地址（内存地址）和被替换对象的地址，偏移量要自己计算，一旦出现问题就是jvm级别的崩溃异常。 unsafe提供的直接内存访问的方法中使用的内存不受jvm管理（无法被gc），需要手动管理，一旦出现疏忽可能成为内存泄漏的源头。 ​ 二，unsafe详解unsafe类中一共有82个public native 修饰的方法，还有几十个基于这个82个方法的其他方法。​ 1.初始化123456789101112131415161718private static native void registerNatives();static &#123; registerNatives(); sun.reflect.Reflection.registerMethodsToFilter(Unsafe.class, &quot;getUnsafe&quot;);&#125;private Unsafe() &#123;&#125;private static final Unsafe theUnsafe = new Unsafe();@CallerSensitivepublic static Unsafe getUnsafe() &#123; Class&lt;?&gt; caller = Reflection.getCallerClass(); if (!VM.isSystemDomainLoader(caller.getClassLoader())) throw new SecurityException(&quot;Unsafe&quot;); return theUnsafe;&#125; 新建一个unsafe实例命名为theunsafe，通过静态方法getUnsafe获取，获取的时候需要做权限判断，由此可见，unsafe类的设计使用了单例设计模式，构造器私有化了。unsafe类做了限制，如果是普通的调用的话，他会抛出一个权限异常，只有由引导类加载器加载的类才能使用这个类的方法。最简单的方式是通过反射获取unsafe实例。​ 123Field f = Unsafe.class.getDeclaredField(&quot;theUnsafe&quot;);f.setAccessible(true);Unsafe unsafe = (Unsafe) f.get(null); 2.类，对象和变量相关方法2.1 getObject​ 通过给定的java变量获取引用值。这里实际上是获取一个java对象o中，获取偏移地址==offset的属性的值。​ 此方法可以突破修饰符的限制。类似的方法还有getInt，getDouble等。​ 2.2 putObject将引用值存储到给定的java变量中，这里实际上是设置一个java对象o中偏移地址为offset的属性的值为x。​ 此方法可以突破修饰符的限制。类似的方法还有putInt，putDouble等。​ 2.3 getObjectVolatile​ 此方法和上面的getObject功能类似，不过附加了volatile，强制从主存中获取属性值。​ 这个方法要求被使用的属性被volatile修饰，否则和getObject没有区别。​ 2.4 putObjectVolatile​ 此方法和上面的putObject功能类似，不过附加了volatile，也就是设置值得时候强制刷新的主存，从而保证这些变更对其他线程可见。​ 2.5 putOrderedObject设置o对象中offset偏移地址对应的Object型field的值为指定值x。这是一个有序或者有延迟的putObjectVolatile方法，并且不保证值得改变立马被其他线程看到。​ 只有在field被volatile修饰的时候期望被修改的时候才会生效​ 2.6 staticFieldOffset返回给定的静态属性在他的类的存储分配中的位置（偏移地址）。这个方法仅仅针对静态属性，非静态属性调用会抛出异常。​ 2.7 objectFieldOffset​ 返回给定的非静态属性在他的类的存储分配中的位置（偏移地址）。这个方法仅仅针对非静态属性，静态属性调用会抛出异常。​ 2.8 staticFieldBase​ 返回给定的静态属性的位置，配合staticFieldOffset使用。实际上这个方法的返回值就是静态属性所在的Class对象的一个内存快照。​ 2.9 shouldBeInitialized​ 检测给定的类是否需要初始化。通常需要使用在获取一个类的静态属性的时候（一个类如果没有初始化，他的静态属性也不会初始化）。​ 2.10 arrayIndexScale返回数组类型的比例因子（其实就是数据中元素偏移量地址的增量，因为数组中的元素地址是连续的）。​ 2.11 arrayBaseOffset​ 返回数组类型的第一个元素的偏移地址（基础偏移地址）。如果arrayIndexScale方法返回的比例因子不为0，可以通过结合基础偏移地址和比例因子访问数组的所有元素。​ 3.内存管理3.1 allocateMemory分配一块新的本地内存，通过bytes指定内存块的大小(单位是byte)，返回新开辟的内存的地址。如果内存块的内容不被初始化，那么它们一般会变成内存垃圾。​ 生成的本机指针永远不会为零，并将对所有值类型进行对齐。可以通过freelemory方法释放内存块 或者通过reallocatelemory方法调整内存块大小。​ bytes值为负数或者过大会抛出IIlegalArgumentException异常，如果系统拒绝分配内存会抛出Out( OfMemoryError异常。​ 3.2 reallocateMemory通过指定的内存地址address重新调整本地内存块的大小，调整后的内存块大小通过bytes指定(单立为byte)。可以通过freelemory方法释放内存块，或者通过reallocateMemory方法调整内存块大小。bytes值为负数或者过大会抛出IllegalArgumentEx ception异常，如果系统拒绝分配内存会抛出OutOfMemoryError异常。​ 3.3 freeMemory释放通过allocatelemory方法申请的内存​ 3.4 setMemory将给定内存块中的所有字节设置为固定值(通常是0)。内存块的地址由对象引用o和偏移地址共同决定，如果对象引用o为null，offset就是绝对地址。​ 第三个参数就是内存块的大小，如果使用allocatelenory进行内存开辟的话，这里的值应该和all ocatelemory的参数一致。value就是设置的固定值，一般为0。​ 4.多线程同步4.1 monitorEnter锁定对象，必须通过nonitorExit方法才能解锁。此方法经过实验是可以重入的，也就是可以多次调用，然后通过多次调用monitorExit进行解锁。​ ​ 4.2 monitorExit​ 解锁对象，前提是对象必须已经调用monitorEnter进行加锁，否则抛出IIlegalMonitorStateException异常。​ 4.3 tryMonitorEnter尝试锁定对象，如果加锁成功返回true，否则返回false。必须通过monitorExit方法才能解锁。​ 4.4 compareAndSwapObject针对Object对象进行CAS操作。即是对应Java变量引用o，原子性地更新o中偏移地址为offset的属性的值为x，当且仅当偏移地址为offset的属性的当前值为expected才会更新成功返回true，否则返回false。​ o:目标Java变量引用。​ offset:目标Java变量中的目标属性的偏移地址。​ expected:目标Java变量中的目标属性的期望的当前值。 x:目标Java变量中的目标属性的目标更新值。​ 类似的方法有compareAndSwapInt和compareAndSwapLong，在Jdk8中基于CAS扩展出来的方法有getAn daddInt、getAndAddLong、getAndSetInt、getAndSetLong、getAndSet0bject，它们的作用都是:通过CAS设置新的值，返回旧的值。​ 5.线程的挂起和恢复5.1 unpark恢复线程，必须制定要恢复的线程thread。​ 5.2 park​ 阻塞当前线程直到一个unpark方法出现(被调用)。​ 参数:isAbsolute true 表示绝对时间，阻塞时间按照纳秒计算。false表示相对时间，阻塞时间按照毫秒计算。​ 简单理解isAbsolute true时精度更高。​ 参数:time表示阻塞时间。值为0时表示无限期阻塞，直到有一个线程通过unsafeunpark(当前阻塞线程)才会唤醒。","categories":[{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"并发编程基础知识","slug":"JUC/并发编程基础知识","date":"2022-01-11T11:05:18.345Z","updated":"2022-01-11T11:21:07.732Z","comments":true,"path":"2022/01/11/JUC/并发编程基础知识/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/JUC/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/","excerpt":"","text":"一，线程的创建在java中，有多种方式来实现多线程。继承Thread类，实现Runnable接口，使用ExecutorService，Callable，Future实现带返回结果的多线程。 1.继承Thread类创建线程Thread类的本质是实现了Runnable接口的一个实例，代表一个线程的实例。启动线程的唯一方法就是通过Thread类的start()方法，这是一个本地方法，他会启动一个新线程，并执行run()。 1234567891011121314151617181920212223242526272829303132333435public class Thread implements Runnable&#123; private Runnable target; @Override public void run() &#123; if (target != null) &#123; target.run(); &#125; &#125; public synchronized void start() &#123; if (threadStatus != 0) throw new IllegalThreadStateException(); group.add(this); boolean started = false; try &#123; start0(); started = true; &#125; finally &#123; try &#123; if (!started) &#123; group.threadStartFailed(this); &#125; &#125; catch (Throwable ignore) &#123; /* do nothing. If start0 threw a Throwable then it will be passed up the call stack */ &#125; &#125; &#125; private native void start0();&#125; 2.实现Runnable接口创建多线程12345@FunctionalInterfacepublic interface Runnable &#123; public abstract void run();&#125; 为什么实现Runnable接口能够实现多线程？ 123456789101112131415161718/** * @author yhd * @description 使用Runnable接口创建多线程 * @email yinhuidong2@xiaomi.com * @since 2021/6/27 */public class RunnableTest implements Runnable&#123; public static void main(String[] args) &#123; RunnableTest runnableTest = new RunnableTest(); new Thread(runnableTest).start(); &#125; @Override public void run() &#123; System.out.println(Thread.currentThread().getName()); &#125;&#125; 在Thread类里面，有两个方法 12345678public Thread(Runnable target) &#123; init(null, target, &quot;Thread-&quot; + nextThreadNum(), 0);&#125;private void init(ThreadGroup g, Runnable target, String name, long stackSize, AccessControlContext acc) &#123; this.target = target; &#125; 3.实现callable接口来创建线程有的时候，我们可能需要让一个执行的线程在执行完以后，提供一个返回值给到当前的主线程，主线程需要依赖这个值进行后续的逻辑处理，那么这个时候，就需要用到带返回值的线程了。Java中提供了这样的实现方式： 12345678910111213141516171819202122232425262728293031/** * @author yhd * @description 使用Callable接口创建多线程 * @email yinhuidong2@xiaomi.com * @since 2021/6/27 */public class CallableTest implements Callable &#123; @Override public Object call() throws Exception &#123; System.out.println(&quot;正在计算结果&quot;); return &quot;123&quot;; &#125; public static void main(String[] args) &#123; try &#123; FutureTask task = new FutureTask&lt;&gt;(new CallableTest()); while (task.isDone())&#123; System.out.println(task.get()); &#125; &#125; catch (Exception e) &#123; &#125; finally &#123; &#125; &#125;&#125; 二，线程的生命周期1.生命周期线程一共有六种状态（NEW、RUNNABLE、BLOCKED、WAITING、TIME_WAITING、TERMINATED）New：初始化状态，线程被创建，但是还没有调用start()。Runnabled：运行状态，Java线程把操作系统中的运行状态和就绪状态统称为运行中状态。Blocked：阻塞状态，表示线程进入等待状态，也就是线程因为某种原因放弃了CPU的使用权。阻塞也分为几种情况： 等待阻塞：运行的线程执行wait(),jvm会把当前线程放入到等待队列 同步阻塞：运行的线程在获取对象的同步锁时，若该同步锁被其他线程占用了，那么JVM会把当前线程放入到锁池中，也就是同步队列 其他阻塞：运行中的线程执行thread.sleep()或者thread.join(),或者发出了IO请求时，JVM会把当前线程设置为阻塞状态，当sleep结束，join线程终止，io处理完毕则线程恢复 time——waiting：超时以后自动返回。terminated：终止状态，表示当前线程执行完毕。 2.代码演示线程状态12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/** * @author yhd * @description 代码演示线程状态 * @email yinhuidong2@xiaomi.com * @since 2021/6/27 */public class ThreadStatus &#123; public static void main(String[] args) &#123; //time_waiting new Thread(()-&gt;&#123; while (true)&#123; try &#123; TimeUnit.SECONDS.sleep(100); &#125; catch (Exception e) &#123; &#125; finally &#123; &#125; &#125; &#125;,&quot;time_waiting&quot;).start(); //waiting 线程拿到当前的类锁以后，执行wait() new Thread(()-&gt;&#123; while (true)&#123; synchronized (ThreadStatus.class)&#123; try &#123; ThreadStatus.class.wait(); &#125; catch (Exception e) &#123; &#125; finally &#123; &#125; &#125; &#125; &#125;).start(); //block 两个线程竞争锁 new Thread(new BlockedDemo(),&quot;block-01&quot;).start(); new Thread(new BlockedDemo(),&quot;block-02&quot;).start(); &#125; static class BlockedDemo extends Thread &#123; @Override public void run() &#123; synchronized (BlockedDemo.class) &#123; while (true) &#123; try &#123; TimeUnit.SECONDS.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; &#125;&#125; 3.通过工具查看线程的状态jstack是java虚拟机自带的一种堆栈跟踪工具。 jstack用于打印给定的java进程ID或者core file 或者远程调试服务的Java 堆栈信息。通过上面的代码演示，可以知道线程在整个生命周期中并不是固定处于某种状态，而是随着代码的执行在不同的状态之间进行切换。 三，线程相关方法1.启动调用start()去启动一个线程，当run()中的代码执行完毕以后，线程的生命周期也将终止。调用start()的语义是当前线程告诉jvm，启动调用start()方法的线程。 1）启动原理启动一个线程为什么是调用start()，而不是run()?调用start()实际上是调用一个native方法start0()来启动一个线程，首先start0()这个方法是在Thread.class的静态代码块中注册的。registerNatives的本地方法的定义在文件Thread.c,Thread.c定义了各个操作系统平台要用的关于线程的公共数据和操作。 1234567891011121314151617181920212223242526272829303132333435363738#include &quot;jni.h&quot;#include &quot;jvm.h&quot;#include &quot;java_lang_Thread.h&quot;#define THD &quot;Ljava/lang/Thread;&quot;#define OBJ &quot;Ljava/lang/Object;&quot;#define STE &quot;Ljava/lang/StackTraceElement;&quot;#define ARRAY_LENGTH(a) (sizeof(a)/sizeof(a[0]))static JNINativeMethod methods[] = &#123; &#123;&quot;start0&quot;, &quot;()V&quot;, (void *)&amp;JVM_StartThread&#125;, &#123;&quot;stop0&quot;, &quot;(&quot; OBJ &quot;)V&quot;, (void *)&amp;JVM_StopThread&#125;, &#123;&quot;isAlive&quot;, &quot;()Z&quot;, (void *)&amp;JVM_IsThreadAlive&#125;, &#123;&quot;suspend0&quot;, &quot;()V&quot;, (void *)&amp;JVM_SuspendThread&#125;, &#123;&quot;resume0&quot;, &quot;()V&quot;, (void *)&amp;JVM_ResumeThread&#125;, &#123;&quot;setPriority0&quot;, &quot;(I)V&quot;, (void *)&amp;JVM_SetThreadPriority&#125;, &#123;&quot;yield&quot;, &quot;()V&quot;, (void *)&amp;JVM_Yield&#125;, &#123;&quot;sleep&quot;, &quot;(J)V&quot;, (void *)&amp;JVM_Sleep&#125;, &#123;&quot;currentThread&quot;, &quot;()&quot; THD, (void *)&amp;JVM_CurrentThread&#125;, &#123;&quot;countStackFrames&quot;, &quot;()I&quot;, (void *)&amp;JVM_CountStackFrames&#125;, &#123;&quot;interrupt0&quot;, &quot;()V&quot;, (void *)&amp;JVM_Interrupt&#125;, &#123;&quot;isInterrupted&quot;, &quot;(Z)Z&quot;, (void *)&amp;JVM_IsInterrupted&#125;, &#123;&quot;holdsLock&quot;, &quot;(&quot; OBJ &quot;)Z&quot;, (void *)&amp;JVM_HoldsLock&#125;, &#123;&quot;getThreads&quot;, &quot;()[&quot; THD, (void *)&amp;JVM_GetAllThreads&#125;, &#123;&quot;dumpThreads&quot;, &quot;([&quot; THD &quot;)[[&quot; STE, (void *)&amp;JVM_DumpThreads&#125;,&#125;;#undef THD#undef OBJ#undef STEJNIEXPORT void JNICALLJava_java_lang_Thread_registerNatives(JNIEnv *env, jclass cls)&#123; (*env)-&gt;RegisterNatives(env, cls, methods, ARRAY_LENGTH(methods));&#125; 从这段代码可以看出，start0(),实际上会执行JVM_StartThread(),这个方法是干嘛的？从名字上来看，似乎是JVM层面去启动一个线程，如果真的是这样，那么在JVM层面，一定会调用Java中的run()。来到jvm.cpp文件（这个文件需要下载hotspot源码才能找到）。 12JVM_ENTRY(void, JVM_StartThread(JNIEnv* env, jobject jthread)) native_thread = new JavaThread(&amp;thread_entry, sz); JVM_ENTRY 是用来定义 JVM_StartThread 函数的，在这个函数里面创建了一个真正和平台有关的本地线程. 继续看看 newJavaThread 做了什么事情,继续寻找 JavaThread 的定义。 1234567891011JavaThread::JavaThread(ThreadFunction entry_point, size_t stack_sz) : JavaThread() &#123; _jni_attach_state = _not_attaching_via_jni; set_entry_point(entry_point); // Create the native thread itself. // %note runtime_23 os::ThreadType thr_type = os::java_thread; thr_type = entry_point == &amp;CompilerThread::thread_entry ? os::compiler_thread : os::java_thread; os::create_thread(this, thr_type, stack_sz); &#125; 这个方法有两个参数，第一个是函数名称，线程创建成功之后会根据这个函数名称调用对应的函数；第二个是当前进程内已经有的线程数量。最后重点关注一下os::create_thread,实际就是调用平台创建线程的方法来创建线程。接下来就是线程的启动，会调用 Thread.cpp 文件中的Thread::start(Thread* thread)方法，代码如下： 123456789void Thread::start(Thread* thread) &#123; if (thread-&gt;is_Java_thread()) &#123; java_lang_Thread::set_thread_status(thread-&gt;as_Java_thread()-&gt;threadObj(), JavaThreadStatus::RUNNABLE); &#125; os::start_thread(thread);&#125; start ()中有一个函数调用： os::start_thread(thread);，调用平台启动线程的方法，最终会调用 Thread.cpp 文件中的 JavaThread::run()方法。 2.终止线程的终止并不是简单的调用stop命令，虽然api仍然可以使用，但是和其他的控制线程的方法（`` suspend、resume ``）一样，都是不建议使用的。就拿stop来说，stop()在结束一个线程时并不会保证线程的资源正常释放，因此可能导致程序出现一些不确定的状态。要优雅的去中断一个线程，在线程中提供了一个**interrupt()**。 1）interrupt（）其他线程通过调用当前线程的interrupt(),表示像当前线程打个招呼，告诉他可以中断线程的执行了，至于什么时候中断，取决于当前线程自己。线程通过检查自身 是否被中断来进行响应，可以通过isInterrupted()来判断是否被中断。代码演示线程的优雅终止 12345678910111213141516171819202122232425262728/** * @author yhd * @description 代码演示线程的优雅终止 * @email yinhuidong2@xiaomi.com * @since 2021/6/27 */public class Stop &#123; private static int i; public static void main(String[] args) &#123; Thread t = new Thread(()-&gt;&#123; while (!Thread.currentThread().isInterrupted())&#123; i++; &#125; System.out.println(&quot;num:&quot;+i); &#125;,&quot;interrupt&quot;); t.start(); try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; t.interrupt(); &#125;&#125; 这种通过标识位或者中断操作的方式能够使线程在终止时有机会去清理资源，而不是武断地将线程停止，因此这种终止线程的做法显得更加安全和优雅。 2）Thread.interrupted()上面的案例中，通过 interrupt，设置了一个标识告诉线程可 以 终 止 了 ， 线 程 中 还 提 供 了 静 态 方 法Thread.interrupted()对设置中断标识的线程复位。比如在上面的案例中，外面的线程调用 thread.interrupt 来设置中断标识，而在线程里面，又通过Thread.interrupted把线程的标识又进行了复位。 1234567891011121314151617181920public class InterruptDemo &#123; public static void main(String[] args) throws InterruptedException &#123; Thread thread = new Thread(() -&gt; &#123; while (true) &#123; if (Thread.currentThread().isInterrupted()) &#123; System.out.println(&quot;before:&quot; + Thread.currentThread().isInterrupted()) ; Thread.interrupted(); // 对线程进行复位，由 true 变成 false System.out.println(&quot;after:&quot; + Thread .currentThread().isInterrupted()); &#125; &#125; &#125;, &quot;interruptDemo&quot;); thread.start(); TimeUnit.SECONDS.sleep(1); thread.interrupt(); &#125;&#125; 3)其他线程的复位除了通过 Thread.interrupted 方法对线程中断标识进行复位 以 外 ， 还 有 一 种 被 动 复 位 的 场 景 ， 就 是 对 抛 出InterruptedException 异 常 的 方 法 ， 在InterruptedException 抛出之前，JVM 会先把线程的中断标识位清除，然后才会抛出InterruptedException，这个时候如果调用 isInterrupted()，将会返回 false。 12345678910111213141516171819202122public class InterruptDemo &#123; private static int i; public static void main(String[] args) throws InterruptedException &#123; Thread thread = new Thread(() -&gt; &#123; while (!Thread.currentThread().isInterrupted()) &#123; try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; System.out.println(&quot;Num:&quot; + i); &#125;, &quot;interruptDemo&quot;); thread.start(); TimeUnit.SECONDS.sleep(1); thread.interrupt(); System.out.println(thread.isInterrupted()); &#125;&#125; 4)为什么要复位Thread.interrupted()是属于当前线程的，是当前线程对外界中断信号的一个响应，表示自己已经得到了中断信号，但不会立刻中断自己，具体什么时候中断由自己决定，让外界知道在自身中断前，他的中断状态仍然是 false，这就是复位的原因。 5)线程的终止原理1234567891011121314public void interrupt() &#123; if (this != Thread.currentThread()) checkAccess(); synchronized (blockerLock) &#123; Interruptible b = blocker; if (b != null) &#123; interrupt0(); // Just to set the interrupt flag b.interrupt(this); return; &#125; &#125; interrupt0();&#125; 这个方法里面，调用了 interrupt0()，这个方法在前面分析start()的时候见过，是一个 native 方法，同样，找到 jvm.cpp 文件，找到JVM_Interrupt 的定义，这个方法比较简单，直接调用了 Thread::interrupt(thr)，这个方法的定义在 Thread.cpp 文件中，Thread::interrupt() 调用了 os::interrupt()，这个是调用平台的 interrupt()，这个方法的实现是在 os_*.cpp文件中，其中星号代表的是不同平台，因为 jvm 是跨平台的，所以对于不同的操作平台，线程的调度方式都是不一样的。以 os_linux.cpp 文件为例：set_interrupted(true)实际上就是调用 osThread.hpp 中的set_interrupted()，在 osThread 中定义了一个成员属性 volatile jint _interrupted。 通过上面的代码分析可以知道，thread.interrupt()实际就是设置一个 interrupted 状态标识为 true、并且通过ParkEvent 的 unpark()来唤醒线程。 对于 synchronized 阻塞的线程，被唤醒以后会继续尝试获取锁，如果失败仍然可能被 park 在调用 ParkEvent 的 park 方法之前，会先判断线程的中断状态，如果为 true，会清除当前线程的中断标识 Object.wait 、 Thread.sleep 、 Thread.join 会 抛 出InterruptedException 为什么 Object.wait、Thread.sleep 和 Thread.join 都 会 抛 出InterruptedException?​ 这几个方法有一个共同点，都是属于阻塞的方法，而阻塞方法的释放会取决于一些外部的事件，但是阻塞方法可能因为等不到外部的触发事件而导致无法终止，所以它允许一个线程请求自己来停止它正在做的事情。当一个方法抛出 InterruptedException 时，它是在告诉调用者如果执行该方法的线程被中断，它会尝试停止正在做的事情并且通过抛出 InterruptedException 表示提前返回。所以，这个异常的意思是表示一个阻塞被其他线程中断了。然后，由于线程调用了 interrupt()中断方法，那么Object.wait、Thread.sleep 等被阻塞的线程被唤醒以后会通过 is_interrupted 方法判断中断标识的状态变化，如果发现中断标识为 true，则先清除中断标识，然后抛出InterruptedException。 InterruptedException 异常的抛出并不意味着线程必须终止，而是提醒当前线程有中断的操作发生，至于接下来怎么处理取决于线程本身， 直接捕获异常不做任何处理 将异常往外抛出 停止当前线程，并打印异常信息​ 总结：如果当前线程正处于wait，sleep，join状态，然后当前线程被打断，如果当前线程中断标识为true，清除当前线程的中端标识，并且当前线程会收到中断异常。​ 6)代码时间①打断sleep，wait，join的线程这几个方法都会让线程进入阻塞状态打断sleep的线程，会清空打断状态，并抛出异常 123456789101112131415161718192021222324252627282930313233/** * @author yhd * @description interrupt打断sleep的线程 * @email yinhuidong2@xiaomi.com * @since 2021/6/27 */public class Sleep &#123; public static void test() throws Exception&#123; Thread t = new Thread(()-&gt;&#123; try &#123; Thread.sleep(10000); &#125; catch (Exception e) &#123; System.out.println(&quot;抛出异常&quot;); &#125; finally &#123; &#125; &#125;); t.start(); Thread.sleep(1000); t.interrupt(); System.out.println(&quot;打断状态：&quot;+t.isInterrupted()); &#125; public static void main(String[] args) &#123; try &#123; test(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; ②打断正常运行的线程打断正常运行的线程，不会清空打断状态 1234567891011121314151617181920212223242526272829/** * @author yhd * @description 打断正常运行的线程 * @email yinhuidong2@xiaomi.com * @since 2021/6/27 */public class InterruptTest &#123; private static void test1() throws InterruptedException &#123; Thread t2 = new Thread(()-&gt;&#123; while(true) &#123; Thread current = Thread.currentThread(); boolean interrupted = current.isInterrupted(); if(interrupted) &#123; System.out.println(&quot; 打断状态: &#123;&#125;&quot;+ Thread.currentThread().isInterrupted()); break; &#125; &#125; &#125;, &quot;t2&quot;); t2.start(); sleep(1); t2.interrupt(); &#125; public static void main(String[] args) throws Exception &#123; test1(); &#125;&#125; ③打断park线程1234567891011121314151617181920212223242526/** * @author yhd * @description 打断park线程 * @email yinhuidong2@xiaomi.com * @since 2021/6/27 */public class ParkTest &#123; private static void test1() throws InterruptedException &#123; Thread t1 = new Thread(() -&gt; &#123; System.out.println(&quot;park...&quot;); LockSupport.park(); System.out.println(&quot;unpark...&quot;); System.out.println(&quot;打断状态：&#123;&#125;&quot;+ Thread.currentThread().isInterrupted()); &#125;, &quot;t1&quot;); t1.start(); sleep(1); t1.interrupt(); &#125; public static void main(String[] args) throws Exception &#123; test1(); &#125;&#125; 如果打断标记已经是true，则park会失效。 12345678910111213141516171819202122232425/** * @author yhd * @description park的第二个案例演示 * @email yinhuidong2@xiaomi.com * @since 2021/6/27 */public class ParkTest2 &#123; private static void test4() throws InterruptedException &#123; Thread t1 = new Thread(() -&gt; &#123; for (int i = 0; i &lt; 5; i++) &#123; System.out.println(&quot;park...&quot;); LockSupport.park(); System.out.println(&quot;打断状态：&#123;&#125;&quot;+Thread.currentThread().isInterrupted()); &#125; &#125;); t1.start(); sleep(1); t1.interrupt(); &#125; public static void main(String[] args) throws Exception &#123; test4(); &#125;&#125; 可以使用 Thread.interrupted() 清除打断状态。 3.sleep() &amp; yield()1)sleep() 调用sleep会让当前线程从running状态进入timedwaiting状态（阻塞） 其他线程可以使用interrupt打断正在睡眠的线程，这时sleep()会抛出中断异常 睡眠结束后的线程未必会立即得到执行 建议使用TimeUnit.second.sleep()替代sleep获得更好的可读性 2)yield() 调用yield会让当前线程从running进入runnable就绪状态，然后调度执行其他线程 具体的实现依赖于操作系统的任务调度器 4.join()1)为什么需要join下面的代码，打印r是什么？ 1234567891011121314151617181920212223242526/** * @author yhd */public class JoinDemo &#123; static int r = 0; public static void main(String[] args) throws InterruptedException &#123; test1(); &#125; private static void test1() throws InterruptedException &#123; System.out.println(&quot;开始&quot;); Thread t1 = new Thread(() -&gt; &#123; System.out.println(&quot;开始&quot;); try &#123; sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;结束&quot;); r = 10; &#125;); t1.start(); System.out.println(&quot;结果为:&#123;&#125;&quot;+r); System.out.println(&quot;结束&quot;); &#125;&#125; 分析：因为主线程和线程t1是并行执行的，t1需要1s之后才能算出r=10而主线程一开始就要打印r的结果，所以只能打印r=0 解决方法： sleep为什么不行？因为主线程无法准确捕捉到t1线程什么时候结束 用join，加在t1.start()之后2）有时效的join()等够时间1234567891011121314151617181920212223public class JoinDemo &#123; static int r1 = 0; static int r2 = 0; public static void main(String[] args) throws InterruptedException &#123; test3(); &#125; public static void test3() throws InterruptedException &#123; Thread t1 = new Thread(() -&gt; &#123; try &#123; sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; r1 = 10; &#125;); long start = System.currentTimeMillis(); t1.start();// 线程执行结束会导致 join 结束 t1.join(1500); long end = System.currentTimeMillis(); System.out.println(&quot;r1: &#123;&#125; r2: &#123;&#125; cost: &#123;&#125;&quot;+ r1 +&quot; &quot;+ r2+&quot; &quot;+ (end - start)); &#125;&#125; t1线程的结束会导致调用了t1.join(1500)的主线程结束。 没等够时间 1234567891011121314151617181920212223public class JoinDemo &#123; static int r1 = 0; static int r2 = 0; public static void main(String[] args) throws InterruptedException &#123; test3(); &#125; public static void test3() throws InterruptedException &#123; Thread t1 = new Thread(() -&gt; &#123; try &#123; sleep(2); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; r1 = 10; &#125;); long start = System.currentTimeMillis(); t1.start(); t1.join(1500); long end = System.currentTimeMillis(); System.out.println(&quot;r1: &#123;&#125; r2: &#123;&#125; cost: &#123;&#125;&quot;+ r1 +&quot; &quot;+ r2+&quot; &quot;+ (end - start)); &#125;&#125; join时间到了线程会尝试重新获取CPU执行权。 3）原理调用者轮训检查alive状态 1t1.join(); 等价于 123456synchronized (t1) &#123;// 调用者线程进入 t1 的 waitSet 等待, 直到 t1 运行结束 while (t1.isAlive()) &#123; t1.wait(0); &#125;&#125; 当前线程调用其他线程的wait(),会让当前线程处于阻塞状态，直到其他线程的wait()执行结束。join()能够让线程顺序执行的原因就是底层实际上是wait(),也就是主线程调用t1线程的join(),会让主线程阻塞，直到t1线程的join()执行结束。 5.java中操作线程的方法​ 方法名 static 功能说明 注意 start() 启动一个新线程，在新的线程运行 run 方法中的代码 start 方法只是让线程进入就绪，里面代码不一定立刻运行（CPU 的时间片还没分给它）。每个线程对象的start方法只能调用一次，如果调用了多次会出现IllegalThreadStateException。 run() 新线程启动后会调用的方法 如果在构造 Thread 对象时传递了 Runnable 参数，则线程启动后会调用 Runnable 中的 run 方法，否则默认不执行任何操作。但可以创建 Thread 的子类对象，来覆盖默认行为 join() 等待线程运行结束 join(long n) 等待线程运行结束,最多等待 n毫秒 getId() 获取线程长整型的 id 唯一 getName() 获取线程名 setName(String) 修改线程名 getPriority() 获取线程优先级 setPriority(int) 修改线程优先级 java中规定线程优先级是1~10 的整数，较大的优先级能提高该线程被 CPU 调度的机率 getState() 获取线程状态 Java 中线程状态是用 6 个 enum 表示，分别为：NEW, RUNNABLE, BLOCKED, WAITING,TIMED_WAITING, TERMINATED isInterrupted() 判断是否被打断 不会清除 打断标记 isAlive() 线程是否存活（还没有运行完毕） interrupt() 打断线程 如果被打断线程正在 sleep，wait，join 会导致被打断的线程抛出 InterruptedException，并清除 打断标记 ；如果打断的正在运行的线程，则会设置 打断标记 ；park 的线程被打断，也会设置 打断标记 interrupted() static 判断当前线程是否被打断 会清除 打断标记 currentThread() static 获取当前正在执行的线程 sleep(long n) static 让当前执行的线程休眠n毫秒，休眠时让出 cpu的时间片给其它线程 yield() static 提示线程调度器让出当前线程对CPU的使用 主要是为了测试和调试 6.LockSupport​ 用于创建锁和其他同步类的基本线程阻塞原语。​ 本质就是线程阻塞和唤醒wait notify的加强版​ park() unpark() 1）线程阻塞唤醒的三种方法①使用Object中的wait()让线程等待，使用Object的notify()唤醒线程。1234567891011121314151617181920212223242526272829303132/** * @author yhd * @description 阻塞&amp;唤醒 * @email yinhuidong2@xiaomi.com * @since 2021/6/28 */public class WaitNotify &#123; private static Object lock = new Object(); public static void main(String[] args) &#123; new Thread(()-&gt;&#123; synchronized (lock)&#123; try &#123; lock.wait(); System.out.println(Thread.currentThread().getName()+&quot;继续执行!&quot;); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;,&quot;t1&quot;).start(); new Thread(()-&gt;&#123; synchronized (lock)&#123; lock.notify(); System.out.println(Thread.currentThread().getName()+&quot;唤醒&quot;); &#125; &#125;,&quot;t2&quot;).start(); &#125;&#125; 存在的问题：必须在synchronized代码块里，顺序不能反。​ ②使用Condition的await和signal方法123456789101112131415161718192021222324252627282930313233343536373839/** * @author yhd * @description 阻塞&amp;唤醒 * @email yinhuidong2@xiaomi.com * @since 2021/6/28 */public class AwaitSignal &#123; private static Lock lock = new ReentrantLock(); private static Condition condition =lock.newCondition(); public static void main(String[] args) &#123; new Thread(()-&gt;&#123; lock.lock(); try &#123; condition.await(); &#125; catch (Exception e) &#123; &#125; finally &#123; lock.unlock(); &#125; &#125;,&quot;t1&quot;).start(); new Thread(()-&gt;&#123; lock.lock(); try &#123; condition.signal(); &#125; catch (Exception e) &#123; &#125; finally &#123; lock.unlock(); &#125; &#125;,&quot;t2&quot;).start(); &#125;&#125; 存在的问题：必须在lock代码块里面使用，使用顺序不能反。​ ③LockSupport类park()可以阻塞当前线程以及unpark(Thread)唤醒指定被阻塞的线程。LockSupport类使用了一种名为Permit（许可）的概念来做到阻塞和唤醒线程的功能，每个线程都有一个许可，permit只有1和0两个值，默认是0.​ permit默认是0，所以一开始调用park()方法，当前线程就会阻塞，直到别的线程将当前线程的permit设置为1时，park方法会被唤醒，然后将permit再次设置为0并返回。​ 1234567891011121314151617/** * @author yhd * @description 阻塞&amp;唤醒 * @email yinhuidong2@xiaomi.com * @since 2021/6/28 */public class ParkUnpark &#123; public static void main(String[] args) &#123; LockSupport.unpark(Thread.currentThread()); LockSupport.park(); LockSupport.park(); &#125;&#125; 2）总结LockSupport是一个线程阻塞工具类，所有的方法都是静态方法，可以让线程在任意位置阻塞，阻塞之后也有对应的唤醒方法。归根结底，LockSupport调用的Unsafe中的native代码。 LockSupport提供park()和unpark()方法实现阻塞线程和解除线程阻塞的过程​ LockSupport和每个使用它的线程都有一个许可(permit)关联。permit相当于1，0的开关，默认是0，调用一次unpark就加1变成1，调用一次park会消费permit，也就是将1变成o，同时park立即返回。​ 如再次调用park会变成阻塞(因为permit为零了会阻塞在这里，一直到permit变为1)，这时调用unpark会把permit置为1。​ 每个线程都有一个相关的permit, permit最多只有一个，重复调用unpark也不会积累凭证。​ 为什么可以先唤醒线程后阻塞线程? 因为unpark获得了一个凭证，之后再调用park方法，就可以名正言顺的凭证消费，故不会阻塞。 为什么唤醒两次后阻塞两次，但最终结果还会阻塞线程? 因为凭证的数量最多为1，连续调用两次unpark和调用一次unpark效果一样，只会增加一个凭证;而调用两次park却需要消费两个凭证，证不够，不能放行。 四，多线程相关概念1.并发？并行单核 cpu 下，线程实际还是 串行执行 的。操作系统中有一个组件叫做任务调度器，将 cpu 的时间片（windows下时间片最小约为 15 毫秒）分给不同的程序使用，只是由于 cpu 在线程间（时间片很短）的切换非常快，人类感觉是 同时运行的 。总结为一句话就是： 微观串行，宏观并行 ，一般会将这种 线程轮流使用 CPU 的做法称为并发， concurrent cpu 时间片1 时间片2 时间片3 时间片4 core 线程1 线程2 线程3 线程4 多核 cpu下，每个 核（core） 都可以调度运行线程，这时候线程可以是并行的。​ cpu 时间片1 时间片2 时间片3 时间片4 core1 线程1 线程1 线程3 线程3 core2 线程2 线程2 线程4 线程4 并发是同一时间应对多件事情的能力。并行是同一时间动手做多件事情的能力。 单核CPU下，多线程不能实际提高运行效率，只是为了能够在不同的任务之间切换，不同的线程轮流使用CPU，不至于一个线程总占用CPU，别的线程没法干活。 多核CPU可以并行跑多个线程，但能否提高运行效率还是要看具体情况的。 IO操作不占用CPU，只是一般拷贝文件使用的是阻塞IO，这时相当于线程虽然不用CPU，但是需要一直等待IO结束，没能充分利用线程。所以后面才有非阻塞IO和异步IO的优化。 2.线程上下文的切换因为以下一些原因导致 cpu 不再执行当前的线程，转而执行另一个线程的代码[1]。 线程的cpu时间片用完 垃圾回收 有更高优先级的线程需要运行 线程自己调用了 sleep、yield、wait、join、park、synchronized、lock 等方法。 当 Context Switch 发生时，需要由操作系统保存当前线程的状态，并恢复另一个线程的状态[2]，Java 中对应的概念就是程序计数器（Program Counter Register），它的作用是记住下一条 jvm 指令的执行地址，是线程私有的 3.主线程与守护线程默认情况下，Java 进程需要等待所有线程都运行结束，才会结束。有一种特殊的线程叫做守护线程，只要其它非守护线程运行结束了，即使守护线程的代码没有执行完，也会强制结束。 垃圾回收器线程就是一种守护线程。 Tomcat 中的 Acceptor 和 Poller 线程都是守护线程，所以 Tomcat 接收到 shutdown 命令后，不会等待它们处理完当前请求。 4.临界区的概念一个程序运行多个线程本身是没有问题的，问题出在多个线程访问共享资源。 多个线程读共享资源其实也没有问题，在多个线程对共享资源读写操作时发生指令交错，就会出现问题。 一段代码块内如果存在对共享资源的多线程读写操作，称这段代码块为临界区。 5.死锁1）死锁有这样的情况：一个线程需要同时获取多把锁，这时就容易发生死锁 t1 线程 获得 A对象 锁，接下来想获取 B对象 的锁 t2 线程 获得 B对象 锁，接下来想获取 A对象 的锁 例： 2）定位死锁检测死锁可以使用 jconsole工具，或者使用 jps 定位进程 id，再用 jstack 定位死锁。 避免死锁要注意加锁顺序。 另外如果由于某个线程进入了死循环，导致其它线程一直等待，对于这种情况 linux 下可以通过 top 先定位到CPU 占用高的 Java 进程，再利用 top -Hp 进程id来定位是哪个线程，最后再用 jstack 排查。 6.活锁活锁出现在两个线程互相改变对方的结束条件，最后谁也无法结束。 7.final原理1）设置 final 变量的原理字节码 发现 final 变量的赋值也会通过 putfield 指令来完成，同样在这条指令之后也会加入写屏障，保证在其它线程读到它的值时不会出现为 0 的情况。 Context Switch 频繁发生会影响性能. ↩︎ 状态包括程序计数器、虚拟机栈中每个栈帧的信息，如局部变量、操作数栈、返回地址等. ↩︎ 8.可重入锁可重入锁又名递归锁 是指在同一个线程在外层方法获取锁的时候，再进去该线程的内层方法会自动获取锁（前提：同一个锁对象），不会因为之前已经获取过还没释放而阻塞。​ java中的ReentrantLock和Synchronized都是可重入锁，可重入锁的一个优点是可一定程度避免死锁。​ 12345678910111213141516171819202122public class Demo2 &#123; private static Object lock = new Object(); public static void main(String[] args) &#123; test(); &#125; public static void test() &#123; new Thread(() -&gt; &#123; synchronized (lock) &#123; System.out.println(Thread.currentThread().getName() + &quot;外&quot;); synchronized (lock) &#123; System.out.println(Thread.currentThread().getName() + &quot;中&quot;); synchronized (lock) &#123; System.out.println(Thread.currentThread().getName() + &quot;内&quot;); &#125; &#125; &#125; &#125;).start(); &#125;&#125; 五，线程通信1.两个线程交替打印题目： i=0,a:i++,b:i–,交替打印10次​ 1.1 使用synchronized12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/** * @author 二十 * @since 2021/8/31 9:01 下午 */public class DemoA &#123; static CountDownLatch count=new CountDownLatch(2); public static void main(String[] args)throws Exception &#123; Lock lock = new Lock(); new Thread(()-&gt;&#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; lock.add(); &#125;catch (Exception e)&#123; &#125; &#125; count.countDown(); &#125;,&quot;A&quot;).start(); new Thread(()-&gt;&#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; lock.del(); &#125;catch (Exception e)&#123; &#125; &#125; count.countDown(); &#125;,&quot;B&quot;).start(); count.await(); &#125; private static class Lock&#123; static int num = 0; public synchronized void add()throws Exception&#123; if (num!=0)&#123; this.wait(); &#125; System.out.println(Thread.currentThread().getName()+&quot;num = &quot; + ++num); this.notify(); &#125; public synchronized void del()throws Exception&#123; if (num!=1)&#123; this.wait(); &#125; System.out.println(Thread.currentThread().getName()+&quot;num = &quot; + --num); this.notify(); &#125; &#125;&#125; 1.2 两个线程可以正常执行，现在增加到四个123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475/** * @author 二十 * @since 2021/8/31 9:24 下午 */public class DemoB &#123; static CountDownLatch count=new CountDownLatch(4); public static void main(String[] args)throws Exception &#123; Lock lock = new Lock(); new Thread(()-&gt;&#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; lock.add(); &#125;catch (Exception e)&#123; &#125; &#125; count.countDown(); &#125;,&quot;A&quot;).start(); new Thread(()-&gt;&#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; lock.del(); &#125;catch (Exception e)&#123; &#125; &#125; count.countDown(); &#125;,&quot;B&quot;).start(); new Thread(()-&gt;&#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; lock.add(); &#125;catch (Exception e)&#123; &#125; &#125; count.countDown(); &#125;,&quot;C&quot;).start(); new Thread(()-&gt;&#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; lock.del(); &#125;catch (Exception e)&#123; &#125; &#125; count.countDown(); &#125;,&quot;D&quot;).start(); count.await(); &#125; private static class Lock&#123; static int num = 0; public synchronized void add()throws Exception&#123; if (num!=0)&#123; this.wait(); &#125; System.out.println(Thread.currentThread().getName()+&quot;num = &quot; + ++num); this.notify(); &#125; public synchronized void del()throws Exception&#123; if (num!=1)&#123; this.wait(); &#125; System.out.println(Thread.currentThread().getName()+&quot;num = &quot; + --num); this.notify(); &#125; &#125;&#125; 1.3 线程间调用化定制通信查看jdkAPI wait（）；​ 注意：判断一定要while循环判断，不能用if，防止多线程虚假唤醒。​ 1.4 使用lock12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970/** * @author 二十 * @since 2021/8/31 9:30 下午 */public class DemoC &#123; private static CountDownLatch count = new CountDownLatch(2); public static void main(String[] args)throws Exception &#123; Data data = new Data(); new Thread(()-&gt;&#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; data.add(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; count.countDown(); &#125;,&quot;A&quot;).start(); new Thread(()-&gt;&#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; data.del(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; count.countDown(); &#125;,&quot;B&quot;).start(); count.await(); &#125; private static class Data &#123; static volatile int num =0; static ReentrantLock lock = new ReentrantLock(); static Condition c =lock.newCondition(); public void add()throws Exception&#123; lock.lock(); try &#123; while (num!=0)&#123; c.await(); &#125; System.out.println(Thread.currentThread().getName()+&quot; num = &quot; + ++num); c.signal(); &#125;finally &#123; lock.unlock(); &#125; &#125; public void del()throws Exception&#123; lock.lock(); try &#123; while (num!=1)&#123; c.await(); &#125; System.out.println(Thread.currentThread().getName()+&quot; num = &quot; + --num); c.signal(); &#125;finally &#123; lock.unlock(); &#125; &#125; &#125;&#125; 2.多个线程按顺序打印题目：​ 多线程之间按照顺序调用，实现A-B-C​ 三个线程启动，要求如下：​ AA打印5次，BB打印10次，CC打印15次​ 接着循环10轮 分析：​ 有顺序通知，需要有标识位 有一个锁，lock，3把钥匙condition 判断标识位 输出线程名+第几次+第几轮 修改标识位，通知下一个 ​ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100/** * @author 二十 * @since 2021/8/31 9:48 下午 */public class DemoD &#123; static CountDownLatch c = new CountDownLatch(3); public static void main(String[] args) throws Exception &#123; Data data = new Data(); for (int i = 0; i &lt; 10; i++) &#123; new Thread(() -&gt; &#123; data.p1(); c.countDown(); &#125;, &quot;A&quot;).start(); new Thread(() -&gt; &#123; data.p2(); c.countDown(); &#125;, &quot;B&quot;).start(); new Thread(() -&gt; &#123; data.p3(); c.countDown(); &#125;, &quot;C&quot;).start(); &#125; c.await(); &#125; private static class Data &#123; static int flag = 0; ReentrantLock lock = new ReentrantLock(); Condition c1 = lock.newCondition(); Condition c2 = lock.newCondition(); Condition c3 = lock.newCondition(); public void printf() &#123; System.out.println(Thread.currentThread().getName() + &quot; flag&quot; + flag); &#125; public void p1() &#123; lock.lock(); try &#123; while (flag != 0) &#123; try &#123; c1.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; for (int i = 0; i &lt; 5; i++) &#123; printf(); &#125; flag++; c2.signal(); &#125; finally &#123; lock.unlock(); &#125; &#125; public void p2() &#123; lock.lock(); try &#123; while (flag != 1) &#123; try &#123; c2.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; for (int i = 0; i &lt; 10; i++) &#123; printf(); &#125; flag++; c3.signal(); &#125; finally &#123; lock.unlock(); &#125; &#125; public void p3() &#123; lock.lock(); try &#123; while (flag != 2) &#123; try &#123; c3.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; for (int i = 0; i &lt; 15; i++) &#123; printf(); &#125; flag = 0; c1.signal(); &#125; finally &#123; lock.unlock(); &#125; &#125; &#125;&#125; 六，线程与集合1.如何证明集合是线程不安全的123456789101112131415161718 public static void main(String[] args) &#123; List&lt;String&gt; list = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 30; i++) &#123; new Thread(() -&gt; &#123; list.add(UUID.randomUUID().toString().substring(8)); System.out.println(list); &#125;, String.valueOf(i)).start(); &#125; &#125;//java.util.ConcurrentModificationException 2.如何让集合变的安全2.1 调用工具类12List&lt;String&gt; list= Arrays.asList(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;);List&lt;String&gt; list2= Collections.synchronizedList(list); 2.2 使用JUC copyOnWriteArrayList ConcurrentLinkedQueue ConcurrentHashMap ​ 1234567891011121314151617public static void main(String[] args) &#123; CopyOnWriteArrayList&lt;String&gt; list = new CopyOnWriteArrayList&lt;&gt;(); for (int i = 0; i &lt; 30; i++) &#123; new Thread(()-&gt;&#123; list.add(UUID.randomUUID().toString().substring(8)); System.out.println(list); &#125;,String.valueOf(i)).start(); &#125;&#125; 源码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576/** * @author 二十 * @since 2021/8/31 10:44 下午 */public class DemoF &#123; public static void main(String[] args) &#123; Printers printers = new Printers(); new Thread(() -&gt; &#123; for (int i = 1; i &lt;= 26; i++) printers.printNum(); &#125;, &quot;数字打印线程&quot;).start(); new Thread(() -&gt; &#123; for (int i = 1; i &lt;= 26; i++) printers.printLetter(i + 64); &#125;, &quot;字母打印线程&quot;).start(); &#125; private static class Printers &#123; private int num = 1; private int a = 0; private ReentrantLock lock = new ReentrantLock(); private Condition cd1 = lock.newCondition(); private Condition cd2 = lock.newCondition(); public void printNum() &#123; try &#123; lock.lock(); while (num != 1) cd1.await(); for (int i = 0; i &lt; 2; i++) &#123; System.out.println(Thread.currentThread().getName() + &quot;打印了：&quot; + ++a); &#125; num++; cd2.signal(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; public void printLetter(int aa) &#123; try &#123; lock.lock(); while (num != 2) cd2.await(); System.out.println(Thread.currentThread().getName() + &quot;打印了：&quot; + (char) aa); num--; cd1.signal(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; &#125;&#125;","categories":[{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"高并发核心模式之异步回调","slug":"JUC/同步队列","date":"2022-01-11T11:05:09.782Z","updated":"2022-01-11T11:22:06.594Z","comments":true,"path":"2022/01/11/JUC/同步队列/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/JUC/%E5%90%8C%E6%AD%A5%E9%98%9F%E5%88%97/","excerpt":"","text":"二，源码1.内部类和成员变量12345678910111213141516171819202122232425262728293031323334353637abstract static class Transferer&lt;E&gt; &#123; /** * * @param e 可以为null，null的时候表示这个是一个request类型的请求， * 如果不是null，说明当前请求是一个data类型的请求 * @param timed true 表示指定了超时时间， false 表示不支持超时，直到匹配到为止 * @param nanos 超时限制 单位：纳秒 * @return E 如果当前请求是一个request类型的请求 * 返回值！=null 表示匹配成功 * 返回值==null 表示超时或者被中断 * 如果当前请求是data类型的请求， * 返回值！=null 表示匹配成功，返回当前线程put的数据 * 返回值 ==null 表示data类型的请求超时 或者被中断 */ abstract E transfer(E e, boolean timed, long nanos);&#125;/** * 表示获取当前系统所拥有的的cpu核心数 */static final int NCPUS = Runtime.getRuntime().availableProcessors();/** * 表示指定了超时时间的话，最大的自旋次数 * 为什么需要自旋操作？因为线程挂起唤醒站在cpu角度，是比较耗费资源的， * 涉及到用户态和内核太的切换浪费性能，自旋期间线程会一直检查自己的 * 状态是否被匹配到，如果自旋期间被匹配到，直接返回，如果 * 未被匹配到，达到某一指标后，还是会挂起。 */static final int maxTimedSpins = (NCPUS &lt; 2) ? 0 : 32;/** * 表示没有指定超时限制的时候，线程等待匹配时，自旋的次数。 */static final int maxUntimedSpins = maxTimedSpins * 16;/** * 如果请求是指定超时限制的话，如果超时参数小于1000纳秒的时候，禁止挂起 */static final long spinForTimeoutThreshold = 1000L; 2.非公平模式TODO","categories":[{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"SimpleDateFormat在多线程下存在并发安全问题","slug":"JUC/SimpleDateFormat类的线程安全问题","date":"2022-01-11T11:05:01.751Z","updated":"2022-01-11T11:32:38.766Z","comments":true,"path":"2022/01/11/JUC/SimpleDateFormat类的线程安全问题/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/JUC/SimpleDateFormat%E7%B1%BB%E7%9A%84%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E9%97%AE%E9%A2%98/","excerpt":"","text":"SimpleDateFormat在多线程下存在并发安全问题。​ 1.重现SimpleDateFormat类的线程安全问题12345678910111213141516171819202122232425262728293031323334353637383940414243/** * * 重现SimpleDateFormat类的线程安全问题 * * @author 二十 * @since 2021/9/12 10:05 下午 */public class SdfTest &#123; /**执行总次数*/ private static final int EXECUTE_COUNT=1000; /**并发线程数*/ private static final int THREAD_COUNT=100; private static SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;); @Test public void test()throws Exception&#123; final Semaphore semaphore =new Semaphore(THREAD_COUNT); final CountDownLatch cdl = new CountDownLatch(EXECUTE_COUNT); ExecutorService executor = Executors.newCachedThreadPool(); for (int i = 0; i &lt; EXECUTE_COUNT; i++) &#123; executor.execute(()-&gt;&#123; try &#123; semaphore.acquire(); sdf.parse(&quot;2021-09-15&quot;); &#125;catch (Exception e)&#123; System.out.println(Thread.currentThread().getName()+&quot;格式化时间失败！&quot;); &#125;finally &#123; semaphore.release(); cdl.countDown(); &#125; &#125;); &#125; cdl.await(); executor.shutdown(); &#125;&#125; 说明：在高并发下使用SimpleDateFormat类格式化日期的时候抛出了异常，SimpleDateFormat类不是线程安全的！为什么SimpleDateFormat不是线程安全的？ 2.SimpleDateFormat为什么不是线程安全的？12345678910public Date parse(String source) throws ParseException &#123; ParsePosition pos = new ParsePosition(0); //调用了parse方法，最终的实现类在SimpleDateFormat类中 Date result = parse(source, pos); if (pos.index == 0) throw new ParseException(&quot;Unparseable date: \\&quot;&quot; + source + &quot;\\&quot;&quot; , pos.errorIndex); return result;&#125; 点进去parse()，这是一个抽象的方法。​ 1public abstract Date parse(String source, ParsePosition pos); 最终的实现类还是在SimpleDateFormat类中。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105public Date parse(String text, ParsePosition pos)&#123; checkNegativeNumberExpression(); int start = pos.index; int oldStart = start; int textLength = text.length(); boolean[] ambiguousYear = &#123;false&#125;; CalendarBuilder calb = new CalendarBuilder(); for (int i = 0; i &lt; compiledPattern.length; ) &#123; int tag = compiledPattern[i] &gt;&gt;&gt; 8; int count = compiledPattern[i++] &amp; 0xff; if (count == 255) &#123; count = compiledPattern[i++] &lt;&lt; 16; count |= compiledPattern[i++]; &#125; switch (tag) &#123; case TAG_QUOTE_ASCII_CHAR: if (start &gt;= textLength || text.charAt(start) != (char)count) &#123; //破坏了线程的安全性 pos.index = oldStart; //破坏了线程的安全性 pos.errorIndex = start; return null; &#125; start++; break; case TAG_QUOTE_CHARS: while (count-- &gt; 0) &#123; if (start &gt;= textLength || text.charAt(start) != compiledPattern[i++]) &#123; pos.index = oldStart;//破坏了线程的安全性 pos.errorIndex = start;//破坏了线程的安全性 return null; &#125; start++; &#125; break; default: boolean obeyCount = false; boolean useFollowingMinusSignAsDelimiter = false; if (i &lt; compiledPattern.length) &#123; int nextTag = compiledPattern[i] &gt;&gt;&gt; 8; if (!(nextTag == TAG_QUOTE_ASCII_CHAR || nextTag == TAG_QUOTE_CHARS)) &#123; obeyCount = true; &#125; if (hasFollowingMinusSign &amp;&amp; (nextTag == TAG_QUOTE_ASCII_CHAR || nextTag == TAG_QUOTE_CHARS)) &#123; int c; if (nextTag == TAG_QUOTE_ASCII_CHAR) &#123; c = compiledPattern[i] &amp; 0xff; &#125; else &#123; c = compiledPattern[i+1]; &#125; if (c == minusSign) &#123; useFollowingMinusSignAsDelimiter = true; &#125; &#125; &#125; start = subParse(text, start, tag, count, obeyCount, ambiguousYear, pos, useFollowingMinusSignAsDelimiter, calb); if (start &lt; 0) &#123; //破坏了线程的安全性 pos.index = oldStart; return null; &#125; &#125; &#125; //破坏了线程的安全性 pos.index = start; Date parsedDate; try &#123; parsedDate = calb.establish(calendar).getTime(); if (ambiguousYear[0]) &#123; if (parsedDate.before(defaultCenturyStart)) &#123; parsedDate = calb.addYear(100).establish(calendar).getTime(); &#125; &#125; &#125; catch (IllegalArgumentException e) &#123; //破坏了线程的安全性 pos.errorIndex = start; //破坏了线程的安全性 pos.index = oldStart; return null; &#125; return parsedDate;&#125; 通过对SimpleDateFormat类中的parse()进行分析可以得知：parse()中存在几处为ParsePosition类中的索引赋值的操作。​ 一旦将SimpleDateFormat类定义成全局静态变量，那么SimpleDateFormat类在多线程之间是共享的，这就会导致ParsePosition类在多线程之间共享。​ 在高并发场景下，一个线程对ParsePosition类中的索引进行修改，一定会影响到其他线程对ParsePosition类中索引的读。这就造成了线程的安全问题。​ 那么在确定了SimpleDateFormat线程不安全的原因以后，如何来解决这个问题。​ 3.SimpleDateFormat类线程安全的解决3.1 局部变量法12345678910111213141516171819202122232425262728293031323334public class SdfTest &#123; /**执行总次数*/ private static final int EXECUTE_COUNT=1000; /**并发线程数*/ private static final int THREAD_COUNT=100; @Test public void test()throws Exception&#123; SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;); final Semaphore semaphore =new Semaphore(THREAD_COUNT); final CountDownLatch cdl = new CountDownLatch(EXECUTE_COUNT); ExecutorService executor = Executors.newCachedThreadPool(); for (int i = 0; i &lt; EXECUTE_COUNT; i++) &#123; executor.execute(()-&gt;&#123; try &#123; semaphore.acquire(); sdf.parse(&quot;2021-09-15&quot;); &#125;catch (Exception e)&#123; System.out.println(Thread.currentThread().getName()+&quot;格式化时间失败！&quot;); &#125;finally &#123; semaphore.release(); cdl.countDown(); &#125; &#125;); &#125; cdl.await(); executor.shutdown(); &#125;&#125; 这种方式在高并发场景下会创建大量的SimpleDateFormat对象，影响程序的性能，所以，这种方式在实际生产环境不太推荐。​ 3.2 synchronized1234567891011121314151617181920212223242526272829303132333435public class SdfTest &#123; /**执行总次数*/ private static final int EXECUTE_COUNT=1000; /**并发线程数*/ private static final int THREAD_COUNT=100; private static SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;);; @Test public void test()throws Exception&#123; final Semaphore semaphore =new Semaphore(THREAD_COUNT); final CountDownLatch cdl = new CountDownLatch(EXECUTE_COUNT); ExecutorService executor = Executors.newCachedThreadPool(); for (int i = 0; i &lt; EXECUTE_COUNT; i++) &#123; executor.execute(()-&gt;&#123; try &#123; semaphore.acquire(); synchronized (sdf)&#123; sdf.parse(&quot;2021-09-15&quot;); &#125; &#125;catch (Exception e)&#123; System.out.println(Thread.currentThread().getName()+&quot;格式化时间失败！&quot;); &#125;finally &#123; semaphore.release(); cdl.countDown(); &#125; &#125;); &#125; cdl.await(); executor.shutdown(); &#125;&#125; 这种方式解决了他的线程安全问题，但是程序在执行的过程中，为SimpleDateFormat类对象加了synchronized锁，导致在同一时刻只能由一个线程执行格式化时间的方法。此时会影响程序的性能，在要求高并发的生产环境下，此种方式也是不太推荐使用的。​ 3.3 Lock锁方式1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class SdfTest &#123; /** * 执行总次数 */ private static final int EXECUTE_COUNT = 1000; /** * 并发线程数 */ private static final int THREAD_COUNT = 100; private static SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;); private static Lock lock = new ReentrantLock(); @Test public void test() throws Exception &#123; final Semaphore semaphore = new Semaphore(THREAD_COUNT); final CountDownLatch cdl = new CountDownLatch(EXECUTE_COUNT); ExecutorService executor = Executors.newCachedThreadPool(); for (int i = 0; i &lt; EXECUTE_COUNT; i++) &#123; executor.execute(() -&gt; &#123; try &#123; semaphore.acquire(); lock.lock(); try &#123; sdf.parse(&quot;2021-09-15&quot;); &#125; finally &#123; lock.unlock(); &#125; &#125; catch (Exception e) &#123; System.out.println(Thread.currentThread().getName() + &quot;格式化时间失败！&quot;); &#125; finally &#123; semaphore.release(); cdl.countDown(); &#125; &#125;); &#125; cdl.await(); executor.shutdown(); &#125;&#125; 通过代码得知：首先定义了Lock类型的全局静态变量作为加锁和释放锁的句柄。然后再SimpleDateFormat.parse()代码执行之前加锁。​ 这里需要注意的一点是：为了防止程序抛出异常而导致锁不能被释放，一定要将释放锁的操作放到finally代码块。​ 此种方式在并发下同样影响性能，不太推荐在高并发生产中使用。​ 3.4ThreadLocal方式123456789101112131415161718192021222324252627282930313233343536373839public class SdfTest &#123; /** * 执行总次数 */ private static final int EXECUTE_COUNT = 1000; /** * 并发线程数 */ private static final int THREAD_COUNT = 100; private static ThreadLocal&lt;DateFormat&gt; threadLocal = ThreadLocal.withInitial(() -&gt; new SimpleDateFormat(&quot;yyyy-MM-dd&quot;)); @Test public void test() throws Exception &#123; final Semaphore semaphore = new Semaphore(THREAD_COUNT); final CountDownLatch cdl = new CountDownLatch(EXECUTE_COUNT); ExecutorService executor = Executors.newCachedThreadPool(); for (int i = 0; i &lt; EXECUTE_COUNT; i++) &#123; executor.execute(() -&gt; &#123; try &#123; semaphore.acquire(); threadLocal.get().parse(&quot;2021-09-15&quot;); &#125; catch (Exception e) &#123; System.out.println(Thread.currentThread().getName() + &quot;格式化时间失败！&quot;); &#125; finally &#123; threadLocal.remove(); semaphore.release(); cdl.countDown(); &#125; &#125;); &#125; cdl.await(); executor.shutdown(); &#125;&#125; 使用ThreadLocal将每个线程使用的SimpleDateFormat副本保存在ThreadLocal中，各个线程在使用时互不干扰，从而解决了线程安全的问题。​ 此种方式运行效率比较高，推荐在高并发场景下使用。​ 3.5 DateTimeFormatterjdk8线程安全的时间日期API。​ ​","categories":[{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"ScheduledThreadPoolExecutor与Timer的区别","slug":"JUC/ScheduledThreadPoolExecutor与Timer的区别","date":"2022-01-11T11:04:52.884Z","updated":"2022-01-11T11:33:09.480Z","comments":true,"path":"2022/01/11/JUC/ScheduledThreadPoolExecutor与Timer的区别/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/JUC/ScheduledThreadPoolExecutor%E4%B8%8ETimer%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"JDK1.5开始提供ScheduledThreadPoolExecutor类，ScheduledThreadPoolExecutor类继承ThreadPoolExecutor类重用线程池实现了任务的周期性调度功能。在IDK1.5之前，实现任务的周期性调度主要使用的是Timer类和TimerTask类。​ 1.1线程角度​ Timer是单线程模式，如果某个TimerTask任务的执行时间比较久，会影响到其他任务的调度执行。 ScheduledThreadPoolExecutor是多线程模式，并且重用线程池，某个ScheduledFutureTask任务执行的时间比较久，不会影响到其他任务的调度执行。 ​ 1.2系统时间敏感度 Timer调度是基于操作系统的绝对时间的，对操作系统的时间敏感，一旦操作系统的时间改变，则Timer的调度不再精确。 ScheduledThreadPoolExecutor调度是基于相对时间的，不受操作系统时间改变的影响。 ​ 1.3是否捕获异常​ Timer不会捕获TimerTask抛出的异常，加上Timer又是单线程的。一旦某个调度任务出现异常则整个线程就会终止，其他需要调度的任务也不再执行。 ScheduledThreadPoolExecutor基于线程池来实现调度功能，某个任务抛出异常后，其他任务仍能正常执行。 ​ 1.4任务是否具备优先级​ Timer中执行的TimerTask任务整体上没有优先级的概念，只是按照系统的绝对时间来执行任务。 ScheduledThreadPoolExecutor中执行的ScheduledFutureTask类实现了iavalang.Comparable接口和java.utilconcurrentDelayed接口，这也就说明了ScheduledFutureTask类中实现了两个非常重要的方法，一个是javalangComparable接口的compareTo方法，一个是java.util.concurrentDelayed接口的getDelay方法。在ScheduledFutureTask类中compareTo方法方法实现了任务的比较，距离下次执行的时间间隔短的任务会排在前面，也就是说，距离下次执行的时间间隔短的任务的优先级比较高。而getDelay方法则能够返回距离下次任务执行的时间间隔。 ​ 1.5是否支持对任务排序 Timer不支持对任务的排序。 ScheduledThreadPoolExecutor类中定义了一个静态内部类DelayedWorkQueue,DelayedWorkQueue类本质上是一个有序队列，为需要调度的每个任务按照距离下次执行时间间隔的大小来排序 ​ 1.6能否获取返回的结果​ Timer中执行的TimerTask类只是实现了iavaangRunnable接口，无法从TimerTask中获取返回的结果。 ScheduledThreadPoolExecutor中执行的ScheduledFutureTask类继承了FutureTask类，能够通过遍Future来获取返回的结果。 ​ 通过以上对ScheduledThreadPoolExecutor类和Timer类的对比，相信在JDK1.5之后，就没有使用Timer来实现定时任务调度的必要了。​ ​ ​ ​","categories":[{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"线程间传递上下文信息","slug":"JUC/线程间传递上下文信息","date":"2022-01-11T11:04:43.563Z","updated":"2022-01-11T11:24:01.784Z","comments":true,"path":"2022/01/11/JUC/线程间传递上下文信息/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/JUC/%E7%BA%BF%E7%A8%8B%E9%97%B4%E4%BC%A0%E9%80%92%E4%B8%8A%E4%B8%8B%E6%96%87%E4%BF%A1%E6%81%AF/","excerpt":"","text":"1.复制父线程变量到子线程只有父线程里面创建一个线程的时候才会去调用init才会有inheritableThreadLocals的拷贝动作。​ 1234567891011121314151617181920212223242526272829303132333435public class InheriableThreadLocalTest &#123; private static ThreadLocal&lt;Integer&gt; context = new InheritableThreadLocal&lt;&gt;(); private static ExecutorService pool = Executors.newFixedThreadPool(2); static class MainThread extends Thread &#123; private int index; public MainThread(int index) &#123; this.index = index; &#125; @Override public void run() &#123; context.set(index); //用线程池的线程就会出现传递信息错乱 //pool.execute(() -&gt; System.out.println(Thread.currentThread().getName()+&quot;:&quot; + context.get())); new Thread(()-&gt;System.out.println(Thread.currentThread().getName()+&quot;:&quot; + context.get())).start(); &#125; &#125; /** * 当使用线程池来运行我们的子线程的任务的时候， * 采用InheriableThreadLocal是无法解决变量传递的问题的 */ public static void main(String[] args) &#123; for (int i = 0; i &lt; 10; i++) &#123; new MainThread(i).start(); &#125; pool.shutdown(); &#125;&#125; ​ 注意：要求必须是new的线程，不能使用线程池的线程，线程复用会导致信息传递出错。 2.解决无法使用线程池的问题12345678910111213141516171819202122232425262728293031/** * @author 二十 * @since 2021/9/14 11:15 上午 */public class TransmittableTest &#123; private static TransmittableThreadLocal&lt;String&gt; context = new TransmittableThreadLocal&lt;&gt;(); private static ExecutorService pool = TtlExecutors.getTtlExecutorService(Executors.newFixedThreadPool(5)); static class MainThread extends Thread &#123; private int index; public MainThread(int index) &#123; this.index = index; &#125; @Override public void run() &#123; context.set(String.valueOf(index)); pool.execute(() -&gt; System.out.println(Thread.currentThread().getName()+&quot;:&quot; + context.get())); // new Thread(()-&gt;System.out.println(Thread.currentThread().getName()+&quot;:&quot; + context.get())).start(); &#125; &#125; public static void main(String[] args) &#123; for (int i = 0; i &lt; 10; i++) &#123; new MainThread(i).start(); &#125; pool.shutdown(); &#125;&#125; 3.源码TransmittableThreadLocal继承了InheritableThreadLocal，先点一下InheritableThreadLocal瞅瞅。​ 1234567891011121314151617181920public class InheritableThreadLocal&lt;T&gt; extends ThreadLocal&lt;T&gt; &#123; /** * 新建线程时，如果当前inheritableThreadLocals非空，则会获取当前inheritableThreadLocals传递给新线程 */ protected T childValue(T parentValue) &#123; return parentValue; &#125; /** * InheritableThreadLocal变量的set/get/remove操作都是在inheritableThreadLocals上 */ ThreadLocalMap getMap(Thread t) &#123; return t.inheritableThreadLocals; &#125; /** * 创建inheritableThreadLocals */ void createMap(Thread t, T firstValue) &#123; t.inheritableThreadLocals = new ThreadLocalMap(this, firstValue); &#125;&#125; 对TL进行了一层包装和增强。​ Thread类中有两个ThreadLocal相关的ThreadLocalMap属性，如下： 12ThreadLocal.ThreadLocalMap threadLocals：ThreadLocal变量使用ThreadLocal.ThreadLocalMap inheritableThreadLocals：InheritableThreadLocal变量使用 新建线程时，将当前线程的inheritableThreadLocals传递给新线程，这里的传递是对InheritableThreadLocal变量的数据做浅拷贝（引用复制），这样新线程可以使用同一个InheritableThreadLocal变量查看上一个线程的数据。​ 下面以TtlRunnable.get()为起点分析TTL的设计实现，TtlRunnable.get源码如下（TtlRunnable.get流程对应的初始化时capture操作，保存快照。TtlCallable和TtlRunnable流程类似）：​ 12345678910111213141516171819202122232425public static TtlRunnable get(@Nullable Runnable runnable) &#123; return get(runnable, false, false);&#125;public static TtlRunnable get(@Nullable Runnable runnable, boolean releaseTtlValueReferenceAfterRun, boolean idempotent) &#123; if (runnable instanceof TtlEnhanced) &#123; // 幂等时直接返回，否则执行会产生问题，直接抛异常 if (idempotent) return (TtlRunnable) runnable; else throw new IllegalStateException(&quot;Already TtlRunnable!&quot;); &#125; return new TtlRunnable(runnable, releaseTtlValueReferenceAfterRun);&#125;private TtlRunnable(@Nonnull Runnable runnable, boolean releaseTtlValueReferenceAfterRun) &#123; this.capturedRef = new AtomicReference&lt;Object&gt;(capture()); this.runnable = runnable; this.releaseTtlValueReferenceAfterRun = releaseTtlValueReferenceAfterRun;&#125;public static Object capture() &#123; Map&lt;TransmittableThreadLocal&lt;?&gt;, Object&gt; captured = new HashMap&lt;TransmittableThreadLocal&lt;?&gt;, Object&gt;(); // 从holder获取所有threadLocal，存到captured，这里相当于对当前线程holder做一个快照保存 // 到TtlRunnable实例属性中，在执行TtlRunnable时进行回放 for (TransmittableThreadLocal&lt;?&gt; threadLocal : holder.get().keySet()) &#123; captured.put(threadLocal, threadLocal.copyValue()); &#125; return captured;&#125; 在新建TtlRunnable过程中，会保存下TransmittableThreadLocal.holder到captured，记录到TtlRunnable实例中的capturedRef字段，TransmittableThreadLocal.holder类型是： 123456789101112131415// Note about holder:// 1. The value of holder is type Map&lt;TransmittableThreadLocal&lt;?&gt;, ?&gt; (WeakHashMap implementation),// but it is used as *set*. 因为没有WeakSet的原因// 2. WeakHashMap support null value.private static InheritableThreadLocal&lt;Map&lt;TransmittableThreadLocal&lt;?&gt;, ?&gt;&gt; holder = new InheritableThreadLocal&lt;Map&lt;TransmittableThreadLocal&lt;?&gt;, ?&gt;&gt;() &#123; @Override protected Map&lt;TransmittableThreadLocal&lt;?&gt;, ?&gt; initialValue() &#123; return new WeakHashMap&lt;TransmittableThreadLocal&lt;?&gt;, Object&gt;(); &#125; @Override protected Map&lt;TransmittableThreadLocal&lt;?&gt;, ?&gt; childValue(Map&lt;TransmittableThreadLocal&lt;?&gt;, ?&gt; parentValue) &#123; return new WeakHashMap&lt;TransmittableThreadLocal&lt;?&gt;, Object&gt;(parentValue); &#125; &#125;; 从上面代码我们知道初始化TtlRunnable时已经将TransmittableThreadLocal保存下来了，那么什么时候应用到当前线程ThreadLocal中呢，这是就需要看下TtlRunnable.run方法： 123456789101112131415public void run() &#123; Object captured = capturedRef.get(); // captured不应该为空，releaseTtlValueReferenceAfterRun为true时设置capturedRef为null，防止当前Runnable重复执行 if (captured == null || releaseTtlValueReferenceAfterRun &amp;&amp; !capturedRef.compareAndSet(captured, null)) &#123; throw new IllegalStateException(&quot;TTL value reference is released after run!&quot;); &#125; // captured进行回放，应用到当前线程中 Object backup = replay(captured); try &#123; runnable.run(); &#125; finally &#123; restore(backup); &#125;&#125; 注意，TTL中的replay操作是以captured为当前inheritableThreadLocals的（处理逻辑是在TtlRunable run时，会以TtlRunnable.get时间点获取的captured（类似TTL快照）为准，holder中不在captured的先移除，在的会被替换）。回放captured和执行完runnable.run之后，再restore恢复到原来inheritableThreadLocals的状态。 4.自己写一个轻量级的123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168/** * @author 二十 * @since 2021/9/14 3:13 下午 */public class EsThreadLocal&lt;T&gt; extends InheritableThreadLocal&lt;T&gt; &#123; private static InheritableThreadLocal&lt;WeakHashMap&lt;EsThreadLocal&lt;Object&gt;, ?&gt;&gt; holder = new InheritableThreadLocal&lt;WeakHashMap&lt;EsThreadLocal&lt;Object&gt;, ?&gt;&gt;() &#123; @Override protected WeakHashMap&lt;EsThreadLocal&lt;Object&gt;, ?&gt; childValue(WeakHashMap&lt;EsThreadLocal&lt;Object&gt;, ?&gt; parentValue) &#123; return new WeakHashMap&lt;&gt;(parentValue); &#125; @Override protected WeakHashMap&lt;EsThreadLocal&lt;Object&gt;, ?&gt; initialValue() &#123; return new WeakHashMap&lt;&gt;(); &#125; &#125;; @Override public int hashCode() &#123; return super.hashCode(); &#125; @Override public boolean equals(Object obj) &#123; return super.equals(obj); &#125; @Override public T get() &#123; T value = super.get(); if (null != value) addToHolder(); return value; &#125; @Override public void set(T value) &#123; if (null == value) &#123; removeFromHolder(); super.remove(); &#125; else &#123; super.set(value); addToHolder(); &#125; &#125; private void removeFromHolder() &#123; holder.get().remove(this); &#125; private void addToHolder() &#123; if (!holder.get().containsKey(this)) holder.get().put((EsThreadLocal&lt;Object&gt;) this, null); &#125; static class SnapShot &#123; final WeakHashMap&lt;EsThreadLocal&lt;Object&gt;, Object&gt; ctlValue; private SnapShot(WeakHashMap&lt;EsThreadLocal&lt;Object&gt;, Object&gt; ctlValue) &#123; this.ctlValue = ctlValue; &#125; &#125; static class Transmitter &#123; public static SnapShot capture() &#123; return new SnapShot(captureCtlValues()); &#125; private static WeakHashMap&lt;EsThreadLocal&lt;Object&gt;, Object&gt; captureCtlValues() &#123; return holder.get().keySet().stream().collect(Collectors.toMap(ctlItem -&gt; ctlItem, EsThreadLocal::get, (a, b) -&gt; b, WeakHashMap::new)); &#125; public static SnapShot replay(SnapShot snapShot) &#123; WeakHashMap&lt;EsThreadLocal&lt;Object&gt;, Object&gt; capture = snapShot.ctlValue; WeakHashMap&lt;EsThreadLocal&lt;Object&gt;, Object&gt; backValue = new WeakHashMap&lt;&gt;(); /* * 从holder中获取当前线程持有的threadLocal的Map,进行迭代保存 */ Iterator&lt;EsThreadLocal&lt;Object&gt;&gt; iterator = holder.get().keySet().iterator(); while (iterator.hasNext()) &#123; EsThreadLocal&lt;Object&gt; threadLocal = iterator.next(); backValue.put(threadLocal, threadLocal.get()); if (!capture.containsKey(threadLocal)) &#123; iterator.remove(); threadLocal.remove(); &#125; &#125; /* 设置上capture */ setThreadLocal(capture); return new SnapShot(backValue); &#125; public static void setThreadLocal(WeakHashMap&lt;EsThreadLocal&lt;Object&gt;, Object&gt; ctlValues) &#123; ctlValues.forEach(EsThreadLocal::set); &#125; public static void restore(EsThreadLocal.SnapShot backUp) &#123; Iterator&lt;EsThreadLocal&lt;Object&gt;&gt; iterator = holder.get().keySet().iterator(); while (iterator.hasNext()) &#123; EsThreadLocal&lt;Object&gt; threadLocal = iterator.next(); if (!backUp.ctlValue.containsKey(threadLocal)) &#123; iterator.remove(); threadLocal.remove(); &#125; &#125; setThreadLocal(backUp.ctlValue); &#125; &#125; static class EsRunnable implements Runnable &#123; private AtomicReference&lt;SnapShot&gt; captureRef; private Runnable runnable; public EsRunnable(Runnable runnable) &#123; this.runnable = runnable; captureRef = new AtomicReference&lt;&gt;(Transmitter.capture()); &#125; @Override public void run() &#123; SnapShot capture = captureRef.get(); SnapShot backUp = Transmitter.replay(capture); try &#123; runnable.run(); &#125; finally &#123; Transmitter.restore(backUp); &#125; &#125; public static EsRunnable getRunnable(Runnable runnable) &#123; return new EsRunnable(runnable); &#125; &#125;&#125;class Test &#123; private static ThreadLocal&lt;String&gt; context = new EsThreadLocal&lt;&gt;(); private static ExecutorService pool = Executors.newFixedThreadPool(5); public static void main(String[] args)throws Exception &#123; for (int i = 1; i &lt;=10; i++) &#123; context.set(String.valueOf(i)); pool.execute(new EsThreadLocal.EsRunnable(()-&gt;System.out.println(Thread.currentThread().getName()+&quot; : &quot; + context.get() ))); TimeUnit.SECONDS.sleep(1); &#125; pool.shutdown(); &#125;&#125; 运行结果：","categories":[{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"高并发设计模式","slug":"JUC/高并发设计模式","date":"2022-01-11T11:04:33.684Z","updated":"2022-01-11T11:23:00.096Z","comments":true,"path":"2022/01/11/JUC/高并发设计模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/JUC/%E9%AB%98%E5%B9%B6%E5%8F%91%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"并发场景下，常见的设计模式可能存在线程安全问题，比如单例模式就是一个典型。另外，为了充分发挥多核的优势，高并发程序通常会将大的任务分割成一些规模比较小的任务，分而治之，这就出现了高并发下特有的一些设计模式，比如ForkJoin模式等等。 一，线程安全的单例模式1.双重检查锁12345678910111213141516171819202122232425262728public class SingletonTestA &#123; private static volatile SingletonTestA instance; private SingletonTestA() &#123; &#125; public static SingletonTestA getInstance() &#123; /**判断对象是否已经初始化，如果已经初始化，直接返回。如果尚未初始化，加锁*/ if (instance == null) synchronized (SingletonTestA.class) &#123; /**再次判断是否已经初始化，通过第一层判断的线程可能有很多，但是能够获得锁的线程只有一个。*/ if (instance == null) /* 分配一块内存M 在内存M上初始化Singleton对象 M的地址赋值给instance变量 指令重排后可能会出现问题 */ instance = new SingletonTestA(); &#125; return instance; &#125;&#125; 2.静态内部类1234567891011121314151617public class SingletonTestA &#123; private static class LazyHolder&#123; private static final SingletonTestA instance = new SingletonTestA(); &#125; private SingletonTestA() &#123; &#125; /** * 只有在方法被调用的时候，才会去加载内部类并且初始化单例。 * @return */ public static SingletonTestA getInstance() &#123; return LazyHolder.instance; &#125;&#125; 二，Master-Worder模式Master-Worker模式是一种常见的高并发模式，它的核心思想是任务的调度和执行分离，调度任务的角包五为Master，执行任务的角色为 Worker，Master负责接收、分配任务务和合并(Merge)任务结果， Worker负责执行任务。Master-Work er模式是一种归并类型的模式。 举一个例子，在TCP服务端的请青求处理过程中，大量的客户端连接相当于大量的任务，Master需要将这些任务存储在一个任务队列中，然后分发给各个Worker，每个Worker是一个工作线程，负责完成连接的传输处理。 假设一个场景，需要执行N个任务，将这些任务的结果进行累加求和，如果任务太多，就可以采用Master-Worker模式来实现。Master持有workerCount个Worker，并且负责接收任务，然后分发给Worker，最后在回调函数中对Worker的结果进行归并求和。 1.代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183/** * @author 二十 * @since 2021/9/14 9:52 下午 */public class MasterWorkerTest &#123; static class SimpleTask extends Task&lt;Integer&gt; &#123; @Override protected Integer doExecute() &#123; System.out.println(&quot;task &quot; + getId() + &quot; is done !&quot;); return getId(); &#125; &#125; public static void main(String[] args) &#123; //创建master，包含4个worker，并启动master的执行线程 Master&lt;SimpleTask, Integer&gt; master = new Master&lt;&gt;(4); //定期向master提交任务 ScheduledExecutorService executorService = Executors.newScheduledThreadPool(10); executorService.scheduleAtFixedRate(() -&gt; master.submit(new SimpleTask()), 2L, 2L, TimeUnit.SECONDS); //定期从master提取结果 executorService.scheduleAtFixedRate(() -&gt; master.printResult(), 5, 2, TimeUnit.SECONDS); &#125;&#125;/** * 异步任务类在执行子类任务的doExecute()之后， * 回调一下Master传递过来的回调函数， * 将执行完成后的任务进行回填。 * * @param &lt;R&gt; */@Dataclass Task&lt;R&gt; &#123; static AtomicInteger index = new AtomicInteger(1); //任务的回调函数 public Consumer&lt;Task&lt;R&gt;&gt; resultAction; //任务的id private int id; //worker id private int workerId; //计算结果 R result = null; public Task() &#123; this.id = index.getAndIncrement(); &#125; public void execute() &#123; this.result = this.doExecute(); resultAction.accept(this); &#125; //钩子方法，交给子类实现 protected R doExecute() &#123; return null; &#125;&#125;/** * Master 负责接收客户端提交额任务，然后通过阻塞队列对任务进行缓存。 * Master所拥有的线程作为阻塞队列的消费者，不断从阻塞队列获取任务并轮流分给Worker。 * * @param &lt;T&gt; * @param &lt;R&gt; */class Master&lt;T extends Task, R&gt; &#123; //worker集合 private Map&lt;String, Worker&lt;T, R&gt;&gt; workers = new HashMap&lt;&gt;(); //任务集合 protected LinkedBlockingQueue&lt;T&gt; taskQueue = new LinkedBlockingQueue&lt;&gt;(); //任务处理结果集合 protected Map&lt;String, R&gt; resultMap = new ConcurrentHashMap&lt;&gt;(); //Master的任务调度线程 private Thread thread = null; //保持最终的和 private AtomicLong sum = new AtomicLong(0); public Master(int workerCount) &#123; //每一个worker对象都需要持有队列的引用，用于领取任务和提交结果 for (int i = 0; i &lt; workerCount; i++) &#123; Worker&lt;T, R&gt; worker = new Worker&lt;&gt;(); workers.put(&quot;子节点：&quot; + i, worker); &#125; thread = new Thread(() -&gt; this.execute()); thread.start(); &#125; //提交任务 public void submit(T task) &#123; taskQueue.add(task); &#125; //获取worker结果处理的回调函数 private void resultCallback(Object o) &#123; Task&lt;R&gt; task = (Task&lt;R&gt;) o; String taskName = &quot;Worker:&quot; + task.getWorkerId() + &quot;-&quot; + &quot;Task:&quot; + task.getId(); R result = task.getResult(); resultMap.put(taskName, result); sum.getAndAdd((Integer) result); &#125; //启动所有的子任务 public void execute() &#123; for (; ; ) &#123; for (Map.Entry&lt;String, Worker&lt;T, R&gt;&gt; entry : workers.entrySet()) &#123; T task = null; try &#123; //获取任务 task = this.taskQueue.take(); //获取节点 Worker worker = entry.getValue(); //分配任务 worker.submit(task, this::resultCallback); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; //获取最终结果 public void printResult() &#123; System.out.println(&quot;sum is : &quot; + sum.get()); for (Map.Entry&lt;String, R&gt; entry : resultMap.entrySet()) System.out.println(entry.getKey() + &quot; : &quot; + entry.getValue()); &#125;&#125;/** * Worker接收Master分配的任务，同样也通过阻塞队列对局部任务进行缓存。 * Worker所拥有的的线程作为拒不任务的阻塞队列的消费者， * 不断从阻塞队列获取任务并执行，执行完成后回调Master传递过来的回调函数。 * * @param &lt;T&gt; * @param &lt;R&gt; */class Worker&lt;T extends Task, R&gt; &#123; //接受任务的阻塞队列 private LinkedBlockingQueue&lt;T&gt; taskQueue = new LinkedBlockingQueue&lt;&gt;(); //Worker的编号 private static AtomicInteger index = new AtomicInteger(1); private int workerId; //执行任务的线程 private Thread thread = null; public Worker() &#123; this.workerId = index.getAndIncrement(); thread = new Thread(() -&gt; this.run()); thread.start(); &#125; //轮训执行任务 public void run() &#123; //轮训启动任务 for (; ; ) &#123; try &#123; //从阻塞队列提取任务 T task = this.taskQueue.take(); task.setWorkerId(workerId); task.execute(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; //接收任务到异步队列 public void submit(T task, Consumer&lt;R&gt; action) &#123; //设置任务的回调方法 task.resultAction = action; try &#123; this.taskQueue.put(task); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 2.Netty中Master-Worker模式的实现Master-Worker模式的核心思想是分而治之，Master角色负责接收和分配任务，Worker角色负责执行任务和结果回填。 实际上，高性能传输模式Reactor模式就是Master-Worker模式在传输领域的一种应用。基于Java的NIO技术，Nettv设计了一套优秀的、高性能Reactor(反应器)模式的具体实现。在Netty中，EventLoop反应器内部有一个线程负责JavaNIO选择器的事件轮询，然后进行对应的事件分发。事件分发的目标就是Netty的Handler处理程序(含用户定义的业务处理程序)。 Netty服务器程序中需要设置两个EventLoopGroup轮询组，一个组负责新连接的监听和接收，另一个组负责IO传输事件的轮询与分发，两个轮询组的职责具体如下: (1)负责新连接的监听和接收的EventLoopGroup轮询组中的反应器完成查询通道的新连接IO事件查询，这些反应器有点像负责招工的包工头，因此该轮询组可以形象地称为“包工头”(Boss)轮询组。 (2)另一个轮询组中的反应器完成查询所有子通道的IO事件，并且执行对应的Handler处理程序完成I0处理，例如数据的输入和输出(有点像搬砖)，这个轮询组可以形象地称为“工人”(Worker)轮询组。 Netty是基于Reactor模式的具体实现，体现了Master-Worker模式的思想。Netty的EventLoop(Reactor角色)可以对应到Master-Worker模式的Worker角色，而Netty的EventLoopGroup轮询组则可以对应到Master-Worker模式的Master角色。 3.Nginx中Master-Worker模式的实现大名鼎鼎的Nginx服务器是Master-Worker模式(更准确地说是Reactor模式)在高性能服务器领域的一种应用。Nginx是一个高性能的 HTTP和反向代理Web服务器，是由伊戈尔·赛索耶夫为俄罗斯访问量第二的Ramblerru站点开发的Web服务器。Nginx源代码以类BSD许可证的形式发布，它的第一个公开版本010发布于2004年10月4日，2011年6月1日发布了1.0.4版本。Nginx因其高稳定性、丰富的功能集、内存消耗少、并发能力强而闻名全球，目前得到非常广泛的使用，比如百度、京东、新浪、网易、腾讯、淘宝等都是它的用户。 Nginx在启动后会以daemon方式在后台运行，它的后台进程有两类:一类称为Master进程(相当于管理进程)，另一类称为Worker进程(工作进程)。Nginx的进程结构图如图。 Nginx的Master进程主要负责调度Worker进程，比如加载配置、启动工作进程、接收来自外界的信号、向各Worker进程发送信号、监控 Worker进程的运行状态等。Master进程负责创建监听套接口，交由Worker进程进行连接监听。Worker进程主要用来处理网络事件，当一个 Worker进程在接收一条连接通道之后，就开始读取请求、解析请求、处理请求，处理完成产生的数据后，再返回给客户端，最后断开连接通道。 Nginx的架构非常直观地体现了Master-Worker模式的思想。Nginx的 Master进程可以对应到Master-Worker模式的Master角色，Nginx的 Worker进程可以对应到Master-Worker模式的Worker角色。 三，ForkJoin模式“分而治之”是一种思想，所谓“分而治之”，就是把一个复杂的算法问题按一定的“分解”方法分为规模较小的若干部分，然后逐个解决，分别找出各部分的解，最后把各部分的解组成整个问题的解。“分而治之”思想在软件体系结构设计、模块化设计、基础算法中得到了非常广泛的应用。许多基础算法都运用了“分而治之”的思想，比如二分查找、快速排序等。 Master-Worker模式是“分而治之”思想的一种应用，ForkJoin模式则是“分而治之”思想的另一种应用。与Master-Worker模式不同，ForkJoin模式没有Master角色，其所有的角色都是Worker,ForkJoin模式中的Worker将大的任务分割成小的任务，一直到任务的规模足够小，可以使用很简单、直接的方式来完成。 1.ForkJoin模式的原理ForkJoin模式先把一个大任务分解成许多个独立的子任务，然后开启多个线程并行去处理这些子任务。有可能子任务还是很大而需要进一步分解，最终得到足够小的任务。ForkJoin模式的任务分解和执行过程如下： ForkJoin模式借助了现代计算机多核的优势并行处理数据。通常情况下，ForkJoin模式将分解出来的子任务放入双端队列中，然后几个启动线程从双端队列中获取任务并执行。子任务执行的结果放到一个队列中，各个线程从队列中获取数据，然后进行局部结果的合并，得到最终结果。 2.ForkJoin框架JUC包提供了一套ForkJoin框架的实现，具体以ForkJoinPool线程池的形式提供，并且该线程池在Java8的Lambda并行流框架中充当着底层框架的角色。JUC包的ForkJoin框架包含如下组件: (1)ForkJoinPool:执行任务的线程池，继承了 AbstractExecutorService类。 (2)ForkJoinWorkerThread:执行任务的工作线程(ForkJoinPool线程池中的线程)。每个线程都维护着一个内部队列，用于存放“内部任务”该类继承了Thread类。 (3)ForkJoinTask:用于ForkJoinPool的任务抽象类，实现了Future接口。 (4)RecursiveTask:带返回结果的递归执行任务，是ForkJoinTask的子类，在子任务带返回结果时使用。 (5)RecursiveAction:不返回结果的递归执行任务，是 ForkJoinTask的子类，在子任务不带返回结果时使用。 因为ForkJoinTask比较复杂，并日其抽象方法比较多，故在日常使用时一般不会直接继承ForkJoinTask来实现自定义的任务类，而是通过继承ForkJoinTask两个子类RecursiveTask或者RecursiveAction之一趋势线自定义任务类，自定义任务类需要实现这些子类的compute(),改方法的执行流程一般如下： 123456if 任务足够小 直接返回结果else 分割成N个子任务 依次调用每个子任务的fork方法执行子任务 依次调用每个子任务的join方法，等待子任务完成，然后合并执行结果 3.ForkJoin框架使用假设需要计算0-100的累加求和，可以使用ForkJoin框架完成。首先需要设计一个可以递归执行的异步任务子类。 3.1 可递归执行的异步任务类AccumulateTask123456789101112131415161718192021222324252627282930313233343536373839404142434445public class AccumulateTask extends RecursiveTask&lt;Integer&gt; &#123; private static final int threshold = 2; private int start; private int end; public AccumulateTask(int start, int end) &#123; this.start = start; this.end = end; &#125; @Override protected Integer compute() &#123; int sum = 0; //判断任务的规模：若规模小可以直接计算 boolean canCompute = (end - start) &lt;= threshold; //若任务已经足够小，则可以直接计算 if (canCompute) &#123; //直接计算并返回结果，Recursive结束 for (int i = start; i &lt;= end; i++) sum += i; System.out.println(&quot;执行任务，计算：&quot; + start + &quot;到&quot; + end + &quot;的和，结果是： &quot; + sum); &#125; else &#123; //任务过大，需要切割，Recursive 递归计算 System.out.println(&quot;切割任务：将&quot; + start + &quot;到&quot; + end + &quot;的和一分为二&quot;); int mid = (start + end) / 2; //切割成2个子任务 AccumulateTask lTask = new AccumulateTask(start, mid); AccumulateTask rTask = new AccumulateTask(mid + 1, end); //依次调用每个子任务的fork方法执行子任务 lTask.fork(); rTask.fork(); //等待子任务完成，依次调用每个子任务的join()合并执行结果 int lResult = lTask.join(); int rResult = rTask.join(); //合并子任务的执行结果 sum = lResult + rResult; &#125; return sum; &#125;&#125; 自定义的异步任务子类AccumulateTask继承自RecursiveTask，每一次执行可以携带返回值。AccumulateTask通过THRESHOLD常量设置子任务分解的阈值，并在它的computeO方法中进行阈值判断，判断的逻辑如下: (1)若当前的计算规模(这里为求和的数字个数)大于THRESHOLD，就当前子任务需要进一步分解，若当前的计算规模没有大于THRESHOLD，则直接计算(这里为求和)。 (2)如果子任务可以直接执行，就进行求和操作，并返回结果。如果任务进行了分解，就需要等待所有的子任务执行完毕、然后对各个分解结果求和。如果一个任务分解为多个子任务(含两个)，就依次调用每个子任务的fork方法执行子任务，然后依次调用每个子任务的join方法合并执行结果。 3.2 使用ForkJoinPool调度AccmulateTask()12345678910@Testpublic void testAccumulateTask()throws Exception&#123; ForkJoinPool forkJoinPool = new ForkJoinPool(); //创建一个累加任务，计算从1 到 10 AccumulateTask countTask = new AccumulateTask(1,100); Future&lt;Integer&gt; future = forkJoinPool.submit(countTask); Integer sum = future.get(1, TimeUnit.SECONDS); System.out.println(&quot;最终计算结果是：&quot;+sum); Assert.assertTrue(sum==5050);&#125; 切割任务：将1到100的和一分为二切割任务：将51到100的和一分为二切割任务：将1到50的和一分为二切割任务：将1到25的和一分为二切割任务：将1到13的和一分为二切割任务：将1到7的和一分为二切割任务：将1到4的和一分为二执行任务，计算：1到2的和，结果是： 3执行任务，计算：3到4的和，结果是： 7执行任务，计算：5到7的和，结果是： 18切割任务：将8到13的和一分为二执行任务，计算：8到10的和，结果是： 27执行任务，计算：11到13的和，结果是： 36切割任务：将26到50的和一分为二切割任务：将26到38的和一分为二切割任务：将26到32的和一分为二切割任务：将26到29的和一分为二执行任务，计算：26到27的和，结果是： 53执行任务，计算：28到29的和，结果是： 57执行任务，计算：30到32的和，结果是： 93切割任务：将33到38的和一分为二执行任务，计算：33到35的和，结果是： 102执行任务，计算：36到38的和，结果是： 111切割任务：将39到50的和一分为二切割任务：将39到44的和一分为二执行任务，计算：39到41的和，结果是： 120执行任务，计算：42到44的和，结果是： 129切割任务：将45到50的和一分为二执行任务，计算：45到47的和，结果是： 138执行任务，计算：48到50的和，结果是： 147切割任务：将76到100的和一分为二切割任务：将76到88的和一分为二切割任务：将76到82的和一分为二切割任务：将76到79的和一分为二执行任务，计算：76到77的和，结果是： 153执行任务，计算：78到79的和，结果是： 157执行任务，计算：80到82的和，结果是： 243切割任务：将83到88的和一分为二执行任务，计算：83到85的和，结果是： 252执行任务，计算：86到88的和，结果是： 261切割任务：将89到100的和一分为二切割任务：将89到94的和一分为二执行任务，计算：89到91的和，结果是： 270执行任务，计算：92到94的和，结果是： 279切割任务：将95到100的和一分为二执行任务，计算：95到97的和，结果是： 288执行任务，计算：98到100的和，结果是： 297切割任务：将51到75的和一分为二切割任务：将51到63的和一分为二切割任务：将51到57的和一分为二切割任务：将51到54的和一分为二执行任务，计算：51到52的和，结果是： 103执行任务，计算：53到54的和，结果是： 107执行任务，计算：55到57的和，结果是： 168切割任务：将58到63的和一分为二执行任务，计算：58到60的和，结果是： 177执行任务，计算：61到63的和，结果是： 186切割任务：将64到75的和一分为二切割任务：将64到69的和一分为二执行任务，计算：64到66的和，结果是： 195执行任务，计算：67到69的和，结果是： 204切割任务：将70到75的和一分为二执行任务，计算：70到72的和，结果是： 213执行任务，计算：73到75的和，结果是： 222切割任务：将14到25的和一分为二切割任务：将14到19的和一分为二执行任务，计算：14到16的和，结果是： 45执行任务，计算：17到19的和，结果是： 54切割任务：将20到25的和一分为二执行任务，计算：20到22的和，结果是： 63执行任务，计算：23到25的和，结果是： 72最终计算结果是：5050 4.ForkJoin框架的核心APIForkJoin框架的核心是ForkJoinPool线程池。该线程池使用一个无锁的栈来管理空闲线程，如果一个工作线程暂时取不到可用的任务，则可能被挂起，而挂起的线程将被压入由ForkJoinPool维护的栈中，等到有新的任务到来的时候，再从栈中唤醒这些线程。 4.1 构造器123456789101112private ForkJoinPool(int parallelism, //并行度，默认为cpu数，最小为1 ForkJoinWorkerThreadFactory factory, //线程创建工厂 UncaughtExceptionHandler handler, //异常处理程序 int mode, String workerNamePrefix) &#123; this.workerNamePrefix = workerNamePrefix; this.factory = factory; this.ueh = handler; this.config = (parallelism &amp; SMASK) | mode; long np = (long)(-parallelism); // offset ctl counts this.ctl = ((np &lt;&lt; AC_SHIFT) &amp; AC_MASK) | ((np &lt;&lt; TC_SHIFT) &amp; TC_MASK);&#125; (1) parallelism:可并行级别 ForkJoin框架将依据parallelism设定的级别决定框架内并行执行的线程数量。并行的每一个任务都会有一个线程进行处理，但parallelism属性并不是ForkJoin框架中最大的线程数量，该属性和ThreadPoolExecutor线程池中的corePoolSize、maximumPoolSize属性有区别，因为 ForkJoinPool的结构和工作方式与ThreadPoolExecutor完全不一样。 ForkJoin框架中可存在的线程数量和parallelism参数值并不是绝对关联的。 (2)factory:线程创建工厂 当ForkJoin框架创建一个新的线程时，同样会用到线程创建工厂。只不过这个线程工厂不再需要实现ThreadFactorv接口，而是需要实现ForkJoinWorkerThreadFactory接口。后者是一个函数式接口，只需要实现一个名叫newThreadO的方法。在ForkJoin框架中有一个默认的ForkJoinWorkerThreadFactory接口实现 DefaultForkJoinWorkerThreadFactory。 (3)handler:异常捕获处理程序 当执行的任务中出现异常，并从任务中被抛出时，就会被handler捕获。 (4)asyncMode:异步模式 asyncMode参数表示任务是否为异步模式，其默认值为false。如果 asyncMode为true，就表示子任务的执行遵循FIFO(先进先出)顺序，并且子任务不能被合并;如果asyncMode为false，就表示子任务的执行遵循FIFO(后进先!)顺序，并日子任务可以被合并。虽然从字面意思来看asyncMode是指异步模式，它并不是指ForkJoin框架的调度模式采用是同步模式还是异步模式工作，仅仅指任务的调度方式。ForkJoin框架中为每一个独立工作的线程准备了对应的待执行任务队列，这个任务队列是使用数组进行组合的双向队列。asyncMode模式的主要意思指的是待执行任务可以使用FIFO(先进先出)的工作模式，也可以使用 FIFO(后进先出)的工作模式，工作模式为FIFO(先进先出)的任务适用于工作线程只负责运行异步事件，不需要合并结果的异步任务。 ForkJoinPool无参数的，默认的构造器如下： 12345static final int MAX_CAP = 0x7fff; // max #workers - 1public ForkJoinPool() &#123; this(Math.min(MAX_CAP, Runtime.getRuntime().availableProcessors()), defaultForkJoinWorkerThreadFactory, null, false);&#125; 该构造器的parallelism值为CPU核心数，factory值为defaultForkJoinWorkerThreadFactory默认的线程工厂，异常捕获处理程序handler值为null；表示不进行异常处理；异步模式asyncMode值为false，使用LIFO的，可以合并子任务的模式。 4.2 common通用池很多场景可以直接使用ForkJoinPool定义的common通用池，调用ForkJoinPool.commonPool()可以获取该ForkJoin线程池，该线程池通过makeCommonPool()来构造。 1234567891011121314151617181920212223242526272829303132333435363738private static ForkJoinPool makeCommonPool() &#123; int parallelism = -1; ForkJoinWorkerThreadFactory factory = null; UncaughtExceptionHandler handler = null; try &#123; //并行度 String pp = System.getProperty (&quot;java.util.concurrent.ForkJoinPool.common.parallelism&quot;); //线程工厂 String fp = System.getProperty (&quot;java.util.concurrent.ForkJoinPool.common.threadFactory&quot;); //异常处理类 String hp = System.getProperty (&quot;java.util.concurrent.ForkJoinPool.common.exceptionHandler&quot;); if (pp != null) parallelism = Integer.parseInt(pp); if (fp != null) factory = ((ForkJoinWorkerThreadFactory)ClassLoader. getSystemClassLoader().loadClass(fp).newInstance()); if (hp != null) handler = ((UncaughtExceptionHandler)ClassLoader. getSystemClassLoader().loadClass(hp).newInstance()); &#125; catch (Exception ignore) &#123; &#125; if (factory == null) &#123; if (System.getSecurityManager() == null) factory = new DefaultCommonPoolForkJoinWorkerThreadFactory(); else // use security-managed default factory = new InnocuousForkJoinWorkerThreadFactory(); &#125; //默认并行度为cores-1 if (parallelism &lt; 0 &amp;&amp; // default 1 less than #cores (parallelism = Runtime.getRuntime().availableProcessors() - 1) &lt;= 0) parallelism = 1; if (parallelism &gt; MAX_CAP) parallelism = MAX_CAP; return new ForkJoinPool(parallelism, factory, handler, LIFO_QUEUE, &quot;ForkJoinPool.commonPool-worker-&quot;);&#125; 使用common池子的优点是可以通过指定系统属性的方式定义”并行度，线程工厂和异常处理类“，并且common池使用的是同步模式，也就是说可以支持任务合并。 通过系统属性的方式指定parallellism的值得示例如下： 1System.setPropert(&quot;java.util.concurrent.ForkJoinPool.common.parallelism&quot;,&quot;8&quot;); 除此之外，还可以通过Java指令的选项的方式指定parallellism值，具体的选项为： 1-Djava.util.concurrent.ForkJoinPool.common.parallelism=8 其他的参数值如异常处理程序handler，都可以通过以上两种方式指定。 4.3 向线程池提交任务的方式可以向ForkJoinPool线程池提交一下两类任务： 外部任务（External/Submissions Task） 向ForkJoinPool提交外部任务有三种方式:方式一是调用invoke()方法，该方法提交任务后线程会等待，等到任务计算完毕返回结果;方式二是调用execute方法提交一个任务来异步执行，无返回结果;方式三是调用submit方法提交一个任务，并且会返回一个ForkJoinTask实例，之后适当的时候可通过ForkJoinTask实例获取执行结果。 子任务（Worker Task）提交 向ForkJoinPool提交子任务的方法相对比较简单，由任务实例的 fork方法完成。当任务被分割之后，内部会调用ForkJoinPool.WorkQueuepush()方法直接把任务放到内部队列中等待被执行。 5.工作窃取算法ForkJoinPool线程池的任务分为“外部任务”和“内部任务”，两种任务的存放位置不同: (1)外部任务存放在ForkJoinPool的全局队列中。 (2)子任务会作为“内部任务”放到内部队列中，ForkJoinPool池中的每个线程都维护着一个内部队列，用于存放这些“内部任务”。 由于ForkJoinPool线程池通常有多个工作线程，与之相对应的就会有多个任务队列，这就会出现任务分配不均衡的问题:有的队列任务多，忙得不停，有的队列没有任务，一直空闲。那么有没有一种机制帮忙将任务从繁忙的线程分摊给空闲的线程呢?答案是使用工作窃取算法。 工作窃取算法的核心思想是:工作线程自己的活干完了之后，会去看看别人有没有没干完的活，如果有就拿过来帮忙干。工作窃取算法的主要逻辑:每个线程拥有一个双端队列(本地队列)，用于存放需要执行的任务，当自己的队列没有任务时，可以从其他线程的任务队列中获得一个任务继续执行。 在实际进行任务窃取操作的时候，操作线程会进行其他线程的任务队列的扫描和任务的出队尝试。为什么说是尝试?因为完全有可能操作失败，主要原因是并行执行肯定涉及线程安全的问题，假如在窃取过程中该任务已经开始执行，那么任务的窃取操作就会失败。 如何尽量避免在任务窃取中发生的线程安全问题呢?一种简单的优化方法是:在线程自己的本地队列采取LIFO(后进先出)策略，窃取其他任务队列的任务时采用FIFO(先进先出)策略。简单来说，获取自己队列的任务时从头开始，窃取其他队列的任务时从尾开始。由于窃取的动作十分快速，会大量降低这种冲突，也是一种优化方式。 6.ForkJoin框架的原理核心原理大致如下： (1)ForkJoin框架的线程池ForkJoinPool的任务分为“外部任务”和“内部任务”。 (2)“外部任务”放在ForkJoinPool的全局队列中。 (3)ForkJoinPool池中的每个线程都维护着一个任务队列，用于存放“内部任务”，线程切割任务得到的子任务会作为“内部任务”放到内部队列中。 (4)当工作线程想要拿到子任务的计算结果时，先判断子任务有没有完成，如果没有完成，再判断子任务有没有被其他线程“窃取”，如果子任务没有被窃取，就由本线程来完成;一旦子任务被窃取了，就去执行本线程“内部队列”的其他任务，或者扫描其他的任务队列并窃取任务。 (5)当工作线程完成其“内部任务”，处于空闲状态时，就会扫描其他的任务队列窃取任务，尽可能不会阻塞等待。 总之，ForkJoin线程在等待一个任务完成时，要么自己来完成这个任务，要么在其他线程窃取了这个任务的情况下，去执行其他任务，是不会阻塞等待的，从而避免资源浪费，除非所有任务队列都为空。 工作窃取算法的优点： (1)线程是不会因为等待某个子任务的执行或者没有内部任务要执行而被阻塞等待、挂起的，而是会扫描所有的队列窃取任务，直到所有队列都为空时才会被挂起。 (2)ForkJoin框架为每个线程维护着一个内部任务队列以及一个全局的任务队列，而且任务队列都是双向队列，可从首尾两端来获取任务，极大地减少了竞争的可能性，提高并行的性能。 ForkJoinPool适合需要“分而治之”的场景，特别是分治之后递归调用的函数，例如快速排序、二分搜索、大整数乘法、矩阵乘法、棋盘覆盖、归并排序、线性时间选择、汉诺塔问题等。ForkJoinPool适合调度的任务为CPU密集型任务，如果任务存在I/0操作、线程同步操作、sleep睡眠等较长时间阻塞的情况，最好配合使用ManagedBlocker进行阻塞管理。总体来说，ForkJoinPool不适合进行I0密集型、混合型的任务调度。 四，生产者-消费者模式生产者-消费者模式是一个经典的多线程设计模式，它为多线程间的协作提供了良好的解决方案，是高并发编程过程中常用的一种设计模式。 在实际的软件开发过程中，经常会碰到如下场景:某些模块负责产生数据，另一些模块负责消费数据(此处的模块可以是类、承数、线程、进程等)。产生数据的模块可以形象地称为生产者，而消费数据的模块可以称为消费者。然而，仅仅抽象出来生产者和消费者还不够，该模式还需要有一个数据缓冲区作为生产者和消费者之间的中介:生产者把数据放入缓冲区，而消费者从缓冲区取出数据。 数据缓冲区的作用主要在于能使生产者和消费者解耦。如果没有数据缓冲区，让生产者直接调用消费者的某个方法，那么生产者对于消费者就会产生依赖(也就是耦合)。将来如果消费者的代码发生变化，可能会影响到生产者。而如果两者都依赖于某个缓冲区，两者之间不直接依赖，耦合也就相应降低了。生产者-消费者模式天生就是用来处理并发问题的。生产者和消费者是两个独立的并发主体，生产者把制造出来的数据往缓冲区一放，就可以再去生产下一个数据了。生产者基本上不用依赖消费者的处理速度。尤其是在生成者的速度时快时慢时，生产者-消费者模式的好处就体现出来了。当数据制造快的时候，消费者来不及处理，未处理的数据可以暂时存在缓冲区中。等生产者的制造速度慢下来，消费者再慢慢处理掉。 在生产者-消费者模式中，缓冲区是性能的关键，缓冲区可以基于 ArrayList、LinkedList、BlockingQueue、环形队列等各种不同的数据存储组件去设计，所使用的组件不同，生产者-消费者模式实现的性能当然也就不同。 五，Future模式Future模式是高并发设计与开发过程中常见的设计模式，它的核心思想是异步调用。对于Future模式来说，它不是立即返回我们所需要的数据，但是它会返回一个契约(或异步任务)，将来我们可以凭借这个契约(或异步任务)获取需要的结果。 在进行传统的RPC(远程调用)时，同步调用RPC是一段耗时的过程。当客户端发出RPC请求后，服务端完成请求处理需要很长的一段时间才会返回，这个过程中客户端一直在等待，直到数据返回后，再进行其他任务的处理。现有一个Client同步对三个Server分别进行一次RPC调用。 假设一次远程调用的时间为500毫秒，则一个Client同步对三个Server分别进行一次RPC调用的总时间需要耗费1500毫秒。如果要节省这个总时间，可以使用Future模式对其进行改造，将同步的RPC调用改为异步并发的RPC调用，一个Client异步并发对三个Server分别进行一次 RPC调用。 一个Client同步对三个Server分别进行一次RPC调用 一个Client异步并发对三个Server分别进行一次RPC调用 假设一次远程调用的时间为500毫秒，则一个Client异步并发对三个 Server分别进行一次RPC调用的总时间只要耗费500毫秒。使用Future模式异步并发地进行RPC调用，客户端在得到一个RPC的返回结果前并不急于获取该结果，而是充分利用等待时间去执行其他的耗时操作(如其他RPC调用)，这就是Future模式的核心所在。 Future模式的核心思想是异步调用，有点类似于异步的Ajax请求。当调用某个耗时方法时，可以不急于立刻获取结果，而是让被调用者立刻返回一个契约(或异步任务)，并且将耗时的方法放到另外的线程中执行，后续凭契约再去获取异步执行的结果。 在具体的实现上，Future模式和异步回调模式既有区别，又有联系。Java的Future实现类并没有支持异步回调，仍然需要主动获取耗时任务的结果;而Java8中的CompletableFuture组件实现了异步回调模式。","categories":[{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"高并发核心模式之异步回调","slug":"JUC/高并发核心模式之异步回调","date":"2022-01-11T11:04:22.500Z","updated":"2022-01-11T11:21:32.266Z","comments":true,"path":"2022/01/11/JUC/高并发核心模式之异步回调/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/JUC/%E9%AB%98%E5%B9%B6%E5%8F%91%E6%A0%B8%E5%BF%83%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%BC%82%E6%AD%A5%E5%9B%9E%E8%B0%83/","excerpt":"","text":"一，Guava的异步回调模式Guava是Google提供的Java扩展包，它提供了一种异步回调的解决方案。Guava中与异步回调相关的源码处于com.google.commonutilconcurrent包中。包中的很多类都用于对javautilconcurrent的能力扩展和能力增强。比如，Guava的异步任务接口ListenableFuture扩展了Java的Future接口，实现了异步回调的能力。​ 1.FutureCallback总体来说，Guava主要增强了Java而不是另起炉灶。为了实现异步回调方式获取异步线程的结果，Guava做了以下增强:​ 引入了一个新的接口ListenableFuture，继承了Java的Future接口，使得Java的Future异步任务在Guava中能被监控和非阻塞获取异步结果。 引入了一个新的接口FutureCallback，这是一个独立的新接口。该接口的目的是在异步任务执行完成后，根据异步结果完成不同的回调处理，并且可以处理异步结果。 ​ FutureCallback是一个新增的接口，用来填写异步任务执行完后的监听逻辑。FutureCallback拥有两个日调方法:​ onSuccess()方法，在异步任务执行成功后被回调。调用时，异步任务的执行结果作为onSuccess方法的参数被传入。 onFailure0方法，在异步任务执行过程中抛出异常时被回调。调用时，异步任务所抛出的异常作为onFailure方法的参数被传入。 ​ Guava的FutureCallback与Java的Callable名字相近，实质不同，存在本质的区别:​ (1)Java的Callable接口代表的是异步执行的逻辑。​ (2)Guava的FutureCallback接口代表的是Callable异步逻辑执行完成之后，根据成功或者异常两种情形所需要执行的善后工作。​ Guava是对JavaFuture异步回调的增强，使用Guava异步回调也需要用到Java的Callable接口。简单地说，只有在Java的Callable任务执行结果出来后，才可能执行Guava中的FutureCallback结果回调。Guava如何实现异步任务Callable和结果回调FutureCallback之间的监控关系呢?Guava引入了一个新接口ListenableFuture，它继承了Java的 Future接口，增强了被监控的能力。 ​ 2.ListenableFutureGuava的ListenableFuture接口是对Java的Future接口的扩展，可以理解为异步任务实例：​ 1234public interface ListenableFuture&lt;V&gt; extends Future&lt;V&gt;&#123; //此方法由guava内部调用 void addListener(Runnable r , Executor e) &#125; ListenableFuture仅仅增加了一个addListener方法。它的作用就是将FutureCallback善后回调逻辑封装成一个内部的Runnable异步口调任务，在Callable异步任务完成后回调FutureCallback善后逻辑。​ 注意，此addListener(方法只在Guava内部调用，在实际编程中，addListener(）不会使用到。​ 在实际编程中，如何将FutureCallback回调逻辑绑定到异步的 ListenableFuture任务呢?可以使用Guava的Futures工具类，它有一个 addCallback0静态方法，可以将FutureCallback的回调实例绑定到 ListenableFuture异步任务。下面是一个简单的绑定实例:​ 123456789Futures.addCallback(listenableFuture,new FutureCallback&lt;Boolean&gt;)&#123; public void onSuccess(Boolean r)&#123; // listenableFuture内的Callable 成功时回调此方法 &#125; public void onFailure(Throwable t)&#123; // listenableFuture内的Callable 异常时回调此方法 &#125;&#125; ​ Guava的ListenableFuture接口是对Java的Future接口的扩展，都表示异步任务，那么Guava的异步任务实例从何而来?​ 3.ListenableFuture异步任务如果要获取Guava的ListenableFuture异步任务实例，主要通过向线程池(ThreadPool)提交Callable任务的方式获取。不过，这里所说的线程池不是Java的线程池，而是经过Guava自己定制过的Guava线程池。​ Guava线程池是对Java线程池的一种装饰。创建Guava线程池的方法如下:​ 12345//java线程池ExecutorService jPool = Executors.newFixedThreadPool(10);//Guava线程池ListeningExecutorService gPool = MoreExecutors.listeningDecorator(jPool); ​ 首先创建Java线程池，然后以其作为Guava线程池的参数再构造一个Guava线程池。有了Guava的线程池之后，就可以通过submit()方法来提交任务了，任务提交之后的返回结果就是我们所要的ListenableFuture异步任务实例。​ 简单来说，获取异步任务实例的方式是通过向线程池提交Callable业务逻辑来实现，代码如下:​ 123456//submit()用来提交任务，返回异步任务实例ListenableFuture&lt;Boolean&gt; hFuture = gPool.submit(hJob);//绑定回调实例Futures.addCallback(listenableFuture, new FutureCallback&lt;Boolean&gt;()&#123; //有两种实现回调的方法&#125;); ​ 取到了ListenableFuture实例后，通过Futures.addCallback0方法将 FutureCallback回调逻辑的实例绑定到ListenableFuture异步任务实例，实现异步执行完成后的回调。​ 总结一下，Guava异步回调的流程如下:​ 实现Java的Callable接口，创建异步执行逻辑。还有一种情况，如果不需要返回值，异步执行逻辑也可以实现Runnable接口。 创建Guava线程池。 将(1)创建的Callable/Runnable异步执行逻辑的实例提交到 Guava线程池，从而获取ListenableFuture异步任务实例。 创建FutureCallback回调实例，通过FuturesaddCallback将回调实例绑定到ListenableFuture异步任务上。 ​ 完成以上4步，当Callable/Runnable异步执行逻辑完成后，就会回调 FutureCallback实例的回调方法onSuccess()/onFailure()。​ 4.使用Guava实现泡茶喝的实例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109public class GuavaFutureDemo &#123; public static final int SLEEP_GAP=3; static class HotWaterJob implements Callable&lt;Boolean&gt;&#123; @Override public Boolean call() throws Exception &#123; try &#123; System.out.println(&quot;洗好水壶&quot;); System.out.println(&quot;烧开水&quot;); TimeUnit.SECONDS.sleep(SLEEP_GAP); System.out.println(&quot;水开了&quot;); &#125;catch (Exception e)&#123; e.printStackTrace(); return false; &#125; return true; &#125; &#125; static class WashJob implements Callable&lt;Boolean&gt;&#123; @Override public Boolean call() throws Exception &#123; try&#123; System.out.println(&quot;洗茶杯&quot;); TimeUnit.SECONDS.sleep(SLEEP_GAP); System.out.println(&quot;洗完了&quot;); &#125;catch (Exception e)&#123; System.out.println(&quot;清洗工作发生异常！&quot;); return false; &#125; System.out.println(&quot;清洗工作运行结束！&quot;); return true; &#125; &#125; static class DrinkJob&#123; boolean waterOk =false; boolean cupOk =false; public void drinkTea()&#123; if (waterOk&amp;&amp;cupOk)&#123; System.out.println(&quot;泡茶喝，茶喝完！&quot;); this.waterOk=false; &#125; &#125; &#125; public static void main(String[] args)throws Exception &#123; Thread.currentThread().setName(&quot;泡茶喝线程&quot;); //新启动一个线程，作为泡茶主线程 DrinkJob drinkJob = new DrinkJob(); //烧水的业务逻辑 Callable&lt;Boolean&gt; hotJob = new HotWaterJob(); //清晰的业务逻辑 Callable&lt;Boolean&gt; washJob = new HotWaterJob(); //创建java线程池 ExecutorService jPool = Executors.newFixedThreadPool(10); //包装java线程池，构造guava线程池 ListeningExecutorService gPool = MoreExecutors.listeningDecorator(jPool); //烧水的回调 FutureCallback&lt;Boolean&gt; hotWaterHook = new FutureCallback&lt;Boolean&gt;() &#123; @Override public void onSuccess(Boolean r) &#123; if (r)&#123; drinkJob.waterOk=true; drinkJob.drinkTea(); &#125; &#125; @Override public void onFailure(Throwable throwable) &#123; System.out.println(&quot;喝nm！&quot;); &#125; &#125;; //启动烧水线程 ListenableFuture&lt;Boolean&gt; hotFuture = gPool.submit(hotJob); //设置烧水任务的回调钩子 Futures.addCallback(hotFuture, hotWaterHook); //启动清洗线程 ListenableFuture&lt;Boolean&gt; washFuture = gPool.submit(washJob); //使用匿名实例，作为清洗之后的回调钩子 Futures.addCallback(washFuture, new FutureCallback&lt;Boolean&gt;() &#123; @Override public void onSuccess(Boolean r) &#123; if (r)&#123; drinkJob.cupOk=true; //执行回调 drinkJob.drinkTea(); &#125; &#125; @Override public void onFailure(Throwable throwable) &#123; System.out.println(&quot;喝nm！&quot;); &#125; &#125;); System.out.println(&quot;干点其他事！&quot;); TimeUnit.SECONDS.sleep(1); System.out.println(&quot;执行完成！&quot;); &#125;&#125; ​ 以上结果，烧水线程为pool-1-thread-1，清洗线程为pool-1-thread-2，在二者完成之前，泡茶喝线程已经执行完了。泡茶喝的工作在异步回调方法drinkTea(中执行，执行的线程并不是“泡茶喝”线程，而是烧水线程和清洗线程。​ 5.Guava异步回调和Java异步回调的区别总结一下Guava异步回调和Java的FutureTask异步调用的区别，具体如下:​ FutureTask是主动调用的模式，“调用线程”主动获得异步结果，在获取异步结果时处于阻塞状态，并且会一直阻塞，直到拿到异步线程的结果。 Guava是异步回调模式，“调用线程”不会主动获得异步结果，而是准备好回调函数，并设置好回调钩子，执行回调函数的并不是“调用线程”自身，回调承数的执行者是“被调用线程”，“调用线程”在执行完自己的业务逻辑后就已经结束了，当回调采数被执行时，“调用线程”可能已经结束很久了。 ​ 二，Netty的异步回调模式Netty官方文档说明Netty的网络操作都是异步的。Netty源码中大量使用了异步回调处理模式。在Netty的业务开发层面，处于Netty应用的 Handler处理程序中的业务处理代码也都是异步执行的。所以，了解 Netty的异步回调，无论是Netty应用开发还是源码级开发都是十分重要的。​ Netty和Guava一样，实现了自己的异步回调体系:Netty继承和扩展了JDKFuture系列异步回调的API，定义了自身的Future系列接口和类，实现了异步任务的监控、异步执行结果的获取。​ 总体来说，Netty对JavaFuture异步任务的扩展如下:​ 继承Java的Future接口得到了一个新的属于Netty自己的Future异步任务接口，该接口对原有的接口进行了增强，使得Netty异步任务能够非阻塞地处理回调结果。注意，Netty没有修改Future的名称，只是调整了所在的包名，Netty的Future类的包名和Java的Future接口的包不同。​ 引入了一个新接口–GenericFutureListener，用于表示异步执行完成的监听器。这个接口和Guava的FutureCallback回调接口不同。Nettv使用了监听器的模式，异步任务执行完成后的回调逻辑抽象成了Listener监听器接口。可以将Netty的GenericFutureListener监听器接口加入Netty异步任务Future中，实现对异步任务执行状态的事件监听。​ 总体来说，在异步非阻塞回调的设计思路上，Netty和Guava是一致的。对应关系为:​ Netty的Future接口可以对应到Guava的ListenableFuture接口。 ​ Netty的GenericFutureListener接口可以对应到Guava的 FutureCallback接口。 ​ 1.GenericFutureListener前面提到，和Guava的FutureCallback一样，Netty新增了一个接口，用来封装异步非阻塞回调的逻辑，那就是GenericFutureListener接口。​ GenericFutureListener位于io.netty.util.concurrent包中，源码如下:​ 123456package io.netty.util.concurrent; import java.util.EventListenerpublic interface GenericFutureListener&lt;F extends Future&lt;?&gt;&gt; extends Eventlistener&#123; //监听器的回调方法 void operationComplete(F var1) throws Exception;&#125; ​ GenericFutureListener拥有一个回调方法operationCompleteO，表示异步任务操作完成。在Future异步任务执行完成后将回调此方法。大多数情况下，Netty的异步回调代码编写在GenericFutureListener接口的实现类的operationComplete方法中。​ 说明一下，GenericFutureListener的父接口EventListener是一个空接口，没有任何抽象方法，是一个仅仅具有标识作用的接口。​ 2.Netty的Future接口Netty也对Java的Future接口进行了扩展，并且名称没有变，还是叫作Future接口，实现在io.nettyutil.concurrent包中。​ 和Guava的ListenableFuture一样，Netty的Future接口扩展了一系列方法，对执行的过程进行监控，对异步回调完成事件进行Listen监听并且回调。​ Netty的Future接口一般不会直接使用，使用过程中会使用它的子接口。Netty有一系列子接口，代表不同类型的异步任务，如ChannelFuture接口。​ ChannelFuture子接口表示Channel通道I/0操作的异步任务，如果在 Channel的异步I/0操作完成后需要执行回调操作，就需要使用到 ChannelFuture接口。​ 3.ChannelFuture在Netty网络编程中，网络连接通道的输入、输出处理都是异步进行的，都会返回一个ChannelFuture接口的实例。通过返回的异步任务实例可以为其增加异步回调的监听器。在异步任务真正完成后，回调执行。​ Netty的网络连接的异步回调实例代码如下:​ 1234567891011121314//connect是异步的，仅仅是提交异步任务ChannelFuture future = bootstrap.connect(new InetSocketAddress(&quot;www.manning.com,80));//connect的异步任务真正执行完成后，future回调监听器会执行future.addListener(new ChannelFutureListener()&#123; @Override public void operationComplete(ChannelFuture channelFuture) throws Exception &#123; if(channelFuture.isSuccess)) System.out.println(&quot;Connection established&quot;); else &#123; System.err.println(&quot;Connection attempt failed&quot;); channelFuture.cause().printStackTrace); &#125; &#125;&#125; ​ GenericFutureListener接口在Netty中是一个基础类型接口。在网络编程的异步回调中，一般使用Netty中提供的某个子接口，如ChannelFutureListener接口。在上面的代码中，使用到的是这个子接口。​ 4.Netty的出站和入站异步回调Netty的出站和入站操作都是异步的。异步回调的方法和前面Netty建立的异步回调是一样的。​ 下面以经典的NIO出站操作write为例说明ChannelFuture的使用。​ 在write操作调用后，Netty并没有立即完成对JavaNIO底层连接的写入操作，底层的写入操作是异步执行的，代码如下:​ 123456789//write()输出方法，返回的是一个异步任务ChannelFuture future=ctx.channel0.write(msg);//为异步任务加上监听器 future.addListener( new ChannelFutureListener()&#123; @Override public void operationComplete(ChannelFuture future)&#123; // write操作完成后的回调代码 &#125; &#125;); ​ 在write操作完成后立即返回，返回的是一个ChannelFuture接口的实例。通过这个实例可以绑定异步回调监听器，编写异步回调的逻辑。​ ​","categories":[{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"RabbitMQ","slug":"消息队列/RabbitMQ","date":"2022-01-11T09:08:03.557Z","updated":"2022-01-11T09:11:01.252Z","comments":true,"path":"2022/01/11/消息队列/RabbitMQ/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/RabbitMQ/","excerpt":"","text":"一.rabbitMQ概述1.搜索与商品服务的问题假设我们已经完成了商品详情和搜索系统的开发。我们思考一下，是否存在问题？ 商品的原始数据保存在数据库中，增删改查都在数据库中完成。搜索服务数据来源是索引库，如果数据库商品发生变化，索引库数据能否及时更新。如果我们在后台修改了商品的价格，搜索页面依然是旧的价格，这样显然不对。该如何解决？ 方案1：每当后台对商品做增删改操作，同时要修改索引库数据 方案2：搜索服务对外提供操作接口，后台在商品增删改后，调用接口 以上两种方式都有同一个严重问题：就是代码耦合，后台服务中需要嵌入搜索和商品页面服务，违背了微服务的独立原则。所以，我们会通过另外一种方式来解决这个问题：消息队列 2.消息队列2.1什么是消息队列消息队列，即MQ，Message Queue。 “消息”是在两台计算机间传送的数据单位。消息可以非常简单，例如只包含文本字符串；也可以更复杂，可能包含嵌入对象。​ 消息被发送到队列中。“消息队列”是在消息的传输过程中保存消息的容器。消息队列管理器在将消息从它的源中继到它的目标时充当中间人。队列的主要目的是提供路由并保证消息的传递；如果发送消息时接收者不可用，消息队列会保留消息，直到可以成功地传递它。 消息队列是典型的：生产者、消费者模型。生产者不断向消息队列中生产消息，消费者不断的从队列中获取消息。因为消息的生产和消费都是异步的，而且只关心消息的发送和接收，没有业务逻辑的侵入，这样就实现了生产者和消费者的解耦。 结合前面所说的问题： · 商品服务对商品增删改以后，无需去操作索引库，只是发送一条消息，也不关心消息被谁接收。 · 搜索服务接收消息，去处理索引库。 如果以后有其它系统也依赖商品服务的数据，同样监听消息即可，商品服务无需任何代码修改。 2.2AMQP和JMSMQ是消息通信的模型，并不是具体实现。现在实现MQ的有两种主流方式：AMQP、JMS。 两者间的区别和联系： · JMS是定义了统一接口，对消息操作进行统一；AMQP通过规定协议统一数据交互的格式； · JMS限定了必须使用Java语言；AMQP只是协议，不规定实现方式，因此是跨语言的 · JMS规定了两种消息模型（queue, topic）；而AMQP的消息模型更加丰富 2.3常见MQ产品· ActiveMQ：基于JMS · RabbitMQ：基于AMQP协议，erlang语言开发，稳定性好 · RocketMQ：基于JMS，阿里巴巴产品，目前交由Apache基金会 · Kafka：分布式消息系统，高吞吐量，处理日志，Scala和Java编写，Apache 2.4 RabbitMQ官网： http://www.rabbitmq.com/ 官方教程：http://www.rabbitmq.com/getstarted.html 2.5 MQ 三大主要功能 异步 解耦 削峰 2.6 RabbitMQ工作模型 1.Broker我们要使用RabbitMQ来收发消息，必须要安装个RabbitMQ的服务，可以安装在Windows上面也可以安装在Linux 上面，默认是5672的端口。这台RabbitMQ的服务器我们把它叫做 Broker， MQ 服务器帮助我们做的事情就是存储、转发消息。 2.Connection无论是生产者发送消息，还是消费者接收消息，都必须要跟 Broker 之间建立一个连接，这个连接是一个 TCP 的长连接。 3.Channel如果所有的生产者发送消息和消费者接收消息，都直接创建和释放 TCP 长连接的话，对于 Broker 来说肯定会造成很大的性能损耗，因为 TCP 连接是非常宝贵的资源，创建和释放也要消耗时间。所以在 AMQP 里面引入了 Channel 的概念，它是一个虚拟的连接。这样我们就可以在保持的 TCP 长连接里面去创建和释放Channel，大大了减少了资源消耗。 4.Queue队列是真正用来存储消息的，是一个独立运行的进程，有自己的数据库（Mnesia）。 我们可以基于事件机制，实现消费者对队列的监听。 由于队列有 FIFO 的特性，只有确定前一条消息被消费者接收之后，才会把这条消息从数据库删除，继续投递下一条消息。 5.Exchange在 RabbitMQ 里面永远不会出现消息直接发送到队列的情况。因为在 AMQP 里面引入了交换机（Exchange）的概念，用来实现消息的灵活路由。 交换机是一个绑定列表，用来查找匹配的绑定关系。 队列使用绑定键（Binding Key）跟交换机建立绑定关系。 生产者发送的消息需要携带路由键（Routing Key），交换机收到消息时会根据它保存的绑定列表，决定将消息路由到哪些与它绑定的队列上。 注意：交换机与队列、队列与消费者都是多对多的关系。 6.VhostVHOST 除了可以提高硬件资源的利用率之外，还可以实现资源的隔离和权限的控制。 不同的 VHOST 中可以有同名的 Exchange 和 Queue，它们是完全透明的。 这个时候，我们可以为不同的业务系统创建不同的用户（User），然后给这些用户分配 VHOST 的权限。 2.7 使用rabbitMQ会带来的一些问题？系统可用性降低：原来是两个节点的通信，现在还需要独立运行一个服务，如果 MQ服务器或者通信网络出现问题，就会导致请求失败。 系统复杂性提高： 为什么说复杂？第一个就是你必须要理解相关的模型和概念，才能正确地配置和使用 MQ。第二个，使用 MQ 发送消息必须要考虑消息丢失和消息重复消费的问题。一旦消息没有被正确地消费，就会带来数据一致性的问题。 所以，我们在做系统架构的时候一定要根据实际情况来分析，不要因为我们说了这么多的 MQ 能解决的问题，就盲目地引入 MQ。 3.下载和安装3.1 下载RabbitMQ是Erlang语言编写，所以Erang环境必须要有，注：Erlang环境一定要与RabbitMQ版本匹配：https://www.rabbitmq.com/which-erlang.html Erlang下载地址：https://www.rabbitmq.com/releases/erlang/（根据自身需求及匹配关系，下载对应rpm包） https://dl.bintray.com/rabbitmq-erlang/rpm/erlang/21/el/7/x86_64/erlang-21.3.8.9-1.el7.x86_64.rpm rabbitmq安装依赖于socat，所以需要下载socat。 socat下载地址：http://repo.iotti.biz/CentOS/7/x86_64/socat-1.7.3.2-5.el7.lux.x86_64.rpm RabbitMQ下载地址：https://www.rabbitmq.com/download.html（根据自身需求及匹配关系，下载对应rpm包）`rabbitmq-server-3.8.1-1.el7.noarch.rpm` 3.2安装1rpm -ivh erlang-21.3.8.9-1.el7.x86_64.rpm 1rpm -ivh socat-1.7.3.2-1.el6.lux.x86_64.rpm 1rpm -ivh rabbitmq-server-3.8.1-1.el7.noarch.rpm 启用管理插件 1rabbitmq-plugins enable rabbitmq_management 启动RabbitMQ 1234systemctl start rabbitmq-server.servicesystemctl status rabbitmq-server.servicesystemctl restart rabbitmq-server.servicesystemctl stop rabbitmq-server.service 查看进程 1ps -ef | grep rabbitmq 启用延时队列插件 1rabbitmq-plugins enable rabbitmq_delayed_message_exchange 3.3 测试o 关闭防火墙：systemctl stop firewalld.service o 在web浏览器中输入地址：http://虚拟机ip:15672/ o 输入默认账号密码:guest : guest，guest用户默认不允许远程连接。 添加用户 1rabbitmqctl add_user root root 分配角色 1rabbitmqctl set_user_tags root administrator 修改密码 1rabbitmqctl change_password root root 查看所有用户 1rabbitmqctl list_users 3.4卸载12rpm -qa | grep rabbitmqrpm -e rabbitmq-server 4管理界面4.1 添加用户 超级管理员(administrator) 可登录管理控制台，可查看所有的信息，并且可以对用户，策略(policy)进行操作。 监控者(monitoring) 可登录管理控制台，同时可以查看rabbitmq节点的相关信息(进程数，内存使用情况，磁盘使用情况等) 策略制定者(policymaker) 可登录管理控制台, 同时可以对policy进行管理。但无法查看节点的相关信息(上图红框标识的部分)。 普通管理者(management) 仅可登录管理控制台，无法看到节点信息，也无法对策略进行管理。 其他 无法登录管理控制台，通常就是普通的生产者和消费者。 4.2 创建 Virtual Hosts虚拟主机：类似于mysql中的database。他们都是以“/”开头 二，五种消息模型RabbitMQ提供了6种消息模型，但是第6种其实是RPC，并不是MQ，因此不予学习。那么也就剩下5种。但是其实3、4、5这三种都属于订阅模型，只不过进行路由的方式不同。​ 准备代码环境 12345678910111213141516171819202122232425262728&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.2.6.RELEASE&lt;/version&gt;&lt;/parent&gt;&lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt; &lt;version&gt;3.3.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.rabbitmq&lt;/groupId&gt; &lt;artifactId&gt;amqp-client&lt;/artifactId&gt; &lt;version&gt;5.4.3&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 抽取一个获取连接的工具类 1234567891011121314151617181920212223242526272829public class ConnectionUtil &#123; /** * 建立与RabbitMQ的连接 * @return * @throws Exception */ public static Connection getConnection() throws Exception &#123; //定义连接工厂 ConnectionFactory factory = new ConnectionFactory(); //设置服务地址 factory.setHost(&quot;121.199.31.160&quot;); //端口 factory.setPort(5672); //设置账号信息，用户名、密码、vhost factory.setVirtualHost(&quot;/shopping&quot;); factory.setUsername(&quot;root&quot;); factory.setPassword(&quot;root&quot;); // 通过工程获取连接 Connection connection = factory.newConnection(); return connection; &#125; public static void main(String[] args) throws Exception &#123; Connection con = ConnectionUtil.getConnection(); System.out.println(con); con.close(); &#125;&#125; 1.基本消息模型​ RabbitMQ是一个消息代理：它接受和转发消息。 你可以把它想象成一个邮局：当你把邮件放在邮箱里时，你可以确定邮差先生最终会把邮件发送给你的收件人。RabbitMQ与邮局的主要区别是它不处理纸张，而是接受，存储和转发数据消息的二进制数据块。​ 1234567891011121314151617181920212223242526/** * 生产者 */public class Send &#123; private final static String QUEUE_NAME = &quot;simple_queue&quot;; public static void main(String[] argv) throws Exception &#123; // 获取到连接 Connection connection = ConnectionUtil.getConnection(); // 从连接中创建通道，使用通道才能完成消息相关的操作 Channel channel = connection.createChannel(); // 声明（创建）队列 channel.queueDeclare(QUEUE_NAME, false, false, false, null); // 消息内容 String message = &quot;Hello World!&quot;; // 向指定的队列中发送消息 channel.basicPublish(&quot;&quot;, QUEUE_NAME, null, message.getBytes()); System.out.println(&quot; [x] Sent &#x27;&quot; + message + &quot;&#x27;&quot;); //关闭通道和连接 channel.close(); connection.close(); &#125;&#125; 12345678910111213141516171819202122232425262728/** * 消费者 */public class Recv &#123; private final static String QUEUE_NAME = &quot;simple_queue&quot;; public static void main(String[] argv) throws Exception &#123; // 获取到连接 Connection connection = ConnectionUtil.getConnection(); // 创建通道 Channel channel = connection.createChannel(); // 声明队列 channel.queueDeclare(QUEUE_NAME, false, false, false, null); // 定义队列的消费者 DefaultConsumer consumer = new DefaultConsumer(channel) &#123; // 获取消息，并且处理，这个方法类似事件监听，如果有消息的时候，会被自动调用 @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; // body 即消息体 String msg = new String(body); System.out.println(&quot; [x] received : &quot; + msg + &quot;!&quot;); &#125; &#125;; // 监听队列，第二个参数：是否自动进行消息确认。 channel.basicConsume(QUEUE_NAME, true, consumer); &#125;&#125; 1.1 消息确认机制-ACKRabbitMQ有一个ACK机制。当消费者获取消息后，会向RabbitMQ发送回执ACK，告知消息已经被接收。不过这种回执ACK分两种情况： 自动ACK：消息一旦被接收，消费者自动发送ACK 手动ACK：消息接收后，不会发送ACK，需要手动调用 选择哪种要看消息的重要性： 如果消息不太重要，丢失也没有影响，那么自动ACK会比较方便 如果消息非常重要，不容丢失。那么最好在消费完成后手动ACK，否则接收消息后就自动ACK，RabbitMQ就会把消息从队列中删除。如果此时消费者宕机，那么消息就丢失了 我们之前的测试都是自动ACK的，如果要手动ACK，需要改动我们的代码：​ 12345678910111213141516171819202122232425262728293031/** * 消费者,手动进行ACK */public class Recv2 &#123; private final static String QUEUE_NAME = &quot;simple_queue&quot;; public static void main(String[] argv) throws Exception &#123; // 获取到连接 Connection connection = ConnectionUtil.getConnection(); // 创建通道 final Channel channel = connection.createChannel(); // 声明队列 channel.queueDeclare(QUEUE_NAME, false, false, false, null); // 定义队列的消费者 DefaultConsumer consumer = new DefaultConsumer(channel) &#123; // 获取消息，并且处理，这个方法类似事件监听，如果有消息的时候，会被自动调用 @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; // body 即消息体 String msg = new String(body); //int i = 1/0 ; System.out.println(&quot; [x] received : &quot; + msg + &quot;!&quot;); // 手动进行ACK channel.basicAck(envelope.getDeliveryTag(), false); &#125; &#125;; // 监听队列，第二个参数false，手动进行ACK channel.basicConsume(QUEUE_NAME, false, consumer); &#125;&#125; 2.work消息模型避免执行资源密集型任务时，必须等待它执行完成。相反我们稍后完成任务，我们将任务封装为消息并将其发送到队列。 在后台运行的工作进程将获取任务并最终执行作业。当运行许多消费者时，任务将在他们之间共享，但是一个消息只能被一个消费者获取。​ 一定程度上，避免消息的堆积。 ​ 平均分摊​ 消费者2与消费者1基本类似，就是没有设置消费耗时时间。这里是模拟有些消费者快，有些比较慢。 接下来，两个消费者一同启动，然后发送50条消息：可以发现，两个消费者各自消费了25条消息，而且各不相同，这就实现了任务的分发。 能者多劳 现在的状态属于是把任务平均分配，正确的做法应该是消费越快的人，消费的越多。怎么实现呢？ 我们可以使用basicQos方法和prefetchCount = 1设置。这告诉RabbitMQ一次不要向工作人员发送多于一条消息。或者换句话说，不要向工作人员发送新消息，直到它处理并确认了前一个消息。相反，它会将其分派给不是仍然忙碌的下一个工作人员。 12345678910111213141516171819202122232425// 生产者public class Send &#123; private final static String QUEUE_NAME = &quot;test_work_queue&quot;; public static void main(String[] argv) throws Exception &#123; // 获取到连接 Connection connection = ConnectionUtil.getConnection(); // 获取通道 Channel channel = connection.createChannel(); // 声明队列 channel.queueDeclare(QUEUE_NAME, false, false, false, null); // 循环发布任务 for (int i = 0; i &lt; 50; i++) &#123; // 消息内容 String message = &quot;task .. &quot; + i; channel.basicPublish(&quot;&quot;, QUEUE_NAME, null, message.getBytes()); System.out.println(&quot; [x] Sent &#x27;&quot; + message + &quot;&#x27;&quot;); //Thread.sleep(i * 2); &#125; // 关闭通道和连接 channel.close(); connection.close(); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435// 消费者1public class Recv &#123; private final static String QUEUE_NAME = &quot;test_work_queue&quot;; public static void main(String[] argv) throws Exception &#123; // 获取到连接 Connection connection = ConnectionUtil.getConnection(); // 获取通道 final Channel channel = connection.createChannel(); // 声明队列 channel.queueDeclare(QUEUE_NAME, false, false, false, null); // 设置每个消费者同时只能处理一条消息 //channel.basicQos(1); // 定义队列的消费者 DefaultConsumer consumer = new DefaultConsumer(channel) &#123; // 获取消息，并且处理，这个方法类似事件监听，如果有消息的时候，会被自动调用 @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; // body 即消息体 String msg = new String(body); System.out.println(&quot; [消费者1] received : &quot; + msg + &quot;!&quot;); try &#123; // 模拟完成任务的耗时：1000ms Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; &#125; // 手动ACK channel.basicAck(envelope.getDeliveryTag(), false); &#125; &#125;; // 监听队列。 channel.basicConsume(QUEUE_NAME, false, consumer); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435//消费者2public class Recv2 &#123; private final static String QUEUE_NAME = &quot;test_work_queue&quot;; public static void main(String[] argv) throws Exception &#123; // 获取到连接 Connection connection = ConnectionUtil.getConnection(); // 获取通道 final Channel channel = connection.createChannel(); // 声明队列 channel.queueDeclare(QUEUE_NAME, false, false, false, null); // 设置每个消费者同时只能处理一条消息 //channel.basicQos(1); // 定义队列的消费者 DefaultConsumer consumer = new DefaultConsumer(channel) &#123; // 获取消息，并且处理，这个方法类似事件监听，如果有消息的时候，会被自动调用 @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; // body 即消息体 String msg = new String(body); System.out.println(&quot; [消费者2] received : &quot; + msg + &quot;!&quot;); try &#123; // 模拟完成任务的耗时：1000ms Thread.sleep(200); &#125; catch (InterruptedException e) &#123; &#125; // 手动ACK channel.basicAck(envelope.getDeliveryTag(), false); &#125; &#125;; // 监听队列。 channel.basicConsume(QUEUE_NAME, false, consumer); &#125;&#125; 3.订阅模型分类​ 在之前的模式中，我们创建了一个工作队列。 工作队列背后的假设是：每个任务只被传递给一个工作人员。在这一部分，我们将做一些完全不同的事情 - 我们将会传递一个信息给多个消费者。 这种模式被称为“发布/订阅”。 1个生产者，多个消费者 每一个消费者都有自己的一个队列 生产者没有将消息直接发送到队列，而是发送到了交换机 每个队列都要绑定到交换机 生产者发送的消息，经过交换机到达队列，实现一个消息被多个消费者获取的目的 X（Exchanges）： 交换机一方面：接收生产者发送的消息。另一方面：知道如何处理消息，例如递交给某个特别队列、递交给所有队列、或是将消息丢弃。到底如何操作，取决于Exchange的类型。 Exchange（交换机）只负责转发消息，不具备存储消息的能力，因此如果没有任何队列与Exchange绑定，或者没有符合路由规则的队列，那么消息会丢失！ Exchange类型有以下几种： Fanout：广播，将消息交给所有绑定到交换机的队列 Direct：定向，把消息交给符合指定routing key 的队列 Topic：通配符，把消息交给符合routing pattern（路由模式） 的队列 ​ 4.订阅模型-Fanout也称为广播。在广播模式下，消息发送流程是这样的： 可以有多个消费者 每个消费者有自己的queue（队列） 每个队列都要绑定到Exchange（交换机） ​生产者发送的消息，只能发送到交换机，交换机来决定要发给哪个队列，生产者无法决定。 交换机把消息发送给绑定过的所有队列 队列的消费者都能拿到消息。实现一条消息被多个消费者消费 ​ 1234567891011121314151617181920212223public class Send &#123; private final static String EXCHANGE_NAME = &quot;fanout_exchange_test&quot;; public static void main(String[] argv) throws Exception &#123; // 获取到连接 Connection connection = ConnectionUtil.getConnection(); // 获取通道 Channel channel = connection.createChannel(); // 声明exchange，指定类型为fanout channel.exchangeDeclare(EXCHANGE_NAME, &quot;fanout&quot;); // 消息内容 String message = &quot;Hello everyone&quot;; // 发布消息到Exchange channel.basicPublish(EXCHANGE_NAME, &quot;&quot;, null, message.getBytes()); System.out.println(&quot; [生产者] Sent &#x27;&quot; + message + &quot;&#x27;&quot;); channel.close(); connection.close(); &#125;&#125; 1234567891011121314151617181920212223242526272829303132//消费者1public class Recv &#123; private final static String QUEUE_NAME = &quot;fanout_exchange_queue_1&quot;; private final static String EXCHANGE_NAME = &quot;fanout_exchange_test&quot;; public static void main(String[] argv) throws Exception &#123; // 获取到连接 Connection connection = ConnectionUtil.getConnection(); // 获取通道 Channel channel = connection.createChannel(); // 声明队列 channel.queueDeclare(QUEUE_NAME, false, false, false, null); // 绑定队列到交换机 channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, &quot;&quot;); // 定义队列的消费者 DefaultConsumer consumer = new DefaultConsumer(channel) &#123; // 获取消息，并且处理，这个方法类似事件监听，如果有消息的时候，会被自动调用 @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; // body 即消息体 String msg = new String(body); System.out.println(&quot; [消费者1] received : &quot; + msg + &quot;!&quot;); &#125; &#125;; // 监听队列，自动返回完成 channel.basicConsume(QUEUE_NAME, true, consumer); &#125;&#125; 1234567891011121314151617181920212223242526272829303132// 消费者2public class Recv2 &#123; private final static String QUEUE_NAME = &quot;fanout_exchange_queue_2&quot;; private final static String EXCHANGE_NAME = &quot;fanout_exchange_test&quot;; public static void main(String[] argv) throws Exception &#123; // 获取到连接 Connection connection = ConnectionUtil.getConnection(); // 获取通道 Channel channel = connection.createChannel(); // 声明队列 channel.queueDeclare(QUEUE_NAME, false, false, false, null); // 绑定队列到交换机 channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, &quot;&quot;); // 定义队列的消费者 DefaultConsumer consumer = new DefaultConsumer(channel) &#123; // 获取消息，并且处理，这个方法类似事件监听，如果有消息的时候，会被自动调用 @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; // body 即消息体 String msg = new String(body); System.out.println(&quot; [消费者2] received : &quot; + msg + &quot;!&quot;); &#125; &#125;; // 监听队列，手动返回完成 channel.basicConsume(QUEUE_NAME, true, consumer); &#125;&#125; 5.订阅模型-Direct​ 在某些场景下，我们希望不同的消息被不同的队列消费。这时就要用到Direct类型的Exchange。 在Direct模型下，队列与交换机的绑定，不能是任意绑定了，而是要指定一个RoutingKey（路由key） 消息的发送方在向Exchange发送消息时，也必须指定消息的routing key。 P：生产者，向Exchange发送消息，发送消息时，会指定一个routing key。 X：Exchange（交换机），接收生产者的消息，然后把消息递交给 与routing key完全匹配的队列 C1：消费者，其所在队列指定了需要routing key 为 error 的消息 C2：消费者，其所在队列指定了需要routing key 为 info、error、warning 的消息 此处我们模拟商品的增删改，发送消息的RoutingKey分别是：insert、update、delete​ 1234567891011121314151617181920212223/** * 生产者，模拟为商品服务 */public class Send &#123; private final static String EXCHANGE_NAME = &quot;direct_exchange_test&quot;; public static void main(String[] argv) throws Exception &#123; // 获取到连接 Connection connection = ConnectionUtil.getConnection(); // 获取通道 Channel channel = connection.createChannel(); // 声明exchange，指定类型为direct channel.exchangeDeclare(EXCHANGE_NAME, &quot;direct&quot;); // 消息内容 String message = &quot;商品删除了， id = 1001&quot;; // 发送消息，并且指定routing key 为：insert ,代表新增商品 channel.basicPublish(EXCHANGE_NAME, &quot;delete&quot;, null, message.getBytes()); System.out.println(&quot; [商品服务：] Sent &#x27;&quot; + message + &quot;&#x27;&quot;); channel.close(); connection.close(); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334/** * 消费者1 */public class Recv &#123; private final static String QUEUE_NAME = &quot;direct_exchange_queue_1&quot;; private final static String EXCHANGE_NAME = &quot;direct_exchange_test&quot;; public static void main(String[] argv) throws Exception &#123; // 获取到连接 Connection connection = ConnectionUtil.getConnection(); // 获取通道 Channel channel = connection.createChannel(); // 声明队列 channel.queueDeclare(QUEUE_NAME, false, false, false, null); // 绑定队列到交换机，同时指定需要订阅的routing key。假设此处需要update和delete消息 channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, &quot;update&quot;); channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, &quot;delete&quot;); // 定义队列的消费者 DefaultConsumer consumer = new DefaultConsumer(channel) &#123; // 获取消息，并且处理，这个方法类似事件监听，如果有消息的时候，会被自动调用 @Override public void handleDelivery(String consumerTag, Envelope envelope, BasicProperties properties, byte[] body) throws IOException &#123; // body 即消息体 String msg = new String(body); System.out.println(&quot; [消费者1] received : &quot; + msg + &quot;!&quot;); &#125; &#125;; // 监听队列，自动ACK channel.basicConsume(QUEUE_NAME, true, consumer); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435/** * 消费者2 */public class Recv2 &#123; private final static String QUEUE_NAME = &quot;direct_exchange_queue_2&quot;; private final static String EXCHANGE_NAME = &quot;direct_exchange_test&quot;; public static void main(String[] argv) throws Exception &#123; // 获取到连接 Connection connection = ConnectionUtil.getConnection(); // 获取通道 Channel channel = connection.createChannel(); // 声明队列 channel.queueDeclare(QUEUE_NAME, false, false, false, null); // 绑定队列到交换机，同时指定需要订阅的routing key。订阅 insert、update、delete channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, &quot;insert&quot;); channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, &quot;update&quot;); channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, &quot;delete&quot;); // 定义队列的消费者 DefaultConsumer consumer = new DefaultConsumer(channel) &#123; // 获取消息，并且处理，这个方法类似事件监听，如果有消息的时候，会被自动调用 @Override public void handleDelivery(String consumerTag, Envelope envelope, BasicProperties properties, byte[] body) throws IOException &#123; // body 即消息体 String msg = new String(body); System.out.println(&quot; [消费者2] received : &quot; + msg + &quot;!&quot;); &#125; &#125;; // 监听队列，自动ACK channel.basicConsume(QUEUE_NAME, true, consumer); &#125;&#125; 6.订阅模型-Topic Topic类型的Exchange与Direct相比，都是可以根据RoutingKey把消息路由到不同的队列。只不过Topic类型Exchange可以让队列在绑定Routing key 的时候使用通配符！ Routingkey 一般都是有一个或多个单词组成，多个单词之间以”.”分割，例如： item.insert 通配符规则： #：匹配0或多个词（含零个）​ *：匹配不多不少恰好1个词（不含零个） 在这个例子中，我们将发送所有描述动物的消息。消息将使用由三个字（两个点）组成的routing key发送。路由关键字中的第一个单词将描述速度，第二个颜色和第三个种类：“**..**”。 我们创建三个绑定： Q1绑定了绑定键“_ .orange.”，Q2绑定了“._.rabbit”和“lazy.＃”。 · Q1匹配所有的橙色动物。 · Q2匹配关于兔子以及懒惰动物的消息。 练习，生产者发送如下消息，会进入那个队列： quick.orange.rabbit Q1 Q2lazy.orange.elephant Q1 Q2quick.orange.fox Q1lazy.pink.rabbit Q2quick.brown.fox 都不能quick.orange.male.rabbit 都不能orange 都不能 123456789101112131415161718192021222324/** * 生产者，模拟为商品服务 */public class Send &#123; private final static String EXCHANGE_NAME = &quot;topic_exchange_test&quot;; public static void main(String[] argv) throws Exception &#123; // 获取到连接 Connection connection = ConnectionUtil.getConnection(); // 获取通道 Channel channel = connection.createChannel(); // 声明exchange，指定类型为topic channel.exchangeDeclare(EXCHANGE_NAME, &quot;topic&quot;,true); // 消息内容 String message = &quot;更新商品 : id = 1001&quot;; // 发送消息，并且指定routing key 为：insert ,代表新增商品 channel.basicPublish(EXCHANGE_NAME, &quot;item.update&quot;, MessageProperties.PERSISTENT_TEXT_PLAIN, message.getBytes()); System.out.println(&quot; [商品服务：] Sent &#x27;&quot; + message + &quot;&#x27;&quot;); channel.close(); connection.close(); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334/** * 消费者1 */public class Recv &#123; private final static String QUEUE_NAME = &quot;topic_exchange_queue_1&quot;; private final static String EXCHANGE_NAME = &quot;topic_exchange_test&quot;; public static void main(String[] argv) throws Exception &#123; // 获取到连接 Connection connection = ConnectionUtil.getConnection(); // 获取通道 Channel channel = connection.createChannel(); // 声明队列 channel.queueDeclare(QUEUE_NAME, true, false, false, null); // 绑定队列到交换机，同时指定需要订阅的routing key。需要 update、delete channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, &quot;item.update&quot;); channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, &quot;item.delete&quot;); // 定义队列的消费者 DefaultConsumer consumer = new DefaultConsumer(channel) &#123; // 获取消息，并且处理，这个方法类似事件监听，如果有消息的时候，会被自动调用 @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; // body 即消息体 String msg = new String(body); System.out.println(&quot; [消费者1] received : &quot; + msg + &quot;!&quot;); &#125; &#125;; // 监听队列，自动ACK channel.basicConsume(QUEUE_NAME, true, consumer); &#125;&#125; 123456789101112131415161718192021222324252627282930313233/** * 消费者2 */public class Recv2 &#123; private final static String QUEUE_NAME = &quot;topic_exchange_queue_2&quot;; private final static String EXCHANGE_NAME = &quot;topic_exchange_test&quot;; public static void main(String[] argv) throws Exception &#123; // 获取到连接 Connection connection = ConnectionUtil.getConnection(); // 获取通道 Channel channel = connection.createChannel(); // 声明队列 channel.queueDeclare(QUEUE_NAME, false, false, false, null); // 绑定队列到交换机，同时指定需要订阅的routing key。订阅 insert、update、delete channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, &quot;item.*&quot;); // 定义队列的消费者 DefaultConsumer consumer = new DefaultConsumer(channel) &#123; // 获取消息，并且处理，这个方法类似事件监听，如果有消息的时候，会被自动调用 @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; // body 即消息体 String msg = new String(body); System.out.println(&quot; [消费者2] received : &quot; + msg + &quot;!&quot;); &#125; &#125;; // 监听队列，自动ACK channel.basicConsume(QUEUE_NAME, true, consumer); &#125;&#125; 三，可靠性投递使用 RabbitMQ 实现异步通信的时候，消息丢了怎么办，消息重复消费怎么办？ 在 RabbitMQ 里面提供了很多保证消息可靠投递的机制，这个也是 RabbitMQ 的一个特性。 首先要明确一个问题，因为效率与可靠性是无法兼得的，如果要保证每一个环节都成功，势必会对消息的收发效率造成影响。所以如果是一些业务实时一致性要求不是特别高的场合，可以牺牲一些可靠性来换取效率。比如发送通知或者记录日志的这种场景，如果用户没有收到通知，不会造成业务影响，只要再次发送就可以了。 在我们使用 RabbitMQ 收发消息的时候，有几个主要环节 ①代表消息从生产者发送到 Broker 生产者把消息发到 Broker 之后，怎么知道自己的消息有没有被 Broker 成功接收？ ②代表消息从 Exchange 路由到 Queue Exchange 是一个绑定列表，如果消息没有办法路由到正确的队列，会发生什么事情？应该怎么处理？ ③代表消息在 Queue 中存储 队列是一个独立运行的服务，有自己的数据库（Mnesia），它是真正用来存储消息的。如果还没有消费者来消费，那么消息要一直存储在队列里面。如果队列出了问题，消息肯定会丢失。怎么保证消息在队列稳定地存储呢？ ④代表消费者订阅 Queue 并消费消息 队列的特性是什么？FIFO。队列里面的消息是一条一条的投递的，也就是说，只有上一条消息被消费者接收以后，才能把这一条消息从数据库删掉，继续投递下一条消息。那么问题来了，Broker 怎么知道消费者已经接收了消息呢？ 1.消息发送到rabbitMQ服务器这个环节可能因为网络或者 Broker 的问题导致消息发送失败，生产者不能确定 Broker 有没有正确的接收。 在 RabbitMQ 里面提供了两种机制服务端确认机制，也就是在生产者发送消息给RabbitMQ 的服务端的时候，服务端会通过某种方式返回一个应答，只要生产者收到了这个应答，就知道消息发送成功了。第一种是 Transaction（事务）模式，第二种 Confirm（确认）模式。 1.1 Transaction模式事务模式怎么使用呢？ 我们通过一个 channel.txSelect()的方法把信道设置成事务模式，然后就可以发布消息给 RabbitMQ 了，如果 channel.txCommit();的方法调用成功，就说明事务提交成功，则消息一定到达了 RabbitMQ 中。 如果在事务提交执行之前由于 RabbitMQ 异常崩溃或者其他原因抛出异常，这个时候我们便可以将其捕获，进而通过执行 channel.txRollback()方法来实现事务回滚。 在事务模式里面，只有收到了服务端的 Commit-OK 的指令，才能提交成功。所以可以解决生产者和服务端确认的问题。但是事务模式有一个缺点，它是阻塞的，一条消息没有发送完毕，不能发送下一条消息，它会榨干 RabbitMQ 服务器的性能。所以不建议大家在生产环境使用。 Spring Boot 中的设置 1rabbitTemplate.setChannelTransacted(true); 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/** * @author yhd * @createtime 2021/1/24 23:29 * rabbitmq事务 */@Componentpublic class MqTx &#123; private static final String EXCHANGE = &quot;exchange.tx&quot;; private static final String QUEUE = &quot;queue.tx&quot;; private static final String ROUTING_KEY = &quot;routing.tx&quot;; @Resource private RabbitTemplate rabbitTemplate; public boolean sendMessage() &#123; rabbitTemplate.setChannelTransacted(true); rabbitTemplate.setConfirmCallback((correlationData, flag, cause) -&gt; &#123; if (flag) &#123; System.out.println(&quot;发送成功！&quot;); &#125; else &#123; System.out.println(&quot;发送失败&quot; + cause); &#125; &#125;); return true; &#125; @RabbitListener(bindings = @QueueBinding( value = @Queue(value = QUEUE, autoDelete = &quot;false&quot;, durable = &quot;true&quot;), exchange = @Exchange(value = EXCHANGE, autoDelete = &quot;true&quot;, durable = &quot;true&quot;), key = &#123;ROUTING_KEY&#125; ) ) public void receiveMessage(String msg, Message message, Channel channel) &#123; try &#123; channel.txSelect(); System.out.println(&quot;msg = &quot; + msg); channel.basicAck(message.getMessageProperties().getDeliveryTag(), false); channel.txCommit(); &#125; catch (IOException e) &#123; try &#123; channel.txRollback(); &#125; catch (IOException ioException) &#123; ioException.printStackTrace(); &#125; &#125; &#125;&#125; 那么有没有其他可以保证消息被 Broker 接收，但是又不大量消耗性能的方式呢？这就是第二种模式，叫做确认（Confirm）模式。 1.2 Confirm （ 确认 ）模式确认模式有三种，一种是普通确认模式。​ 普通确认模式 在生产者这边通过调用 channel.confirmSelect()方法将信道设置为 Confirm 模式，然后发送消息。一旦消息被投递到所有匹配的队列之后，RabbitMQ 就会发送一个确认（Basic.Ack）给生产者，也就是调用 channel.waitForConfirms()返回 true，这样生产者就知道消息被服务端接收了。 批量确认 这种发送 1 条确认 1 条的方式消息还不是太高，所以我们还有一种批量确认的方式。批 量 确 认 ， 就 是 在 开 启 Confirm 模 式 后 ， 先 发 送 一 批 消 息 。 只 要channel.waitForConfirmsOrDie();方法没有抛出异常，就代表消息都被服务端接收了。 批量确认的方式比单条确认的方式效率要高，但是也有两个问题，第一个就是批量的数量的确定。对于不同的业务，到底发送多少条消息确认一次？数量太少，效率提升不上去。数量多的话，又会带来另一个问题，比如我们发 1000 条消息才确认一次，如果前面 999 条消息都被服务端接收了，如果第 1000 条消息被拒绝了，那么前面所有的消息都要重发。 异步确认 ​ 有没有一种方式，可以一边发送一边确认的呢？这个就是异步确认模式。 异步确认模式需要添加一个 ConfirmListener，并且用一个 SortedSet 来维护没有被确认的消息。 Confirm 模式是在 Channel 上开启的，因为 RabbitTemplate 对 Channel 进行了封装，叫做 ConfimrCallback。 123456rabbitTemplate.setConfirmCallback((correlationData, ack, cause) -&gt; &#123; if (!ack) &#123; System.out.println(&quot;发送消息失败：&quot; + cause); throw new RuntimeException(&quot;发送异常：&quot; + cause); &#125;&#125;); 2. 消息从交换机路由到队列在什么情况下，消息会无法路由到正确的队列？可能因为路由键错误，或者队列不存在。 有两种方式处理无法路由的消息，一种就是让服务端重发给生产者，一种是让交换机路由到另一个备份的交换机。 消息回发的方式：使用 mandatory 参数和 ReturnListener（在 Spring AMQP 中是ReturnCallback）。 1234567891011121314rabbitTemplate.setMandatory(true);rabbitTemplate.setReturnCallback((Message message, int replyCode, String replyText, String exchange, String routingKey) -&gt; &#123; // 反序列化对象输出 log.info(&quot;消息主体: &#123;&#125;&quot;, new String(message.getBody())); log.info(&quot;应答码: &#123;&#125;&quot;, replyCode); log.info(&quot;描述：&#123;&#125;&quot;, replyText); log.info(&quot;消息使用的交换器 exchange : &#123;&#125;&quot;, exchange); log.info(&quot;消息使用的路由键 routing : &#123;&#125;&quot;, routingKey);&#125;); 消息路由到备份交换机的方式：在创建交换机的时候，从属性中指定备份交换机。 123456789101112131415161718192021222324252627private static final String EXCHANGE_NAME = &quot;amqp.yhd.exchange&quot;;private static final String EXCHANGE_NAME_COPY = &quot;amqp.yhd.exchange.copy&quot;;private static final String QUEUE_NAME = &quot;amqp.yhd.queue&quot;;private static final String ROUTING_KEY = &quot;amqp.admin&quot;;/** * AmqpAdmin * * @param factory * @return */@Beanpublic AmqpAdmin amqpAdmin(ConnectionFactory factory) &#123; RabbitAdmin admin = new RabbitAdmin(factory); //给交换机指定备份交换机 Map&lt;String,Object&gt; arguments = new HashMap(); arguments.put(&quot;alternate-exchange&quot;,EXCHANGE_NAME_COPY); //声明一个交换机 交换机名 是否持久化 是否自动删除 admin.declareExchange(new DirectExchange(EXCHANGE_NAME, true, false,arguments)); //队列名 持久化 是否批处理 自动删除 admin.declareQueue(new Queue(QUEUE_NAME, true, false, false)); //声明一个绑定 队列名 ，绑定类型，交换机名，路由键 参数 admin.declareBinding(new Binding(QUEUE_NAME, Binding.DestinationType.QUEUE, EXCHANGE_NAME, ROUTING_KEY, null)); return admin;&#125; 队列可以指定死信交换机；交换机可以指定备份交换机 3.消息在队列存储如果没有消费者的话，队列一直存在在数据库中。 如果 RabbitMQ 的服务或者硬件发生故障，比如系统宕机、重启、关闭等等，可能会导致内存中的消息丢失，所以我们要把消息本身和元数据（队列、交换机、绑定）都保存到磁盘。 解决方案队列持久化+交换机持久化+消息持久化 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849@Slf4j@Componentpublic class MQProducerAckTest &#123; @Autowired private RabbitTemplate rabbitTemplate; private static final String EXCHANGE = &quot;exchange.confirm&quot;; private static final String QUEUE = &quot;queue.confirm&quot;; private static final String ROUTING_KEY = &quot;routing.confirm&quot;; @Bean public AmqpAdmin amqpAdmin(ConnectionFactory factory) &#123; RabbitAdmin admin = new RabbitAdmin(factory); //声明一个交换机 交换机名 是否持久化 是否自动删除 admin.declareExchange(new DirectExchange(EXCHANGE, true, false, null)); //队列名 持久化 是否批处理 自动删除 admin.declareQueue(new org.springframework.amqp.core.Queue(QUEUE, true, false, false)); //声明一个绑定 队列名 ，绑定类型，交换机名，路由键 参数 admin.declareBinding(new Binding(QUEUE, Binding.DestinationType.QUEUE, EXCHANGE, ROUTING_KEY, null)); return admin; &#125; /** * 发送消息 * * @param exchange 交换机 * @param routingKey 路由键 * @param message 消息 */ public boolean sendMessage(String exchange, String routingKey, String message) &#123; MessageProperties messageProperties = new MessageProperties(); messageProperties.setDeliveryMode(MessageDeliveryMode.PERSISTENT); Message msg = new Message(message.getBytes(), messageProperties); rabbitTemplate.convertAndSend(exchange, routingKey, msg); return true; &#125; @SneakyThrows @RabbitListener(bindings = @QueueBinding( value = @Queue(value = QUEUE, autoDelete = &quot;false&quot;, durable = &quot;true&quot;), exchange = @Exchange(value = EXCHANGE, autoDelete = &quot;true&quot;, durable = &quot;true&quot;), key = &#123;ROUTING_KEY&#125;)) public void process(Message message, Channel channel) &#123; log.info(&quot;RabbitListener:&#123;&#125;&quot;, new String(message.getBody())); channel.basicAck(message.getMessageProperties().getDeliveryTag(), false); &#125;&#125; 集群 如果只有一个 RabbitMQ 的节点，即使交换机、队列、消息做了持久化，如果服务崩溃或者硬件发生故障，RabbitMQ 的服务一样是不可用的，所以为了提高 MQ 服务的可用性，保障消息的传输，我们需要有多个 RabbitMQ 的节点。 4.消息投递到消费者如果消费者收到消息后没来得及处理即发生异常，或者处理过程中发生异常，会导致接收消息失败。服务端应该以某种方式得知消费者对消息的接收情况，并决定是否重新投递这条消息给其他消费者。RabbitMQ 提供了消费者的消息确认机制（message acknowledgement），消费者可以自动或者手动地发送 ACK 给服务端。 没有收到 ACK 的消息，消费者断开连接后，RabbitMQ 会把这条消息发送给其他消费者。如果没有其他消费者，消费者重启后会重新消费这条消息，重复执行业务逻辑。 消费者在订阅队列时，可以指定autoAck参数，当autoAck等于false时，RabbitMQ会等待消费者显式地回复确认信号后才从队列中移去消息。 如何设置手动 ACK？ SimpleRabbitListenerContainer 或者 SimpleRabbitListenerContainerFactory 1factory.setAcknowledgeMode(AcknowledgeMode.MANUAL); application.properties 12spring.rabbitmq.listener.direct.acknowledge-mode=manualspring.rabbitmq.listener.simple.acknowledge-mode=manual 注意这三个值的区别：NONE：自动 ACK，MANUAL： 手动 ACK，AUTO：如果方法未抛出异常，则发送 ack。 当抛出 AmqpRejectAndDontRequeueException 异常的时候，则消息会被拒绝，且不重新入队。当抛出 ImmediateAcknowledgeAmqpException 异常，则消费者会发送 ACK。其他的异常，则消息会被拒绝，且 requeue = true 会重新入队。 在 Spring Boot 中，消费者又怎么调用 ACK，或者说怎么获得 Channel 参数呢？ 123456789@SneakyThrows@RabbitListener(bindings = @QueueBinding( value = @Queue(value = QUEUE, autoDelete = &quot;false&quot;, durable = &quot;true&quot;), exchange = @Exchange(value = EXCHANGE, autoDelete = &quot;true&quot;, durable = &quot;true&quot;), key = &#123;ROUTING_KEY&#125;))public void process(Message message, Channel channel) &#123; log.info(&quot;RabbitListener:&#123;&#125;&quot;, new String(message.getBody())); channel.basicAck(message.getMessageProperties().getDeliveryTag(), false);&#125; 如果消息无法处理或者消费失败，也有两种拒绝的方式，Basic.Reject()拒绝单条，Basic.Nack()批量拒绝。如果 requeue 参数设置为 true，可以把这条消息重新存入队列，以便发给下一个消费者（当然，只有一个消费者的时候，这种方式可能会出现无限循环重复消费的情况。可以投递到新的队列中，或者只打印异常日志）。 服务端收到了 ACK 或者 NACK，即使消费者没有接收到消息，或者消费时出现异常，生产者也是完全不知情的。 5.消费者回调 调用生产者 API 发送响应消息给生产者 6.补偿机制如果生产者的 API 就是没有被调用，也没有收到消费者的响应消息，怎么办？ 可能是消费者处理时间太长或者网络超时。 生产者与消费者之间应该约定一个超时时间，比如 5 分钟，对于超出这个时间没有得到响应的消息，可以设置一个定时重发的机制，但要发送间隔和控制次数，比如每隔 2分钟发送一次，最多重发 3 次，否则会造成消息堆积。 重发可以通过（本地消息表）消息落库+（异步）定时任务来实现。 7.消息幂等性如果消费者每一次接收生产者的消息都成功了，只是在响应或者调用 API 的时候出了问题，会不会出现消息的重复处理？ 为了避免相同消息的重复处理，必须要采取一定的措施。RabbitMQ 服务端是没有这种控制的（同一批的消息有个递增的 DeliveryTag），它不知道你是不是就要把一条消息发送两次，只能在消费端控制。 导致消息的重复消费的原因： 生产者的问题，环节①重复发送消息，比如在开启了 Confirm 模式但未收到确认，消费者重复投递。 环节④出了问题，由于消费者未发送 ACK 或者其他原因，消息重复投递。 生产者代码或者网络问题。 对于重复发送的消息，可以对每一条消息生成一个唯一的业务 ID，通过日志或者消息落库来做重复控制。 8.最终一致性如果确实是消费者宕机了，或者代码出现了 BUG 导致无法正常消费，在我们尝试多次重发以后，消息最终也没有得到处理，怎么办？手动处理。 9.消息的顺序性消息的顺序性指的是消费者消费消息的顺序跟生产者生产消息的顺序是一致的。 比如：1、发表微博；2、发表评论；3、删除微博。顺序不能颠倒。 在 RabbitMQ 中，一个队列有多个消费者时，由于不同的消费者消费消息的速度是不一样的，顺序无法保证。只有一个队列仅有一个消费者的情况才能保证顺序消费（不同的业务消息发送到不同的专用的队列）。​ 10.代码123456789101112spring.rabbitmq.host=121.199.31.160spring.rabbitmq.port=5672spring.rabbitmq.username=rootspring.rabbitmq.password=root#交换机确认spring.rabbitmq.publisher-confirms=true#队列确认spring.rabbitmq.publisher-returns=true#默认情况下消息消费者是自动确认消息的，如果要手动确认消息则需要修改确认模式为manualspring.rabbitmq.listener.simple.cknowledge-mode=manual# 消费者每次从队列获取的消息数量。此属性当不设置时为：轮询分发，设置为1为：公平分发spring.rabbitmq.listener.simple.prefetch=1 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * @author yhd * @createtime 2021/1/23 1:38 * @description 消息发送确认 * 关于实现的这两个类： * ConfirmCallback：只确认消息是否正确到达Exchange中 * 1.如果消息没有到exchange,则confirm回调,ack=false * 2.如果消息到达exchange,则confirm回调,ack=true * ReturnCallback：消息没有正确到达队列时触发回调，如果正确到达队列不执行 * 1.exchange到queue成功,则不回调return * 2.exchange到queue失败,则回调return */@Component@Slf4jpublic class MQProducerAckConfig implements RabbitTemplate.ConfirmCallback, RabbitTemplate.ReturnCallback&#123; @Resource private RabbitTemplate rabbitTemplate; /** * 修饰一个非静态的void（）方法,在服务器加载Servlet的时候运行， * 并且只会被服务器执行一次。 * 在构造函数之后执行，init（）方法之前执行。 */ @PostConstruct public void init() &#123; rabbitTemplate.setConfirmCallback(this); //指定 ConfirmCallback rabbitTemplate.setReturnCallback(this); //指定 ReturnCallback &#125; @Override public void confirm(CorrelationData correlationData, boolean ack, String cause) &#123; if (ack) &#123; log.info(&quot;消息发送成功：&quot; + GsonUtil.toJson(correlationData)); &#125; else &#123; log.info(&quot;消息发送失败：&quot; + cause + &quot; 数据：&quot; + GsonUtil.toJson(correlationData)); &#125; &#125; @Override public void returnedMessage(Message message, int replyCode, String replyText, String exchange, String routingKey) &#123; // 反序列化对象输出 log.info(&quot;消息主体: &#123;&#125;&quot;,new String(message.getBody())); log.info(&quot;应答码: &#123;&#125;&quot;,replyCode); log.info(&quot;描述：&#123;&#125;&quot;,replyText); log.info(&quot;消息使用的交换器 exchange : &#123;&#125;&quot;,exchange); log.info(&quot;消息使用的路由键 routing : &#123;&#125;&quot;,routingKey); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859/** * @author yhd * @createtime 2021/1/23 1:59 * 测试消息发送确认 */@Slf4j@Componentpublic class MQProducerAckTest &#123; @Autowired private RabbitTemplate rabbitTemplate; private static final String EXCHANGE = &quot;exchange.confirm&quot;; private static final String QUEUE = &quot;queue.confirm&quot;; private static final String ROUTING_KEY = &quot;routing.confirm&quot;; /** * 发送消息 * * @param exchange 交换机 * @param routingKey 路由键 * @param message 消息 */ public boolean sendMessage(String exchange, String routingKey, Object message) &#123; rabbitTemplate.convertAndSend(exchange, routingKey, message); return true; &#125; //接收消息 @SneakyThrows @RabbitListener(bindings = @QueueBinding( value = @Queue(value = QUEUE, autoDelete = &quot;false&quot;), exchange = @Exchange(value = EXCHANGE, autoDelete = &quot;true&quot;), key = &#123;ROUTING_KEY&#125;)) public void process(Message message, Channel channel) &#123; log.info(&quot;RabbitListener:&#123;&#125;&quot;, new String(message.getBody())); // 采用手动应答模式, 手动确认应答更为安全稳定 //如果手动确定了，再出异常，mq不会通知；如果没有手动确认，抛异常mq会一直通知 try &#123; int i = 1 / 0; // false 确认一个消息，true 批量确认 channel.basicAck(message.getMessageProperties().getDeliveryTag(), false); &#125; catch (Exception e) &#123; // 消息是否再次被拒绝！ System.out.println(&quot;come on!&quot;); // getRedelivered() 判断是否已经处理过一次消息！ if (message.getMessageProperties().getRedelivered()) &#123; System.out.println(&quot;消息已重复处理,拒绝再次接收&quot;); // 拒绝消息，requeue=false 表示不再重新入队，如果配置了死信队列则进入死信队列 channel.basicReject(message.getMessageProperties().getDeliveryTag(), false); &#125; else &#123; System.out.println(&quot;消息即将再次返回队列处理&quot;); // 参数二：是否批量， 参数三：为是否重新回到队列，true重新入队 channel.basicNack(message.getMessageProperties().getDeliveryTag(), false, true); &#125; &#125; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263/** * @author yhd * @createtime 2021/1/24 23:29 * rabbitmq事务 */@Componentpublic class MqTx &#123; private static final String EXCHANGE = &quot;exchange.tx&quot;; private static final String QUEUE = &quot;queue.tx&quot;; private static final String ROUTING_KEY = &quot;routing.tx&quot;; @Resource private RabbitTemplate rabbitTemplate; @Resource private TransactionTemplate transactionTemplate; public boolean sendMessage() &#123; rabbitTemplate.setConfirmCallback((correlationData, flag, cause) -&gt; &#123; if (flag) &#123; System.out.println(&quot;发送成功！&quot;); &#125; else &#123; System.out.println(&quot;发送失败&quot; + cause); &#125; &#125;); rabbitTemplate.setReturnCallback((RabbitTemplate.ReturnsCallback) returnedMessage -&gt; &#123; &#125;); transactionTemplate.execute(transactionStatus -&gt; &#123; rabbitTemplate.convertAndSend(EXCHANGE, ROUTING_KEY, &quot;rabbitmq-tx&quot;); return rabbitTemplate.receiveAndConvert(); &#125;); return true; &#125; @RabbitListener(bindings = @QueueBinding( value = @Queue(value = QUEUE, autoDelete = &quot;false&quot;, durable = &quot;true&quot;), exchange = @Exchange(value = EXCHANGE, autoDelete = &quot;true&quot;, durable = &quot;true&quot;), key = &#123;ROUTING_KEY&#125; ) ) public void receiveMessage(String msg, Message message, Channel channel) &#123; try &#123; channel.txSelect(); System.out.println(&quot;msg = &quot; + msg); channel.basicAck(message.getMessageProperties().getDeliveryTag(), false); channel.txCommit(); &#125; catch (IOException e) &#123; try &#123; channel.txRollback(); &#125; catch (IOException ioException) &#123; ioException.printStackTrace(); &#125; &#125; &#125;&#125; 四，消息流控1.服务端流控当生产 MQ 消息的速度远大于消费消息的速度时，会产生大量的消息堆积，占用系统资源，导致机器的性能下降。我们想要控制服务端接收的消息的数量，应该怎么做呢？ 1.1设置队列长度队列有两个控制长度的属性 12x-max-length：队列中最大存储最大消息数，超过这个数量，队头的消息会被丢弃。x-max-length-bytes：队列中存储的最大消息容量（单位 bytes），超过这个容量，队头的消息会被丢弃。 设置队列长度只在消息堆积的情况下有意义，而且会删除先入队的消息，不能真正地实现服务端限流。 1.2.内存控制RabbitMQ 会在启动时检测机器的物理内存数值。默认当 MQ 占用 40% 以上内存时，MQ 会主动抛出一个内存警告并阻塞所有连接。可以通过修改rabbitmq.config 文件来调整内存阈值，默认值是 0.4。 1[&#123;rabbit, [&#123;vm_memory_high_watermark, 0.4&#125;]&#125;] 也可以用命令动态设置，如果设置成 0，则所有的消息都不能发布。 1rabbitmqctl set_vm_memory_high_watermark 0.3 1.3磁盘控制通过磁盘来控制消息的发布。当磁盘空间低于指定的值时（默认50MB），触发流控措施。 例如：指定为磁盘的 30%或者 2GB 12disk_free_limit.relative = 3.0disk_free_limit.absolute = 2GB 2.消费端限流2.1官网描述默认情况下，如果不进行配置，RabbitMQ 会尽可能快速地把队列中的消息发送到消费者。因为消费者会在本地缓存消息，如果消息数量过多，可能会导致 OOM 或者影响其他进程的正常运行。 在消费者处理消息的能力有限，例如消费者数量太少，或者单条消息的处理时间过长的情况下，如果我们希望在一定数量的消息消费完之前，不再推送消息过来，就要用到消费端的流量限制措施。 可以基于 Consumer 或者 channel 设置 prefetch count 的值，含义为 Consumer端的最大的 unacked messages 数目。当超过这个数值的消息未被确认，RabbitMQ 会停止投递新的消息给该消费者。 2.2代码配置RabbitMQ12channel.basicQos(2); // 如果超过 2 条消息没有发送 ACK，当前消费者不再接受队列消息channel.basicConsume(QUEUE_NAME, false, consumer); SimpleMessageListenerContainer 1container.setPrefetchCount(2); Spring Boot 配置1spring.rabbitmq.listener.simple.prefetch=2 channel 的 prefetch count 设置为 5。当消费者有 5 条消息没有给 Broker 发送 ACK后，RabbitMQ 不再给这个消费者投递消息。 3.消息积压，丢失生产环境中，如果消息在队列和交换机发生积压，并已经开始丢失，应该怎么处理？ 临时扩容消费者，先保证现有的业务逻辑，丢失的消息，等待流量高峰期过后，利用程序排查出来，重新灌入MQ队列。也可以考虑将消息临时写入到一个新的topic里，缓解原本的队列压力。 其实还有broker，消息都是存磁盘，但是MQ高吞吐量一个很重要的原因是利用了page Cache ，数据量没特别大的情况下，mq发消息到broker磁盘，此时broker的page cache 中其实也是有这份消息的，当生产者正常消费时，大概率是直接可以从page cache 中拉消息，这个速度是内存级别，page cache没有拉到消息采取磁盘，当消息堆积在broker时，说明生产者生产速度过快，消费者消费不过来，这时broker的page cache被大量的更新，导致消费者拉消息都是去磁盘去读取，page cache失效了，所以扩容消费者数量有用，但是还需要扩容broker的数量。 五，集群与高可用1.为什么要做集群集群主要用于实现高可用与负载均衡。 高可用：如果集群中的某些 MQ 服务器不可用，客户端还可以连接到其他 MQ 服务器。 负载均衡：在高并发的场景下，单台 MQ 服务器能处理的消息有限，可以分发给多台 MQ 服务器。 RabbitMQ 有两种集群模式：普通集群模式和镜像队列模式。 2.RabbitMQ 如何支持集群应用做集群，需要面对数据同步和通信的问题。因为 Erlang 天生具备分布式的特性，所以 RabbitMQ 天然支持集群，不需要通过引入 ZK 或者数据库来实现数据同步。 RabbitMQ 通过/var/lib/rabbitmq/.erlang.cookie 来验证身份，需要在所有节点上保持一致。 3.rabbitMQ的节点类型集群有两种节点类型，一种是磁盘节点（Disc Node），一种是内存节点（RAMNode）。 磁盘节点：将元数据（包括队列名字属性、交换机的类型名字属性、绑定、vhost）放在磁盘中。 内存节点：将元数据放在内存中。 内存节点会将磁盘节点的地址存放在磁盘（不然重启后就没有办法同步数据了）。如果是持久化的消息，会同时存放在内存和磁盘。 集群中至少需要一个磁盘节点用来持久化元数据，否则全部内存节点崩溃时，就无从同步元数据。未指定类型的情况下，默认为磁盘节点。 我们一般把应用连接到内存节点（读写快），磁盘节点用来备份。 集群通过 25672 端口两两通信，需要开放防火墙的端口。 RabbitMQ 集群无法搭建在广域网上 集群的配置步骤 配置 hosts 同步 erlang.cookie 加入集群（join cluster） 4.普通集群普通集群模式下，不同的节点之间只会相互同步元数据。 为什么不直接把队列的内容（消息）在所有节点上复制一份？ 主要是出于存储和同步数据的网络开销的考虑，如果所有节点都存储相同的数据，就无法达到线性地增加性能和存储容量的目的（堆机器）。 假如生产者连接的是节点 3，要将消息通过交换机 A 路由到队列 1，最终消息还是会转发到节点 1 上存储，因为队列 1 的内容只在节点 1 上。 同理，如果消费者连接是节点 2，要从队列 1 上拉取消息，消息会从节点 1 转发到节点 2。其它节点起到一个路由的作用，类似于指针。 普通集群模式不能保证队列的高可用性，因为队列内容不会复制。如果节点失效将导致相关队列不可用，因此我们需要第二种集群模式。 5.镜像集群第二种集群模式叫做镜像队列。 镜像队列模式下，消息内容会在镜像节点间同步，可用性更高。不过也有一定的副作用，系统性能会降低，节点过多的情况下同步的代价比较大。 6.高可用集群搭建成功后，如果有多个内存节点，那么生产者和消费者应该连接到哪个内存节点？如果在我们的代码中根据一定的策略来选择要使用的服务器，那每个地方都要修改，客户端的代码就会出现很多的重复，修改起来也比较麻烦。 所以需要一个负载均衡的组件（例如 HAProxy，LVS，Nignx），由负载的组件来做路由。这个时候，只需要连接到负载组件的 IP 地址就可以了。 负载分为四层负载和七层负载。 四层负载：工作在 OSI 模型的第四层，即传输层（TCP 位于第四层），它是根据 IP端口进行转发（LVS 支持四层负载）。RabbitMQ 是 TCP 的 5672 端口。 （修改报文中目标地址和原地址） 七层负载：工作在第七层，应用层（HTTP 位于第七层）。可以根据请求资源类型分配到后端服务器（Nginx 支持七层负载；HAProxy 支持四层和七层负载）。 （处理请求，代理至服务器） 但是，如果这个负载的组件也挂了呢？ 我们应该需要这样一个组件 它本身有路由（负载）功能，可以监控集群中节点的状态（比如监控HAProxy），如果某个节点出现异常或者发生故障，就把它剔除掉。 为了提高可用性，它也可以部署多个服务，但是只有一个自动选举出来的 MASTER 服务器（叫做主路由器），通过广播心跳消息实现。 MASTER 服务器对外提供一个虚拟 IP，提供各种网络功能。也就是谁抢占到 VIP，就由谁对外提供网络服务。应用端只需要连接到这一个 IP 就行了。 这个协议叫做 VRRP 协议（虚拟路由冗余协议 Virtual Router RedundancyProtocol），这个组件就是 Keepalived，它具有 Load Balance 和 High Availability的功能。​ 六，MQ延迟消息的实现​ mq实现延迟消息有两种方式，一种是基于死信队列，一种是基于延迟插件。​ 1.基于死信队列1.1.理论 消息的TTL 消息的存活时间。RabbitMQ可以对队列和消息分别设置TTL。对队列设置就是队列没有消费者连着的保留时间，也可以对每一个单独的消息做单独的设置。超过了这个时间，我们认为这个消息就死了，称之为死信。如何设置TTL 我们创建一个队列queue.temp，在Arguments 中添加x-message-ttl 为5000 （单位是毫秒），那所在压在这个队列的消息在5秒后会消失。死信交换机 Dead Letter Exchange其实就是一种普通的exchange，和创建其他exchange没有两样。只是在某一个设置Dead Letter Exchange的队列中有消息过期了，会自动触发消息的转发，发送到Dead Letter Exchange中去。 何时进入死信路由 一个消息被Consumer拒收了，并且reject方法的参数里requeue是false。也就是说不会被再次放在队列里，被其他消费者使用。 上面的消息的TTL到了，消息过期了。 队列的长度限制满了。排在前面的消息会被丢弃或者扔到死信路由上。 1.2 代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/** * @author yhd * @createtime 2021/1/23 2:26 * @description 死信交换机配置类 */@SpringBootConfigurationpublic class DeadLetterMqConfig &#123; public static final String EXCHANGE_DEAD = &quot;exchange.dead&quot;; public static final String ROUTING_DEAD_1 = &quot;routing.dead.1&quot;; public static final String ROUTING_DEAD_2 = &quot;routing.dead.2&quot;; public static final String QUEUE_DEAD_1 = &quot;queue.dead.1&quot;; public static final String QUEUE_DEAD_2 = &quot;queue.dead.2&quot;; // 定义交换机 @Bean public DirectExchange exchange()&#123; return new DirectExchange(EXCHANGE_DEAD,true,false,null); &#125; @Bean public Queue queue1()&#123; // 设置如果队列一 出现问题，则通过参数转到EXCHANGE_DEAD，ROUTING_DEAD_2 上！ Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(); // 参数绑定 此处的key 固定值，不能随意写 map.put(&quot;x-dead-letter-exchange&quot;,EXCHANGE_DEAD); map.put(&quot;x-dead-letter-routing-key&quot;,ROUTING_DEAD_2); // 设置延迟时间 map.put(&quot;x-message-ttl&quot;, 10 * 1000); // 队列名称，是否持久化，是否独享、排外的【true:只可以在本次连接中访问】，是否自动删除，队列的其他属性参数 return new Queue(QUEUE_DEAD_1,true,false,false,map); &#125; @Bean public Binding binding()&#123; // 将队列一 通过ROUTING_DEAD_1 key 绑定到EXCHANGE_DEAD 交换机上 return BindingBuilder.bind(queue1()).to(exchange()).with(ROUTING_DEAD_1); &#125; // 这个队列二就是一个普通队列 @Bean public Queue queue2()&#123; return new Queue(QUEUE_DEAD_2,true,false,false,null); &#125; // 设置队列二的绑定规则 @Bean public Binding binding2()&#123; // 将队列二通过ROUTING_DEAD_2 key 绑定到EXCHANGE_DEAD交换机上！ return BindingBuilder.bind(queue2()).to(exchange()).with(ROUTING_DEAD_2); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package com.yhd.rabbitmq.dead;import com.rabbitmq.client.Channel;import lombok.SneakyThrows;import lombok.extern.slf4j.Slf4j;import org.springframework.amqp.core.Message;import org.springframework.amqp.rabbit.annotation.RabbitListener;import org.springframework.amqp.rabbit.core.RabbitTemplate;import org.springframework.stereotype.Component;import javax.annotation.Resource;/** * @author yhd * @createtime 2021/1/23 2:28 */@Component@Slf4jpublic class DeadLetterMqTest &#123; @Resource private RabbitTemplate rabbitTemplate; /** * 消息发送 */ public boolean sendMsg()&#123; rabbitTemplate.convertAndSend(DeadLetterMqConfig.EXCHANGE_DEAD, DeadLetterMqConfig.ROUTING_DEAD_1, &quot;ok&quot;); return true; &#125; /** * 消息接收 * 正常交换机 */ @SneakyThrows @RabbitListener(queues = DeadLetterMqConfig.QUEUE_DEAD_2) public void get(String msg, Message message, Channel channel) &#123; channel.basicNack(message.getMessageProperties().getDeliveryTag(),false,false); log.info(&quot;正常队列接受消息： &#123;&#125;&quot; , msg); &#125; /** * 消息接收 * 死信交换机 * @param msg * @param message * @param channel */ @SneakyThrows @RabbitListener(queues = DeadLetterMqConfig.QUEUE_DEAD_1) public void get2(String msg, Message message, Channel channel) &#123; log.info(&quot;死信队列接收消息: &#123;&#125;&quot; , msg); channel.basicAck(message.getMessageProperties().getDeliveryTag(),false); &#125;&#125; 2.基于延迟插件2.1.理论Rabbitmq实现了一个插件x-delay-message来实现延时队列。​ 基于插件的延迟消息可能有一个小bug（不影响业务），就是生产者发送消息时会回调returnedMessage方法（消息确认时我们配置的回调方法，表示交换机到队列发送失败），其实基于插件的延迟消息是发送成功了的，如果发生该bug，我们可以根据交换机或队列过滤掉该消息，别让他加入重试队列；如果不能接受后续业务我们可以使用死信的方式发送延迟消息。 插件安装​https://www.rabbitmq.com/community-plugins.html 将插件拷贝到plugins目录下 进入plugins目录 执行 rabbitmq-plugins enable rabbitmq_delayed_message_exchange 命令启用插件 重启 rabbitmq 队列不要在RabbitListener上面做绑定，否则不会成功，必须在配置类绑定。 2.2 代码12345678910111213141516171819202122232425262728293031323334/** * @author yhd * @createtime 2021/1/22 14:37 */@SpringBootApplicationpublic class DelayConfig &#123; //延时交换机 public static final String EXCHANGE_DIRECT_ORDER_CANCEL = &quot;spring.boot.test.delay.exchange&quot;; //路由键 public static final String ROUTING_ORDER_CANCEL = &quot;spring.boot.test.delay.routing&quot;; //延迟队列 public static final String QUEUE_ORDER_CANCEL = &quot;spring.boot.test.delay.queue&quot;; // 延迟时间 单位：秒 public static final int DELAY_TIME = 60; @Bean //声明死信队列 public Queue delayQueue() &#123; // 第一个参数是创建的queue的名字，第二个参数是是否支持持久化 return new Queue(QUEUE_ORDER_CANCEL, true); &#125; @Bean //声明私信交换机 public CustomExchange delayExchange() &#123; Map&lt;String, Object&gt; args = new HashMap&lt;String, Object&gt;(); args.put(&quot;x-delayed-type&quot;, &quot;direct&quot;); return new CustomExchange(EXCHANGE_DIRECT_ORDER_CANCEL, &quot;x-delayed-message&quot;, true, false, args); &#125; @Bean //死信交换机绑定死信队列并设置路由键 public Binding bindingDelay() &#123; return BindingBuilder.bind(delayQueue()).to(delayExchange()).with(ROUTING_ORDER_CANCEL).noargs(); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839/** * @author yhd * @createtime 2021/1/22 14:34 * 测试springboot整合mq利用死信队列发送消息并接收 */@Component@Slf4jpublic class SpringBootDelayQueueTest &#123; @Resource private RabbitTemplate rabbitTemplate; @Resource private AmqpTemplate amqpTemplate; /** * 发送消息 */ public void sendMessage() &#123; amqpTemplate.convertAndSend( DelayConfig.EXCHANGE_DIRECT_ORDER_CANCEL,DelayConfig.ROUTING_ORDER_CANCEL, &quot;try send message to delay queue !&quot;, msg -&gt; &#123; msg.getMessageProperties().setDelay(DelayConfig.DELAY_TIME * 1000); return msg; &#125;); &#125; /** * 接收消息 */ @RabbitListener(queues = DelayConfig.QUEUE_ORDER_CANCEL) public void receiveMessage(String msg, Message message, Channel channel) throws Exception &#123; log.info(&quot;the delaty queue received message : &#123;&#125;&quot;, msg); //log.info(&quot;the delaty queue received message : &#123;&#125;&quot;, new String(message.getBody())); channel.basicAck(message.getMessageProperties().getDeliveryTag(), false); &#125;&#125; 七，面试经验总结1.到底在消费者创建还是在生产者创建？如果A项目和B项目有相互发送和接收消息，应该创建几个vhost，几个Exchange？ 交换机和队列，实际上是作为资源，由运维管理员创建的。 2.信息落库+ 定时任务将需要发送的消息保存在数据库中，可以实现消息的可追溯和重复控制，需要配合定时任务来实现。 将需要发送的消息登记在消息表中。 定时任务一分钟或半分钟扫描一次，将未发送的消息发送到 MQ 服务器，并且修改状态为已发送。 如果需要重发消息，将指定消息的状态修改为未发送即可。 副作用：降低效率，浪费存储空间。 3.日志追踪RabbitMQ 可以通过 Firehose 功能来记录消息流入流出的情况，用于调试，排错。 它是通过创建一个 TOPIC 类型的交换机（amq.rabbitmq.trace），把生产者发送给Broker 的消息或者 Broker 发送给消费者的消息发到这个默认的交换机上面来实现的。 另外 RabbitMQ 也提供了一个 Firehose 的 GUI 版本，就是 Tracing 插件。 启用 Tracing 插件后管理界面右侧选项卡会多一个 Tracing，可以添加相应的策略。 RabbitMQ 还提供了其他的插件来增强功能。 4.如何减少连接数在发送大批量消息的情况下，创建和释放连接依然有不小的开销。我们可以跟接收方约定批量消息的格式，比如支持 JSON 数组的格式，通过合并消息内容，可以减少生产者/消费者与 Broker 的连接。 比如：活动过后，要全范围下线产品，通过 Excel 导入模板，通常有几万到几十万条解绑数据，合并发送的效率更高。 建议单条消息不要超过 4M（4096KB），一次发送的消息数需要合理地控制。 5.无法被路由的消息，去了哪里？直接丢弃。可用备份交换机（alternate-exchange）接收。 6.大量消息堆积怎么办？ 重启（不是开玩笑的） 多创建几个消费者同时消费 直接清空队列，重发消息 7.设计一个 MQ，你的思路是什么？存储与转发。 存储：内存：用什么数据结构？ 磁盘：文件系统？数据库？ 通信：通信协议（TCP HTTP AMQP ）？一对一？一对多？一对多 推模式？拉模式？后者 其他特性……​ 八，Spring AMQP1.简单使用Spring-amqp是对AMQP协议的抽象实现，而spring-rabbit是对协议的具体实现，也是目前的唯一实现。底层使用的就是RabbitMQ。 1.1依赖1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt;&lt;/dependency&gt; 1.2 配置1234567spring: rabbitmq: host: 121.199.31.160 username: root password: root virtual-host: /shopping port: 5672 1.3 监听者在SpringAmqp中，对消息的消费者进行了封装和抽象，一个普通的JavaBean中的普通方法，只要通过简单的注解，就可以成为一个消费者。​ 1234567891011121314151617@Componentpublic class Listener &#123; @RabbitListener(bindings = @QueueBinding( //value：队列名 value = @Queue(value = &quot;spring.test.queue&quot;, durable = &quot;true&quot;), exchange = @Exchange( //交换机名 value = &quot;spring.test.exchange&quot;, ignoreDeclarationExceptions = &quot;true&quot;, type = ExchangeTypes.TOPIC ), key = &#123;&quot;#.#&quot;&#125;)) public void listen(String msg)&#123; System.out.println(&quot;接收到消息：&quot; + msg); &#125;&#125; @Componet：类上的注解，注册到Spring容器 @RabbitListener：方法上的注解，声明这个方法是一个消费者方法，需要指定下面的属性： bindings：指定绑定关系，可以有多个。值是@QueueBinding的数组。@QueueBinding包含下面属性： value：这个消费者关联的队列。值是@Queue，代表一个队列 exchange：队列所绑定的交换机，值是@Exchange类型 key：队列和交换机绑定的RoutingKey durable = “true” 代表持久化 ignoreDeclarationExceptions = “true”, 异常情况是否忽略 类似listen这样的方法在一个类中可以写多个，就代表多个消费者。​ 1.4AmqpTemplateSpring为AMQP提供了统一的消息处理模板：AmqpTemplate，非常方便的发送消息，其发送方法红框圈起来的是比较常用的3个方法，分别是： 指定消息 指定RoutingKey和消息，会向默认的交换机发送消息 指定交换机、RoutingKey和消息体 测试代码：​ 123456789101112131415@RunWith(SpringRunner.class)@SpringBootTest(classes = Application.class)public class MqDemo &#123; @Autowired private AmqpTemplate amqpTemplate; @Test public void testSend() throws InterruptedException &#123; String msg = &quot;hello, Spring boot amqp&quot;; this.amqpTemplate.convertAndSend(&quot;spring.test.exchange&quot;, &quot;a.b&quot;, msg); // 等待10秒后再结束 Thread.sleep(10000); &#125;&#125; 2.思考​ Java API 方式编程，有什么问题？ Spring 封装 RabbitMQ 的时候，它做了什么事情？ 管理对象（队列、交换机、绑定） 封装方法（发送消息、接收消息） Spring AMQP 是对 Spring 基于 AMQP 的消息收发解决方案，它是一个抽象层，不依赖于特定的 AMQP Broker 实现和客户端的抽象，所以可以很方便地替换。比如我们可以使用 spring-rabbit 来实现。​ 3.Spring AMQP核心组件3.1 ConnectionFactorySpring AMQP 的连接工厂接口，用于创建连接。CachingConnectionFactory 是ConnectionFactory 的一个实现类。 3.2 RabbitAdminRabbitAdmin 是 AmqpAdmin 的实现，封装了对 RabbitMQ 的基础管理操作，比如对交换机、队列、绑定的声明和删除等。 123456789101112131415161718192021222324252627282930313233343536/** * @author yhd * @createtime 2021/1/28 19:24 * @description Spring AMQP configuration class */@SpringBootConfigurationpublic class AMQPConfig &#123; private static final String EXCHANGE_NAME = &quot;amqp.yhd.exchange&quot;; private static final String QUEUE_NAME = &quot;amqp.yhd.queue&quot;; private static final String ROUTING_KEY = &quot;amqp.admin&quot;; @Bean public ConnectionFactory factory() &#123; CachingConnectionFactory factory = new CachingConnectionFactory(); factory.setAddresses(&quot;121.199.31.160&quot;); factory.setPort(5672); factory.setUsername(&quot;root&quot;); factory.setPassword(&quot;root&quot;); return factory; &#125; @Bean public AmqpAdmin amqpAdmin( ConnectionFactory factory) &#123; RabbitAdmin admin = new RabbitAdmin(factory); //声明一个交换机 交换机名 是否持久化 是否自动删除 admin.declareExchange(new DirectExchange(EXCHANGE_NAME, true, false)); //队列名 持久化 是否批处理 自动删除 admin.declareQueue(new Queue(QUEUE_NAME, true, false, false)); //声明一个绑定 队列名 ，绑定类型，交换机名，路由键 参数 admin.declareBinding(new Binding(QUEUE_NAME, Binding.DestinationType.QUEUE, EXCHANGE_NAME, ROUTING_KEY, null)); return admin; &#125;&#125; 为什么我们在配置文件（Spring）或者配置类（SpringBoot）里面定义了交换机、队列、绑定关系，并没有直接调用 Channel 的 declare 的方法，Spring 在启动的时候就可以帮我们创建这些元数据？这些事情就是由 RabbitAdmin 完成的。 RabbitAdmin 实 现 了 InitializingBean 接 口 ， 里 面 有 唯 一 的 一 个 方 法afterPropertiesSet()，这个方法会在 RabbitAdmin 的属性值设置完的时候被调用。 在 afterPropertiesSet ()方法中，调用了一个 initialize()方法。这里面创建了三个Collection，用来盛放交换机、队列、绑定关系。 最后依次声明返回类型为 Exchange、Queue 和 Binding 这些 Bean，底层还是调用了 Channel 的 declare 的方法。 123declareExchanges(channel, exchanges.toArray(new Exchange[exchanges.size()]));declareQueues(channel, queues.toArray(new Queue[queues.size()]));declareBindings(channel, bindings.toArray(new Binding[bindings.size()])); 3.3 MessageMessage 是 Spring AMQP 对消息的封装。两个重要的属性：body：消息内容。 messageProperties：消息属性。 3.4 RabbitTemplate 消息模板RabbitTemplate 是 AmqpTemplate 的一个实现（目前为止也是唯一的实现），用来简化消息的收发，支持消息的确认（Confirm）与返回（Return）。跟 JDBCTemplate一 样 ， 它 封 装 了 创 建 连 接 、 创 建 消 息 信 道 、 收 发 消 息 、 消 息 格 式 转 换（ConvertAndSend→Message）、关闭信道、关闭连接等等操作。 针对于多个服务器连接，可以定义多个 Template。可以注入到任何需要收发消息的地方使用。 123456789101112131415161718192021222324252627/** * return callback &amp;&amp; confirm callable * * @param factory * @return */@Beanpublic RabbitTemplate rabbitTemplate(ConnectionFactory factory) &#123; RabbitTemplate template = new RabbitTemplate(factory); template.setMandatory(true); template.setReturnCallback((Message message, int replyCode, String replyText, String exchange, String routingKey) -&gt; &#123; &#125;); template.setConfirmCallback((CorrelationData correlationData, boolean ack, String cause) -&gt; &#123; if (ack) &#123; log.info(&quot;消息确认成功！&quot;); &#125; else &#123; log.info(&quot;消息确认失败！&quot;); &#125; &#125;); return template;&#125; 3.5 Messager Listener 消息监听MessageListenerMessageListener 是 Spring AMQP 异步消息投递的监听器接口，它只有一个方法onMessage，用于处理消息队列推送来的消息，作用类似于 Java API 中的 Consumer。 MessageListenerContainerMessageListenerContainer可以理解为MessageListener的容器，一个Container只有一个 Listener，但是可以生成多个线程使用相同的 MessageListener 同时消费消息。 Container 可以管理 Listener 的生命周期，可以用于对于消费者进行配置。 例如：动态添加移除队列、对消费者进行设置，例如 ConsumerTag、Arguments、并发、消费者数量、消息确认模式等等。 1234567891011121314151617181920212223/** * 消息监听器容器 * @param connectionFactory * @return */@Beanpublic SimpleMessageListenerContainer messageContainer(ConnectionFactory connectionFactory) &#123; SimpleMessageListenerContainer container = new SimpleMessageListenerContainer(connectionFactory); //监听的队列 container.setQueues(new Queue(QUEUE_NAME, true, false, false)); // 最小消费者数 container.setConcurrentConsumers(1); // 最大的消费者数量 container.setMaxConcurrentConsumers(5); //是否重回队列 container.setDefaultRequeueRejected(false); //签收模式 container.setAcknowledgeMode(AcknowledgeMode.AUTO); container.setExposeListenerChannel(true); //消费端的标签策略 container.setConsumerTagStrategy(queue -&gt; queue + &quot;_&quot; + UUID.randomUUID().toString()); return container;&#125; 在 SpringBoot2.0 中新增了一个 DirectMessageListenerContainer。 MessageListenerContainerFactory Spring 去整合 IBM MQ、JMS、Kafka 也是这么做的。 1234567891011121314/** * * @param connectionFactory * @return */@Beanpublic SimpleRabbitListenerContainerFactory rabbitListenerContainerFactory(ConnectionFactory connectionFactory) &#123; SimpleRabbitListenerContainerFactory factory = new SimpleRabbitListenerContainerFactory(); factory.setConnectionFactory(connectionFactory); factory.setMessageConverter(new Jackson2JsonMessageConverter()); factory.setAcknowledgeMode(AcknowledgeMode.NONE); factory.setAutoStartup(true); return factory;&#125; 可以在消费者上指定，当我们需要监听多个 RabbitMQ 的服务器的时候，指定不同的 MessageListenerContainerFactory。​ 1234567891011@Slf4j@Component@PropertySource(&quot;classpath:application.properties&quot;)@RabbitListener(queues = &quot;$&#123;amqp.yhd.queue&#125;&quot;, containerFactory = &quot;rabbitListenerContainerFactory&quot;)public class FirstConsumer &#123; @RabbitHandler public void process(@Payload String message) &#123; log.info(&quot;First Queue received msg : &#123;&#125;&quot;, message); &#125;&#125; 3.6 转换器 MessageConvertorMessageConvertor 的 作用？ RabbitMQ 的消息在网络传输中需要转换成 byte[]（字节数组）进行发送，消费者需要对字节数组进行解析。 在 Spring AMQP 中，消息会被封装为 org.springframework.amqp.core.Message对象。消息的序列化和反序列化，就是处理 Message 的消息体 body 对象。 如果消息已经是 byte[]格式，就不需要转换。 如果是 String，会转换成 byte[]。 如果是 Java 对象，会使用 JDK 序列化将对象转换为 byte[]（体积大，效率差）。 在 调 用 RabbitTemplate 的 convertAndSend() 方 法 发 送 消 息 时 ， 会 使 用MessageConvertor 进行消息的序列化，默认使用 SimpleMessageConverter。 在某些情况下，我们需要选择其他的高效的序列化工具。如果我们不想在每次发送消息时自己处理消息，就可以直接定义一个 MessageConvertor。 123456@Beanpublic RabbitTemplate rabbitTemplate(final ConnectionFactory connectionFactory) &#123; final RabbitTemplate rabbitTemplate = new RabbitTemplate(connectionFactory); rabbitTemplate.setMessageConverter(new Jackson2JsonMessageConverter()); return rabbitTemplate;&#125; MessageConvertor 如何 工作？ 调 用 了 RabbitTemplate 的 convertAndSend() 方 法 时 会 使 用 对 应 的MessageConvertor 进行消息的序列化和反序列化。 序列化：Object —— Json —— Message(body) —— byte[] 反序列化：byte[] ——Message —— Json —— Object 有 哪些 MessageConvertor ？ 在 Spring 中提供了一个默认的转换器：SimpleMessageConverter。 Jackson2JsonMessageConverter（RbbitMQ 自带）：将对象转换为 json，然后再转换成字节数组进行传递。 如何 自定义 MessageConverter ？ 例如：我们要使用 Gson 格式化消息： 创建一个类，实现 MessageConverter 接口，重写 toMessage()和 fromMessage()方法。 12toMessage(): Java 对象转换为 MessagefromMessage(): Message 对象转换为 Java 对象 4.SpringBoot集成RabbitMQ为什么没有定义 Spring AMQP 的任何一个对象，也能实现消息的收发？Spring Boot 做了什么？ 老套路 源码：RabbitAutoConfiguration​","categories":[{"name":"消息队列","slug":"消息队列","permalink":"https://yinhuidong.github.io/categories/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"https://yinhuidong.github.io/tags/RabbitMQ/"}]},{"title":"Spring Data Jpa使用篇","slug":"MyBatis/Spring Data Jpa使用篇","date":"2022-01-11T06:37:31.534Z","updated":"2022-01-11T06:41:57.741Z","comments":true,"path":"2022/01/11/MyBatis/Spring Data Jpa使用篇/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/MyBatis/Spring%20Data%20Jpa%E4%BD%BF%E7%94%A8%E7%AF%87/","excerpt":"","text":"一，orm思想和hibernate以及jpa1.orm思想 主要目的：操作实体类就相当于操作数据库表 建立两个映射关系 `实体类`和 `表` 实体类中的属性 和 表中字段的映射关系 不再重点关注： SQL语句 实现了ORM思想的框架：mybatis，hibernate 2. hibernate框架介绍Hibernate是一个开放源代码的对象关系映射框架，它对JDBC进行了非常轻量级的对象封装，它将POJO与数据库表建立映射关系，是一个全自动的orm框架，hibernate可以自动生成SQL语句，自动执行，使得Java程序员可以随心所欲的使用对象编程思维来操纵数据库。 3.jpa规范 JPA的全称是Java Persistence API， 即Java 持久化API，是SUN公司推出的一套基于ORM的规范，内部是由一系列的接口和抽象类构成。 JPA通过JDK 5.0注解描述对象－关系表的映射关系，并将运行期的实体对象持久化到数据库中。 JPA与hibernate的关系 JPA和Hibernate的关系就像JDBC和JDBC驱动的关系，JPA是规范，Hibernate除了作为ORM框架之外，它也是一种JPA实现。JPA怎么取代Hibernate呢？JDBC规范可以驱动底层数据库吗？答案是否定的，也就是说，如果使用JPA规范进行数据库操作，底层需要hibernate作为其实现类完成数据持久化工作。 4.jpa的入门案例1.案例：是客户的相关操作（增删改查） 客户：就是一家公司 2.创建项目导入依赖123456789101112131415161718192021222324252627282930313233343536373839404142&lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.hibernate.version&gt;5.0.7.Final&lt;/project.hibernate.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;!-- junit --&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- hibernate对jpa的支持包 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.hibernate&lt;/groupId&gt; &lt;artifactId&gt;hibernate-entitymanager&lt;/artifactId&gt; &lt;version&gt;$&#123;project.hibernate.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- c3p0 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.hibernate&lt;/groupId&gt; &lt;artifactId&gt;hibernate-c3p0&lt;/artifactId&gt; &lt;version&gt;$&#123;project.hibernate.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- log日志 --&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.17&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Mysql and MariaDB --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.6&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 3.配置文件位置：配置到类路径下的一个叫做 META-INF 的文件夹下命名：persistence.xml 12345678910111213141516171819202122232425262728293031323334&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;persistence xmlns=&quot;http://java.sun.com/xml/ns/persistence&quot; version=&quot;2.0&quot;&gt; &lt;!--配置持久化单元 name：持久化单元名称 transaction-type：事务类型 RESOURCE_LOCAL：本地事务管理 JTA：分布式事务管理 --&gt; &lt;persistence-unit name=&quot;myJpa&quot; transaction-type=&quot;RESOURCE_LOCAL&quot;&gt; &lt;!--配置JPA规范的服务提供商 --&gt; &lt;provider&gt;org.hibernate.jpa.HibernatePersistenceProvider&lt;/provider&gt; &lt;properties&gt; &lt;!-- 数据库驱动 --&gt; &lt;property name=&quot;javax.persistence.jdbc.driver&quot; value=&quot;com.mysql.jdbc.Driver&quot;/&gt; &lt;!-- 数据库地址 --&gt; &lt;property name=&quot;javax.persistence.jdbc.url&quot; value=&quot;jdbc:mysql://localhost:3306/jpa&quot;/&gt; &lt;!-- 数据库用户名 --&gt; &lt;property name=&quot;javax.persistence.jdbc.user&quot; value=&quot;root&quot;/&gt; &lt;!-- 数据库密码 --&gt; &lt;property name=&quot;javax.persistence.jdbc.password&quot; value=&quot;root&quot;/&gt; &lt;!--jpa提供者的可选配置：我们的JPA规范的提供者为hibernate，所以jpa的核心配置中兼容hibernate的配 --&gt; &lt;!--显示sql语句--&gt; &lt;property name=&quot;hibernate.show_sql&quot; value=&quot;true&quot;/&gt; &lt;property name=&quot;hibernate.format_sql&quot; value=&quot;true&quot;/&gt; &lt;!-- 建表方式 create:每次执行都创建表 update:没有表才创建 none:不创建表 --&gt; &lt;property name=&quot;hibernate.hbm2ddl.auto&quot; value=&quot;update&quot;/&gt; &lt;/properties&gt; &lt;/persistence-unit&gt;&lt;/persistence&gt; 4.实体类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107/** * @author yinhuidong * @createTime 2020-06-01-21:43 *///作用：指定当前类是实体类。@Entity/** * 作用：指定实体类和表之间的对应关系。 * 属性： * name：指定数据库表的名称 */@Table(name = &quot;t_person&quot;)public class Person &#123; //作用：指定当前字段是主键。 /** * @Id * 作用：指定主键的生成方式。。 * 属性： * strategy ：指定主键生成策略。 */ @GeneratedValue(strategy = GenerationType.IDENTITY) /** * @Column * 作用：指定实体类属性和数据库表之间的对应关系 * 属性： * name：指定数据库表的列名称。 * unique：是否唯一 * nullable：是否可以为空 * inserttable：是否可以插入 * updateable：是否可以更新 * columnDefinition: 定义建表时创建此列的DDL * secondaryTable: 从表名。如果此列不建在主表上（默认建在主表）， * 该属性定义该列所在从表的名字搭建开发环境[重点] */ @Column(name = &quot;id&quot;) private Integer id; @Column(name = &quot;p_name&quot;) private String name; @Column(name = &quot;p_age&quot;) private Integer age; @Column(name = &quot;p_email&quot;) private String email; @Column(name = &quot;p_birth&quot;) private Date birth; public Person() &#123; &#125; public Person(Integer id, String name, Integer age, String email, Date birth) &#123; this.id = id; this.name = name; this.age = age; this.email = email; this.birth = birth; &#125; public Integer getId() &#123; return id; &#125; public void setId(Integer id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public Integer getAge() &#123; return age; &#125; public void setAge(Integer age) &#123; this.age = age; &#125; public String getEmail() &#123; return email; &#125; public void setEmail(String email) &#123; this.email = email; &#125; public Date getBirth() &#123; return birth; &#125; public void setBirth(Date birth) &#123; this.birth = birth; &#125; @Override public String toString() &#123; return &quot;Person&#123;&quot; + &quot;id=&quot; + id + &quot;, name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, age=&quot; + age + &quot;, email=&#x27;&quot; + email + &#x27;\\&#x27;&#x27; + &quot;, birth=&quot; + birth + &#x27;&#125;&#x27;; &#125;&#125; 5.测试类jpa操作的操作步骤12345678910111213141516171819202122232425262728293031323334351.加载配置文件创建实体管理器工厂 Persisitence：静态方法（根据持久化单元名称创建实体管理器工厂） createEntityMnagerFactory（持久化单元名称） 作用：创建实体管理器工厂2.根据实体管理器工厂，创建实体管理器 EntityManagerFactory ：获取EntityManager对象 方法：createEntityManager * 内部维护的很多的内容 内部维护了数据库信息， 维护了缓存信息 维护了所有的实体管理器对象 再创建EntityManagerFactory的过程中会根据配置创建数据库表 * EntityManagerFactory的创建过程比较浪费资源 特点：线程安全的对象 多个线程访问同一个EntityManagerFactory不会有线程安全问题 * 如何解决EntityManagerFactory的创建过程浪费资源（耗时）的问题？ 思路：创建一个公共的EntityManagerFactory的对象 * 静态代码块的形式创建EntityManagerFactory3.创建事务对象，开启事务 EntityManager对象：实体类管理器 beginTransaction : 创建事务对象 presist ： 保存 merge ： 更新 remove ： 删除 find/getRefrence ： 根据id查询 Transaction 对象 ： 事务 begin：开启事务 commit：提交事务 rollback：回滚4.增删改查操作5.提交事务6.释放资源 测试12345678910@Testpublic void testSave()&#123; EntityManagerFactory factory = Persistence.createEntityManagerFactory(&quot;myJpa&quot;); EntityManager em = factory.createEntityManager(); EntityTransaction tx = em.getTransaction(); tx.begin(); Person person = new Person(null,&quot;aaa&quot;,22,&quot;aaa@163.com&quot;,new Date()); em.persist(person); tx.commit();&#125; 5.主键生成策略@GeneratedValue:配置主键的生成策略strategyGenerationType.IDENTITY ：自增，mysql底层数据库必须支持自动增长（底层数据库支持的自动增长方式，对id自增）GenerationType.SEQUENCE : 序列，oracle底层数据库必须支持序列GenerationType.TABLE : jpa提供的一种机制，通过一张数据库表的形式帮助我们完成主键自增GenerationType.AUTO ： 由程序自动的帮助我们选择主键生成策略 6.JPA的API介绍1.Persistence对象Persistence对象主要作用是用于获取EntityManagerFactory对象的 。通过调用该类的createEntityManagerFactory静态方法，根据配置文件中持久化单元名称创建EntityManagerFactory。 1EntityManagerFactory factory = Persistence.createEntityManagerFactory(&quot;myJpa&quot;); 2.EntityManagerFactoryEntityManagerFactory 接口主要用来创建 EntityManager 实例 1EntityManager em = factory.createEntityManager(); 由于EntityManagerFactory 是一个线程安全的对象（即多个线程访问同一个EntityManagerFactory 对象不会有线程安全问题），并且EntityManagerFactory 的创建极其浪费资源，所以在使用JPA编程时，我们可以对EntityManagerFactory 的创建进行优化，只需要做到一个工程只存在一个EntityManagerFactory 即可。 3. EntityManager在 JPA 规范中,EntityManager是完成持久化操作的核心对象。实体类作为普通 java对象，只有在调用 EntityManager将其持久化后才会变成持久化对象。EntityManager对象在一组实体类与底层数据源之间进行 O/R 映射的管理。它可以用来管理和更新 Entity Bean, 根椐主键查找 Entity Bean, 还可以通过JPQL语句查询实体。我们可以通过调用EntityManager的方法完成获取事务，以及持久化数据库的操作 12345getTransaction : 获取事务对象persist ： 保存操作merge ： 更新操作remove ： 删除操作find/getReference ： 根据id查询 4.EntityTransaction在 JPA 规范中, EntityTransaction是完成事务操作的核心对象，对于EntityTransaction在我们的java代码中承接的功能比较简单 123begin：开启事务commit：提交事务rollback：回滚事务 7. 抽取JPAUtil工具类1234567891011121314151617/** * @author yinhuidong * @createTime 2020-06-02-0:13 */public class JpaUtils &#123; private static EntityManagerFactory factory; static &#123; factory = Persistence.createEntityManagerFactory(&quot;myJpa&quot;); &#125; public static EntityManager getEm()&#123; return factory.createEntityManager(); &#125;&#125; 8. 使用JPA完成增删改查操作1.保存1234567891011121314/** * 添加 */@Testpublic void testSave()&#123; EntityManager em = JpaUtils.getEm(); EntityTransaction tx = em.getTransaction(); tx.begin(); Person person = new Person(null,&quot;aaa&quot;,22,&quot;aaa@163.com&quot;,new Date()); em.persist(person); tx.commit(); em.close();&#125; 2.删除12345678910111213/** * 删除：remove */ @Test public void testRemove()&#123; EntityManager em = JpaUtils.getEm(); EntityTransaction tx = em.getTransaction(); tx.begin(); Person person = em.getReference(Person.class, 1); em.remove(person); tx.commit(); em.close(); &#125; 3.更新12345678910111213/** * 更新:merge */@Testpublic void testmerge()&#123; EntityManager em = JpaUtils.getEm(); EntityTransaction tx = em.getTransaction(); tx.begin(); Person person = new Person(1,&quot;bbb&quot;,22,&quot;aaa@163.com&quot;,new Date()); em.merge(person); tx.commit(); em.close();&#125; 4.根据ID查询1）立即加载1234567891011/** * 查询:find * 立即加载，执行find()立刻发出sql */@Testpublic void testFind()&#123; EntityManager em = JpaUtils.getEm(); Person person = em.find(Person.class, 1); System.out.println(person); em.close();&#125; 2）懒加载123456789101112/** * 查询：getReference * 懒加载机制，什么时候调用查询结果什么时候发出sql语句 * 得到的是一个动态代理对象 */@Testpublic void testgetReference()&#123; EntityManager em = JpaUtils.getEm(); Person person = em.getReference(Person.class, 1); System.out.println(person); em.close();&#125; 5.复杂查询JPQL全称Java Persistence Query Language 123Java持久化查询语言(JPQL)是一种可移植的查询语言，旨在以面向对象表达式语言的表达式，将SQL语法和简单查询语义绑定在一起·使用这种语言编写的查询是可移植的，可以被编译成所有主流数据库服务器上的SQL。其特征与原生SQL语句类似，并且完全面向对象，通过类名和属性访问，而不是表名和表的属性 1)查询全部1234567891011/** * 查询所有 */@Testpublic void test1()&#123; EntityManager em = JpaUtils.getEm(); String jpql =&quot;from Person&quot;; Query query = em.createQuery(jpql); query.getResultList().forEach(System.out::println); em.close();&#125; 2）分页查询12345678910111213/** * 分页查询 */@Testpublic void test1()&#123; EntityManager em = JpaUtils.getEm(); String jpql =&quot;from Person&quot;; Query query = em.createQuery(jpql); query.setFirstResult(0); query.setMaxResults(2); query.getResultList().forEach(System.out::println); em.close();&#125; 3）条件查询1234567891011121314/** * 条件查询 */@Testpublic void test1()&#123; EntityManager em = JpaUtils.getEm(); String jpql =&quot;from Person where name like ?&quot;; Query query = em.createQuery(jpql); query.setParameter(1,&quot;aa%&quot;); //获取唯一结果集 //System.out.println(query.getSingleResult()); query.getResultList().forEach(System.out::println); em.close();&#125; 4）排序查询1234567891011/** * 排序查询 */@Testpublic void test1()&#123; EntityManager em = JpaUtils.getEm(); String jpql =&quot;from Person order by id desc&quot;; Query query = em.createQuery(jpql); query.getResultList().forEach(System.out::println); em.close();&#125; 5）统计查询1234567891011/** * 统计查询 */@Testpublic void test1()&#123; EntityManager em = JpaUtils.getEm(); String jpql =&quot;select count(id) from Person&quot;; Query query = em.createQuery(jpql); System.out.println(query.getSingleResult()); em.close();&#125; 二，springdatajpa的运行原理以及基本操作1.springDataJpa概述1.1SpringDataJpa概述Spring Data JPA 是 Spring 基于 ORM 框架、JPA 规范的基础上封装的一套JPA应用框架，可使开发者用极简的代码即可实现对数据库的访问和操作。它提供了包括增删改查等在内的常用功能，且易于扩展！学习并使用 Spring Data JPA 可以极大提高开发效率！ Spring Data JPA 让我们解脱了DAO层的操作，基本上所有CRUD都可以依赖于它来实现,在实际的工作工程中，推荐使用Spring Data JPA + ORM（如：hibernate）完成操作，这样在切换不同的ORM框架时提供了极大的方便，同时也使数据库层操作更加简单，方便解耦 1.2springdatajpa的特性SpringData Jpa 极大简化了数据库访问层代码。 如何简化的呢？ 使用了SpringDataJpa，我们的dao层中只需要写接口，就自动具有了增删改查、分页查询等方法。 1.3Spring Data JPA 与 JPA和hibernate之间的关系JPA是一套规范，内部是有接口和抽象类组成的。hibernate是一套成熟的ORM框架，而且Hibernate实现了JPA规范，所以也可以称hibernate为JPA的一种实现方式，我们使用JPA的API编程，意味着站在更高的角度上看待问题（面向接口编程） Spring Data JPA是Spring提供的一套对JPA操作更加高级的封装，是在JPA规范下的专门用来进行数据持久化的解决方案。 2.SpringDataJpa快速入门2.1需求说明Spring Data JPA完成客户的基本CRUD操作 2.2搭建环境项目依赖1234567891011121314151617181920212223242526272829303132&lt;!--hibernate--&gt;&lt;dependency&gt; &lt;groupId&gt;org.hibernate&lt;/groupId&gt; &lt;artifactId&gt;hibernate-core&lt;/artifactId&gt; &lt;version&gt;5.4.15.Final&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.hibernate&lt;/groupId&gt; &lt;artifactId&gt;hibernate-entitymanager&lt;/artifactId&gt; &lt;version&gt;5.0.7.Final&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.hibernate&lt;/groupId&gt; &lt;artifactId&gt;hibernate-validator&lt;/artifactId&gt; &lt;version&gt;5.3.6.Final&lt;/version&gt;&lt;/dependency&gt;&lt;!--springdatajpa--&gt;&lt;dependency&gt; &lt;groupId&gt;javax.el&lt;/groupId&gt; &lt;artifactId&gt;javax.el-api&lt;/artifactId&gt; &lt;version&gt;2.2.4&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.glassfish.web&lt;/groupId&gt; &lt;artifactId&gt;javax.el&lt;/artifactId&gt; &lt;version&gt;2.2.4&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.data&lt;/groupId&gt; &lt;artifactId&gt;spring-data-jpa&lt;/artifactId&gt; &lt;version&gt;2.3.0.RELEASE&lt;/version&gt;&lt;/dependency&gt; Spring整合SpringDataJPA1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:jdbc=&quot;http://www.springframework.org/schema/jdbc&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xmlns:jpa=&quot;http://www.springframework.org/schema/data/jpa&quot; xmlns:task=&quot;http://www.springframework.org/schema/task&quot; xsi:schemaLocation=&quot; http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/jdbc http://www.springframework.org/schema/jdbc/spring-jdbc.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd http://www.springframework.org/schema/data/jpa http://www.springframework.org/schema/data/jpa/spring-jpa.xsd&quot;&gt;&lt;!--组件扫描--&gt; &lt;context:component-scan base-package=&quot;com.example&quot;&gt; &lt;!-- 配置要忽略的注解 --&gt; &lt;context:exclude-filter type=&quot;annotation&quot; expression=&quot;org.springframework.stereotype.Controller&quot;/&gt; &lt;/context:component-scan&gt; &lt;!--Spring整合MyBatis--&gt; &lt;!--配置数据源--&gt; &lt;!--引入外部属性文件--&gt; &lt;context:property-placeholder location=&quot;classpath:jdbc.properties&quot;/&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot;&gt; &lt;property name=&quot;username&quot; value=&quot;$&#123;jdbc.username&#125;&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;$&#123;jdbc.password&#125;&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;$&#123;jdbc.url&#125;&quot;/&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;$&#123;jdbc.DriverName&#125;&quot;/&gt; &lt;/bean&gt; &lt;!--配置事务管理器--&gt; &lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.orm.jpa.JpaTransactionManager&quot;&gt; &lt;property name=&quot;entityManagerFactory&quot; ref=&quot;entityManagerFactory&quot;/&gt; &lt;/bean&gt; &lt;jpa:repositories base-package=&quot;com.example.mapper&quot; transaction-manager-ref=&quot;transactionManager&quot; entity-manager-factory-ref=&quot;entityManagerFactory&quot;&gt;&lt;/jpa:repositories&gt; &lt;!--事务管理--&gt; &lt;tx:advice id=&quot;txAdvice&quot; transaction-manager=&quot;transactionManager&quot;&gt; &lt;tx:attributes&gt; &lt;tx:method name=&quot;save*&quot; propagation=&quot;REQUIRED&quot;/&gt; &lt;tx:method name=&quot;insert*&quot; propagation=&quot;REQUIRED&quot;/&gt; &lt;tx:method name=&quot;update*&quot; propagation=&quot;REQUIRED&quot;/&gt; &lt;tx:method name=&quot;delete*&quot; propagation=&quot;REQUIRED&quot;/&gt; &lt;tx:method name=&quot;get*&quot; read-only=&quot;true&quot;/&gt; &lt;tx:method name=&quot;find*&quot; read-only=&quot;true&quot;/&gt; &lt;tx:method name=&quot;*&quot; propagation=&quot;REQUIRED&quot;/&gt; &lt;/tx:attributes&gt; &lt;/tx:advice&gt; &lt;aop:config&gt; &lt;!--配置切入点表达式--&gt; &lt;aop:pointcut id=&quot;my&quot; expression=&quot;execution(* com.example.service.impl.*.* (..))&quot;&gt;&lt;/aop:pointcut&gt; &lt;aop:advisor advice-ref=&quot;txAdvice&quot; pointcut-ref=&quot;my&quot;&gt;&lt;/aop:advisor&gt; &lt;/aop:config&gt; &lt;bean id=&quot;entityManagerFactory&quot; class=&quot;org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;&gt;&lt;/property&gt; &lt;property name=&quot;packagesToScan&quot; value=&quot;com.example.domain&quot;&gt;&lt;/property&gt; &lt;property name=&quot;persistenceProvider&quot;&gt; &lt;bean class=&quot;org.hibernate.jpa.HibernatePersistenceProvider&quot;/&gt; &lt;/property&gt; &lt;!--jpa的供应商适配器--&gt; &lt;property name=&quot;jpaVendorAdapter&quot;&gt; &lt;bean class=&quot;org.springframework.orm.jpa.vendor.HibernateJpaVendorAdapter&quot;&gt; &lt;property name=&quot;generateDdl&quot; value=&quot;false&quot;&gt;&lt;/property&gt; &lt;property name=&quot;database&quot; value=&quot;MYSQL&quot;&gt;&lt;/property&gt; &lt;property name=&quot;databasePlatform&quot; value=&quot;org.hibernate.dialect.MySQLDialect&quot;&gt;&lt;/property&gt; &lt;property name=&quot;showSql&quot; value=&quot;true&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;/property&gt; &lt;property name=&quot;jpaDialect&quot;&gt; &lt;bean class=&quot;org.springframework.orm.jpa.vendor.HibernateJpaDialect&quot;&gt;&lt;/bean&gt; &lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; 关联映射实体类1234567891011121314151617@Entity@Table(name = &quot;t_person&quot;)public class Person &#123; @Id @GeneratedValue(strategy = IDENTITY) @Column(name = &quot;id&quot;) private Long id; @Column(name = &quot;p_name&quot;) private String name; @Column(name = &quot;p_age&quot;) private Integer age; @Column(name = &quot;p_email&quot;) private String email; @Column(name = &quot;p_birth&quot;) private Date birth;&#125; mapper接口12345678/** * @author yinhuidong * @createTime 2020-06-04-15:46 * JpaRepository&lt;Person, Long&gt; 用来完成基本CRUD * JpaSpecificationExecutor&lt;Person&gt; 用来完成复杂查询 */public interface PersonMapper extends JpaRepository&lt;Person, Long&gt;, JpaSpecificationExecutor&lt;Person&gt; &#123;&#125; 测试类123456789101112131415161718192021222324252627282930313233343536373839404142@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(locations = &quot;classpath:applicationContext.xml&quot;)public class TestCRUD &#123; @Autowired private PersonMapper mapper; //保存操作 @Test public void testSave()&#123; mapper.save(new Person(&quot;aaa&quot;,20,&quot;aaa@qq.com&quot;,new Date())); &#125; /** * 修改操作： * 修改与保存共用一个方法save * 当传入的实体类有主键就是修改，没有主键就是保存 */ @Test public void testUpdate()&#123; Person person = new Person(&quot;aaa&quot;,20,&quot;bbb@qq.com&quot;,new Date()); person.setId(8l); mapper.save(person); &#125; //删除 @Test public void testDelete()&#123; Person person = new Person(&quot;aaa&quot;,20,&quot;bbb@qq.com&quot;,new Date()); person.setId(8l); mapper.delete(person); &#125; //根据ID查询 @Test public void testFindById()&#123; System.out.println(mapper.findById(7l).get()); &#125; //查询所有 @Test public void testFindAll()&#123; mapper.findAll().forEach(System.out::println); &#125;&#125; 3.SpringDataJPA的底层代码分析3.1常用接口分析在自定义mapper接口中，并没有提供任何方法就可以使用其中的方法，那么这些方法究竟是怎么来的？，由于我们的接口继承了JpaRepository和JpaSpecificationExecutor，所以我们可以使用这两个接口的所有方法。 在使用Spring Data JPA时，一般实现JpaRepository和JpaSpecificationExecutor接口，这样就可以使用这些接口中定义的方法，但是这些方法都只是一些声明，没有具体的实现方式，那么在 Spring Data JPA中它又是怎么实现的呢？ 3.2SpringDataJPA的实现过程通过刚才的入门案例，打断点来分析SpringDataJPA的执行过程。 以findById（）为例。 断点执行到方法上时，我们可以发现注入的是personmapper对象，本质上是通过jdk的动态代理生成的一个对象。 查看具体过程： 当程序执行的时候，会通过JdkDynamicAopProxy的invoke方法，对Dao对象生成动态代理对象。根据对Spring Data JPA介绍而知，要想进行findOne查询方法，最终还是会出现JPA规范的API完成操作，那么这些底层代码存在于何处呢？答案很简单，都隐藏在通过JdkDynamicAopProxy生成的动态代理对象当中，而这个动态代理对象就是SimpleJpaRepository,通过 SimpleJpaRepository的源码分析，定位到了findOne方法，在此方法中，返回em.find()的返回结果，那么em又是什么呢？ 继续查找em对象，我们发现em就是EntityManager对象，而他是JPA原生的实现方式，所以我们得到结论Spring Data JPA只是对标准JPA操作进行了进一步封装，简化了Dao层代码的开发。 由此可得：SpringDataJpa完整执行流程： SpringDataJpa–&gt;JPA–&gt;hibernate–&gt;数据库 4.SpringDataJPA的查询方式4.1使用jpql的方式查询使用Spring Data JPA提供的查询方法已经可以解决大部分的应用场景，但是对于某些业务来说，我们还需要灵活的构造查询条件，这时就可以使用@Query注解，结合JPQL的语句方式完成查询 **@Query **** 注解的使用非常简单，只需在方法上面标注该注解，同时提供一个JPQL查询语句即可 ** 1234567891011/** * 测试使用JPQL的方式进行查询 */@Query(&quot;from Person &quot;)List&lt;Person&gt; MyfindAll();@Query(&quot;from Person where id=?1&quot;)Person MyfindById(Long id);@Query(value = &quot;update Person set name=?1 where id=?2&quot;)@Modifying@Transactionalvoid MyUpdate(String name,Long id); 4.2使用SQL语句查询1234567891011/** * nativeQuery : 使用本地sql的方式查询 */@Query(value = &quot;select * from t_person&quot;,nativeQuery = true)List&lt;Person&gt; MyfindAll();@Query(value = &quot;select * from t_person where id=?1&quot;,nativeQuery = true)Person MyfindById(Long id);@Query(value = &quot;update t_person set p_name=?1 where id=?2&quot;,nativeQuery = true)@Modifying@Transactionalvoid MyUpdate(String name,Long id); 4.3方法命名规则查询方法命名规则查询就是根据方法的名字，就能创建查询。只需要按照Spring Data JPA提供的方法命名规则定义方法的名称，就可以完成查询工作。Spring Data JPA在程序执行的时候会根据方法名称进行解析，并自动生成查询语句进行查询 按照Spring Data JPA 定义的规则，查询方法以findBy开头，涉及条件查询时，条件的属性用条件关键字连接，要注意的是：条件属性首字母需大写。框架在进行方法名解析时，会先把方法名多余的前缀截取掉，然后对剩下部分进行解析。 Keyword Sample JPQL And findByLastnameAndFirstname … where x.lastname = ?1 and x.firstname = ?2 Or findByLastnameOrFirstname … where x.lastname = ?1 or x.firstname = ?2 Is,Equals findByFirstnameIs, findByFirstnameEquals … where x.firstname = ?1 Between findByStartDateBetween … where x.startDate between ?1 and ?2 LessThan findByAgeLessThan … where x.age &lt; ?1 LessThanEqual findByAgeLessThanEqual … where x.age ⇐ ?1 GreaterThan findByAgeGreaterThan … where x.age &gt; ?1 GreaterThanEqual findByAgeGreaterThanEqual … where x.age &gt;= ?1 After findByStartDateAfter … where x.startDate &gt; ?1 Before findByStartDateBefore … where x.startDate &lt; ?1 IsNull findByAgeIsNull … where x.age is null IsNotNull,NotNull findByAge(Is)NotNull … where x.age not null Like findByFirstnameLike … where x.firstname like ?1 NotLike findByFirstnameNotLike … where x.firstname not like ?1 StartingWith findByFirstnameStartingWith … where x.firstname like ?1 (parameter bound with appended %) EndingWith findByFirstnameEndingWith … where x.firstname like ?1 (parameter bound with prepended %) Containing findByFirstnameContaining … where x.firstname like ?1 (parameter bound wrapped in %) OrderBy findByAgeOrderByLastnameDesc … where x.age = ?1 order by x.lastname desc Not findByLastnameNot … where x.lastname &lt;&gt; ?1 In findByAgeIn(Collection ages) … where x.age in ?1 NotIn findByAgeNotIn(Collection age) … where x.age not in ?1 TRUE findByActiveTrue() … where x.active = true FALSE findByActiveFalse() … where x.active = false IgnoreCase findByFirstnameIgnoreCase … where UPPER(x.firstame) = UPPER(?1) 三，springdataJpa的动态查询与多表查询1.动态查询有时我们在查询某个实体的时候，给定的条件是不固定的，这时就需要动态构建相应的查询语句，在Spring Data JPA中可以通过JpaSpecificationExecutor接口查询。相比JPQL,其优势是类型安全,更加的面向对象。 12345678910111213141516171819202122import java.util.List;import org.springframework.data.domain.Page;import org.springframework.data.domain.Pageable;import org.springframework.data.domain.Sort;import org.springframework.data.jpa.domain.Specification;/** * JpaSpecificationExecutor中定义的方法 **/ public interface JpaSpecificationExecutor&lt;T&gt; &#123; //根据条件查询一个对象 T findOne(Specification&lt;T&gt; spec); //根据条件查询集合 List&lt;T&gt; findAll(Specification&lt;T&gt; spec); //根据条件分页查询 Page&lt;T&gt; findAll(Specification&lt;T&gt; spec, Pageable pageable); //排序查询查询 List&lt;T&gt; findAll(Specification&lt;T&gt; spec, Sort sort); //统计查询 long count(Specification&lt;T&gt; spec);&#125; 对于JpaSpecificationExecutor，这个接口基本是围绕着Specification接口来定义的。我们可以简单的理解为，Specification构造的就是查询条件。 Specification接口中只定义了如下一个方法： 1234567//构造查询条件 /** * root ：Root接口，代表查询的根对象，可以通过root获取实体中的属性 * query ：代表一个顶层查询对象，用来自定义查询 * cb ：用来构建查询，此对象里有很多条件方法 **/ public Predicate toPredicate(Root&lt;T&gt; root, CriteriaQuery&lt;?&gt; query, CriteriaBuilder cb); 1.1 使用Specifications完成条件查询1234567891011121314//动态条件查询@Testpublic void test7()&#123; //使用匿名了内部类的方式，创建一个Specification的实现类，并实现toPredicate（） Specification&lt;Person&gt; sp=new Specification&lt;Person&gt;() &#123; @Override public Predicate toPredicate(Root&lt;Person&gt; root, CriteriaQuery&lt;?&gt; criteriaQuery, CriteriaBuilder criteriaBuilder) &#123; //criteriaBuilder:构建查询，添加查询方式 like：模糊匹配 return criteriaBuilder.like(root.get(&quot;name&quot;).as(String.class),&quot;少妇白洁&quot;); &#125; &#125;; mapper.findAll(sp).forEach(System.out::println);&#125; 1.2基于Specification的分页查询1234567891011121314151617181920212223242526272829//动态分页查询@Testpublic void test8()&#123; //构建查询条件 Specification&lt;Person&gt; sp=new Specification&lt;Person&gt;() &#123; @Override public Predicate toPredicate(Root&lt;Person&gt; root, CriteriaQuery&lt;?&gt; criteriaQuery, CriteriaBuilder criteriaBuilder) &#123; return criteriaBuilder.like(root.get(&quot;name&quot;).as(String.class),&quot;少妇%&quot;); &#125; &#125;; /** * 构建分页参数： * Pageable：接口 * PageRequet实现了Pageable接口，调用of方法的形式构造。 * 第一个参数：页码（从0开始） * 第二个参数：每页查询条数 */ Pageable request = PageRequest.of(0,1); /** * 分页查询：封装为SpringDataJpa内部的pageBean * 此重载findAlll方法为分页方法需要两个参数 * 第一个参数、；查询条件 * 第二个参数：分页参数 */ Page&lt;Person&gt; page = mapper.findAll(sp, request); page.forEach(System.out::println);&#125; 1.3方法对应关系 方法名称 Sql对应关系 equle filed = value gt（greaterThan ） filed &gt; value lt（lessThan ） filed &lt; value ge（greaterThanOrEqualTo ） filed &gt;= value le（ lessThanOrEqualTo） filed &lt;= value notEqule filed != value like filed like value notLike filed not like value 2.多表设计2.1表之间关系的划分数据库中多表之间存在着三种关系，如图所示。 从图可以看出，系统设计的三种实体关系分别为：多对多、一对多和一对一关系。注意：一对多关系可以看为两种： 即一对多，多对一。所以说四种更精确。 2.2 在JPA框架中表关系的分析步骤在实际开发中，我们数据库的表难免会有相互的关联关系，在操作表的时候就有可能会涉及到多张表的操作。而在这种实现了ORM思想的框架中（如JPA），可以让我们通过操作实体类就实现对数据库表的操作。所以学习重点是：掌握配置实体之间的关联关系。 第一步：首先确定两张表之间的关系。 如果关系确定错了，后面做的所有操作就都不可能正确。 第二步：在数据库中实现两张表的关系 第三步：在实体类中描述出两个实体的关系 第四步：配置出实体类和数据库表的关系映射（重点） 3.jpa中的一对多3.1情景模拟上个案例我创建的是一张Person表，此时在创建一张Dog表，每个人都可以对应着多个宠物狗。 此时从人的角度就是一对多的关系。 3.2表关系建立在一对多关系中，我们习惯把一的一方称之为主表，把多的一方称之为从表。在数据库中建立一对多的关系，需要使用数据库的外键约束。 什么是外键？ 指的是从表中有一列，取值参照主表的主键，这一列就是外键。 一对多数据库关系的建立，如下图所示 3.3 实体类关系建立以及映射配置数据库建表语句 123456789101112CREATE TABLE t_person(id INT PRIMARY KEY AUTO_INCREMENT,p_age INT ,p_birth DATE,p_email VARCHAR(20),p_name VARCHAR(20));CREATE TABLE t_dog(d_id INT PRIMARY KEY AUTO_INCREMENT,d_name VARCHAR(20),p_id INT REFERENCES t_person(id) ); 实体类关系映射 12345678910111213141516171819@Entity@Table(name = &quot;t_person&quot;)public class Person &#123; @Id @GeneratedValue(strategy = IDENTITY) @Column(name = &quot;id&quot;) private Long id; @Column(name = &quot;p_name&quot;) private String name; @Column(name = &quot;p_age&quot;) private Integer age; @Column(name = &quot;p_email&quot;) private String email; @Column(name = &quot;p_birth&quot;) private Date birth; @OneToMany(mappedBy=&quot;person&quot;) private List&lt;Dog&gt; dogs=new ArrayList&lt;&gt;();&#125; 1234567891011121314@Entity@Table(name = &quot;t_dog&quot;)public class Dog &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) @Column(name = &quot;d_id&quot;) private Long id; @Column(name = &quot;d_name&quot;) private String name; @ManyToOne(targetEntity = Person.class) @JoinColumn(name = &quot;p_id&quot;,referencedColumnName = &quot;id&quot;) private Person person;&#125; 3.4映射的注解说明@OneToMany:作用：建立一对多的关系映射 属性： targetEntityClass：指定多的多方的类的字节码 mappedBy：指定从表实体类中引用主表对象的名称。 cascade：指定要使用的级联操作 fetch：指定是否采用延迟加载 orphanRemoval：是否使用孤儿删除 **@ManyToOne **** **作用：建立多对一的关系 属性： targetEntityClass：指定一的一方实体类字节码 cascade：指定要使用的级联操作 fetch：指定是否采用延迟加载 optional：关联是否可选。如果设置为false，则必须始终存在非空关系。 **@JoinColumn **** **作用：用于定义主键字段和外键字段的对应关系。 属性： name：指定外键字段的名称 referencedColumnName：指定引用主表的主键字段名称 unique：是否唯一。默认值不唯一 nullable：是否允许为空。默认值允许。 insertable：是否允许插入。默认值允许。 updatable：是否允许更新。默认值允许。 columnDefinition：列的定义信息。 3.5一对多的操作1）添加1234567891011121314151617181920212223242526@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(locations = &quot;classpath:applicationContext.xml&quot;)public class OneToManyTest &#123; @Autowired private PersonMapper personMapper; @Autowired private DogMapper dogMapper; /** * 测试保存： * 同时保存一个狗主人和一只狗的信息 */ @Test @Transactional @Rollback(value = false) public void testSave()&#123; Person person = new Person(&quot;张敏&quot;,26,&quot;baijie@qq.com&quot;,new Date()); Dog dog = new Dog(); dog.setName(&quot;金毛&quot;); person.getDogs().add(dog); dog.setPerson(person); personMapper.save(person); dogMapper.save(dog); &#125;&#125; 2）删除1234567891011/** * 删除操作 */@Testpublic void testDelete()&#123; //删除主表数据，可以直接删除，对应从表外键字段还是删除的主表ID personMapper.deleteById(4l); //此时在删除从表会报错 dogMapper.deleteById(4l); //如果先删除从表再删除主表则没有问题&#125; 3）级联操作级联操作：指操作一个对象同时操作它的关联对象 使用方法：只需要在操作主体的注解上配置cascade 1@OneToMany(mappedBy=&quot;person&quot;,cascade = CascadeType.ALL) cascade:配置级联操作CascadeType.MERGE 级联更新CascadeType.PERSIST 级联保存：CascadeType.REFRESH 级联刷新：CascadeType.REMOVE 级联删除：CascadeType.ALL 包含所有 4.jpa中的多对多4.1.情景模拟用户和角色：一个用户可以有多个角色，一个角色可以对应多个用户 4.2表关系建立 123456789CREATE TABLE t_role(r_id INT PRIMARY KEY AUTO_INCREMENT,NAME VARCHAR(20));CREATE TABLE t_person_role(id INT PRIMARY KEY AUTO_INCREMENT,r_id INT REFERENCES t_role(r_id),p_id INT REFERENCES t_person(id)) 4.3实体关系建立以及映射配置12345678910111213141516171819202122@Entity@Table(name = &quot;t_person&quot;)public class Person &#123; @Id @GeneratedValue(strategy = IDENTITY) @Column(name = &quot;id&quot;) private Long id; @Column(name = &quot;p_name&quot;) private String name; @Column(name = &quot;p_age&quot;) private Integer age; @Column(name = &quot;p_email&quot;) private String email; @Column(name = &quot;p_birth&quot;) private Date birth; @OneToMany(mappedBy=&quot;person&quot;,cascade = CascadeType.ALL) private List&lt;Dog&gt; dogs=new ArrayList&lt;&gt;(); //多对多关系映射 @ManyToMany(mappedBy =&quot;persons&quot;) private List&lt;Role&gt; roles=new ArrayList&lt;&gt;(); &#125; 12345678910111213141516@Entity@Table(name = &quot;t_role&quot;)public class Role &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) @Column(name = &quot;r_id&quot;) private Integer id; @Column(name = &quot;name&quot;) private String name; @ManyToMany @JoinTable(name=&quot;t_person_role&quot;,//中间表的名称 joinColumns=&#123;@JoinColumn(name=&quot;r_id&quot;,referencedColumnName=&quot;r_id&quot;)&#125;, inverseJoinColumns=&#123;@JoinColumn(name=&quot;p_id&quot;,referencedColumnName=&quot;id&quot;)&#125; ) private List&lt;Person&gt; persons=new ArrayList&lt;&gt;(); &#125; 4.4映射的注解说明**@ManyToMany **** ** 作用：用于映射多对多关系 属性： cascade：配置级联操作。 fetch：配置是否采用延迟加载。 targetEntity：配置目标的实体类。映射多对多的时候不用写。 **@JoinTable **** **作用：针对中间表的配置 属性： nam：配置中间表的名称 joinColumns：中间表的外键字段关联当前实体类所对应表的主键字段 inverseJoinColumn：中间表的外键字段关联对方表的主键字段 ​ **@JoinColumn **** **作用：用于定义主键字段和外键字段的对应关系。 属性： name：指定外键字段的名称 referencedColumnName：指定引用主表的主键字段名称 unique：是否唯一。默认值不唯一 nullable：是否允许为空。默认值允许。 insertable：是否允许插入。默认值允许。 updatable：是否允许更新。默认值允许。 columnDefinition：列的定义信息。 4.5多对多的操作1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * @author yinhuidong * @createTime 2020-06-04-21:19 */@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(locations = &quot;classpath:applicationContext.xml&quot;)public class ManyToManyTest &#123; @Autowired private PersonMapper personMapper; @Autowired private RoleMapper roleMapper; /** * 保存 */ @Test @Transactional @Rollback(false) public void test1()&#123; Person person = new Person(); person.setName(&quot;尹会东&quot;); Person person2 = new Person(); person2.setName(&quot;张贝贝&quot;); Role r1 = new Role(); r1.setName(&quot;学生&quot;); Role r2 = new Role(); r2.setName(&quot;丈夫&quot;); person.getRoles().add(r1); person.getRoles().add(r2); person2.getRoles().add(r1); person2.getRoles().add(r2); r1.getPersons().add(person); r2.getPersons().add(person); r1.getPersons().add(person2); r2.getPersons().add(person2); personMapper.save(person); personMapper.save(person2); roleMapper.save(r1); roleMapper.save(r2); &#125; @Test @Transactional @Rollback(false) public void test2()&#123; personMapper.deleteById(10l); &#125;&#125; 5.SpringDataJPA的多表查询对象导航图查询对象图导航检索方式是根据已经加载的对象，导航到他的关联对象。它利用类与类之间的关系来检索对象。例如：我们通过ID查询方式查出一个客户，可以调用Customer类中的getLinkMans()方法来获取该客户的所有联系人。对象导航查询的使用要求是：两个对象之间必须存在关联关系。 查询一个Person获取它对应的所有Role 1234567@Transactional@Testpublic void test3()&#123; Person person = personMapper.findById(11l).get(); System.out.println(&quot;person = &quot; + person); person.getRoles().forEach(System.out::println);&#125; 查询一个角色，获取角色对应的所有person 1234567@Transactional@Testpublic void test3()&#123; Role role = roleMapper.findById(5l).get(); System.out.println(&quot;role = &quot; + role); role.getPersons().forEach(System.out::println);&#125; 对象导航查询的问题分析 问题1：我们查询客户时，要不要把联系人查询出来？ 分析：如果我们不查的话，在用的时候还要自己写代码，调用方法去查询。如果我们查出来的，不使用时又会白白的浪费了服务器内存。 解决：采用延迟加载的思想。通过配置的方式来设定当我们在需要使用时，发起真正的查询。 配置方式： 12//LAZY @ManyToMany(mappedBy =&quot;persons&quot;,fetch = FetchType.EAGER) FetchType.EAGER 立即加载 FetchType.LAZY 懒加载 12345678910111213141516 r1.getPersons().add(person); r2.getPersons().add(person); r1.getPersons().add(person2); r2.getPersons().add(person2); personMapper.save(person); personMapper.save(person2); roleMapper.save(r1); roleMapper.save(r2);&#125;@Test@Transactional@Rollback(false)public void test2()&#123; personMapper.deleteById(10l);&#125; } 1234567891011121314151617## 5.SpringDataJPA的多表查询### 5.1对象导航图查询对象图导航检索方式是根据已经加载的对象，导航到他的关联对象。它利用类与类之间的关系来检索对象。例如：我们通过ID查询方式查出一个客户，可以调用Customer类中的getLinkMans()方法来获取该客户的所有联系人。对象导航查询的使用要求是：两个对象之间必须存在关联关系。**查询一个Person获取它对应的所有Role**```java @Transactional @Test public void test3()&#123; Person person = personMapper.findById(11l).get(); System.out.println(&quot;person = &quot; + person); person.getRoles().forEach(System.out::println); &#125; 查询一个角色，获取角色对应的所有person 1234567@Transactional@Testpublic void test3()&#123; Role role = roleMapper.findById(5l).get(); System.out.println(&quot;role = &quot; + role); role.getPersons().forEach(System.out::println);&#125; 对象导航查询的问题分析 问题1：我们查询客户时，要不要把联系人查询出来？ 分析：如果我们不查的话，在用的时候还要自己写代码，调用方法去查询。如果我们查出来的，不使用时又会白白的浪费了服务器内存。 解决：采用延迟加载的思想。通过配置的方式来设定当我们在需要使用时，发起真正的查询。 配置方式： 12//LAZY @ManyToMany(mappedBy =&quot;persons&quot;,fetch = FetchType.EAGER) FetchType.EAGER 立即加载 FetchType.LAZY 懒加载","categories":[{"name":"MyBatis","slug":"MyBatis","permalink":"https://yinhuidong.github.io/categories/MyBatis/"}],"tags":[{"name":"MyBatis","slug":"MyBatis","permalink":"https://yinhuidong.github.io/tags/MyBatis/"}]},{"title":"MyBatis应用分析与最佳实践","slug":"MyBatis/MyBatis应用分析与最佳实践","date":"2022-01-11T06:37:23.826Z","updated":"2022-01-11T06:40:09.343Z","comments":true,"path":"2022/01/11/MyBatis/MyBatis应用分析与最佳实践/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/MyBatis/MyBatis%E5%BA%94%E7%94%A8%E5%88%86%E6%9E%90%E4%B8%8E%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/","excerpt":"","text":"写在前面：全文代码git地址：https://gitee.com/yin_huidong/mybatis-use.git 一，MyBatis入门持久层技术的解决方案： JDBC Spring的JdbcTemplate Apache的DbUtils 以上都不是框架，JDBC是规范，Spring的JdbcTemplate和Apache的DBUtils都只是工具类。​ MyBatis概述：一个用Java编写的持久层框架，封装了JDBC操作的很多细节，是开发者只需要关注SQL语句本身，不需要关注整个请求过程。 ORM：Object Relational Mappging 对象关系映射。简单的说：就是把数据库表和实体类及实体类的属性对应起来，让我们可以操作实体类就实现操作数据库表。 t_user TUser id id name name ​ 1.mybatis环境搭建 创建maven工程，导入坐标 创建实体类和mapper接口 创建mybatis的主配置文件和日志文件 创建映射配置文件 ​ 2.注意事项 在idea中创建目录的时候，它和包是不一样的。 包在创建时：com.itheima.dao它是三级结构 目录在创建时：com.itheima.dao是一级目录 mybatis的映射配置文件位置必须和dao接口的包结构相同 映射配置文件的mapper标签namespace属性的取值必须是dao接口的全限定类名 映射配置文件的操作配置（select），id属性的取值必须是dao接口的方法名 ​ 3.基于XML形式的配置代码：mybatis-01​ 4.基于注解形式的配置代码：mybatis-02​ 5.思考自己实现mybatis框架 5.1 需求分析 它需要实现对连接资源的自动管理，也就是把创建 Connection、Statement、关闭 Connection、Statement、ResultSet 这些操作封装到底层的对象中，不需要在应用层手动调用。 它需要把 SQL 语句抽离出来实现集中管理，开发人员不用在业务代码里面写 SQL语句。 它需要实现对结果集的转换，也就是我们指定了映射规则之后，这个框架会自动帮我们把 ResultSet 映射成实体类对象。 需要提供一个 API 来给我们操作数据库，这里面封装了对数据库的操作的常用的方法。 5.2 概要设计① 核心对象 存放参数和结果映射关系、存放 SQL 语句，我们需要定义一个配置类 执行对数据库的操作，处理参数和结果集的映射，创建和释放资源，我们需要定义一个执行器 有了这个执行器以后，我们不能直接调用它，而是定义一个给应用层使用的 API，它可以根据 SQL 的 id 找到 SQL 语句，交给执行器执行 直接使用 id 查找 SQL 语句太麻烦了，我们干脆把存放 SQL 的命名空间定义成一个接口，把 SQL 的 id 定义成方法，这样只要调用接口方法就可以找到要执行的 SQL。这个时候我们需要引入一个代理类。 ② 流程分析 定义接口 Mapper 和方法，用来调用数据库操作。Mapper 接口操作数据库需要通过代理类。 定义配置类对象 Configuration。 定义应用层的 API SqlSession。它有一个 getMapper()方法，我们会从配置类Configuration 里面使用 Proxy.newProxyInatance()拿到一个代理对象 MapperProxy。 有了代理对象 MapperProxy 之后，我们调用接口的任意方法，就是调用代理对象的 invoke()方法。 代理对象 MapperProxy 的 invoke()方法调用了 SqlSession 的 selectOne()。 SqlSession 只是一个 API，还不是真正的 SQL 执行者，所以接下来会调用执行器 Executor 的 query()方法。 执行器 Executor 的 query()方法里面就是对 JDBC 底层的 Statement 的封装，最终实现对数据库的操作，和结果的返回。 5.3 代码&amp;思考mybatis-0 在 Executor 中，对参数、语句和结果集的处理是耦合的，没有实现职责分离； 参数：没有实现对语句的预编译，只有简单的格式化（format），效率不高，还存在 SQL 注入的风险； 语句执行：数据库连接硬编码； 结果集：还只能处理 Blog 类型，没有实现根据实体类自动映射。确实有点搓，拿不出手。 展望 支持参数预编译 支持结果集的自动处理（通过反射） 对 Executor 的职责进行细化 在方法上使用注解配置 SQL 查询带缓存功能 支持自定义插件6.Mybatis与jdbc编程比较 数据库链接创建、释放频繁造成系统资源浪费从而影响系统性能，如果使用数据库链接池可解决此问题。 解决：在 SqlMapConfig.xml 中配置数据链接池，使用连接池管理数据库链接。 Sql 语句写在代码中造成代码不易维护，实际应用 sql 变化的可能较大，sql 变动需要改变 java 代码。 解决：将 Sql 语句配置在 XXXXmapper.xml 文件中与 java 代码分离。 向 sql 语句传参数麻烦，因为 sql 语句的 where 条件不一定，可能多也可能少，占位符需要和参数对应。 解决：Mybatis 自动将 java 对象映射至 sql 语句，通过 statement 中的 parameterType 定义输入参数的类型。 对结果集解析麻烦，sql 变化导致解析代码变化，且解析前需要遍历，如果能将数据库记录封装成 pojo 对象解析比较方便。 解决：Mybatis 自动将 sql 执行结果映射至 java 对象，通过 statement 中的 resultType 定义输出结果的类型 二，基于Mybatis实现基本的增删改查1.代码mybatis-03​ 2.Mapper映射文件详解 映射器里面最主要的是配置了 SQL 语句，也解决了我们的参数映射和结果集映射的问题。一共有 8 个标签： cache – 给定命名空间的缓存配置（是否开启二级缓存）。 cache-ref – 其他命名空间缓存配置的引用。 resultMap – 是最复杂也是最强大的元素，用来描述如何从数据库结果集中来加载对象。 sql – 可被其他语句引用的可重用语句块。 insert – 映射插入语句 update – 映射更新语句 delete – 映射删除语句 select – 映射查询语句 2.1 select 标签 resultType 属性：用于指定结果集的类型。 parameterType 属性：用于指定传入参数的类型。 sql 语句中使用#{}字符： 它代表占位符，相当于原来 jdbc 部分所学的?，都是用于执行语句时替换实际的数据。具体的数据是由#{}里面的内容决定的。#{}中内容的写法：由于数据类型是基本类型，所以此处可以随意写。​ 2.2 insert 标签 parameterType 属性：代表参数的类型，因为我们要传入的是一个类的对象，所以类型就写类的全名称。 sql 语句中使用#{}字符： 它代表占位符，相当于原来 jdbc 部分所学的?，都是用于执行语句时替换实际的数据。具体的数据是由#{}里面的内容决定的。 #{}中内容的写法：由于我们保存方法的参数是 一个 User 对象，此处要写 User 对象中的属性名称。它用的是 ognl 表达式。 ​ 2.3 OGNL 表达式ognl 表达式：它是 apache 提供的一种表达式语言，全称是：Object Graphic Navigation Language 对象图导航语言，它是按照一定的语法格式来获取数据的。​ 语法格式就是使用 #{对象.对象}的方式：#{user.username}它会先去找 user 对象，然后在 user 对象中找到 username 属性，并调用getUsername()方法把值取出来。但是我们在 parameterType 属性上指定了实体类名称，所以可以省略 user.而直接写 username。​ 2.4 插入后主键返回的三种方式新增用户后，同时还要返回当前新增用户的 id 值，因为 id 是由数据库的自动增长来实现的，所以就相​当于我们要在新增后将自动增长 auto_increment 的值返回。 ①方法一123&lt;insert id=&quot;add&quot; useGeneratedKeys=&quot;true&quot; keyProperty=&quot;id&quot; parameterType=&quot;com.yhd.domain.Account&quot;&gt; insert into account(name,money) values (#&#123;name&#125;,#&#123;money&#125;);&lt;/insert&gt; ②方法二123456&lt;insert id=&quot;add&quot; &gt; &lt;selectKey order=&quot;AFTER&quot; keyProperty=&quot;id&quot; resultType=&quot;int&quot;&gt; select last_insert_id(); &lt;/selectKey&gt; insert into account(name,money) values (#&#123;name&#125;,#&#123;money&#125;);&lt;/insert&gt; ③方法三可以把查询回来的多条数据封装为Map，Map的键是我们指定的唯一键的值，Map的value是每一行记录转换的对象。 12@MapKey(&quot;id&quot;)Map&lt;Integer,Account&gt; findAllByMap(); 123&lt;select id=&quot;findAllByMap&quot; resultType=&quot;account&quot;&gt; select * from account;&lt;/select&gt; 2.5 根据名称模糊查询①第一种方式我们在配置文件中没有加入%来作为模糊查询的条件，所以在传入字符串实参时，就需要给定模糊查询的标识%。配置文件中的#{username}也只是一个占位符，所以 SQL 语句显示为“？”。​ 123&lt;!-- 根据名称模糊查询 --&gt; &lt;select id=&quot;findByName&quot; resultType=&quot;com.yhd.domain.User&quot; parameterType=&quot;String&quot;&gt; select * from user where username like % #&#123;username&#125; %&lt;/select&gt; ②第二种方式123456第一步：修改 SQL 语句的配置，配置如下：&lt;!-- 根据名称模糊查询 --&gt; &lt;select id=&quot;findByName&quot; parameterType=&quot;string&quot; resultType=&quot;com.yhd.domain.User&quot;&gt; select * from user where username like &#x27;%$&#123;value&#125;%&#x27;&lt;/select&gt; 我们在上面将原来的#{}占位符，改成了${value}。注意如果用模糊查询的这种写法，那么${value}的写法就是固定的，不能写成其它名字。​ ③ #{}和${}的区别 #{}表示一个占位符号 通过#{}可以实现 preparedStatement 向占位符中设置值，自动进行 java 类型和 jdbc 类型转换， #{}可以有效防止 sql 注入。 #{}可以接收简单类型值或 pojo 属性值。 如果 parameterType 传输单个简单 类型值，#{}括号中可以是 value 或其它名称。${}表示拼接 sql 串 通过${}可以将 parameterType 传入的内容拼接在 sql 中且不进行 jdbc 类型转换， ${}可以接收简 单类型值或 pojo 属性值，如果 parameterType 传输单个简单类型值，${}括号中只能是 value。 ④模糊查询${value}的源码12345678910111213@Override public String handleToken(String content) &#123; Object parameter = context.getBindings().get(&quot;_parameter&quot;); if (parameter == null) &#123; context.getBindings().put(&quot;value&quot;, null); &#125; else if (SimpleTypeRegistry.isSimpleType(parameter.getClass())) &#123; context.getBindings().put(&quot;value&quot;, parameter); &#125; Object value = OgnlCache.getValue(content, context.getBindings()); String srtValue = (value == null ? &quot;&quot; : String.valueOf(value)); checkInjection(srtValue); return srtValue; &#125; 这就说明了源码中指定了读取的 key 的名字就是”value”，所以我们在绑定参数时就只能叫 value 的名字了。​ 2.6 resultType 配置结果类型resultType配置结果类型:当他为实体类全限定类名，必须让实体类的属性名与数据库表的列名对应，否则，数据会封装不进去，当然，也存在解决办法。解决办法： 起别名，在sql语句中给数据库表的列名起别名，别名与实体类的属性名一致 优点：执行效率高 缺点：开发效率低 配置resultMap 自定义一个resultMap，在select标签中进行引用 优点：开发效率高 缺点：执行效率低​ 123456789101112131415&lt;resultMap id=&quot;map&quot; type=&quot;com.atguigu.domain.User&quot;&gt; &lt;!--配置主键--&gt; &lt;id column=&quot;id&quot; property=&quot;id&quot;&gt;&lt;/id&gt; &lt;!--配置其他列--&gt; &lt;result column=&quot;birthday&quot; property=&quot;birthday&quot;&gt;&lt;/result&gt; &lt;result column=&quot;sex&quot; property=&quot;sex&quot;&gt;&lt;/result&gt; &lt;result column=&quot;address&quot; property=&quot;address&quot;&gt;&lt;/result&gt; &lt;result column=&quot;username&quot; property=&quot;username&quot;&gt;&lt;/result&gt; &lt;!-- id 标签：用于指定主键字段 result 标签：用于指定非主键字段 column 属性：用于指定数据库列名 property 属性：用于指定实体类属性名称 --&gt;&lt;/resultMap&gt; 1234&lt;!-- 配置查询所有操作 --&gt; &lt;select id=&quot;findAll&quot; resultMap=&quot;userMap&quot;&gt; select * from user&lt;/select&gt; 3.核心配置解读configurationMyBatis 全局配置文件顺序是固定的，否则启动的时候会报错。 properties配置参数信息，比如最常见的数据库连接信息。 为了避免直接把参数写死在 xml 配置文件中，我们可以把这些参数单独放在properties 文件中，用 properties 标签引入进来，然后在 xml 配置文件中用${}引用就可以了。 可以用 resource 引用应用里面的相对路径，也可以用 url 指定本地服务器或者网络的绝对路径。 settings 属性名 含义 简介 有效值 默认值 cacheEnabled 是否使用缓存 是整个工程中所有映射器配置缓存的开关，即是一个全局缓存开关 true/false true lazyLoadingEnabled 是否开启延迟加载 控制全局是否使用延迟加载（association、collection）。当有特殊关联关系需要单独配置时，可以使用 fetchType 属性来覆盖此配置 true/false false aggressiveLazyLoading 是否需要侵入式延迟加载 开启时，无论调用什么方法加载某个对象，都会加载该对象的所有属性，关闭之后只会按需加载 true/false false defaultExecutorType 设置默认的执行器 有三种执行器：SIMPLE 为普通执行器；REUSE 执行器会重用与处理语句；BATCH 执行器将重用语句并执行批量更新 SIMPLE/REUSE/BATCH SIMPLE lazyLoadTriggerMethods 指定哪个对象的方法触发一次延迟加载 配置需要触发延迟加载的方法的名字，该方法就会触发一次延迟加载 一个逗号分隔的方法名称列表 equals，clone，hashCode，toString localCacheScope MyBatis 利用本地缓存机制（LocalCache）防止循环引用（circularreferences）和加速重复嵌套查询 默认值为 SESSION，这种情况下会缓存一个会话中执行的所有查询。若设置值为 STATEMENT，本地会话仅用在语句执行上，对相同 SqlSession 的不同调用将不会共享数据 SESSION/STATEMENT SESSION logImpl 日志实现 指定 MyBatis 所用日志的具体实现，未指定时将自动查找 SLF4J、LOG4J、LOG4J2、JDK_LOGGING、COMMONS_LOGGING、STDOUT_LOGGING、NO_LOGGING 无 multipleResultSetsEnabled 是否允许单一语句返回多结果集 即 Mapper 配置中一个单一的 SQL 配置是否能返回多个结果集 true/false true useColumnLabel 使用列标签代替列 设置是否使用列标签代替列名 true/false true useGeneratedKeys 是否支持 JDBC 自动生成主键 设置之后，将会强制使用自动生成主键的策略 true/false false autoMappingBehavior 指定 MyBatis 自动映射字段或属性的方式 有三种方式，NONE 时将取消自动映射；PARTIAL 时只会自动映射没有定义结果集的结果映射；FULL 时会映射任意复杂的结果集 NONE/PARTIAL/FULL PARTIAL autoMappingUnknownColumnBehavior 设置当自动映射时发现未知列的动作 有三种动作：NONE 时不做任何操作；WARNING 时会输出提醒日志；FAILING时会抛出 SqlSessionException 异常表示映射失败 NONE/WARNING/FAILING NONE defaultStatementTimeout 设置超时时间 该超时时间即数据库驱动连接数据库时，等待数据库回应的最大秒数 任意正整数 无 defaultFetchSize 设置驱动的结果集获取数量（fetchSize）的提示值 为了防止从数据库查询出来的结果过多，而导致内存溢出，可以通过设置fetchSize 参数来控制结果集的数量 任意正整数 无 safeRowBoundsEnabled 允许在嵌套语句中使用分页（RowBound，即行内嵌套语句） 如果允许在 SQL 的行内嵌套语句中使用分页，就设置该值为 false true/false false safeResultHandlerEnabled 允许在嵌套语句中使用分页（ResultHandler，即结果集处理） 如果允许在 SQL 的结果集使用分页，就设置该值为 false true/false false mapUnderscoreToCamelCase 是否开启驼峰命名规则（camel case）映射 表明数据库中的字段名称与工程中Java 实体类的映射是否采用驼峰命名规则校验 true/false false jdbcTypeForNull JDBC类型的默认设置 当没有为参数提供特定的 JDBC 类型时，为空值指定 JDBC 类型。某些驱动需要指定列的 JDBC 类型，多数情况直接用一般类型即可，比如 NULL、VARCHAR 或 OTHER 常用 NULL、VARCHAR、OTHER OTHER defaultScriptingLanguage 动态 SQL 默认语言 指定动态 SQL 生成的默认语言 一个类型别名或者一个类的全路径名 org.apache.ibatis.scripting.xmltags.XMLLanguageDriver callSettersOnNulls 是否在控制情况下调用 Set 方法 指定当结果集中值为 null 时是否调用映射对象的 setter （map对象时为put）方法，这对于有 Map.keySet()依赖或null 值初始化时是有用的。注意基本类型是不能设置成 null 的 true/false false returnInstanceForEmptyRow 返回空实体集对象 当返回行的所有列都是空时，MyBatis默认返回 null。当开启这个设置时，MyBatis 会返回一个空实例。请注意，它也适用于嵌套的结果集（从MyBatis3.4.2 版本开始） true/false false logPrefix 日志前缀 指定 MyBatis 所用日志的具体实现，未指定时将自动查找 任意字符串 无 vfsImpl vfs 实现 指定 vfs 的实现 自定义 VFS 的实现的类的全限定名，以逗号分隔 无 useActualParamName 使用方法签名 允许使用方法签名中的名称作为语句参数名称。要使用该特性，工程必须采用 Java8 编译，并且加上-parameters选项（从 MyBatis3.4.1 版本开始） 自定义 VFS 的实现的类的全限定名，以逗号分隔 无 configurationFactory 配置工厂 指定提供配置示例的类。返回的配置实例用于加载反序列化的懒加载参数。这个类必须有一个签名的静态配置getconfiguration()方法（从MyBatis3.2.3 版本开始） 一个类型别名或者一个类型的全路径名 无 typeAliasesTypeAlias 是类型的别名，跟 Linux 系统里面的 alias 一样，主要用来简化全路径类名的拼写。比如我们的参数类型和返回值类型都可能会用到我们的 Bean，如果每个地方都配置全路径的话，那么内容就比较多，还可能会写错。 我们可以为自己的 Bean 创建别名，既可以指定单个类，也可以指定一个 package，自动转换。配置了别名以后，只需要写别名就可以了，比如 com.gupaoedu.domain.Blog都只要写 blog 就可以了。 MyBatis 里面有系统预先定义好的类型别名，在 TypeAliasRegistry 中。 typeHandlers由于 Java 类型和数据库的 JDBC 类型不是一一对应的（比如 String 与 varchar），所以我们把 Java 对象转换为数据库的值，和把数据库的值转换成 Java 对象，需要经过一定的转换，这两个方向的转换就要用到 TypeHandler。 MyBatis 已经内置了很多 TypeHandler（在 type 包下），它们全部注册在 TypeHandlerRegistry 中，他们都继承了抽象类 BaseTypeHandler，泛型就是要处理的 Java 数据类型。 当我们做数据类型转换的时候，就会自动调用对应的 TypeHandler 的方法。 如果我们需要自定义一些类型转换规则，或者要在处理类型的时候做一些特殊的动作，就可以编写自己的 TypeHandler，跟系统自定义的 TypeHandler 一样，继承抽象类BaseTypeHandler。有 4 个抽象方法必须实现，我们把它分成两类：set 方法从 Java 类型转换成 JDBC 类型的，get 方法是从 JDBC 类型转换成 Java 类型的。 objectFactory当我们把数据库返回的结果集转换为实体类的时候，需要创建对象的实例，由于我们不知道需要处理的类型是什么，有哪些属性，所以不能用 new 的方式去创建。在MyBatis 里面，它提供了一个工厂类的接口，叫做 ObjectFactory，专门用来创建对象的实例，里面定义了 4 个方法。 方法 作用 void setProperties(Properties properties); 设置参数时调用 T create(Class type); 创建对象（调用无参构造函数） T create(Class type, List&lt;Class&lt;?&gt;constructorArgTypes, List 创建对象（调用带参数构造函数） n boolean isCollection(Class type) 判断是否集合 ObjectFactory 有一个默认的实现类 DefaultObjectFactory，创建对象的方法最终都调用了 instantiateClass()，是通过反射来实现的。 如果想要修改对象工厂在初始化实体类的时候的行为，就可以通过创建自己的对象工厂，继承 DefaultObjectFactory 来实现（不需要再实现 ObjectFactory 接口）。 plugins插件是 MyBatis 的一个很强大的机制，跟很多其他的框架一样，MyBatis 预留了插件的接口，让 MyBatis 更容易扩展。 environments 、environmentenvironments 标签用来管理数据库的环境，比如我们可以有开发环境、测试环境、生产环境的数据库。可以在不同的环境中使用不同的数据库地址或者类型。 1234567891011&lt;environments default=&quot;dev&quot;&gt; &lt;environment id=&quot;dev&quot;&gt; &lt;transactionManager type=&quot;JDBC&quot;/&gt; &lt;dataSource type=&quot;POOLED&quot;&gt; &lt;property name=&quot;driver&quot; value=&quot;com.mysql.jdbc.Driver&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://127.0.0.1:3306/aaa&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;Alibaba741&quot;/&gt; &lt;/dataSource&gt; &lt;/environment&gt;&lt;/environments&gt; 一个 environment 标签就是一个数据源，代表一个数据库。这里面有两个关键的标签，一个是事务管理器，一个是数据源。 transactionManager如果配置的是 JDBC，则会使用 Connection 对象的 commit()、rollback()、close()管理事务。 如果配置成 MANAGED，会把事务交给容器来管理，比如 JBOSS，Weblogic。因为我们跑的是本地程序，如果配置成 MANAGE 不会有任何事务。 如 果 是 Spring + MyBatis ， 则 没 有 必 要 配 置 ， 因 为 我 们 会 直 接 在applicationContext.xml 里面配置数据源，覆盖 MyBatis 的配置。 dataSourcemappers标签配置的是我们的映射器，也就是 Mapper.xml 的路径。这里配置的目的是让 MyBatis 在启动的时候去扫描这些映射器，创建映射关系。 使用相对于类路径的资源引用（resource） 使用完全限定资源定位符（绝对路径）（URL） 使用映射器接口实现类的完全限定类名 将包内的映射器接口实现全部注册为映射器（最常用） 完整的配置文件12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE configuration PUBLIC &quot;-//mybatis.org//DTD Config 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-config.dtd&quot;&gt;&lt;configuration&gt; &lt;!-- MyBatis标签 --&gt; &lt;!-- properties： 通过properties属性指定数据源的配置 resource=&quot;jdbc.properties&quot; 通过resource属性引入外部属性文件 --&gt; &lt;properties&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot;&gt;&lt;/property&gt; &lt;property name=&quot;password&quot; value=&quot;root&quot;&gt;&lt;/property&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql:///ssm&quot;&gt;&lt;/property&gt; &lt;property name=&quot;driverName&quot; value=&quot;com.mysql.jdbc.Driver&quot;&gt;&lt;/property&gt; &lt;/properties&gt; &lt;!-- setting mybatis运行时的重要设置，谨慎修改 --&gt; &lt;settings&gt; &lt;!--开启驼峰命名法--&gt; &lt;setting name=&quot;mapUnderscoreToCamelCase&quot; value=&quot;true&quot;/&gt; &lt;/settings&gt; &lt;!-- typeAliases：类型别名 --&gt; &lt;typeAliases&gt; &lt;!--为一个实体类起别名，代替子映射文件的实体类全限定类名--&gt; &lt;!--&lt;typeAlias type=&quot;com.atguigu.pojo.User&quot; alias=&quot;user&quot;/&gt;--&gt; &lt;!--直接为一个包下的所有类起别名，默认类名首字母小写--&gt; &lt;package name=&quot;com.atguigu.pojo&quot;/&gt; &lt;/typeAliases&gt; &lt;!--typeHandlers：自己注册类型处理器--&gt; &lt;!--&lt;typeHandlers&gt;--&gt; &lt;!--&lt;typeHandler handler=&quot;&quot; javaType=&quot;&quot; jdbcType=&quot;&quot;/&gt;--&gt; &lt;!--&lt;/typeHandlers&gt;--&gt; &lt;!--ObjectFactory:配置对象工厂--&gt; &lt;!--plugins:插件--&gt; &lt;plugins&gt; &lt;!--5.0版本之前写pagehelper，5.0以后写PageInterceptor--&gt; &lt;plugin interceptor=&quot;com.github.pagehelper.PageInterceptor&quot;&gt; &lt;!--分页合理化参数--&gt; &lt;property name=&quot;reasonable&quot; value=&quot;true&quot;/&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;environments default=&quot;mysql&quot;&gt; &lt;!--配置环境- 此处不光可以配置mysql 还可以配置SQLserver，db2，oracle 使用时在标签上指定databaseId --&gt; &lt;environment id=&quot;mysql&quot;&gt; &lt;!--配置事务--&gt; &lt;transactionManager type=&quot;JDBC&quot;/&gt; &lt;!--配置连接池： type=&quot;POOLED&quot; 使用连接池 type=&quot;UNPOOLED&quot; 不使用连接池 --&gt; &lt;dataSource type=&quot;POOLED&quot;&gt; &lt;property name=&quot;driver&quot; value=&quot;$&#123;driver&#125;&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;$&#123;url&#125;&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;$&#123;username&#125;&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;$&#123;password&#125;&quot;/&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; &lt;!--映射文件处理器--&gt; &lt;mappers&gt; &lt;!--使用相对于类路径下的资源--&gt; &lt;!--&lt;mapper resource=&quot;com/atguigu/mapper/AccountMapper.xml&quot;/&gt;--&gt; &lt;!--此种方法要求接口与映射文件在同一包下且同名--&gt; &lt;mapper class=&quot;com.atguigu.mapper.UserMapper&quot;/&gt; &lt;!--批量注册：依此制定一个包 此种方法要求接口与映射文件在同一包下且同名 --&gt; &lt;package name=&quot;com/atguigu/mapper&quot;/&gt; &lt;/mappers&gt;&lt;/configuration&gt; 4.关于传递参数是多个基本数据类型在传递参数是多个普通类型的情况下，有两种方案可以传递参数的值到占位符。​ 1234567891011121314第一种情况： args0 表示第一个参数 args1 表示第二个参数 。。。以此类推 argsn 表示第n+1个参数 第二种情况： param1 表示第一个参数 param2 表示第二个参数 。。。以此类推 paramn 表示第n个参数 第三种情况： @Param(&quot;name&quot;) String name, @Param(&quot;sex&quot;) Integer sex 如果传递的参数是Map类型，则在#{}中需要写上map的key值表示传递相应key的值到sql的占位符中。mybatis底层传递参数就是使用的map集合。​ 三，Mybatis连接池和事务深入1.连接池Mybatis 中也有连接池技术，但是它采用的是自己的连接池技术。在 Mybatis 的 主 配置文件中，通过来实现 Mybatis 中连接池的配置。​ Mybatis 将它自己的数据源分为三类：​ UNPOOLED 不使用连接池的数据源 POOLED 使用连接池的数据源 JNDI 使用 JNDI 实现的数据源 ​ 相应地，MyBatis 内部分别定义了实现了 java.sql.DataSource 接口的 UnpooledDataSource，PooledDataSource 类来表示 UNPOOLED、POOLED 类型的数据源。一般采用的是 POOLED 数据源（很多时候我们所说的数据源就是为了更好的管理数据库连接，也就是我们所说的连接池技术）连接池其实就是一个容器，容器就可以用集合来充当，而且他必须具有，队列的特性，先进先出。还得是线程安全的，不能让多个线程拿到同一个连接。​ 1.1 MyBatis中数据源的配置1234567&lt;!-- 配置数据源（连接池）信息 --&gt; &lt;dataSource type=&quot;POOLED&quot;&gt; &lt;property name=&quot;driver&quot; value=&quot;$&#123;jdbc.driver&#125;&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;$&#123;jdbc.url&#125;&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;$&#123;jdbc.username&#125;&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;$&#123;jdbc.password&#125;&quot;/&gt;&lt;/dataSource&gt; ​ MyBatis 在初始化时，根据的 type 属性来创建相应类型的的数据源 DataSource，即：type=”POOLED”：MyBatis 会创建 PooledDataSource 实例type=”UNPOOLED” ： MyBatis 会创建 UnpooledDataSource 实例type=”JNDI”：MyBatis 会从 JNDI 服务上查找 DataSource 实例，然后返回使用 1.2 MyBatis中Datasource的存取MyBatis 是 通 过 工 厂 模 式 来 创 建 数 据 源 DataSource 对 象 的 ， MyBatis 定 义 了 抽 象 的 工 厂 接口:org.apache.ibatis.datasource.DataSourceFactory,通过其 getDataSource()方法返回数据源DataSource。​ MyBatis 创建了 DataSource 实例后，会将其放到 Configuration 对象内的 Environment 对象中， 供以后使用。（具体可以查看一个类，叫 XMLConfigBuilder）。​ 1.3 Mybatis中连接的获取过程分析123456789101112131415161718192021222324252627282930313233343536373839当我们需要创建 SqlSession 对象并需要执行 SQL 语句时，这时候 MyBatis 才会去调用 dataSource 对象来创建java.sql.Connection对象。也就是说，java.sql.Connection对象的创建一直延迟到执行SQL语句的时候。 @Test public void testSql() throws Exception &#123; InputStream in = Resources.getResourceAsStream(&quot;SqlMapConfig.xml&quot;); SqlSessionFactory factory = new SqlSessionFactoryBuilder().build(in); SqlSession sqlSession = factory.openSession(); List&lt;User&gt; list = sqlSession.selectList(&quot;findUserById&quot;,41); System.out.println(list.size()); &#125;只有当第 4 句 sqlSession.selectList(&quot;findUserById&quot;)，才会触发 MyBatis 在底层执行下面这个方法来创建 java.sql.Connection 对象。查看加载过程：通过断点调试，在 PooledDataSource 中找到如下 popConnection()方法，如下所示 **386行左右 if (!state.idleConnections.isEmpty()) &#123; // Pool has available connection //代表池子里有可用连接 conn = state.idleConnections.remove(0); if (log.isDebugEnabled()) &#123; log.debug(&quot;Checked out connection &quot; + conn.getRealHashCode() + &quot; from pool.&quot;); &#125; &#125; else &#123; // Pool does not have available connection if (state.activeConnections.size() &lt; poolMaximumActiveConnections) &#123; // Can create new connection conn = new PooledConnection(dataSource.getConnection(), this); if (log.isDebugEnabled()) &#123; log.debug(&quot;Created connection &quot; + conn.getRealHashCode() + &quot;.&quot;); &#125; &#125; else &#123; // Cannot create new connection PooledConnection oldestActiveConnection = state.activeConnections.get(0); long longestCheckoutTime = oldestActiveConnection.getCheckoutTime(); .... //此处省略一部分，暂时看不懂 &#125;大概意思就是：底层有两个池子，一个是空闲池，一个是存放活跃连接的池子，当获取连接的时候，首先去空闲池子看看有没有空闲的连接，有的话直接拿走，没有的话，就去活跃池子，看看到没到最大连接数，如果到了，就把最先活跃的连接拿过来，把里面绑定的数据都清了，然后拿来用。排序规则就是，拿走最先进来的，然后后面的依次向前补位，比如说拿走0，那么1就会向前补位，变成0，然后后面的依次向前补位。 2.事务控制2.1 JDBC事务回顾在 JDBC 中我们可以通过手动方式将事务的提交改为手动方式，通过 setAutoCommit()方法就可以调整。​ 查找jdk文档：默认为自动提交，当做单个事务处理，可以通过设置true，false来改变。​ 2.2 mybatis事务分析框架的本质也是调用jdk的这个方法，只是进行了一些处理。对于之前的增删该方法：通过查看控制台的日志，可以发现，mybatis对于增删改，默认提交方式是false，我们要在提交之后将他的提交方式设置为true，或者在session.getCommit()方法的（）里面直接传一个true。​ 四，动态SQL我们根据实体类的不同取值，使用不同的 SQL 语句来进行查询。比如在 id 如果不为空时可以根据 id 查询， 如果 username 不同空时还要加入用户名作为条件。这种情况在我们的多条件组合查询中经常会碰到。​ 1.抽取重复代码片段123456&lt;!-- 抽取重复的语句代码片段 --&gt;&lt;sql id=&quot;defaultSql&quot;&gt; select * from user&lt;/sql&gt;&lt;!-- 然后就可以在标签中对抽取出来的语句进行引用 --&gt;&lt;include refid=&quot;defaultSql&quot;/&gt; 2.if1234/** * 根据id查询 */User findById(User user); 12345678&lt;!--根据id查询--&gt;&lt;select id=&quot;findById&quot; parameterType=&quot;com.yhd.domain.User&quot; resultType=&quot;com.yhd.domain.User&quot;&gt; &lt;include refid=&quot;defaultSql&quot;/&gt; where 1=1 &lt;if test=&quot;id!=null and id!=&#x27;&#x27;&quot;&gt; and id = #&#123;id&#125;; &lt;/if&gt;&lt;/select&gt; 注意 where 1=1 的作用~！ 1234567&lt;!--Account selectByIdSelective(Integer id);--&gt;&lt;select id=&quot;selectByIdSelective&quot; parameterType=&quot;int&quot; resultMap=&quot;baseAccountMap&quot;&gt; select id ,name ,money from account where 1=1 &lt;if test=&quot;_parameter!=null and _parameter !=&#x27;&#x27;&quot;&gt; and id = #&#123;_parameter&#125; &lt;/if&gt;&lt;/select&gt; 3. where1234 /** * 根据id查询 */User findById(User user); 123456789&lt;!--根据id查询--&gt; &lt;select id=&quot;findById&quot; parameterType=&quot;com.yhd.domain.User&quot; resultType=&quot;com.yhd.domain.User&quot;&gt; &lt;include refid=&quot;defaultSql&quot;/&gt; &lt;where&gt; &lt;if test=&quot;id!=null and id!=&#x27;&#x27;&quot;&gt; and id = #&#123;id&#125; &lt;/if&gt; &lt;/where&gt; &lt;/select&gt; 有了where就不用写1=1了。​ 4.foreach1List&lt;Account&gt; selectByIds(Integer []ids); 12345678910111213&lt;!--List&lt;Account&gt; selectByIds(Integer []ids);--&gt;&lt;select id=&quot;selectByIds&quot; parameterType=&quot;int&quot; resultMap=&quot;baseAccountMap&quot;&gt; select id ,name ,money from account &lt;where&gt; &lt;if test=&quot;_parameter!=null and _parameter.size()&gt;0&quot;&gt; id in &lt;foreach collection=&quot;array&quot; item=&quot;id&quot; open=&quot;(&quot; close=&quot;)&quot; separator=&quot;,&quot;&gt; #&#123;id&#125; &lt;/foreach&gt; &lt;/if&gt; &lt;/where&gt;&lt;/select&gt; SQL 语句：select 字段 from user where id in (?)标签用于遍历集合，它的属性：collection:代表要遍历的集合元素，注意编写时不要写#{}open:代表语句的开始部分close:代表结束部分item:代表遍历集合的每个元素，生成的变量名sperator:代表分隔符 五，Mybatis多表查询1.一对一查询代码：mybatis-04 2.一对多查询代码：mybatis-05 3.多对多查询代码：mybatis-06​ 六，延迟加载延迟加载：就是在需要用到数据时才进行加载，不需要用到数据时就不加载数据。延迟加载也称懒加载. 好处：先从单表查询，需要时再从关联表去关联查询，大大提高数据库性能，因为查询单表要比关联查询多张表速度要快。​ 坏处：因为只有当需要用到数据时，才会进行数据库查询，这样在大批量数据查询时，因为查询工作也要消耗时间，所以可能造成用户等待时间变长，造成用户体验下降。 ​ 如何开启懒加载策略？​ 12345&lt;!-- 开启延迟加载的支持 --&gt;&lt;settings&gt; &lt;setting name=&quot;lazyLoadingEnabled&quot; value=&quot;true&quot;/&gt; &lt;setting name=&quot;aggressiveLazyLoading&quot; value=&quot;false&quot;/&gt;&lt;/settings&gt; 1.一对一懒加载​ mybatis-07​ 2.多对一懒加载mybatis-08​ 七，缓存机制1.一级缓存​ 一级缓存也叫本地缓存，MyBatis 的一级缓存是在会话（SqlSession）层面进行缓存的。MyBatis 的一级缓存是默认开启的，不需要任何的配置。 要在同一个会话里面共享一级缓存，这个对象肯定是在 SqlSession 里面创建的，作为 SqlSession 的一个属性。 DefaultSqlSession 里面只有两个属性，Configuration 是全局的，所以缓存只可能放在 Executor 里面维护——SimpleExecutor/ReuseExecutor/BatchExecutor 的父类BaseExecutor 的构造函数中持有了 PerpetualCache。 在同一个会话里面，多次执行相同的 SQL 语句，会直接从内存取到缓存的结果，不会再发送 SQL 到数据库。但是不同的会话里面，即使执行的 SQL 一模一样（通过一个Mapper 的同一个方法的相同参数调用），也不能使用到一级缓存。​ 一级缓存在 BaseExecutor 的 query()——queryFromDatabase()中存入。在queryFromDatabase()之前会 get()。 一级缓存是在 BaseExecutor 中的 update()方法中调用 clearLocalCache()清空的（无条件），query 中会判断。 一级缓存的 不足 使用一级缓存的时候，因为缓存不能跨会话共享，不同的会话之间对于相同的数据可能有不一样的缓存。在有多个会话或者分布式环境下，会存在脏数据的问题。如果要解决这个问题，就要用到二级缓存。​ 虽然在上面的代码中我们查询了两次，但最后只执行了一次数据库操作，这就是 Mybatis 提供给我们的一级缓存在起作用了。因为一级缓存的存在，导致第二次查询 id 为 41 的记录时，并没有发出 sql 语句从数据库中查询数据，而是从一级缓存中查询。​ 如何清空一级缓存？​ 一级缓存是 SqlSession 范围的缓存，当调用 SqlSession 的修改，添加，删除，commit()，close()等方法时，就会清空一级缓存。​ 第一次发起查询用户 id 为 1 的用户信息，先去找缓存中是否有 id 为 1 的用户信息，如果没有，从数据库查询用户信息。得到用户信息，将用户信息存储到一级缓存中。​ 如果 sqlSession 去执行 commit 操作（执行插入、更新、删除），清空 SqlSession 中的一级缓存，这样做的目的为了让缓存中存储的是最新的信息，避免脏读。​ 第二次发起查询用户 id 为 1 的用户信息，先去找缓存中是否有 id 为 1 的用户信息，缓存中有，直接从缓存中获取用户信息。​ 2.二级缓存二级缓存是用来解决一级缓存不能跨会话共享的问题的，范围是 namespace 级别的，可以被多个 SqlSession 共享（只要是同一个接口里面的相同方法，都可以共享），生命周期和应用同步。 作为一个作用范围更广的缓存，它肯定是在 SqlSession 的外层，否则不可能被多个SqlSession 共享。而一级缓存是在 SqlSession 内部的，所以，肯定是工作在一级缓存之前，也就是只有取不到二级缓存的情况下才到一个会话中去取一级缓存。 要跨会话共享的话，SqlSession 本身和它里面的 BaseExecutor 已经满足不了需求了，那我们应该在 BaseExecutor 之外创建一个对象。 实际上 MyBatis 用了一个装饰器的类来维护，就是 CachingExecutor。如果启用了二级缓存，MyBatis 在创建 Executor 对象的时候会对 Executor 进行装饰。CachingExecutor 对于查询请求，会判断二级缓存是否有缓存结果，如果有就直接返回，如果没有委派交给真正的查询器 Executor 实现类，比如 SimpleExecutor 来执行查询，再走到一级缓存的流程。最后会把结果缓存起来，并且返回给用户。​ ​ 二级缓存的开启与关闭​ 主配置文件12345678&lt;!-- 声明这个 namespace 使用二级缓存 --&gt;&lt;cache type=&quot;org.apache.ibatis.cache.impl.PerpetualCache&quot; size=&quot;1024&quot; eviction=&quot;LRU&quot; flushInterval=&quot;120000&quot; readOnly=&quot; false&quot;&gt; &lt;!--自动刷新时间 ms，未配置时只有调用时刷新 --&gt; &lt;!-- 回收策略--&gt; &lt;!-- 最多缓存对象个数，默认 1024--&gt; &lt;!-- 默认是 false（安全），改为 true 可读写时，对象必须支持序列化--&gt;&lt;/cache&gt; 因为 cacheEnabled 的取值默认就为 true，所以这一步可以省略不配置。为 true 代表开启二级缓存；为false 代表不开启二级缓存。​ mapper映射文件123456789&lt;mapper namespace=&quot;com.itheima.dao.IUserDao&quot;&gt; &lt;!-- 开启二级缓存的支持 --&gt; &lt;cache&gt;&lt;/cache&gt; &lt;!-- 根据 id 查询 --&gt; &lt;!-- 在此处将userCache属性设置为true --&gt; &lt;select id=&quot;findById&quot; resultType=&quot;user&quot; parameterType=&quot;int&quot; useCache=&quot;true&quot;&gt; select * from user where id = #&#123;uid&#125; &lt;/select&gt;&lt;/mapper&gt; 标签表示当前这个 mapper 映射将使用二级缓存，区分的标准就看 mapper 的 namespace 值。 将 UserDao.xml 映射文件中的标签中设置 useCache=”true”代表当前这个 statement 要使用 二级缓存，如果不使用二级缓存可以设置为 false。 注意：针对每次查询都需要最新的数据 sql，要设置成 useCache=false，禁用二级缓存。​ 当我们在使用二级缓存时，所缓存的类一定要实现 java.io.Serializable 接口，这种就可以使用序列化方式来保存对象。否则会报java,io.SerializableException。​ Mapper.xml 配置了之后，select()会被缓存。update()、delete()、insert()会刷新缓存。 如果 cacheEnabled=true，Mapper.xml 没有配置标签，还有二级缓存吗？还会出现 CachingExecutor 包装对象吗？ 只要 cacheEnabled=true 基本执行器就会被装饰。有没有配置，决定了在启动的时候会不会创建这个 mapper 的 Cache 对象，最终会影响到 CachingExecutorquery 方法里面的判断： 1if (cache != null) 如果某些查询方法对数据的实时性要求很高，不需要二级缓存，怎么办？ 可以在单个 Statement ID 上显式关闭二级缓存（默认是 true） 12&lt;select id=&quot;selectBlog&quot; resultMap=&quot;BaseResultMap&quot; useCache=&quot;false&quot;&gt;&lt;/select&gt; 为什么事务不提交，二级缓存不生效？ 因为二级缓存使用 TransactionalCacheManager（TCM）来管理，最后又调用了TransactionalCache的getObject()、putObject和commit()方法，TransactionalCache里面又持有了真正的 Cache 对象，比如是经过层层装饰的 PerpetualCache。在 putObject 的时候，只是添加到了 entriesToAddOnCommit 里面，只有它的commit()方法被调用的时候才会调用 flushPendingEntries()真正写入缓存。它就是在DefaultSqlSession 调用 commit()的时候被调用的。 为什么增删改操作会清空缓存？ 在 CachingExecutor 的 update()方法里面会调用 flushCacheIfRequired(ms)，isFlushCacheRequired 就是从标签里面渠道的 flushCache 的值。而增删改操作的flushCache 属性默认为 true。 第三方缓存 做 二级缓存 除了 MyBatis 自带的二级缓存之外，我们也可以通过实现 Cache 接口来自定义二级缓存。 MyBatis 官方提供了一些第三方缓存集成方式，比如 ehcache 和 redis。​ 八，Mybatis扩展1.批量操作在 MyBatis 里面是支持批量的操作的，包括批量的插入、更新、删除。我们可以直接传入一个 List、Set、Map 或者数组，配合动态 SQL 的标签，MyBatis 会自动帮我们生成语法正确的 SQL 语句。 1.1 批量插入批量插入的语法是这样的，只要在 values 后面增加插入的值就可以了。 1insert into tbl_emp (emp_id, emp_name, gender,email, d_id) values ( ?,?,?,?,? ) , ( ?,?,?,?,? ) , ( ?,?,?,?,? ) , ( ?,?,?,?,? ) ,( ?,?,?,?,? ) , ( ?,?,?,?,? ) , ( ?,?,?,?,? ) , ( ?,?,?,?,? ) , ( ?,?,?,?,? ) , ( ?,?,?,?,? ) 在 Mapper 文件里面，我们使用 foreach 标签拼接 values 部分的语句： 12345678910&lt;!-- 批量插入 --&gt;&lt;insert id=&quot;batchInsert&quot; parameterType=&quot;java.util.List&quot; useGeneratedKeys=&quot;true&quot;&gt; &lt;selectKey resultType=&quot;long&quot; keyProperty=&quot;id&quot; order=&quot;AFTER&quot;&gt; SELECT LAST_INSERT_ID() &lt;/selectKey&gt; insert into tbl_emp (emp_id, emp_name, gender,email, d_id) values &lt;foreach collection=&quot;list&quot; item=&quot;emps&quot; index=&quot;index&quot; separator=&quot;,&quot;&gt; (#&#123;emps.empId&#125;,#&#123;emps.empName&#125;,#&#123;emps.gender&#125;,#&#123;emps.email&#125;,#&#123;emps.dId&#125;) &lt;/foreach&gt;&lt;/insert&gt; Java 代码里面，直接传入一个 List 类型的参数。 效率要比循环发送 SQL 执行要高得多。最关键的地方就在于减少了跟数据库交互的次数，并且避免了开启和结束事务的时间消耗。 1.2 批量更新批量更新的语法是这样的，通过 case when，来匹配 id 相关的字段值。 1234567891011121314151617update tbl_emp setemp_name = case emp_id when ? then ? when ? then ? when ? then ? end , gender = case emp_id when ? then ? when ? then ? when ? then ? end , email = case emp_id when ? then ? when ? then ? when ? then ? endwhere emp_id in ( ? , ? , ? ) 所以在 Mapper 文件里面最关键的就是 case when 和 where 的配置。需要注意一下 open 属性和 separator 属性。 12345678910111213141516171819202122&lt;update id=&quot;updateBatch&quot;&gt; update tbl_emp set emp_name = &lt;foreach collection=&quot;list&quot; item=&quot;emps&quot; index=&quot;index&quot; separator=&quot; &quot; opene=&quot;case emp_id&quot; close=&quot;end&quot;&gt; when #&#123;emps.empId&#125; then #&#123;emps.empName&#125; &lt;/ foreach&gt; ,gender = &lt;foreach collection=&quot;list&quot; item=&quot;emps&quot; index=&quot;index&quot; separator=&quot; &quot; opene=&quot;case emp_id&quot; close=&quot;end&quot;&gt; when #&#123;emps.empId&#125; then #&#123;emps.gender&#125; &lt;/foreach&gt; ,email = &lt;foreach collection=&quot;list&quot; item=&quot;emps&quot; index=&quot;index&quot; separator=&quot; &quot; opene=&quot;case emp_id&quot; close=&quot;end&quot;&gt; when #&#123;emps.empId&#125; then #&#123;emps.email&#125; &lt;/foreach&gt; where emp_id in &lt;foreach collection=&quot;list&quot; item=&quot;emps&quot; index=&quot;index&quot; separator=&quot;,&quot; open=&quot;(&quot; close=&quot;)&quot;&gt; #&#123;emps.empId&#125; &lt;/foreach&gt;&lt;/update&gt; 1.3 BatchExecutor当然 MyBatis 的动态标签的批量操作也是存在一定的缺点的，比如数据量特别大的时候，拼接出来的 SQL 语句过大。 MySQL 的服务端对于接收的数据包有大小限制，max_allowed_packet 默认是4M，需要修改默认配置才可以解决这个问题。在我们的全局配置文件中，可以配置默认的 Executor 的类型。其中有一种BatchExecutor。 1&lt;setting name =&quot;defaultExecutorType&quot; value =&quot;BATCH&quot; /&gt; 也可以在创建会话的时候指定执行器类型 1SqlSession session = sqlSessionFactory.openSession(ExecutorType. BATCH ); BatchExecutor 底层是对 JDBC ps.addBatch()的封装，原理是攒一批 SQL 以后再发。 2.翻页在我们查询数据库的操作中，有两种翻页方式，一种是逻辑翻页（假分页），一种是物理翻页（真分页）。逻辑翻页的原理是把所有数据查出来，在内存中删选数据。 物理翻页是真正的翻页，比如 MySQL 使用 limit 语句，Oracle 使用 rownum 语句，SQLServer 使用 top 语句。 2.1逻辑翻页MyBatis 里面有一个逻辑分页对象 RowBounds，里面主要有两个属性，offset 和limit（从第几条开始，查询多少条）。 我们可以在Mapper接口的方法上加上这个参数，不需要修改xml里面的SQL语句。 1public List&lt;Blog&gt; selectBlogList(RowBounds rowBounds); 它的底层其实是对 ResultSet 的处理。它会舍弃掉前面 offset 条数据，然后再取剩下的数据的 limit 条。 如果数据量大的话，这种翻页方式效率会很低（跟查询到内存中再使用subList(start,end)没什么区别）。所以我们要用到物理翻页。 2.2物理翻页物理翻页是真正的翻页，它是通过数据库支持的语句来翻页。 第一种简单的办法就是传入参数（或者包装一个 page 对象），在 SQL 语句中翻页。 123&lt;select id=&quot;selectBlogPage&quot; parameterType=&quot;map&quot; resultMap=&quot;BaseResultMap&quot;&gt; select * from blog limit #&#123;curIndex&#125; , #&#123;pageSize&#125;&lt;/ select&gt; 第一个问题是我们要在 Java 代码里面去计算起止序号；第二个问题是：每个需要翻页的 Statement 都要编写 limit 语句，会造成 Mapper 映射器里面很多代码冗余。 需要一种通用的方式，不需要去修改配置的任何一条 SQL 语句，只要在我们需要翻页的地方封装一下翻页对象就可以了。 使用翻页的插件，这个是基于 MyBatis 的拦截器实现的，比如 PageHelper。 123456// pageSize 每一页几条PageHelper. startPage (pn, 10);List&lt;Employee&gt; emps = employeeService.getAll();// navigatePages 导航页码数PageInfo page = w new PageInfo(emps, 10);return Msg. success ().add( &quot;pageInfo&quot;, page); 3.通用Mapper问题：当我们的表字段发生变化的时候，我们需要修改实体类和 Mapper 文件定义的字段和方法。如果是增量维护，那么一个个文件去修改。如果是全量替换，我们还要去对比用 MBG 生成的文件。字段变动一次就要修改一次，维护起来非常麻烦。 解决这个问题，我们有两种思路。 第 一 个 ， 因 为 MyBatis 的 Mapper 是 支 持 继 承 的 。 所 以 我 们 可 以 把 我 们 的Mapper.xml 和 Mapper 接口都分成两个文件。一个是 MBG 生成的，这部分是固定不变的。然后创建 DAO 类继承生成的接口，变化的部分就在 DAO 里面维护。 GitHub地址 4.MyBatis-PlusMyBatis-Plus 是原生 MyBatis 的一个增强工具，可以在使用原生 MyBatis 的所有功能的基础上，使用 plus 特有的功能。​ 九，Mybatis注解开发1.常用注解 @Insert:实现新增@Update:实现更新@Delete:实现删除@Select:实现查询@Result:实现结果集封装@Results:可以与@Result 一起使用，封装多个结果集@ResultMap:实现引用@Results 定义的封装@One:实现一对一结果集封装@Many:实现一对多结果集封装@SelectProvider: 实现动态 SQL 映射@CacheNamespace:实现注解二级缓存的使用 ​ 2.使用Mybatis注解实现CRUD2.1 复杂映射实现复杂关系映射之前我们可以在映射文件中通过配置来实现，在使用注解开发时我们需要借助@Results 注解，@Result 注解，@One 注解，@Many 注解。​ @Results 注解代替的是标签该注解中可以使用单个@Result 注解，也可以使用@Result 集合@Results（{@Result（），@Result（）}）或@Results（@Result（））@Resutl 注解代替了 标签和标签@Result 中 属性介绍：id 是否是主键字段column 数据库的列名property 需要装配的属性名one 需要使用的@One 注解（@Result（one=@One）（）））many 需要使用的@Many 注解（@Result（many=@many）（）））@One 注解（一对一）代替了标签，是多表查询的关键，在注解中用来指定子查询返回单一对象。@One 注解属性介绍：select 指定用来多表查询的 sqlmapperfetchType 会覆盖全局的配置参数 lazyLoadingEnabled。。使用格式：@Result(column=” “,property=””,one=@One(select=””))@Many 注解（多对一）代替了标签,是是多表查询的关键，在注解中用来指定子查询返回对象集合。注意：聚集元素用来处理“一对多”的关系。需要指定映射的 Java 实体类的属性，属性的 javaType（一般为 ArrayList）但是注解中可以不定义；使用格式：@Result(property=””,column=””,many=@Many(select=””)) 2.2 二级缓存 主配置文件 ​ 12345&lt;!-- 配置二级缓存 --&gt; &lt;settings&gt; &lt;!-- 开启二级缓存的支持 --&gt; &lt;setting name=&quot;cacheEnabled&quot; value=&quot;true&quot;/&gt;&lt;/settings&gt; 映射文件1234@CacheNamespace(blocking=true)//mybatis 基于注解方式实现配置二级缓存public interface UserMapper &#123; &#125;","categories":[{"name":"MyBatis","slug":"MyBatis","permalink":"https://yinhuidong.github.io/categories/MyBatis/"}],"tags":[{"name":"MyBatis","slug":"MyBatis","permalink":"https://yinhuidong.github.io/tags/MyBatis/"}]},{"title":"MyBatis源码分析","slug":"MyBatis/MyBatis源码分析","date":"2022-01-11T06:37:14.818Z","updated":"2022-01-11T06:41:11.784Z","comments":true,"path":"2022/01/11/MyBatis/MyBatis源码分析/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/MyBatis/MyBatis%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","excerpt":"","text":"MyBatis SQL Mapper Framework for Java The MyBatis SQL mapper framework makes it easier to use a relational database with object-oriented applications.MyBatis couples objects with stored procedures or SQL statements using an XML descriptor or annotations.Simplicity is the biggest advantage of the MyBatis data mapper over object relational mapping tools. 一，Mybatis原生使用方式1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public class EsTest &#123; /** * 字节流 */ private InputStream in; /** * 数据库连接工厂 */ private SqlSessionFactory sqlSessionFactory; /** * 数据库连接 */ private SqlSession sqlSession; &#123; try &#123; //将配置文件以二进制字节流的方式加载到内存 in = Resources.getResourceAsStream(&quot;mybatis-config.xml&quot;); //通过构建者模式创建一个数据库连接工厂 sqlSessionFactory = new SqlSessionFactoryBuilder().build(in); //通过工厂来管理和获取数据库连接 sqlSession = sqlSessionFactory.openSession(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; public void query() &#123; //获取一个指定类型的mapper代理对象 UserMapper userMapper = sqlSession.getMapper(UserMapper.class); //代理对象执行目标方法 List&lt;Object&gt; users = userMapper.queryUser(1L); users.forEach(System.out::println); &#125; private static interface UserMapper &#123; List&lt;Object&gt; queryUser(Long id); &#125; /** * 利用对象的finalize 方法进行资源回收 * @throws Throwable */ @Override protected void finalize() throws Throwable &#123; sqlSession.commit(true); assert sqlSession != null; sqlSession.close(); assert in != null; try &#123; in.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 分析下流程： 通过Resources对象加载Mybatis主配置文件到内存形成一个二进制的字节流。 将上一步生成的二进制字节流当做参数传递到SqlSessionFactoryBuilder 的build()，用来构建数据库连接工厂 SqlSessionFactory。 通过生成的数据库连接工厂获取数据库连接。 通过数据库连接获取指定类型的Mapper的代理对象。 通过代理对象调用方法，获得结果。 关闭数据库连接，关闭字节流。 二，Mybatis源码执行流程分析 首先，通过Resources对象里面的方法去加载配置文件，这里会默认传入一个类加载器数组，循环尝试使用各种类加载器加载配置文件，直到获取到二进制字节流，如果最终仍然没有获取到，会抛出异常。 如果成功加载配置文件生成了二进制字节流，那么会将二进制字节流传入到SqlSessionFactoryBuilder的build方法，为生成一个数据库连接工厂对象 SqlSessionFactory 对象赋能。 首先在build方法里面会去构建一个xml解析器对象 XMLConfigBuilder，用来解析主配置文件。 通过解析器的parse()返回了一个Configuration对象，用来构建数据库连接工厂。 判断是否已经加载过主配置文件，如果已经加载过，会抛出异常。 从configuration标签开始解析配置文件。 mybatis的主配置文件里面的标签是有顺序的，他会按照顺序来解析配置文件中的标签，重点在于解析mappers标签。 他会循环获取到所有的mapper.xml文件，利用mapper解析器解析mapper.xml文件，解析封装的到Configuration对象中。 具体的解析过程：首先去定位根标签mapper，然后绑定mapper的命名空间，把所有的命名空间放到configuration对象的一个set集合里面，把当前mapper的类型通过MapperRegistry对象添加到一个map里面。 具体放到Mapper注册中心的其实是Mapper对象的类型和Mapper接口的代理工厂。 最终解析完mappers标签以后，返回了一个Configuration对象。 最终通过build方法返回了一个默认的数据库连接工厂对象，DefaultSqlSessionFactory，这个工厂里面持有一个Configuration对象。 获取到数据库连接工厂以后就是去通过openSession()去获取数据库连接。 SqlSession的获取主要是通过SqlSessionFactory的默认实现类DefaultSqlSessionFactory的openSessionFromDataSource封装一个DefaultSqlSession(实现SqlSession接口）返回。 首先通过Configuration对象获取环境信息。 再通过环境信息获取事务工厂，事务工厂主要是看配置文件有没有配置，没有配置的话就创建一个新的。 通过事务工厂来获取事务对象。 通过一个Configuration对象来创建一个Executor对象，使用它来执行SQL。 判断executorType的类型，分为批处理，可复用和普通三种类型的Executor对象。 SimpleExecutor：每执行一次 update 或 select，就开启一个 Statement 对象，用完立刻关闭 Statement 对象 ReuseExecutor：用完后，不关闭 Statement 对象，而是放置于 Map 内，供下一次使用 BatchExecutor：里面缓存了多个statement用来做批处理 上面选择完具体的Executor对象后，判断是否开启了二级缓存，如果开启了二级缓存的话，使用装饰器模式对Executor进行一个包装，生成一个CachingExecutor对象，里面持有一个Executor对象。 在此处会执行插件的拦截器链，这个拦截器链是mybatis的一个很核心的扩展点机制，最终会返回一个executor对象。 回头看一下Configuration对象，这个Configuration对象很有意思，里面间接的持有几个对象 ，为什么是间接？因为类里面没有这几个对象的属性，但是却可以通过当前类创建这几个对象。 Executor 对象 StatementHandler 对象 PameterHandler 对象 ResultSetHandler 对象 mybatis的插件拦截器会对这四个接口进行拦截，也就是说会对这四种对象生成代理对象，mybatis 的拦截器用到了责任链+代理+反射机制。（通过源码可以知道：所有可能被拦截的处理类都会生成一个代理类，如果有N个拦截器，就会有N个代理，层层生成动态代理是比较消耗性能的。而且虽然能指定插件拦截的位置，但这个是在执行方法的时候利用反射动态判断的，初始化的时候就是简单的把拦截器插入到了所有可以拦截的地方。所以尽量不要编写不必要的拦截器。）其实mybatis的插件实现原理和spring的aop的实现原理是一样的，就是一个多重的代理，多重的代理有两种实现方式，一个是通过责任链 返回一个默认的 SQLSession ，这个DefaultSQLSession里面持有 Configuration对象 executor对象 ，executor对象里面持有一个事务对象。 通过SqlSession对象的getMapper方法获取一个指定类型的mapper代理对象。 数据库连接对象里面的getMapper实际上调用了configuration对象的getMapper方法。 通过mapperRegistry去获取一个mapper的代理对象。 前面解析配置文件的时候，将mapper对象类型和mapper的代理工厂封装到了一个map，这里实际上从这个map里面拿出来了一个mapper的代理工厂。 使用代理工厂去创建对象，通过传递数据库连接去创建一个mapperProxy对象，这个mapperProxy实现了InvocationHandler接口。 通过mapperProxy返回一个代理对象，实际上就是使用JDK的动态代理创建一个代理对象。 当代理对象执行目标方法的时候：实际上就是执行mapperProxy的invoke方法。 这里对目标方法进行一个包装，生成一个invoker，通过invoker执行invoke()。 实际上这里调用了MapperMethod的execute方法。 在execute方法里面实际上就是判断执行的增删改查的类型，然后调用SqlSession的crud方法。(动态代理实际上就是生成了一个statement的字符串，然后调用SqlSession的crud方法。) 以sqlSession.selectOne()进行分析 从configuration对象构建一个MappedStatement对象，然后执行executor的query方法进行查询，executor分为三种： 一个是批处理的 一个是走二级缓存的 一个是BaseExecutor，直接执行的 接下来看executor的query方法： 组装构建待执行的SQL。 创建一级缓存的缓存key，一级缓存默认是开启的。 方法重载query() 判断如果命中一级缓存的话，直接返回。 否则的话，queryFromDatabase 直接去查询数据库 委派给子类取走真正的查询逻辑，然后将查询结果房放到一级缓存。 在子类里面通过原生jdbc的prepareStatement执行查询sql，查询之后通过ResultHandler对象去处理结果，最终返回。 二十中文注释版源码地址：https://gitee.com/yin_huidong/mybatis-3.git 三，mybatis整体架构分析mybatis的整体架构分为三层，分别是基础支持层，核心处理层，接口层。 接口层核心对象是 SqlSession，它是上层应用和 MyBatis打交道的桥梁，SqlSession 上定义了非常多的对数据库的操作方法。接口层在接收到调用请求的时候，会调用核心处理层的相应模块来完成具体的数据库操作。 核心处理层既然叫核心处理层，也就是跟数据库操作相关的动作都是在这一层完成的。 核心处理层主要做了这几件事： 把接口中传入的参数解析并且映射成 JDBC 类型； 解析 xml 文件中的 SQL 语句，包括插入参数，和动态 SQL 的生成； 执行 SQL 语句； 处理结果集，并映射成 Java 对象。 插件也属于核心层，这是由它的工作方式和拦截的对象决定的。 基础支持层基础支持层主要是一些抽取出来的通用的功能（实现复用），用来支持核心处理层的功能。比如数据源、缓存、日志、xml 解析、反射、IO、事务等等这些功能。 2.再看mybatis的SQL执行流程 SQL语句的执行设涉及到很多个组件，其中比较重要的就是Executor，StatementHandler，ParameterHandler，ResultSetHandler。Executor主要负责维护一级缓存和二级缓存，并提供事务管理的相关操作。他会将数据库相关的操作交给StatementHandler完成。StatementHandler首先通过ParameterHandler完成SQL语句的实参绑定，然后通过jdk内置的Statement对象执行SQL语句并得到结果集，最后通过ResultSetHandler完成结果集的映射，得到结果对象并返回。 3. 核心对象生命周期3.1 SqlSessionFactoryBuiler它 是 用 来 构 建 SqlSessionFactory 的 ， 而SqlSessionFactory 只需要一个，所以只要构建了这一个 SqlSessionFactory，它的使命就完成了，也就没有存在的意义了。所以它的生命周期只存在于方法的局部。 3.2 SqlSessionFactorySqlSessionFactory 是用来创建 SqlSession 的，每次应用程序访问数据库，都需要创建一个会话。因为我们一直有创建会话的需要，所以 SqlSessionFactory 应该存在于应用的整个生命周期中（作用域是应用作用域）。创建 SqlSession 只需要一个实例来做这件事就行了，否则会产生很多的混乱，和浪费资源。所以我们要采用单例模式。 3.3 SqlSessionSqlSession 是一个会话，因为它不是线程安全的，不能在线程间共享。所以我们在请求开始的时候创建一个 SqlSession 对象，在请求结束或者说方法执行完毕的时候要及时关闭它（一次请求或者操作中）。 3.4 MapperMapper（实际上是一个代理对象）是从 SqlSession 中获取的。它的作用是发送 SQL 来操作数据库的数据。它应该在一个 SqlSession 事务方法之内。 对象 生命周期 SqlSessionFactoryBuiler 方法局部（method） SqlSessionFactory（单例） 应用级别（application） SqlSession 请求和操作（request/method） Mapper 方法（method） 四，扩展：PageHelper原理上面分析源码的时候其实已经分析过，mybatis的拦截器实际上就是代理模式加拦截器来实现的（同AOP），而pagehelper实际上是基于插件机制实现的。 先看 PageHelper jar 包中 PageInterceptor 的源码。拦截的是 Executor 的两个query()方法。在这里对 SQL 进行了改写。 跟踪到最后，是在 MySqlDialect.getPageSql()对 SQL 进行了改写，翻页参数是从一个 Page 对象中拿到的，那么 Page 对象是怎么传到这里的呢？ 上一步，AbstractHelperDialect.getPageSql()中：Page 对象是从一个 ThreadLocal&lt;&gt;变量中拿到的，那它是什么时候赋值的？ PageHelper.startPage()方法，把分页参数放到了 ThreadLocal&lt;&gt;变量中。 扩展：插件机制的应用场景： 作用 实现方式 水平分表 对 query update 方法进行拦截在接口上添加注解，通过反射获取接口注解，根据注解上配置的参数进行分表，修改原 SQL，例如 id 取模，按月分表 数据加解密 update——加密；query——解密获得入参和返回值 菜单权限控制 对 query 方法进行拦截在方法上添加注解，根据权限配置，以及用户登录信息，在 SQL 上加上权限过滤条件 五，整合Spring大部分时候我们不会在项目中单独使用 MyBatis 的工程，而是集成到 Spring 里面使用，但是却没有看到这三个对象在代码里面的出现。我们直接注入了一个 Mapper 接口，调用它的方法。 SqlSessionFactory 是什么时候创建的？ SqlSession 去哪里了？为什么不用它来 getMapper？ 为什么@Autowired 注入一个接口，在使用的时候却变成了代理对象？在 IOC的容器里面我们注入的是什么？ 注入的时候发生了什么事情？ 1.关键配置12345&lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt; &lt;version&gt;2.0.6&lt;/version&gt;&lt;/dependency&gt; 然后在 Spring 的 applicationContext.xml 里面配置 SqlSessionFactoryBean，它是用来帮助我们创建会话的，其中还要指定全局配置文件和 mapper 映射器文件的路径。 12345&lt;bean id=&quot;sqlSessionFactory&quot; class=&quot;org.mybatis.spring.SqlSessionFactoryBean&quot;&gt; &lt;property name=&quot;configLocation&quot; value=&quot;classpath:mybatis-config.xml&quot;&gt;&lt;/ property&gt; &lt;property name=&quot;mapperLocations&quot; value=&quot;classpath:mapper/*.xml&quot;&gt;&lt;/ property&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt;&lt;/ bean&gt; 然后在 applicationContext.xml 配置需要扫描 Mapper 接口的路径。 123&lt;bean id=&quot;mapperScanner&quot; class=&quot;org.mybatis.spring.mapper.MapperScannerConfigurer&quot;&gt; &lt;property name=&quot;basePackage&quot; value=&quot;com.yhd.crud.dao&quot;/&gt;&lt;/ bean&gt; 1&lt;mybatis-springn:scan base-package =&quot;com.yhd.crud.dao&quot;/&gt; 1@MapperScan( &quot;com.yhd.crud.dao&quot;) Spring 对 MyBatis 的对象进行了管理，但是并不会替换 MyBatis 的核心对象。也就意味着：MyBatis jar 包中的 SqlSessionFactory、SqlSession、MapperProxy 这些都会用到。而 mybatis-spring.jar 里面的类只是做了一些包装或者桥梁的工作。 2.创建会话工厂我们在 Spring 的配置文件中配置了一个 SqlSessionFactoryBean，我们来看一下这个类。 它实现了 InitializingBean 接口，所以要实现 afterPropertiesSet()方法，这个方法会在 bean 的属性值设置完的时候被调用 另外它实现了 FactoryBean 接口，所以它初始化的时候，实际上是调用 getObject()方法，它里面调用的也是 afterPropertiesSet()方法。 在 afterPropertiesSet()方法里面：解析配置文件，指定事务工厂。 3.创建SqlSession3.1可以直接使用 DefaultSqlSession 吗？现在已经有一个 DefaultSqlSessionFactory，按照编程式的开发过程，我们接下来就会创建一个 SqlSession 的实现类，但是在 Spring 里面，我们不是直接使用DefaultSqlSession 的，而是对它进行了一个封装，这个 SqlSession 的实现类就是SqlSessionTemplate。这个跟 Spring 封装其他的组件是一样的，比如 JdbcTemplate，RedisTemplate 等等，也是 Spring 跟 MyBatis 整合的最关键的一个类。 为什么不用 DefaultSqlSession？它是线程不安全的，注意看类上的注解：而 SqlSessionTemplate 是线程安全的。 1Note that this class is not Thread-Safe. 3.2怎么拿到一个 SqlSessionTemplate ？MyBatis提供了一个 SqlSessionDaoSupport，里面持有一个SqlSessionTemplate 对象，并且提供了一个 getSqlSession()方法，让我们获得一个SqlSessionTemplate。 123456public abstract class SqlSessionDaoSupport extends DaoSupport &#123; private SqlSessionTemplate sqlSessionTemplate; public SqlSession getSqlSession() &#123; return this.sqlSessionTemplate; &#125;&#125; 先创建一个 BaseDao 继承 SqlSessionDaoSupport。在BaseDao 里面封装对数据库的操作，包括 selectOne()、selectList()、insert()、delete()这些方法，子类就可以直接调用。 然后让我们的实现类继承 BaseDao 并且实现我们的 DAO 层接口，这里就是我们的Mapper 接口。实现类需要加上@Repository 的注解。 在实现类的方法里面，我们可以直接调用父类（BaseDao）封装的 selectOne()方法，那么它最终会调用 sqlSessionTemplate 的 selectOne()方法。 3.3有没有更好的拿到 SqlSessionTemplate我们的每一个 DAO 层的接口（Mapper 接口也属于），如果要拿到一个 SqlSessionTemplate，去操作数据库，都要创建实现一个实现类，加上@Repository 的注解，继承 BaseDao，这个工作量也不小。 另外一个，我们去直接调用 selectOne()方法，还是出现了 Statement ID 的硬编码，MapperProxy 在这里根本没用上。 4.接口的扫描注册在 applicationContext.xml 里 面 配 置 了 一 个MapperScannerConfigurer。 MapperScannerConfigurer 实现了 BeanDefinitionRegistryPostProcessor 接口，BeanDefinitionRegistryPostProcessor 是 BeanFactoryPostProcessor 的子类，可以通过编码的方式修改、新增或者删除某些 Bean 的定义。 我们只需要重写 postProcessBeanDefinitionRegistry()方法，在这里面操作 Bean就可以了。 在这个方法里面： scanner.scan() 方 法 是 ClassPathBeanDefinitionScanner 中 的 ， 而 它 的 子 类ClassPathMapperScanner 覆 盖 了 doScan() 方 法 ， 在 doScan() 中 调 用 了processBeanDefinitions，它先调用父类的 doScan()扫描所有的接口。 processBeanDefinitions 方法里面，在注册 beanDefinitions 的时候，BeanClass被改为 MapperFactoryBean（注意灰色的注释）。 为什么要把 BeanClass 修改成 MapperFactoryBean，这个类有什么作用？ MapperFactoryBean 继 承 了 SqlSessionDaoSupport ， 可 以 拿 到SqlSessionTemplate。 5.接口注入使用我们使用 Mapper 的时候，只需要在加了 Service 注解的类里面使用@Autowired注入 Mapper 接口就好了。 123456789 @Service public class EmployeeService &#123; @Autowired EmployeeMapper employeeMapper; public List&lt;Employee&gt; getAll() &#123; return employeeMapper.selectByMap( null); &#125;&#125; Spring 在启动的时候需要去实例化 EmployeeService。 EmployeeService 依赖了 EmployeeMapper 接口（是 EmployeeService 的一个属性）。 Spring 会根据 Mapper 的名字从 BeanFactory 中获取它的 BeanDefination，再从BeanDefination 中 获 取 BeanClass ，EmployeeMapper 对 应 的 BeanClass 是MapperFactoryBean（上一步已经分析过）。 接下来就是创建 MapperFactoryBean，因为实现了 FactoryBean 接口，同样是调用 getObject()方法。 1234// MapperFactoryBean.java public T getObject() throws Exception &#123; return getSqlSession().getMapper( this. mapperInterface);&#125; 因为 MapperFactoryBean 继 承 了 SqlSessionDaoSupport ， 所 以 这 个getSqlSession()就是调用父类的方法，返回 SqlSessionTemplate。 1234// SqlSessionDaoSupport.javapublic SqlSession getSqlSession() &#123; return this. sqlSessionTemplate;&#125; 我们注入到 Service 层的接口，实际上还是一个 MapperProxy 代理对象。所以最后调用 Mapper 接口的方法，也是执行 MapperProxy 的 invoke()方法。 DaoSupport ， 所 以 这 个getSqlSession()就是调用父类的方法，返回 SqlSessionTemplate。 1234// SqlSessionDaoSupport.javapublic SqlSession getSqlSession() &#123; return this. sqlSessionTemplate;&#125; 我们注入到 Service 层的接口，实际上还是一个 MapperProxy 代理对象。所以最后调用 Mapper 接口的方法，也是执行 MapperProxy 的 invoke()方法。 6.总结​ Mybatis整合Spring框架首先利用的是Spring框架的SPI机制，在项目的META-INF目录下有一个文件【spring.handlers】，里面给Spring容器中导入了一个类【NamespaceHandler】。【NamespaceHandler】 继承关系上：实现了spring的扩展点接口 【init】给容器中注册了一个【BeanDefinitionParser】bean定义信息的解析器 【MapperScannerBeanDefinitionParser】【MapperScannerBeanDefinitionParser】 他会在spring容器创建过程中去解析【mapperScanner】标签 解析出来的属性会给【MapperScannerConfigurer】赋能另一个需要配置的bean就是【SqlSessionFactoryBean】 继承关系 FactoryBean InitializingBean 初始化的时候会执行【afterPropertiesSet()】 ApplicationListener afterPropertiesSet() buildSqlSessionFactory() 创建Mybatis的主配置文件解析器，解析主配置文件 创建mapper映射文件的解析器，解析mapper映射文件 构建出一个Configuration对象传入到【SqlSessionFactoryBuilder】的【build()】 最终返回了一个【SqlSessionFactory】对象实例【MapperScannerConfigurer】 继承关系 【BeanDefinitionRegistryPostProcessor】 【postProcessBeanDefinitionRegistry()】 在系统初始化的过程中被调用，扫描了配置文件中配置的basePackage 下的所有 Mapper 类，最终生成 Spring 的 Bean 对象，注册到容器中 这里面调用了包扫描的方法【scanner.scan()】经过一系列调用，调用到了 【ClassPathMapperScanner】的【doScan】 调用父类的doScan()方法，遍历basePackages中指定的所有包，扫描每个包下的Java文件并进行解析。 使用之前注册的过滤器进行过滤，得到符合条件的BeanDefinitionHolder对象 【processBeanDefinitions()】处理扫描得到的BeanDefinitionHolder集合 循环 将BeanDefinition中记录的Bean类型修改为【MapperFactoryBean】 将扫描到的接口类型作为构造方法的参数 构造MapperFactoryBean的属性，将sqlSessionFactory、sqlSessionTemplate 等信息填充到BeanDefinition中，修改自动注入方式 重新注册到容器 ClassPathMapperScanner 在处理 Mapper 接口的时候用到了 MapperFactoryBean 类，动态代理的实现，可以直接将 Mapper 接口注入到 Service 层的 Bean 中，这样就不需要编写任何 DAO 实现的代码。【MapperFactoryBean】 继承关系 InitializingBean DaoSupport SqlSessionDaoSupport FactoryBean MapperFactoryBean MapperFactoryBean 类的动态代理功能是通过实现了 Spring 提供的 FactoryBean 接口实现的，该接口是一个用于创建 Bean 对象的工厂接口，通过 getObject() 方法获取真实的对象。 【getObject()】 【getSqlSession().getMapper(this.mapperInterface)】 【getSqlSession()】是她父类【SqlSessionDaoSupport】的方法 【getMapper()】 通过sqlSession获取到mapper代理对象这里面涉及到了一个类 【SqlSessionDaoSupport】 构造器 通过 Spring 容器自动注入 sqlSessionFactory 属性 【createSqlSessionTemplate()】 创建了一个【SqlSessionTemplate】对象并且里面持有【SqlSessionFactory】【SqlSessionTemplate】 SqlSessionTemplate 实现了 SqlSession 接口，在 MyBatis 与 Spring 集成开发时，用来代替 MyBatis 中的 DefaultSqlSession 的功能。 SqlSessionTemplate 是线程安全的，可以在 DAO 之间共享使用，比如上面生成的 Mapper 对象会持有一个 SqlSessionTemplate 对象，每次请求都会共用该对象。 在 MyBatis 中 SqlSession 的 Scope 是会话级别，请求结束后需要关闭本次会话，怎么集成了 Spring 后，可以共用了？ 首先，在集成 Spring 后，Mapper 对象是单例，由 Spring 容器管理，供 Service 层使用，SqlSessionTemplate 在设计的时候，功能分成了如下两部分： 1. 获取 MapperProxy 代理对象； 2. 执行 SQL 操作，该部分功能通过代理对象 SqlSessionInterceptor 实现； 【构造器】 sqlSession通过动态代理来创建的，【SqlSessionInterceptor】实现了 【InvocationHandler】当调用mapper里面的方法的时候，就会执行【SqlSessionInterceptor】的【invoke()】 通过SqlSessionUtils.getSqlSession()获取SqlSession对象，同一个事务共享SqlSession 通过【invoke】调用SqlSession对象的相应方法 检测事务是否由Spring进行管理，并据此决定是否提交事务。​ Mybatis-Spring 中文注释源码地址：https://gitee.com/yin_huidong/mybatis-spring.git\u0000","categories":[{"name":"MyBatis","slug":"MyBatis","permalink":"https://yinhuidong.github.io/categories/MyBatis/"}],"tags":[{"name":"MyBatis","slug":"MyBatis","permalink":"https://yinhuidong.github.io/tags/MyBatis/"}]},{"title":"Spring[十一]注解版Aop流程分析","slug":"Spring/Spring[十一]注解版Aop流程分析","date":"2022-01-11T06:13:44.508Z","updated":"2022-01-11T06:18:16.601Z","comments":true,"path":"2022/01/11/Spring/Spring[十一]注解版Aop流程分析/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/Spring/Spring[%E5%8D%81%E4%B8%80]%E6%B3%A8%E8%A7%A3%E7%89%88Aop%E6%B5%81%E7%A8%8B%E5%88%86%E6%9E%90/","excerpt":"","text":"上一篇介绍了实现AOP的两种方式，本篇我们通过分析源码流程，来看一下注解版AOP的实现。具体的源码细节，会在后面的篇章一行行翻译。 1.开启AOP的功能读源码需要找到入口或者抓手，AOP的源码我们如何入手呢？想要使用AOP的功能就需要在Spring的配置类上加上**@EnableAspectJAutoProxy**注解。​ 先分析一下这个注解：\u0000 12345678910111213141516171819@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Import(AspectJAutoProxyRegistrar.class)public @interface EnableAspectJAutoProxy &#123; /** * 是否要创建基于子类 (CGLIB) 的代理，而不是基于标准 Java 接口的代理。 默认值为false 。 * @return */ boolean proxyTargetClass() default false; /** * 代理应由 AOP 框架公开为ThreadLocal以通过AopContext类进行检索。 * 默认关闭，即不保证AopContext访问将起作用。 */ boolean exposeProxy() default false;&#125; 可以看到这个注解的底层有一个**@Import(AspectJAutoProxyRegistrar.class)**，他往容器中导入了一个组件**AspectJAutoProxyRegistrar**。​ 看一下这个组件：​ 12345678910111213141516171819202122class AspectJAutoProxyRegistrar implements ImportBeanDefinitionRegistrar &#123; @Override public void registerBeanDefinitions( AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) &#123; /*导入组件到容器中 AnnotationAwareAspectJAutoProxyCreator*/ AopConfigUtils.registerAspectJAnnotationAutoProxyCreatorIfNecessary(registry); AnnotationAttributes enableAspectJAutoProxy = AnnotationConfigUtils.attributesFor(importingClassMetadata, EnableAspectJAutoProxy.class); if (enableAspectJAutoProxy != null) &#123; if (enableAspectJAutoProxy.getBoolean(&quot;proxyTargetClass&quot;)) &#123; AopConfigUtils.forceAutoProxyCreatorToUseClassProxying(registry); &#125; if (enableAspectJAutoProxy.getBoolean(&quot;exposeProxy&quot;)) &#123; AopConfigUtils.forceAutoProxyCreatorToExposeProxy(registry); &#125; &#125; &#125;&#125; 12345678910@Nullablepublic static BeanDefinition registerAspectJAnnotationAutoProxyCreatorIfNecessary( BeanDefinitionRegistry registry, @Nullable Object source) &#123; /* * 参数一：固定类型 * 参数二：spring容器 * 参数三：标签 * */ return registerOrEscalateApcAsRequired(AnnotationAwareAspectJAutoProxyCreator.class, registry, source);&#125; \u0000\u0000**AspectJAutoProxyRegistrar** 实现了 **ImportBeanDefinitionRegistrar**接口，往容器中注册了一个bd信息。**AnnotationAwareAspectJAutoProxyCreator**​ 2.加载时机\u0000上一节我们分析到，在Spring的配置类上打一个注解，最后总会往容器中导入一个类：**AnnotationAwareAspectJAutoProxyCreator**。​ 我们来分析一下这个类的继承关系：​ 按照顺序分析一下这个类的加载时机和加载的时候，里面的 **initBeanFactory()** &amp; **setBeanFactory()** 是什么时候执行的。​ ​ \u0000 首先执行的是**AbstractAdvisorAutoProxyCreator**的**setBeanFactory()**。 123456789@Overridepublic void setBeanFactory(BeanFactory beanFactory) &#123; super.setBeanFactory(beanFactory); if (!(beanFactory instanceof ConfigurableListableBeanFactory)) &#123; throw new IllegalArgumentException( &quot;AdvisorAutoProxyCreator requires a ConfigurableListableBeanFactory: &quot; + beanFactory); &#125; initBeanFactory((ConfigurableListableBeanFactory) beanFactory);&#125; 接下来执行的是**AbstractAutoProxyCreator**的**setBeanFactory()**。 \u0000 1234@Overridepublic void setBeanFactory(BeanFactory beanFactory) &#123; this.beanFactory = beanFactory;&#125; ​ 然后执行的是**AnnotationAwareAspectJAutoProxyCreator**的**initBeanFactory()** 。 ​ 123456789@Overrideprotected void initBeanFactory(ConfigurableListableBeanFactory beanFactory) &#123; super.initBeanFactory(beanFactory); if (this.aspectJAdvisorFactory == null) &#123; this.aspectJAdvisorFactory = new ReflectiveAspectJAdvisorFactory(beanFactory); &#125; this.aspectJAdvisorsBuilder = new BeanFactoryAspectJAdvisorsBuilderAdapter(beanFactory, this.aspectJAdvisorFactory);&#125; 最终执行的是**AbstractAdvisorAutoProxyCreator**的**initBeanFactory()**。 ​ 123protected void initBeanFactory(ConfigurableListableBeanFactory beanFactory) &#123; this.advisorRetrievalHelper = new BeanFactoryAdvisorRetrievalHelperAdapter(beanFactory);&#125; 接下来分析**AnnotationAwareAspectJAutoProxyCreator**的执行时机。​ 3.创建代理对象 4.获取拦截器 5.链式调用通知方法 6.流程总结 AOP其实就是往容器中导入了一个组件，这个组件是一个后置处理器，他会在对象创建之前尝试返回一个代理对象，如果不能成功返回，会在对象创建之后，init方法执行前后去判断当前对象是否需要被代理，如果需要被代理则根据各种条件去选择代理方式，创建代理对象，同时会去判断哪些切面和方法需要增强代理对象里面的方法，生成一条拦截器链。\u0000\u0000在代理对象执行目标方法前后，通过拦截器对目标方法进行拦截，执行增强逻辑。","categories":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/tags/Spring/"}]},{"title":"Spring[扩展]模拟核心原理","slug":"Spring/Spring[扩展]模拟核心原理","date":"2022-01-11T06:11:42.407Z","updated":"2022-01-11T06:20:30.143Z","comments":true,"path":"2022/01/11/Spring/Spring[扩展]模拟核心原理/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/Spring/Spring[%E6%89%A9%E5%B1%95]%E6%A8%A1%E6%8B%9F%E6%A0%B8%E5%BF%83%E5%8E%9F%E7%90%86/","excerpt":"","text":"一，实现思路1，配置阶段 配置web.xml DispatcherServlet 设定init-param contextConfigLocation=classpath:application.xml 设定url-pattern /* 配置Annotation @Controller @Service @Autowrited @RequestMapping 2,初始化阶段 调用init方法 加载配置文件 IOC容器初始化 MAP 扫描相关的类 scan-package=”” 创建实例化并保存至容器 通过反射机制将类实例化放入IOC容器 进行DI操作 扫描IOC容器的实例，给没有赋值的属性自动填充 初始化HandlerMapping 讲一个URL和一个Method进行一对一的映射 3，运行阶段 调用doGet/doPost web容器调用doget、dopost，获取req和resp对象 匹配HandlerMapping 从req对象获取输入的URL，找到其对应的method 反射调用method.invoker() 利用反射调用方法并返回结果 response.getWrite().write() 将返回结果输出到浏览器 二，自定义配置1，配置 application.properties 文件为了解析方便，用 application.properties 来代替 application.xml 文件，具体配置内容如下： 1scanPackage=com.yhd.spring01 2，配置web.xml文件12345678910111213141516171819202122&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;web-app xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns=&quot;http://java.sun.com/xml/ns/j2ee&quot; xmlns:javaee=&quot;http://java.sun.com/xml/ns/javaee&quot; xmlns:web=&quot;http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd&quot; xsi:schemaLocation=&quot;http://java.sun.com/xml/ns/j2eehttp://java.sun.com/xml/ns/j2ee/web-app_2_4.xsd&quot; version=&quot;2.4&quot;&gt; &lt;display-name&gt;YHD Web Application&lt;/display-name&gt; &lt;servlet&gt; &lt;servlet-name&gt;yhdmvc&lt;/servlet-name&gt; &lt;servlet-class&gt;com.yhd.spring01.servlet.HdDispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;application.properties&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;yhdmvc&lt;/servlet-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/servlet-mapping&gt;&lt;/web-app&gt; 3，自定义注解123456@Target(&#123;ElementType.FIELD&#125;)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface HdAutowired &#123; String value() default &quot;&quot;;&#125; 1234567import java.lang.annotation.*;@Target(&#123;ElementType.TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface HdController &#123; String value() default &quot;&quot;;&#125; 123456@Target(&#123;ElementType.TYPE, ElementType.METHOD&#125;)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface HdRequestMapping &#123; String value() default &quot;&quot;;&#125; 123456@Target(&#123;ElementType.PARAMETER&#125;)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface HdRequestParam &#123; String value() default &quot;&quot;;&#125; 123456@Target(&#123;ElementType.TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface HdService &#123; String value() default &quot;&quot;;&#125; 4，编写模拟业务1234567@HdServicepublic class DemoService implements IDemoService &#123; @Override public String get(String name) &#123; return &quot;My name is &quot; + name; &#125;&#125; 12345678910111213141516171819202122232425262728293031@HdController@HdRequestMapping(&quot;/demo&quot;)public class DemoController &#123; @HdAutowired private IDemoService demoService; @HdRequestMapping(&quot;/query&quot;) public void query(HttpServletRequest req, HttpServletResponse resp, @HdRequestParam(&quot;name&quot;) String name)&#123; String result = demoService.get(name); try &#123; resp.getWriter().write(result); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; @HdRequestMapping(&quot;/add&quot;) public void add(HttpServletRequest req, HttpServletResponse resp, @HdRequestParam(&quot;a&quot;) Integer a, @HdRequestParam(&quot;b&quot;) Integer b)&#123; try &#123; resp.getWriter().write(a + &quot;+&quot; + b + &quot;=&quot; + (a + b)); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; @HdRequestMapping(&quot;/remove&quot;) public void remove(HttpServletRequest req,HttpServletResponse resp, @HdRequestParam(&quot;id&quot;) Integer id)&#123; &#125;&#125; 三，容器初始化1.0版本流程分析1.首先在doGet方法里面调用doDispatcher方法，根据请求路径判断路径是否存在，如果不存在就返回404存在就从容器中拿到路径对应的方法，通过动态代理执行对应的方法 2.在类加载阶段，用流来加载配置文件，从配置文件读取配置的包扫描路径根据包扫描路径进行迭代遍历，利用反射创建所有类上标有controller注解的类加入到容器，并下钻到类中，将类中每一个方法的绝对访问路径和方法加入到容器，迭代遍历创建所有标有service注解的类，如果该类实现了接口，将该接口的全限定类型名和类实例对象也放入容器，达到根据接口注入的效果。 3.属性赋值，遍历容器中所有类，如果类中标有@autowried注解，将属性对应的值设置进去。 重要方法1.clazz.isAnnotationPresent(HdController.class)判断clazz上有没有HdController注解 2.field.set(mappings.get(clazz.getName()), mappings.get(beanName));属性赋值：args1：给哪个属性设值，args2：设置的什么值 3.method.invoke(mappings.get(method.getDeclaringClass().getName()), new Object[]{req, resp, params.get(“name”)[0]});通过动态代理执行方法，方法所在类名，方法参数 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139/** * @author yhd * @createtime 2021/1/31 15:49 * @description 模拟IOC容器的创建 */public class HdDispatcherServlet extends HttpServlet &#123; //映射关系 访问路径-方法名 全限定类名-实例对象 private Map&lt;String, Object&gt; mappings = new HashMap&lt;&gt;(); @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; this.doPost(req, resp); &#125; @SneakyThrows @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; doDispatch(req, resp); &#125; private void doDispatch(HttpServletRequest req, HttpServletResponse resp) throws IOException, InvocationTargetException, IllegalAccessException &#123; //组装路径 String url = req.getRequestURI(); String contextPath = req.getContextPath(); url = url.replace(contextPath, &quot;&quot;).replaceAll(&quot;/+&quot;, &quot;/&quot;); //判断路径是否存在 if (!this.mappings.containsKey(url)) &#123; resp.getWriter().write(&quot;404 NotFound!&quot;); return; &#125; //获取路径对应的方法参数，通过动态代理进行增强 Method method = (Method) this.mappings.get(url); Map&lt;String, String[]&gt; params = req.getParameterMap(); method.invoke(mappings.get(method.getDeclaringClass().getName()), new Object[]&#123;req, resp, params.get(&quot;name&quot;)[0]&#125;); &#125; @Override public void init(ServletConfig config) throws ServletException &#123; InputStream is = null; try &#123; //加载配置文件 Properties configContext = new Properties(); is = this.getClass().getClassLoader().getResourceAsStream(config.getInitParameter(&quot;contextConfigLocation&quot;)); configContext.load(is); //获取扫描路径 String scanPackage = configContext.getProperty(&quot;scanPackage&quot;); doScanner(scanPackage); for (String className : mappings.keySet()) &#123; if (!className.contains(&quot;.&quot;)) &#123; continue; &#125; Class&lt;?&gt; clazz = Class.forName(className); //当前这个类上有没有controller注解 if (clazz.isAnnotationPresent(HdController.class)) &#123; mappings.put(className, clazz.newInstance()); String baseUrl = &quot;&quot;; //判断有没有一级访问路径 if (clazz.isAnnotationPresent(HdRequestMapping.class)) &#123; HdRequestMapping requestMapping = clazz.getAnnotation(HdRequestMapping.class); baseUrl = requestMapping.value(); &#125; Method[] methods = clazz.getMethods(); for (Method method : methods) &#123; if (!method.isAnnotationPresent(HdRequestMapping.class)) &#123; continue; &#125; HdRequestMapping requestMapping = method.getAnnotation(HdRequestMapping.class); //拼装路径 String url = (baseUrl + &quot;/&quot; + requestMapping.value()).replaceAll(&quot;/+&quot;, &quot;/&quot;); //map放的是：controller里面一个方法的访问绝对路径，这个对应的方法 mappings.put(url, method); System.out.println(&quot;Mapped &quot; + url + &quot;,&quot; + method); &#125; &#125; else if (clazz.isAnnotationPresent(HdService.class)) &#123; HdService service = clazz.getAnnotation(HdService.class); String beanName = service.value(); if (&quot;&quot;.equals(beanName)) &#123; beanName = clazz.getName(); &#125; Object instance = clazz.newInstance(); //map里面放的是类名和实例对象 mappings.put(beanName, instance); //将这个类实现的接口和实例对象放进去 for (Class&lt;?&gt; i : clazz.getInterfaces()) &#123; mappings.put(i.getName(), instance); &#125; &#125; else &#123; continue; &#125; &#125; //属性注入 for (Object object : mappings.values()) &#123; if (object == null) &#123; continue; &#125; Class clazz = object.getClass(); if (clazz.isAnnotationPresent(HdController.class)) &#123; Field[] fields = clazz.getDeclaredFields(); for (Field field : fields) &#123; if (!field.isAnnotationPresent(HdAutowired.class)) &#123; continue; &#125; HdAutowired autowired = field.getAnnotation(HdAutowired.class); String beanName = autowired.value(); if (&quot;&quot;.equals(beanName)) &#123; beanName = field.getType().getName(); &#125; field.setAccessible(true); try &#123; field.set(mappings.get(clazz.getName()), mappings.get(beanName)); &#125; catch (IllegalAccessException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; System.out.print(&quot;Diy MVC Framework is init&quot;); &#125; catch (Exception e) &#123; &#125; &#125; private void doScanner(String scanPackage) &#123; URL url = this.getClass().getClassLoader().getResource(&quot;/&quot; + scanPackage.replaceAll(&quot;\\\\.&quot;, &quot;/&quot;)); File classDir = new File(url.getFile()); Arrays.stream(classDir.listFiles()).forEach(file -&gt; &#123; if (file.isDirectory()) &#123; doScanner(scanPackage + &quot;.&quot; + file.getName()); &#125; else &#123; if (!file.getName().endsWith(&quot;.class&quot;)) &#123; return; &#125; String clazzName = (scanPackage + &quot;.&quot; + file.getName().replace(&quot;.class&quot;, &quot;&quot;)); mappings.put(clazzName, null); &#125; &#125;); &#125;&#125; 2.0版本分析1.0版本的所有代码都写在了一个方法里面，代码耦合度 十分高，不符合开发规范 思路采用设计模式（工厂模式、单例模式、委派模式、策略模式），改造业务逻辑。 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202/** * @author yhd * @createtime 2021/2/1 11:29 */public class HdDispatcherServlet2 extends HttpServlet &#123; private Map&lt;String, Object&gt; ioc = new ConcurrentHashMap&lt;&gt;(); private Map&lt;String, Method&gt; handlerMappings = new ConcurrentHashMap&lt;&gt;(); private List&lt;String&gt; classNames = new CopyOnWriteArrayList&lt;&gt;(); private Properties configContext = new Properties(); private static final String CONFIG_LOCATION = &quot;contextConfigLocation&quot;; @Override public void init(ServletConfig config) throws ServletException &#123; //1.加载配置文件 loadConfig(config.getInitParameter(CONFIG_LOCATION)); //2.扫描所有的组件 doScanPackages(configContext.getProperty(&quot;scanPackage&quot;)); //3.将组件加入到容器 refersh(); //4.属性设值 population(); //5.建立方法与路径的映射 routingAndMapping(); &#125; /** * 建立方法与路径的映射 */ private void routingAndMapping() &#123; classNames.forEach(className -&gt; &#123; Object instance = ioc.get(className); if (instance.getClass().isAnnotationPresent(HdController.class)) &#123; String baseUrl = &quot;&quot;; if (instance.getClass().isAnnotationPresent(HdRequestMapping.class)) &#123; baseUrl += instance.getClass().getAnnotation(HdRequestMapping.class).value().trim(); &#125; String finalBaseUrl = baseUrl; Arrays.asList(instance.getClass().getDeclaredMethods()).forEach(method -&gt; &#123; if (method.isAnnotationPresent(HdRequestMapping.class)) &#123; String methodUrl = finalBaseUrl; methodUrl += method.getAnnotation(HdRequestMapping.class).value().trim(); handlerMappings.put(methodUrl, method); &#125; &#125;); &#125; &#125;); &#125; /** * 属性设值 */ private void population() &#123; Set&lt;String&gt; keySet = ioc.keySet(); keySet.forEach(key -&gt; &#123; Field[] fields = ioc.get(key).getClass().getFields(); Arrays.asList(fields).forEach(field -&gt; &#123; if (field.isAnnotationPresent(HdAutowired.class)) &#123; HdAutowired autowired = field.getAnnotation(HdAutowired.class); String name = autowired.value().trim(); if (&quot;&quot;.equals(autowired.value().trim())) &#123; name = field.getType().getName(); &#125; try &#123; field.setAccessible(true); field.set(name, ioc.get(name)); &#125; catch (IllegalAccessException e) &#123; &#125; &#125; &#125;); &#125;); &#125; /** * 容器刷新 * 组件加入到容器中 */ @SneakyThrows private void refersh() &#123; if (classNames == null || classNames.isEmpty()) &#123; throw new RuntimeException(&quot;组件扫描出现异常！&quot;); &#125; for (String className : classNames) &#123; Class&lt;?&gt; clazz = Class.forName(className); if (clazz.isAnnotationPresent(HdController.class)) &#123; //TODO 类名处理 ioc.put(clazz.getSimpleName(), clazz.newInstance()); &#125; else if (clazz.isAnnotationPresent(HdService.class)) &#123; Object instance = clazz.newInstance(); ioc.put(clazz.getSimpleName(), instance); Class&lt;?&gt;[] interfaces = clazz.getInterfaces(); for (Class&lt;?&gt; inter : interfaces) &#123; ioc.put(inter.getSimpleName(), clazz); &#125; &#125; else &#123; continue; &#125; &#125; &#125; /** * 组件扫描 * * @param scanPackage */ private void doScanPackages(String scanPackage) &#123; URL url = getClass().getClassLoader().getResource(&quot;/&quot; + scanPackage.replaceAll(&quot;\\\\.&quot;, &quot;/&quot;)); File files = new File(url.getFile()); for (File file : files.listFiles()) &#123; if (file.isDirectory()) &#123; doScanPackages(scanPackage + &quot;.&quot; + file.getName()); &#125; else &#123; if (!file.getName().endsWith(&quot;.class&quot;)) &#123; continue; &#125; String className = scanPackage + &quot;.&quot; + file.getName().replace(&quot;.class&quot;, &quot;&quot;); classNames.add(className); &#125; &#125; &#125; /** * 加载配置文件 * * @param initParameter */ @SneakyThrows private void loadConfig(String initParameter) &#123; InputStream is = getClass().getClassLoader().getResourceAsStream(initParameter); configContext.load(is); &#125; @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; doPost(req, resp); &#125; @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; try &#123; doDispatcher(req, resp); &#125; catch (Exception e) &#123; throw new RuntimeException(&quot; 500 server error!&quot;); &#125; &#125; @SneakyThrows private void doDispatcher(HttpServletRequest req, HttpServletResponse resp) &#123; String realPath = req.getRequestURI().replace(req.getContextPath(), &quot;&quot;); Map&lt;String, String[]&gt; parameterMap = req.getParameterMap(); if (!handlerMappings.containsKey(realPath)) &#123; throw new RuntimeException(&quot;404 Not Found!&quot;); &#125; Method method = handlerMappings.get(realPath); Class&lt;?&gt;[] parameterTypes = method.getParameterTypes(); Object[] paramValues = new Object[parameterTypes.length]; for (int i = 0; i &lt; parameterTypes.length - 1; i++) &#123; Class param = parameterTypes[i]; if (param == HttpServletRequest.class) &#123; paramValues[i] = req; &#125; if (param == HttpServletResponse.class) &#123; paramValues[i] = resp; &#125; if (param == String.class) &#123; HdRequestParam requestParam = parameterTypes[i].getAnnotation(HdRequestParam.class); String value = requestParam.value(); String[] realParam = parameterMap.get(value); paramValues[i] = Arrays.toString(realParam) .replaceAll(&quot;\\\\[|\\\\]&quot;, &quot;&quot;) .replaceAll(&quot;\\\\s&quot;, &quot;,&quot;); &#125; &#125; method.invoke(method.getDeclaringClass().getSimpleName(), paramValues); &#125; private Object convertParamType() &#123; return null; &#125;&#125; 3.0版本分析HandlerMapping还不能像SpringMVC一样支持正则，url参数还不支持强制类型转换，反射调用之前还需要重新获取bean的name。 改造 HandlerMapping，在真实的 Spring 源码中，HandlerMapping 其实是一个 List 而非 Map。List 中的元素是一个自定义的类型。 思路使用内部类维护requestMapping和url之间的关系。 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282public class HdDispatcherServlet3 extends HttpServlet &#123; private Map&lt;String, Object&gt; ioc = new ConcurrentHashMap&lt;&gt;(); private Map&lt;String, Method&gt; handlerMappings = new ConcurrentHashMap&lt;&gt;(); private List&lt;String&gt; classNames = new CopyOnWriteArrayList&lt;&gt;(); private Properties configContext = new Properties(); private static final String CONFIG_LOCATION = &quot;contextConfigLocation&quot;; private List&lt;Handler&gt; handlerMapping = new ArrayList&lt;&gt;(); /** * */ @Data private class Handler &#123; //保存方法对应的实例 private Object controller; //保存映射的方法 private Method method; //正则匹配 private Pattern pattern; //参数顺序 private Map&lt;String, Integer&gt; paramIndexMapping = new ConcurrentHashMap&lt;&gt;(); public Handler(Pattern pattern, Object controller, Method method) &#123; this.controller = controller; this.method = method; this.pattern = pattern; paramIndexMapping = new HashMap&lt;String, Integer&gt;(); putParamIndexMapping(method); &#125; private void putParamIndexMapping(Method method) &#123; //提取方法中加了注解的参数 Annotation[][] pa = method.getParameterAnnotations(); for (int i = 0; i &lt; pa.length; i++) &#123; for (Annotation a : pa[i]) &#123; if (a instanceof HdRequestParam) &#123; String paramName = ((HdRequestParam) a).value(); if (!&quot;&quot;.equals(paramName.trim())) &#123; paramIndexMapping.put(paramName, i); &#125; &#125; &#125; &#125; //提取方法中的req和resp Class&lt;?&gt;[] parameterTypes = method.getParameterTypes(); for (int i = 0; i &lt; parameterTypes.length; i++) &#123; Class&lt;?&gt; type = parameterTypes[i]; if (type == HttpServletRequest.class || type == HttpServletResponse.class) &#123; paramIndexMapping.put(type.getName(), i); &#125; &#125; &#125; &#125; @Override public void init(ServletConfig config) throws ServletException &#123; //1.加载配置文件 loadConfig(config.getInitParameter(CONFIG_LOCATION)); //2.扫描所有的组件 doScanPackages(configContext.getProperty(&quot;scanPackage&quot;)); //3.将组件加入到容器 refersh(); //4.属性设值 population(); //5.建立方法与路径的映射 routingAndMapping(); &#125; /** * 建立方法与路径的映射 */ private void routingAndMapping() &#123; if (ioc.isEmpty()) &#123; return; &#125; for (Map.Entry&lt;String, Object&gt; entry : ioc.entrySet()) &#123; Class&lt;?&gt; clazz = entry.getValue().getClass(); if (!clazz.isAnnotationPresent(HdController.class)) &#123; continue; &#125; String url = &quot;&quot;; if (clazz.isAnnotationPresent(HdRequestMapping.class)) &#123; HdRequestMapping requestMapping = clazz.getAnnotation(HdRequestMapping.class); url = requestMapping.value(); &#125; for (Method method : clazz.getMethods()) &#123; if (!method.isAnnotationPresent(HdRequestMapping.class)) &#123; continue; &#125; HdRequestMapping requestMapping = method.getAnnotation(HdRequestMapping.class); String regex = (&quot;/&quot; + url + requestMapping.value()).replaceAll(&quot;/+&quot;, &quot;/&quot;); Pattern pattern = Pattern.compile(regex); handlerMapping.add(new Handler(pattern, entry.getValue(), method)); &#125; &#125; &#125; /** * 属性设值 */ private void population() &#123; Set&lt;String&gt; keySet = ioc.keySet(); keySet.forEach(key -&gt; &#123; Field[] fields = ioc.get(key).getClass().getFields(); Arrays.asList(fields).forEach(field -&gt; &#123; if (field.isAnnotationPresent(HdAutowired.class)) &#123; HdAutowired autowired = field.getAnnotation(HdAutowired.class); String name = autowired.value().trim(); if (&quot;&quot;.equals(autowired.value().trim())) &#123; name = field.getType().getName(); &#125; try &#123; field.setAccessible(true); field.set(name, ioc.get(name)); &#125; catch (IllegalAccessException e) &#123; &#125; &#125; &#125;); &#125;); &#125; /** * 容器刷新 * 组件加入到容器中 */ @SneakyThrows private void refersh() &#123; if (classNames == null || classNames.isEmpty()) &#123; throw new RuntimeException(&quot;组件扫描出现异常！&quot;); &#125; for (String className : classNames) &#123; Class&lt;?&gt; clazz = Class.forName(className); if (clazz.isAnnotationPresent(HdController.class)) &#123; //TODO 类名处理 ioc.put(clazz.getSimpleName(), clazz.newInstance()); &#125; else if (clazz.isAnnotationPresent(HdService.class)) &#123; Object instance = clazz.newInstance(); ioc.put(clazz.getSimpleName(), instance); Class&lt;?&gt;[] interfaces = clazz.getInterfaces(); for (Class&lt;?&gt; inter : interfaces) &#123; ioc.put(inter.getSimpleName(), clazz); &#125; &#125; else &#123; continue; &#125; &#125; &#125; /** * 组件扫描 * * @param scanPackage */ private void doScanPackages(String scanPackage) &#123; URL url = getClass().getClassLoader().getResource(&quot;/&quot; + scanPackage.replaceAll(&quot;\\\\.&quot;, &quot;/&quot;)); File files = new File(url.getFile()); for (File file : files.listFiles()) &#123; if (file.isDirectory()) &#123; doScanPackages(scanPackage + &quot;.&quot; + file.getName()); &#125; else &#123; if (!file.getName().endsWith(&quot;.class&quot;)) &#123; continue; &#125; String className = scanPackage + &quot;.&quot; + file.getName().replace(&quot;.class&quot;, &quot;&quot;); classNames.add(className); &#125; &#125; &#125; /** * 加载配置文件 * * @param initParameter */ @SneakyThrows private void loadConfig(String initParameter) &#123; InputStream is = getClass().getClassLoader().getResourceAsStream(initParameter); configContext.load(is); &#125; @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; doPost(req, resp); &#125; @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; try &#123; doDispatcher(req, resp); &#125; catch (Exception e) &#123; throw new RuntimeException(&quot; 500 server error!&quot;); &#125; &#125; @SneakyThrows private void doDispatcher(HttpServletRequest req, HttpServletResponse resp) &#123; Handler handler = getHandler(req); if (handler == null) &#123; throw new RuntimeException(&quot;404 Not Found!&quot;); &#125; Class&lt;?&gt;[] parameterTypes = handler.getMethod().getParameterTypes(); Object[] paramValues = new Object[parameterTypes.length]; Map&lt;String, String[]&gt; params = req.getParameterMap(); for (Map.Entry&lt;String, String[]&gt; param : params.entrySet()) &#123; String value = Arrays.toString(param.getValue()).replaceAll(&quot;\\\\[|\\\\]&quot;, &quot;&quot;) .replaceAll(&quot;\\\\s&quot;, &quot;,&quot;); if (!handler.getParamIndexMapping().containsKey(param.getKey())) &#123; continue; &#125; Integer index = handler.getParamIndexMapping().get(param.getKey()); paramValues[index] = this.convert(parameterTypes[index], value); &#125; if (handler.paramIndexMapping.containsKey(HttpServletRequest.class.getName())) &#123; int reqIndex = handler.paramIndexMapping.get(HttpServletRequest.class.getName()); paramValues[reqIndex] = req; &#125; if (handler.paramIndexMapping.containsKey(HttpServletResponse.class.getName())) &#123; int respIndex = handler.paramIndexMapping.get(HttpServletResponse.class.getName()); paramValues[respIndex] = resp; &#125; Object returnValue = handler.getMethod().invoke(handler.getController(), paramValues); if (returnValue == null || returnValue instanceof Void) &#123; return; &#125; resp.getWriter().write(returnValue.toString()); &#125; private Object convert(Class&lt;?&gt; parameterType, String value) &#123; if (Integer.class == parameterType) &#123; return Integer.parseInt(value); &#125; return value; &#125; private Handler getHandler(HttpServletRequest req) &#123; if (handlerMapping.isEmpty()) &#123; return null; &#125; String url = req.getRequestURI(); String contextPath = req.getContextPath(); url = url.replace(contextPath, &quot;&quot;) .replaceAll(&quot;/+&quot;, &quot;/&quot;); for (Handler handler : handlerMapping) &#123; try &#123; Matcher matcher = handler.pattern.matcher(url); //如果没有匹配上继续下一个匹配 if (!matcher.matches()) &#123; continue; &#125; return handler; &#125; catch (Exception e) &#123; throw e; &#125; &#125; return null; &#125;&#125;","categories":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/tags/Spring/"}]},{"title":"Spring[扩展]MVC使用篇","slug":"Spring/Spring[扩展]MVC使用篇","date":"2022-01-11T06:11:34.427Z","updated":"2022-01-11T06:20:52.215Z","comments":true,"path":"2022/01/11/Spring/Spring[扩展]MVC使用篇/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/Spring/Spring[%E6%89%A9%E5%B1%95]MVC%E4%BD%BF%E7%94%A8%E7%AF%87/","excerpt":"","text":"一，springmvc基本概念1.三层架构开发架构一般基于两种形式，一种是c/s架构，也就是客户端服务器，另一种是b/s架构，也就是浏览器服务器。javaee的开发基本都是b/s架构。在b/s架构中，系统表转的三层架构包括：表现层，业务层，持久层。 表现层：web层。负责接收客户端请求，向客户端响应结果。依赖于业务层，接受请求调用业务层进行业务处理，并将处理结果响应回客户端。 展示层：展示结果。 控制层：接受请求 表现层的设计一般都是使用MVC设计模式。 业务层：service层。负责业务逻辑处理。 业务层可能会依赖于持久层，如果需要对数据持久化，需要保证事物的一致性。 持久层：dao层。负责数据持久化。 数据库：对数据进行持久化的载体。 数据访问层：业务层和持久层交互的接口 持久层就是和数据库交互，对数据库表进行增删改查。 2.MVC模型MVC 全名是 Model View Controller，是模型(model)－视图(view)－控制器(controller)的缩写，是一种用于设计创建 Web 应用程序表现层的模式。MVC 中每个部分各司其职： Model（模型）：通常指的就是我们的数据模型。作用一般情况下用于封装数据。 View（视图）：通常指的就是我们的 jsp 或者 html。作用一般就是展示数据的。通常视图是依据模型数据创建的。 Controller（控制器）：是应用程序中处理用户交互的部分。作用一般就是处理程序逻辑的。 3.springmvc是什么SpringMVC 是一种基于 Java 的实现 MVC 设计模型的请求驱动类型的轻量级 Web 框架，属于 SpringFrameWork 的后续产品，已经融合在 Spring Web Flow 里面。Spring 框架提供了构建 Web 应用程序的全功能 MVC 模块。使用 Spring 可插入的 MVC 架构，从而在使用 Spring 进行 WEB 开发时，可以选择使用 Spring的 Spring MVC 框架或集成其他 MVC 开发框架，如 Struts1(现在一般不用)，Struts2 等。​ SpringMVC 已经成为目前最主流的 MVC 框架之一，并且随着 Spring3.0 的发布，全面超越 Struts2，成为最优秀的 MVC 框架。​ 它通过一套注解，让一个简单的 Java 类成为处理请求的控制器，而无须实现任何接口。同时它还支持RESTful 编程风格的请求。 4.springmvc和struts2的优劣对比共同点： 它们都是表现层框架，都是基于 MVC 模型编写的。 它们的底层都离不开原始 ServletAPI。 它们处理请求的机制都是一个核心控制器。 区别： Spring MVC 的入口是 Servlet, 而 Struts2 是 Filter Spring MVC 是基于方法设计的，而 Struts2 是基于类，Struts2 每次执行都会创建一个动作类。所 以 Spring MVC 会稍微比 Struts2 快些。 Spring MVC 使用更加简洁,同时还支持 JSR303, 处理 ajax 的请求更方便 (JSR303 是一套 JavaBean 参数校验的标准，它定义了很多常用的校验注解，我们可以直接将这些注解加在我们 JavaBean 的属性上面，就可以在需要校验的时候进行校验了。) Struts2 的 OGNL 表达式使页面的开发效率相比 Spring MVC 更高些，但执行效率并没有比 JSTL 提 升，尤其是 struts2 的表单标签，远没有 html 执行效率高。 二，Springmvc入门1.入门案例需求：点击页面超链接，跳转到成功页面 1&lt;a href=&quot;success/success&quot;&gt;testSuccess&lt;/a&gt;&lt;br/&gt; 12345678910111213@Controller@RequestMapping(&quot;/success&quot;)public class success &#123; /** * 入门案例 * @return */ @RequestMapping(&quot;/success&quot;) public String testSuccess()&#123; System.out.println(&quot;testSuccess()...&quot;); return &quot;success&quot;; &#125; &#125; web.xml配置核心控制器：一个Servlet 12345678910111213141516&lt;!-- 配置 DispatcherServlet --&gt; &lt;servlet&gt; &lt;servlet-name&gt;springDispatcherServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:springmvc.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;!-- 映射地址 --&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;springDispatcherServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; springmvc.xml1234567891011121314151617181920212223&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot; http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;!--组件扫描--&gt; &lt;context:component-scan base-package=&quot;com.atguigu&quot;&gt;&lt;/context:component-scan&gt; &lt;!--配置视图解析器--&gt; &lt;bean id=&quot;internalResourceViewResolver&quot; class=&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;&gt; &lt;!--前缀解析器--&gt; &lt;property name=&quot;prefix&quot; value=&quot;/WEB-INF/views/&quot;&gt;&lt;/property&gt; &lt;!--后缀解析器--&gt; &lt;property name=&quot;suffix&quot; value=&quot;.jsp&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!--开启注解支持--&gt; &lt;mvc:annotation-driven /&gt; &lt;/beans&gt; 2.入门案例流程分析 服务器启动，加载应用。读取web.xml中的配置创建spring容器并且初始化容器中的对象。从案例中可以发现创建的是successController和InternalResourceViewResolver，实际上远不止这些。 浏览器发送请求，经过前端控制器，被其捕获，他并不处理请求，只是根据路径的URL匹配有没有对应的，如果匹配到@RequestMapping中的内容，就转发。 转发到控制层执行对应的方法，该方法有一个返回值。 根据方法的返回值，借助视图解析器对象找到对应的视图结果。 渲染结果视图，响应浏览器 3.请求响应流程 1234567891011请求相应流程： 请求先来到springDispatcherServlet看Springmvc是否存在对应的映射？ 如果不存在，看springmvc的配置文件是否配置了&lt;mvc:default-servlet-handler/&gt; 如果不存在，页面响应404，控制台打印no mapping found 如果存在，转发到目标资源。 如果存在，请求交给handlerMapping（处理器映射器），由它获取HandlerExecutionChain对象， 再交给HandlerAdapter（处理器适配器）对象，调用拦截器的PreHandle方法， 然后调用目标Handler方法得到ModelAndview对象，调用拦截器的PostHnadle方法， 查看是否存在异常 如果存在，由异常处理器处理异常，得到新的ModelAndView对象， 如果不存在异常，由视图解析器解析视图，得到实际的View，渲染视图，调用拦截器的afterCompletion方法 4.案例中涉及的组件 DispatcherServlet：前端控制器:用户请求到达前端控制器，它就相当于 mvc 模式中的 c，dispatcherServlet 是整个流程控制的中心，由它调用其它组件处理用户的请求，dispatcherServlet 的存在降低了组件之间的耦合性 HandlerMapping：处理器映射器:HandlerMapping 负责根据用户请求找到 Handler 即处理器，SpringMVC 提供了不同的映射器实现不同的映射方式，例如：配置文件方式，实现接口方式，注解方式等。 Handler：处理器:开发中要编写的具体业务控制器。由 DispatcherServlet 把用户请求转发到 Handler。由Handler 对具体的用户请求进行处理。 HandlAdapter：处理器适配器:通过适配器对处理器进行执行，通过拓展适配器可以处理更多类型。 View Resolver：视图解析器:View Resolver 负责将处理结果生成 View 视图，View Resolver 首先根据逻辑视图名解析成物理视图名.即具体的页面地址，再生成 View 视图对象，最后对 View 进行渲染将处理结果通过页面展示给用户 View：视图:SpringMVC 框架提供了很多的 View 视图类型的支持，包括：jstlView、freemarkerView、pdfView等。我们最常用的视图就是 jsp。一般情况下需要通过页面标签或页面模版技术将模型数据通过页面展示给用户，需要由程序员根据业务需求开发具体的页面。 mvc:annotation-driven说明:在 SpringMVC 的各个组件中，处理器映射器、处理器适配器、视图解析器称为 SpringMVC 的三大组件。使 用 mvc:annotation-driven 自动加载 RequestMappingHandlerMapping （处理映射器） 和RequestMappingHandlerAdapter （ 处 理 适 配 器 ） ， 可 用 在 SpringMVC.xml 配 置 文 件 中 使 用mvc:annotation-driven/替代注解处理器和适配器的配置。 5.RequestMapping注解123456789101112131415161718192021222324源码：@Target(&#123;ElementType.METHOD, ElementType.TYPE&#125;)//可以加到类上和方法上@Retention(RetentionPolicy.RUNTIME)@Documented@Mappingpublic @interface RequestMapping &#123; &#125;作用： 用于建立请求 URL 和处理请求方法之间的对应关系。出现位置： 1.类上 请求 URL 的第一级访问目录，便于模块化管理。 2.方法上 请求 URL 的第二级访问目录。属性： value：用于指定请求的 URL。它和 path 属性的作用是一样的。 method：用于指定请求的方式。 params：用于指定限制请求参数的条件。它支持简单的表达式。要求请求参数的 key 和 value 必须和配置的一模一样。例如：params = &#123;&quot;accountName&quot;&#125;，表示请求参数必须有 accountNameparams = &#123;&quot;moeny!100&quot;&#125;，表示请求参数中 money 不能是 100。 headers：用于指定限制请求消息头的条件。注意：以上四个属性只要出现 2 个或以上时，他们的关系是与的关系。 三，请求参数的绑定1.绑定的机制123456789101112131415表单中请求参数都是基于 key=value 的。SpringMVC 绑定请求参数的过程是通过把表单提交请求参数，作为控制器中方法参数进行绑定的。例如：&lt;a href=&quot;account/findAccount?accountId=10&quot;&gt;查询账户&lt;/a&gt;中请求参数是： accountId=10/*** 查询账户* @return*/@RequestMapping(&quot;/findAccount&quot;)public String findAccount(Integer accountId) &#123; System.out.println(&quot;查询了账户。。。。&quot;+accountId); return &quot;success&quot;;&#125; 2.支持的数据类型 基本类型参数： 包括基本类型和 String 类型 POJO 类型参数： 包括实体类，以及关联的实体类 数组和集合类型参数： 包括 List 结构和 Map 结构的集合（包括数组） SpringMVC 绑定请求参数是自动实现的，但是要想使用，必须遵循使用要求。 3.使用要求 如果是基本类型或者 String 类型： 要求我们的参数名称必须和控制器中方法的形参名称保持一致。(严格区分大小写) 如果是 POJO 类型，或者它的关联对象： 要求表单中参数名称和 POJO 类的属性名称保持一致。并且控制器方法的参数类型是 POJO 类型。 如果是集合类型,有两种方式： 第一种： 要求集合类型的请求参数必须在 POJO 中。在表单中请求参数名称要和 POJO 中集合属性名称相同。 给 List 集合中的元素赋值，使用下标。 给 Map 集合中的元素赋值，使用键值对。 第二种： 接收的请求参数是 json 格式数据。需要借助一个注解实现 注意: 它还可以实现一些数据类型自动转换。如遇特殊类型转换要求，需要我们自己编写自定义类型转换器。 4.代码示例1234567891011121314151617181920/** * 参数绑定1: * bean中包含bean */ @RequestMapping(&quot;/getBean&quot;) public String getBean(Person person)&#123; System.out.println(&quot;getBean()...&quot;); System.out.println(person); return &quot;success&quot;; &#125; /** * 参数绑定2 * 集合类型 */ @RequestMapping(&quot;/getCollection&quot;) public String getList(MyCollection collection)&#123; System.out.println(&quot;getList()....&quot;); System.out.println(collection); return &quot;success&quot;; &#125; 12345678910111213&lt;form action=&quot;success/getBean&quot; method=&quot;post&quot;&gt; id: &lt;input type=&quot;text&quot; name=&quot;id&quot;/&gt;&lt;br/&gt; name: &lt;input type=&quot;text&quot; name=&quot;user.name&quot;/&gt;&lt;br/&gt; age:&lt;input type=&quot;text&quot; name=&quot;user.age&quot;/&gt;&lt;br/&gt; &lt;input type=&quot;submit&quot; value=&quot;submit&quot;/&gt;&lt;br/&gt;&lt;/form&gt;&lt;form method=&quot;post&quot; action=&quot;success/getCollection&quot;&gt; name: &lt;input type=&quot;text&quot; name=&quot;list[0].name&quot;/&gt;&lt;br/&gt; age: &lt;input type=&quot;text&quot; name=&quot;list[0].age&quot;/&gt;&lt;br/&gt; name: &lt;input type=&quot;text&quot; name=&quot;map[1].name&quot;/&gt;&lt;br/&gt; age: &lt;input type=&quot;text&quot; name=&quot;map[1].age&quot;/&gt;&lt;br/&gt; &lt;input type=&quot;submit&quot; value=&quot;submit&quot;/&gt;&lt;/form&gt; 5.请求参数乱码问题idea控制台输出中文乱码：-Dfile.encoding=UTF-8 1234567891011121314&lt;!-- 编码过滤器,必须放在web.xml最上面，防止缓存 --&gt; &lt;filter&gt; &lt;filter-name&gt;CharacterEncodingFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;CharacterEncodingFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;!-- 此处/*代表过滤所有请求 --&gt; &lt;/filter-mapping&gt; 6.关于静态资源处理在 springmvc 的配置文件中可以配置，静态资源不过滤： 1234&lt;!-- location 表示路径，mapping 表示文件，**表示该目录下的文件以及子目录的文件 --&gt;&lt;mvc:resources location=&quot;/css/&quot; mapping=&quot;/css/**&quot;/&gt;&lt;mvc:resources location=&quot;/images/&quot; mapping=&quot;/images/**&quot;/&gt;&lt;mvc:resources location=&quot;/scripts/&quot; mapping=&quot;/javascript/**&quot;/&gt; 7.关于get请求tomacat 对 GET 和 POST 请求处理方式是不同的，GET 请求的编码问题，要改 tomcat 的 server.xml 配置文件，如下： 123456789&lt;Connector connectionTimeout=&quot;20000&quot; port=&quot;8080&quot;protocol=&quot;HTTP/1.1&quot; redirectPort=&quot;8443&quot;/&gt;改为：&lt;Connector connectionTimeout=&quot;20000&quot; port=&quot;8080&quot;protocol=&quot;HTTP/1.1&quot; redirectPort=&quot;8443&quot;useBodyEncodingForURI=&quot;true&quot;/&gt;如果遇到 ajax 请求仍然乱码，请把：useBodyEncodingForURI=&quot;true&quot;改为 URIEncoding=&quot;UTF-8&quot;即可 8.自定义类型转换器代码：jsp页面 1234&lt;form action=&quot;success/date&quot; method=&quot;post&quot;&gt; &lt;input type=&quot;text&quot; name=&quot;date&quot;/&gt;&lt;br/&gt; &lt;input type=&quot;submit&quot; value=&quot;submit&quot;/&gt;&lt;/form&gt; 转换器 123456789101112131415public class StringToDate implements Converter&lt;String, Date&gt; &#123; @Override public Date convert(String s) &#123; if (s==null||&quot;&quot;.equals(s))&#123; throw new RuntimeException(&quot;输入不能为空&quot;); &#125; try &#123; SimpleDateFormat format = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;); return format.parse(s); &#125; catch (Exception e) &#123; throw new ClassCastException(&quot;类型转换异常&quot;); &#125; &#125;&#125; 自定义类型转换器 spring 配置类型转换器的机制是，将自定义的转换器注册到类型转换服务中去。 1234567891011&lt;!--自定义类型转换--&gt;&lt;!-- 配置类型转换器工厂 --&gt; &lt;bean id=&quot;conversionServiceFactoryBean&quot; class=&quot;org.springframework.context.support.ConversionServiceFactoryBean&quot;&gt; &lt;!-- 给工厂注入一个新的类型转换器 --&gt; &lt;property name=&quot;converters&quot;&gt; &lt;set&gt;&lt;!-- 配置自定义类型转换器 --&gt; &lt;bean class=&quot;com.atguigu.utils.StringToDate&quot;&gt;&lt;/bean&gt; &lt;/set&gt; &lt;/property&gt; &lt;/bean&gt;&lt;mvc:annotation-driven conversion-service=&quot;conversionServiceFactoryBean&quot;/&gt; 控制器 1234567891011/** * 自定义数据类型转换 * 1.编写类型转换类，实现 Converter 接口，该接口有两个泛型。 * 2.配置文件中配置转换器,并在注解支持里面注册。 * 3.控制层参数列表传入转换类 */ @RequestMapping(&quot;/date&quot;) public String getDate(Date date)&#123; System.out.println(date.toString()); return &quot;success&quot;; &#125; 9.使用servletAPI对象作为方法参数springMVC 还支持使用原始 ServletAPI 对象作为控制器方法的参数。支持原始 ServletAPI 对象有： 12345678/** * 获取原生ServletAPI */@RequestMapping(&quot;/getServlet&quot;)public String getServlet(HttpServletRequest request, HttpServletResponse response)&#123; System.out.println(&quot;getServlet()...&quot;); return &quot;success&quot;;&#125; 1&lt;a href=&quot;success/getServlet&quot;&gt;getServlet&lt;/a&gt;&lt;br/&gt; 四，常用注解1.RequestParam作用： 把请求中指定名称的参数给控制器中的形参赋值。属性： value：请求参数中的名称。 required：请求参数中是否必须提供此参数。默认值：true。表示必须提供，如果不提供将报错。 1234567891011/** * @RequestParam 当表单name和形参列表名字不一致时，使用此注解。 * 属性： * name：指定表单的name * required：指定是否为必须 */ @RequestMapping(&quot;/test1&quot;) public String test1(@RequestParam(name = &quot;name&quot;, required = false) String username, Integer age) &#123; System.out.println(username + &quot; &quot; + age); return &quot;success&quot;; &#125; 12345&lt;form method=&quot;post&quot; action=&quot;anno/test1&quot;&gt; &lt;input type=&quot;text&quot; name=&quot;name&quot;/&gt;&lt;br/&gt; &lt;input type=&quot;text&quot; name=&quot;age&quot;/&gt;&lt;br/&gt; &lt;input type=&quot;submit&quot; value=&quot;submit&quot;/&gt;&lt;/form&gt; 2.RequestBody作用： 用于获取请求体内容。直接使用得到是 key=value&amp;key=value…结构的数据。 get 请求方式不适用。属性： required：是否必须有请求体。默认值是:true。当取值为 true 时,get 请求方式会报错。如果取值 为 false，get 请求得到是 null。 12345678910111213/** * @RequestBody 用于获取请求体内容 * 属性： * 属性： * required：指定是否为必须 * 当请求方式为get时，指定为true会报错， * 指定为false时，拿到的是null。 */ @RequestMapping(&quot;/test2&quot;) public String test2(@RequestBody(required = false) String name) &#123; System.out.println(name); return &quot;success&quot;; &#125; 1234&lt;form method=&quot;post&quot; action=&quot;anno/test2&quot;&gt; &lt;input type=&quot;text&quot; name=&quot;name&quot;/&gt;&lt;br/&gt; &lt;input type=&quot;submit&quot; value=&quot;submit&quot;/&gt;&lt;/form&gt; 3.PathVariable作用： 用于绑定 url 中的占位符。例如：请求 url 中 /delete/{id}，这个{id}就是 url 占位符。 url 支持占位符是 spring3.0 之后加入的。是 springmvc 支持 rest 风格 URL 的一个重要标志。属性： value：用于指定 url 中占位符名称。 required：是否必须提供占位符。 1&lt;a href=&quot;anno/test3/张三&quot;&gt;test3&lt;/a&gt;&lt;br/&gt; 12345678910/** * @PathVariable restful风格，spring3.0新特性 * 以/参数方式传递参数 * 次注解的value属性指定传递的参数名 */@RequestMapping(&quot;/test3/&#123;name&#125;&quot;)public String test3(@PathVariable(&quot;name&quot;) String name) &#123; System.out.println(name); return &quot;success&quot;;&#125; 关于rest风格url123456789101112131415161718192021222324252627*什么是 rest：REST（英文：Representational State Transfer，简称 REST）描述了一个架构样式的网络系统，比如 web 应用程序。它首次出现在 2000 年 Roy Fielding 的博士论文中，他是 HTTP 规范的主要编写者之一。在目前主流的三种 Web 服务交互方案中，REST 相比于 SOAP（Simple Object Access protocol，简单对象访问协议）以及 XML-RPC 更加简单明了，无论是对 URL 的处理还是对 Payload 的编码，REST 都倾向于用更加简单轻量的方法设计和实现。值得注意的是 REST 并没有一个明确的标准，而更像是一种设计的风格。它本身并没有什么实用性，其核心价值在于如何设计出符合 REST 风格的网络接口。*restful 的优点它结构清晰、符合标准、易于理解、扩展方便，所以正得到越来越多网站的采用。*restful 的特性：*资源（Resources）：网络上的一个实体，或者说是网络上的一个具体信息。它可以是一段文本、一张图片、一首歌曲、一种服务，总之就是一个具体的存在。可以用一个 URI（统一资源定位符）指向它，每种资源对应一个特定的 URI 。要获取这个资源，访问它的 URI 就可以，因此 URI 即为每一个资源的独一无二的识别符。 表现层（Representation）：把资源具体呈现出来的形式，叫做它的表现层 （Representation）。比如，文本可以用 txt 格式表现，也可以用 HTML 格式、XML 格式、JSON 格式表现，甚至可以采用二进制格式。*状态转化（State Transfer）：每 发出一个请求，就代表了客户端和服务器的一次交互过程。*HTTP 协议，是一个无状态协议，即所有的状态都保存在服务器端。因此，如果客户端想要操作服务器，必须通过某种手段，让服务器端发生“状态转化”（State Transfer）。而这种转化是建立在表现层之上的，所以就是 “表现层状态转化”。具体说，就是 HTTP 协议里面，四个表示操作方式的动词：GET 、POST 、PUT、DELETE。它们分别对应四种基本操作：GET 用来获取资源，POST 用来新建资源，PUT 用来更新资源，DELETE 用来删除资源。*restful 的示例： /account/1 HTTP GET ： 得到 id = 1 的 account /account/1 HTTP DELETE： 删除 id = 1 的 account /account/1 HTTP PUT： 更新 id = 1 的 account /account HTTP POST： 新增 account 4.requestHeader作用： 用于获取请求消息头。属性： value：提供消息头名称 required：是否必须有此消息头注： 在实际开发中一般不怎么用。 1234567891011/** * @RequestHeader 获取请求头信息，不常用 * 属性： * value: * required: */@RequestMapping(&quot;/test4&quot;)public String test4(@RequestHeader(value = &quot;Accept-Language&quot;, required = false) String value) &#123; System.out.println(value); return &quot;success&quot;;&#125; 1&lt;a href=&quot;anno/test4&quot;&gt;test4&lt;/a&gt;&lt;br/&gt; 5.CookieValue作用： 用于把指定 cookie 名称的值传入控制器方法参数。属性： value：指定 cookie 的名称。 required：是否必须有此 cookie。 1234567891011/** * @CookieValue 获取cookie里面的信息 * 属性： * value： * required： */@RequestMapping(&quot;/test5&quot;)public String test5(@CookieValue(value = &quot;JSESSIONID&quot;, required = false) String value) &#123; System.out.println(value); return &quot;success&quot;;&#125; 1&lt;a href=&quot;anno/test5&quot;&gt;test5&lt;/a&gt;&lt;br/&gt; 6.ModelAttribute作用： 该注解是 SpringMVC4.3 版本以后新加入的。它可以用于修饰方法和参数。 出现在方法上，表示当前方法会在控制器的方法执行之前，先执行。它可以修饰没有返回值的方法，也可 以修饰有具体返回值的方法。 出现在参数上，获取指定的数据给参数赋值。属性： value：用于获取数据的 key。key 可以是 POJO 的属性名称，也可以是 map 结构的 key。应用场景： 当表单提交数据不是完整的实体类数据时，保证没有提交数据的字段使用数据库对象原来的数据。 例如： 我们在编辑一个用户时，用户有一个创建信息字段，该字段的值是不允许被修改的。在提交表单数 据是肯定没有此字段的内容，一旦更新会把该字段内容置为 null，此时就可以使用此注解解决问题。 12345678910111213141516171819202122/** * @ModelAttribute * 1.夹在方法上： * 1.没有返回值 * 可以应用在从表单获取的值不全，在返回方法前先给其将值补全 * 2.有返回值 * 可以根据表单提交的一个值在到达控制层之前先去从数据库查询， * *严重怀疑这个方法利用动态代理对方法进行增强。 * 2.加在方法参数上： * 可以为指定的属性赋值 * */ @RequestMapping(&quot;/test6&quot;) public String test6(User user) &#123; System.out.println(user.getName() + &quot;...&quot; + user.getAge()); return &quot;success&quot;; &#125; @ModelAttribute public void test11(User user)&#123; System.out.println(user.getName()+&quot; &quot;+user.getAge()+&quot;......&quot;); user.setAge(20); &#125; 1234&lt;form action=&quot;anno/test6&quot;&gt; &lt;input type=&quot;text&quot; name=&quot;name&quot;/&gt;&lt;br/&gt; &lt;input type=&quot;submit&quot; value=&quot;submit&quot;/&gt;&lt;/form&gt; 场景二：123456789101112131415161718 @RequestMapping(&quot;/test6&quot;) public String test6(User user) &#123; System.out.println(user.getName() + &quot;...&quot; + user.getAge()); return &quot;success&quot;; &#125; @ModelAttribute public User test22(String name) &#123; //模拟从数据库查询数据 User user=findUserByname(name); return user; &#125; //模拟服务层dao层方法 private User findUserByname(String name) &#123; User user = new User(); user.setName(name); user.setAge(20); return user; &#125; 1234&lt;form action=&quot;anno/test6&quot;&gt; &lt;input type=&quot;text&quot; name=&quot;name&quot;/&gt;&lt;br/&gt; &lt;input type=&quot;submit&quot; value=&quot;submit&quot;/&gt;&lt;/form&gt; 场景三123456789101112131415161718//模拟服务层dao层方法private User findUserByname(String name) &#123; User user = new User(); user.setName(name); user.setAge(20); return user;&#125;@RequestMapping(&quot;/test7&quot;)public String test7(@ModelAttribute(value=&quot;1&quot;) User user)&#123; System.out.println(user); return &quot;success&quot;;&#125;@ModelAttributepublic void test33(String name, Map&lt;String,User&gt;map)&#123; //模拟从数据库查询 User user=findUserByname(name); map.put(&quot;1&quot;,user);&#125; 1234&lt;form action=&quot;anno/test7&quot;&gt; &lt;input type=&quot;text&quot; name=&quot;name&quot;/&gt;&lt;br/&gt; &lt;input type=&quot;submit&quot; value=&quot;submit&quot;/&gt;&lt;/form&gt; 7.SessionAttribute作用： 用于多次执行控制器方法间的参数共享。属性： value：用于指定存入的属性名称 type：用于指定存入的数据类型 123&lt;a href=&quot;session/put&quot;&gt;put&lt;/a&gt;&lt;br/&gt;&lt;a href=&quot;session/get&quot;&gt;get&lt;/a&gt;&lt;br/&gt;&lt;a href=&quot;session/delete&quot;&gt;delete&lt;/a&gt;&lt;br/&gt; 123456789101112131415161718192021222324252627282930313233@Controller@RequestMapping(&quot;/session&quot;)@SessionAttributes(value = &#123;&quot;name&quot;,&quot;age&quot;&#125;)public class Session &#123; /** * 存入Session * Model 是 spring 提供的一个接口，该接口有一个实现类 ExtendedModelMap * 该类继承了 ModelMap，而 ModelMap 就是 LinkedHashMap 子类 */ @RequestMapping(&quot;/put&quot;) public String put(Model model)&#123; model.addAttribute(&quot;name&quot;,&quot;尹会东&quot;); model.addAttribute(&quot;age&quot;,23); return &quot;success&quot;; &#125; /** * 取出Session */ @RequestMapping(&quot;/get&quot;) public String get(ModelMap map)&#123; System.out.println(map.get(&quot;name&quot;)); System.out.println(map.get(&quot;age&quot;)); return &quot;success&quot;; &#125; /** * 清除Session */ @RequestMapping(&quot;/delete&quot;) public String delete(SessionStatus status)&#123; status.setComplete(); return &quot;success&quot;; &#125;&#125; 五，响应数据和结果视图1.返回值分类①void在 controller 方法形参上可以定义 request 和 response，使用 request 或 response 指定响应结果： 123456781、使用 request 转向页面，如下：request.getRequestDispatcher(&quot;/WEB-INF/pages/success.jsp&quot;).forward(request, response);2、也可以通过 response 页面重定向：response.sendRedirect(&quot;testRetrunString&quot;) 3、也可以通过 response 指定响应结果，例如响应 json 数据：response.setCharacterEncoding(&quot;utf-8&quot;);response.setContentType(&quot;application/json;charset=utf-8&quot;);response.getWriter().write(&quot;json 串&quot;); ②ModelAndViewModelAndView 是 SpringMVC 为我们提供的一个对象，该对象也可以用作控制器方法的返回值。 该对象中有两个方法： ③转发和重定向forward 转发controller 方法在提供了 String 类型的返回值之后，默认就是请求转发。​ 需要注意的是，如果用了 formward：则路径必须写成实际视图 url，不能写逻辑视图。​ 它相当于“request.getRequestDispatcher(“url”).forward(request,response)”。使用请求转发，既可以转发到 jsp，也可以转发到其他的控制器方法。 Redirect 重定向contrller 方法提供了一个 String 类型返回值之后，它需要在返回值里使用:redirect:它相当于“response.sendRedirect(url)”。需要注意的是，如果是重定向到 jsp 页面，则 jsp 页面不能写在 WEB-INF 目录中，否则无法找到 1234567891011121314151617181920212223242526272829303132333435363738394041424344@Controller@RequestMapping(&quot;/return&quot;)public class returnController &#123; /** * 返回值类型为String，在request作用于存放值，并显示到页面 * @param model * @return */ @RequestMapping(&quot;/string&quot;) public String test1(Model model)&#123; model.addAttribute(&quot;name&quot;,&quot;尹会东&quot;); model.addAttribute(&quot;age&quot;,20); return &quot;success&quot;; &#125; /** * 返回值类型为void */ @RequestMapping(&quot;/void&quot;) public void test2(HttpServletResponse response, HttpServletRequest request)throws Exception&#123;// //request.getRequestDispatcher(&quot;/WEB-INF/views/success.jsp&quot;).forward(request,response); //response.sendRedirect(&quot;/index.jsp&quot;); response.getWriter().write(&quot;aaaa&quot;); &#125; /** * 返回值类型为ModelAndView */ @RequestMapping(&quot;/model&quot;) public ModelAndView test3()&#123; ModelAndView mv = new ModelAndView(); mv.addObject(&quot;name&quot;,&quot;zhangsan&quot;); mv.addObject(&quot;age&quot;,20); mv.setViewName(&quot;success&quot;); return mv; &#125; /** * 关键字：forward和redirect */ @RequestMapping(&quot;/fr&quot;) public String test4()&#123; System.out.println(&quot;..................&quot;); //return &quot;forward:/WEB-INF/views/success.jsp&quot;; return &quot;redirect:/index.jsp&quot;; &#125;&#125; 12345678910111213&lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;return&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h3&gt;return返回值类型&lt;/h3&gt;&lt;a href=&quot;return/string&quot; &gt;string&lt;/a&gt;&lt;br/&gt;&lt;a href=&quot;return/void&quot; &gt;void&lt;/a&gt;&lt;br/&gt;&lt;a href=&quot;return/model&quot; &gt;model&lt;/a&gt;&lt;br/&gt;&lt;a href=&quot;return/fr&quot; &gt;fr&lt;/a&gt;&lt;br/&gt;&lt;/body&gt;&lt;/html&gt; ④@ResponseBody 注解响应json数据作用：​ 该注解用于将 Controller 的方法返回的对象，通过 HttpMessageConverter 接口转换为指定格式的数据如：json,xml 等，通过 Response 响应给客户端 123456789101112131415161718192021222324@Controller@RequestMapping(&quot;/ajax&quot;)public class Ajax &#123; /** * 发送Ajax异步请求 * 1.静态资源处理：在配置文件中加入&lt;mvc:resource /&gt;标签，指定放行的资源。 * 2.导入jackson的依赖 * 3. * @RequestBody * 接受请求体消息 * @ResponseBody * 发送响应体消息 * 4.springmvc框架已经为我们封装好了处理json数据的方法，底层会自动执行。 * @param user * @return */ @RequestMapping(&quot;/ajax&quot;) public @ResponseBody User testAjax(@RequestBody User user) &#123; user.setName(&quot;yinhuidong&quot;); user.setAge(23); return user; &#125;&#125; 123456789101112131415161718192021222324252627&lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;Ajax&lt;/title&gt; &lt;script src=&quot;js/jquery-3.3.1.min.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot;&gt; $(function () &#123; $(&quot;#btn&quot;).click(function () &#123; $.ajax(&#123; url: &quot;ajax/ajax&quot;, contentType:&quot;application/json;charset=UTF-8&quot;, data: &#x27;&#123;&quot;name&quot;:&quot;aa&quot;,&quot;age&quot;:20&#125;&#x27;, dataType: &quot;json&quot;, type: &quot;post&quot;, success: function (data) &#123; alert(data.name); alert(data.age); &#125; &#125;); &#125;); &#125;); &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;input type=&quot;button&quot; id=&quot;btn&quot; value=&quot;别点我&quot;/&gt;&lt;/body&gt;&lt;/html&gt; 12//data:JSON.stringify(&#123;&quot;name&quot;:&quot;张三&quot;,&quot;msg&quot;:message&#125;),data:&#x27;&#123;&quot;name&quot;:&quot;张三&quot;,&quot;msg&quot;:&quot;123&quot;&#125;&#x27;, 六，文件上传和下载1.文件上传前提条件 form 表单的 enctype 取值必须是：multipart/form-data (默认值是:application/x-www-form-urlencoded) enctype:是表单请求正文的类型 method 属性取值必须是 Post 提供一个文件选择域 原理分析1234567891011121314当 form 表单的 enctype 取值不是默认值后，request.getParameter()将失效。 enctype=”application/x-www-form-urlencoded”时，form 表单的正文内容是： key=value&amp;key=value&amp;key=value当 form 表单的 enctype 取值为 Mutilpart/form-data 时，请求正文内容就变成： 每一部分都是 MIME 类型描述的正文-----------------------------7de1a433602ac 分界符Content-Disposition: form-data; name=&quot;userName&quot; 协议头aaa 协议的正文-----------------------------7de1a433602acContent-Disposition: form-data; name=&quot;file&quot;; filename=&quot;C:\\Users\\zhy\\Desktop\\fileupload_demofile\\b.txt&quot;Content-Type: text/plain 协议的类型（MIME 类型）bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb-----------------------------7de1a433602ac-- ①传统模式的文件上传123456&lt;h3&gt;文件上传&lt;/h3&gt;&lt;form action=&quot;file/fileupload&quot; method=&quot;post&quot; enctype=&quot;multipart/form-data&quot;&gt; 择文件：&lt;input type=&quot;file&quot; name=&quot;upload&quot;/&gt;&lt;br/&gt; &lt;input type=&quot;submit&quot; value=&quot;上传文件&quot;/&gt;&lt;/form&gt;&lt;hr/&gt; 12345678910111213141516171819202122232425262728293031323334353637383940@Controller@RequestMapping(&quot;/file&quot;)public class upload &#123; /** * 文件上传1: * 传统的文件上传 * */ @RequestMapping(value = &quot;/fileupload&quot;) public String fileupload(HttpServletRequest request) throws Exception &#123; // 先获取到要上传的文件目录 String path = request.getSession().getServletContext().getRealPath(&quot;/uploads&quot;); // 创建File对象，一会向该路径下上传文件 File file = new File(path); // 判断路径是否存在，如果不存在，创建该路径 if (!file.exists()) &#123; file.mkdirs(); &#125; // 创建磁盘文件项工厂 DiskFileItemFactory factory = new DiskFileItemFactory(); ServletFileUpload fileUpload = new ServletFileUpload(factory); // 解析request对象 List&lt;FileItem&gt; list = fileUpload.parseRequest(request); // 遍历 for (FileItem fileItem : list) &#123; // 判断文件项是普通字段，还是上传的文件 if (fileItem.isFormField()) &#123; &#125; else &#123; // 上传文件项 &#125; // 获取到上传文件的名称 String filename = fileItem.getName(); // 上传文件 fileItem.write(new File(file, filename)); // 删除临时文件 fileItem.delete(); &#125; return &quot;success&quot;; &#125; &#125; ②springmvc文件上传12345&lt;form action=&quot;file/upload&quot; method=&quot;post&quot; enctype=&quot;multipart/form-data&quot;&gt; 择文件：&lt;input type=&quot;file&quot; name=&quot;upload&quot;/&gt;&lt;br/&gt; &lt;input type=&quot;submit&quot; value=&quot;上传文件&quot;/&gt;&lt;/form&gt;&lt;hr/&gt; 12345678910111213141516171819202122232425262728/** * springmvc文件上传 * 1.导入依赖 * commons-upload * commons-io * 2.配置文件解析器 * 3.编写jsp页面 * 4.代码实现 */ @RequestMapping(&quot;/upload&quot;) public String fileupload2(HttpSession session, MultipartFile upload)throws Exception&#123; //获取文件上传路径 String path = session.getServletContext().getRealPath(&quot;/img&quot;); File file = new File(path); //判断不存在该目录就创建 if (!file.exists())&#123; file.mkdirs(); &#125; //获取文件名 String filename = upload.getOriginalFilename(); System.out.println(filename); //起别名 String s = UUID.randomUUID().toString().replace(&quot;_&quot;, &quot;&quot;).toUpperCase(); filename=s+filename; //开始上传 upload.transferTo(new File(file,filename)); return &quot;success&quot;; &#125; 2.文件下载12345678910&lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h3&gt;文件下载&lt;/h3&gt;&lt;a href=&quot;down/down?name=6.jpg&quot;&gt;点击下载&lt;/a&gt;&lt;/body&gt;&lt;/html&gt; 123456789101112131415161718192021222324252627282930313233@Controller@RequestMapping(&quot;/down&quot;)public class down &#123; /** * 文件下载 * 1.获取文件名 * 2.获得文件下载路径 * 3.拼接 * 4.用流来加载文件到字节数组 * 5.设置头信息以附件形式打开 * 6.设置响应状态吗 * 7.下载 */ @RequestMapping(&quot;/down&quot;) public ResponseEntity&lt;byte[]&gt; down(HttpSession session,String name)throws Exception&#123; //获取文件下载路径 String path = session.getServletContext().getRealPath(&quot;/img&quot;); //加上文件名 String finalpath = path + File.separator + name; FileInputStream is = new FileInputStream(finalpath); //is.available()获取流的字节数 byte[] bytes = new byte[is.available()]; is.read(bytes); HttpHeaders headers = new HttpHeaders(); //设置有附件形式打开 headers.add(&quot;Content-Disposition&quot;, &quot;attachment;filename=&quot;+name); //设置响应吗 HttpStatus status=HttpStatus.OK; ResponseEntity&lt;byte[]&gt; entity = new ResponseEntity&lt;&gt;(bytes, headers, status); is.close(); return entity; &#125;&#125; 七，springmvc中的异常处理1.异常处理的思路系统中异常包括两类：预期异常和运行时异常 RuntimeException，前者通过捕获异常从而获取异常信息，后者主要通过规范代码开发、测试通过手段减少运行时异常的发生。​ 系统的 dao、service、controller 出现都通过 throws Exception 向上抛出，最后由 springmvc 前端控制器交由异常处理器进行异常处理。 2.代码自定义异常类1234567891011121314151617181920212223/** * @author yinhuidong * @createTime 2020-03-09-17:18 */public class MyException extends Exception&#123; private String message; public MyException() &#123; &#125; public MyException(String message) &#123; this.message = message; &#125; @Override public String getMessage() &#123; return message; &#125; public void setMessage(String message) &#123; this.message = message; &#125;&#125; 123456789101112131415161718192021/** * 自定义异常处理器 * @author yinhuidong * @createTime 2020-03-09-17:20 */public class HandlerException implements HandlerExceptionResolver &#123; @Override public ModelAndView resolveException(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o, Exception e) &#123; MyException mye=null; if (e instanceof MyException)&#123; mye= (MyException) e; &#125;else&#123; mye=new MyException(); &#125; mye.setMessage(&quot;系统繁忙，请稍后再试！&quot;); ModelAndView mv = new ModelAndView(); mv.addObject(&quot;message&quot;,mye.getMessage()); mv.setViewName(&quot;/error&quot;); return mv; &#125;&#125; 12&lt;!--异常处理类--&gt; &lt;bean id=&quot;handlerException&quot; class=&quot;com.atguigu.exception.HandlerException&quot;&gt;&lt;/bean&gt; 123456789101112131415161718192021/** * @author yinhuidong * @createTime 2020-03-09-17:27 */@Controller@RequestMapping(&quot;/exception&quot;)public class TestException &#123; /** * 测试异常 * 1.编写自定义异常类，继承Exception类 * 2.编写异常处理类， * 3.在配置文件中配置异常处理类 * 4.模拟发生异常 * @return */ @RequestMapping(&quot;/test&quot;) public String test1()throws MyException &#123; int i=1/0; return &quot;success&quot;; &#125;&#125; 1&lt;a href=&quot;exception/test&quot;&gt;测试异常&lt;/a&gt; 全局异常处理器 123456789101112131415161718192021222324252627282930313233343536public class ExceptionHandler implements HandlerExceptionResolver &#123; @Override public ModelAndView resolveException (HttpServletRequest request, HttpServletResponse response, Object o, Exception e) &#123; boolean isAjax = JudgeRequestType.judgeIsAjax(request); if (isAjax)&#123; try &#123; String message=e.getMessage(); Gson gson = new Gson(); String json = gson.toJson(message); response.getWriter().write(json); &#125; catch (IOException e1) &#123; e1.printStackTrace(); &#125; return null; &#125; ModelAndView mv = new ModelAndView(); mv.addObject(&quot;message&quot;,e.getMessage()); mv.setViewName(&quot;error&quot;); return mv; &#125;&#125;class JudgeRequestType &#123; public static boolean judgeIsAjax(HttpServletRequest request)&#123; String accept = request.getHeader(&quot;Accept&quot;); String header = request.getHeader(&quot;X-Requested-With&quot;); return (accept!=null &amp;&amp;accept.length()&gt;0&amp;&amp;accept.contains(&quot;application/json&quot;)) || (header!=null&amp;&amp;header.length()&gt;0&amp;&amp;header.equals(&quot;XMLHttpRequest&quot;)); &#125;&#125; 八，springmvc中的拦截器拦截器的作用Spring MVC 的处理器拦截器类似于 Servlet 开发中的过滤器 Filter，用于对处理器进行预处理和后处理。用户可以自己定义一些拦截器来实现特定的功能。​ 谈到拦截器，还要向大家提一个词——拦截器链（Interceptor Chain）。拦截器链就是将拦截器按一定的顺序联结成一条链。在访问被拦截的方法或字段时，拦截器链中的拦截器就会按其之前定义的顺序被调用。​ 说到这里，可能大家脑海中有了一个疑问，这不是我们之前学的过滤器吗？是的它和过滤器是有几分相似，但是也有区别，接下来我们就来说说他们的区别： 过滤器是 servlet 规范中的一部分，任何 java web 工程都可以使用。 拦截器是 SpringMVC 框架自己的，只有使用了 SpringMVC 框架的工程才能用。 过滤器在 url-pattern 中配置了/*之后，可以对所有要访问的资源拦截。 拦截器它是只会拦截访问的控制器方法，如果访问的是 jsp，html,css,image 或者 js 是不会进行拦 截的。​ 它也是 AOP 思想的具体应用。我们要想自定义拦截器， 要求必须实现：HandlerInterceptor 接口。 自定义拦截器步骤1.写一个类继承HandlerInterceptor接口12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/** * @author yinhuidong * @createTime 2020-03-09-17:33 */public class Intercepter1 implements HandlerInterceptor &#123; /** * 1. preHandle方法是controller方法执行前拦截的方法 * 1. 可以使用request或者response跳转到指定的页面 * 2. return true放行，执行下一个拦截器，如果没有拦截器， * 执行controller中的方法。 * 3. return false不放行，不会执行controller中的方法 * @param request * @param response * @param handler * @return * @throws Exception */ public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; System.out.println(&quot;preHandle()....&quot;); //request.getRequestDispatcher(&quot;/WEB-INF/views/error.jsp&quot;).forward(request, response); return true; &#125; /** * 2. postHandle是controller方法执行后执行的方法，在JSP视图执行前。 * 1. 可以使用request或者response跳转到指定的页面 * 2. 如果指定了跳转的页面，那么controller方法跳转的页面将不会显示。 * @param request * @param response * @param handler * @param modelAndView * @throws Exception */ public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, @Nullable ModelAndView modelAndView) throws Exception &#123; System.out.println(&quot;postHandle()....&quot;); &#125; /** * 3. postHandle方法是在JSP执行后执行 * request或者response不能再跳转页面了 * @param request * @param response * @param handler * @param ex * @throws Exception */ public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, @Nullable Exception ex) throws Exception &#123; System.out.println(&quot;afterCompletion()...&quot;); &#125;&#125; 2.在springmvc的配置文件中配置拦截器1234567891011121314151617181920&lt;!--配置拦截器--&gt; &lt;mvc:interceptors&gt; &lt;!--配置一个拦截器--&gt; &lt;mvc:interceptor&gt; &lt;!--设置拦截路径--&gt; &lt;mvc:mapping path=&quot;/**&quot;/&gt; &lt;!--设置哪些不拦截--&gt; &lt;!--&lt;mvc:exclude-mapping path=&quot;&quot;/&gt;--&gt; &lt;!--配置bean--&gt; &lt;bean class=&quot;com.atguigu.intercept.Intercepter1&quot;/&gt; &lt;/mvc:interceptor&gt; &lt;mvc:interceptor&gt; &lt;!--设置拦截路径--&gt; &lt;mvc:mapping path=&quot;/**&quot;/&gt; &lt;!--设置哪些不拦截--&gt; &lt;!--&lt;mvc:exclude-mapping path=&quot;&quot;/&gt;--&gt; &lt;!--配置bean--&gt; &lt;bean class=&quot;com.atguigu.intercept.Intercepter2&quot;/&gt; &lt;/mvc:interceptor&gt; &lt;/mvc:interceptors&gt; 3.编写测试类和jsp页面12345678910111213141516171819/** * @author yinhuidong * @createTime 2020-03-09-17:37 */@Controller@RequestMapping(&quot;/intercepter&quot;)public class TestIntercepter &#123; /** * 1.编写拦截器 * 2.在配置文件中配置拦截器 * 3.测试 * 4.多个拦截器执行顺序 */ @RequestMapping(&quot;/test&quot;) public String test()&#123; System.out.println(&quot;controller().....&quot;); return &quot;success&quot;; &#125;&#125; 1&lt;a href=&quot;intercepter/test&quot;&gt;测试拦截器&lt;/a&gt; 4.定义多个拦截器12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/** * @author yinhuidong * @createTime 2020-03-09-17:33 */public class Intercepter2 implements HandlerInterceptor &#123; /** * 1. preHandle方法是controller方法执行前拦截的方法 * 1. 可以使用request或者response跳转到指定的页面 * 2. return true放行，执行下一个拦截器，如果没有拦截器， * 执行controller中的方法。 * 3. return false不放行，不会执行controller中的方法 * @param request * @param response * @param handler * @return * @throws Exception */ public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; System.out.println(&quot;preHandle2()....&quot;); //request.getRequestDispatcher(&quot;/WEB-INF/views/error.jsp&quot;).forward(request, response); return true; &#125; /** * 2. postHandle是controller方法执行后执行的方法，在JSP视图执行前。 * 1. 可以使用request或者response跳转到指定的页面 * 2. 如果指定了跳转的页面，那么controller方法跳转的页面将不会显示。 * @param request * @param response * @param handler * @param modelAndView * @throws Exception */ public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, @Nullable ModelAndView modelAndView) throws Exception &#123; System.out.println(&quot;postHandle()2....&quot;); &#125; /** * 3. postHandle方法是在JSP执行后执行 * request或者response不能再跳转页面了 * @param request * @param response * @param handler * @param ex * @throws Exception */ public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, @Nullable Exception ex) throws Exception &#123; System.out.println(&quot;afterCompletion()2...&quot;); &#125;&#125; 拦截器的简单应用需求：123456781、有一个登录页面，需要写一个 controller 访问页面 2、登录页面有一提交表单的动作。需要在 controller 中处理。 2.1、判断用户名密码是否正确 2.2、如果正确 向 session 中写入用户信息 2.3、返回登录成功。3、拦截用户请求，判断用户是否登录 3.1、如果用户已经登录。放行 3.2、如果用户未登录，跳转到登录页面 代码实现：1.控制器代码1234567891011121314151617181920//登陆页面@RequestMapping(&quot;/login&quot;)public String login(Model model)throws Exception&#123; return &quot;login&quot;; &#125;//登陆提交//userid：用户账号，pwd：密码@RequestMapping(&quot;/loginsubmit&quot;)public String loginsubmit(HttpSession session,String userid,String pwd)throws Exception&#123; //向 session 记录用户身份信息 session.setAttribute(&quot;activeUser&quot;, userid); return &quot;redirect:/main.jsp&quot;; &#125;//退出@RequestMapping(&quot;/logout&quot;)public String logout(HttpSession session)throws Exception&#123; //session 过期 session.invalidate(); return &quot;redirect:index.jsp&quot;;&#125; 2.拦截器代码123456789101112131415161718public class LoginInterceptor implements HandlerInterceptor&#123;@OverridePublic boolean preHandle(HttpServletRequest request,HttpServletResponse response, Object handler) throws Exception &#123; //如果是登录页面则放行 if(request.getRequestURI().indexOf(&quot;login.action&quot;)&gt;=0)&#123; return true; &#125; HttpSession session = request.getSession(); //如果用户已登录也放行 if(session.getAttribute(&quot;user&quot;)!=null)&#123; return true; &#125; //用户没有登录挑战到登录页面 request.getRequestDispatcher(&quot;/WEB-INF/jsp/login.jsp&quot;).forward(request,response); return false; &#125; &#125;","categories":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/tags/Spring/"}]},{"title":"Spring[扩展]基本使用篇","slug":"Spring/Spring[扩展]基本使用篇","date":"2022-01-11T06:11:20.219Z","updated":"2022-01-11T06:20:10.278Z","comments":true,"path":"2022/01/11/Spring/Spring[扩展]基本使用篇/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/Spring/Spring[%E6%89%A9%E5%B1%95]%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%E7%AF%87/","excerpt":"","text":"一，Spring的概述1.Spring是什么？Spring 是分层的 Java SE/EE 应用 full-stack 轻量级开源框架，以 IoC（Inverse Of Control：反转控制）和 AOP（Aspect Oriented Programming：面向切面编程）为内核，提供了展现层 Spring MVC 和持久层 Spring JDBC 以及业务层事务管理等众多的企业级应用技术，还能整合开源世界众多著名的第三方框架和类库，逐渐成为使用最多的 Java EE 企业应用开源框架。​ 2.Spring的优势？ 方便解耦，简化开发 AOP编程的支持 声明式事务的支持 方便程序的测试 方便集成各种优秀框架 降低JavaEE API 的使用难度 源码是经典的学习范例 ​ 3.Spring的体系结构图 二，IOC1.程序的耦合和解耦耦合是影响软件复杂程度和设计质量的一个重要因素，在设计上我们应采用以下原则：如果模块间必须 存在耦合，就尽量使用数据耦合，少用控制耦合，限制公共耦合的范围，尽量避免使用内容耦合。​ 2.解决耦合的思路Class.forName(&quot;com.mysql.jdbc.Driver&quot;);//此处只是一个字符串，此时的好处是，我们的类中不再依赖具体的驱动类，此时就算删除 mysql 的驱动 jar 包，依然可以编译（运行就不要想了，没有驱动不可能运行成功的）。​ 同时，也产生了一个新的问题，mysql 驱动的全限定类名字符串是在 java 类中写死的，一旦要改还是要修改源码。解决这个问题也很简单，使用配置文件配置。​ 3.工厂模式解耦在实际开发中可以把三层的对象都使用配置文件配置起来，当启动服务器应用加载的时候，让一个类中的方法通过读取配置文件，把这些对象创建出来并存起来。在接下来的使用的时候，直接拿过来用就好了。那么，这个读取配置文件，创建和获取三层对象的类就是工厂。​ 4.代码案例4.1 AccountMapper123456789101112131415161718/** * @author 二十 * @since 2021/9/21 3:23 下午 */public class AccountMapper &#123; public void update(Account account) &#123; System.out.println(&quot;调用了update()...&quot;); &#125; public void add(Account account) &#123; System.out.println(&quot;调用了add()...&quot;); &#125; public void delete(Integer id) &#123; System.out.println(&quot;调用了delete()...&quot;); &#125;&#125; 4.2 AccountService123456789101112131415161718192021222324252627/** * @author 二十 * @since 2021/9/21 3:25 下午 */public class AccountService &#123; private Account account = (Account) BeanFactory.getBean(&quot;account&quot;); private AccountMapper accountDao = (AccountMapper) BeanFactory.getBean(&quot;accountMapper&quot;); int i = 1; public void update(Account account) &#123; accountDao.update(account); &#125; public void add(Account account) &#123; accountDao.add(account); System.out.println(i + 1); &#125; public void delete(Integer id) &#123; System.out.println(accountDao); accountDao.delete(id); System.out.println(account.toString()); System.out.println(i + 1); &#125;&#125; 4.3 BeanFactory1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * @author 二十 * @since 2021/9/21 3:27 下午 */public class BeanFactory &#123; private static Properties prop; private static Map&lt;String, Object&gt; beans; private static InputStream in; private final static String configFile = &quot;beans.properties&quot;; static &#123; try &#123; in = BeanFactory.class.getClassLoader().getResourceAsStream(configFile); prop = new Properties(); prop.load(in); beans = new HashMap&lt;&gt;(); Enumeration&lt;Object&gt; keys = prop.keys(); while (keys.hasMoreElements()) &#123; String key = keys.nextElement().toString(); String path = prop.getProperty(key); Object value = Class.forName(path).newInstance(); beans.put(key, value); &#125; &#125; catch (Exception e) &#123; &#125; finally &#123; try &#123; assert in != null; in.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; public static Object getBean(String name) &#123; assert name != null; return beans.get(name); &#125;&#125; 4.4 beans.properties123accountService=com.es.service.AccountServiceaccountMapper=com.es.mapper.AccountMapperaccount=com.es.domain.Account 4.5 单元测试123456@Testpublic void test()&#123;//AccountService service=new AccountServiceImpl();AccountService service = (AccountService) BeanFactory.getBean(&quot;accountService&quot;);service.delete(1);&#125; 5.控制反转 存哪去？ 分析：由于我们是很多对象，肯定要找个集合来存。这时候有 Map 和 List 供选择。到底选 Map 还是 List 就看我们有没有查找需求。有查找需求，选 Map。所以我们的答案就是在应用加载时，创建一个 Map，用于存放三层对象。我们把这个 map 称之为容器。 还是没解释什么是工厂？ 工厂就是负责给我们从容器中获取指定对象的类。这时候我们获取对象的方式发生了改变。原来：我们在获取对象时，都是采用 new 的方式。是主动的。现在：我们获取对象时，同时跟工厂要，有工厂为我们查找或者创建对象。是被动的。 这种被动接收的方式获取对象的思想就是控制反转，它是 spring 框架的核心之一。 明确 ioc 的作用： 削减计算机程序的耦合(解除我们代码中的依赖关系)。 三，使用IOC解决程序的耦合1.基于xml形式的装配1.1 步骤​ 创建maven工程，导入相关依赖 创建spring的配置文件，applicationContext.xml配置文件 让spring管理资源，在spring的配置文件中配置service和mapper ​ 1.2 代码12345678910/** * 测试基于xml形式的spring ioc获取对象 * */@Testpublic void test3()&#123; ApplicationContext ioc=new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); User user= (User) ioc.getBean(&quot;user&quot;);//在此处打断点验证对象是什么时候被创建的。 user.show();&#125; 12&lt;!-- 基于xml形式装配bean --&gt;&lt;bean id=&quot;user&quot; class=&quot;com.es.domain.User&quot;&gt;&lt;/bean&gt; 2.细节2.1 BeanFactory和ApplicationContext的区别BeanFactory 才是 Spring 容器中的顶层接口。​ ApplicationContext 是它的子接口。​ BeanFactory 和 ApplicationContext 的区别：创建对象的时间点不一样。 ApplicationContext：只要一读取配置文件，默认情况下就会创建对象。 BeanFactory：什么使用什么时候创建对象 ​ 2.2 ApplicationContext接口的实现类ClasspathXmlApplicationContext：从类的根路径下加载配置文件，推荐使用这种方式。​ FileSystemXmlApplicationContext：从磁盘路径上加载配置文件，可以指定在任意位置。​ AnnotationConfigApplicationContext：当使用注解配置容器或者对象的时候，需要使用此类来创建 spring 容器，用来读取注解。​ 2.3 bean标签作用：用于配置对象让 spring 来创建的。默认情况下它调用的是类中的无参构造函数。如果没有无参构造函数则不能创建成功。 属性： id：给对象在容器中提供一个唯一标识。用于获取对象。 class：指定类的全限定类名。用于反射创建对象。默认情况下调用无参构造函数。 scope：指定对象的作用范围。 singleton :默认值，单例的. prototype :多例的. request :WEB 项目中,Spring 创建一个 Bean 的对象,将对象存入到 request 域中. session :WEB 项目中,Spring 创建一个 Bean 的对象,将对象存入到 session 域中. global session :WEB 项目中,应用在 Portlet 环境.如果没有 Portlet 环境那么globalSession 相当于 session. init-method：指定类中的初始化方法名称。 destroy-method：指定类中销毁方法名称。 2.4 bean的生命周期和作用范围单例对象：scope=&quot;singleton&quot; 一个应用只有一个对象的实例。它的作用范围就是整个引用。​ 生命周期： 对象出生：当应用加载，创建容器时，对象就被创建了。 对象活着：只要容器在，对象一直活着。 对象死亡：当应用卸载，销毁容器时，对象就被销毁了。 ​ 多例对象：scope=&quot;prototype&quot; 每次访问对象时，都会重新创建对象实例。​ 生命周期： 对象出生：当使用对象时，创建新的对象实例。 对象活着：只要对象在使用中，就一直活着。 对象死亡：当对象长时间不用时，被 java 的垃圾回收器回收了。 2.5 实例化bean的三种方式2.5.1.使用无参构造器默认情况下：它会根据默认无参构造器来创建类对象。如果bean中没有默认无参构造器，将会创建失败。​ 1&lt;bean id=&quot;accountService&quot; class=&quot;com.itheima.service.impl.AccountServiceImpl&quot;/&gt; 2.5.2.静态工厂此种方式是使用StaticFactory类中的静态方法创建对象，并存入Spring容器。​ id属性：指定bean的id，用于从容器中获取。​ class属性：指定静态工厂的全限定类名。​ factory-method属性：指定生产对象的静态方法。​ 案例：​ 12345678/*** 模拟一个静态工厂，创建业务层实现类*/public class StaticFactory &#123; public static IAccountService createAccountService()&#123; return new AccountServiceImpl(); &#125; &#125; 1&lt;bean id=&quot;accountService&quot; class=&quot;com.itheima.factory.StaticFactory&quot; factory-method=&quot;createAccountService&quot;&gt;&lt;/bean&gt; 2.5.3 实例工厂此种方式先把工厂的创建交给spring容器来管理。然后再使用工厂的bean来调用里面的方法。​ factory-bean属性：用于指定实例工厂bean的id。​ factory-method属性：用于指定实例工厂中创建对象的方法。​ 案例：​ 123456789/*** 模拟一个实例工厂，创建业务层实现类* 此工厂创建对象，必须现有工厂实例对象，再调用方法*/public class InstanceFactory &#123; public IAccountService createAccountService()&#123; return new AccountServiceImpl(); &#125; &#125; 12&lt;bean id=&quot;instancFactory&quot; class=&quot;com.itheima.factory.InstanceFactory&quot;&gt;&lt;/bean&gt; &lt;bean id=&quot;accountService&quot;factory-bean=&quot;instancFactory&quot;factory- method=&quot;createAccountService&quot;&gt;&lt;/bean&gt; 3.依赖注入依赖注入（Dependency Injection）他是spring框架的核心，ioc的具体实现。​ 编写程序的时候，通过控制反转，把对象的创建交给spring容器，但是代码中不可能出现没有依赖的情况。​ ioc解耦只是降低他们的依赖关系，但是不会消除。比如我们的业务层仍然会调用持久层的方法。​ 那么这种业务层和持久层的依赖关系，在使用spring框架之后，就让spring来维护了。​ 简单地说，就是坐等框架把持久层的对象传入业务层，不需要开发人员手动去获取。​ 3.1 构造函数注入就是使用类中的构造函数给成员变量赋值。 注意：赋值的操作不是我们自己做的，而是通过配置的方式，让spring框架来为我们注入。 ​ 3.2 set()注入在类中提供需要注入的成员的set方法。​ 3.3 注入集合属性给类中的集合成员传递值，他用的也是set方法的注入方式，只不过变量的数据类型都是集合。​ 3.4 案例3.4.1 User123456789101112131415161718192021222324252627282930313233public class User &#123; private String name; private Integer age; private Date birth; public void setName(String name) &#123; this.name = name; &#125; public void setAge(Integer age) &#123; this.age = age; &#125; public void setBirth(Date birth) &#123; this.birth = birth; &#125; public User()&#123; System.out.println(&quot;我被创建了...&quot;); &#125; public void show()&#123; System.out.println(&quot;user中的show方法背调用了。。。&quot;); &#125; @Override public String toString() &#123; return &quot;User&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, age=&quot; + age + &quot;, birth=&quot; + birth + &#x27;&#125;&#x27;; &#125;&#125; 3.4.2 Person1234567891011121314151617181920public class Person &#123; private String name; private int age; public Person() &#123; &#125; public Person(String name, int age) &#123; this.name = name; this.age = age; &#125; @Override public String toString() &#123; return &quot;Person&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, age=&quot; + age + &#x27;&#125;&#x27;; &#125;&#125; 3.4.3 CollectionDemo1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class CollectionDemo &#123; private String [] arr; private List&lt;String&gt; myList; private Set&lt;String&gt; mySet; private Map&lt;String,String&gt; myMap; private Properties myProp; public void setArr(String[] arr) &#123; this.arr = arr; &#125; public void setMyList(List&lt;String&gt; myList) &#123; this.myList = myList; &#125; public void setMySet(Set&lt;String&gt; mySet) &#123; this.mySet = mySet; &#125; public void setMyMap(Map&lt;String, String&gt; myMap) &#123; this.myMap = myMap; &#125; public void setMyProp(Properties myProp) &#123; this.myProp = myProp; &#125; public String[] getArr() &#123; return arr; &#125; public List&lt;String&gt; getMyList() &#123; return myList; &#125; public Set&lt;String&gt; getMySet() &#123; return mySet; &#125; public Map&lt;String, String&gt; getMyMap() &#123; return myMap; &#125; public Properties getMyProp() &#123; return myProp; &#125;&#125; 3.4.4配置文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354&lt;!-- 基于xml形式装配bean --&gt;&lt;bean id=&quot;user&quot; class=&quot;com.es.java1.User&quot;&gt;&lt;/bean&gt;&lt;!--使用get方法创建bean--&gt;&lt;bean id=&quot;user2&quot; class=&quot;com.es.java1.User&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;张&quot;&gt;&lt;/property&gt; &lt;property name=&quot;age&quot;&gt; &lt;value&gt;20&lt;/value&gt; &lt;/property&gt; &lt;property name=&quot;birth&quot; ref=&quot;now&quot;&gt;&lt;/property&gt;&lt;/bean&gt;&lt;bean id=&quot;now&quot; class=&quot;java.util.Date&quot;&gt;&lt;/bean&gt;&lt;!--集合和数组类型的依赖注入--&gt;&lt;bean id=&quot;demo&quot; class=&quot;com.es.java1.CollectionDemo&quot;&gt; &lt;property name=&quot;arr&quot;&gt; &lt;array&gt; &lt;value&gt;111&lt;/value&gt; &lt;value&gt;222&lt;/value&gt; &lt;value&gt;333&lt;/value&gt; &lt;/array&gt; &lt;/property&gt; &lt;property name=&quot;myList&quot;&gt; &lt;list&gt; &lt;value&gt;111&lt;/value&gt; &lt;value&gt;222&lt;/value&gt; &lt;value&gt;333&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;property name=&quot;mySet&quot;&gt; &lt;set&gt; &lt;value&gt;111&lt;/value&gt; &lt;value&gt;222&lt;/value&gt; &lt;value&gt;333&lt;/value&gt; &lt;/set&gt; &lt;/property&gt; &lt;property name=&quot;myMap&quot;&gt; &lt;map&gt; &lt;entry key=&quot;aaa&quot; value=&quot;aaa&quot;&gt;&lt;/entry&gt; &lt;entry key=&quot;bbb&quot; value=&quot;bbb&quot;&gt;&lt;/entry&gt; &lt;entry key=&quot;ccc&quot; value=&quot;ccc&quot;&gt;&lt;/entry&gt; &lt;/map&gt; &lt;/property&gt; &lt;property name=&quot;myProp&quot;&gt; &lt;props&gt; &lt;prop key=&quot;aaa&quot;&gt;aaa&lt;/prop&gt; &lt;prop key=&quot;bbb&quot;&gt;bbb&lt;/prop&gt; &lt;prop key=&quot;ccc&quot;&gt;ccc&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt;&lt;/bean&gt;&lt;!--使用默认构造器创建bean--&gt;&lt;bean id=&quot;person&quot; class=&quot;com.es.java1.Person&quot;&gt; &lt;constructor-arg name=&quot;name&quot; value=&quot;张&quot;&gt;&lt;/constructor-arg&gt; &lt;constructor-arg name=&quot;age&quot; value=&quot;20&quot;&gt;&lt;/constructor-arg&gt;&lt;/bean&gt; 3.4.5 测试类12345678910111213141516171819202122232425262728293031323334353637383940414243/** * 测试基于xml形式的spring ioc获取对象 * */@Testpublic void test3()&#123; ApplicationContext ioc=new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); User user= (User) ioc.getBean(&quot;user&quot;);//在此处打断点验证对象是什么时候被创建的。 user.show();&#125;/** * 采用默认构造器的形式创建bean对象 */@Testpublic void test()&#123; ApplicationContext ioc=new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); Person p= (Person) ioc.getBean(&quot;person&quot;); Person p2= (Person) ioc.getBean(&quot;person&quot;); System.out.println(p.toString());&#125;/** * 使用get方法进行依赖注入 */@Testpublic void test4()&#123; ApplicationContext ioc=new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); User user= (User) ioc.getBean(&quot;user2&quot;);//在此处打断点验证对象是什么时候被创建的。 System.out.println(user.toString());&#125;/** * 集合和数组的依赖注入 */@Testpublic void test5()&#123; ApplicationContext ioc=new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); CollectionDemo demo= (CollectionDemo) ioc.getBean(&quot;demo&quot;); System.out.println(Arrays.toString(demo.getArr())); System.out.println(demo.getMyList()); System.out.println(demo.getMySet()); System.out.println(demo.getMyMap()); System.out.println(demo.getMyProp());&#125; 四，使用ioc实现账户CRUD1.基于xml形式1.1 引用外部属性文件12345678910&lt;!-- 引用外部属性文件 --&gt; &lt;context:property-placeholder location=&quot;classpath:druid.properties&quot;/&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot;&gt; &lt;property name=&quot;username&quot; value=&quot;$&#123;jdbc.username&#125;&quot;&gt;&lt;/property&gt; &lt;property name=&quot;url&quot; value=&quot;$&#123;jdbc.url&#125;&quot;&gt;&lt;/property&gt; &lt;property name=&quot;password&quot; value=&quot;$&#123;jdbc.password&#125;&quot;&gt;&lt;/property&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;$&#123;driverClassName&#125;&quot;&gt;&lt;/property&gt; &lt;property name=&quot;initialSize&quot; value=&quot;$&#123;initialSize&#125;&quot;&gt;&lt;/property&gt; &lt;property name=&quot;maxActive&quot; value=&quot;$&#123;maxActive&#125;&quot;&gt;&lt;/property&gt; &lt;/bean&gt; 1.2 SPEL表达式1.2.1 简介​ Spring Expression Language，Spring表达式语言，简称SpEL。支持运行时查询并可以操作对象图。​ 和JSP页面上的EL表达式、Struts2中用到的OGNL表达式一样，SpEL根据JavaBean风格的getXxx()、setXxx()方法定义的属性访问对象图，完全符合我们熟悉的操作习惯。​ 1.2.2 基本语法​ SpEL使用#{…}作为定界符，所有在大框号中的字符都将被认为是SpEL表达式。​ 1.2.3 使用字面量●整数：&lt;property name=&quot;count&quot; value=&quot;#&#123;5&#125;&quot;/&gt; ●小数：&lt;property name=&quot;frequency&quot; value=&quot;#&#123;89.7&#125;&quot;/&gt; ●科学计数法：&lt;property name=&quot;capacity&quot; value=&quot;#&#123;1e4&#125;&quot;/&gt; ●String类型的字面量可以使用单引号或者双引号作为字符串的定界符号 &lt;property name=”name” value=&quot;#&#123;&#39;Chuck&#39;&#125;&quot;/&gt; &lt;property name=&#39;name&#39; value=&#39;#&#123;&quot;Chuck&quot;&#125;&#39;/&gt; ●Boolean：&lt;property name=&quot;enabled&quot; value=&quot;#&#123;false&#125;&quot;/&gt; 1.2.4 引用其他bean123456&lt;bean id=&quot;emp04&quot; class=&quot;com.es.parent.bean.Employee&quot;&gt; &lt;property name=&quot;empId&quot; value=&quot;1003&quot;/&gt; &lt;property name=&quot;empName&quot; value=&quot;Kate&quot;/&gt; &lt;property name=&quot;age&quot; value=&quot;21&quot;/&gt; &lt;property name=&quot;detp&quot; value=&quot;#&#123;dept&#125;&quot;/&gt;&lt;/bean&gt; 1.2.5 引用其他bean的属性值作为自己某个属性的值123456&lt;bean id=&quot;emp05&quot; class=&quot;com.es.parent.bean.Employee&quot;&gt; &lt;property name=&quot;empId&quot; value=&quot;1003&quot;/&gt; &lt;property name=&quot;empName&quot; value=&quot;Kate&quot;/&gt; &lt;property name=&quot;age&quot; value=&quot;21&quot;/&gt; &lt;property name=&quot;deptName&quot; value=&quot;#&#123;dept.deptName&#125;&quot;/&gt;&lt;/bean&gt; 1.2.6调用非静态方法1234567&lt;!-- 创建一个对象，在SpEL表达式中调用这个对象的方法 --&gt;&lt;bean id=&quot;salaryGenerator&quot; class=&quot;com.es.spel.bean.SalaryGenerator&quot;/&gt;&lt;bean id=&quot;employee&quot; class=&quot;com.es.spel.bean.Employee&quot;&gt; &lt;!-- 通过对象方法的返回值为属性赋值 --&gt; &lt;property name=&quot;salayOfYear&quot; value=&quot;#&#123;salaryGenerator.getSalaryOfYear(5000)&#125;&quot;/&gt;&lt;/bean&gt; 1.2.7调用静态方法1234&lt;bean id=&quot;employee&quot; class=&quot;com.es.spel.bean.Employee&quot;&gt; &lt;!-- 在SpEL表达式中调用类的静态方法 --&gt; &lt;property name=&quot;circle&quot; value=&quot;#&#123;T(java.lang.Math).PI*20&#125;&quot;/&gt;&lt;/bean&gt; 1.2.8 运算符①算术运算符：+、-、*、/、%、^②字符串连接：+③比较运算符：&lt;、&gt;、==、&lt;=、&gt;=、lt、gt、eq、le、ge④逻辑运算符：and, or, not, |⑤三目运算符：判断条件?判断结果为true时的取值:判断结果为false时的取值⑥正则表达式：matches​ 1.3 Spring的#和$的区别$&#123;key名称&#125;：​ 用户获取外部文件中指定key的值； ​ 可以出现在xml配置文件中，也可以出现在注解@Value中； ​ 一般用户获取数据库配置文件的内容信息等。 #&#123;表达式&#125;：​ SpEL表达式的格式，详情(https://blog.csdn.net/xingfei_work/article/details/76058178))； ​ 可以出现在xml配置文件中，也可以出现在注解@Value中 ​ 可以任意表达式，支持运算符等。 ​ 1.4 案例1.4.1 配置文件12345678910111213141516171819202122232425262728293031&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:jdbc=&quot;http://www.springframework.org/schema/jdbc&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/jdbc http://www.springframework.org/schema/jdbc/spring-jdbc-4.2.xsd http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.2.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-4.2.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-4.2.xsd&quot;&gt; &lt;!--设置自动扫描的包--&gt; &lt;context:component-scan base-package=&quot;com.es&quot;&gt;&lt;/context:component-scan&gt; &lt;bean id=&quot;accountDao&quot; class=&quot;com.es.dao.impl.AccountDaoImpl&quot;&gt; &lt;property name=&quot;runner&quot; ref=&quot;runner&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;bean id=&quot;accountService&quot; class=&quot;com.es.service.impl.AccountServiceImpl&quot;&gt; &lt;property name=&quot;accountDao&quot; ref=&quot;accountDao&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;bean id=&quot;account&quot; class=&quot;com.es.domain.Account&quot;&gt;&lt;/bean&gt; &lt;bean id=&quot;runner&quot; class=&quot;org.apache.commons.dbutils.QueryRunner&quot; scope=&quot;prototype&quot;&gt; &lt;constructor-arg name=&quot;ds&quot; ref=&quot;dataSource&quot;&gt;&lt;/constructor-arg&gt; &lt;/bean&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot;&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:3306/eesy&quot;&gt;&lt;/property&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot;&gt;&lt;/property&gt; &lt;property name=&quot;password&quot; value=&quot;yhd666&quot;&gt;&lt;/property&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;com.mysql.jdbc.Driver&quot;&gt;&lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; 1.4.2 持久层12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970package com.es.dao.impl;import com.es.dao.IAccountDao;import com.es.domain.Account;import org.apache.commons.dbutils.QueryRunner;import org.apache.commons.dbutils.handlers.BeanHandler;import org.apache.commons.dbutils.handlers.BeanListHandler;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Repository;import java.util.List;/** * 账户的持久层实现类 */public class AccountDaoImpl implements IAccountDao &#123; private QueryRunner runner; public void setRunner(QueryRunner runner) &#123; this.runner = runner; &#125; public List&lt;Account&gt; findAllAccount() &#123; try&#123; return runner.query(&quot;select * from account&quot;,new BeanListHandler&lt;Account&gt;(Account.class)); &#125;catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; public Account findAccountById(Integer accountId) &#123; try&#123; return runner.query(&quot;select * from account where id = ? &quot;,new BeanHandler&lt;Account&gt;(Account.class),accountId); &#125;catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; public void saveAccount(Account account) &#123; try&#123; runner.update(&quot;insert into account(name,money)values(?,?)&quot;,account.getName(),account.getMoney()); &#125;catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; public void updateAccount(Account account) &#123; try&#123; runner.update(&quot;update account set name=?,money=? where id=?&quot;,account.getName(),account.getMoney(),account.getId()); &#125;catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; public void deleteAccount(Integer accountId) &#123; try&#123; runner.update(&quot;delete from account where id=?&quot;,accountId); &#125;catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125;&#125; 1.4.3 业务层12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package com.es.service.impl;import com.es.dao.IAccountDao;import com.es.domain.Account;import com.es.domain.Account;import com.es.service.IAccountService;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import java.util.List;/** * 账户的业务层实现类 */public class AccountServiceImpl implements IAccountService&#123; private IAccountDao accountDao; public void setAccountDao(IAccountDao accountDao) &#123; this.accountDao = accountDao; &#125; public List&lt;Account&gt; findAllAccount() &#123; return accountDao.findAllAccount(); &#125; public Account findAccountById(Integer accountId) &#123; return accountDao.findAccountById(accountId); &#125; public void saveAccount(Account account) &#123; accountDao.saveAccount(account); &#125; public void updateAccount(Account account) &#123; accountDao.updateAccount(account); &#125; public void deleteAccount(Integer acccountId) &#123; accountDao.deleteAccount(acccountId); &#125;&#125; 1.4.4 测试类12345678public class Test1 &#123; ApplicationContext ioc=new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); @Test public void test1()&#123; IAccountService service= (IAccountService) ioc.getBean(&quot;accountService&quot;); service.deleteAccount(2); &#125;&#125; 2.xml和注解混搭2.1 用于创建对象他们的作用就和在XML配置文件中编写一个标签实现的功能是一样的。 ​Component: 作用：用于把当前类对象存入spring容器中 属性： value：用于指定bean的id。当我们不写时，它的默认值是当前类名，且首字母改小写。 ​Controller：一般用在表现层 ​Service：一般用在业务层 ​Repository：一般用在持久层 ​ 以上个注解他们的作用和属性与Component是一模一样。​ 他们是spring框架为我们提供明确的层使用的注解，使我们的层对象更加清晰。​ 2.2 用于注入数据他们的作用就和在xml配置文件中的bean标签中写一个标签的作用是一样的。 Autowired: 作用：自动照类型注入。只要容器中唯一的一个bean对象类型和要注入的变量类型匹配，就可以注入成功 如果ioc容器中没任何bean的类型和要注入的变量类型匹配，则报错。 如果Ioc容器中多个类型匹配时： 出现位置： 可以是变量上，也可以是方法上 细节： 在使用注解注入时，set方法就不是必须的了。 ​Qualifier: 作用：在照类型注入的基础之上再照名称注入。它在给类成员注入时不能单独使用。但是在给方法参数注入时可以 属性： value：用于指定注入bean的id。 ​Resource 作用：直接照bean的id注入。它可以独立使用 属性： name：用于指定bean的id。以上个注入都只能注入其他bean类型的数据，而基本类型和String类型无法使用上述注解实现。另外，集合类型的注入只能通过XML来实现。​ ​Value 作用：用于注入基本类型和String类型的数据 属性： value：用于指定数据的值。它可以使用spring中SpEL(也就是spring的el表达式，SpEL的写法：${表达式}​ 2.3.用于改变作用范围他们的作用就和在bean标签中使用scope属性实现的功能是一样的。 ​Scope 作用：用于指定bean的作用范围 属性： value：指定范围的取值。常用取值：singleton prototype​ 2.4 和生命周期相关他们的作用就和在bean标签中使用init-method和destroy-methode的作用是一样的。​ ​PreDestroy 作用：用于指定销毁方法 ​PostConstruct 作用：用于指定初始化方法​ 2.5 案例2.5.1 配置文件123456789101112131415161718192021222324&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:jdbc=&quot;http://www.springframework.org/schema/jdbc&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/jdbc http://www.springframework.org/schema/jdbc/spring-jdbc-4.2.xsd http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.2.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-4.2.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-4.2.xsd&quot;&gt; &lt;!--设置自动扫描的包--&gt; &lt;context:component-scan base-package=&quot;com.es&quot;&gt;&lt;/context:component-scan&gt; &lt;bean id=&quot;runner&quot; class=&quot;org.apache.commons.dbutils.QueryRunner&quot; scope=&quot;prototype&quot;&gt; &lt;constructor-arg name=&quot;ds&quot; ref=&quot;dataSource&quot;&gt;&lt;/constructor-arg&gt; &lt;/bean&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot;&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:3306/eesy&quot;&gt;&lt;/property&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot;&gt;&lt;/property&gt; &lt;property name=&quot;password&quot; value=&quot;yhd666&quot;&gt;&lt;/property&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;com.mysql.jdbc.Driver&quot;&gt;&lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; 2.5.2 持久层1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768package com.es.dao.impl;import com.es.dao.IAccountDao;import com.es.domain.Account;import org.apache.commons.dbutils.QueryRunner;import org.apache.commons.dbutils.handlers.BeanHandler;import org.apache.commons.dbutils.handlers.BeanListHandler;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Repository;import java.util.List;/** * 账户的持久层实现类 */@Repository(value = &quot;accountDao&quot;)public class AccountDaoImpl implements IAccountDao &#123; @Autowired private QueryRunner runner; public List&lt;Account&gt; findAllAccount() &#123; try&#123; return runner.query(&quot;select * from account&quot;,new BeanListHandler&lt;Account&gt;(Account.class)); &#125;catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; public Account findAccountById(Integer accountId) &#123; try&#123; return runner.query(&quot;select * from account where id = ? &quot;,new BeanHandler&lt;Account&gt;(Account.class),accountId); &#125;catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; public void saveAccount(Account account) &#123; try&#123; runner.update(&quot;insert into account(name,money)values(?,?)&quot;,account.getName(),account.getMoney()); &#125;catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; public void updateAccount(Account account) &#123; try&#123; runner.update(&quot;update account set name=?,money=? where id=?&quot;,account.getName(),account.getMoney(),account.getId()); &#125;catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; public void deleteAccount(Integer accountId) &#123; try&#123; runner.update(&quot;delete from account where id=?&quot;,accountId); &#125;catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125;&#125; 2.5.3 业务层1234567891011121314151617181920212223242526272829303132333435363738394041424344package com.es.service.impl;import com.es.dao.IAccountDao;import com.es.domain.Account;import com.es.domain.Account;import com.es.service.IAccountService;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import java.util.List;/** * 账户的业务层实现类 */@Service(&quot;accountService&quot;)public class AccountServiceImpl implements IAccountService&#123; @Autowired private IAccountDao accountDao; public List&lt;Account&gt; findAllAccount() &#123; return accountDao.findAllAccount(); &#125; public Account findAccountById(Integer accountId) &#123; return accountDao.findAccountById(accountId); &#125; public void saveAccount(Account account) &#123; accountDao.saveAccount(account); &#125; public void updateAccount(Account account) &#123; accountDao.updateAccount(account); &#125; public void deleteAccount(Integer acccountId) &#123; accountDao.deleteAccount(acccountId); &#125;&#125; 2.5.4 测试类1234567891011121314151617import com.es.service.IAccountService;import org.junit.Test;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;/** * @author yinhuidong * @createTime 2020-03-01-11:03 */public class Test1 &#123; ApplicationContext ioc=new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); @Test public void test1()&#123; IAccountService service= (IAccountService) ioc.getBean(&quot;accountService&quot;); service.deleteAccount(2); &#125;&#125; 3.纯注解配置3.1 注解 ​Configuration 作用：指定当前类是一个配置类细节：当配置类作为AnnotationConfigApplicationContext对象创建的参数时，该注解可以不写。 ​ComponentScan 作用：用于通过注解指定spring在创建容器时要扫描的包属性： value：它和basePackages的作用是一样的，都是用于指定创建容器时要扫描的包。 我们使用此注解就等同于在xml中配置了: &lt;context:component-scan base-package=&quot;com.itheima&quot;&gt;&lt;/context:component-scan&gt; ​Bean 作用：用于把当前方法的返回值作为bean对象存入spring的ioc容器中属性: name:用于指定bean的id。当不写时，默认值是当前方法的名称细节： 当我们使用注解配置方法时，如果有方法参数，spring框架会去容器中查找没可用的bean对象。 查找的方式和Autowired注解的作用是一样的 ​Import 作用：用于导入其他的配置类 属性： value：用于指定其他配置类的字节码。 当我们使用Import的注解之后，Import注解的类就父配置类，而导入的都是子配置类 ​PropertySource 作用：用于指定properties文件的位置 属性： value：指定文件的名称和路径。 关键字：classpath，表示类路径下 ​ 3.2 spring整合junit4123456789101112131415161718192021222324252627/**1、应用程序的入口 main方法2、junit单元测试中，没有main方法也能执行 junit集成了一个main方法 该方法就会判断当前测试类中哪些方法有 @Test注解 junit就让有Test注解的方法执行3、junit不会管我们是否采用spring框架 在执行测试方法时，junit根本不知道我们是不是使用了spring框架 所以也就不会为我们读取配置文件/配置类创建spring核心容器4、由以上三点可知 当测试方法执行时，没有Ioc容器，就算写了Autowired注解，也无法实现注入------------------------------------------------------------------------- * 使用Junit单元测试：测试我们的配置 * Spring整合junit的配置 * 1、导入spring整合junit的jar(坐标) * 2、使用Junit提供的一个注解把原有的main方法替换了，替换成spring提供的 * @Runwith(SpringJUnit4ClassRunner.class) * 3、告知spring的运行器，spring和ioc创建是基于xml还是注解的，并且说明位置 * @ContextConfiguration * locations：指定xml文件的位置，加上classpath关键字，表示在类路径下 * classes：指定注解类所在地位置 * * 当我们使用spring 5.x版本的时候，要求junit的jar必须是4.12及以上 */ 3.3 案例3.3.1 配置类1234567891011/** * @author yinhuidong * @createTime 2020-03-01-16:43 */@Configuration@ComponentScan(&quot;com.es&quot;)@Import(JdbcConfig.class)@PropertySource(&quot;classpath:c3p0.properties&quot;)public class SpringConfig &#123;&#125; 3.3.2 配置子类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/** * @author yinhuidong * @createTime 2020-03-01-16:56 */public class JdbcConfig &#123; @Bean(name=&quot;runner&quot;) @Scope(value = &quot;prototype&quot;) public QueryRunner getRunner(@Qualifier(&quot;ds1&quot;) DataSource dataSource) &#123; QueryRunner runner = new QueryRunner(dataSource); return runner; &#125; private static DataSource dataSource = null; @Bean(name=&quot;ds1&quot;) public DataSource getDataSource() &#123; try &#123; Properties prop = new Properties(); InputStream is = JdbcConfig.class.getClassLoader().getResourceAsStream(&quot;jdbc.properties&quot;); prop.load(is); dataSource = DruidDataSourceFactory.createDataSource(prop); return dataSource; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return null; &#125; @Value(&quot;$&#123;jdbc.driver&#125;&quot;) private String driver; @Value(&quot;$&#123;jdbc.url&#125;&quot;) private String url; @Value(&quot;$&#123;jdbc.username&#125;&quot;) private String username; @Value(&quot;$&#123;jdbc.password&#125;&quot;) private String password; @Bean(name=&quot;ds2&quot;) public DataSource getDataSource2()&#123; try &#123; ComboPooledDataSource dataSource=new ComboPooledDataSource(); dataSource.setDriverClass(driver); dataSource.setJdbcUrl(url); dataSource.setUser(username); dataSource.setPassword(password); return dataSource; &#125; catch (PropertyVetoException e) &#123; e.printStackTrace(); &#125; return null; &#125;&#125; 3.3.3 测试类1234567891011@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(classes = com.es.java1.SpringConfig.class)public class Test1 &#123; @Autowired private IAccountService service = null; @Test public void test1() &#123; service.deleteAccount(5); &#125;&#125; 五，AOP面向切面编程1.动态代理1.1 基于接口的动态代理1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public class Test1 &#123; @Test public void test1()&#123; //被代理类对象要声明为最终的 final Producer producer=new Producer(); /** * 动态代理： * 特点：字节码随用随创建，随用随加载 * 作用：不修改源码的基础上对方法增强 * 分类： * 基于接口的动态代理 * 基于子类的动态代理 * 基于接口的动态代理： * 涉及的类：Proxy * 提供者：JDK官方 * 如何创建代理对象： * 使用Proxy类中的newProxyInstance方法 * 创建代理对象的要求： * 被代理类最少实现一个接口，如果没有则不能使用 * newProxyInstance方法的参数： * ClassLoader：类加载器 * 它是用于加载代理对象字节码的。和被代理对象使用相同的类加载器。固定写法。 * Class[]：字节码数组 * 它是用于让代理对象和被代理对象相同方法。固定写法。 * InvocationHandler：用于提供增强的代码 * 它是让我们写如何代理。我们一般都是些一个该接口的实现类，通常情况下都是匿名内部类，但不是必须的。 * 此接口的实现类都是谁用谁写。 */ //代理对象和被代理类对象要实现同一个接口 IProducer proxyProducer = (IProducer) Proxy.newProxyInstance(producer.getClass().getClassLoader(), producer.getClass().getInterfaces(), new InvocationHandler() &#123; /** * 作用：执行被代理对象的任何接口方法都会经过该方法 * 方法参数的含义 * @param proxy 代理对象的引用 * 1. 可以使用反射获取代理对象的信息（也就是proxy.getClass().getName()。 * 2. 可以将代理对象返回以进行连续调用，这就是proxy存在的目的，因为this并不是代理对象。 * @param method 当前执行的方法 * @param args 当前执行方法所需的参数 * @return 和被代理对象方法相同的返回值 * @throws Throwable */ public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; Object value=null; //获取方法执行的参数 //判断当前方法是不是销售 if (&quot;saleProduct&quot;.equals(method.getName()))&#123; Float money= (Float) args[0]; //两个参数：被代理类对象，方法增强的参数 value=method.invoke(producer,money*0.8f); &#125; return value; &#125; &#125;); proxyProducer.saleProduct(10000f); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243package com.es.java1;/** * 一个生产者 */public class Producer implements IProducer&#123; /** * 销售 * @param money */ public void saleProduct(float money)&#123; System.out.println(&quot;销售产品，并拿到钱：&quot;+money); &#125; /** * 售后 * @param money */ public void afterService(float money)&#123; System.out.println(&quot;提供售后服务，并拿到钱：&quot;+money); &#125;&#125;-------------------------------------------------------------------/** * 对生产厂家要求的接口 */public interface IProducer &#123; /** * 销售 * @param money */ public void saleProduct(float money); /** * 售后 * @param money */ public void afterService(float money);&#125; 1.2 基于子类的动态代理123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657/** * @author yinhuidong * @createTime 2020-03-02-1:08 */public class Test4 &#123; @Test public void test() &#123; final Producer producer = new Producer(); /** * 动态代理： * 特点：字节码随用随创建，随用随加载 * 作用：不修改源码的基础上对方法增强 * 分类： * 基于接口的动态代理 * 基于子类的动态代理 * 基于子类的动态代理： * 涉及的类：Enhancer * 提供者：第方cglib库 * 如何创建代理对象： * 使用Enhancer类中的create方法 * 创建代理对象的要求： * 被代理类是最终类 * create方法的参数： * Class：字节码 * 它是用于指定被代理对象的字节码。 * * Callback：用于提供增强的代码 * 它是让我们写如何代理。我们一般都是些一个该接口的实现类，通常情况下都是匿名内部类，但不是必须的。 * 此接口的实现类都是谁用谁写。 * 我们一般写的都是该接口的子接口实现类：MethodInterceptor */ Producer cglibProducer = (Producer) Enhancer.create(producer.getClass(), new MethodInterceptor() &#123; /** * 执行被代理对象的任何方法都会经过该方法 * @param proxy * @param method * @param args * 以上个参数和基于接口的动态代理中invoke方法的参数是一样的 * @param methodProxy ：当前执行方法的代理对象 * @return * @throws Throwable */ public Object intercept(Object proxy, Method method, Object[] args, MethodProxy methodProxy) throws Throwable &#123; //提供增强的代码 Object returnValue = null; //1.获取方法执行的参数 Float money = (Float) args[0]; //2.判断当前方法是不是销售 if (&quot;saleProduct&quot;.equals(method.getName())) &#123; returnValue = method.invoke(producer, money * 0.8f); &#125; return returnValue; &#125; &#125;); cglibProducer.saleProduct(12000f); &#125;&#125; 1234567891011121314151617181920212223package com.es.java2;/** * 一个生产者 */public class Producer &#123; /** * 销售 * @param money */ public void saleProduct(float money)&#123; System.out.println(&quot;销售产品，并拿到钱：&quot;+money); &#125; /** * 售后 * @param money */ public void afterService(float money)&#123; System.out.println(&quot;提供售后服务，并拿到钱：&quot;+money); &#125;&#125; 1.3 使用动态代理对spring进行方法增强接口和实现类 12345678910111213141516171819202122public interface MyInterface &#123; public int add(int a,int b); public int del(int a,int b); public int che(int a,int b); public int div(int a,int b);&#125;----------------------------------------------------------public class java1 implements MyInterface&#123; public int add(int a,int b)&#123; return a+b; &#125; public int del(int a,int b)&#123; return a-b; &#125; public int che(int a,int b)&#123; return a*b; &#125; public int div(int a,int b)&#123; return a/b; &#125;&#125; BeanFactory 123456789101112131415161718192021222324public class BeanFactory &#123; private java1 java; public void setJava(java1 java) &#123; this.java = java; &#125; public MyInterface getBean()&#123; MyInterface proxyJava = (MyInterface) Proxy.newProxyInstance( java.getClass().getClassLoader(), java.getClass().getInterfaces(), new InvocationHandler() &#123; public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; Object value = null; System.out.println(&quot;方法执行前....&quot;); value = method.invoke(java, args); System.out.println(&quot;方法执行之后....&quot;); return value; &#125; &#125; ); return proxyJava; &#125;&#125; 测试类 1234567891011121314151617181920212223@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(&quot;classpath:applicationContext.xml&quot;)public class Test1 &#123; @Autowired @Qualifier(&quot;proxyJava&quot;) private MyInterface myInterface; @Test public void test1()&#123; System.out.println(myInterface.add(1, 2)); &#125; @Test public void test2()&#123; System.out.println(myInterface.del(1, 2)); &#125; @Test public void test3()&#123; System.out.println(myInterface.che(1, 2)); &#125; @Test public void test4()&#123; System.out.println(myInterface.div(1, 2)); &#125;&#125; 配置文件 1234567&lt;bean id=&quot;factory&quot; class=&quot;com.es.factory.BeanFactory&quot;&gt; &lt;property name=&quot;java&quot; ref=&quot;java&quot;&gt;&lt;/property&gt;&lt;/bean&gt;&lt;bean id=&quot;java&quot; class=&quot;com.es.java1.java1&quot;&gt;&lt;/bean&gt;&lt;bean id=&quot;proxyJava&quot; factory-bean=&quot;factory&quot; factory-method=&quot;getBean&quot;&gt;&lt;/bean&gt; 2.AOP2.1 AOP相关概念AOP：全称是 Aspect Oriented Programming 即：面向切面编程。就是把我们程序重复的代码抽取出来，在需要执行的时候，使用动态代理的技术，在不修改源码的基础上，对我们的已有方法进行增强。 作用： 在程序运行期间，不修改源码对已有方法进行增强。 优势： 减少重复代码 提高开发效率 维护方便 AOP 相关术语 Joinpoint(连接点): 所谓连接点是指那些被拦截到的点。在 spring 中,这些点指的是方法,因为 spring 只支持方法类型的连接点。 Pointcut(切入点): 所谓切入点是指我们要对哪些 Joinpoint 进行拦截的定义。 Advice(通知/增强): 所谓通知是指拦截到 Joinpoint 之后所要做的事情就是通知。 通知的类型：前置通知,后置通知,异常通知,最终通知,环绕通知。 Introduction(引介): 引介是一种特殊的通知在不修改类代码的前提下, Introduction 可以在运行期为类动态地添加一些方法或 Field。 Target(目标对象):被代理对象 代理的目标对象。 Weaving(织入): 是指把增强应用到目标对象来创建新的代理对象的过程。 spring 采用动态代理织入，而 AspectJ 采用编译期织入和类装载期织入。 Proxy（代理: 一个类被 AOP 织入增强后，就产生一个结果代理类。 Aspect(切面): 是切入点和通知（引介的结合 ​ Spring 框架监控切入点方法的执行。一旦监控到切入点方法被运行，使用代理机制，动态创建目标对象的代理对象，根据通知类别，在代理对象的对应位置，将通知对应的功能织入，完成完整的代码逻辑运行。​ 2.2 基于xml形式的配置2.2.1 配置步骤12345678910111213141516171819202122232425262728293031323334353637383940&lt;!--spring中基于XML的AOP配置步骤 1、把通知Bean也交给spring来管理 2、使用aop:config标签表明开始AOP的配置 3、使用aop:aspect标签表明配置切面 id属性：是给切面提供一个唯一标识 ref属性：是指定通知类bean的Id。 4、在aop:aspect标签的内部使用对应标签来配置通知的类型 我们现在示例是让printLog方法在切入点方法执行之前之前：所以是前置通知 aop:before：表示配置前置通知 method属性：用于指定Logger类中哪个方法是前置通知 pointcut属性：用于指定切入点表达式，该表达式的含义指的是对业务层中哪些方法增强 切入点表达式的写法： 关键字：execution(表达式) 表达式： 访问修饰符 返回值 包名.包名.包名...类名.方法名(参数列表) 标准的表达式写法： public void com.itheima.service.impl.AccountServiceImpl.saveAccount() 访问修饰符可以省略 void com.itheima.service.impl.AccountServiceImpl.saveAccount() 返回值可以使用通配符，表示任意返回值 * com.itheima.service.impl.AccountServiceImpl.saveAccount() 包名可以使用通配符，表示任意包。但是几级包，就需要写几个*. * *.*.*.*.AccountServiceImpl.saveAccount()) 包名可以使用..表示当前包及其子包 * *..AccountServiceImpl.saveAccount() 类名和方法名都可以使用*来实现通配 * *..*.*() 参数列表： 可以直接写数据类型： 基本类型直接写名称 int 引用类型写包名.类名的方式 java.lang.String 可以使用通配符表示任意类型，但是必须参数 可以使用..表示无参数均可，参数可以是任意类型 全通配写法： * *..*.*(..) 实际开发中切入点表达式的通常写法： 切到业务层实现类下的所方法 * com.itheima.service.impl.*.*(..)--&gt; 配置文件 1234567891011121314151617181920212223242526&lt;!--配置bean--&gt;&lt;bean id=&quot;java&quot; class=&quot;com.es.java1.java1&quot;&gt;&lt;/bean&gt;&lt;bean id=&quot;logging&quot; class=&quot;com.es.java2.Logging&quot;&gt;&lt;/bean&gt;&lt;!-- 配置aop --&gt;&lt;aop:config&gt; &lt;!-- 配置切入点表达式 id属性用于指定表达式的唯一标识。expression属性用于指定表达式内容 此标签写在aop:aspect标签内部只能当前切面使用。 它还可以写在aop:aspect外面，此时就变成了所切面可用 --&gt; &lt;aop:pointcut id=&quot;a&quot; expression=&quot;execution(public int com.es.java1.java1.*(int,int))&quot;&gt;&lt;/aop:pointcut&gt; &lt;!--配置切面--&gt; &lt;aop:aspect id=&quot;log&quot; ref=&quot;logging&quot;&gt; &lt;!-- 配置通知的类型，并且建立通知方法和切入点方法的关联--&gt; &lt;!-- 配置前置通知：在切入点方法执行之前执行 --&gt; &lt;aop:before method=&quot;before&quot; pointcut=&quot;execution(public int com.es.java1.java1.*(int,int))&quot;&gt;&lt;/aop:before&gt; &lt;!-- 配置后置通知：在切入点方法正常执行之后值。它和异常通知永远只能执行一个--&gt; &lt;aop:after-returning method=&quot;afterReturn&quot; pointcut-ref=&quot;a&quot;&gt;&lt;/aop:after-returning&gt; &lt;!-- 配置异常通知：在切入点方法执行产生异常之后执行。它和后置通知永远只能执行一个--&gt; &lt;aop:after-throwing method=&quot;afterThrowing&quot; pointcut-ref=&quot;a&quot;&gt;&lt;/aop:after-throwing&gt; &lt;!-- 配置最终通知：无论切入点方法是否正常执行它都会在其后面执行 --&gt; &lt;aop:after method=&quot;after&quot; pointcut-ref=&quot;a&quot;&gt;&lt;/aop:after&gt; &lt;!-- 配置环绕通知 详细的注释 :Logger类中--&gt; &lt;aop:around method=&quot;around&quot; pointcut-ref=&quot;a&quot;&gt;&lt;/aop:around&gt; &lt;/aop:aspect&gt;&lt;/aop:config&gt; 实现类 1234567891011121314public class java1 implements MyInterface&#123; public int add(int a,int b)&#123; return a+b; &#125; public int del(int a,int b)&#123; return a-b; &#125; public int che(int a,int b)&#123; return a*b; &#125; public int div(int a,int b)&#123; return a/b; &#125;&#125; 测试类 1234567891011@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(locations = &quot;classpath:applicationContext.xml&quot;)public class Test1 &#123; @Autowired @Qualifier(&quot;java&quot;) private MyInterface java; @Test public void test1()&#123; java.div(1, 1); &#125;&#125; 日志类 123456789101112131415161718192021222324252627282930313233343536373839404142434445public class Logging &#123; public void before()&#123; System.out.println(&quot;★★★前置★★★&quot;); &#125; public void afterReturn()&#123; System.out.println(&quot;★★★后置★★★&quot;); &#125; public void afterThrowing()&#123; System.out.println(&quot;★★★异常★★★&quot;); &#125; public void after()&#123; System.out.println(&quot;★★★最终★★★&quot;); &#125; /** * 环绕通知 * 问题： * 当我们配置了环绕通知之后，切入点方法没执行，而通知方法执行了。 * 分析： * 通过对比动态代理中的环绕通知代码，发现动态代理的环绕通知明确的切入点方法调用，而我们的代码中没。 * 解决： * Spring框架为我们提供了一个接口：ProceedingJoinPoint。该接口一个方法proceed()，此方法就相当于明确调用切入点方法。 * 该接口可以作为环绕通知的方法参数，在程序执行时，spring框架会为我们提供该接口的实现类供我们使用。 * * spring中的环绕通知： * 它是spring框架为我们提供的一种可以在代码中手动控制增强方法何时执行的方式。 */ public Object around(ProceedingJoinPoint pjp)&#123; Object value=null; //获取方法执行的参数 Object []args=pjp.getArgs(); //获取方法名 String methodName = pjp.getSignature().getName(); try &#123; System.out.println(&quot;★★★前置★★★&quot;+methodName+&quot;...&quot;+Arrays.toString(args)); value=pjp.proceed(args); System.out.println(&quot;★★★后置★★★&quot;+methodName+&quot;...&quot;+Arrays.toString(args)+&quot;...&quot;+value); return value; &#125; catch (Throwable throwable) &#123; System.out.println(&quot;★★★异常★★★&quot;+methodName+&quot;...&quot;+Arrays.toString(args)); throw new RuntimeException(throwable); &#125;finally &#123; System.out.println(&quot;★★★最终★★★&quot;+methodName+&quot;...&quot;+Arrays.toString(args)+&quot;...&quot;+value); &#125; &#125;&#125; 2.3 基于注解的配置2.3.1 步骤 首先在配置文件里开启声明式aop注解支持 12&lt;!--开启声明式事务注解--&gt;&lt;aop:aspectj-autoproxy&gt;&lt;/aop:aspectj-autoproxy&gt; 在logging类上声明其为一个切面类 1@Aspect//声明一个切面 在类中声明一个方法作为切入点表达式 1234@Pointcut(&quot;execution(public * com.es.java1.java1.*(..))&quot;)public void pointcut()&#123;&#125; 在各个方法上添加注解 设置切面优先级 1@Order(2)//通过@Order(2)注解指定切面优先级，value值越小，优先级越高，默认是int最大值。 2.3.2 案例 配置文件 1234 &lt;!--开启声明式事务注解--&gt;&lt;aop:aspectj-autoproxy&gt;&lt;/aop:aspectj-autoproxy&gt; &lt;!--设置自动扫描的包--&gt;&lt;context:component-scan base-package=&quot;com.es&quot;&gt;&lt;/context:component-scan&gt; 日志类 1234567891011121314151617181920212223242526272829303132333435363738394041package com.es.utils;import org.aspectj.lang.JoinPoint;import org.aspectj.lang.ProceedingJoinPoint;import org.aspectj.lang.annotation.*;import org.springframework.stereotype.Component;import java.util.Arrays;/** * @author yinhuidong * @createTime 2020-03-04-16:37 */@Component(&quot;logging&quot;)@Aspectpublic class Logging &#123; @Before(value=&quot;execution(* com.es.java1.*.*(..))&quot;) public void before(JoinPoint jp)&#123; Object[] args = jp.getArgs(); String name = jp.getSignature().getName(); System.out.println(&quot;before...&quot;+name+&quot;...&quot;+Arrays.toString(args)); &#125; @AfterReturning(value=&quot;execution(* com.es.java1.*.*(..))&quot;,returning = &quot;result&quot;) public void afterReturning(JoinPoint jp,Object result)&#123; Object[] args = jp.getArgs(); String name = jp.getSignature().getName(); System.out.println(&quot;afterReturning...&quot;+name+&quot;...&quot;+Arrays.toString(args)+&quot;...&quot;+result); &#125; @AfterThrowing(value=&quot;execution(* com.es.java1.*.*(..))&quot;,throwing = &quot;e&quot;) public void afterThrowing(JoinPoint jp,Exception e)&#123; Object[] args = jp.getArgs(); String name = jp.getSignature().getName(); System.out.println(&quot;afterThrowing...&quot;+name+&quot;...&quot;+Arrays.toString(args)+&quot;...&quot;+e); &#125; @After(&quot;execution(* com.es.java1.*.*(..))&quot;) public void after(JoinPoint jp)&#123; Object[] args = jp.getArgs(); String name = jp.getSignature().getName(); System.out.println(&quot;after...&quot;+name+&quot;...&quot;+Arrays.toString(args)); &#125;&#125; 实现类 123456789@Component(&quot;add&quot;)public class Add &#123; public void add(int a ,int b)&#123; System.out.println(a+b); &#125; public int del(int a, int b)&#123; return a-b; &#125;&#125; 测试类 1234567891011@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(locations = &quot;classpath:applicationContext.xml&quot;)public class Test1 &#123; @Autowired private Add add; @Test public void test1()&#123; add.add(1,1); System.out.println(add.del(1, 1)); &#125;&#125; 补充： 12345678910//配置通用的切入点表达式@Pointcut(value = &quot;execution(* com.es.dao.impl.ComputerDaoImpl.*(..))&quot;)public void pointCut()&#123;&#125;//引用通用的切入点表达式@Before(&quot;pointCut()&quot;)public void before(JoinPoint point)&#123; System.out.println(&quot;前置通知--&gt;&quot;+&quot;方法名：&quot;+point.getSignature().getName()+&quot;参数列表：&quot;+ Arrays.asList(point.getArgs()));&#125; 六，spring的事务1.基于AOP的事务处理模拟1.1 步骤 创建一个链接工具类，负责从线程获取连接，并实现与线程的绑定。 创建和事务管理相关的工具类，负责处理事务操作 利用aop实现事务处理 ​ 1.2 案例配置文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:jdbc=&quot;http://www.springframework.org/schema/jdbc&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/jdbc http://www.springframework.org/schema/jdbc/spring-jdbc-4.2.xsd http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.2.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-4.2.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-4.2.xsd&quot;&gt; &lt;!--配置dao--&gt; &lt;bean id=&quot;accountDao&quot; class=&quot;com.es.dao.impl.AccountDaoImpl&quot;&gt; &lt;property name=&quot;runner&quot; ref=&quot;runner&quot;&gt;&lt;/property&gt; &lt;property name=&quot;connectionUtils&quot; ref=&quot;connection&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!--配置service--&gt; &lt;bean id=&quot;accountService&quot; class=&quot;com.es.service.impl.AccountServiceImpl&quot;&gt; &lt;property name=&quot;accountDao&quot; ref=&quot;accountDao&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!--配置queryRunner--&gt; &lt;bean id=&quot;runner&quot; class=&quot;org.apache.commons.dbutils.QueryRunner&quot;&gt; &lt;!--&lt;constructor-arg name=&quot;ds&quot; ref=&quot;dataSource&quot;&gt;&lt;/constructor-arg&gt;--&gt; &lt;/bean&gt; &lt;!--配置数据源--&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot;&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;com.mysql.jdbc.Driver&quot;&gt;&lt;/property&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:3306/eesy&quot;&gt;&lt;/property&gt; &lt;property name=&quot;password&quot; value=&quot;yhd666&quot;&gt;&lt;/property&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!--配置transactionManager--&gt; &lt;bean id=&quot;transactionManager&quot; class=&quot;com.es.utils.TransactionManager&quot;&gt; &lt;property name=&quot;connectionUtils&quot; ref=&quot;connection&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!--配置ConnectionUtils--&gt; &lt;bean id=&quot;connection&quot; class=&quot;com.es.utils.ConnectionUtils&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!--配置aop--&gt; &lt;aop:config&gt; &lt;!--配置切入点表达式--&gt; &lt;aop:pointcut id=&quot;txAdvice&quot; expression=&quot;execution(* com.es.service.impl.*.*(..))&quot;&gt;&lt;/aop:pointcut&gt; &lt;aop:aspect id=&quot;txA&quot; ref=&quot;transactionManager&quot;&gt; &lt;aop:before method=&quot;beginTransaction&quot; pointcut-ref=&quot;txAdvice&quot;&gt;&lt;/aop:before&gt; &lt;aop:after-returning method=&quot;commit&quot; pointcut-ref=&quot;txAdvice&quot;&gt;&lt;/aop:after-returning&gt; &lt;aop:after-throwing method=&quot;rollback&quot; pointcut-ref=&quot;txAdvice&quot;&gt;&lt;/aop:after-throwing&gt; &lt;aop:after method=&quot;release&quot; pointcut-ref=&quot;txAdvice&quot;&gt;&lt;/aop:after&gt; &lt;/aop:aspect&gt; &lt;/aop:config&gt;&lt;/beans&gt; 持久层 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293package com.es.dao.impl;import com.es.dao.IAccountDao;import com.es.domain.Account;import com.es.utils.ConnectionUtils;import org.apache.commons.dbutils.QueryRunner;import org.apache.commons.dbutils.handlers.BeanHandler;import org.apache.commons.dbutils.handlers.BeanListHandler;import org.springframework.jdbc.core.BeanPropertyRowMapper;import org.springframework.jdbc.core.support.JdbcDaoSupport;import org.springframework.stereotype.Repository;import java.util.List;/** * 账户的持久层实现类 */public class AccountDaoImpl implements IAccountDao &#123; private QueryRunner runner; private ConnectionUtils connectionUtils; public void setRunner(QueryRunner runner) &#123; this.runner = runner; &#125; public void setConnectionUtils(ConnectionUtils connectionUtils) &#123; this.connectionUtils = connectionUtils; &#125; public List&lt;Account&gt; findAllAccount() &#123; try&#123; return runner.query(connectionUtils.getThreadConnection(),&quot;select * from account&quot;,new BeanListHandler&lt;Account&gt;(Account.class)); &#125;catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; public Account findAccountById(Integer accountId) &#123; try&#123; return runner.query(connectionUtils.getThreadConnection(),&quot;select * from account where id = ? &quot;,new BeanHandler&lt;Account&gt;(Account.class),accountId); &#125;catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; public void saveAccount(Account account) &#123; try&#123; runner.update(connectionUtils.getThreadConnection(),&quot;insert into account(name,money)values(?,?)&quot;,account.getName(),account.getMoney()); &#125;catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; public void updateAccount(Account account) &#123; try&#123; runner.update(connectionUtils.getThreadConnection(),&quot;update account set name=?,money=? where id=?&quot;,account.getName(),account.getMoney(),account.getId()); &#125;catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; public void deleteAccount(Integer accountId) &#123; try&#123; runner.update(connectionUtils.getThreadConnection(),&quot;delete from account where id=?&quot;,accountId); &#125;catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; public Account findAccountByName(String accountName) &#123; try&#123; List&lt;Account&gt; accounts = runner.query(connectionUtils.getThreadConnection(),&quot;select * from account where name = ? &quot;,new BeanListHandler&lt;Account&gt;(Account.class),accountName); if(accounts == null || accounts.size() == 0)&#123; return null; &#125; if(accounts.size() &gt; 1)&#123; throw new RuntimeException(&quot;结果集不唯一，数据有问题&quot;); &#125; return accounts.get(0); &#125;catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125;&#125; 服务层 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package com.es.service.impl;import com.es.dao.IAccountDao;import com.es.domain.Account;import com.es.service.IAccountService;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;/** * 账户的业务层实现类 * * 事务控制应该都是在业务层 */public class AccountServiceImpl implements IAccountService&#123; private IAccountDao accountDao; public void setAccountDao(IAccountDao accountDao) &#123; this.accountDao = accountDao; &#125; public Account findAccountById(Integer accountId) &#123; return accountDao.findAccountById(accountId); &#125; public void transfer(String sourceName, String targetName, Float money) &#123; System.out.println(&quot;transfer....&quot;); //2.1根据名称查询转出账户 Account source = accountDao.findAccountByName(sourceName); //2.2根据名称查询转入账户 Account target = accountDao.findAccountByName(targetName); //2.3转出账户减钱 source.setMoney(source.getMoney()-money); //2.4转入账户加钱 target.setMoney(target.getMoney()+money); //2.5更新转出账户 accountDao.updateAccount(source); int i=1/0; //2.6更新转入账户 accountDao.updateAccount(target); &#125;&#125; 连接工具类 12345678910111213141516171819202122232425262728293031323334353637383940414243444546package com.es.utils;import javax.sql.DataSource;import java.sql.Connection;/** * 连接的工具类，它用于从数据源中获取一个连接，并且实现和线程的绑定 */public class ConnectionUtils &#123; private ThreadLocal&lt;Connection&gt; tl = new ThreadLocal&lt;Connection&gt;(); private DataSource dataSource; public void setDataSource(DataSource dataSource) &#123; this.dataSource = dataSource; &#125; /** * 获取当前线程上的连接 * @return */ public Connection getThreadConnection() &#123; try&#123; //1.先从ThreadLocal上获取 Connection conn = tl.get(); //2.判断当前线程上是否连接 if (conn == null) &#123; //3.从数据源中获取一个连接，并且存入ThreadLocal中 conn = dataSource.getConnection(); tl.set(conn); &#125; //4.返回当前线程上的连接 return conn; &#125;catch (Exception e)&#123; throw new RuntimeException(e); &#125; &#125; /** * 把连接和线程解绑 */ public void removeConnection()&#123; tl.remove(); &#125;&#125; 事务处理类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162package com.es.utils;import org.springframework.stereotype.Component;/** * 和事务管理相关的工具类，它包含了，开启事务，提交事务，回滚事务和释放连接 */public class TransactionManager &#123; private ConnectionUtils connectionUtils; public void setConnectionUtils(ConnectionUtils connectionUtils) &#123; this.connectionUtils = connectionUtils; &#125; /** * 开启事务 */ public void beginTransaction()&#123; try &#123; connectionUtils.getThreadConnection().setAutoCommit(false); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; &#125; /** * 提交事务 */ public void commit()&#123; try &#123; connectionUtils.getThreadConnection().commit(); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; &#125; /** * 回滚事务 */ public void rollback()&#123; try &#123; connectionUtils.getThreadConnection().rollback(); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; &#125; /** * 释放连接 */ public void release()&#123; try &#123; connectionUtils.getThreadConnection().close();//还回连接池中 connectionUtils.removeConnection(); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; &#125;&#125; 2.spring的事务配置2.1 spring中基于xml的事务配置2.2.1 配置步骤spring中基于XML的声明式事务控制配置步骤​ 配置事务管理器 配置事务的通知 此时我们需要导入事务的约束 tx名称空间和约束，同时也需要aop的 使用tx:advice标签配置事务通知 属性： id：给事务通知起一个唯一标识 transaction-manager：给事务通知提供一个事务管理器引用 配置AOP中的通用切入点表达式 建立事务通知和切入点表达式的对应关系 配置事务的属性 是在事务的通知tx:advice标签的内部 ​ 配置事务的属性​ isolation：用于指定事务的隔离级别。默认值是DEFAULT，表示使用数据库的默认隔离级别。 propagation：用于指定事务的传播行为。默认值是REQUIRED，表示一定会有事务，增删改的选择。查询方法可以选择SUPPORTS。 read-only：用于指定事务是否只读。只查询方法才能设置为true。默认值是false，表示读写。 timeout：用于指定事务的超时时间，默认值是-1，表示永不超时。如果指定了数值，以秒为单位。 rollback-for：用于指定一个异常，当产生该异常时，事务回滚，产生其他异常时，事务不回滚。没有默认值。表示任何异常都回滚。 no-rollback-for：用于指定一个异常，当产生该异常时，事务不回滚，产生其他异常时事务回滚。没有默认值。表示任何异常都回滚。 ​ 2.2.2 案例配置文件 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:jdbc=&quot;http://www.springframework.org/schema/jdbc&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/jdbc http://www.springframework.org/schema/jdbc/spring-jdbc-4.2.xsd http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.2.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-4.2.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-4.2.xsd&quot;&gt; &lt;context:component-scan base-package=&quot;com.es&quot;&gt;&lt;/context:component-scan&gt; &lt;bean id=&quot;accountDao&quot; class=&quot;com.es.dao.impl.AccountDaoImpl&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;bean id=&quot;account&quot; class=&quot;com.es.domain.Account&quot;&gt;&lt;/bean&gt; &lt;bean id=&quot;accountService&quot; class=&quot;com.es.service.impl.AccountServiceImpl&quot;&gt; &lt;property name=&quot;accountDao&quot; ref=&quot;accountDao&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot;&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot;&gt;&lt;/property&gt; &lt;property name=&quot;password&quot; value=&quot;yhd666&quot;&gt;&lt;/property&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql:///eesy&quot;&gt;&lt;/property&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;com.mysql.jdbc.Driver&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- spring中基于XML的声明式事务控制配置步骤 1、配置事务管理器 2、配置事务的通知 此时我们需要导入事务的约束 tx名称空间和约束，同时也需要aop的 使用tx:advice标签配置事务通知 属性： id：给事务通知起一个唯一标识 transaction-manager：给事务通知提供一个事务管理器引用 3、配置AOP中的通用切入点表达式 4、建立事务通知和切入点表达式的对应关系 5、配置事务的属性 是在事务的通知tx:advice标签的内部 --&gt; &lt;!--配置事务管理器--&gt; &lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!--配置事务的通知--&gt; &lt;tx:advice id=&quot;txAdvice&quot; transaction-manager=&quot;transactionManager&quot;&gt; &lt;!-- 配置事务的属性 isolation：用于指定事务的隔离级别。默认值是DEFAULT，表示使用数据库的默认隔离级别。 propagation：用于指定事务的传播行为。默认值是REQUIRED，表示一定会有事务，增删改的选择。查询方法可以选择SUPPORTS。 read-only：用于指定事务是否只读。只查询方法才能设置为true。默认值是false，表示读写。 timeout：用于指定事务的超时时间，默认值是-1，表示永不超时。如果指定了数值，以秒为单位。 rollback-for：用于指定一个异常，当产生该异常时，事务回滚，产生其他异常时，事务不回滚。没有默认值。表示任何异常都回滚。 no-rollback-for：用于指定一个异常，当产生该异常时，事务不回滚，产生其他异常时事务回滚。没有默认值。表示任何异常都回滚。 --&gt; &lt;tx:attributes&gt; &lt;tx:method name=&quot;*&quot; isolation=&quot;DEFAULT&quot; propagation=&quot;REQUIRED&quot; timeout=&quot;-1&quot; read-only=&quot;false&quot;/&gt; &lt;tx:method name=&quot;find*&quot; isolation=&quot;DEFAULT&quot; propagation=&quot;SUPPORTS&quot; read-only=&quot;true&quot; timeout=&quot;-1&quot;/&gt; &lt;/tx:attributes&gt; &lt;/tx:advice&gt; &lt;aop:config&gt; &lt;aop:pointcut id=&quot;pointCut&quot; expression=&quot;execution(* com.es.service.impl.*.*(..))&quot;&gt;&lt;/aop:pointcut&gt; &lt;aop:advisor advice-ref=&quot;txAdvice&quot; pointcut-ref=&quot;pointCut&quot;&gt;&lt;/aop:advisor&gt; &lt;/aop:config&gt;&lt;/beans&gt; 持久层 12345678910111213141516171819202122232425262728293031323334353637package com.es.dao.impl;import com.es.dao.IAccountDao;import com.es.domain.Account;import org.springframework.jdbc.core.BeanPropertyRowMapper;import org.springframework.jdbc.core.support.JdbcDaoSupport;import java.util.List;/** * 账户的持久层实现类 */public class AccountDaoImpl extends JdbcDaoSupport implements IAccountDao &#123; public Account findAccountById(Integer accountId) &#123; List&lt;Account&gt; accounts = super.getJdbcTemplate().query(&quot;select * from account where id = ?&quot;,new BeanPropertyRowMapper&lt;Account&gt;(Account.class),accountId); return accounts.isEmpty()?null:accounts.get(0); &#125; public Account findAccountByName(String accountName) &#123; List&lt;Account&gt; accounts = super.getJdbcTemplate().query(&quot;select * from account where name = ?&quot;,new BeanPropertyRowMapper&lt;Account&gt;(Account.class),accountName); if(accounts.isEmpty())&#123; return null; &#125; if(accounts.size()&gt;1)&#123; throw new RuntimeException(&quot;结果集不唯一&quot;); &#125; return accounts.get(0); &#125; public void updateAccount(Account account) &#123; super.getJdbcTemplate().update(&quot;update account set name=?,money=? where id=?&quot;,account.getName(),account.getMoney(),account.getId()); &#125;&#125; 服务层 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package com.es.service.impl;import com.es.dao.IAccountDao;import com.es.domain.Account;import com.es.service.IAccountService;/** * 账户的业务层实现类 * * 事务控制应该都是在业务层 */public class AccountServiceImpl implements IAccountService&#123; private IAccountDao accountDao; public void setAccountDao(IAccountDao accountDao) &#123; this.accountDao = accountDao; &#125; public Account findAccountById(Integer accountId) &#123; return accountDao.findAccountById(accountId); &#125; public void transfer(String sourceName, String targetName, Float money) &#123; System.out.println(&quot;transfer....&quot;); //2.1根据名称查询转出账户 Account source = accountDao.findAccountByName(sourceName); //2.2根据名称查询转入账户 Account target = accountDao.findAccountByName(targetName); //2.3转出账户减钱 source.setMoney(source.getMoney()-money); //2.4转入账户加钱 target.setMoney(target.getMoney()+money); //2.5更新转出账户 accountDao.updateAccount(source); // int i=1/0; //2.6更新转入账户 accountDao.updateAccount(target); &#125;&#125; 测试类 123456789101112131415161718192021import com.es.service.IAccountService;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.test.context.ContextConfiguration;import org.springframework.test.context.junit4.SpringJUnit4ClassRunner;/** * @author yinhuidong * @createTime 2020-03-03-12:03 */@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(locations = &quot;classpath:applicationContext.xml&quot;)public class Test1 &#123; @Autowired private IAccountService service; @Test public void test1()&#123; service.transfer(&quot;aaa&quot;,&quot;bbb&quot;,1000f); &#125;&#125; 2.2 spring中基于注解的事务配置2.2.1 配置步骤 在配置文件配置事务管理器 开始声明式事务的支持 在对应的方法上加@Transcational注解，事务声明注解:该注解可以添加到类或者方法上 属性： propagation:用来设置事务的传播行为：一个方法运行在一个开启了事务的方法中，当前方法是使用原有的事务还是开启新事物 required:如果有事务就使用，没有就开启一个新的（默认） required_new:必须开启新事物 supports：如果有事务就运行，否则也不开启新的事务 no_supports:即使有事务也不用​ 2.2.2 案例配置文件 123456789101112131415161718192021222324252627282930313233343536&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:jdbc=&quot;http://www.springframework.org/schema/jdbc&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/jdbc http://www.springframework.org/schema/jdbc/spring-jdbc-4.2.xsd http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.2.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-4.2.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-4.2.xsd&quot;&gt; &lt;bean id=&quot;jdbcTemplate&quot; class=&quot;org.springframework.jdbc.core.JdbcTemplate&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;context:property-placeholder location=&quot;classpath:druid.properties&quot;&gt;&lt;/context:property-placeholder&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot;&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;$&#123;driverClassName&#125;&quot;&gt;&lt;/property&gt; &lt;property name=&quot;url&quot; value=&quot;$&#123;url&#125;&quot;&gt;&lt;/property&gt; &lt;property name=&quot;username&quot; value=&quot;$&#123;jdbc.username&#125;&quot;&gt;&lt;/property&gt; &lt;property name=&quot;password&quot; value=&quot;$&#123;jdbc.password&#125;&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- spring中基于注解 的声明式事务控制配置步骤 1、配置事务管理器 2、开启spring对注解事务的支持 3、在需要事务支持的地方使用@Transactional注解 --&gt; &lt;!-- 配置事务管理器 --&gt; &lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 开启spring对注解事务的支持--&gt; &lt;tx:annotation-driven&gt;&lt;/tx:annotation-driven&gt; &lt;!--组件扫描--&gt; &lt;context:component-scan base-package=&quot;com.es&quot;&gt;&lt;/context:component-scan&gt;&lt;/beans&gt; 持久层 1234567891011121314151617181920212223242526272829/** * 账户的持久层实现类 */@Repository(&quot;accountDao&quot;)public class AccountDaoImpl implements IAccountDao &#123; @Autowired private JdbcTemplate jdbcTemplate; public Account findAccountById(Integer accountId) &#123; List&lt;Account&gt; accounts = jdbcTemplate.query(&quot;select * from account where id = ?&quot;,new BeanPropertyRowMapper&lt;Account&gt;(Account.class),accountId); return accounts.isEmpty()?null:accounts.get(0); &#125; public Account findAccountByName(String accountName) &#123; List&lt;Account&gt; accounts = jdbcTemplate.query(&quot;select * from account where name = ?&quot;,new BeanPropertyRowMapper&lt;Account&gt;(Account.class),accountName); if(accounts.isEmpty())&#123; return null; &#125; if(accounts.size()&gt;1)&#123; throw new RuntimeException(&quot;结果集不唯一&quot;); &#125; return accounts.get(0); &#125; public void updateAccount(Account account) &#123; jdbcTemplate.update(&quot;update account set name=?,money=? where id=?&quot;,account.getName(),account.getMoney(),account.getId()); &#125;&#125; 服务层 12345678910111213141516171819202122232425262728293031323334353637383940414243/** * 账户的业务层实现类 * * 事务控制应该都是在业务层 */@Service(&quot;accountService&quot;)@Transactional(readOnly = false,propagation = Propagation.SUPPORTS)public class AccountServiceImpl implements IAccountService&#123; @Autowired private IAccountDao accountDao; public void setAccountDao(IAccountDao accountDao) &#123; this.accountDao = accountDao; &#125; public Account findAccountById(Integer accountId) &#123; return accountDao.findAccountById(accountId); &#125; @Transactional(readOnly = false,propagation = Propagation.REQUIRED) public void transfer(String sourceName, String targetName, Float money) &#123; System.out.println(&quot;transfer....&quot;); //2.1根据名称查询转出账户 Account source = accountDao.findAccountByName(sourceName); //2.2根据名称查询转入账户 Account target = accountDao.findAccountByName(targetName); //2.3转出账户减钱 source.setMoney(source.getMoney()-money); //2.4转入账户加钱 target.setMoney(target.getMoney()+money); //2.5更新转出账户 accountDao.updateAccount(source); int i=1/0; //2.6更新转入账户 accountDao.updateAccount(target); &#125;&#125; 测试类​ 12345678910@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(locations = &quot;classpath:applicationContext.xml&quot;)public class Test1 &#123; @Autowired private IAccountService service; @Test public void test1()&#123; service.transfer(&quot;aaa&quot;,&quot;bbb&quot;,1000f); &#125;&#125;","categories":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/tags/Spring/"}]},{"title":"Spring[十五]事务增强器源码分析","slug":"Spring/Spring[十五]事务增强器源码分析","date":"2022-01-11T06:11:10.808Z","updated":"2022-01-11T06:18:09.756Z","comments":true,"path":"2022/01/11/Spring/Spring[十五]事务增强器源码分析/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/Spring/Spring[%E5%8D%81%E4%BA%94]%E4%BA%8B%E5%8A%A1%E5%A2%9E%E5%BC%BA%E5%99%A8%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","excerpt":"","text":"在上一篇回顾了一下传统的JDBC事务，并分析了Spring解析事务标签，创建事务代理对象的源码，本篇我们来分析在目标方法执行的前后，事务增强器是如何工作的。 1.相关接口 在分析事务增强器的源码之前，先来过一些接口，明确接口的作用和接口之间的关系。 java jdbc规范接口：DataSource、 Connection 事务属性承载对象：TransactionAttribute 事务管理器：TransactionManager、PlatformTransactionManager 数据库链接holder：ConnectionHolder Spring抽象出来的事务对象：JdbcTransactionObjectSupport、DataSourceTransactionObject Spring抽象出来的事务状态（包装“事务对象”，做了一些增强）：AbstractTransactionStatus、DefaultTransactionStatus Spring抽象出来的事务信息（集大成者“txMgr”、“txStatus”、“txAttr”…）：TransactionInfo 2.事务增强器 接下来我们来分析事务增强器源码。 ​ 事务增强器的入口自然是**invoke()**。​ ​ 1234567891011121314151617181920212223242526272829303132333435363738/** * 事务增强器的入口 * invocation：后续事务增强器向后驱动事务拦截器的时候还需要使用它 * @param invocation the method invocation joinpoint * @return * @throws Throwable */@Override@Nullablepublic Object invoke(MethodInvocation invocation) throws Throwable &#123; // Work out the target class: may be &#123;@code null&#125;. // The TransactionAttributeSource should be passed the target class // as well as the method, which may be from an interface. /*需要被事务增强器增强的目标类型 * invocation.getThis() 拿到目标对象 * */ Class&lt;?&gt; targetClass = (invocation.getThis() != null ? AopUtils.getTargetClass(invocation.getThis()) : null); // 参数一：目标方法 //参数二：目标对象类型 //参数三： return invokeWithinTransaction(invocation.getMethod(), targetClass, new CoroutinesInvocationCallback() &#123; @Override @Nullable public Object proceedWithInvocation() throws Throwable &#123; return invocation.proceed(); &#125; @Override public Object getTarget() &#123; return invocation.getThis(); &#125; @Override public Object[] getArguments() &#123; return invocation.getArguments(); &#125; &#125;);&#125; 我们直接往下看**invokeWithinTransaction()**。​ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162&#123; Object result; final ThrowableHolder throwableHolder = new ThrowableHolder(); // It&#x27;s a CallbackPreferringPlatformTransactionManager: pass a TransactionCallback in. try &#123; result = ((CallbackPreferringPlatformTransactionManager) ptm).execute(txAttr, status -&gt; &#123; TransactionInfo txInfo = prepareTransactionInfo(ptm, txAttr, joinpointIdentification, status); try &#123; Object retVal = invocation.proceedWithInvocation(); if (retVal != null &amp;&amp; vavrPresent &amp;&amp; VavrDelegate.isVavrTry(retVal)) &#123; // Set rollback-only in case of Vavr failure matching our rollback rules... retVal = VavrDelegate.evaluateTryFailure(retVal, txAttr, status); &#125; return retVal; &#125; catch (Throwable ex) &#123; if (txAttr.rollbackOn(ex)) &#123; // A RuntimeException: will lead to a rollback. if (ex instanceof RuntimeException) &#123; throw (RuntimeException) ex; &#125; else &#123; throw new ThrowableHolderException(ex); &#125; &#125; else &#123; // A normal return value: will lead to a commit. throwableHolder.throwable = ex; return null; &#125; &#125; finally &#123; cleanupTransactionInfo(txInfo); &#125; &#125;); &#125; catch (ThrowableHolderException ex) &#123; throw ex.getCause(); &#125; catch (TransactionSystemException ex2) &#123; if (throwableHolder.throwable != null) &#123; logger.error(&quot;Application exception overridden by commit exception&quot;, throwableHolder.throwable); ex2.initApplicationException(throwableHolder.throwable); &#125; throw ex2; &#125; catch (Throwable ex2) &#123; if (throwableHolder.throwable != null) &#123; logger.error(&quot;Application exception overridden by commit exception&quot;, throwableHolder.throwable); &#125; throw ex2; &#125; // Check result state: It might indicate a Throwable to rethrow. if (throwableHolder.throwable != null) &#123; throw throwableHolder.throwable; &#125; return result; &#125; &#125; 这个方法的内容很多：​ 获取事务注解解析器，解析事务注解 获取事务名称 判断如果是声明式事务 **createTransactionIfNecessary()** 创建一个最上层的事务上下文，包含所有的事务资源 驱动方法增强逻辑继续往下执行 **completeTransactionAfterThrowing(txInfo, ex)**执行业务代码出现异常时的逻辑 **cleanupTransactionInfo(txInfo)**还原现场逻辑 **commitTransactionAfterReturning(txInfo)**提交事务逻辑 判断如果是编程式事务，走编程式事务的逻辑…. ​ 接下来我们来分析这几个核心逻辑。​ 3.创建最上层的事务上下文​ 这里是事务的核心逻辑，涉及到事务嵌套和传播行为，隔离级别。​ 1234567891011121314151617181920212223242526272829303132protected TransactionInfo createTransactionIfNecessary(@Nullable PlatformTransactionManager tm, @Nullable TransactionAttribute txAttr, final String joinpointIdentification) &#123; // If no name specified, apply method identification as transaction name. if (txAttr != null &amp;&amp; txAttr.getName() == null) &#123; //进行一个包装，提供事务名 txAttr = new DelegatingTransactionAttribute(txAttr) &#123; @Override public String getName() &#123; return joinpointIdentification; &#125; &#125;; &#125; //事务状态对象 TransactionStatus status = null; if (txAttr != null) &#123; if (tm != null) &#123; //根据事物属性创建事务状态对象，事务状态：一般情况下包装着事务对象；特殊情况 status.Transaction 有可能为空 //方法上的注解为 @Transactional上的注解 传播行为设置为了 NOT_SUPPORTED || NEVER //具体看一下这个方法 status = tm.getTransaction(txAttr); &#125; else &#123; if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Skipping transactional joinpoint [&quot; + joinpointIdentification + &quot;] because no transaction manager has been configured&quot;); &#125; &#125; &#125; //包装成一个上层的事务上下文对象 return prepareTransactionInfo(tm, txAttr, joinpointIdentification, status);&#125; 获取事务状态对象 包装成事务上下文对象 主要的逻辑在这里**getTransaction()**。\u0000 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859@Overridepublic final TransactionStatus getTransaction(@Nullable TransactionDefinition definition) throws TransactionException &#123; // 事务属性信息 TransactionDefinition def = (definition != null ? definition : TransactionDefinition.withDefaults()); //获取事务对象，非常关键 Object transaction = doGetTransaction(); boolean debugEnabled = logger.isDebugEnabled(); //条件成立说明当前是重入的事务的情况 //a开启事务 a调用b ，b也加了事务注解 ，开启了事务的情况 if (isExistingTransaction(transaction)) &#123; //事务重入的分支逻辑 ，这里就涉及到了传播行为 // Existing transaction found -&gt; check propagation behavior to find out how to behave. return handleExistingTransaction(def, transaction, debugEnabled); &#125; //执行到这里说明当前线程没有绑定连接资源，没开启事务 // Check definition settings for new transaction. if (def.getTimeout() &lt; TransactionDefinition.TIMEOUT_DEFAULT) &#123; throw new InvalidTimeoutException(&quot;Invalid transaction timeout&quot;, def.getTimeout()); &#125; //使用当前事务，没有事务就会抛出异常 // No existing transaction found -&gt; check propagation behavior to find out how to proceed. if (def.getPropagationBehavior() == TransactionDefinition.PROPAGATION_MANDATORY) &#123; throw new IllegalTransactionStateException( &quot;No existing transaction found for transaction marked with propagation &#x27;mandatory&#x27;&quot;); &#125; //PROPAGATION_REQUIRED PROPAGATION_REQUIRES_NEW PROPAGATION_NESTED 则进入 else if (def.getPropagationBehavior() == TransactionDefinition.PROPAGATION_REQUIRED || def.getPropagationBehavior() == TransactionDefinition.PROPAGATION_REQUIRES_NEW || def.getPropagationBehavior() == TransactionDefinition.PROPAGATION_NESTED) &#123; //挂起了个寂寞，因为线程并没有绑定事务 SuspendedResourcesHolder suspendedResources = suspend(null); if (debugEnabled) &#123; logger.debug(&quot;Creating new transaction with name [&quot; + def.getName() + &quot;]: &quot; + def); &#125; try &#123; //线程未开启事务时，这三种情况都会去开启一个新的事物 //开启事务的逻辑 return startTransaction(def, transaction, debugEnabled, suspendedResources); &#125; catch (RuntimeException | Error ex) &#123; resume(null, suspendedResources); throw ex; &#125; &#125; else &#123; //走到这里是什么情况？ support || not support || never //没有使用新的事务 // Create &quot;empty&quot; transaction: no actual transaction, but potentially synchronization. if (def.getIsolationLevel() != TransactionDefinition.ISOLATION_DEFAULT &amp;&amp; logger.isWarnEnabled()) &#123; logger.warn(&quot;Custom isolation level specified but no actual transaction initiated; &quot; + &quot;isolation level will effectively be ignored: &quot; + def); &#125; boolean newSynchronization = (getTransactionSynchronization() == SYNCHRONIZATION_ALWAYS); return prepareTransactionStatus(def, null, true, newSynchronization, debugEnabled, null); &#125;&#125; 获取事务属性信息&amp;事务对象 如果是事务重入的逻辑，就做事务重入逻辑的处理 判断当前事务如果超时了，抛出异常 PROPAGATION_MANDATORY，有就使用当前事务，没有就抛异常 PROPAGATION_REQUIRED PROPAGATION_REQUIRES_NEW PROPAGATION_NESTED 开启一个新的事物 support || not support || never 没有使用新的事务，不会主动开启 最终构建事务状态对象 **doGetTransaction() 获取事务对象。**​ **handleExistingTransaction()处理事务重入的逻辑。**​ **startTransaction()开启新事物的逻辑。**​ 3.1 获取事务对象1234567891011121314151617@Overrideprotected Object doGetTransaction() &#123; //先创建事务对象 DataSourceTransactionObject txObject = new DataSourceTransactionObject(); //事务的保存点，这个由事务管理器控制 txObject.setSavepointAllowed(isNestedTransactionAllowed()); //TransactionSynchronizationManager 事务同步管理器 //从tl中获取连接资源，有可能拿到null，也有可能不是null //什么时候是null，什么时候不是null？ //==null:事务方法a调用了非事务方法b //!=null:事务方法a调用了事务方法b ConnectionHolder conHolder = (ConnectionHolder) TransactionSynchronizationManager.getResource(obtainDataSource()); //为事务对象赋能，参数二传递的false， 表示当前事务是否新分配了连接资源，而不是和上层事务共享，默认是false，表示共享。 txObject.setConnectionHolder(conHolder, false); return txObject;&#125; 3.2处理事务重入123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105private TransactionStatus handleExistingTransaction( TransactionDefinition definition, /*事务属性*/Object transaction/*事务对象*/, boolean debugEnabled) throws TransactionException &#123; /* 进入这个方法的时候说明当前线程已经持有一个事务了，需要根据新方法的事务注解传播行为，走不同的逻辑 */ //PROPAGATION_NEVER：需要抛异常 if (definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_NEVER) &#123; throw new IllegalTransactionStateException( &quot;Existing transaction found for transaction marked with propagation &#x27;never&#x27;&quot;); &#125; //PROPAGATION_NOT_SUPPORTED：有事务就挂起当前事务 if (definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_NOT_SUPPORTED) &#123; if (debugEnabled) &#123; logger.debug(&quot;Suspending current transaction&quot;); &#125; //看一下挂起的逻辑 Object suspendedResources = suspend(transaction); boolean newSynchronization = (getTransactionSynchronization() == SYNCHRONIZATION_ALWAYS); //将挂起事务返回的 挂起持有者对象(这里面持有上一个事务对象的连接资源和线程上下文参数) 给事务状态对象赋能 //step into 创建一个新的 事务状态对象 ，这里的第二个参数 事务 为null ；表示线程执行到当前方法执行到事务增强的后置处理逻辑的时候 //提交事务的时候会检查事务状态的事务是否有值，如果没有值，Spring就不会做提交的操作。 //参数6：suspendedResources 线程执行到后置处理的逻辑的时候，执行到恢复现场的时候会检查这个参数是否有值，如果有值会进行恢复现场的操作。 return prepareTransactionStatus( definition, null/*说明当前线程未手动开启事务，连接是直接从数据源拿的，不需要手动提交事务了*/, false, newSynchronization, debugEnabled, suspendedResources); &#125; //PROPAGATION_REQUIRES_NEW：挂起当前事务，开启一个新的事务 if (definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_REQUIRES_NEW) &#123; if (debugEnabled) &#123; logger.debug(&quot;Suspending current transaction, creating new transaction with name [&quot; + definition.getName() + &quot;]&quot;); &#125; //挂起上层事务，新建事务 SuspendedResourcesHolder suspendedResources = suspend(transaction); try &#123; //开启一个专属于当前方法的新事务，因为当前方法被挂起的事务执行完当前方法后还要回到上层继续执行，所以 //suspendedResources用来恢复现场 return startTransaction(definition, transaction, debugEnabled, suspendedResources); &#125; catch (RuntimeException | Error beginEx) &#123; resumeAfterBeginException(transaction, suspendedResources, beginEx); throw beginEx; &#125; &#125; //嵌套事务的逻辑，如果当前存在事务，则在嵌套事务内执行 ， //如果当前没有事务，则与required的操作类似 if (definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_NESTED) &#123; //默认情况下，spring是不开启这种传播行为的，除非手动开启，所以默认是false。 if (!isNestedTransactionAllowed()) &#123; throw new NestedTransactionNotSupportedException( &quot;Transaction manager does not allow nested transactions by default - &quot; + &quot;specify &#x27;nestedTransactionAllowed&#x27; property with value &#x27;true&#x27;&quot;); &#125; if (debugEnabled) &#123; logger.debug(&quot;Creating nested transaction with name [&quot; + definition.getName() + &quot;]&quot;); &#125; //一般成立，默认是true if (useSavepointForNestedTransaction()) &#123; // 为当前方法创建一个 事务状态对象，共享的上层事务，执行的扩展点是外层的，也不需要挂起事务。 DefaultTransactionStatus status = prepareTransactionStatus(definition, transaction, false, false, debugEnabled, null); //创建一个保存点 ，重要！！！ status.createAndHoldSavepoint(); return status; &#125; else &#123; // 开启新事物 return startTransaction(definition, transaction, debugEnabled, null); &#125; &#125; if (debugEnabled) &#123; logger.debug(&quot;Participating in existing transaction&quot;); &#125; //是否需要验证，默认是false if (isValidateExistingTransaction()) &#123; if (definition.getIsolationLevel() != TransactionDefinition.ISOLATION_DEFAULT) &#123; Integer currentIsolationLevel = TransactionSynchronizationManager.getCurrentTransactionIsolationLevel(); if (currentIsolationLevel == null || currentIsolationLevel != definition.getIsolationLevel()) &#123; Constants isoConstants = DefaultTransactionDefinition.constants; throw new IllegalTransactionStateException(&quot;Participating transaction with definition [&quot; + definition + &quot;] specifies isolation level which is incompatible with existing transaction: &quot; + (currentIsolationLevel != null ? isoConstants.toCode(currentIsolationLevel, DefaultTransactionDefinition.PREFIX_ISOLATION) : &quot;(unknown)&quot;)); &#125; &#125; if (!definition.isReadOnly()) &#123; if (TransactionSynchronizationManager.isCurrentTransactionReadOnly()) &#123; throw new IllegalTransactionStateException(&quot;Participating transaction with definition [&quot; + definition + &quot;] is not marked as read-only but existing transaction is&quot;); &#125; &#125; &#125; /* 执行到这里就剩下 required &amp; supports */ boolean newSynchronization = (getTransactionSynchronization() != SYNCHRONIZATION_NEVER); //prepareTransactionStatus（）上面已经分析过了，属于构造器重载逻辑 //参数二：事务对象 -&gt; connHodler 是 doGetTransaction()的时候从西安城上下文内获取的上层事务的连接资源 //参数六：是空，因为我们没有挂起任何事务 return prepareTransactionStatus(definition, transaction/*事务也是使用上层的*/, false/*表示并不是一个为自己创建的事务，与上层方法共享*/, newSynchronization, debugEnabled, null);&#125; PROPAGATION_NEVER：需要抛异常 PROPAGATION_NOT_SUPPORTED：有事务就挂起当前事务 PROPAGATION_REQUIRES_NEW：挂起当前事务，开启一个新的事务 嵌套事务的逻辑，如果当前存在事务，则在嵌套事务内执行 ，如果当前没有事务，则与required的操作类似 required &amp; supports，不会创建事务对象 最终创建事务状态对象并返回 ​ 看一下创建事务保存点的逻辑，**createAndHoldSavepoint()**。 123456public void createAndHoldSavepoint() throws TransactionException &#123; /* getSavepointManager()：这里实际上就执行到了jdbc的方法，创建保存点，然后保存。 */ setSavepoint(getSavepointManager().createSavepoint());&#125; 3.3 开启新事物1234567891011121314private TransactionStatus startTransaction(TransactionDefinition definition, Object transaction, boolean debugEnabled, @Nullable SuspendedResourcesHolder suspendedResources) &#123; //一般情况下是true ，这个值控制是否执行事务的扩展逻辑，这东西有点类似ioc的后置处理器 boolean newSynchronization = (getTransactionSynchronization() != SYNCHRONIZATION_NEVER); //创建默认的事务状态对象，第三个参数为true，会为当前事务分配 连接资源，就是事务是专门为了当前方法开启的 //suspendedResources:表示挂起的事务 ，从上面过来这里实际上是null。 DefaultTransactionStatus status = newTransactionStatus( definition, transaction, true, newSynchronization, debugEnabled, suspendedResources); //开启事务 核心逻辑 ，参数一：事务对象；参数二：事务属性 doBegin(transaction, definition); //处理TransactionSynchronization prepareSynchronization(status, definition); return status;&#125; 核心逻辑在这里**doBegin()**。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263@Overrideprotected void doBegin(Object transaction, TransactionDefinition definition) &#123; //事务对象 DataSourceTransactionObject txObject = (DataSourceTransactionObject) transaction; Connection con = null; try &#123;/*判断当前事务对象是不是有线程资源，没有就会走if的逻辑，有就说明需要为当前方法分为连接资源*/ if (!txObject.hasConnectionHolder() || txObject.getConnectionHolder().isSynchronizedWithTransaction()) &#123; //通过数据源拿到真实的数据库连接 Connection newCon = obtainDataSource().getConnection(); if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Acquired Connection [&quot; + newCon + &quot;] for JDBC transaction&quot;); &#125; //重点：将上一步创建的数据库连接包装为ConnectionHolder，并且为事务对象赋能 //参数二很关键，给事务新申请的而连接资源，那么就将事务对象的 newConnectionHolder 设置为true。表示当前目标方法开启了一个自己的事务。 txObject.setConnectionHolder(new ConnectionHolder(newCon), true); &#125; txObject.getConnectionHolder().setSynchronizedWithTransaction(true); //获取事务对象商的数据库连接 con = txObject.getConnectionHolder().getConnection(); //修改数据库连接上的一些属性 step into Integer previousIsolationLevel = DataSourceUtils.prepareConnectionForTransaction(con, definition); //将连接原来的隔离级别保存到事务对象，方便释放连接的时候，设置回原来的状态 txObject.setPreviousIsolationLevel(previousIsolationLevel); txObject.setReadOnly(definition.isReadOnly()); //如果连接的自动提交是true，一般会成立 if (con.getAutoCommit()) &#123; //因为接下来就是设置自动提交为false，这里设置 must ，表示回头释放的时候要设置回去 txObject.setMustRestoreAutoCommit(true); if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Switching JDBC Connection [&quot; + con + &quot;] to manual commit&quot;); &#125; //源头，相当于在数据库开启了事务 con.setAutoCommit(false); &#125; //没啥实际的东西 prepareTransactionalConnection(con, definition); //激活holder的事务状态 txObject.getConnectionHolder().setTransactionActive(true); //获取超时时间 int timeout = determineTimeout(definition); //如果时间不相等，就设置 if (timeout != TransactionDefinition.TIMEOUT_DEFAULT) &#123; txObject.getConnectionHolder().setTimeoutInSeconds(timeout); &#125; // 如果是新开启的事务，分配了新的连接就会成立，这个时候需要将线程和连接进行一个绑定 tl if (txObject.isNewConnectionHolder()) &#123; TransactionSynchronizationManager.bindResource(obtainDataSource(), txObject.getConnectionHolder()); &#125; &#125; catch (Throwable ex) &#123; if (txObject.isNewConnectionHolder()) &#123; DataSourceUtils.releaseConnection(con, obtainDataSource()); txObject.setConnectionHolder(null, false); &#125; throw new CannotCreateTransactionException(&quot;Could not open JDBC Connection for transaction&quot;, ex); &#125;&#125; 判断当前事务对象是不是有线程资源，没有就会走if的逻辑，有就说明需要为当前方法分为连接资源 通过数据源拿到真实的数据库连接 将上一步创建的数据库连接包装为ConnectionHolder，并且为事务对象赋能 获取事务对象商的数据库连接 修改数据库连接上的一些属性 将连接原来的隔离级别保存到事务对象，方便释放连接的时候，设置回原来的状态 如果连接的自动提交是true，改成false 激活holder的事务状态 设置超时时间 如果是新开启的事务，分配了新的连接就会成立，这个时候需要将线程和连接进行一个绑定 tl 4.异常回滚逻辑12345678910111213141516171819202122232425262728293031323334353637383940414243protected void completeTransactionAfterThrowing(@Nullable TransactionInfo txInfo/*当前事务上下文*/, Throwable ex/*目标方法抛出的异常信息*/) &#123; if (txInfo != null &amp;&amp; txInfo.getTransactionStatus() != null) &#123; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Completing transaction for [&quot; + txInfo.getJoinpointIdentification() + &quot;] after exception: &quot; + ex); &#125; //条件一：一般都是成立的 //条件二：transactionAttribute.rollbackOn(ex) 判断目标方法抛出的异常是否需要回滚，条件成立，说明需要回滚。 if (txInfo.transactionAttribute != null &amp;&amp; txInfo.transactionAttribute.rollbackOn(ex)) &#123; try &#123; //如果需要回滚，就会走到事务管理器的回滚逻辑 txInfo.getTransactionManager().rollback(txInfo.getTransactionStatus()/*当前事务状态对象*/); &#125; catch (TransactionSystemException ex2) &#123; logger.error(&quot;Application exception overridden by rollback exception&quot;, ex); ex2.initApplicationException(ex); throw ex2; &#125; catch (RuntimeException | Error ex2) &#123; logger.error(&quot;Application exception overridden by rollback exception&quot;, ex); throw ex2; &#125; &#125; else &#123; //执行到这里，说明当前事务虽然抛出了异常，但是该异常并不会导致整个事务回滚 // We don&#x27;t roll back on this exception. // Will still roll back if TransactionStatus.isRollbackOnly() is true. try &#123; txInfo.getTransactionManager().commit(txInfo.getTransactionStatus()); &#125; catch (TransactionSystemException ex2) &#123; logger.error(&quot;Application exception overridden by commit exception&quot;, ex); ex2.initApplicationException(ex); throw ex2; &#125; catch (RuntimeException | Error ex2) &#123; logger.error(&quot;Application exception overridden by commit exception&quot;, ex); throw ex2; &#125; &#125; &#125;&#125; 判断目标方法的异常是否需要回滚？ 需要，**rollback()**。 不需要，**commit()**。 ​ 来看一下**rollback()**的逻辑。​ 1234567891011@Overridepublic final void rollback(TransactionStatus status) throws TransactionException &#123; if (status.isCompleted()) &#123; throw new IllegalTransactionStateException( &quot;Transaction is already completed - do not call commit or rollback more than once per transaction&quot;); &#125; DefaultTransactionStatus defStatus = (DefaultTransactionStatus) status; //看这个逻辑 processRollback(defStatus, false);&#125; 继续往下看**processRollback()**。​ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475private void processRollback(DefaultTransactionStatus status, boolean unexpected) &#123; try &#123; boolean unexpectedRollback = unexpected; try &#123; //事务扩展逻辑的调用点 triggerBeforeCompletion(status); //说明当前事务是一个内嵌事务 ，当前方法使用的事务是上层的事务，如果有保存点，就回滚到保存点 if (status.hasSavepoint()) &#123; if (status.isDebug()) &#123; logger.debug(&quot;Rolling back transaction to savepoint&quot;); &#125; //回滚到保存点的操作 status.rollbackToHeldSavepoint(); &#125; //条件成立：说明当前方法是一个开启了一个新的事物的方法 else if (status.isNewTransaction()) &#123; if (status.isDebug()) &#123; logger.debug(&quot;Initiating transaction rollback&quot;); &#125; //委派模式，核心逻辑 doRollback(status); &#125; else &#123; //执行到这里说明，这个事务不是当前方法开启的 （共享上层事务）|| 当前方法压根没开启事务（not_supports，never, supports） // Participating in larger transaction //说的是第一种情况：当前方法共享上层事务 if (status.hasTransaction()) &#123; //条件一：什么时候成立？当前方法共享上层事务，业务代码强制设置当前整个事务 需要回滚的话，可以通过 设置 status.isLocalRollbackOnly() = true //条件二：默认是true if (status.isLocalRollbackOnly() || isGlobalRollbackOnParticipationFailure()) &#123; if (status.isDebug()) &#123; logger.debug(&quot;Participating transaction failed - marking existing transaction as rollback-only&quot;); &#125; /* 一个共享上层事务的方法可以直接回滚嘛？不行的，需要将回滚的操作交给上层方法来做。 如何交给？设置status.isLocalRollbackOnly()=true，这样的话，线程回到上层事务提交逻辑的时候，会检查该字段，发现是true，就会执行回滚逻辑 */ //这里其实就是设置回滚字段为true 先拿到事务状态对象-&gt; 事务对象 -&gt; 连接持有者 -&gt; 设置rollbackOnly =true。 doSetRollbackOnly(status); &#125; else &#123; if (status.isDebug()) &#123; logger.debug(&quot;Participating transaction failed - letting transaction originator decide on rollback&quot;); &#125; &#125; &#125;//没有事务 else &#123; logger.debug(&quot;Should roll back transaction but cannot - no transaction available&quot;); &#125; // Unexpected rollback only matters here if we&#x27;re asked to fail early if (!isFailEarlyOnGlobalRollbackOnly()) &#123; unexpectedRollback = false; &#125; &#125; &#125; catch (RuntimeException | Error ex) &#123; //事务扩展调用点 triggerAfterCompletion(status, TransactionSynchronization.STATUS_UNKNOWN); throw ex; &#125; //事务扩展调用点 triggerAfterCompletion(status, TransactionSynchronization.STATUS_ROLLED_BACK); // Raise UnexpectedRollbackException if we had a global rollback-only marker if (unexpectedRollback) &#123; throw new UnexpectedRollbackException( &quot;Transaction rolled back because it has been marked as rollback-only&quot;); &#125; &#125; finally &#123; //看这里 cleanupAfterCompletion(status); &#125;&#125; 关注两个核心的方法**doRollback() &amp; rollbackToHeldSavepoint()**。 1234567891011121314public void rollbackToHeldSavepoint() throws TransactionException &#123; //获取事务的保存点 Object savepoint = getSavepoint(); if (savepoint == null) &#123; throw new TransactionUsageException( &quot;Cannot roll back to savepoint - no savepoint associated with current transaction&quot;); &#125; //回滚到保存点 getSavepointManager().rollbackToSavepoint(savepoint); //删除保存点 getSavepointManager().releaseSavepoint(savepoint); //清空保存点 setSavepoint(null);&#125; 1234567891011121314@Overrideprotected void doRollback(DefaultTransactionStatus status) &#123; DataSourceTransactionObject txObject = (DataSourceTransactionObject) status.getTransaction(); Connection con = txObject.getConnectionHolder().getConnection(); if (status.isDebug()) &#123; logger.debug(&quot;Rolling back JDBC transaction on Connection [&quot; + con + &quot;]&quot;); &#125; try &#123; con.rollback(); &#125; catch (SQLException ex) &#123; throw translateException(&quot;JDBC rollback&quot;, ex); &#125;&#125; 5.还原现场逻辑123456protected void cleanupTransactionInfo(@Nullable TransactionInfo txInfo) &#123; if (txInfo != null) &#123; //往下看 txInfo.restoreThreadLocalStatus(); &#125;&#125; 我们往下看 123456private void restoreThreadLocalStatus() &#123; // Use stack to restore old transaction TransactionInfo. // Will be null if none was set. //相当于事务出栈的逻辑 transactionInfoHolder.set(this.oldTransactionInfo);&#125; 这里很简单，就是让当前事务出栈。将栈内的事务设置成上一层事务。 6.提交事务逻辑123456789protected void commitTransactionAfterReturning(@Nullable TransactionInfo txInfo) &#123; if (txInfo != null &amp;&amp; txInfo.getTransactionStatus() != null) &#123; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Completing transaction for [&quot; + txInfo.getJoinpointIdentification() + &quot;]&quot;); &#125; //调用事务管理器的提交事务方法 txInfo.getTransactionManager().commit(txInfo.getTransactionStatus()/*事务状态*/); &#125;&#125; 调用了事务管理器的提交事务方法。​ 12345678910111213141516171819202122232425262728293031@Overridepublic final void commit(TransactionStatus status) throws TransactionException &#123; if (status.isCompleted()) &#123; throw new IllegalTransactionStateException( &quot;Transaction is already completed - do not call commit or rollback more than once per transaction&quot;); &#125; DefaultTransactionStatus defStatus = (DefaultTransactionStatus) status; //说明是业务强制回滚 if (defStatus.isLocalRollbackOnly()) &#123; if (defStatus.isDebug()) &#123; logger.debug(&quot;Transactional code has requested rollback&quot;); &#125; //处理回滚 processRollback(defStatus, false); return; &#125; //shouldCommitOnGlobalRollbackOnly() 默认是true //defStatus.isGlobalRollbackOnly() 其实就是defStatus -&gt;txObject-&gt;connHolder-&gt;rollbackOnly 字段 //下层事务使用上层事务的时候，想回滚，就会设置这个标记 if (!shouldCommitOnGlobalRollbackOnly() &amp;&amp; defStatus.isGlobalRollbackOnly()) &#123; if (defStatus.isDebug()) &#123; logger.debug(&quot;Global transaction is marked as rollback-only but transactional code requested commit&quot;); &#125; processRollback(defStatus, true); return; &#125; //处理提交 processCommit(defStatus);&#125; 核心的逻辑在**processCommit()**。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576private void processCommit(DefaultTransactionStatus status) throws TransactionException &#123; try &#123; boolean beforeCompletionInvoked = false; try &#123; boolean unexpectedRollback = false; prepareForCommit(status); triggerBeforeCommit(status); triggerBeforeCompletion(status); beforeCompletionInvoked = true; if (status.hasSavepoint()) &#123; if (status.isDebug()) &#123; logger.debug(&quot;Releasing transaction savepoint&quot;); &#125; unexpectedRollback = status.isGlobalRollbackOnly(); //有保存点就清理 status.releaseHeldSavepoint(); &#125; else if (status.isNewTransaction()) &#123; if (status.isDebug()) &#123; logger.debug(&quot;Initiating transaction commit&quot;); &#125; unexpectedRollback = status.isGlobalRollbackOnly(); //底层提交事务 doCommit(status); &#125; else if (isFailEarlyOnGlobalRollbackOnly()) &#123; unexpectedRollback = status.isGlobalRollbackOnly(); &#125; // Throw UnexpectedRollbackException if we have a global rollback-only // marker but still didn&#x27;t get a corresponding exception from commit. if (unexpectedRollback) &#123; throw new UnexpectedRollbackException( &quot;Transaction silently rolled back because it has been marked as rollback-only&quot;); &#125; &#125; catch (UnexpectedRollbackException ex) &#123; // can only be caused by doCommit triggerAfterCompletion(status, TransactionSynchronization.STATUS_ROLLED_BACK); throw ex; &#125; catch (TransactionException ex) &#123; // can only be caused by doCommit if (isRollbackOnCommitFailure()) &#123; doRollbackOnCommitException(status, ex); &#125; else &#123; triggerAfterCompletion(status, TransactionSynchronization.STATUS_UNKNOWN); &#125; throw ex; &#125; catch (RuntimeException | Error ex) &#123; if (!beforeCompletionInvoked) &#123; triggerBeforeCompletion(status); &#125; doRollbackOnCommitException(status, ex); throw ex; &#125; // Trigger afterCommit callbacks, with an exception thrown there // propagated to callers but the transaction still considered as committed. try &#123; triggerAfterCommit(status); &#125; finally &#123; triggerAfterCompletion(status, TransactionSynchronization.STATUS_COMMITTED); &#125; &#125; finally &#123; //清理资源，比如保存点 cleanupAfterCompletion(status); &#125;&#125; 逻辑很简单，清理保存点，提交事务，执行扩展点逻辑，清理资源。​ 我们来看一下资源的清理**cleanupAfterCompletion(status)**。​ 123456789101112131415161718192021private void cleanupAfterCompletion(DefaultTransactionStatus status) &#123; //设置当前方法的事务状态为完成状态 status.setCompleted(); //清理逻辑 线程上下文变量 &amp; 扩展点注册的东西 if (status.isNewSynchronization()) &#123; TransactionSynchronizationManager.clear(); &#125; //如果是开启的新事物，还原现场操作 最重要的就是解绑线程持有的连接 if (status.isNewTransaction()) &#123; doCleanupAfterCompletion(status.getTransaction()); &#125; //说明当前事务执行的时候，挂起了一个上层的事务 if (status.getSuspendedResources() != null) &#123; if (status.isDebug()) &#123; logger.debug(&quot;Resuming suspended transaction after completion of inner transaction&quot;); &#125; Object transaction = (status.hasTransaction() ? status.getTransaction() : null); //唤醒上层的事务 resume(transaction, (SuspendedResourcesHolder) status.getSuspendedResources()); &#125;&#125; \u0000看几个核心方法**doCleanupAfterCompletion()** &amp; **resume()**。​ 12345678910111213141516171819202122232425262728293031@Overrideprotected void doCleanupAfterCompletion(Object transaction) &#123; DataSourceTransactionObject txObject = (DataSourceTransactionObject) transaction; // Remove the connection holder from the thread, if exposed. if (txObject.isNewConnectionHolder()) &#123; TransactionSynchronizationManager.unbindResource(obtainDataSource()); &#125; // Reset connection. Connection con = txObject.getConnectionHolder().getConnection(); try &#123; if (txObject.isMustRestoreAutoCommit()) &#123; con.setAutoCommit(true); &#125; DataSourceUtils.resetConnectionAfterTransaction( con, txObject.getPreviousIsolationLevel(), txObject.isReadOnly()); &#125; catch (Throwable ex) &#123; logger.debug(&quot;Could not reset JDBC Connection after transaction&quot;, ex); &#125; if (txObject.isNewConnectionHolder()) &#123; if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Releasing JDBC Connection [&quot; + con + &quot;] after transaction&quot;); &#125; DataSourceUtils.releaseConnection(con, this.dataSource); &#125; txObject.getConnectionHolder().clear();&#125; 1234567891011121314151617181920protected final void resume(@Nullable Object transaction, @Nullable SuspendedResourcesHolder resourcesHolder) throws TransactionException &#123; if (resourcesHolder != null) &#123; Object suspendedResources = resourcesHolder.suspendedResources; if (suspendedResources != null) &#123; //重新绑定上一个事务的资源 doResume(transaction, suspendedResources); &#125; List&lt;TransactionSynchronization&gt; suspendedSynchronizations = resourcesHolder.suspendedSynchronizations; if (suspendedSynchronizations != null) &#123; //将线程上下文变量恢复为上一个事务的现场 TransactionSynchronizationManager.setActualTransactionActive(resourcesHolder.wasActive); TransactionSynchronizationManager.setCurrentTransactionIsolationLevel(resourcesHolder.isolationLevel); TransactionSynchronizationManager.setCurrentTransactionReadOnly(resourcesHolder.readOnly); TransactionSynchronizationManager.setCurrentTransactionName(resourcesHolder.name); doResumeSynchronization(suspendedSynchronizations); &#125; &#125;&#125; **doResume()**​ 1234@Overrideprotected void doResume(@Nullable Object transaction, Object suspendedResources) &#123; TransactionSynchronizationManager.bindResource(obtainDataSource(), suspendedResources);&#125; 补充一个逻辑**doSuspend()**。挂起逻辑​ 12345678910@Overrideprotected Object doSuspend(Object transaction) &#123; DataSourceTransactionObject txObject = (DataSourceTransactionObject) transaction; //将事务对象的 连接持有者设置为null，不想和上层事务共享连接资源... //当前方法有可能是不开启事务 || 要开启一个独立的事务 txObject.setConnectionHolder(null); //解绑线程上的事务，将连接持有者从tl移除掉，这样业务就不会再拿上层事务的连接资源了 return TransactionSynchronizationManager.unbindResource(obtainDataSource());&#125; 7.思考与沉淀事务的源码有一些琐碎，至此整个事务的源码其实已经分析完了，为了形成一个清晰，完整的链路，我画了一张事务源码的流程图。​ 我们再来回顾一下事务的流程。@EnableTransactionManagement 利用TransactionManagementConfigurationSelector给容器中会导入组件 给容器中导入了两个类： 1.AutoProxyRegistrar 往容器中导入了一个组件：InfrastructureAdvisorAutoProxyCreator 这个类是什么？利用后置处理器，包装对象，返回一个代理对象（增强器），代理对象执行方法的时候利用拦截器进行调用 2.ProxyTransactionManagementConfiguration 给容器中注册了三个bean： 1.事务增强器 ：advisor切面 2.事务注解解析器：解析事务注解，获取注解信息 3.事务增强器的拦截器：拦截事务相关的advisor切面 ctrl + H 查看类的继承关系：这个类是MethodInterceptor的子类 在目标方法执行的时候： 执行拦截器链条 事务拦截器： 1.先获取事务相关的属性 2.在获取事务管理器，如果事先没有添加和执行任何事务管理器，最终会从容器中拿出来一个默认的 3.执行目标方法 1.如果是异常，获取到事务管理器，利用事务管理器进行回滚 2.如果是正常执行，利用事务管理器，提交事务。 事务注解解析器和事务增强器的拦截器都包含在事务增强器中，为事务增强器赋能。\u0000 至此，事务源码分析完成，后续的篇章，我们会去分析web请求的相关流程，我是二十，熟读Java生态圈源码，精通Java高并发编程，欢迎点赞关注。","categories":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/tags/Spring/"}]},{"title":"Spring[十四]事务标签解析&创建代理对象","slug":"Spring/Spring[十四]事务标签解析&创建代理对象","date":"2022-01-11T06:11:03.124Z","updated":"2022-01-11T06:17:49.759Z","comments":true,"path":"2022/01/11/Spring/Spring[十四]事务标签解析&创建代理对象/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/Spring/Spring[%E5%8D%81%E5%9B%9B]%E4%BA%8B%E5%8A%A1%E6%A0%87%E7%AD%BE%E8%A7%A3%E6%9E%90&%E5%88%9B%E5%BB%BA%E4%BB%A3%E7%90%86%E5%AF%B9%E8%B1%A1/","excerpt":"","text":"本篇我们开始分析Spring的事务，在看源码之前，我们先来做一些铺垫。 1.JDBC&amp;Spring事务12345678910111213141516171819202122232425@Testpublic void test() &#123; Connection co = null; try &#123; co = JDBCUtils.getConnection(); co.setAutoCommit(false); String sql1 = &quot;update user_table set balance=balance-? where user=?&quot;; String sql2 = &quot;update user_table set balance=balance+? where user=?&quot;; update(co,sql1,100,&quot;AA&quot;); //System.out.println(10/0); update(co,sql2,100,&quot;BB&quot;); co.commit(); System.out.println(&quot;转账成功！&quot;); &#125; catch (SQLException e) &#123; try &#123; co.rollback(); System.out.println(&quot;转账失败！&quot;); &#125; catch (SQLException e1) &#123; e1.printStackTrace(); &#125; &#125; finally &#123; JDBCUtils.closeResource(co,null,null); &#125;&#125; 123456789101112131415public static int update(Connection co, String sql, Object... args) &#123; PreparedStatement ps = null; try &#123; ps = co.prepareStatement(sql); for (int i = 0; i &lt; args.length; i++) &#123; ps.setObject(i + 1, args[i]); &#125; return ps.executeUpdate(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; finally &#123; JDBCUtils.closeResource(null, ps, null); &#125; return 0;&#125; 接下来看一下Spring事务。 2.事务传播行为​ 所谓 spring 事务的传播属性，就是定义在存在多个事务同时存在的时候，spring 应该如何处理这些事务的行为。这些属性在 TransactionDefinition 中定义。​ 常量名称 常量解释 PROPAGATION_REQUIRED 支持当前事务，如果当前没有事务，就新建一个事务。这是最常见的选择，也是 Spring默认的事务的传播。 PROPAGATION_REQUIRES_NEW 新建事务，如果当前存在事务，把当前事务挂起。新建的事务将和被挂起的事务没有任何关系，是两个独立的事务，外层事务失败回滚之后，不能回滚内层事务执行的结果，内层事务失败抛出异常，外层事务捕获，也可以不处理回滚操作 PROPAGATION_SUPPORTS 支持当前事务，如果当前没有事务，就以非事务方式执行。 PROPAGATION_MANDATORY 支持当前事务，如果当前没有事务，就抛出异常。 PROPAGATION_NOT_SUPPORTED 以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。 PROPAGATION_NEVER 以非事务方式执行，如果当前存在事务，则抛出异常。 PROPAGATION_NESTED 如果一个活动的事务存在，则运行在一个嵌套的事务中。如果没有活动事务，则按REQUIRED 属性执行。它使用了一个单独的事务，这个事务拥有多个可以回滚的保存点。内部事务的回滚不会对外部事务造成影响。它只对DataSourceTransactionManager 事务管理器起效。 1234567891011121314151617181920PROPAGATION_REQUIREDclass ServiceA &#123; @Autowired ServiceB serviceB； @Transactional void a() &#123; .... serviceB.b(); .... &#125;&#125;class ServiceB &#123; @Transactional void b() &#123; ...... &#125;&#125; 线程执行到serviceA.a() 方法时，其实是执行的 代理serviceA对象的a方法。 执行代理serviceA对象的a方法 执行a方法的增强逻辑-&gt; 事务增强器 (环绕增强) 事务增强器会做什么事? 提取事务标签属性 检查当前线程有没有绑定 conn 数据库连接 资源？ 发现当前线程未绑定（TransactionSync…Manager#resources 是 ThreadLocal&lt;Map&lt;obj,obj&gt;&gt;，检查key:datasource 有没有数据） 因为未绑定conn资源，所以线程下一步就是 到 datasource.getConnection() 获取一个conn资源 因为新获取的conn资源的autocommit是true，所以这一步 修改 autocommit 为false，表示手动提交事务，这一步也表示 开启事务（修改conn其它 属性..） 绑定conn资源到 TransactionSync…Manager#resources，key：datasource 执行事务增强器后面的增强器.. 最后一个advice调用 target的目标方法 a() 方法 假设target a方法 需要访问数据库 执行SQL 的话，程序需要获取一个 conn 资源，到哪拿？ DataSourceUtils.getConnection(datasource) 这一步最终会拿到 事务增强器 前置增强逻辑 存放在 TransactionSync..Manager#resources 内的conn 资源 执行方法a逻辑…可能会执行一些 SQL 语句… 线程执行到这样一行代码：serviceB.b() serviceB 它是一个代理对象，因为它也使用了 @Transactional 注解了，Spring 会为它创建代理的。 执行代理serviceB对象的b方法 执行b方法的增强逻辑-&gt; 事务增强器（环绕增强） 事务增强器会做什么事? 提取事务标签属性 检查当前线程有没有绑定 conn 数据库连接 资源？发现当前线程已经绑定了 conn 数据库连接资源了 检查事务注解属性，发现自己打的propagation == REQUIRED，所以继续共享 conn 数据库链接资源 执行事务增强器后面的增强器.. 最后一个device调用 target (serviceB)的目标方法 b() 方法 假设target b方法 需要访问数据库 执行SQL 的话，程序需要获取一个 conn 资源，到哪拿？ DataSourceUtils.getConnection(datasource) 这一步最终会拿到 代理serviceA对象存放在 TransactionSync..Manager#resources 内的conn 资源 执行方法b逻辑…可能会执行一些 SQL 语句… 线程继续执行 事务增强器 环绕增强的后置逻辑 （代理serviceB.b() 方法的 后置增强） 检查发现，serviceB.b() 事务并不是 当前 b方法开启的，所以 基本不做什么事情.. 线程继续回到 目标 serviceA.a() 方法内，继续执行 执行方法a逻辑…可能会执行一些 SQL 语句… 线程继续回到 代理 serviceA.a() 方法内，继续执行 执行a方法的增强逻辑-&gt; 事务增强器 (环绕增强-后置增强逻辑) 提交事务/回滚事务 恢复连接状态 （将conn的autocommit 设置回 true…等等） 清理工作（将绑定的conn资源从TransactionSync…Manager#resources移除） conn 连接关闭 （归还连接到datasource） 1234567891011121314151617181920PROPAGATION_SUPPORTSclass ServiceA &#123; @Autowired ServiceB serviceB； @Transactional void a() &#123; .... serviceB.b(); .... &#125;&#125;class ServiceB &#123; @Transactional(propagation = SUPPORTS) void b() &#123; ...... &#125;&#125; 逻辑和上面完全一致。 12345678class ServiceA &#123; @Transactional(Propagation = SUPPORTS) void a() &#123; .... .... &#125;&#125; 线程在未绑定事务的情况下，去调用serviceA.a() 方法会发生什么呢？ 线程执行到serviceA.a() 方法时，其实是执行的 代理serviceA对象的a方法。 执行代理serviceA对象的a方法 执行a方法的增强逻辑-&gt; 事务增强器 (环绕增强) 事务增强器会做什么事? 提取事务标签属性 检查当前线程有没有绑定 conn 数据库连接 资源？ 发现当前线程未绑定（TransactionSync…Manager#resources 是 ThreadLocal&lt;Map&lt;obj,obj&gt;&gt;，检查key:datasource 有没有数据） 啥也不用做.. 执行事务增强器后面的增强器.. 最后一个advice调用 target的目标方法 a() 方法 假设target a方法 需要访问数据库 执行SQL 的话，程序需要获取一个 conn 资源，到哪拿？ DataSourceUtils.getConnection(datasource) ，因为事务增强器前置增强逻辑 并没有 向TransactionSync..Manager#resources 内绑定conn资源 因为 上一步未拿到 conn资源，所以 DataSourceUtils 通过 datasource.getConnection() 获取了一个全新的 conn 资源（注意：conn.autocommit == true,执行的每一条sql 都是一个 独立事务！！） 执行方法a逻辑…可能会执行一些 SQL 语句… 线程继续执行到代理serviceA对象的a方法 （事务增强器-后置增强逻辑） 检查发现 TrasactionSync..Manager#resources 并未绑定任何 conn 资源，所以 这一步啥也不做了… 1234567891011121314151617181920PROPAGATION_MANDATORY 很少使用..class ServiceA &#123; @Autowired ServiceB serviceB； @Transactional void a() &#123; .... serviceB.b(); .... &#125;&#125;class ServiceB &#123; @Transactional(propagation = MANDATORY) void b() &#123; ...... &#125;&#125; 如果是这样的话，情况和 PROPAGATION_REQUIRED 案例分析 完全一致。 12345678class ServiceA &#123; @Transactional(Propagation = MANDATORY) void a() &#123; .... .... &#125;&#125; 线程在未绑定事务的情况下，去调用serviceA.a() 方法会发生什么呢？ 线程执行到serviceA.a() 方法时，其实是执行的 代理serviceA对象的a方法。 执行代理serviceA对象的a方法 执行a方法的增强逻辑-&gt; 事务增强器 (环绕增强) 事务增强器会做什么事? 提取事务标签属性 检查当前线程有没有绑定 conn 数据库连接 资源？ 发现当前线程未绑定（TransactionSync…Manager#resources 是 ThreadLocal&lt;Map&lt;obj,obj&gt;&gt;，检查key:datasource 有没有数据） 直接抛出异常… … 123456789101112131415161718192021PROPAGATION_REQUIRES_NEWclass ServiceA &#123; @Autowired ServiceB serviceB； @Transactional void a() &#123; .... serviceB.b(); .... &#125;&#125;class ServiceB &#123; @Transactional(propagation = REQUIRES_NEW) void b() &#123; ...... &#125;&#125; 线程执行到serviceA.a() 方法时，其实是执行的 代理serviceA对象的a方法。 执行代理serviceA对象的a方法 执行a方法的增强逻辑-&gt; 事务增强器 (环绕增强) 事务增强器会做什么事? 提取事务标签属性 检查当前线程有没有绑定 conn 数据库连接 资源？ 发现当前线程未绑定（TransactionSync…Manager#resources 是 ThreadLocal&lt;Map&lt;obj,obj&gt;&gt;，检查key:datasource 有没有数据） 因为未绑定conn资源，所以线程下一步就是 到 datasource.getConnection() 获取一个conn资源 因为新获取的conn资源的autocommit是true，所以这一步 修改 autocommit 为false，表示手动提交事务，这一步也表示 开启事务（修改conn其它 属性..） 绑定conn资源到 TransactionSync…Manager#resources，key：datasource 执行事务增强器后面的增强器.. 最后一个advice调用 target的目标方法 a() 方法 假设target a方法 需要访问数据库 执行SQL 的话，程序需要获取一个 conn 资源，到哪拿？ DataSourceUtils.getConnection(datasource) 这一步最终会拿到 事务增强器 前置增强逻辑 存放在 TransactionSync..Manager#resources 内的conn 资源 执行方法a逻辑…可能会执行一些 SQL 语句… 线程执行到这样一行代码：serviceB.b() serviceB 它是一个代理对象，因为它也使用了 @Transactional 注解了，Spring 会为它创建代理的。 执行代理serviceB对象的b方法 执行b方法的增强逻辑-&gt; 事务增强器（环绕增强） 事务增强器会做什么事? 提取事务标签属性 检查发现当前线程已经绑定了conn资源（并且手动开启了事务..），又发现 当前方法的 传播行为：REQUIRES_NEW ，需要开启一个新的事务.. 将已经绑定的conn资源 保存到 suspand 变量内 因为 REQUIRES_NEW 不会和上层共享同一个事务，所以这一步 又到 datasource.getConnection() 获取了一个全新的 conn 数据库连接资源 因为新获取的conn资源的autocommit是true，所以这一步 修改 autocommit 为false，表示手动提交事务，这一步也表示 开启事务（修改conn其它 属性..） 绑定conn资源到 TransactionSync…Manager#resources，key：datasource 执行事务增强器后面的增强器.. 最后一个advice调用 target （serviceB）的目标方法 b() 方法 假设target b方法 需要访问数据库 执行SQL 的话，程序需要获取一个 conn 资源，到哪拿？ DataSourceUtils.getConnection(datasource) 这一步最终会拿到 事务增强器 前置增强逻辑 存放在 TransactionSync..Manager#resources 内的conn 资源 执行方法a逻辑…可能会执行一些 SQL 语句… 线程继续执行 事务增强器 环绕增强的后置逻辑 （代理serviceB.b() 方法的 后置增强） 检查发现，serviceB.b() 事务是 b方法开启的，所以 需要做一些事情了 执行b方法的增强逻辑-&gt; 事务增强器 (环绕增强-后置增强逻辑) 提交事务/回滚事务 恢复连接状态 （将conn的autocommit 设置回 true…等等） 清理工作（将绑定的conn资源从TransactionSync…Manager#resources移除） conn 连接关闭 （归还连接到datasource） 检查suspand 发现 该变量有值，需要执行 恢复现场的工作 resume() 恢复现场 将suspand 挂起的 conn 资源再次 绑定到 TransactionSync…Manager#resources 内，方便 serviceA 继续使用它的conn资源 （它自己的事务） 线程继续回到 serviceA.a() 方法内 继续执行一些sql …注意 这里它使用的 conn 是 serviceA 申请的 conn 线程继续执行 事务增强器 环绕增强的后置逻辑 （代理serviceA.a() 方法的 后置增强） 检查发现，serviceA.a() 事务是 a方法开启的，所以 需要做一些事情了 执行a方法的增强逻辑-&gt; 事务增强器 (环绕增强-后置增强逻辑) 提交事务/回滚事务 恢复连接状态 （将conn的autocommit 设置回 true…等等） 清理工作（将绑定的conn资源从TransactionSync…Manager#resources移除） conn 连接关闭 （归还连接到datasource） 3.隔离级别3.1 Mysql 隔离级别 隔离级别的值 导致的问题 Read-Uncommitted 0 导致脏读 Read-Committed 1 避免脏读，允许不可重复读和幻读 Repeatable-Read 2 避免脏读，不可重复读，允许幻读 Serializable 3 串行化读，事务只能一个一个执行，避免了脏读、不可重复读、幻读。执行效率慢，使用时慎重 脏读：一事务对数据进行了增删改，但未提交，另一事务可以读取到未提交的数据。如果第一个事务这时候回滚了，那么第二个事务就读到了脏数据。 不可重复读：一个事务中发生了两次读操作，第一次读操作和第二次操作之间，另外一个事务对数据进行了修改，这时候两次读取的数据是不一致的。 幻读：第一个事务对一定范围的数据进行批量修改，第二个事务在这个范围增加一条数据，这时候第一个事务就会丢失对新增数据的修改。 隔离级别越高，越能保证数据的完整性和一致性，但是对并发性能的影响也越大。大多数的数据库默认隔离级别为 Read Commited，比如 SqlServer、Oracle少数数据库默认隔离级别为：Repeatable Read 比如： MySQL InnoDB。​ 3.2 Spring 常量 解释 ISOLATION_DEFAULT 这是个 PlatfromTransactionManager 默认的隔离级别，使用数据库默认的事务隔离级别。另外四个与 JDBC 的隔离级别相对应。 ISOLATION_READ_UNCOMMITTED 这是事务最低的隔离级别，它允许另外一个事务可以看到这个事务未提交的数据。这种隔离级别会产生脏读，不可重复读和幻像读。 ISOLATION_READ_COMMITTED 保证一个事务修改的数据提交后才能被另外一个事务读取。另外一个事务不能读取该事务未提交的数据。 ISOLATION_REPEATABLE_READ 这种事务隔离级别可以防止脏读，不可重复读。但是可能出现幻像读。 ISOLATION_SERIALIZABLE 这是花费最高代价但是最可靠的事务隔离级别。事务被处理为顺序执行。 4.事务嵌套这里内容在实际开发中比较重要，因此我们再来做一下简单的回顾。 假设外层事务 Service A 的 Method A() 调用 内层 Service B 的 Method B() PROPAGATION_REQUIRED(Spring 默认) 如果 ServiceB.MethodB() 的事务级别定义为 PROPAGATION_REQUIRED，那么执行ServiceA.MethodA() 的时候 Spring 已经起了事务，这时调用 ServiceB.MethodB()，ServiceB.MethodB() 看到自己已经运行在 ServiceA.MethodA() 的事务内部，就不再起新的事务。 假如 ServiceB.MethodB() 运行的时候发现自己没有在事务中，他就会为自己分配一个事务。 这样，在 ServiceA.MethodA() 或者在 ServiceB.MethodB() 内的任何地方出现异常，事务都会被回滚。 PROPAGATION_REQUIRES_NEW 比如我们设计 ServiceA.MethodA() 的事务级别为 PROPAGATION_REQUIRED，ServiceB.MethodB() 的事务级别为 PROPAGATION_REQUIRES_NEW。那么当执行到 ServiceB.MethodB() 的时候，ServiceA.MethodA() 所在的事务就会挂起，ServiceB.MethodB() 会起一个新的事务，等待 ServiceB.MethodB() 的事务完成以后，它才继续执行。 他 与 PROPAGATION_REQUIRED 的 事 务 区 别 在 于 事 务 的 回 滚 程 度 了 。 因 为ServiceB.MethodB() 是新起一个事务，那么就是存在两个不同的事务。如果ServiceB.MethodB() 已 经 提 交 ， 那 么 ServiceA.MethodA() 失 败 回 滚 ，ServiceB.MethodB() 是不会回滚的。如果 ServiceB.MethodB() 失败回滚，如果他抛出的异常被 ServiceA.MethodA() 捕获，ServiceA.MethodA() 事务仍然可能提交(主要看 B 抛出的异常是不是 A 会回滚的异常)。 PROPAGATION_SUPPORTS假设 ServiceB.MethodB() 的事务级别为 PROPAGATION_SUPPORTS，那么当执行到ServiceB.MethodB()时，如果发现 ServiceA.MethodA()已经开启了一个事务，则加入当前的事务，如果发现 ServiceA.MethodA()没有开启事务，则自己也不开启事务。这种时候，内部方法的事务性完全依赖于最外层的事务。 PROPAGATION_NESTED现 在 的 情 况 就 变 得 比 较 复 杂 了 , ServiceB.MethodB() 的 事 务 属 性 被 配 置 为PROPAGATION_NESTED, 此时两者之间又将如何协作呢? ServiceB.MethodB() 如果 rollback, 那么内部事务(即 ServiceB.MethodB()) 将回滚到它执行前的 SavePoint而外部事务(即 ServiceA.MethodA()) 可以有以下两种处理方式: 捕获异常，执行异常分支逻辑 1234567void MethodA() &#123; try &#123; ServiceB.MethodB(); &#125; catch (SomeException) &#123; // 执行其他业务, 如 ServiceC.MethodC(); &#125;&#125; 这 种 方 式 也 是 嵌 套 事 务 最 有 价 值 的 地 方 , 它 起 到 了 分 支 执 行 的 效 果 , 如 果ServiceB.MethodB()失败, 那么执行 ServiceC.MethodC(), 而 ServiceB.MethodB()已经回滚到它执行之前的 SavePoint, 所以不会产生脏数据(相当于此方法从未执行过),这 种 特 性 可 以 用 在 某 些 特 殊 的 业 务 中 , 而 PROPAGATION_REQUIRED 和PROPAGATION_REQUIRES_NEW 都没有办法做到这一点。 外部事务回滚/提交 代码不做任何修改, 那么如果内部事务(ServiceB.MethodB())rollback, 那么首先 ServiceB.MethodB() 回滚到它执行之前的 SavePoint(在任何情况下都会如此), 外部事务(即 ServiceA.MethodA()) 将根据具体的配置决定自己是commit 还是 rollback。 ​ 接下来我们来看事务的源码。 5.Spring事务标签解析1234567891011121314151617&lt;bean id=&quot;dataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot;&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;$&#123;driverClassName&#125;&quot;&gt;&lt;/property&gt; &lt;property name=&quot;url&quot; value=&quot;$&#123;url&#125;&quot;&gt;&lt;/property&gt; &lt;property name=&quot;username&quot; value=&quot;$&#123;jdbc.username&#125;&quot;&gt;&lt;/property&gt; &lt;property name=&quot;password&quot; value=&quot;$&#123;jdbc.password&#125;&quot;&gt;&lt;/property&gt;&lt;/bean&gt;&lt;!-- spring中基于注解 的声明式事务控制配置步骤 1、配置事务管理器 2、开启spring对注解事务的支持 3、在需要事务支持的地方使用@Transactional注解 --&gt;&lt;!-- 配置事务管理器 --&gt;&lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;&gt;&lt;/property&gt;&lt;/bean&gt;&lt;!-- 开启spring对注解事务的支持--&gt;&lt;tx:annotation-driven&gt;&lt;/tx:annotation-driven&gt; 这是开启Spring事务需要在配置文件写的一些配置标签，主要就是最后一行，开启事务功能。​ 这个标签不是Spring的原生标签，所以需要额外的解析器来解析。在spring-tx包下的META-INF目录下，有一个spring.handlers配置文件，里面注册了事务标签的解析器。​ 我们来看下这个解析器。​ 12345678910111213141516171819202122232425/** * 这个类就是事务标签解析器，注意这里有一个init方法，加载一些当前解析器能够解析的bean标签，给解析器赋能。 */public class TxNamespaceHandler extends NamespaceHandlerSupport &#123; static final String TRANSACTION_MANAGER_ATTRIBUTE = &quot;transaction-manager&quot;; static final String DEFAULT_TRANSACTION_MANAGER_BEAN_NAME = &quot;transactionManager&quot;; static String getTransactionManagerName(Element element) &#123; return (element.hasAttribute(TRANSACTION_MANAGER_ATTRIBUTE) ? element.getAttribute(TRANSACTION_MANAGER_ATTRIBUTE) : DEFAULT_TRANSACTION_MANAGER_BEAN_NAME); &#125; @Override public void init() &#123; registerBeanDefinitionParser(&quot;advice&quot;, new TxAdviceBeanDefinitionParser()); //拉看这个标签的解析逻辑 registerBeanDefinitionParser(&quot;annotation-driven&quot;, new AnnotationDrivenBeanDefinitionParser()); registerBeanDefinitionParser(&quot;jta-transaction-manager&quot;, new JtaTransactionManagerBeanDefinitionParser()); &#125;&#125; 主要来看下**AnnotationDrivenBeanDefinitionParser**解析器的解析标签逻辑。​ 1234567891011121314151617181920@Override@Nullablepublic BeanDefinition parse(Element element, ParserContext parserContext) &#123; //向Spring容器注册一个BD：TransactionalEventListenerFactory registerTransactionalEventListenerFactory(parserContext); //从标签里面获取mode属性，一般我们不配置这个属性，所以走else的逻辑 String mode = element.getAttribute(&quot;mode&quot;); if (&quot;aspectj&quot;.equals(mode)) &#123; // mode=&quot;aspectj&quot; registerTransactionAspect(element, parserContext); if (ClassUtils.isPresent(&quot;jakarta.transaction.Transactional&quot;, getClass().getClassLoader())) &#123; registerJtaTransactionAspect(element, parserContext); &#125; &#125; else &#123; // mode=&quot;proxy&quot; 往下走 AopAutoProxyConfigurer.configureAutoProxyCreator(element, parserContext); &#125; return null;&#125; 继续往下走。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public static void configureAutoProxyCreator(Element element, ParserContext parserContext) &#123; //向Spring容器中注册BD -&gt; InfrastructureAdvisorAutoProxyCreator //BD的名称：internalAutoProxyCreator ，相当于往容器中加入了AOP的组件 AopNamespaceUtils.registerAutoProxyCreatorIfNecessary(parserContext, element); //事务切面的名称 ：org.springframework.transaction.config.internalTransactionAdvisor String txAdvisorBeanName = TransactionManagementConfigUtils.TRANSACTION_ADVISOR_BEAN_NAME; //如果容器中没有事务切面,就往容器中注册一个事务切面 if (!parserContext.getRegistry().containsBeanDefinition(txAdvisorBeanName)) &#123; //把标签包装成一个对象 Object eleSource = parserContext.extractSource(element); // Create the TransactionAttributeSource definition. RootBeanDefinition sourceDef = new RootBeanDefinition( &quot;org.springframework.transaction.annotation.AnnotationTransactionAttributeSource&quot;); sourceDef.setSource(eleSource); sourceDef.setRole(BeanDefinition.ROLE_INFRASTRUCTURE); String sourceName = parserContext.getReaderContext().registerWithGeneratedName(sourceDef); // Create the TransactionInterceptor definition. 事务增强器 RootBeanDefinition interceptorDef = new RootBeanDefinition(TransactionInterceptor.class); interceptorDef.setSource(eleSource); interceptorDef.setRole(BeanDefinition.ROLE_INFRASTRUCTURE); //往bd中注册信息 事务管理器 registerTransactionManager(element, interceptorDef); //往bd里面注册事务属性信息 interceptorDef.getPropertyValues().add(&quot;transactionAttributeSource&quot;, new RuntimeBeanReference(sourceName)); String interceptorName = parserContext.getReaderContext().registerWithGeneratedName(interceptorDef); // Create the TransactionAttributeSourceAdvisor definition. RootBeanDefinition advisorDef = new RootBeanDefinition(BeanFactoryTransactionAttributeSourceAdvisor.class); advisorDef.setSource(eleSource); advisorDef.setRole(BeanDefinition.ROLE_INFRASTRUCTURE); advisorDef.getPropertyValues().add(&quot;transactionAttributeSource&quot;, new RuntimeBeanReference(sourceName)); advisorDef.getPropertyValues().add(&quot;adviceBeanName&quot;, interceptorName); if (element.hasAttribute(&quot;order&quot;)) &#123; advisorDef.getPropertyValues().add(&quot;order&quot;, element.getAttribute(&quot;order&quot;)); &#125; parserContext.getRegistry().registerBeanDefinition(txAdvisorBeanName, advisorDef); CompositeComponentDefinition compositeDef = new CompositeComponentDefinition(element.getTagName(), eleSource); compositeDef.addNestedComponent(new BeanComponentDefinition(sourceDef, sourceName)); compositeDef.addNestedComponent(new BeanComponentDefinition(interceptorDef, interceptorName)); compositeDef.addNestedComponent(new BeanComponentDefinition(advisorDef, txAdvisorBeanName)); parserContext.registerComponent(compositeDef); &#125;&#125; 主要就是往容器中注册了几个bean。我们通过一张图来看一下。​ ​ 6.创建代理对象上面说到，解析事务标签的时候会往Spring容器中注入一个类**InfrastructureAdvisorAutoProxyCreator**。我们来看一下这个类的继承关系。​ \u0000这个是是一个抽象自动代理创建器，也是一个后置处理器。这里就和AOP的逻辑关联起来了。我们直接看AOP创建代理对象的后置处理器的方法。​ 1234567891011121314@Overridepublic Object postProcessAfterInitialization(@Nullable Object bean/*spring容器完全初始化完毕的对象*/, String beanName/*bean名称*/) &#123; if (bean != null) &#123; /*获取缓存建：大部分情况下都是beanName，如果是工厂bean对象，也有可能是 &amp; */ Object cacheKey = getCacheKey(bean.getClass(), beanName); /*防止重复代理某个bean实例*/ if (this.earlyProxyReferences.remove(cacheKey) != bean) &#123; /*判断是否需要包装 AOP操作的入口*/ return wrapIfNecessary(bean, beanName, cacheKey); &#125; &#125; return bean;&#125; 这里的逻辑前面已经解释过，直接往下走。​ 123456789101112131415161718192021222324252627282930313233343536373839404142protected Object wrapIfNecessary(Object bean, String beanName, Object cacheKey) &#123; /*条件一般不成立，因为正常情况下很少使用TargetSourceCreator 去创建对象。BeforeInstantiation阶段*/ if (StringUtils.hasLength(beanName) &amp;&amp; this.targetSourcedBeans.contains(beanName)) &#123; return bean; &#125; /* * 如果当前bean对象不需要增强处理 * 判断是在BeforeInstantiation阶段阶段做的 */ if (Boolean.FALSE.equals(this.advisedBeans.get(cacheKey))) &#123; return bean; &#125; /* * 条件一：判断当前bean类型是否是基础框架类型的实例，不能被增强 * 条件二：判断当前beanname是否是是忽略的bean，不需要被增强 */ if (isInfrastructureClass(bean.getClass()) || shouldSkip(bean.getClass(), beanName)) &#123; //进入这里表示不需要增强 this.advisedBeans.put(cacheKey, Boolean.FALSE); //直接返回上层 return bean; &#125; //查找适合当前类的通知 非常重要 ！！！ Object[] specificInterceptors = getAdvicesAndAdvisorsForBean(bean.getClass(), beanName, null); //判断当前查询出来的通知是不是空，如果不是空，说明走增强逻辑 if (specificInterceptors != DO_NOT_PROXY) &#123; //记得放在缓存true this.advisedBeans.put(cacheKey, Boolean.TRUE); /*真正去创建代理对象*/ Object proxy = createProxy( bean.getClass()/*目标对象*/, beanName/*beanName*/, specificInterceptors/*匹配当前目标对象class的拦截器*/, new SingletonTargetSource(bean)/*把当前bean进行了一个封装*/); //保存代理对象类型 this.proxyTypes.put(cacheKey, proxy.getClass()); //返回代理对象 return proxy; &#125; //执行到这里说明没查到这个类相关的通知，没法增强，直接返回 this.advisedBeans.put(cacheKey, Boolean.FALSE); return bean;&#125; 看一下**getAdvicesAndAdvisorsForBean()**。​ 12345678910111213@Override@Nullableprotected Object[] getAdvicesAndAdvisorsForBean( Class&lt;?&gt; beanClass, String beanName, @Nullable TargetSource targetSource) &#123; //查询合适当前类型的通知 List&lt;Advisor&gt; advisors = findEligibleAdvisors(beanClass, beanName); //通知为空返回空 if (advisors.isEmpty()) &#123; return DO_NOT_PROXY; &#125; //否则转成一个数组返回 return advisors.toArray();&#125; 继续往下。 123456789101112protected List&lt;Advisor&gt; findEligibleAdvisors(Class&lt;?&gt; beanClass, String beanName) &#123; /*获取到当前项目里面所有可以使用的增强器*/ List&lt;Advisor&gt; candidateAdvisors = findCandidateAdvisors(); /*将上一步获取到的全部增强器进行过滤，留下适合当前类的*/ List&lt;Advisor&gt; eligibleAdvisors = findAdvisorsThatCanApply(candidateAdvisors, beanClass, beanName); /*在这一步，会在index为0 的位置添加一个增强器*/ extendAdvisors(eligibleAdvisors); if (!eligibleAdvisors.isEmpty()) &#123; eligibleAdvisors = sortAdvisors(eligibleAdvisors); &#125; return eligibleAdvisors;&#125; 继续往下看过滤增强器的逻辑。 12345678910111213141516171819202122232425262728public static List&lt;Advisor&gt; findAdvisorsThatCanApply(List&lt;Advisor&gt; candidateAdvisors, Class&lt;?&gt; clazz) &#123; /*如果这个类全部可用的增强器为空，直接返回*/ if (candidateAdvisors.isEmpty()) &#123; return candidateAdvisors; &#125; //匹配当前class 的 advisor 信息 List&lt;Advisor&gt; eligibleAdvisors = new ArrayList&lt;&gt;(); //不考虑音阶增强 for (Advisor candidate : candidateAdvisors) &#123; if (candidate instanceof IntroductionAdvisor &amp;&amp; canApply(candidate, clazz)) &#123; eligibleAdvisors.add(candidate); &#125; &#125; //假设 值为false boolean hasIntroductions = !eligibleAdvisors.isEmpty(); for (Advisor candidate : candidateAdvisors) &#123; if (candidate instanceof IntroductionAdvisor) &#123; // already processed continue; &#125; //判断当前增强器是否匹配class if (canApply(candidate, clazz, hasIntroductions)) &#123; eligibleAdvisors.add(candidate); &#125; &#125; //返回的都是匹配当前class的advisor return eligibleAdvisors;&#125; 看这个重载的方法**canApply()**。​ 123456789101112131415public static boolean canApply(Advisor advisor, Class&lt;?&gt; targetClass, boolean hasIntroductions) &#123; if (advisor instanceof IntroductionAdvisor) &#123; return ((IntroductionAdvisor) advisor).getClassFilter().matches(targetClass); &#125; //大多数情况下是走这里，因为创建的增强器是 InstantiationModelAwarePointcutAdvisorImpl else if (advisor instanceof PointcutAdvisor) &#123; PointcutAdvisor pca = (PointcutAdvisor) advisor; //方法重载 return canApply(pca.getPointcut(), targetClass, hasIntroductions); &#125; else &#123; // It doesn&#x27;t have a pointcut so we assume it applies. return true; &#125;&#125; Spring事务导入到容器中的增强器是哪一个呢？回顾一下上面的图，**BeanFactoryTransactionAttributeSourceAdvisor**。​ 我们直接来到这个bean。​ 这个时候我们在来看**canApply()**。​ 123456789101112131415161718192021222324252627282930313233343536373839404142434445/*判断当前切点是否匹配当前class*/public static boolean canApply(Pointcut pc, Class&lt;?&gt; targetClass, boolean hasIntroductions) &#123; Assert.notNull(pc, &quot;Pointcut must not be null&quot;); //条件成立：说明当前class就不满足切点的定义 ，直接返回，因为后面是判断方法匹配的逻辑，直接返回 if (!pc.getClassFilter().matches(targetClass)) &#123; return false; &#125; //事务逻辑：BeanFactoryTransactionAttributeSourceAdvisor 里面有一个连接点。这个时候获取到的实际上就是事务增强器里面的连接点的方法匹配器。 //其他逻辑不用管，我们直接跳到方法匹配的逻辑。 //获取方法匹配器 MethodMatcher methodMatcher = pc.getMethodMatcher(); //如果是true，直接返回true，因为true不做判断，直接匹配所有方法 if (methodMatcher == MethodMatcher.TRUE) &#123; // No need to iterate the methods if we&#x27;re matching any method anyway... return true; &#125; //skip IntroductionAwareMethodMatcher introductionAwareMethodMatcher = null; if (methodMatcher instanceof IntroductionAwareMethodMatcher) &#123; introductionAwareMethodMatcher = (IntroductionAwareMethodMatcher) methodMatcher; &#125; //保存当前目标对象clazz + 目标对象 父类 爷爷类 ... 的接口 + 自身实现的接口 Set&lt;Class&lt;?&gt;&gt; classes = new LinkedHashSet&lt;&gt;(); //判断目标对象是不是代理对象，确保classes内存储的数据包括目标对象的class，而不是代理类class if (!Proxy.isProxyClass(targetClass)) &#123; classes.add(ClassUtils.getUserClass(targetClass)); &#125; classes.addAll(ClassUtils.getAllInterfacesForClassAsSet(targetClass)); //遍历classes，获取当前class定义的method，整个for循环会检查当前目标clazz 上级接口的所有方法 //看看是否会被方法匹配器匹配，如果有一个方法匹配成功，就说明目标class需要被AOP代理增强 for (Class&lt;?&gt; clazz : classes) &#123; Method[] methods = ReflectionUtils.getAllDeclaredMethods(clazz); for (Method method : methods) &#123; //事务注释：方法匹配：TransactionAttributeSourcePointcut 的 方法实现。 if (introductionAwareMethodMatcher != null ? introductionAwareMethodMatcher.matches(method, targetClass, hasIntroductions) : methodMatcher.matches(method, targetClass)) &#123; return true; &#125; &#125; &#125; //执行到这里，说明当前类的所有方法都没有匹配成功，当前类不需要AOP的增强。 return false;&#125; \u0000这里会调用到**TransactionAttributeSourcePointcut**类的**matches()**。我们来看一下。​ 12345@Overridepublic boolean matches(Method method, Class&lt;?&gt; targetClass) &#123; TransactionAttributeSource tas = getTransactionAttributeSource(); return (tas == null || tas.getTransactionAttribute(method, targetClass) != null);&#125; 继续往下看 **getTransactionAttribute()**， 来到了 **AbstractFallbackTransactionAttributeSource**。\u0000 12345678910111213141516171819202122232425262728293031323334353637383940414243@Override@Nullablepublic TransactionAttribute getTransactionAttribute(Method method, @Nullable Class&lt;?&gt; targetClass) &#123; if (method.getDeclaringClass() == Object.class) &#123; return null; &#125; //缓存的逻辑 // First, see if we have a cached value. Object cacheKey = getCacheKey(method, targetClass); TransactionAttribute cached = this.attributeCache.get(cacheKey); if (cached != null) &#123; // Value will either be canonical value indicating there is no transaction attribute, // or an actual transaction attribute. if (cached == NULL_TRANSACTION_ATTRIBUTE) &#123; return null; &#125; else &#123; return cached; &#125; &#125; else &#123; //真正去执行的逻辑。 // We need to work it out. 解析事务属性注解，获取事务属性信息。 TransactionAttribute txAttr = computeTransactionAttribute(method, targetClass); // Put it in the cache. 加缓存。 if (txAttr == null) &#123; this.attributeCache.put(cacheKey, NULL_TRANSACTION_ATTRIBUTE); &#125; else &#123; String methodIdentification = ClassUtils.getQualifiedMethodName(method, targetClass); if (txAttr instanceof DefaultTransactionAttribute) &#123; DefaultTransactionAttribute dta = (DefaultTransactionAttribute) txAttr; dta.setDescriptor(methodIdentification); dta.resolveAttributeStrings(this.embeddedValueResolver); &#125; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Adding transactional method &#x27;&quot; + methodIdentification + &quot;&#x27; with attribute: &quot; + txAttr); &#125; this.attributeCache.put(cacheKey, txAttr); &#125; return txAttr; &#125;&#125; 查找事物注解信息并加缓存。​ **computeTransactionAttribute()**\u0000 123456789101112131415161718192021222324252627282930313233343536373839404142434445@Nullableprotected TransactionAttribute computeTransactionAttribute(Method method, @Nullable Class&lt;?&gt; targetClass) &#123; // Don&#x27;t allow no-public methods as required. if (allowPublicMethodsOnly() &amp;&amp; !Modifier.isPublic(method.getModifiers())) &#123; return null; &#125; //获取目标类上的method，因为@Transactional注解可能是标记在接口上的 // The method may be on an interface, but we need attributes from the target class. // If the target class is null, the method will be unchanged. Method specificMethod = AopUtils.getMostSpecificMethod(method, targetClass); //获取目标类上的方法的注解信息。 // First try is the method in the target class. TransactionAttribute txAttr = findTransactionAttribute(specificMethod); if (txAttr != null) &#123; //获取到了就返回 return txAttr; &#125; // Second try is the transaction attribute on the target class. //到实现类的方法上去找 txAttr = findTransactionAttribute(specificMethod.getDeclaringClass()); if (txAttr != null &amp;&amp; ClassUtils.isUserLevelMethod(method)) &#123; //找到则返回 return txAttr; &#125; //此时说明注解是打在了接口上，到目标接口上提取method信息 if (specificMethod != method) &#123; // Fallback is to look at the original method. txAttr = findTransactionAttribute(method); if (txAttr != null) &#123; //找到则返回 return txAttr; &#125; //此时说明注解可能打在了目标接口的方法上，到接口的方法上提取注解信息 // Last fallback is the class of the original method. txAttr = findTransactionAttribute(method.getDeclaringClass()); if (txAttr != null &amp;&amp; ClassUtils.isUserLevelMethod(method)) &#123; return txAttr; &#125; &#125; //说明method并没有定义事务注解信息，不需要事务支持。 return null;&#125; 这个方法的主要逻辑就是获取到类，接口或者方法上的事务注解信息。我们来看一下具体的解析事务注解的逻辑。**findTransactionAttribute()**。​ 123456@Override@Nullableprotected TransactionAttribute findTransactionAttribute(Method method) &#123; return determineTransactionAttribute(method);&#125; 我们先来看这个类的属性，有一个事务注解解析器集合，这个集合是何时赋值的呢？是在创建这个类的时候，看构造器，我们只需要关注 **SpringTransactionAnnotationParser** 整一个解析器即可。 123456789101112131415161718private final Set&lt;TransactionAnnotationParser&gt; annotationParsers; public AnnotationTransactionAttributeSource(boolean publicMethodsOnly) &#123; this.publicMethodsOnly = publicMethodsOnly; if (jta12Present || ejb3Present) &#123; this.annotationParsers = new LinkedHashSet&lt;&gt;(4); this.annotationParsers.add(new SpringTransactionAnnotationParser()); if (jta12Present) &#123; this.annotationParsers.add(new JtaTransactionAnnotationParser()); &#125; if (ejb3Present) &#123; this.annotationParsers.add(new Ejb3TransactionAnnotationParser()); &#125; &#125; else &#123; this.annotationParsers = Collections.singleton(new SpringTransactionAnnotationParser()); &#125; &#125; 我们再回到提取事务注解信息的逻辑。**determineTransactionAttribute()**​ 1234567891011@Nullableprotected TransactionAttribute determineTransactionAttribute(AnnotatedElement element) &#123; for (TransactionAnnotationParser parser : this.annotationParsers) &#123; //我们来看 SpringTransactionAnnotationParser 里面的逻辑 TransactionAttribute attr = parser.parseTransactionAnnotation(element); if (attr != null) &#123; return attr; &#125; &#125; return null;&#125; 这里是循环所有的解析器，提取解析事务注解信息，我们来看 **SpringTransactionAnnotationParser** 里面的逻辑。​ 1234567891011121314@Override@Nullablepublic TransactionAttribute parseTransactionAnnotation(AnnotatedElement element) &#123; //从类或者方法上查找@Transactional这个注解 AnnotationAttributes attributes = AnnotatedElementUtils.findMergedAnnotationAttributes( element, Transactional.class, false, false); if (attributes != null) &#123; //解析注解阶段为事务属性TransactionAttribute return parseTransactionAnnotation(attributes); &#125; else &#123; return null; &#125;&#125; 首先是从类或者方法上查找到注解，然后通过**parseTransactionAnnotation()**解析注解为**TransactionAttribute**。​ 12345678910111213141516171819202122232425262728293031323334353637383940/** * 解析事务注解 * @param attributes * @return */protected TransactionAttribute parseTransactionAnnotation(AnnotationAttributes attributes) &#123; RuleBasedTransactionAttribute rbta = new RuleBasedTransactionAttribute(); Propagation propagation = attributes.getEnum(&quot;propagation&quot;); rbta.setPropagationBehavior(propagation.value()); Isolation isolation = attributes.getEnum(&quot;isolation&quot;); rbta.setIsolationLevel(isolation.value()); rbta.setTimeout(attributes.getNumber(&quot;timeout&quot;).intValue()); String timeoutString = attributes.getString(&quot;timeoutString&quot;); Assert.isTrue(!StringUtils.hasText(timeoutString) || rbta.getTimeout() &lt; 0, &quot;Specify &#x27;timeout&#x27; or &#x27;timeoutString&#x27;, not both&quot;); rbta.setTimeoutString(timeoutString); rbta.setReadOnly(attributes.getBoolean(&quot;readOnly&quot;)); rbta.setQualifier(attributes.getString(&quot;value&quot;)); rbta.setLabels(Arrays.asList(attributes.getStringArray(&quot;label&quot;))); List&lt;RollbackRuleAttribute&gt; rollbackRules = new ArrayList&lt;&gt;(); for (Class&lt;?&gt; rbRule : attributes.getClassArray(&quot;rollbackFor&quot;)) &#123; rollbackRules.add(new RollbackRuleAttribute(rbRule)); &#125; for (String rbRule : attributes.getStringArray(&quot;rollbackForClassName&quot;)) &#123; rollbackRules.add(new RollbackRuleAttribute(rbRule)); &#125; for (Class&lt;?&gt; rbRule : attributes.getClassArray(&quot;noRollbackFor&quot;)) &#123; rollbackRules.add(new NoRollbackRuleAttribute(rbRule)); &#125; for (String rbRule : attributes.getStringArray(&quot;noRollbackForClassName&quot;)) &#123; rollbackRules.add(new NoRollbackRuleAttribute(rbRule)); &#125; rbta.setRollbackRules(rollbackRules); return rbta;&#125; 这里就是具体解析事务注解信息的逻辑。​ 阶段性梳理一下，这里我们匹配到了事务相关的增强器，接下来我们要去为当前加了Transaction注解的bean创建代理对象。 至此，我们分析完了解析事务标签，创建打了Transaction注解的bean创建代理对象的源码流程分析，接下来就是分析，需要被事务增强的目标方法执行过程中，事务增强器是如何对目标方法加上事务的。 ​","categories":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/tags/Spring/"}]},{"title":"Spring[十三]Ioc整合Aop创建代理对象","slug":"Spring/Spring[十三]Ioc整合Aop创建代理对象","date":"2022-01-11T06:10:54.686Z","updated":"2022-01-11T06:17:27.836Z","comments":true,"path":"2022/01/11/Spring/Spring[十三]Ioc整合Aop创建代理对象/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/Spring/Spring[%E5%8D%81%E4%B8%89]Ioc%E6%95%B4%E5%90%88Aop%E5%88%9B%E5%BB%BA%E4%BB%A3%E7%90%86%E5%AF%B9%E8%B1%A1/","excerpt":"","text":"临时补充一篇内容，昨晚忽然看源码发现忽略了这Ioc整合Aop创建代理对象的过程，本篇我们来补充一下核心的逻辑。 1.解析xml标签之前我们说过了注解版开发AOP功能，如果是xml版的需要在配置文件配置_**&lt;aop:aspectj-autoproxy /&gt;**_。​ 我们先找到这个标签的解析器**AspectJAutoProxyBeanDefinitionParser**。​ 123456789101112@Override@Nullable/** * @param element 包装 &lt;aop:aspectj-autoproxy /&gt;标签数据 * @param parserContext 它持有一个 readerContext ， readerContext里面 有持有一个 registry ，也就是 bf。 */public BeanDefinition parse(Element element, ParserContext parserContext) &#123; /*解析标签，创建一个自动代理创建器*/ AopNamespaceUtils.registerAspectJAnnotationAutoProxyCreatorIfNecessary(parserContext, element); extendBeanDefinition(element, parserContext); return null;&#125; \u0000\u0000这个方法就是解析标签，创建抽象自动代理创建器，注册到容器中，是不是和前面的流程很像？​ 12345678910public static void registerAspectJAnnotationAutoProxyCreatorIfNecessary( ParserContext parserContext, Element sourceElement) &#123; /*拿到 bf ， 包装标签*/ BeanDefinition beanDefinition = AopConfigUtils.registerAspectJAnnotationAutoProxyCreatorIfNecessary( parserContext.getRegistry(), parserContext.extractSource(sourceElement)); /*执行到这里，spring容器中已经有了aop相关的bd信息，接下来的逻辑属于扩展 。 * 这里的主要逻辑就是对aop标签上可配置的属性进行解析。*/ useClassProxyingIfNecessary(parserContext.getRegistry(), sourceElement); registerComponentIfNecessary(beanDefinition, parserContext);&#125; 先看一下解析标签属性的逻辑**useClassProxyingIfNecessary()** &amp; **registerComponentIfNecessary()**。​ 123456789101112131415private static void useClassProxyingIfNecessary(BeanDefinitionRegistry registry, @Nullable Element sourceElement) &#123; /*属于扩展逻辑，判断aop的标签上有没有配置proxy-target-class属性，这个属性默认是关闭的，如果开启的话，目标对象不管有没有实现接口，都会使用cglib的代理方式*/ if (sourceElement != null) &#123; boolean proxyTargetClass = Boolean.parseBoolean(sourceElement.getAttribute(PROXY_TARGET_CLASS_ATTRIBUTE)); if (proxyTargetClass) &#123; /*这里的逻辑就是假如你配置了这个属性是true，就会拿到bd信息，往bd信息里面在添加一个属性，设置成true。*/ AopConfigUtils.forceAutoProxyCreatorToUseClassProxying(registry); &#125; /*exposeProxy：也是可以在aop标签配置的属性，就是判断当前代理对象是否要暴露在aop上下文，方便代理对象内部的真实对象拿到代理对象。*/ boolean exposeProxy = Boolean.parseBoolean(sourceElement.getAttribute(EXPOSE_PROXY_ATTRIBUTE)); if (exposeProxy) &#123; AopConfigUtils.forceAutoProxyCreatorToExposeProxy(registry); &#125; &#125;&#125; 这个方法主要就是决定用哪种方式创建代理对象。​ 123456private static void registerComponentIfNecessary(@Nullable BeanDefinition beanDefinition, ParserContext parserContext) &#123; if (beanDefinition != null) &#123; parserContext.registerComponent( new BeanComponentDefinition(beanDefinition, AopConfigUtils.AUTO_PROXY_CREATOR_BEAN_NAME)); &#125;&#125; 这个就是判断是不是需要将抽象自动代理创建器注册到解析器的上下文。​ 2.抽象自动代理创建器我们接下来回归主线，来看**AopConfigUtils.registerAspectJAnnotationAutoProxyCreatorIfNecessary()**的逻辑。​ 12345678910@Nullablepublic static BeanDefinition registerAspectJAnnotationAutoProxyCreatorIfNecessary( BeanDefinitionRegistry registry, @Nullable Object source) &#123; /* * 参数一：固定类型 * 参数二：spring容器 * 参数三：标签 * */ return registerOrEscalateApcAsRequired(AnnotationAwareAspectJAutoProxyCreator.class, registry, source);&#125; 再往下跟​ 1234567891011121314151617181920212223242526@Nullableprivate static BeanDefinition registerOrEscalateApcAsRequired( Class&lt;?&gt; cls, BeanDefinitionRegistry registry, @Nullable Object source) &#123; Assert.notNull(registry, &quot;BeanDefinitionRegistry must not be null&quot;); /*判断容器里面有没有这个名字的bean，如果有的话就拿出来*/ if (registry.containsBeanDefinition(AUTO_PROXY_CREATOR_BEAN_NAME)) &#123; BeanDefinition apcDefinition = registry.getBeanDefinition(AUTO_PROXY_CREATOR_BEAN_NAME); if (!cls.getName().equals(apcDefinition.getBeanClassName())) &#123; int currentPriority = findPriorityForClass(apcDefinition.getBeanClassName()); int requiredPriority = findPriorityForClass(cls); if (currentPriority &lt; requiredPriority) &#123; apcDefinition.setBeanClassName(cls.getName()); &#125; &#125; return null; &#125; /*通常情况下，其实走不到上面的逻辑，除非自己手写了aop*/ /*创建一个bd，并且注册到容器中*/ RootBeanDefinition beanDefinition = new RootBeanDefinition(cls); beanDefinition.setSource(source); beanDefinition.getPropertyValues().add(&quot;order&quot;, Ordered.HIGHEST_PRECEDENCE); beanDefinition.setRole(BeanDefinition.ROLE_INFRASTRUCTURE); registry.registerBeanDefinition(AUTO_PROXY_CREATOR_BEAN_NAME, beanDefinition); return beanDefinition;&#125; 这里就是给容器中注册了一个**beanDefinition**，这个**beanDefinition**就是**AbstractAutoProxyCreator**。​ 前面在注解流程里面我们分析了，Ioc在通过**getBean()**创建单实例bean对象的时候，执行到**initializeBean()**的时候，会执行bean的后置处理器，我们再来回顾一下这里的逻辑。​ 12345678910111213141516171819202122232425262728293031/** 初始化给定的 bean 实例，应用工厂回调以及 init 方法和 bean 后处理器。* 从createBean调用传统定义的 bean，从initializeBean调用现有 bean 实例。* */protected Object initializeBean(String beanName, Object bean, @Nullable RootBeanDefinition mbd) &#123; /*检查当前bean是否实现了aware接口，再具体判断实现的哪个aware接口，做一些赋能操作。*/ invokeAwareMethods(beanName, bean); Object wrappedBean = bean; if (mbd == null || !mbd.isSynthetic()) &#123; /*初始化之前，后置处理器的调用点*/ wrappedBean = applyBeanPostProcessorsBeforeInitialization(wrappedBean, beanName); &#125; try &#123; /*执行初始化方法*/ invokeInitMethods(beanName, wrappedBean, mbd); &#125; catch (Throwable ex) &#123; throw new BeanCreationException( (mbd != null ? mbd.getResourceDescription() : null), beanName, &quot;Invocation of init method failed&quot;, ex); &#125; if (mbd == null || !mbd.isSynthetic()) &#123; /*初始化后的后置处理器执行点*/ /*典型应用：AOP的具体实现*/ wrappedBean = applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName); &#125; return wrappedBean;&#125; \u0000通常都是在执行**invokeInitMethods()**,之后的后置处理器的after方法返回一个代理对象，我们继续往下走。​ 1234567891011121314151617@Overridepublic Object applyBeanPostProcessorsAfterInitialization(Object existingBean, String beanName) throws BeansException &#123; Object result = existingBean; for (BeanPostProcessor processor : getBeanPostProcessors()) &#123; Object current = processor.postProcessAfterInitialization(result, beanName); /*注意： * 一旦某个后置处理器返回的结果为空 * 就返回上一个后置处理器的结果，后面的后置处理器方法不在执行*/ if (current == null) &#123; return result; &#125; result = current; &#125; return result;&#125; 这里是后处理器的逻辑。在这里就会调用到我们前面注册到容器中的**AbstractAutoProxyCreator**。​ 1234567891011121314@Overridepublic Object postProcessAfterInitialization(@Nullable Object bean/*spring容器完全初始化完毕的对象*/, String beanName/*bean名称*/) &#123; if (bean != null) &#123; /*获取缓存建：大部分情况下都是beanName，如果是工厂bean对象，也有可能是 &amp; */ Object cacheKey = getCacheKey(bean.getClass(), beanName); /*防止重复代理某个bean实例*/ if (this.earlyProxyReferences.remove(cacheKey) != bean) &#123; /*判断是否需要包装 AOP操作的入口*/ return wrapIfNecessary(bean, beanName, cacheKey); &#125; &#125; return bean;&#125; 构建缓存key 判断如果**earlyProxyReferences**里面移除的对象和当前完全初始化好的对象不是同一个，说明什么？说明有其他地方通过**FactoryBean**的**getObject()**创建了当前bean的代理对象，所以需要移除。 判断是否需要包装**wrapIfNecessary(bean, beanName, cacheKey)** ​ 3.earlyProxyReferences我们先来分析下这个属性 1private final Map&lt;Object, Object&gt; earlyProxyReferences = new ConcurrentHashMap&lt;&gt;(16); 假设A，B两个类现在发生了循环依赖，创建A的时候发现需要B对象，然后A会把自己的代理对象放到三级缓存，然后递归去创建B对象。​ 1addSingletonFactory(beanName, () -&gt; getEarlyBeanReference(beanName, mbd, bean)) 就是这行代码，前面我们是有讲过的，然后我们继续往下跟。​ 1234567891011121314151617181920/** * 获取早期bean实例对象的引用，用来解决循环依赖。 * 这里说明了一个问题：为什么是三级缓存不是二级缓存。 * Obtain a reference for early access to the specified bean, * typically for the purpose of resolving a circular reference. * @param beanName the name of the bean (for error handling purposes) * @param mbd the merged bean definition for the bean * @param bean the raw bean instance * @return the object to expose as bean reference */protected Object getEarlyBeanReference(String beanName, RootBeanDefinition mbd, Object bean) &#123; Object exposedObject = bean; if (!mbd.isSynthetic() &amp;&amp; hasInstantiationAwareBeanPostProcessors()) &#123; for (SmartInstantiationAwareBeanPostProcessor bp : getBeanPostProcessorCache().smartInstantiationAware) &#123; /*判断要返回的早期单实例对象是否需要增强，如果需要增强，就进行包装，返回包装好的对象*/ exposedObject = bp.getEarlyBeanReference(exposedObject, beanName); &#125; &#125; return exposedObject;&#125; 这里面会遍历所有的**SmartInstantiationAwareBeanPostProcessor**，判断是否需要返回增强的对象。​ 而我们的**AbstractAutoProxyCreator**恰恰实现了**SmartInstantiationAwareBeanPostProcessor**。\u0000所以**getEarlyBeanReference()**会执行**AbstractAutoProxyCreator**里面的。​ 123456@Overridepublic Object getEarlyBeanReference(Object bean, String beanName) &#123; Object cacheKey = getCacheKey(bean.getClass(), beanName); this.earlyProxyReferences.put(cacheKey, bean); return wrapIfNecessary(bean, beanName, cacheKey);&#125; 这里会把目标对象本身A放入到缓存中，返回A的早期代理对象。​ 在我们递归创建B的的时候，为B进行属性赋值的时候，回去从缓存拿A。​ 1234567891011121314151617/*从一级缓存拿*/singletonObject = this.singletonObjects.get(beanName);if (singletonObject == null) &#123; /*从二级缓存拿*/ singletonObject = this.earlySingletonObjects.get(beanName); if (singletonObject == null) &#123; /*从三级缓存拿*/ ObjectFactory&lt;?&gt; singletonFactory = this.singletonFactories.get(beanName); /*条件成立：说明第三级缓存有数据。这里就涉及到了缓存的升级 ，很简单 ，从三级挪到二级 ，再反手干掉三级的。*/ if (singletonFactory != null) &#123; singletonObject = singletonFactory.getObject(); this.earlySingletonObjects.put(beanName, singletonObject); this.singletonFactories.remove(beanName); &#125; &#125;&#125; ​ 这里第二次调用到了**singletonFactory.getObject()**，（此时是B创建过程中获取A），然后又会走到**getEarlyBeanReference()**。​ 123456@Overridepublic Object getEarlyBeanReference(Object bean, String beanName) &#123; Object cacheKey = getCacheKey(bean.getClass(), beanName); this.earlyProxyReferences.put(cacheKey, bean); return wrapIfNecessary(bean, beanName, cacheKey);&#125; 这个时候，会将A的代理对象放到缓存，返回A的代理对象。​ 所以当我们递归创建B后，回头用B为A属性赋值之后，执行A的初始化方法，就会走到抽象自动代理创建器的后置处理器逻辑，然后再来看这个方法​ 1234567891011121314@Overridepublic Object postProcessAfterInitialization(@Nullable Object bean/*A的代理对象*/, String beanName/*bean名称*/) &#123; if (bean != null) &#123; /*获取缓存建：大部分情况下都是beanName，如果是工厂bean对象，也有可能是 &amp; */ Object cacheKey = getCacheKey(bean.getClass(), beanName); /*防止重复代理某个bean实例*/ if (this.earlyProxyReferences.remove(cacheKey)/*A本身*/ != bean) &#123; /*判断是否需要包装 AOP操作的入口*/ return wrapIfNecessary(bean, beanName, cacheKey); &#125; &#125; return bean;&#125; **earlyProxyReferences.remove(cacheKey) != bean**，因为此时缓存里面的是A的代理对象，传进来的也是A的代理对象，所以判断相等，直接返回，不需要再次创建A的代理对象。​ 至此，这里就解释清楚了。​ 接下来，我们来看代理对象的创建过程。**wrapIfNecessary()**​ 4.是否需要创建代理对象123456789101112131415161718192021222324252627282930313233343536373839404142protected Object wrapIfNecessary(Object bean, String beanName, Object cacheKey) &#123; /*条件一般不成立，因为正常情况下很少使用TargetSourceCreator 去创建对象。BeforeInstantiation阶段*/ if (StringUtils.hasLength(beanName) &amp;&amp; this.targetSourcedBeans.contains(beanName)) &#123; return bean; &#125; /* * 如果当前bean对象不需要增强处理 * 判断是在BeforeInstantiation阶段阶段做的 */ if (Boolean.FALSE.equals(this.advisedBeans.get(cacheKey))) &#123; return bean; &#125; /* * 条件一：判断当前bean类型是否是基础框架类型的实例，不能被增强 * 条件二：判断当前beanname是否是是忽略的bean，不需要被增强 */ if (isInfrastructureClass(bean.getClass()) || shouldSkip(bean.getClass(), beanName)) &#123; //进入这里表示不需要增强 this.advisedBeans.put(cacheKey, Boolean.FALSE); //直接返回上层 return bean; &#125; //查找适合当前类的通知 非常重要 ！！！ Object[] specificInterceptors = getAdvicesAndAdvisorsForBean(bean.getClass(), beanName, null); //判断当前查询出来的通知是不是空，如果不是空，说明走增强逻辑 if (specificInterceptors != DO_NOT_PROXY) &#123; //记得放在缓存true this.advisedBeans.put(cacheKey, Boolean.TRUE); /*真正去创建代理对象*/ Object proxy = createProxy( bean.getClass()/*目标对象*/, beanName/*beanName*/, specificInterceptors/*匹配当前目标对象class的拦截器*/, new SingletonTargetSource(bean)/*把当前bean进行了一个封装*/); //保存代理对象类型 this.proxyTypes.put(cacheKey, proxy.getClass()); //返回代理对象 return proxy; &#125; //执行到这里说明没查到这个类相关的通知，没法增强，直接返回 this.advisedBeans.put(cacheKey, Boolean.FALSE); return bean;&#125; ​ 通过**getAdvicesAndAdvisorsForBean()**查找适合当前类的通知。​ 然后通过**createProxy()**去创建代理对象，最终返回代理对象。​ 5.查找通知12345678910111213@Override@Nullableprotected Object[] getAdvicesAndAdvisorsForBean( Class&lt;?&gt; beanClass, String beanName, @Nullable TargetSource targetSource) &#123; //查询合适当前类型的通知 List&lt;Advisor&gt; advisors = findEligibleAdvisors(beanClass, beanName); //通知为空返回空 if (advisors.isEmpty()) &#123; return DO_NOT_PROXY; &#125; //否则转成一个数组返回 return advisors.toArray();&#125; 继续往下看**findEligibleAdvisors()**。\u0000 123456789101112protected List&lt;Advisor&gt; findEligibleAdvisors(Class&lt;?&gt; beanClass, String beanName) &#123; /*获取到当前项目里面所有可以使用的增强器*/ List&lt;Advisor&gt; candidateAdvisors = findCandidateAdvisors(); /*将上一步获取到的全部增强器进行过滤，留下适合当前类的*/ List&lt;Advisor&gt; eligibleAdvisors = findAdvisorsThatCanApply(candidateAdvisors, beanClass, beanName); /*在这一步，会在index为0 的位置添加一个增强器*/ extendAdvisors(eligibleAdvisors); if (!eligibleAdvisors.isEmpty()) &#123; eligibleAdvisors = sortAdvisors(eligibleAdvisors); &#125; return eligibleAdvisors;&#125; 获取全部增强器，过滤排序，返回。​ 先看一下如何获取的。​ 123456789protected List&lt;Advisor&gt; findCandidateAdvisors() &#123; Assert.state(this.advisorRetrievalHelper != null, &quot;No BeanFactoryAdvisorRetrievalHelper available&quot;); //查询出来通过 bean 的方式配置的 增强器 /* * advisorRetrievalHelper 是怎么初始化的？ * 这个类实现了 beanFactoryAware接口 ，在初始化beanFactory的时候， 创建了一个 helper 对象 */ return this.advisorRetrievalHelper.findAdvisorBeans();&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public List&lt;Advisor&gt; findAdvisorBeans() &#123; // Determine list of advisor bean names, if not cached already. String[] advisorNames = this.cachedAdvisorBeanNames; if (advisorNames == null) &#123; //通过 bf 查询出来 bd 配置的 class 是 增强器的 子类 的beanName advisorNames = BeanFactoryUtils.beanNamesForTypeIncludingAncestors( this.beanFactory, Advisor.class, true, false); this.cachedAdvisorBeanNames = advisorNames; &#125; //要是没拿到，就直接返回，没必要往下走了 if (advisorNames.length == 0) &#123; return new ArrayList&lt;&gt;(); &#125; List&lt;Advisor&gt; advisors = new ArrayList&lt;&gt;(); for (String name : advisorNames) &#123; //注意：当前的helper是适配器包装的，真正的逻辑在适配器里面，但是实际上，适配器里面的这个方法也是返回 true //这个方法的作用是判断当前给定名字的bean是否合格 if (isEligibleBean(name)) &#123; //当前bean如果是在创建中的话，那就打印个日志，记录下 if (this.beanFactory.isCurrentlyInCreation(name)) &#123; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Skipping currently created advisor &#x27;&quot; + name + &quot;&#x27;&quot;); &#125; &#125; else &#123; try &#123; //从 bf 查询出来 当前这个名字和类型的增强器实例加入到增强器列表中 advisors.add(this.beanFactory.getBean(name, Advisor.class)); &#125; catch (BeanCreationException ex) &#123; Throwable rootCause = ex.getMostSpecificCause(); if (rootCause instanceof BeanCurrentlyInCreationException) &#123; BeanCreationException bce = (BeanCreationException) rootCause; String bceBeanName = bce.getBeanName(); if (bceBeanName != null &amp;&amp; this.beanFactory.isCurrentlyInCreation(bceBeanName)) &#123; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Skipping advisor &#x27;&quot; + name + &quot;&#x27; with dependency on currently created bean: &quot; + ex.getMessage()); &#125; // Ignore: indicates a reference back to the bean we&#x27;re trying to advise. // We want to find advisors other than the currently created bean itself. continue; &#125; &#125; throw ex; &#125; &#125; &#125; &#125; return advisors;&#125; 再看一下如何过滤出当前类需要的增强器的。​ 123456789101112protected List&lt;Advisor&gt; findAdvisorsThatCanApply( List&lt;Advisor&gt; candidateAdvisors, Class&lt;?&gt; beanClass, String beanName) &#123; ProxyCreationContext.setCurrentProxiedBeanName(beanName); try &#123; /*核心逻辑*/ return AopUtils.findAdvisorsThatCanApply(candidateAdvisors, beanClass); &#125; finally &#123; ProxyCreationContext.setCurrentProxiedBeanName(null); &#125;&#125; 继续往下走 12345678910111213141516171819202122232425262728public static List&lt;Advisor&gt; findAdvisorsThatCanApply(List&lt;Advisor&gt; candidateAdvisors, Class&lt;?&gt; clazz) &#123; /*如果这个类全部可用的增强器为空，直接返回*/ if (candidateAdvisors.isEmpty()) &#123; return candidateAdvisors; &#125; //匹配当前class 的 advisor 信息 List&lt;Advisor&gt; eligibleAdvisors = new ArrayList&lt;&gt;(); //不考虑音阶增强 for (Advisor candidate : candidateAdvisors) &#123; if (candidate instanceof IntroductionAdvisor &amp;&amp; canApply(candidate, clazz)) &#123; eligibleAdvisors.add(candidate); &#125; &#125; //假设 值为false boolean hasIntroductions = !eligibleAdvisors.isEmpty(); for (Advisor candidate : candidateAdvisors) &#123; if (candidate instanceof IntroductionAdvisor) &#123; // already processed continue; &#125; //判断当前增强器是否匹配class if (canApply(candidate, clazz, hasIntroductions)) &#123; eligibleAdvisors.add(candidate); &#125; &#125; //返回的都是匹配当前class的advisor return eligibleAdvisors;&#125; 看一下_**canApply()**_。​ 123456789101112131415public static boolean canApply(Advisor advisor, Class&lt;?&gt; targetClass, boolean hasIntroductions) &#123; if (advisor instanceof IntroductionAdvisor) &#123; return ((IntroductionAdvisor) advisor).getClassFilter().matches(targetClass); &#125; //大多数情况下是走这里，因为创建的增强器是 InstantiationModelAwarePointcutAdvisorImpl else if (advisor instanceof PointcutAdvisor) &#123; PointcutAdvisor pca = (PointcutAdvisor) advisor; //方法重载 return canApply(pca.getPointcut(), targetClass, hasIntroductions); &#125; else &#123; // It doesn&#x27;t have a pointcut so we assume it applies. return true; &#125;&#125; \u0000方法重载​ 123456789101112131415161718192021222324252627282930313233343536373839404142/*判断当前切点是否匹配当前class*/public static boolean canApply(Pointcut pc, Class&lt;?&gt; targetClass, boolean hasIntroductions) &#123; Assert.notNull(pc, &quot;Pointcut must not be null&quot;); //条件成立：说明当前class就不满足切点的定义 ，直接返回，因为后面是判断方法匹配的逻辑，直接返回 if (!pc.getClassFilter().matches(targetClass)) &#123; return false; &#125; //获取方法匹配器 MethodMatcher methodMatcher = pc.getMethodMatcher(); //如果是true，直接返回true，因为true不做判断，直接匹配所有方法 if (methodMatcher == MethodMatcher.TRUE) &#123; // No need to iterate the methods if we&#x27;re matching any method anyway... return true; &#125; //skip IntroductionAwareMethodMatcher introductionAwareMethodMatcher = null; if (methodMatcher instanceof IntroductionAwareMethodMatcher) &#123; introductionAwareMethodMatcher = (IntroductionAwareMethodMatcher) methodMatcher; &#125; //保存当前目标对象clazz + 目标对象 父类 爷爷类 ... 的接口 + 自身实现的接口 Set&lt;Class&lt;?&gt;&gt; classes = new LinkedHashSet&lt;&gt;(); //判断目标对象是不是代理对象，确保classes内存储的数据包括目标对象的class，而不是代理类class if (!Proxy.isProxyClass(targetClass)) &#123; classes.add(ClassUtils.getUserClass(targetClass)); &#125; classes.addAll(ClassUtils.getAllInterfacesForClassAsSet(targetClass)); //遍历classes，获取当前class定义的method，整个for循环会检查当前目标clazz 上级接口的所有方法 //看看是否会被方法匹配器匹配，如果有一个方法匹配成功，就说明目标class需要被AOP代理增强 for (Class&lt;?&gt; clazz : classes) &#123; Method[] methods = ReflectionUtils.getAllDeclaredMethods(clazz); for (Method method : methods) &#123; if (introductionAwareMethodMatcher != null ? introductionAwareMethodMatcher.matches(method, targetClass, hasIntroductions) : methodMatcher.matches(method, targetClass)) &#123; return true; &#125; &#125; &#125; //执行到这里，说明当前类的所有方法都没有匹配成功，当前类不需要AOP的增强。 return false;&#125; **matches()**上一篇已经分析过了，这里不再赘述。​ 查找到匹配当前类的切面以后，我们再看一看如何创建的代理对象。​ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556protected Object createProxy(Class&lt;?&gt; beanClass, @Nullable String beanName, @Nullable Object[] specificInterceptors, TargetSource targetSource) &#123; /*类型断言，成立*/ if (this.beanFactory instanceof ConfigurableListableBeanFactory) &#123; /*给当前的bd添加了一个属性*/ AutoProxyUtils.exposeTargetClass((ConfigurableListableBeanFactory) this.beanFactory, beanName, beanClass); &#125; //创建一个代理对象的工厂，他必须持有创建AOP代理class的生产资料 // 1. 目标对象 //2. 增强信息 ProxyFactory proxyFactory = new ProxyFactory(); //从当前类赋值一些信息到工厂 proxyFactory.copyFrom(this); //说明咱们有使用 xml 配置修改过 aop ProxyTargetClass if (proxyFactory.isProxyTargetClass()) &#123; // Explicit handling of JDK proxy targets (for introduction advice scenarios) if (Proxy.isProxyClass(beanClass)) &#123; // Must allow for introductions; can&#x27;t just set interfaces to the proxy&#x27;s interfaces only. for (Class&lt;?&gt; ifc : beanClass.getInterfaces()) &#123; proxyFactory.addInterface(ifc); &#125; &#125; &#125; else &#123; // No proxyTargetClass flag enforced, let&#x27;s apply our default checks... //如果bd定义内有 preserverTargetClass = true ，那么bd对应的class创建代理对象的时候 //使用cglib，否则还得继续判断，判断需要代理的接口 if (shouldProxyTargetClass(beanClass, beanName)) &#123; proxyFactory.setProxyTargetClass(true); &#125; else &#123; //评估需要代理的接口，判断使用什么代理 evaluateProxyInterfaces(beanClass, proxyFactory); &#125; &#125; //构建切面集合 Advisor[] advisors = buildAdvisors(beanName, specificInterceptors); proxyFactory.addAdvisors(advisors); proxyFactory.setTargetSource(targetSource); //扩展点 customizeProxyFactory(proxyFactory); proxyFactory.setFrozen(this.freezeProxy); if (advisorsPreFiltered()) &#123; //传递给代理工厂的这些增强器信息做过基础匹配，也就是classFilter匹配 proxyFactory.setPreFiltered(true); &#125; // ClassLoader classLoader = getProxyClassLoader(); if (classLoader instanceof SmartClassLoader &amp;&amp; classLoader != beanClass.getClassLoader()) &#123; classLoader = ((SmartClassLoader) classLoader).getOriginalClassLoader(); &#125; return proxyFactory.getProxy(classLoader);&#125; 最终还是调用了**proxyFactory.getProxy(classLoader)**。​ 至此，整个AOP的逻辑算是正式完成，下一篇我将带领大家一起分析下事务的源码。​ ​","categories":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/tags/Spring/"}]},{"title":"Spring[十二]Aop源码分析","slug":"Spring/Spring[十二]Aop源码分析","date":"2022-01-11T06:10:47.608Z","updated":"2022-01-11T06:17:03.028Z","comments":true,"path":"2022/01/11/Spring/Spring[十二]Aop源码分析/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/Spring/Spring[%E5%8D%81%E4%BA%8C]Aop%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","excerpt":"","text":"上一篇通过简单的分析，我们大概清楚了整个AOP代码实现的大体流程。本篇我们将从代码入手，一点点分解AOP的实现代码。 1.使用AOP的代码先看一段代码，看看如何使用AOP的。​ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677public class CodeMain &#123; public static void main(String[] args) &#123; //默认代理所有方法 proxyAllMethods(); //定制代理 //proxyMethod(); &#125; /** * 默认情况。代理所有方法 */ private static void proxyAllMethods()&#123; //1.创建被代理对象 Tiger tiger = new Tiger(); //2.创建spring代理工厂对象 ProxyFactory //proxyFactory 是 config + factory 的存在 持有 aop 操作 的 所有的生产资料 ProxyFactory proxyFactory = new ProxyFactory(tiger); //3. 添加拦截器 proxyFactory.addAdvice(new MethodInterceptor01()); proxyFactory.addAdvice(new MethodInterceptor02()); //4.获取代理对象，分析如何获取的代理对象？ Animals proxy = (Animals) proxyFactory.getProxy(); proxy.eat(&quot;人&quot;); System.out.println(&quot;===================================&quot;); proxy.run(); &#125; /** * 代理指定的方法 */ private static void proxyMethod()&#123; //1.创建被代理对象 Tiger tiger = new Tiger(); //2.创建spring代理工厂对象 ProxyFactory //proxyFactory 是 config + factory 的存在 持有 aop 操作 的 所有的生产资料 ProxyFactory proxyFactory = new ProxyFactory(tiger); //3.添加方法拦截 MyPointCut pointCut = new MyPointCut(); proxyFactory.addAdvisor(new DefaultPointcutAdvisor(pointCut,new MethodInterceptor01())); proxyFactory.addAdvisor(new DefaultPointcutAdvisor(pointCut,new MethodInterceptor02())); //4.获取代理对象 分析如何获得代理对象 Animals proxy = (Animals) proxyFactory.getProxy(); proxy.eat(&quot;人肉&quot;); System.out.println(&quot;===================================&quot;); proxy.run(); &#125; /** * 方法拦截器 */ private static class MethodInterceptor01 implements MethodInterceptor &#123; @Nullable @Override public Object invoke(@NonNull MethodInvocation invocation) throws Throwable&#123; System.out.println(&quot;method interceptor begin!----1&quot;); Object result = invocation.proceed(); System.out.println(&quot;method interceptor end!---4&quot;); return result; &#125; &#125; private static class MethodInterceptor02 implements MethodInterceptor &#123; @Nullable @Override public Object invoke(@NonNull MethodInvocation invocation) throws Throwable&#123; System.out.println(&quot;method interceptor begin!----2&quot;); Object result = invocation.proceed(); System.out.println(&quot;method interceptor end!----3&quot;); return result; &#125; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445interface Animals&#123; void eat(String food); void run();&#125;class Tiger implements Animals&#123; @Override public void eat(String food) &#123; System.out.println(&quot;老虎吃&quot;+food); &#125; @Override public void run() &#123; System.out.println(&quot;跑的贼快！&quot;); &#125;&#125;public class MyPointCut implements Pointcut &#123; @Override public ClassFilter getClassFilter() &#123; return clazz -&gt; true; &#125; @Override public MethodMatcher getMethodMatcher() &#123; return new MethodMatcher() &#123; @Override public boolean matches(Method method, Class&lt;?&gt; targetClass) &#123; return method.getName().equals(&quot;eat&quot;); &#125; @Override public boolean isRuntime() &#123; return false; &#125; @Override public boolean matches(Method method, Class&lt;?&gt; targetClass, Object... args) &#123; return false; &#125; &#125;; &#125;&#125; 这是使用AOP的两种方式，默认是代理全部方法，另一个则是代理指定的方法。接下来，我们来对源码进行分析。​ 2.抓手123456789101112131415private static void proxyAllMethods()&#123; //1.创建被代理对象 Tiger tiger = new Tiger(); //2.创建spring代理工厂对象 ProxyFactory //proxyFactory 是 config + factory 的存在 持有 aop 操作 的 所有的生产资料 ProxyFactory proxyFactory = new ProxyFactory(tiger); //3. 添加拦截器 proxyFactory.addAdvice(new MethodInterceptor01()); proxyFactory.addAdvice(new MethodInterceptor02()); //4.获取代理对象，分析如何获取的代理对象？ Animals proxy = (Animals) proxyFactory.getProxy(); proxy.eat(&quot;人&quot;); System.out.println(&quot;===================================&quot;); proxy.run();&#125; 这里面首先首先是创建了一个代理工厂，然后给代理工厂添加拦截器，最后通过代理工厂来获取代理对象，最后通过代理对象执行目标方法。​ 接下来我们来看代理工厂。​ 3.代理工厂123456public ProxyFactory(Object target) &#123; //将目标对象封装成为 SingletonTargetSource 保存到父类字段内 setTarget(target); //获取目标对象class 的所有接口 ，保存到父类字段内 setInterfaces(ClassUtils.getAllInterfaces(target));&#125; 在**ProxyFactory**的构造器内将目标对象和目标对象实现的接口封装到了父类的字段里面。​ 上图是**ProxyFactory**的继承关系，可以先简单过一下，有一个印象。​ 4.添加切面**proxyFactory.addAdvice(new MethodInterceptor01())**\u0000这一行代码是往代理工厂添加拦截器/切面。​ 看一下添加的流程​ 12345@Overridepublic void addAdvice(Advice advice) throws AopConfigException &#123; int pos = this.advisors.size(); addAdvice(pos, advice);&#125; 首先是调用了**AdvisedSupport**类的添加切面方法。​ 在添加切面的方法里获取了当前类的增强器个数。然后和切面一起传递到重载的方法里。​ 12345678910111213141516171819@Overridepublic void addAdvice(int pos, Advice advice) throws AopConfigException &#123; Assert.notNull(advice, &quot;Advice must not be null&quot;); //不考虑，引介增强，很少用 if (advice instanceof IntroductionInfo) &#123; // We don&#x27;t need an IntroductionAdvisor for this kind of introduction: // It&#x27;s fully self-describing. addAdvisor(pos, new DefaultIntroductionAdvisor(advice, (IntroductionInfo) advice)); &#125; //不考虑，引介增强，很少用 else if (advice instanceof DynamicIntroductionAdvice) &#123; // We need an IntroductionAdvisor for this kind of introduction. throw new AopConfigException(&quot;DynamicIntroductionAdvice may only be added as part of IntroductionAdvisor&quot;); &#125; else &#123; //spring中Advice对应的接口就是Advisor，Spring使用Advisor包装着AOP的Advice实例 addAdvisor(pos, new DefaultPointcutAdvisor(advice)); &#125;&#125; ​ 然后调用了**addAdvisor()**，添加增强器。​ 因为我们没有指定切点，所以创建了一个默认的切点的增强器**DefaultPointcutAdvisor**。​ 123456789@Overridepublic void addAdvisor(int pos, Advisor advisor) throws AopConfigException &#123; //引介相关的逻辑，不考虑 if (advisor instanceof IntroductionAdvisor) &#123; validateIntroductionAdvisor((IntroductionAdvisor) advisor); &#125; //委派模式 addAdvisorInternal(pos, advisor);&#125; ​ 这里面调用了**addAdvisorInternal(pos, advisor)**，继续往下看。​ 12345678910111213141516private void addAdvisorInternal(int pos, Advisor advisor) throws AopConfigException &#123; Assert.notNull(advisor, &quot;Advisor must not be null&quot;); //如果当前AOP配置已经冻结了，不能在添加切面了，添加的话会抛出异常。 if (isFrozen()) &#123; throw new AopConfigException(&quot;Cannot add advisor: Configuration is frozen.&quot;); &#125; //如果传入的切面的位置大于当前切面的个数，抛异常，因为位置下标默认从-1开始。 if (pos &gt; this.advisors.size()) &#123; throw new IllegalArgumentException( &quot;Illegal position &quot; + pos + &quot; in advisor list with size &quot; + this.advisors.size()); &#125; //添加增强器 this.advisors.add(pos, advisor); //清理缓存 adviceChanged();&#125; 这里进行一些校验逻辑，然后添加增强器，清理缓存。​ 5.获取代理对象**proxyFactory.getProxy()**\u0000 1234567891011121314151617181920/** * 根据工厂的设置创建代理对象 * 可以反复的调用。 * 如果我们添加或者删除接口，效果会有所不同，可以添加和删除拦截器。 * 使用默认的类加载器：默认是线程上下文类加载器（如果需要创建代理） */public Object getProxy() &#123; /** * 创建AOP 的 代理 ，那么 AOP 的代理是什么 ？ * * AopProxy * * createAopProxy() ：去创建代理对象的逻辑 * getProxy()：获取创建好的代理对象，这里有两个实现分别是jdk的动态代理和cglib的动态代理。 * CglibAopProxy * JdkDynamicAopProxy * */ return createAopProxy().getProxy();&#125; 这里面先是利用**createAopProxy()**创建了一个代理对象，然后通过**getProxy()**来获取一个代理对象。​ 我们先来分析**createAopProxy()**，看一看代理对象是如何创建的？​ 12345678910111213141516171819/** * 子类应该调用它来获得一个新的 AOP 代理。 他们不应该创建一个AOP代理this作为参数。 */protected final synchronized AopProxy createAopProxy() &#123; /** * active属性实际上就是一个标记，在创建第一个代理对象的时候，会将他设置为true。 */ if (!this.active) &#123; activate(); &#125; /** * 分析一下下面这行代码的流程： * * 1. 获取aop的代理工厂 看一下AopProxyFactory * 2. 使用工厂创建一个aop的代理 ,如何创建代理的？ * */ return getAopProxyFactory().createAopProxy(this);&#125; 先是通过**getAopProxyFactory()**获取AOP的代理工厂，然后通过**createAopProxy(this)**传入当前类来获取一个代理对象。​ 123456789/** * 这个类里面已经持有了一个默认的aop的代理工厂 * ctrl + h 查看当前类的继承关系 * 当前类是 ProxyFactory的父类，所以里面的代理工厂会被默认的初始化加载 * @return */public AopProxyFactory getAopProxyFactory() &#123; return this.aopProxyFactory;&#125; 看一下上图类的继承关系，其实在创建**ProxyFactory**的时候，隐式调用父类的构造器的时候，就已经在**ProxyCreatorSupport**里面创建了一个默认的AOP代理工厂**DefaultAopProxyFactory**。​ 接下来再来看如何创建一个AOP代理对象的。​ 123456789101112131415161718192021222324252627282930313233343536373839/** * @param config 就是我们的ProxyFactory对象，ProxyFactory他是一个配置管理对象 * 保存着创建代理对象所有的生产资料。 * @return 返回一个AOP的代理对象 * @throws AopConfigException 如果某些不期望我们修改的配置被修改，就会抛出异常 */@Overridepublic AopProxy createAopProxy(AdvisedSupport config) throws AopConfigException &#123; /** * 条件一：暂且不管 * 条件二：true 表示强制使用cglib代理， * 条件三：true 表示被代理对象没有实现任何接口没有办法使用jdk的动态代理，只能使用cglib的动态代理 */ if (!NativeDetector.inNativeImage() &amp;&amp; //该条件不需要考虑 ( config.isOptimize() || //设置了这个属性，那么就是强制使用cglib的动态代理 config.isProxyTargetClass() || //设置了这个属性，那么就是强制使用cglib的动态代理 hasNoUserSuppliedProxyInterfaces(config) //判断被代理对象有没有实现接口，没有实现接口，那还用锤子jdk的动态代理 ) ) &#123; //走到这里的话，很大程度上就已经会使用cglib的动态代理 //获取目标对象的类型，为空的话肯定没法继续往下走了，直接异常中断 Class&lt;?&gt; targetClass = config.getTargetClass(); if (targetClass == null) &#123; throw new AopConfigException(&quot;TargetSource cannot determine target class: Either an interface or a target is required for proxy creation.&quot;); &#125; //目标对象是一个接口 或者 已经是一个被代理过得类型（此时是多重代理） ，只能使用jdk的动态代理 if (targetClass.isInterface() || Proxy.isProxyClass(targetClass)) &#123; return new JdkDynamicAopProxy(config); &#125; //走cglib的动态代理 return new ObjenesisCglibAopProxy(config); &#125; else &#123; //执行到这里的情况 ： 实现了接口,大多数情况我们都是面向接口编程，走这里 return new JdkDynamicAopProxy(config); &#125;&#125; 根据条件判断我们到底是创建JDK的代理对象还是创建Cglib的代理对象，因为我们的案例代码的目标类是实现了接口的，所以默认会走jdk的动态代理。​ 6.JdkDynamicAopProxy1234567891011121314151617/** * 使用给定的配置，通过构造器创建aop 的动jdk态代理对象 * 这里的config是啥？就是我们的代理工厂对象 */public JdkDynamicAopProxy(AdvisedSupport config) throws AopConfigException &#123; //非空断言 Assert.notNull(config, &quot;AdvisedSupport must not be null&quot;); //如果配置里面的切面数==0 &amp;&amp; 配置里面的目标对象是空对象，那么代理无法继续往下走了，直接抛异常中断 if (config.getAdvisorCount() == 0 &amp;&amp; config.getTargetSource() == AdvisedSupport.EMPTY_TARGET_SOURCE) &#123; throw new AopConfigException(&quot;No advisors and no TargetSource specified&quot;); &#125; this.advised = config; //获取当前被代理对象实现的接口数组 ,具体的实现逻辑？ this.proxiedInterfaces = AopProxyUtils.completeProxiedInterfaces(this.advised, true); //查找所有被代理的接口，如果有equals 和 hashcode就打个标 findDefinedEqualsAndHashCodeMethods(this.proxiedInterfaces);&#125; ​ 判断如果配置里面没有该被代理对象的切面，或者被代理对象是空，那就不能往下走了，抛出异常。​ 获取当前被代理对象实现的接口数组**AopProxyUtils.completeProxiedInterfaces(this.advised, true)**。​ 查找所有被代理的接口，如果有equals 和 hashcode就打个标。​ 看一下如何获取到当前被代理对象实现的接口数组的。​ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * 拿到被代理对象的所有接口 */static Class&lt;?&gt;[] completeProxiedInterfaces(AdvisedSupport advised, boolean decoratingProxy) &#123; /*从proxyFactory中获取所有的target提取出来的接口*/ Class&lt;?&gt;[] specifiedInterfaces = advised.getProxiedInterfaces(); /*如果接口长度是0*/ if (specifiedInterfaces.length == 0) &#123; //拿到目标对象的类型 Class&lt;?&gt; targetClass = advised.getTargetClass(); //如果目标对象的类型不为空 if (targetClass != null) &#123; //如果目标对象是一个接口，那么就将目标对象设置到接口列表里面 if (targetClass.isInterface()) &#123; advised.setInterfaces(targetClass); &#125; //如果目标对象是一个代理类，那么也将目标对象设置到接口列表 else if (Proxy.isProxyClass(targetClass)) &#123; advised.setInterfaces(targetClass.getInterfaces()); &#125; specifiedInterfaces = advised.getProxiedInterfaces(); &#125; &#125; /*创建一个新的接口数组，长度是原接口数量+spring追加的三个接口数量*/ List&lt;Class&lt;?&gt;&gt; proxiedInterfaces = new ArrayList&lt;&gt;(specifiedInterfaces.length + 3); for (Class&lt;?&gt; ifc : specifiedInterfaces) &#123; // 只有非密封接口实际上有资格进行JDK代理(在JDK 17上) if (!ifc.isSealed()) &#123; proxiedInterfaces.add(ifc); &#125; &#125; /*如果这个接口里面没有SpringProxy这个接口，那么就需要添加一个，打标，标识这个代理对象是Spring创建的*/ if (!advised.isInterfaceProxied(SpringProxy.class)) &#123; proxiedInterfaces.add(SpringProxy.class); &#125; /*判断目标对象的所有接口是否有advice接口，没有就手动添加*/ if (!advised.isOpaque() &amp;&amp; !advised.isInterfaceProxied(Advised.class)) &#123; proxiedInterfaces.add(Advised.class); &#125; //如果目标对象的所有接口里面，没有DecoratingProxy的接口，那就添加一个 if (decoratingProxy &amp;&amp; !advised.isInterfaceProxied(DecoratingProxy.class)) &#123; proxiedInterfaces.add(DecoratingProxy.class); &#125; //返回接口类型数组 return ClassUtils.toClassArray(proxiedInterfaces);&#125; 接下来我们来看**getProxy(**)的逻辑。​ 7.getProxy()12345@Overridepublic Object getProxy() &#123; //这里如果没有传类加载器，就使用默认的类加载器，默认是线程上下文类加载器 return getProxy(ClassUtils.getDefaultClassLoader());&#125; 方法重载，继续往下走。​ 123456789@Overridepublic Object getProxy(@Nullable ClassLoader classLoader) &#123; //打印日志的逻辑 if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Creating JDK dynamic proxy: &quot; + this.advised.getTargetSource()); &#125; //通过jdk的动态代理来创建代理对象 this == this::invoke 该方法最终会返回一个代理类对象 return Proxy.newProxyInstance(classLoader, this.proxiedInterfaces, this);&#125; 这里通过JDK的动态代理来创建代理对象，为啥会传入当前类**JdkDynamicAopProxy**，因为当前类实现了**InvocationHandler**接口。​ 因此，当代理对象调用目标方法的时候，就会执行该类的**invoke()**。​ 8.代理对象执行目标方法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293@Override@Nullablepublic Object invoke(Object proxy/*代理对象*/, Method method/*目标方法*/, Object[] args/*目标方法对应的参数*/) throws Throwable &#123; Object oldProxy = null; boolean setProxyContext = false; //advised 这里实际上就是proxyFactory的引用，targetSource 实际上就是上层传递封装的targetsource TargetSource targetSource = this.advised.targetSource; Object target = null; try &#123; /*如果代理类实现的接口里面有equals方法，就使用里面的，否则使用jdk提供的equals方法*/ if (!this.equalsDefined &amp;&amp; AopUtils.isEqualsMethod(method)) &#123; return equals(args[0]); &#125; //如果代理类实现的接口里面提供了hashcode方法，就是用里面的，否则用jdk的 else if (!this.hashCodeDefined &amp;&amp; AopUtils.isHashCodeMethod(method)) &#123; return hashCode(); &#125; /*暂时尚未用到，TODO*/ else if (method.getDeclaringClass() == DecoratingProxy.class) &#123; return AopProxyUtils.ultimateTargetClass(this.advised); &#125; else if (!this.advised.opaque &amp;&amp; method.getDeclaringClass().isInterface() &amp;&amp; method.getDeclaringClass().isAssignableFrom(Advised.class)) &#123; return AopUtils.invokeJoinpointUsingReflection(this.advised, method, args); &#125; //返回值 Object retVal; /** * 是否需要将当前的代理对象设置在aop上下文中 * aop上下文对象实际上就是一个threadLocal * 为什么要引入一个aop上下文？ * 目标对象A B * 通过代理的方式调用A.eat() * 这个eat方法里面有恰恰调用到了B的方法，这个时候B对象实际上并不是代理对象，所以 * b的方法执行前后并不会被增强，为了解决这个问题，就引入了aop的上下文 */ if (this.advised.exposeProxy) &#123; // 将当前代理对象设置到aop上下文中，并返回老的代理对象 oldProxy = AopContext.setCurrentProxy(proxy); setProxyContext = true; &#125; /*根据targetSource拿到目标对象*/ target = targetSource.getTarget(); /*根据目标对象拿到目标对象的类型*/ Class&lt;?&gt; targetClass = (target != null ? target.getClass() : null); // 这里是最关键的地方，查找适合该方法的增强 具体是如何查找的？ List&lt;Object&gt; chain = this.advised.getInterceptorsAndDynamicInterceptionAdvice(method, targetClass); /*查询出匹配当前方法拦截器的数量是0 说明当前方法不需要被增强，直接通过反射调用目标对象的目标方法。*/ if (chain.isEmpty()) &#123; Object[] argsToUse = AopProxyUtils.adaptArgumentsIfNecessary(method, args); /*调用目标对象的目标方法*/ retVal = AopUtils.invokeJoinpointUsingReflection(target, method, argsToUse); &#125; else &#123; /*说明有匹配当前method的方法拦截器，说明要做增强处理 */ MethodInvocation invocation = new ReflectiveMethodInvocation(proxy, target, method, args, targetClass, chain); /*核心驱动逻辑在ReflectiveMethodInvocation*/ retVal = invocation.proceed(); &#125; /*获取方法的返回值类型*/ Class&lt;?&gt; returnType = method.getReturnType(); /*如果目标方法返回目标对象 ，做一个替换 ，返回代理对象*/ if (retVal != null &amp;&amp; retVal == target &amp;&amp; returnType != Object.class &amp;&amp; returnType.isInstance(proxy) &amp;&amp; !RawTargetAccess.class.isAssignableFrom(method.getDeclaringClass())) &#123; retVal = proxy; &#125; //方法是void类型，但是返回值类型还不为空，说明有问题，抛异常 else if (retVal == null &amp;&amp; returnType != Void.TYPE &amp;&amp; returnType.isPrimitive()) &#123; throw new AopInvocationException( &quot;Null return value from advice does not match primitive return type for: &quot; + method); &#125; return retVal; &#125; finally &#123; if (target != null &amp;&amp; !targetSource.isStatic()) &#123; // Must have come from TargetSource. targetSource.releaseTarget(target); &#125; if (setProxyContext) &#123; // 将上次设置的proxy在此设置回去到aop上下文内 //因为当前代理对象的目标方法已经完成了，需要回到上一层逻辑 //属于恢复现场的逻辑 AopContext.setCurrentProxy(oldProxy); &#125; &#125;&#125; \u0000\u0000抛开一切不是很重要的逻辑​ 判断当前代理对象是否应该暴露出去，aop上下文**AopContext**实际上就是一个ThreadLocal。​ 为什么要引入AOP的上下文？​ 假设有目标对象A,B。​ 通过代理的方式调用A.eat()。​ 这个eat()方法里面恰恰调用了B的方法，这个时候对象实际上并不是代理对象，所以B的方法执行前后并不会被增强，为了解决这个问题，就引入了AOP上下文。​ 通过**getInterceptorsAndDynamicInterceptionAdvice()**查找到当前方法执行前后需要执行的增强器。\u0000如果当前方法匹配的增强器数量是0，那么直接通过反射调用目标方法。​ 否则说明有匹配的增强器，需要做增强处理。**retVal=ReflectiveMethodInvocation.proceed(）**​ retVal就是方法的返回值。​ 判断如果方法最终返回的目标对象，那就替换成代理对象。​ 判断如果方法是void类型，但是返回值类型还不为空，说明有问题，抛异常。​ 最终返回结果，并将AOP上下文的代理对象还原成里面原有的对象，因为当前代理对象的目标方法已经完成了，需要回到上一层逻辑。\u0000\u0000至此，AOP整个流程就分析完了，剩下的一些核心的细节：​ **getInterceptorsAndDynamicInterceptionAdvice()**查找到当前方法执行前后需要执行的增强器 **retVal=ReflectiveMethodInvocation.proceed(）**执行增强器逻辑 ​ 通过前面的分析，我们大体上了解了Spring的Aop的执行流程。接下来我们在看一些核心的细节，如何查找目标方法的增强器。 9.查找增强器**List&lt;Object&gt; chain = this.advised.getInterceptorsAndDynamicInterceptionAdvice(method, targetClass);**\u0000 1234567891011121314151617public List&lt;Object&gt; getInterceptorsAndDynamicInterceptionAdvice(Method method, @Nullable Class&lt;?&gt; targetClass) &#123; //先尝试从缓存拿 MethodCacheKey cacheKey = new MethodCacheKey(method); List&lt;Object&gt; cached = this.methodCache.get(cacheKey); /*如果缓存为空*/ if (cached == null) &#123; /*那就走查找逻辑，并刷新缓存 * advisorChainFactory什么时候创建的？ * 这个是在proxyFactory里面的一个变量，代理工厂创建出来，他就创建出来了 * */ cached = this.advisorChainFactory.getInterceptorsAndDynamicInterceptionAdvice( this, method, targetClass); this.methodCache.put(cacheKey, cached); &#125; /*最终返回查找到的值*/ return cached;&#125; 查找缓存，如果没命中则去查找并放入缓存放回。我们继续往下看查找逻辑**this.advisorChainFactory.getInterceptorsAndDynamicInterceptionAdvice()**。​ 这个**advisorChainFactory**是什么？看当前类的属性**AdvisorChainFactory advisorChainFactory = new DefaultAdvisorChainFactory();** 从这里我们就定位到了看哪个方法的逻辑\u0000 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879@Overridepublic List&lt;Object&gt; getInterceptorsAndDynamicInterceptionAdvice( Advised config/*代理工厂*/, Method method/*目标方法*/, @Nullable Class&lt;?&gt; targetClass/*目标对象类型*/) &#123; /** * 这个接口是切面适配器的注册中心 * 1.可以注册AdvisorAdapter 适配器目的：将非advisor类型的增强包装成advisor ，将advisor类型的增强提取出来对应的 方法拦截器 * * */ AdvisorAdapterRegistry registry = GlobalAdvisorAdapterRegistry.getInstance(); /** * 获取代理工厂内部持有的增强信息 * 1. addAdvice * 2. addAdvisor * 最终在代理工厂中都会包装成 advisor * */ Advisor[] advisors = config.getAdvisors(); /*创建一个拦截器列表，长度就是advisor的长度*/ List&lt;Object&gt; interceptorList = new ArrayList&lt;&gt;(advisors.length); /*真实的目标对象类型*/ Class&lt;?&gt; actualClass = (targetClass != null ? targetClass : method.getDeclaringClass()); /*引介增强相关*/ Boolean hasIntroductions = null; for (Advisor advisor : advisors) &#123; /*包含切点信息的增强，内部逻辑就是做匹配算法*/ if (advisor instanceof PointcutAdvisor) &#123; // 转换成 PointcutAdvisor 类型，可以获取到切点信息 PointcutAdvisor pointcutAdvisor = (PointcutAdvisor) advisor; /*条件二成立，说明当前被代理对象的class匹配当前advisor成功，可能被advisor增强，具体还要看方法匹配。 这里可以看一下 Pointcut 源码*/ if (config.isPreFiltered() || pointcutAdvisor.getPointcut().getClassFilter().matches(actualClass)) &#123; /*获取切点信息的方法匹配器，做方法级别的匹配*/ MethodMatcher mm = pointcutAdvisor.getPointcut().getMethodMatcher(); boolean match; /*引介相关的，不需要考虑*/ if (mm instanceof IntroductionAwareMethodMatcher) &#123; if (hasIntroductions == null) &#123; hasIntroductions = hasMatchingIntroductions(advisors, actualClass); &#125; match = ((IntroductionAwareMethodMatcher) mm).matches(method, actualClass, hasIntroductions); &#125; else &#123; /*进行方法匹配 目标方法匹配成功 ， match = true，当前的增强器可以应用到method*/ match = mm.matches(method, actualClass); &#125; /*判断是否还需要运行时的匹配*/ if (match) &#123; /*提取出advisor类持有的拦截器信息 registry里面包含三个默认的增强器*/ MethodInterceptor[] interceptors = registry.getInterceptors(advisor); if (mm.isRuntime()) &#123; /*如果是运行时匹配，那就走运行时匹配的逻辑*/ for (MethodInterceptor interceptor : interceptors) &#123; interceptorList.add(new InterceptorAndDynamicMethodMatcher(interceptor, mm)); &#125; &#125; else &#123; /*将方法拦截器追加到拦截器列表里面去*/ interceptorList.addAll(Arrays.asList(interceptors)); &#125; &#125; &#125; &#125; /*不考虑引介，所以直接跳过*/ else if (advisor instanceof IntroductionAdvisor) &#123; IntroductionAdvisor ia = (IntroductionAdvisor) advisor; if (config.isPreFiltered() || ia.getClassFilter().matches(actualClass)) &#123; Interceptor[] interceptors = registry.getInterceptors(advisor); interceptorList.addAll(Arrays.asList(interceptors)); &#125; &#125; /*适配所有方法的*/ else &#123; Interceptor[] interceptors = registry.getInterceptors(advisor); interceptorList.addAll(Arrays.asList(interceptors)); &#125; &#125; /*返回所有匹配当前方法的拦截器*/ return interceptorList;&#125; 方法有点长，我们挑重点慢慢分析。​ 首先获取到切面适配器的注册中心，切面适配器**AdvisorAdapter**是做什么的？​ 将非advisor类型的增强包装成advisor ，将advisor类型的增强提取出来对应的 方法拦截器。​ 获取代理工厂内部持有的增强信息​ Advice Advisor 最终在代理工厂中都会被包装成Advisor。​ 遍历所有的拦截器：​ 先是处理切点类型的增强器 先将增强器转化成**PointcutAdvisor**类型 判断如果当前被代理对象的class匹配增强器成功，说明可能增强成功，还要看具体的方法匹配，这里可以看一下**PointCut** 源码 获取切点信息的方法匹配器，准备做方法级别的匹配 **match = mm.matches(method, actualClass)**进行具体的方法匹配 判断是否需要运行时匹配 如果需要，提取出增强器持有的拦截器信息(registry里面默认持有三个增强器)，走运行时匹配的逻辑 -&gt; 将**InterceptorAndDynamicMethodMatcher**加入到拦截器列表 如果不需要，将方法拦截器添加到拦截器列表 处理引介类型的增强器 这里的逻辑我们不需要关注 处理适配所有方法的增强器 从增强器的适配中心获取所有的拦截器 最终返回匹配当前方法的所有拦截器​ 10.方法匹配这里我们再看一下具体的方法匹配逻辑。​ 看一下**AbstractRegexpMethodPointcut**，说实话，营养价值不大，有兴趣可以自行琢磨。 123456789101112131415161718192021222324252627@Overridepublic boolean matches(Method method, Class&lt;?&gt; targetClass) &#123; return (matchesPattern(ClassUtils.getQualifiedMethodName(method, targetClass)) || (targetClass != method.getDeclaringClass() &amp;&amp; matchesPattern(ClassUtils.getQualifiedMethodName(method, method.getDeclaringClass()))));&#125;/** * Match the specified candidate against the configured patterns. * @param signatureString &quot;java.lang.Object.hashCode&quot; style signature * @return whether the candidate matches at least one of the specified patterns */protected boolean matchesPattern(String signatureString) &#123; for (int i = 0; i &lt; this.patterns.length; i++) &#123; boolean matched = matches(signatureString, i); if (matched) &#123; for (int j = 0; j &lt; this.excludedPatterns.length; j++) &#123; boolean excluded = matchesExclusion(signatureString, j); if (excluded) &#123; return false; &#125; &#125; return true; &#125; &#125; return false;&#125; 11.拦截器的核心驱动获取到所有的匹配当前方法的拦截器后，最终我们是要驱动所有的拦截器去执行，接下来分析下拦截器的核心驱动逻辑。**invocation.proceed()**，核心逻辑在**ReflectiveMethodInvocation**中。​ 123456789101112131415161718192021222324252627282930@Override@Nullablepublic Object proceed() throws Throwable &#123; // 因为从-1开始，如果当前拦截器下标 == 拦截器数量-1 ，说明所有方法拦截器都执行过了，接下来需要执行目标对象的目标方法 if (this.currentInterceptorIndex == this.interceptorsAndDynamicMethodMatchers.size() - 1) &#123; /*调用连接点*/ return invokeJoinpoint(); &#125; /*获取下一个方法拦截器*/ Object interceptorOrInterceptionAdvice = this.interceptorsAndDynamicMethodMatchers.get(++this.currentInterceptorIndex); /*判断是否需要运行时匹配*/ if (interceptorOrInterceptionAdvice instanceof InterceptorAndDynamicMethodMatcher) &#123; InterceptorAndDynamicMethodMatcher dm = (InterceptorAndDynamicMethodMatcher) interceptorOrInterceptionAdvice; Class&lt;?&gt; targetClass = (this.targetClass != null ? this.targetClass : this.method.getDeclaringClass()); if (dm.methodMatcher.matches(this.method, targetClass, this.arguments)) &#123; return dm.interceptor.invoke(this); &#125; else &#123; return proceed(); &#125; &#125; /*大部分情况下会走到else这里静态匹配*/ else &#123; // 让当前方法拦截器执行invoke即可 ，并且将当前对象传递进去 return ((MethodInterceptor) interceptorOrInterceptionAdvice).invoke(this); &#125;&#125; 判断是否所有拦截器都执行完了，如果是的话，执行目标方法**invokeJoinpoint()**。​ \u0000获取下一个拦截器，判断是否需要做运行时匹配，大部分情况下，我们都是走静态匹配的逻辑。​ 让当前方法拦截器执行**invoke()**。​ 12345678910@Override@Nullablepublic Object invoke(MethodInvocation mi) throws Throwable &#123; try &#123; return mi.proceed(); &#125; finally &#123; invokeAdviceMethod(getJoinPointMatch(), null, null); &#125;&#125; ​ 通过这样类似递归的链式调用，每一个拦截器等待下一个拦截器执行完成返回以后在执行，拦截器的机制保证了通知方法与目标方法的执行顺序。​ 再来看下如何调用目标方法。**invokeJoinpoint()**​ 1234@Nullableprotected Object invokeJoinpoint() throws Throwable &#123; return AopUtils.invokeJoinpointUsingReflection(this.target, this.method, this.arguments);&#125; \u0000继续往下追。​ 12345678910111213141516171819202122@Nullablepublic static Object invokeJoinpointUsingReflection(@Nullable Object target, Method method, Object[] args) throws Throwable &#123; // Use reflection to invoke the method. try &#123; ReflectionUtils.makeAccessible(method); return method.invoke(target, args); &#125; catch (InvocationTargetException ex) &#123; // Invoked method threw a checked exception. // We must rethrow it. The client won&#x27;t see the interceptor. throw ex.getTargetException(); &#125; catch (IllegalArgumentException ex) &#123; throw new AopInvocationException(&quot;AOP configuration seems to be invalid: tried calling method [&quot; + method + &quot;] on target [&quot; + target + &quot;]&quot;, ex); &#125; catch (IllegalAccessException ex) &#123; throw new AopInvocationException(&quot;Could not access method [&quot; + method + &quot;]&quot;, ex); &#125;&#125; 最终通过暴力反射来调用目标方法执行。​ 12.切点表达式**PointCut**​ 123456789101112131415161718192021222324public interface Pointcut &#123; /** * Return the ClassFilter for this pointcut. * @return the ClassFilter (never &#123;@code null&#125;) * * 类过滤器：判断某个类是否符合切点位置 */ ClassFilter getClassFilter(); /** * Return the MethodMatcher for this pointcut. * @return the MethodMatcher (never &#123;@code null&#125;) * 方法匹配器：判断类中某个方法是否匹配条件，匹配条件的方法才会被增强 */ MethodMatcher getMethodMatcher(); /** * Canonical Pointcut instance that always matches. */ Pointcut TRUE = TruePointcut.INSTANCE;&#125; 前面案例代码中，有一个我们自己实现的切点，可以回顾一下。 最终回顾下开头的一张图，明确下**Advised --持有--&gt; Advisor --持有--&gt; Advice --子类--&gt;Interceptor --子类--&gt;MethodInterceptor**关系：​ 至此，整个AOP的全部流程已经梳理清晰。下一篇，我们将开始分析Spring的事务。​ ​","categories":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/tags/Spring/"}]},{"title":"Spring[十]Aop的两种实现方式","slug":"Spring/Spring[十]Aop的两种实现方式","date":"2022-01-11T06:00:15.970Z","updated":"2022-01-11T06:08:00.592Z","comments":true,"path":"2022/01/11/Spring/Spring[十]Aop的两种实现方式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/Spring/Spring[%E5%8D%81]Aop%E7%9A%84%E4%B8%A4%E7%A7%8D%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F/","excerpt":"","text":"本篇开始Aop的相关的内容，aop的底层使用到了动态代理的，我们针对一个方法可以配置多个切面，也就实现了多重代理。本篇先不看源码，从开发者的角度观望如何实现多重代理。 ​ 一，代理模式代理模式的定义：由于某些原因需要给某对象提供一个代理以控制对该对象的访问。这时，访问对象不适合或者不能直接引用目标对象，代理对象作为访问对象和目标对象之间的中介。代理模式的结构比较简单，主要是通过定义一个继承抽象主题的代理来包含真实主题，从而实现对真实主题的访问。代理模式的主要角色如下。 抽象主题（Subject）类：通过接口或抽象类声明真实主题和代理对象实现的业务方法。 真实主题（Real Subject）类：实现了抽象主题中的具体业务，是代理对象所代表的真实对象，是最终要引用的对象。 代理（Proxy）类：提供了与真实主题相同的接口，其内部含有对真实主题的引用，它可以访问、控制或扩展真实主题的功能。 在代码中，一般代理会被理解为代码增强，实际上就是在原代码逻辑前后增加一些代码逻辑，而使调用者无感知。根据代理的创建时期，代理模式分为静态代理和动态代理。 静态：由程序员创建代理类或特定工具自动生成源代码再对其编译，在程序运行前代理类的 .class 文件就已经存在了。 动态：在程序运行时，运用反射机制动态创建而成。 1.JDK的动态代理 动态代理 特点：字节码随用随修改，随用随加载 作用：在不修改源码的基础上在运行时动态的对方法进行增强 分类： 基于接口的动态代理 基于子类的动态代理 基于接口的动态代理 涉及的类：Proxy 提供者：JDK官方 如何创建代理对象 使用Proxy类的**newProxyInstance()** 创建代理对象的要求 被代理类最少实现一个接口，如果没有则不能使用 **newProxyInstance()**的参数 ClassLoader：类加载器，用于加载代理对象字节码文件；和被代理对象使用相同的类加载器。 Class[]：字节码数组，用于让代理对象和被代理对象实现相同方法的。 InvocationHandler：用于提供增强的代码，他是让我们自定义如何代理，我们一般都是写一个该接口的实现类。 简单的实现12345678910111213141516171819202122232425262728293031323334353637public class Test1 &#123; @Test public void test1()&#123; //被代理类对象要声明为最终的 final Producer producer=new Producer(); //代理对象和被代理类对象要实现同一个接口 IProducer proxyProducer = (IProducer) Proxy.newProxyInstance(producer.getClass().getClassLoader(), producer.getClass().getInterfaces(), new InvocationHandler() &#123; /** * 作用：执行被代理对象的任何接口方法都会经过该方法 * 方法参数的含义 * @param proxy 代理对象的引用 * 1. 可以使用反射获取代理对象的信息（也就是proxy.getClass().getName()。 * 2. 可以将代理对象返回以进行连续调用，这就是proxy存在的目的，因为this并不是代理对象。 * @param method 当前执行的方法 * @param args 当前执行方法所需的参数 * @return 和被代理对象方法相同的返回值 * @throws Throwable */ public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; Object value=null; //获取方法执行的参数 //判断当前方法是不是销售 if (&quot;saleProduct&quot;.equals(method.getName()))&#123; Float money= (Float) args[0]; //两个参数：被代理类对象，方法增强的参数 value=method.invoke(producer,money*0.8f); &#125; return value; &#125; &#125;); proxyProducer.saleProduct(10000f); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243package com.es.java1;/** * 一个生产者 */public class Producer implements IProducer&#123; /** * 销售 * @param money */ public void saleProduct(float money)&#123; System.out.println(&quot;销售产品，并拿到钱：&quot;+money); &#125; /** * 售后 * @param money */ public void afterService(float money)&#123; System.out.println(&quot;提供售后服务，并拿到钱：&quot;+money); &#125;&#125;-------------------------------------------------------------------/** * 对生产厂家要求的接口 */public interface IProducer &#123; /** * 销售 * @param money */ public void saleProduct(float money); /** * 售后 * @param money */ public void afterService(float money);&#125; 2.cglib动态代理 基于子类的动态代理 涉及的类：Enhancer ，提供者：第方cglib库 如何创建代理对象： 使用Enhancer类中的create方法 创建代理对象的要求： 被代理类是最终类 create方法的参数： Class：字节码 ：它是用于指定被代理对象的字节码。 Callback：用于提供增强的代码 ：它是让我们写如何代理。我们一般都是些一个该接口的实现类。 我们一般写的都是该接口的子接口实现类：**MethodInterceptor** 简单的实现12345678910111213141516171819202122232425262728293031323334/** * @author yinhuidong * @createTime 2020-03-02-1:08 */public class Test4 &#123; @Test public void test() &#123; final Producer producer = new Producer(); Producer cglibProducer = (Producer) Enhancer.create(producer.getClass(), new MethodInterceptor() &#123; /** * 执行被代理对象的任何方法都会经过该方法 * @param proxy * @param method * @param args * 以上个参数和基于接口的动态代理中invoke方法的参数是一样的 * @param methodProxy ：当前执行方法的代理对象 * @return * @throws Throwable */ public Object intercept(Object proxy, Method method, Object[] args, MethodProxy methodProxy) throws Throwable &#123; //提供增强的代码 Object returnValue = null; //1.获取方法执行的参数 Float money = (Float) args[0]; //2.判断当前方法是不是销售 if (&quot;saleProduct&quot;.equals(method.getName())) &#123; returnValue = method.invoke(producer, money * 0.8f); &#125; return returnValue; &#125; &#125;); cglibProducer.saleProduct(12000f); &#125;&#125; 二，多重代理1.基于责任链模式的多重代理1.1 被代理的方法123456789public interface Animal &#123; void eat(String food);&#125;public class Cat implements Animal&#123; @Override public void eat(String food) &#123; System.out.println(&quot;猫吃&quot;+food+&quot;!&quot;); &#125;&#125; 1.2封装目标对象的目标方法12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class TargetMethod &#123; /** * 目标对象 */ private Object target; /** * 目标方法 */ private Method method; /** * 方法参数 */ private Object[] args; public TargetMethod(Object target, Method method, Object[] args) &#123; this.target = target; this.method = method; this.args = args; &#125; public Object getTarget() &#123; return target; &#125; public void setTarget(Object target) &#123; this.target = target; &#125; public Method getMethod() &#123; return method; &#125; public void setMethod(Method method) &#123; this.method = method; &#125; public Object[] getArgs() &#123; return args; &#125; public void setArgs(Object[] args) &#123; this.args = args; &#125;&#125; 1.3 抽象的责任链节点和驱动责任链往下执行的头节点1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public abstract class AbstractSlot &#123; TargetMethod targetMethod; AbstractSlot next; abstract Object invoke(TargetMethod targetMethod); private boolean hasNextSlot() &#123; return next != null; &#125; public Object proceed(TargetMethod targetMethod) throws Exception &#123; return hasNextSlot() ? next.invoke(targetMethod) : targetMethod.getMethod() .invoke( targetMethod.getTarget(), targetMethod.getArgs() ); &#125; public AbstractSlot(TargetMethod targetMethod, AbstractSlot next) &#123; this.targetMethod = targetMethod; this.next = next; &#125; public TargetMethod getTargetMethod() &#123; return targetMethod; &#125; public void setTargetMethod(TargetMethod targetMethod) &#123; this.targetMethod = targetMethod; &#125; public AbstractSlot getNext() &#123; return next; &#125; public void setNext(AbstractSlot next) &#123; this.next = next; &#125; public AbstractSlot() &#123; &#125; public static class Head extends AbstractSlot&#123; @Override Object invoke(TargetMethod targetMethod) &#123; return null; &#125; &#125;&#125; 1.4 对方法增强的类1234567891011121314151617181920212223242526public class JdkDynamic implements InvocationHandler &#123; Object target ; AbstractSlot head; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; return head.proceed(new TargetMethod( target,method,args )); &#125; public Object getProxy()&#123; return Proxy.newProxyInstance( target.getClass().getClassLoader(), target.getClass().getInterfaces(), this ); &#125; public JdkDynamic(Object target, AbstractSlot head) &#123; this.target = target; this.head = head; &#125;&#125; 1.5测试类12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class Main &#123; public static void main(String[] args) &#123; AbstractSlot.Head head = new AbstractSlot.Head(); AbstractSlot first = new First(); AbstractSlot second=new Second(); first.setNext(second); head.setNext(first); JdkDynamic jdkDynamic = new JdkDynamic(new Cat(),head); Animal proxy = (Animal) jdkDynamic.getProxy(); proxy.eat(&quot;事物&quot;); &#125; private static class First extends AbstractSlot&#123; @Override Object invoke(TargetMethod targetMethod) &#123; Object result = null; System.out.println(&quot;增强逻辑&quot;); try &#123; result= proceed(targetMethod); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;增强逻辑&quot;); return result; &#125; &#125; private static class Second extends AbstractSlot&#123; @Override Object invoke(TargetMethod targetMethod) &#123; Object result = null; System.out.println(&quot;增强逻辑&quot;); try &#123; result= proceed(targetMethod); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;增强逻辑&quot;); return result; &#125; &#125;&#125; 2.基于拦截器的多重代理2.1 拦截器12345678910111213141516171819202122232425262728public interface MyInterceptor &#123; Object invoke(MyInvocation myInvocation);&#125;class One implements MyInterceptor&#123; @Override public Object invoke(MyInvocation myInvocation) &#123; Object result = null; System.out.println(&quot;1&quot;); result=myInvocation.proceed(); System.out.println(&quot;4&quot;); return result; &#125;&#125;class Two implements MyInterceptor&#123; @Override public Object invoke(MyInvocation myInvocation) &#123; Object result = null; System.out.println(&quot;2&quot;); result=myInvocation.proceed(); System.out.println(&quot;3&quot;); return result; &#125;&#125; 2.2 增强器123456789101112131415161718192021222324252627282930313233343536public interface MyInvocation &#123; Object proceed();&#125;class MyInvocationImpl implements MyInvocation&#123; List&lt;MyInterceptor&gt; interceptors = new ArrayList&lt;&gt;(); int size =0; TargetMethod targetMethod; @Override public Object proceed() &#123; try &#123; return size == interceptors.size()? targetMethod.getMethod() .invoke( targetMethod.getTarget(), targetMethod.getArgs() ): interceptors.get(size++) .invoke(this); &#125; catch (IllegalAccessException | InvocationTargetException e) &#123; e.printStackTrace(); &#125; return null; &#125; public MyInvocationImpl(List&lt;MyInterceptor&gt; interceptors, TargetMethod targetMethod) &#123; this.interceptors = interceptors; this.targetMethod = targetMethod; &#125;&#125; 2.3 对方法的增强类12345678910111213141516171819202122232425262728293031323334public class JdkProxy implements InvocationHandler &#123; Object target; List&lt;MyInterceptor&gt; interceptors = new ArrayList&lt;&gt;(); public void add(MyInterceptor a) &#123; interceptors.add(a); &#125; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; return new MyInvocationImpl( interceptors, new TargetMethod( target, method, args ) ).proceed(); &#125; public Object getProxy()&#123; return Proxy.newProxyInstance( target.getClass().getClassLoader(), target.getClass().getInterfaces(), this ); &#125; public JdkProxy(Object target) &#123; this.target = target; &#125;&#125; 2.4 测试类12345678910public class Main &#123; public static void main(String[] args) &#123; JdkProxy jdkProxy = new JdkProxy(new Cat()); jdkProxy.add(new One()); jdkProxy.add(new Two()); ((Animal) jdkProxy.getProxy()).eat(&quot;事务&quot;); &#125;&#125; 三，AOP相关概念 AOP：全称是 Aspect Oriented Programming 即：面向切面编程。就是把我们程序重复的代码抽取出来，在需要执行的时候，使用动态代理的技术，在不修改源码的基础上，对我们的已有方法进行增强。​ 作用：在程序运行期间，不修改源码对已有方法进行增强。 优势：减少重复代码提高开发效率维护方便 AOP 相关术语 Joinpoint(连接点):所谓连接点是指那些被拦截到的点。在 spring 中,这些点指的是方法,因为 spring 只支持方法类型的连接点。 Pointcut(切入点):所谓切入点是指我们要对哪些 Joinpoint 进行拦截的定义。 Advice(通知/增强):所谓通知是指拦截到 Joinpoint 之后所要做的事情就是通知。通知的类型：前置通知,后置通知,异常通知,最终通知,环绕通知。 Introduction(引介):引介是一种特殊的通知在不修改类代码的前提下, Introduction 可以在运行期为类动态地添加一些方法或 Field。 Target(目标对象):被代理对象代理的目标对象。 Weaving(织入):是指把增强应用到目标对象来创建新的代理对象的过程。spring 采用动态代理织入，而 AspectJ 采用编译期织入和类装载期织入。 Proxy（代理:一个类被 AOP 织入增强后，就产生一个结果代理类。 Aspect(切面):是切入点和通知（引介的结合） ​ Spring 框架监控切入点方法的执行。一旦监控到切入点方法被运行，使用代理机制，动态创建目标对象的代理对象，根据通知类别，在代理对象的对应位置，将通知对应的功能织入，完成完整的代码逻辑运行。​ 至此，我们通过两种方式完成了多重代理实现AOP，也简单介绍了两种实现动态代理的方式，和AOP相关的一些概念。下一篇将开始分析注解版Aop源码。 ​","categories":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/tags/Spring/"}]},{"title":"Spring[九]三级缓存&循环依赖","slug":"Spring/Spring[九]三级缓存&循环依赖","date":"2022-01-11T06:00:05.852Z","updated":"2022-01-11T06:08:29.459Z","comments":true,"path":"2022/01/11/Spring/Spring[九]三级缓存&循环依赖/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/Spring/Spring[%E4%B9%9D]%E4%B8%89%E7%BA%A7%E7%BC%93%E5%AD%98&%E5%BE%AA%E7%8E%AF%E4%BE%9D%E8%B5%96/","excerpt":"","text":"上一篇补充了一下Spring的组件，注解和扩展点，在开发中如果清楚的了解这些东西，会对你的案例设计，产生意想不到的效果。我呢，只是对这些组件注解扩展点进行一个介绍分析，具体的如何为业务赋能还需要结合实际的开发场景。 什么是循环依赖？ 有几种循环依赖？ Spring是如何解决循环依赖的？ Spring为什么用三级缓存解决循环依赖，用二级可不可以？ 当目标对象产生代理对象时，Spring容器中(第一级缓存)到底存储的是谁？一，问题&amp;答案1.什么是循环依赖 类和类之间的依赖关系形成了闭环，就叫做循环依赖。​ 2.有几种循环依赖 通过构造方法进行依赖注入时产生的循环依赖问题。 通过setter方法进行依赖注入且是在多例(原型)模式下产生的循环依赖问题。 通过setter方法进行依赖注入且是在单例模式下产生的循环依赖问题。 ​ 注意：在Spring中，只有【第三种方式】的循环依赖问题被解决了，其他两种方式在遇到循环依赖问题时都会产生异常。 第一种构造方法注入的情况下，在new对象的时候就会堵塞住了，其实也就是”先有鸡还是先有蛋“的历史难题。 第二种setter方法&amp;&amp;多例的情况下，每一次getBean()时，都会产生一个新的Bean，如此反复下去就会有无穷无尽的Bean产生了，最终就会导致OOM问题的出现。 回顾一下单实例bean创建的过程​ 3.Spring是如何解决循环依赖的？ 下载原图可点击这里​ 1.A创建过程中需要B，于是A将自己放到三级缓存去实例化B 2.B实例化的时候发现需要A，于是B先查一级缓存，没有，再查二级缓存，没有，再查三级缓存，找到了A，然后把三级缓存里面的A放到二级缓存里面，并删除三级缓存里面的A。 3.B顺利初始化完毕，将自己放到一级缓存里面（此时B里面的A还未创建完），然后接着回来创建A，此时B已经创建结束，直接从一级缓存里面拿到B，然后完成创建，并将A自己放到一级缓存里面。 spring解决循环依赖依靠的是Bean的中间态这个概念，而这个中间态指的是已经实例化但还没初始化的状态—–&gt;半成品。​ 12345678910//DefaultSingletonBeanRegistry//一级缓存：实例化完的beanprivate final Map&lt;String, Object&gt; singletonObjects = new ConcurrentHashMap&lt;&gt;(256);//三级缓存：单例bean工厂private final Map&lt;String, ObjectFactory&lt;?&gt;&gt; singletonFactories = new HashMap&lt;&gt;(16);//二级缓存：早期暴露的beanprivate final Map&lt;String, Object&gt; earlySingletonObjects = new HashMap&lt;&gt;(16); 4.Spring为什么用三级缓存解决循环依赖，用二级可不可以？为什么第三级缓存要使用ObjectFactory？需要提前产生代理对象。​ ​ 什么时候将Bean的引用提前暴露给第三级缓存的ObjectFactory持有？时机就是在第一步实例化之后，第二步依赖注入之前，完成此操作。​ 至此，我就解释清楚了整个三级缓存和循环依赖。不得不感慨，在我大三的时候，这道题还号称是阿里P7的面试题，大四的时候，我就连续四场面试被问到，行业内卷啊。","categories":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/tags/Spring/"}]},{"title":"Spring[八]注解&组件篇","slug":"Spring/Spring[八]注解&组件篇","date":"2022-01-11T05:59:54.649Z","updated":"2022-01-11T06:08:57.690Z","comments":true,"path":"2022/01/11/Spring/Spring[八]注解&组件篇/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/Spring/Spring[%E5%85%AB]%E6%B3%A8%E8%A7%A3&%E7%BB%84%E4%BB%B6%E7%AF%87/","excerpt":"","text":"写在前面 写在前面：最近在编译最新版spring源码的时候，踩了写小坑。先来看一下最新的官方文档：https://github.com/spring-projects/spring-framework在build from source里面声明了最新的编译过程：https://github.com/spring-projects/spring-framework/wiki/Build-from-Source由于最近spring做了一些升级，想要在本地构建spring源码，需要使用jdk17。此外，最近gradle的语法也因为版本的原因，改动很大。想要在spring源码里面对gradle进行一些定制化配置的时候，可以参照gradle官网。https://gradle.org/releases/ 1.注解1.1总览 @Bean 容器中注册组件 @Primary 同类组件如果有多个，标注主组件 @DependsOn 组件之间声明依赖关系 @Lazy 组件懒加载（最后使用的时候才会去创建） @Scope 声明组件的作用范围 @Configuration 声明这是一个配置类，替换xml @Component @Controller @Service @Repository @Indexed 加速注解，所有标注了@Indexed的组件，直接会启动快速加载 @Order 数字越小，优先级越高，越先工作 @ComponentScan 包扫描 @Conditional 条件注入 @Import 导入第三方jar包中的组件，或者定制批量导入组件逻辑 @ImportResource 导入以前的xml配置文件，让其生效 @Profile 基于多环境激活 @PropertySource 外部properties配置文件和javaBean进行绑定，结合ConfigurationProperties @PropertySources @PropertySource的组合注解 @Autowired 自动装配 @Qualifier 精准指定 @Resource jsr250规范的jdk自带注解 @Value 取值，计算机环境变量，jvm系统 xxx @Lookup 单例组件依赖非单例组件，非单例组件获取需要使用方法1.2.案例com.yhd.annotation.AnnoMainTest​ 2.组件与SPI扩展点机制2.1总览 基础接口 Resource+ResourceLoader 将来自各种不同渠道的配置文件等进行一层抽象封装，让开发人员不必关注于底层的细节实现，通过资源加载器加载资源。 BeanFactory ioc容器顶层接口 ✅ BeanDefinition 从resource解析出bean的定义信息 ✅ BeanDefinitionReader bean定义信息读取器 ，从resource 读取解析 BeanDefinition BeanDefinitionRegistry bean定义信息的注册中心 解析出来的BeanDefinition信息会被注册到这里 ApplicationContext ioc核心接口 ✅ Aware 实现xxxAware接口是为了能够获取到xxx相关的一些属性 ✅ BeanNameAware BeanFactoryAware ApplicationEventPublisherAware ApplicationContextAware ApplicationStartupAware BeanClassLoaderAware ImportAware EnvironmentAware 生命周期-后置处理器 BeanFactoryPostProcessor ✅ BeanDefinitionRegistryPostProcessor ✅ InitializingBean ✅ DisposableBean ✅ BeanPostProcessor ✅ SmartInitializingSingleton ✅ 监听器 ApplicationListener ✅2.2案例 \u0000com.yhd.annotation.AnnoMainTest​ 补充：spring源码地址：https://gitee.com/yin_huidong/spring.git​ \u0000此源码main分支为手动翻译好的中文注释版spring源码。 一，组件注册1.Configuration&amp;Bean@Configuration注解标识的类标识这是一个Spring的配置类。​ @Bean注解：给容器中注册一个bean，id默认是方法名作为id。​ value：指定id名 initMethod：指定初始化方法 destoryMethod：指定销毁方法 ​ 1.1 SpringConfig12345678@Configurationpublic class SpringConfig &#123; @Bean(value = &quot;person&quot;,initMethod = &quot;init&quot;,destroyMethod = &quot;destroy&quot;) public Person person()&#123; return new Person(1,&quot;二十&quot;); &#125;&#125; 1.2 Person1234567891011121314151617@Data@AllArgsConstructor@NoArgsConstructorpublic class Person &#123; private Integer id; private String name ; public void init()&#123; System.out.println(&quot;init()...&quot;); &#125; public void destroy()&#123; System.out.println(&quot;destroy()...&quot;); &#125;&#125; 1.3 Test12345678910public class TestA &#123; ApplicationContext ioc = new AnnotationConfigApplicationContext(SpringConfig.class); @Test public void testBean()&#123; Person person = ioc.getBean(&quot;person&quot;, Person.class); System.out.println(&quot;person = &quot; + person); &#125;&#125; 2.RunWith&amp;ContextConfiguration@RunWith：Spring整合Junit4。​ @ContextConfiguration：指定配置类。​ 1234567891011@RunWith(SpringRunner.class)@ContextConfiguration(classes = &#123;SpringConfig.class&#125;)public class TestA &#123; @Autowired private Person person; @Test public void testBean()&#123; System.out.println(&quot;person = &quot; + person); &#125;&#125; 3.ComponentScan@ComponentScan(“com.yhd”) 组件扫描 value:指定扫描的包 includeFilters = &#123;@ComponentScan.Filter(type = FilterType.ANNOTATION,classes = Controller.class)&#125; 只包含哪些包（需要指明：useDefaultFilters = false） type type:FilterType.ANNOTATION:按照注解过滤type=FilterType.ASSIGNABLE_TYPE：按照类型过滤type=ASPECTJ,切面type=REGEX,正则type=CUSTOM，定制（需要实现TypeFilter接口） excludeFilters:指明排除哪些包不被扫描 ​ 3.1 SpringConfig1234567891011121314@Configuration@ComponentScan(value = &quot;com.yhd&quot;, includeFilters = &#123; @ComponentScan.Filter(type = FilterType.ANNOTATION, classes = Controller.class), @ComponentScan.Filter(type = FilterType.ASSIGNABLE_TYPE) &#125;, excludeFilters = &#123;&#125;, useDefaultFilters = true)public class SpringConfig &#123; @Bean(value = &quot;person&quot;, initMethod = &quot;init&quot;, destroyMethod = &quot;destroy&quot;) public Person person() &#123; return new Person(1, &quot;二十&quot;); &#125;&#125; 3.2 Test12345678910111213141516@RunWith(SpringRunner.class)@ContextConfiguration(classes = &#123;SpringConfig.class&#125;)public class TestA &#123; @Autowired private ApplicationContext ioc; @Test public void test()&#123; //查看容器中bean的名字 String[] names = ioc.getBeanDefinitionNames(); for (String name : names) &#123; System.out.println(&quot;name = &quot; + name); &#125; &#125;&#125; 4.Scope作用：设置组件作用域。​ @Scope 默认是单例的点击进入该注解，在此点击进入文档注释中的ConfigurableBeanFactory可以看到该注解的几个取值： singleton 单例 prototype 多例 request 同一次请求 session 同一个session作用域 在scope=singleton时，对象在容器已创建立即加入容器。在scope=prototype时，对象每次调用的时候都会添加到容器。​ 4.1 SpringConfig123456789@Configurationpublic class SpringConfig &#123; @Bean(value = &quot;person&quot;, initMethod = &quot;init&quot;, destroyMethod = &quot;destroy&quot;) @Scope(&quot;prototype&quot;) public Person person() &#123; return new Person(1, &quot;二十&quot;); &#125;&#125; 4.2 Test123456@Testpublic void test()&#123; Person p1 = ioc.getBean(&quot;person&quot;, Person.class); Person p2 = ioc.getBean(&quot;person&quot;, Person.class); System.out.println(p1==p2?&quot;true&quot;:&quot;false&quot;);//false&#125; 5.Lazy@Lazy 懒加载单实例bean，默认在容器创建时候添加对象。懒加载：容器启动不创建对象，第一次获取/使用时在创建对象并初始化。​ 5.1 SpringConfig12345678910@Configurationpublic class SpringConfig &#123; @Lazy @Bean(value = &quot;person&quot;, initMethod = &quot;init&quot;, destroyMethod = &quot;destroy&quot;) public Person person() &#123; System.out.println(&quot;person已经加载到容器中！&quot;); return new Person(1, &quot;二十&quot;); &#125;&#125; 5.2 Test1234@Testpublic void test()&#123; Person person = ioc.getBean(&quot;person&quot;, Person.class);&#125; 对比加上这个注解前后，容器中单实例bean的变化：​ 6.Conditional@Conditional点击进入Class&lt;? extends Condition&gt;[] value();发现这个注解里面的value属性需要传入一个Condition数组，点击进入Conditionboolean matches(ConditionContext context, AnnotatedTypeMetadata metadata);因此，可以实现Condition接口，来使用该注解​ 当该注解加在方法上，标识满足条件时，该方法会执行当该注解加在类上时，表示满足条件时，该类里面的所有方法才会执行，否则一个都不执行。​ 123456789101112@Target(&#123;ElementType.TYPE, ElementType.METHOD&#125;)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface Conditional &#123; /** * All &#123;@link Condition&#125;s that must &#123;@linkplain Condition#matches match&#125; * in order for the component to be registered. */ Class&lt;? extends Condition&gt;[] value();&#125; 1234567891011121314@FunctionalInterfacepublic interface Condition &#123; /** * Determine if the condition matches. * @param context the condition context * @param metadata metadata of the &#123;@link org.springframework.core.type.AnnotationMetadata class&#125; * or &#123;@link org.springframework.core.type.MethodMetadata method&#125; being checked * @return &#123;@code true&#125; if the condition matches and the component can be registered, * or &#123;@code false&#125; to veto the annotated component&#x27;s registration */ boolean matches(ConditionContext context, AnnotatedTypeMetadata metadata);&#125; 6.1 案例：根据当前系统环境将对应的bean加入到容器6.1.1 SpringConfig1234567891011121314151617@Configurationpublic class SpringConfig &#123; @Bean(&quot;person&quot;) @Conditional(&#123;Mac.class&#125;) public Person person() &#123; System.out.println(&quot;person已经加载到容器中！&quot;); return new Person(1, &quot;二十&quot;); &#125; @Bean(&quot;person2&quot;) @Conditional(&#123;Linux.class&#125;) public Person person2() &#123; System.out.println(&quot;person已经加载到容器中！&quot;); return new Person(1, &quot;二十&quot;); &#125;&#125; 6.1.2 条件类12345678public class Mac implements Condition &#123; @Override public boolean matches(ConditionContext context, AnnotatedTypeMetadata metadata) &#123; return context.getEnvironment().getProperty(&quot;os.name&quot;).contains(&quot;Mac&quot;); &#125;&#125; 123456public class Linux implements Condition &#123; @Override public boolean matches(ConditionContext context, AnnotatedTypeMetadata metadata) &#123; return context.getEnvironment().getProperty(&quot;os.name&quot;).contains(&quot;linux&quot;); &#125;&#125; 6.1.3 Test12345678910111213141516@RunWith(SpringRunner.class)@ContextConfiguration(classes = &#123;SpringConfig.class&#125;)public class TestA &#123; @Autowired private ApplicationContext ioc; @Test public void test()&#123; Person person = ioc.getBean(&quot;person&quot;, Person.class); Person person2 = ioc.getBean(&quot;person2&quot;, Person.class); System.out.println(person); System.out.println(person2); &#125;&#125; 7.Import用来快速导入一个bean，默认类名是该类的全限定类名，单例。​ 7.1 SpringConfig123456789101112131415161718@Configuration@Import(&#123;Dog.class&#125;)public class SpringConfig &#123; @Bean(&quot;person&quot;) @Conditional(&#123;Mac.class&#125;) public Person person() &#123; System.out.println(&quot;person已经加载到容器中！&quot;); return new Person(1, &quot;二十&quot;); &#125; @Bean(&quot;person2&quot;) @Conditional(&#123;Linux.class&#125;) public Person person2() &#123; System.out.println(&quot;person已经加载到容器中！&quot;); return new Person(1, &quot;二十&quot;); &#125;&#125; 7.2 Test12345678910@Testpublic void test()&#123; Dog dog = ioc.getBean(Dog.class); assert dog!=null; dog.setId(1); dog.setName(&quot;asdfg&quot;); System.out.println(&quot;dog = &quot; + dog);&#125; 8.ImportSelector123456789public @interface Import &#123; /** * &#123;@link Configuration&#125;, &#123;@link ImportSelector&#125;, &#123;@link ImportBeanDefinitionRegistrar&#125; * or regular component classes to import. */ Class&lt;?&gt;[] value();&#125; 点击查看@Import注解可以发现：​ 里面还可以传入{@link Configuration}, {@link ImportSelector}, {@link ImportBeanDefinitionRegistrar}​ importSelector其实就是一个接口，我们需要通过实现它来传入impot注解。​ 8.1 MyImportSelector123456public class MyImportSelector implements ImportSelector &#123; @Override public String[] selectImports(AnnotationMetadata importingClassMetadata) &#123; return new String[]&#123;&quot;com.yhd.pojo.Person&quot;,&quot;com.yhd.pojo.Dog&quot;&#125;; &#125;&#125; 8.2 SpringConfig12345@Configuration@Import(&#123;MyImportSelector.class&#125;)public class SpringConfig &#123; &#125; 8.3 Test12345678910111213@RunWith(SpringRunner.class)@ContextConfiguration(classes = &#123;SpringConfig.class&#125;)public class TestA &#123; @Autowired private ApplicationContext ioc; @Test public void test()&#123; String[] names = ioc.getBeanDefinitionNames(); for (String name : names) System.out.println(name); &#125;&#125; 9.ImportBeanDefinitionRegistrar1234567public interface ImportBeanDefinitionRegistrar &#123; public void registerBeanDefinitions( AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry);&#125; 9.1 MyImportBeanDefinitionRegistrar123456public class MyImportBeanDefinitionRegistrar implements ImportBeanDefinitionRegistrar &#123; @Override public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) &#123; registry.registerBeanDefinition(&quot;dog&quot;, new RootBeanDefinition(&quot;com.yhd.pojo.Dog&quot;)); &#125;&#125; 9.2 MyImportBeanDefinitionRegistrar12345@Configuration@Import(&#123;MyImportBeanDefinitionRegistrar.class&#125;)public class SpringConfig &#123;&#125; 9.3 Test12345678910111213@RunWith(SpringRunner.class)@ContextConfiguration(classes = &#123;SpringConfig.class&#125;)public class TestA &#123; @Autowired private ApplicationContext ioc; @Test public void test()&#123; String[] names = ioc.getBeanDefinitionNames(); for (String name : names) System.out.println(name); &#125;&#125; 10.FactoryBean​ spring的工厂模式造Bean 如果传入id获取到的是工厂造的bean 如果传入的是&amp;id获取到的是工厂本身 ​ 10.1 PersonFactory12345678910111213141516public class PersonFactory implements FactoryBean&lt;Person&gt; &#123; @Override public Person getObject() throws Exception &#123; return new Person(1,&quot;FactoryBean&quot;); &#125; @Override public Class&lt;?&gt; getObjectType() &#123; return Person.class; &#125; @Override public boolean isSingleton() &#123; return true; &#125;&#125; 10.2 SpringConfig12345678@Configurationpublic class SpringConfig &#123; @Bean public PersonFactory personFactory()&#123; return new PersonFactory(); &#125;&#125; 10.3 Test12345678910111213141516171819@RunWith(SpringRunner.class)@ContextConfiguration(classes = &#123;SpringConfig.class&#125;)public class TestA &#123; @Autowired private ApplicationContext ioc; @Test public void test()&#123; PersonFactory personFactory = (PersonFactory) ioc.getBean(&quot;&amp;personFactory&quot;); System.out.println(&quot;personFactory = &quot; + personFactory); Person person = (Person) ioc.getBean(&quot;personFactory&quot;); System.out.println(&quot;person = &quot; + person); /*personFactory = com.yhd.factory.PersonFactory@6e20b53a person = Person(id=1, name=FactoryBean)*/ &#125;&#125; 10.4 原理1234public interface BeanFactory &#123; //more能获取到工厂bean的方法就是在id前加上前缀&amp; String FACTORY_BEAN_PREFIX = &quot;&amp;&quot;; 11.LookUp​ 如果有一个类C,需要用到类B,如果使用@Autowired注解注入B,那么B每次调用都是同一个对象，即使B不是单例的，现在我希望每次调用B都是不一样的，那么实现方案有2个：​ 11.1 每次从容器中获取B123456789101112131415161718192021@Component@Scope(scopeName= ConfigurableBeanFactory.SCOPE_PROTOTYPE) //原型 也就是非单例public class B &#123; public void sayHi()&#123; System.out.println(&quot;hi&quot;); &#125;&#125;@Componentpublic class C implements ApplicationContextAware &#123; private ApplicationContext applicationContext; public void hello()&#123; B b = (B)applicationContext.getBean(&quot;b&quot;); b.sayHi(); &#125; @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; this.applicationContext=applicationContext; &#125;&#125; 11.2 使用@lookup注解12345678910111213141516171819@Component@Scope(scopeName= ConfigurableBeanFactory.SCOPE_PROTOTYPE) //原型 也就是非单例public class B &#123; public void sayHi()&#123; System.out.println(&quot;hi&quot;); &#125;&#125;@Componentpublic abstract class C &#123; public void hello()&#123; B b = getB(); b.sayHi(); &#125; @Lookup public abstract B getB(); //一般都是抽象方法&#125; 二，生命周期1.@Bean指定初始化和销毁方法bean的生命周期：创建，初始化，使用，销毁。​ 容器管理bean的生命周期：我们可以自定义初始化和销毁方法，容器在bean进行到当前生命周期的时候，来调用我们自定义的初始化和销毁方法。​ 单例模式：先是执行对象的无参构造，赋值后，执行初始化方法，在创建容器的时候加入对象，在容器关闭时，执行销毁方法。​ 多例模式：先创建容器，每次调用对象时，调用无参构造方法，赋值后，执行初始化方法。对象的销毁由java垃圾回收机制回收。​ 1.1 单例1234567891011121314151617181920@Data@AllArgsConstructorpublic class Person &#123; private Integer id; private String name ; public void init()&#123; System.out.println(&quot;init()...&quot;); &#125; public void destroy()&#123; System.out.println(&quot;destroy()...&quot;); &#125; public Person()&#123; System.out.println(&quot;constructor()....&quot;); &#125;&#125; 12345678@Configurationpublic class SpringConfig &#123; @Bean(value = &quot;person&quot;,initMethod = &quot;init&quot;,destroyMethod = &quot;destroy&quot;) public Person person()&#123; return new Person(); &#125;&#125; 12345678910111213@RunWith(SpringRunner.class)@ContextConfiguration(classes = &#123;SpringConfig.class&#125;)public class TestA &#123; @Autowired private ApplicationContext ioc; //constructor @Test public void test()&#123; Person person = ioc.getBean(&quot;person&quot;, Person.class); System.out.println(&quot;person = &quot; + person); &#125;&#125; 1.2 多例12345678@Configurationpublic class SpringConfig &#123; @Scope(value = &quot;prototype&quot;) @Bean(value = &quot;person&quot;,initMethod = &quot;init&quot;,destroyMethod = &quot;destroy&quot;) public Person person()&#123; return new Person(); &#125;&#125; 2.实现接口完成对象的初始化和销毁InitializingBean, DisposableBean​ 1234567891011121314151617181920212223@Data@AllArgsConstructorpublic class Person implements InitializingBean, DisposableBean &#123; private Integer id; private String name ; public Person()&#123; System.out.println(&quot;constructor()....&quot;); &#125; @Override public void destroy() throws Exception &#123; System.out.println(&quot;destroy()....&quot;); &#125; @Override public void afterPropertiesSet() throws Exception &#123; System.out.println(&quot;afterPropertiesSet()....&quot;); &#125; &#125; 12345678@Configurationpublic class SpringConfig &#123; @Bean(value = &quot;person&quot;) public Person person()&#123; return new Person(); &#125;&#125; 1234567891011121314@RunWith(SpringRunner.class)@ContextConfiguration(classes = &#123;SpringConfig.class&#125;)public class TestA &#123; @Autowired private ApplicationContext ioc; //constructor @Test public void test()&#123; System.out.println(&quot;容器创建成功！&quot;); Person person = ioc.getBean(&quot;person&quot;, Person.class); System.out.println(&quot;person = &quot; + person); &#125;&#125; 3.BeanPostProcessorbean的后置处理器，在bean的初始化前后做一些处理工作，需要加入到容器中。​ 1234567891011121314public class MyBeanPostProcessor implements BeanPostProcessor &#123; @Override public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException &#123; System.out.println(&quot;初始化前&quot;); return BeanPostProcessor.super.postProcessBeforeInitialization(bean, beanName); &#125; @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException &#123; System.out.println(&quot;初始化后&quot;); return BeanPostProcessor.super.postProcessAfterInitialization(bean, beanName); &#125;&#125; 123456789@Configuration@Import(MyBeanPostProcessor.class)public class SpringConfig &#123; @Bean(value = &quot;person&quot;) public Person person()&#123; return new Person(); &#125;&#125; 4.BeanFactoryPostProcessorbean工厂的后置处理器，可以在spring解析完配置文件，创建对象之前，对bean的定义信息进行修改。 12345678910111213@Componentpublic class Computer &#123; String name; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125;&#125; 12345678910@Componentpublic class MyBeanFactoryPostProcessor implements BeanFactoryPostProcessor &#123; @Override public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException &#123; System.out.println(&quot;=======BeanFactoryPostProcessor=========&quot;); BeanDefinition person = beanFactory.getBeanDefinition(&quot;computer&quot;); person.setScope(BeanDefinition.SCOPE_PROTOTYPE); System.out.println(&quot;=======BeanFactoryPostProcessor=========&quot;); &#125;&#125; 123456789101112/** * 默认Computer对象是单实例的， * 通过beanFactory的后置处理器处理之后，变成原型的 * &lt;p&gt; * 执行时机：打断点 */private static void testBeanFactoryPostProcessor() &#123; /*ioc容器创建的12个核心方法里面*/ Computer computer = ioc.getBean(Computer.class); Computer computer2 = ioc.getBean(Computer.class); System.out.println(computer == computer2);&#125; 5.BeanDefinitionRegistryPostProcessor他是bean工厂后置处理器的子类插播一条广告：BeanFactoryPostProcessor和BeanDefinitionRegistryPostProcessor之间的关系？BeanDefinitionRegistryPostProcessor 是 BeanFactoryPostProcessor的子类和扩展它里面 搞了一个新的方法 postProcessBeanDefinitionRegistry ，可以往容器中注册更多的bd信息。扩展点：①BeanFactoryPostProcessor 对bd信息进行修改②postProcessBeanDefinitionRegistry 添加更多的bd信息 12public class RegistryBean &#123;&#125; 123456789101112131415161718@Componentpublic class MyBeanDefinitionRegistryPostProcessor implements BeanDefinitionRegistryPostProcessor &#123; @Override public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException &#123; /*这个是她的父类里面的方法 对 bd信息进行修改*/ &#125; @Override public void postProcessBeanDefinitionRegistry(BeanDefinitionRegistry registry) throws BeansException &#123; /*这个是给他本身的方法，可以添加额外的 bd 信息*/ GenericBeanDefinition beanDefinition = new GenericBeanDefinition(); beanDefinition.setBeanClass(RegistryBean.class); //beanDefinition.setScope(); registry.registerBeanDefinition(&quot;registryBean&quot;,beanDefinition); &#125;&#125; 1234private static void testBeanDefinitionRegistryPostProcessor() &#123; RegistryBean registryBean = ioc.getBean(&quot;registryBean&quot;, RegistryBean.class); System.out.println(&quot;registryBean = &quot; + registryBean);&#125; 6.SmartInitializingSingleton在所有的单实例bean 通过getBean方法完成初始化之后,就会去查找这个类型的SmartInitializingSingleton 的 组件 ，执行里面的 方法. 1234567@Componentpublic class MySmartInitializingSingleton implements SmartInitializingSingleton &#123; @Override public void afterSingletonsInstantiated() &#123; System.out.println(&quot;断点调试。。。。&quot;); &#125;&#125; 7.Lifecycle &amp;&amp; SmartLifecycle容器创建完成之后的回调，相当于传递的参数autoStartUpOnly是干嘛的？表示只启动SmartLifeCycle生命周期对象，并且启动的对象autoStartUpOnly必须是true，不会启动普通的生命周期对象，false的时候，会启动全部的生命周期对象。 1234567891011121314151617181920public class DemoLifeCycle implements Lifecycle &#123; private boolean running =false; @Override public void start() &#123; this.running=true; System.out.println(&quot;demo one start!&quot;); &#125; @Override public void stop() &#123; this.running=false; System.out.println(&quot;demo one stop!&quot;); &#125; @Override public boolean isRunning() &#123; return running; &#125;&#125; 1234567891011121314151617181920public class DemoSmartLifeCycle implements SmartLifecycle &#123; private boolean running = false; @Override public void start() &#123; this.running=true; System.out.println(&quot;demo two start!&quot;); &#125; @Override public void stop() &#123; this.running=false; System.out.println(&quot;demo two stop!&quot;); &#125; @Override public boolean isRunning() &#123; return running; &#125;&#125; 三，属性赋值1.Value@Value注解可以给属性赋值，支持SPEL表达式，${},,基本数值。​ 12345678910111213@Data@AllArgsConstructorpublic class Person &#123; @Value(&quot;1&quot;) private Integer id; @Value(&quot;二十&quot;) private String name ; public Person()&#123; System.out.println(&quot;constructor()....&quot;); &#125; &#125; 2.PropertySource@PropertySource加载外部属性文件​ 支持一次写多个，标注在配置类上。​ 12person.name=二十person.age=20 123456789@Data@AllArgsConstructor@NoArgsConstructorpublic class Person &#123; @Value(&quot;$&#123;person.age&#125;&quot;) private Integer id; @Value(&quot;$&#123;person.name&#125;&quot;) private String name ;&#125; 123456789@Configuration@PropertySource(&quot;classpath:person.properties&quot;)public class SpringConfig &#123; @Bean(value = &quot;person&quot;) public Person person()&#123; return new Person(); &#125;&#125; 四，自动装配Spring利用依赖注入DI，完成对IOC容器中各个组件的依赖关系赋值。​ 1.@Autowired &amp;&amp; @Qualifier &amp;&amp; @Primary 默认优先按照类型去容器中找对应的组件，ioc.getBean(BookDao.class); 如果找到多个相同类型的组件，再讲属性的名称作为组件的id去容器中查找ioc.getBean(&quot;bookDao&quot;); @Qualifier(&quot;bookDao&quot;),使用@Qualifier指定要装配的组件的id，而不是使用属性名 自动装配默认一定要将属性赋值好，没有就会报错.可以使用@Autowired（required=false） @Primary 让Spring进行自动装配的时候，默认使用首选bean，也可以继续使用@Qualifier指定需要装配的bean的id ​ 2.@Resource &amp;&amp; @Injectspring还支持使用@Resource（jsr250）和@inject（jsr330）【java规范的注解】​ @Resource：可以和@Autowired一样实现自动装配，默认按照组件名称进行装配.不支持@Primary和required=false​ @Inject：需要导入javax.inject包，没有required=false​ 3.方法，构造器位置的自动注入@Autowired：构造器，参数，方法，属性，都是从容器中获取参数组件的值。 标注在方法上：@Bean+方法参数，参数从容器中获取，默认不写@Autowired效果是一样的，都能自动装配 标在构造器上：如果组件只有一个构造器，这个有参构造器的@Autowired可以省略 放在参数位置 ​ 3.1 构造器注入12345678@Component@Data@AllArgsConstructor@NoArgsConstructorpublic class Person &#123; private Integer id; private String name ;&#125; 12345678910111213@Component@Data@NoArgsConstructorpublic class Dog &#123; private Person person; @Autowired //此处注解可以省略 private Dog(Person person)&#123; this.person=person; &#125;&#125; 12345678910111213@RunWith(SpringRunner.class)@ContextConfiguration(classes = &#123;SpringConfig.class&#125;)public class TestA &#123; @Autowired private ApplicationContext ioc; //constructor @Test public void test()&#123; Dog dog = ioc.getBean(&quot;dog&quot;, Dog.class); System.out.println(&quot;dog = &quot; + dog); &#125;&#125; 3.2 set()注入12345678910111213@Component@Data@NoArgsConstructorpublic class Dog &#123; private Person person; @Autowired //此处的注解不能省略 public void setPerson(Person person)&#123; this.person=person; &#125;&#125; 3.3 @Bean的方式123456789101112131415@Configurationpublic class SpringConfig &#123; @Bean(value = &quot;person&quot;) public Person person() &#123; return new Person(); &#125; @Bean(value = &quot;dog&quot;) @Autowired //此处的注解可以省略不写 public Dog dog(Person person) &#123; return new Dog(person); &#125;&#125; 4.Aware接口，自定义组件使用Spring底层的组件自定义接口想要使用Spring底层的一些组件（ApplicationContext，BeanFactory，xxx）​ 自定义接口实现xxxAware，在创建对象的时候，会调用接口规定的方法注入相关组件，Aware吧Spring底层的一些接口注入到自定义的Bean中​ xxxAware：功能：使用xxxProcessor​ ApplicationContextAware--&gt;ApplicationContextAwareProcessor​ 1234567891011121314151617@Componentpublic class Dog implements ApplicationContextAware, BeanFactoryAware &#123; private ApplicationContext ioc; private BeanFactory beanFactory; @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; //自此处打断点 this.ioc=applicationContext; &#125; @Override public void setBeanFactory(BeanFactory beanFactory) throws BeansException &#123; //此处打断点 this.beanFactory=beanFactory; &#125;&#125; 5.@Profile根据环境装配@Profile:指定组件在那个环境的情况下才能被注册到容器中，不指定，任何环境下都能注册这个组件 加了环境标识的bean，只有这个环境被激活的时候才能注册到容器中，默认是default环境 写在配置类上，只有是指定的环境的时候，整个配置类里面的所有配置才能开始生效 没有标识环境标识的bean在任何环境下都是加载的。 ​ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748@Configuration@PropertySource(&quot;classpath:jdbc.properties&quot;)public class SpringConfig implements EmbeddedValueResolverAware &#123; @Value(&quot;$&#123;jdbc.username&#125;&quot;) private String name; private StringValueResolver resolver; private String driver; @Profile(&quot;dev&quot;) @Bean(&quot;dev&quot;) public DataSource dataSource1(@Value(&quot;$&#123;jdbc.password&#125;&quot;) String pwd)&#123; DruidDataSource source = new DruidDataSource(); source.setName(name); source.setPassword(pwd); source.setDriverClassName(driver); source.setUrl(&quot;jdbc:mysql:///test&quot;); return source; &#125; @Profile(&quot;pro&quot;) @Bean(&quot;pro&quot;) public DataSource dataSource2(@Value(&quot;$&#123;jdbc.password&#125;&quot;) String pwd)&#123; DruidDataSource source = new DruidDataSource(); source.setName(name); source.setPassword(pwd); source.setDriverClassName(driver); source.setUrl(&quot;jdbc:mysql:///ssm&quot;); return source; &#125; @Profile(&quot;test&quot;) @Bean(&quot;test&quot;) public DataSource dataSource3(@Value(&quot;$&#123;jdbc.password&#125;&quot;) String pwd)&#123; DruidDataSource source = new DruidDataSource(); source.setName(name); source.setPassword(pwd); source.setDriverClassName(driver); source.setUrl(&quot;jdbc:mysql:///zuoye&quot;); return source; &#125; @Override public void setEmbeddedValueResolver(StringValueResolver resolver) &#123; this.resolver=resolver; driver = resolver.resolveStringValue(&quot;$&#123;jdbc.driverClassName&#125;&quot;); &#125;&#125; 激活环境： 使用命令行动态参数：在虚拟机参数位置加载： -Dspring.profiles.active=test 代码方式激活 ​ 123456789101112@Testpublic void test() &#123; AnnotationConfigApplicationContext ioc = new AnnotationConfigApplicationContext(); ioc.getEnvironment().setActiveProfiles(&quot;dev&quot;); ioc.register(SpringConfig4.class); ioc.refresh(); String[] names = ioc.getBeanNamesForType(DataSource.class); for (String name : names) &#123; System.out.println(name); &#125;&#125; 五，事件驱动1.ApplicationListener &amp;&amp; ApplicationEvent通过自定义不同类型的事件，使用不同的监听器监听不同类型的事件，做到jvm进程内的消息队列，事件驱动，解耦。 1234567891011121314151617public class MyEvent extends ApplicationEvent &#123; String message; public MyEvent(Object source) &#123; super(source); &#125; public MyEvent(Object source,String message) &#123; super(source); this.message=message; &#125; public void print()&#123; System.out.println(&quot;发布了一个事件：&quot;+message); &#125;&#125; 12345678public class MyListener implements ApplicationListener&lt;MyEvent&gt; &#123; @Override public void onApplicationEvent(MyEvent event) &#123; event.print(); &#125;&#125; 1234567/** * 测试spring 的 ioc 容器的事件发布 */private static void testPublishEvent() &#123; ApplicationContext ioc = new ClassPathXmlApplicationContext(CONFIG_LOCATION); ioc.publishEvent(new MyEvent(&quot;&quot;, &quot;这是我自定义的一个事件&quot;));&#125; 2.@EventListener对上面写法的一个优化，更加简洁，开发量更少，懒人必备神器。 12345678private static void testEventListener() &#123; ioc.publishEvent(new ApplicationEvent(&quot;hello，spring&quot;) &#123; @Override public Object getSource() &#123; return super.getSource(); &#125; &#125;);&#125; 12345678@Componentpublic class MyEventListener &#123; @EventListener(classes = ApplicationEvent.class) public void listener(ApplicationEvent event)&#123; System.out.println(&quot;event = &quot; + event); &#125;&#125;","categories":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/tags/Spring/"}]},{"title":"Spring[七]initializeBean()","slug":"Spring/Spring[七]initializeBean()","date":"2022-01-11T05:59:43.828Z","updated":"2022-01-11T06:09:21.774Z","comments":true,"path":"2022/01/11/Spring/Spring[七]initializeBean()/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/Spring/Spring[%E4%B8%83]initializeBean()/","excerpt":"","text":"上一篇分析了单实例bean创建过程中的属性赋值，接下来我们分析，单实例bean属性赋值之后的初始化**initializeBean()**。 1. initializeBean()123456789101112131415161718192021222324252627protected Object initializeBean(String beanName, Object bean, @Nullable RootBeanDefinition mbd) &#123; /*检查当前bean是否实现了aware接口，再具体判断实现的哪个aware接口，做一些赋能操作。*/ invokeAwareMethods(beanName, bean); Object wrappedBean = bean; if (mbd == null || !mbd.isSynthetic()) &#123; /*初始化之前，后置处理器的调用点*/ wrappedBean = applyBeanPostProcessorsBeforeInitialization(wrappedBean, beanName); &#125; try &#123; /*执行初始化方法*/ invokeInitMethods(beanName, wrappedBean, mbd); &#125; catch (Throwable ex) &#123; throw new BeanCreationException( (mbd != null ? mbd.getResourceDescription() : null), beanName, &quot;Invocation of init method failed&quot;, ex); &#125; if (mbd == null || !mbd.isSynthetic()) &#123; /*初始化后的后置处理器执行点*/ /*典型应用：AOP的具体实现*/ wrappedBean = applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName); &#125; return wrappedBean;&#125; 在初始化之前，把当前bean实现的所有Aware接口都执行一遍。**invokeAwareMethods()**在初始化之前，调用后置处理器。**applyBeanPostProcessorsBeforeInitialization()**执行初始化逻辑。**invokeInitMethods()**执行后置处理器的调用点，典型的实现其实就是AOP。**applyBeanPostProcessorsAfterInitialization()**返回包装对象。​ 2.invokeAwareMethods()12345678910111213141516private void invokeAwareMethods(String beanName, Object bean) &#123; if (bean instanceof Aware) &#123; if (bean instanceof BeanNameAware) &#123; ((BeanNameAware) bean).setBeanName(beanName); &#125; if (bean instanceof BeanClassLoaderAware) &#123; ClassLoader bcl = getBeanClassLoader(); if (bcl != null) &#123; ((BeanClassLoaderAware) bean).setBeanClassLoader(bcl); &#125; &#125; if (bean instanceof BeanFactoryAware) &#123; ((BeanFactoryAware) bean).setBeanFactory(AbstractAutowireCapableBeanFactory.this); &#125; &#125;&#125; 执行相关的Aware接口方法。 3.applyBeanPostProcessorsBeforeInitialization()​ 123456789101112131415@Overridepublic Object applyBeanPostProcessorsBeforeInitialization(Object existingBean, String beanName) throws BeansException &#123; Object result = existingBean; /*beanPostProcessor的执行逻辑*/ for (BeanPostProcessor processor : getBeanPostProcessors()) &#123; Object current = processor.postProcessBeforeInitialization(result, beanName); if (current == null) &#123; return result; &#125; result = current; &#125; return result;&#125; 循环所有的后置处理器，执行前置方法，如果有一个返回结果为空，直接短路，后面的都不执行了。​ 4.invokeInitMethods()1234567891011121314151617181920212223protected void invokeInitMethods(String beanName, Object bean, @Nullable RootBeanDefinition mbd) throws Throwable &#123; /*如果是实现了InitializingBean接口*/ boolean isInitializingBean = (bean instanceof InitializingBean); if (isInitializingBean &amp;&amp; (mbd == null || !mbd.isExternallyManagedInitMethod(&quot;afterPropertiesSet&quot;))) &#123; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Invoking afterPropertiesSet() on bean with name &#x27;&quot; + beanName + &quot;&#x27;&quot;); &#125; /*执行afterPropertiesSet方法*/ ((InitializingBean) bean).afterPropertiesSet(); &#125; if (mbd != null &amp;&amp; bean.getClass() != NullBean.class) &#123; /*判断重写init方法没？*/ String initMethodName = mbd.getInitMethodName(); if (StringUtils.hasLength(initMethodName) &amp;&amp; !(isInitializingBean &amp;&amp; &quot;afterPropertiesSet&quot;.equals(initMethodName)) &amp;&amp; !mbd.isExternallyManagedInitMethod(initMethodName)) &#123; /*执行重写的init方法*/ invokeCustomInitMethod(beanName, bean, mbd); &#125; &#125;&#125; 判断实现了**InitializingBean**接口的，执行里面的方法。判断重写了初始化方法的，执行重写的方法。 4.1 invokeCustomInitMethod()1234567891011121314151617181920212223242526272829303132333435363738394041424344protected void invokeCustomInitMethod(String beanName, Object bean, RootBeanDefinition mbd) throws Throwable &#123; /*从bd获取init方法*/ String initMethodName = mbd.getInitMethodName(); Assert.state(initMethodName != null, &quot;No init method set&quot;); /* * isNonPublicAccessAllowed 非公开访问 * * 这里完成之后就会获取到通过init方法定义的方法对象。 * */ Method initMethod = (mbd.isNonPublicAccessAllowed() ? BeanUtils.findMethod(bean.getClass(), initMethodName) : ClassUtils.getMethodIfAvailable(bean.getClass(), initMethodName)); /*如果init方法为空*/ if (initMethod == null) &#123; if (mbd.isEnforceInitMethod()) &#123; throw new BeanDefinitionValidationException(&quot;Could not find an init method named &#x27;&quot; + initMethodName + &quot;&#x27; on bean with name &#x27;&quot; + beanName + &quot;&#x27;&quot;); &#125; else &#123; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;No default init method named &#x27;&quot; + initMethodName + &quot;&#x27; found on bean with name &#x27;&quot; + beanName + &quot;&#x27;&quot;); &#125; // Ignore non-existent default lifecycle methods. return; &#125; &#125; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Invoking init method &#x27;&quot; + initMethodName + &quot;&#x27; on bean with name &#x27;&quot; + beanName + &quot;&#x27;&quot;); &#125; /*将init方法转换成接口层面获取的initMethod*/ Method methodToInvoke = ClassUtils.getInterfaceMethodIfPossible(initMethod); try &#123; /*反射执行方法*/ ReflectionUtils.makeAccessible(methodToInvoke); methodToInvoke.invoke(bean); &#125; catch (InvocationTargetException ex) &#123; throw ex.getTargetException(); &#125;&#125; 经过一些列处理，反射调用初始化方法。 5.applyBeanPostProcessorsAfterInitialization()1234567891011121314151617@Overridepublic Object applyBeanPostProcessorsAfterInitialization(Object existingBean, String beanName) throws BeansException &#123; Object result = existingBean; for (BeanPostProcessor processor : getBeanPostProcessors()) &#123; Object current = processor.postProcessAfterInitialization(result, beanName); /*注意： * 一旦某个后置处理器返回的结果为空 * 就返回上一个后置处理器的结果，后面的后置处理器方法不在执行*/ if (current == null) &#123; return result; &#125; result = current; &#125; return result;&#125; 初始化之后，遍历所有的后置处理器，执行后置处理器的后置方法，如果有一个返回结果是空，直接返回，后面的后置处理器都不执行了。​ 至此，整个ioc容器的刷新流程就都分析完了，重点就在于单实例bean的创建流程，首先会去**getBean() doGetBean() getSinglton() createBean() doCreateBean() populateBean() initializeBean()**。下一篇我们将会去分析Spring的三级缓存和循环依赖。","categories":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/tags/Spring/"}]},{"title":"Spring[六]populateBean()","slug":"Spring/Spring[六]populateBean()","date":"2022-01-11T05:59:34.475Z","updated":"2022-01-11T06:09:48.766Z","comments":true,"path":"2022/01/11/Spring/Spring[六]populateBean()/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/Spring/Spring[%E5%85%AD]populateBean()/","excerpt":"","text":"在上一篇中我们分析完了**createBean()**，并详解讲解了三中创建单实例bean对象的方法，在bean实例创建出来之后，就是对bean的属性赋值和初始化操作，本篇我们继续往下分析。 1.populateBean()1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192protected void populateBean(String beanName, RootBeanDefinition mbd, @Nullable BeanWrapper bw) &#123; /*这里是判断假如beanWrapper是空，但是mbd中的属性还有值，那就说明需要抛出异常，因为没法给一个null赋值。*/ if (bw == null) &#123; if (mbd.hasPropertyValues()) &#123; throw new BeanCreationException( mbd.getResourceDescription(), beanName, &quot;Cannot apply property values to null instance&quot;); &#125; else &#123; // Skip property population phase for null instance. return; &#125; &#125; /*在设置属性之前，给任何实例化的bean后处理器修改bean状态的机会。例如，这可以用来支持各种类型的属性注入。*/ /* * mbd.isSynthetic()默认是false，取反成立。 * 判断有没有instantiationAwareBeanPostProcessors，条件成立说明有。 */ if (!mbd.isSynthetic() &amp;&amp; hasInstantiationAwareBeanPostProcessors()) &#123; /*这里又是后置处理器的一个调用点：实例化之后的调用，调用的是后置处理器的afterInstantiation方法。*/ for (InstantiationAwareBeanPostProcessor bp : getBeanPostProcessorCache().instantiationAware) &#123; /*返回值将决定当前实例需要在进行依赖注入处理，默认返回true*/ if (!bp.postProcessAfterInstantiation(bw.getWrappedInstance(), beanName)) &#123; return; &#125; &#125; &#125; /*下面是处理依赖注入的逻辑*/ /*三元运算符获取属性，有就把属性赋值给pvs ，没有就给一个null*/ PropertyValues pvs = (mbd.hasPropertyValues() ? mbd.getPropertyValues() : null); int resolvedAutowireMode = mbd.getResolvedAutowireMode(); /*这里判断依赖注入是按照名字注入还是按照类型注入，根据不同的选择走不同的处理逻辑。*/ if (resolvedAutowireMode == AUTOWIRE_BY_NAME || resolvedAutowireMode == AUTOWIRE_BY_TYPE) &#123; /*吧属性进行一个包装*/ MutablePropertyValues newPvs = new MutablePropertyValues(pvs); // Add property values based on autowire by name if applicable. /*按照名字注入，根据字段名称查找依赖bean完成注入*/ if (resolvedAutowireMode == AUTOWIRE_BY_NAME) &#123; autowireByName(beanName, mbd, bw, newPvs); &#125; // Add property values based on autowire by type if applicable. /*按照类型注入*/ if (resolvedAutowireMode == AUTOWIRE_BY_TYPE) &#123; autowireByType(beanName, mbd, bw, newPvs); &#125; /*相当于处理了依赖数据后的pvs*/ pvs = newPvs; &#125; /*表示当前是否拥有instantiationAwareBeanPostProcessors需要执行*/ boolean hasInstAwareBpps = hasInstantiationAwareBeanPostProcessors(); /*是否需要进行依赖检察，不重要*/ boolean needsDepCheck = (mbd.getDependencyCheck() != AbstractBeanDefinition.DEPENDENCY_CHECK_NONE); PropertyDescriptor[] filteredPds = null; if (hasInstAwareBpps) &#123; if (pvs == null) &#123; pvs = mbd.getPropertyValues(); &#125; /*后置处理器的调用点*/ for (InstantiationAwareBeanPostProcessor bp : getBeanPostProcessorCache().instantiationAware) &#123; /*典型应用：@Autowired 注解的注入*/ PropertyValues pvsToUse = bp.postProcessProperties(pvs, bw.getWrappedInstance(), beanName); if (pvsToUse == null) &#123; if (filteredPds == null) &#123; filteredPds = filterPropertyDescriptorsForDependencyCheck(bw, mbd.allowCaching); &#125; pvsToUse = bp.postProcessPropertyValues(pvs, filteredPds, bw.getWrappedInstance(), beanName); if (pvsToUse == null) &#123; return; &#125; &#125; pvs = pvsToUse; &#125; &#125; if (needsDepCheck) &#123; if (filteredPds == null) &#123; filteredPds = filterPropertyDescriptorsForDependencyCheck(bw, mbd.allowCaching); &#125; checkDependencies(beanName, mbd, filteredPds, pvs); &#125; if (pvs != null) &#123; /*将完成依赖注入（合并后的pvs）的信息应用到实例上去*/ /** * 1.对man进行属性赋值的过程 * 2.对women进行属性赋值 */ applyPropertyValues(beanName, mbd, bw, pvs); &#125;&#125; \u0000 上来一套组合拳，先判断假如bean包装对象是空的，但是mbd中的属性还有值，那就说明需要抛出异常，因为没法给一个null赋值。 接下来就是在属性赋值之前，给任何实例化的bean后置处理器修改bean状态的机会。例如，这可以用来支持各种类型的属性注入。**InstantiationAwareBeanPostProcessors** 这里又是一个后置处理器的调用点，实例化之后的调用，调用的是后置处理器的**afterInstantiation**方法。这个处理器的返回值将决定当前bean实例是否要进行属性注入，返回false则表示直接返回，默认返回true。\u0000 判断依赖注入按照名字注入还是按照类型注入，根据不同的选择走不同的处理逻辑。注意这里的处理并不是直接注入到bean中，而是解析到pvs中。 什么时候走这里？必须显式的指定了配置了依赖，才会走这里。 如果是按照名字注入，就执行**autowireByName()**。 如果是按照类型注入，就执行**autowireByType()**。 ​ 判断当前是否还有**InstantiationAwareBeanPostProcessors**需要执行。 判断当前是否需要进行依赖检查、 如果还有**InstantiationAwareBeanPostProcessors**需要执行，循环执行后置处理器的方法 如果需要依赖检查，就走依赖检查的逻辑 如果合并后的依赖信息（pvs）不为空，就执行**applyPropertyValues()**进行属性赋值。 ​ 接下来分析两种属性注入的方式，按照名字注入&amp;按照类型注入。 2.autowireByName()\u0000 1234567891011121314151617181920212223242526protected void autowireByName( String beanName, AbstractBeanDefinition mbd, BeanWrapper bw, MutablePropertyValues pvs) &#123; /*bean实例中有该字段，且有该字段的setter方法，但是在bd中没有property属性。*/ String[] propertyNames = unsatisfiedNonSimpleProperties(mbd, bw); /*便利查找出来的集合，完成依赖注入。*/ for (String propertyName : propertyNames) &#123; /*条件成立说明beanFactory存在当前属性的bean实例，可以注入，说明找到对应的依赖信息数据了*/ if (containsBean(propertyName)) &#123; /*拿到属性所对应的bean实例*/ Object bean = getBean(propertyName); /*在属性里面追加一个property*/ pvs.add(propertyName, bean); /*管理依赖信息*/ registerDependentBean(propertyName, beanName); if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Added autowiring by name from bean name &#x27;&quot; + beanName + &quot;&#x27; via property &#x27;&quot; + propertyName + &quot;&#x27; to bean named &#x27;&quot; + propertyName + &quot;&#x27;&quot;); &#125; &#125; else &#123; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Not autowiring property &#x27;&quot; + propertyName + &quot;&#x27; of bean &#x27;&quot; + beanName + &quot;&#x27; by name: no matching bean found&quot;); &#125; &#125; &#125;&#125; 总结：autowireByName主要完成以下逻辑：​ 获取需要填充对象得非简单类型得属性名； 遍历第一步获取得属性名，调用getBean方法从容器中获取此属性名对应的object； 然后，如果能找到，则把此属性名和object对象保存到pvs的propertyValueList里面。 3.autowireByType()\u0000 12345678910111213141516171819202122232425262728293031323334353637383940414243444546protected void autowireByType( String beanName, AbstractBeanDefinition mbd, BeanWrapper bw, MutablePropertyValues pvs) &#123; /*获取类型转换器*/ TypeConverter converter = getCustomTypeConverter(); if (converter == null) &#123; converter = bw; &#125; Set&lt;String&gt; autowiredBeanNames = new LinkedHashSet&lt;&gt;(4); /*拿到属性信息，循环遍历*/ String[] propertyNames = unsatisfiedNonSimpleProperties(mbd, bw); for (String propertyName : propertyNames) &#123; try &#123; PropertyDescriptor pd = bw.getPropertyDescriptor(propertyName); // Don&#x27;t try autowiring by type for type Object: never makes sense, // even if it technically is a unsatisfied, non-simple property. /*如果不是Object类型*/ if (Object.class != pd.getPropertyType()) &#123; /*拿到setter方法*/ MethodParameter methodParam = BeanUtils.getWriteMethodParameter(pd); // Do not allow eager init for type matching in case of a prioritized post-processor. boolean eager = !(bw.getWrappedInstance() instanceof PriorityOrdered); /*包装成一个依赖描述信息对象*/ DependencyDescriptor desc = new AutowireByTypeDependencyDescriptor(methodParam, eager); /*解析器根据依赖描述信息查找对象，或者 容器没有该对象实例的话，但是有该类型的bd的话，也会调用getBeanByType生成对象。*/ Object autowiredArgument = resolveDependency(desc, beanName, autowiredBeanNames, converter); /*查询出来的依赖对象加入到pvs里面*/ if (autowiredArgument != null) &#123; pvs.add(propertyName, autowiredArgument); &#125; /*将查询到的依赖对象注册*/ for (String autowiredBeanName : autowiredBeanNames) &#123; registerDependentBean(autowiredBeanName, beanName); if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Autowiring by type from bean name &#x27;&quot; + beanName + &quot;&#x27; via property &#x27;&quot; + propertyName + &quot;&#x27; to bean named &#x27;&quot; + autowiredBeanName + &quot;&#x27;&quot;); &#125; &#125; autowiredBeanNames.clear(); &#125; &#125; catch (BeansException ex) &#123; throw new UnsatisfiedDependencyException(mbd.getResourceDescription(), beanName, propertyName, ex); &#125; &#125;&#125; 获取当前对象的非简单类型的属性名数组propertyNames； 遍历属性名数组propertyNames，获取当前属性名多对应的类型filedType； 通过filedType找到匹配的候选Bean对象； 把属性名以及bean对象添加到pvs的propertyValueList里面。 可以看到，无论是按照名字注入还是按照类型的注入方式，其实里面都会调用同一个公共的方法。**unsatisfiedNonSimpleProperties()**，接下来分析这个方法是做什么的。 123456789101112131415161718192021protected String[] unsatisfiedNonSimpleProperties(AbstractBeanDefinition mbd, BeanWrapper bw) &#123; Set&lt;String&gt; result = new TreeSet&lt;&gt;(); /*拿到配置的properties集合*/ PropertyValues pvs = mbd.getPropertyValues(); /*拿到bean的所有字段信息，然后便利所有的字段信息*/ PropertyDescriptor[] pds = bw.getPropertyDescriptors(); /* * 条件1成立说明 当前字段有setter方法 * 条件2成立说明 当前字段类型是否在忽略自动注入列表中，条件成立，说明不在。 * 条件3成立说明 当前字段不在xml或者其他方式的配置中配置过。 * 条件4成立说明 当前字段类型是不是简单的八种基本数据类型。基本数据类型不允许自动注入。当前字段不是基本数据类型。 * */ for (PropertyDescriptor pd : pds) &#123; if (pd.getWriteMethod() != null &amp;&amp; !isExcludedFromDependencyCheck(pd) &amp;&amp; !pvs.contains(pd.getName()) &amp;&amp; !BeanUtils.isSimpleProperty(pd.getPropertyType())) &#123; result.add(pd.getName()); &#125; &#125; return StringUtils.toStringArray(result);&#125; 过滤出mbd中符合依赖注入条件的属性。 4.applyPropertyValues()大部分情况下，我们走属性注入的逻辑，其实都是对解析出来的依赖bean进行注入。我们来梳理一下大致的流程，看一下核心的逻辑即可。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697protected void applyPropertyValues(String beanName, BeanDefinition mbd, BeanWrapper bw, PropertyValues pvs) &#123; // 为空直接返回 if (pvs == null || pvs.isEmpty()) &#123; return; &#125; MutablePropertyValues mpvs = null; List&lt;PropertyValue&gt; original; if (System.getSecurityManager() != null) &#123; if (bw instanceof BeanWrapperImpl) &#123; ((BeanWrapperImpl) bw).setSecurityContext(getAccessControlContext()); &#125; &#125; if (pvs instanceof MutablePropertyValues) &#123; mpvs = (MutablePropertyValues) pvs; if (mpvs.isConverted()) &#123; // 如果已被设置转换完成，直接完成配置 try &#123; bw.setPropertyValues(mpvs); return; &#125; catch (BeansException ex) &#123; throw new BeanCreationException( mbd.getResourceDescription(), beanName, &quot;Error setting property values&quot;, ex); &#125; &#125; original = mpvs.getPropertyValueList(); &#125; else &#123; original = Arrays.asList(pvs.getPropertyValues()); &#125; TypeConverter converter = getCustomTypeConverter(); if (converter == null) &#123; converter = bw; &#125; // 创建BeanDefinitionValueResolver解析器，用来解析未被解析的PropertyValue。 BeanDefinitionValueResolver valueResolver = new BeanDefinitionValueResolver(this, beanName, mbd, converter); // 开始遍历检查original中的属性，对未被解析的先解析/已解析的直接加入deepCopy中，最后再填充到具体的Bean实例中 List&lt;PropertyValue&gt; deepCopy = new ArrayList&lt;PropertyValue&gt;(original.size()); boolean resolveNecessary = false; for (PropertyValue pv : original) &#123; // 如果属性已经转化，直接添加 if (pv.isConverted()) &#123; deepCopy.add(pv); &#125; else &#123; String propertyName = pv.getName(); Object originalValue = pv.getValue(); // 核心逻辑，解析获取实际的值 // 对于RuntimeReference，会解析拿到具体的beanName,最终通过getBean(beanName)拿到具体的对象 Object resolvedValue = valueResolver.resolveValueIfNecessary(pv, originalValue); Object convertedValue = resolvedValue; // 判断是否可以转换 boolean convertible = bw.isWritableProperty(propertyName) &amp;&amp; !PropertyAccessorUtils.isNestedOrIndexedProperty(propertyName); if (convertible) &#123; // 尝试进行转换 convertedValue = convertForProperty(resolvedValue, propertyName, bw, converter); &#125; // 避免需要重复转换，设定已转换 if (resolvedValue == originalValue) &#123; if (convertible) &#123; pv.setConvertedValue(convertedValue); &#125; deepCopy.add(pv); &#125; else if (convertible &amp;&amp; originalValue instanceof TypedStringValue &amp;&amp; !((TypedStringValue) originalValue).isDynamic() &amp;&amp; !(convertedValue instanceof Collection || ObjectUtils.isArray(convertedValue))) &#123; pv.setConvertedValue(convertedValue); deepCopy.add(pv); &#125; else &#123; resolveNecessary = true; deepCopy.add(new PropertyValue(pv, convertedValue)); &#125; &#125; &#125; if (mpvs != null &amp;&amp; !resolveNecessary) &#123; mpvs.setConverted(); &#125; // 完成设置 try &#123; bw.setPropertyValues(new MutablePropertyValues(deepCopy)); &#125; catch (BeansException ex) &#123; throw new BeanCreationException( mbd.getResourceDescription(), beanName, &quot;Error setting property values&quot;, ex); &#125;&#125; ​ 主要是关心下，解析出来的依赖，如果注入到当前正在执行属性赋值的bean对象。​ **valueResolver.resolveValueIfNecessary()** 利用值解析器解析当前bean实例所依赖的bean实例。​ **resolveReference()**通过这个方法去拿到当前属性赋值的bean需要注入的bean。​ **this.beanFactory.getBean()** 最终这里又递归回到了getBean()去获取所需要的bean对象。​ **registerDependentBean()**在获取到bean对象之后，会将获取到的bean对象注册到当前bean所依赖的bean集合。​ 至此，单实例bean的属性赋值大体流程我们就分析完了，下一篇我们将去分析单实例bean的初始化逻辑。 \u0000\u0000\u0000","categories":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/tags/Spring/"}]},{"title":"Spring[五]createBean()","slug":"Spring/Spring[五]createBean()","date":"2022-01-11T05:59:23.628Z","updated":"2022-01-11T06:10:12.056Z","comments":true,"path":"2022/01/11/Spring/Spring[五]createBean()/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/Spring/Spring[%E4%BA%94]createBean()/","excerpt":"","text":"在上一篇中，分析了getBean()方法的详细流程，里面涉及到了单实例bean的创建，createBean(),本篇将分析这个方法，分析一下Spring中单实例对象时如何创建出来的。 1.createBean1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677@Overrideprotected Object createBean(String beanName, RootBeanDefinition mbd,/*按照传递的参数构建bean的实例并且返回，一般很少使用到参数*/ @Nullable Object[] args) throws BeanCreationException &#123; /* * 创建实例使用的bean定义信息 * */ RootBeanDefinition mbdToUse = mbd; /* * 检查当前bean的定义信息是否有class信息，如果有的话返回class类型，如果没有的话，使用类加载器加载class实例加载到jvm，并返回class对象 * * */ Class&lt;?&gt; resolvedClass = resolveBeanClass(mbd, beanName); /* * 条件一：拿到了bean定义信息 实例化时候需要的真实class对象 * 条件二：说明bean定义信息在解析bean对象之前（resolveBeanClass()）是没有class对象的 * 条件三：说明mbd有className * * 都成立之后就会去重新初始化mbd对象，进行一层包装。 * */ if (resolvedClass != null &amp;&amp; !mbd.hasBeanClass() &amp;&amp; mbd.getBeanClassName() != null) &#123; mbdToUse = new RootBeanDefinition(mbd); mbdToUse.setBeanClass(resolvedClass); &#125; try &#123; /* * 预处理 方法重写的逻辑。。 给重写的方法打一个标。 * */ mbdToUse.prepareMethodOverrides(); &#125; catch (BeanDefinitionValidationException ex) &#123; throw new BeanDefinitionStoreException(mbdToUse.getResourceDescription(), beanName, &quot;Validation of method overrides failed&quot;, ex); &#125; try &#123; /* * 通过后置处理器在这里返回一个代理的实例对象 * 这里的代理对象不是spring 的 aop 的实现。 * 实例化之前 并不是init执行前后 后置处理器的执行机会 * */ Object bean = resolveBeforeInstantiation(beanName, mbdToUse); /*如果对象不为空，直接返回。*/ if (bean != null) &#123; return bean; &#125; &#125; catch (Throwable ex) &#123; throw new BeanCreationException(mbdToUse.getResourceDescription(), beanName, &quot;BeanPostProcessor before instantiation of bean failed&quot;, ex); &#125; try &#123; /* * 真正的创建bean的逻辑，spring里面真正干活的方法都是以do开头 * 创建bean实例 执行依赖注入 执行init 后置处理器的逻辑 * */ /** * 1.第一次创建man的逻辑 * 2.创建women */ Object beanInstance = doCreateBean(beanName, mbdToUse, args); return beanInstance; &#125; catch (BeanCreationException | ImplicitlyAppearedSingletonException ex) &#123; throw ex; &#125; catch (Throwable ex) &#123; throw new BeanCreationException( mbdToUse.getResourceDescription(), beanName, &quot;Unexpected exception during bean creation&quot;, ex); &#125;&#125; 首先使用**resolveBeanClass()**解析出当前bean的类型 如果解析出的bean类型不为空，并且mbd里面没有bean的类型，并且mbd的className不为空 创建一个新的**RootBeanDefinition**,将Class类型设置进去 处理方法重写的逻辑**prepareMethodOverrides()** **resolveBeforeInstantiation() **通过后置处理器在这里尝试返回一个代理对象，这里的代理并不是Aop的实现，此时是实例化之前，并不是**init()**执行前后 如果后置处理器成功返回了对象，直接返回 如果后置处理器没有返回一个代理对象，那就将创建bean的逻辑委派给**doCreateBean()**,返回创建好的bean实例对象。 ​ 按照执行流程来分析，我们先分析**resolveBeanClass()**方法。 2.resolveBeanClass()123456789101112131415161718@Nullableprotected Class&lt;?&gt; resolveBeanClass(RootBeanDefinition mbd, String beanName, Class&lt;?&gt;... typesToMatch) throws CannotLoadBeanClassException &#123; try &#123; if (mbd.hasBeanClass()) &#123; return mbd.getBeanClass(); &#125; return doResolveBeanClass(mbd, typesToMatch); &#125; catch (ClassNotFoundException ex) &#123; throw new CannotLoadBeanClassException(mbd.getResourceDescription(), beanName, mbd.getBeanClassName(), ex); &#125; catch (LinkageError err) &#123; throw new CannotLoadBeanClassException(mbd.getResourceDescription(), beanName, mbd.getBeanClassName(), err); &#125;&#125; 可以看到，这里其实又是一个委派模式。**doResolveBeanClass()**。​ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960@Nullableprivate Class&lt;?&gt; doResolveBeanClass(RootBeanDefinition mbd, Class&lt;?&gt;... typesToMatch) throws ClassNotFoundException &#123; ClassLoader beanClassLoader = getBeanClassLoader(); ClassLoader dynamicLoader = beanClassLoader; boolean freshResolve = false; if (!ObjectUtils.isEmpty(typesToMatch)) &#123; // When just doing type checks (i.e. not creating an actual instance yet), // use the specified temporary class loader (e.g. in a weaving scenario). ClassLoader tempClassLoader = getTempClassLoader(); if (tempClassLoader != null) &#123; dynamicLoader = tempClassLoader; freshResolve = true; if (tempClassLoader instanceof DecoratingClassLoader) &#123; DecoratingClassLoader dcl = (DecoratingClassLoader) tempClassLoader; for (Class&lt;?&gt; typeToMatch : typesToMatch) &#123; dcl.excludeClass(typeToMatch.getName()); &#125; &#125; &#125; &#125; String className = mbd.getBeanClassName(); if (className != null) &#123; Object evaluated = evaluateBeanDefinitionString(className, mbd); if (!className.equals(evaluated)) &#123; // A dynamically resolved expression, supported as of 4.2... if (evaluated instanceof Class) &#123; return (Class&lt;?&gt;) evaluated; &#125; else if (evaluated instanceof String) &#123; className = (String) evaluated; freshResolve = true; &#125; else &#123; throw new IllegalStateException(&quot;Invalid class name expression result: &quot; + evaluated); &#125; &#125; if (freshResolve) &#123; // When resolving against a temporary class loader, exit early in order // to avoid storing the resolved Class in the bean definition. if (dynamicLoader != null) &#123; try &#123; return dynamicLoader.loadClass(className); &#125; catch (ClassNotFoundException ex) &#123; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Could not load class [&quot; + className + &quot;] from &quot; + dynamicLoader + &quot;: &quot; + ex); &#125; &#125; &#125; return ClassUtils.forName(className, dynamicLoader); &#125; &#125; // Resolve regularly, caching the result in the BeanDefinition... return mbd.resolveBeanClass(beanClassLoader);&#125; 这里面没有什么核心的逻辑，不作具体分析。关注一下重要的逻辑，在bean实例创建之前尝试返回一个代理对象。​ 3.实例化之前尝试返回一个代理对象**resolveBeforeInstantiation()**​ 1234567891011121314151617181920212223242526protected Object resolveBeforeInstantiation(String beanName, RootBeanDefinition mbd) &#123; Object bean = null; if (!Boolean.FALSE.equals(mbd.beforeInstantiationResolved)) &#123; // hasInstantiationAwareBeanPostProcessors() 看当前需要执行的集合里面 //是否有相关的后置处理器，如果有的话，条件成立 ，才能执行if 里面的逻辑 if (!mbd.isSynthetic() &amp;&amp; hasInstantiationAwareBeanPostProcessors()) &#123; Class&lt;?&gt; targetType = determineTargetType(beanName, mbd); if (targetType != null) &#123; /* * 执行bean 实例化 前后的后置处理器 before 方法，默认是返回null，如果想要做特殊的逻辑处理 * 可以自己继承接口进行扩展。需要继承的接口：InstantiationAwareBeanPostProcessor * */ bean = applyBeanPostProcessorsBeforeInstantiation(targetType, beanName); /* * 如果bean真的在这里创建出来了，那么，下面就会执行bean实例化后的后置处理器逻辑 * */ if (bean != null) &#123; bean = applyBeanPostProcessorsAfterInitialization(bean, beanName); &#125; &#125; &#125; mbd.beforeInstantiationResolved = (bean != null); &#125; return bean;&#125; 如果有**InstantiationAwareBeanPostProcessors**这个类型的后置处理器 获取被代理对象的类型 如果类型不为空 执行bean实例化前后的后置处理器的before()，默认是返回null。如果想要做特殊的逻辑处理，可以自己继承接口进行扩展。**applyBeanPostProcessorsBeforeInstantiation()** \u0000 如果有特殊处理的逻辑，也就是返回了一个代理对象，那就执行bean实例化后的后置处理器方法。**applyBeanPostProcessorsAfterInitialization()** 返回后置处理器创建的bean对象3.1 applyBeanPostProcessorsBeforeInstantiation()12345678910、 @Nullable protected Object applyBeanPostProcessorsBeforeInstantiation(Class&lt;?&gt; beanClass, String beanName) &#123; for (InstantiationAwareBeanPostProcessor bp : getBeanPostProcessorCache().instantiationAware) &#123; Object result = bp.postProcessBeforeInstantiation(beanClass, beanName); if (result != null) &#123; return result; &#125; &#125; return null; &#125; 遍历所有的后置处理器，执行后置处理器的前置方法，如果某一个后置处理器返回了一个不为空的对象，直接返回该对象，后面的后置处理器的方法不在执行。3.2 applyBeanPostProcessorsAfterInitialization()1234567891011121314151617@Overridepublic Object applyBeanPostProcessorsAfterInitialization(Object existingBean, String beanName) throws BeansException &#123; Object result = existingBean; for (BeanPostProcessor processor : getBeanPostProcessors()) &#123; Object current = processor.postProcessAfterInitialization(result, beanName); /*注意： * 一旦某个后置处理器返回的结果为空 * 就返回上一个后置处理器的结果，后面的后置处理器方法不在执行*/ if (current == null) &#123; return result; &#125; result = current; &#125; return result;&#125; 遍历所有的后置处理器，执行后置处理器的后置方法，如果某一个后置处理器返回了null，直接返回null，后面的后置处理器的方法不在执行。​ **InstantiationAwareBeanPostProcessors**的逻辑可以看前面的Spring组件与注解篇，再次不再赘述。​ 4.doCreateBean()123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145protected Object doCreateBean(String beanName, RootBeanDefinition mbd, @Nullable Object[] args) throws BeanCreationException &#123; // BeanWrapper：一个包装的bean实例，继承了可配置的属性访问器 BeanWrapper instanceWrapper = null; /*如果是单实例bean对象，清理缓存并返回该对象，这里正常情况下是拿不到的，因为还未创建bean对象实例。*/ if (mbd.isSingleton()) &#123; instanceWrapper = this.factoryBeanInstanceCache.remove(beanName); &#125; /*如果上面的逻辑没拿到，说明还是正常的逻辑，执行创建bean实例的方法并且包装到wrapper中。*/ if (instanceWrapper == null) &#123; /** * 1.第一次走到这里是去创建man * 2.创建women */ instanceWrapper = createBeanInstance(beanName, mbd, args); &#125; /*获取包装好的bean实例对象*/ Object bean = instanceWrapper.getWrappedInstance(); /*拿到bean实例的Class类型*/ Class&lt;?&gt; beanType = instanceWrapper.getWrappedClass(); /*如果bean类型不为空，将bean类型设置到mergedBeanDefinition中*/ if (beanType != NullBean.class) &#123; mbd.resolvedTargetType = beanType; &#125; /*允许post-processors 在此刻对 merged bean definition 进行修改*/ synchronized (mbd.postProcessingLock) &#123; /*如果后置处理器的逻辑还尚未执行，那就在此刻执行，执行之后将是否执行过修改为true，这也就限定了后置处理器只能执行一次。*/ if (!mbd.postProcessed) &#123; try &#123; /*执行后置处理器的核心逻辑：合并bd信息，接下来就是populate处理依赖。*/ applyMergedBeanDefinitionPostProcessors(mbd, beanType, beanName); &#125; catch (Throwable ex) &#123; throw new BeanCreationException(mbd.getResourceDescription(), beanName, &quot;Post-processing of merged bean definition failed&quot;, ex); &#125; mbd.postProcessed = true; &#125; &#125; // Eagerly cache singletons to be able to resolve circular references // even when triggered by lifecycle interfaces like BeanFactoryAware. /* * 提前地缓存单例，以便能够解析循环引用，即使被生命周期接口(如BeanFactoryAware)触发。 * 其实这里的逻辑就是判断是否过早的暴露早期的bean实例，已经初始化，但是没实例化。 * */ boolean earlySingletonExposure = (mbd.isSingleton() &amp;&amp; this.allowCircularReferences &amp;&amp; isSingletonCurrentlyInCreation(beanName)); /*如果支持早期暴露单实例bean对象*/ if (earlySingletonExposure) &#123; /*日志打印*/ if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Eagerly caching bean &#x27;&quot; + beanName + &quot;&#x27; to allow for resolving potential circular references&quot;); &#125; /*加入到单例bean工厂，将早期bean&#x27;实例（已经实例化但是尚未初始化）的引用加入到第三级缓存 * getEarlyBeanReference（）*/ /** * 1.man现在刚创建出来，还没有进行属性赋值和初始化，此时将他放到第三级缓存 * 2.此时的women也是通过反射刚刚创建出来，还没有进行属性赋值和初始化的逻辑，此时把他放到了三级缓存中 */ addSingletonFactory(beanName, () -&gt; getEarlyBeanReference(beanName, mbd, bean)); &#125; // Initialize the bean instance. Object exposedObject = bean; try &#123; /*对bean进行属性赋值，依赖注入。*/ /** * 1.对man进行属性赋值 * 2.对women进行属性赋值 */ populateBean(beanName, mbd, instanceWrapper); /*生命周期中的初始化方法的调用*/ /** * 1.women先经过这里 */ exposedObject = initializeBean(beanName, exposedObject, mbd); &#125; catch (Throwable ex) &#123; if (ex instanceof BeanCreationException &amp;&amp; beanName.equals(((BeanCreationException) ex).getBeanName())) &#123; throw (BeanCreationException) ex; &#125; else &#123; throw new BeanCreationException( mbd.getResourceDescription(), beanName, &quot;Initialization of bean failed&quot;, ex); &#125; &#125; if (earlySingletonExposure) &#123; Object earlySingletonReference = getSingleton(beanName, false); //条件成立：说明当前bean实例是从二级缓存就获取到了 //说明产生了，循环依赖，三级缓存 当前对象的ObjectFactory.getObject()被调用过 if (earlySingletonReference != null) &#123; /*什么时候相等呢？ * 1.当前的真实实例不需要被代理 * 2.当前实例已经被代理过，，，是在ObjectFactory.getObject()方法调用的时候，实现的增强代理*/ if (exposedObject == bean) &#123; exposedObject = earlySingletonReference; &#125; else if (!this.allowRawInjectionDespiteWrapping &amp;&amp; hasDependentBean(beanName)) &#123; /*获取依赖当前bean的其他beanName*/ String[] dependentBeans = getDependentBeans(beanName); Set&lt;String&gt; actualDependentBeans = new LinkedHashSet&lt;&gt;(dependentBeans.length); for (String dependentBean : dependentBeans) &#123; /*如果依赖当前bean的bean已经创建完了，就把他加入到集合中去*/ if (!removeSingletonIfCreatedForTypeCheckOnly(dependentBean)) &#123; actualDependentBeans.add(dependentBean); &#125; &#125; /*当前bean正在创建，但是依赖当前bean的bean已经创建完了，那说明指定是有问题，不对劲。*/ /** * 为什么有问题？ * 因为当前对象的aop操作是在当前方法 initbean 完成的 * 在这之前 ，外部其他bean持有的当前bean实例 都是尚未增强的。 */ if (!actualDependentBeans.isEmpty()) &#123; throw new BeanCurrentlyInCreationException(beanName, &quot;Bean with name &#x27;&quot; + beanName + &quot;&#x27; has been injected into other beans [&quot; + StringUtils.collectionToCommaDelimitedString(actualDependentBeans) + &quot;] in its raw version as part of a circular reference, but has eventually been &quot; + &quot;wrapped. This means that said other beans do not use the final version of the &quot; + &quot;bean. This is often the result of over-eager type matching - consider using &quot; + &quot;&#x27;getBeanNamesForType&#x27; with the &#x27;allowEagerInit&#x27; flag turned off, for example.&quot;); &#125; &#125; &#125; &#125; // Register bean as disposable. try &#123; /*判断当前bean是否需要注册一个容器关闭时候的析构函数回调*/ registerDisposableBeanIfNecessary(beanName, bean, mbd); &#125; catch (BeanDefinitionValidationException ex) &#123; throw new BeanCreationException( mbd.getResourceDescription(), beanName, &quot;Invalid destruction signature&quot;, ex); &#125; return exposedObject;&#125; 如果是单实例bean对象，拿到该对象，并清理缓存，这里正常情况下是什么都拿不到的，因为还没有创建bean对象实例。 如果上面的逻辑什么也没有拿到，说明还是正常的情况，执行创建bean实例的方法并且包装到wrapper中。**createBeanInstance()** 获取包装好的bean实例对象，拿到bean实例对象的class类型，将他设置到mbd中。 在属性赋值之前，给尚未执行的后置处理器最后一次机会来执行，执行之后修改执行过状态为true，这也就限定了后置处理器只能执行一次。**applyMergedBeanDefinitionPostProcessors()** 判断是否允许过早的暴露早期bean实例的引用(已经初始化，但是尚未实例化) 将早期bean实例加入到三级缓存 对早期暴露的bean进行属性赋值**populateBean()** 在属性赋值之后执行bean对象的初始化方法**initializeBean()** 这里开始其实就是循环依赖的逻辑，首先判断是不是允许早期暴露的单实例bean对象，如果是的话 从缓存中拿到尝试获取bean，**getSingleton()** 如果从缓存拿到bean了，说明当前bean实例是从二级缓存就获取到了，说明产生了循环依赖 如果依赖于当前bean其他bean对象，，如果依赖当前bean的对象都已经创建完了，那就吧他们加入到集合中。 当前bean正在创建，但是依赖他的bean都创建完了，那就说明逻辑上出现问题了 为什么会出现问题？因为当前对象的aop操作是在当前方法的init方法里面执行的，在这之前，其他对象拿到的bean都是尚未增强的bean。 判断当前bean是否需要注册一个容器关闭时候执行的析构函数**registerDisposableBeanIfNecessary()** \u0000\u0000这里面其实主要关注五个点：**createBeanInstance()**，**applyMergedBeanDefinitionPostProcessors()**，**populateBean()**，**initializeBean()**，**registerDisposableBeanIfNecessary()**。​ 5.createBeanInstance()使用适当的实例化策略为指定的 bean 创建一个新实例：工厂方法，构造函数自动装配或简单实例化。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697protected BeanWrapper createBeanInstance(String beanName, RootBeanDefinition mbd, @Nullable Object[] args) &#123; // 解析bean定义信息中的 bean 的实例类型 Class&lt;?&gt; beanClass = resolveBeanClass(mbd, beanName); /* * 条件一：Class 实例不为空 * 条件二：Class实例的访问权限不是公开的 * 条件三：CLass实例的权限没打开，没打开就不能反射我记得。 * */ if (beanClass != null &amp;&amp; !Modifier.isPublic(beanClass.getModifiers()) &amp;&amp; !mbd.isNonPublicAccessAllowed()) &#123; throw new BeanCreationException(mbd.getResourceDescription(), beanName, &quot;Bean class isn&#x27;t public, and non-public access not allowed: &quot; + beanClass.getName()); &#125; /*spring5新特性，没研究*/ Supplier&lt;?&gt; instanceSupplier = mbd.getInstanceSupplier(); if (instanceSupplier != null) &#123; return obtainFromSupplier(instanceSupplier, beanName); &#125; /*说明bean标签配置了 factoryMethod属性：利用工厂创建对象的逻辑。*/ if (mbd.getFactoryMethodName() != null) &#123; return instantiateUsingFactoryMethod(beanName, mbd, args); &#125; // 创建相同bean的时候，可以走捷径。解析构造器比较耗时，所以如果一个对象创建两次，可以不用那么麻烦。 /* * resolved：bd对应的构造信息是否已经解析成可以发射调用的构造方法信息 * autowireNecessary：是否自动匹配构造方法 * * */ boolean resolved = false; boolean autowireNecessary = false; if (args == null) &#123; /* * mbd.resolvedConstructorOrFactoryMethod != null * 条件成立：说明bd的构造信息已经转化成可以反射调用的method，并且被缓存。 * */ synchronized (mbd.constructorArgumentLock) &#123; if (mbd.resolvedConstructorOrFactoryMethod != null) &#123; /*说明已经解析过*/ resolved = true; /*判断构造方法的参数是否已经被解析了，resolvedConstructorOrFactoryMethod 有值 * 且构造方法有参数，可以认为字段值就是true * 设么情况下为false？ * 1.resolvedConstructorOrFactoryMethod == null * 2.当resolvedConstructorOrFactoryMethod 表示的是默认的无参构造器的时候 * */ autowireNecessary = mbd.constructorArgumentsResolved; &#125; &#125; &#125; /* * 如果已经解析过， * * */ if (resolved) &#123; /*并且可以自动匹配构造方法*/ if (autowireNecessary) &#123; /*自动匹配有参构造器*/ return autowireConstructor(beanName, mbd, null, null); &#125; /*没有参数解析，也就是无参构造方法处理*/ else &#123; /** * 1.第一次去创建man * 2.创建women */ return instantiateBean(beanName, mbd); &#125; &#125; // Candidate constructors for autowiring? /** * 后置处理器的调用点：SmartInstantiationAwareBeanPostProcessor * 这个后置处理器里面默认是实现方法为空，想要执行这里的逻辑可以自己继承接口实现 * * 典型应用 @Autowired 打在了构造器方法上，就会用到后置处理器 AutowiredAnnotationBeanPostProcessor */ Constructor&lt;?&gt;[] ctors = determineConstructorsFromBeanPostProcessors(beanClass, beanName); /* * 条件1：后置处理器指定了构造方法数组 * 条件2：一般不会成立。bean标签里面配置了autowired=constructor 才会触发 * 条件3：说明bean标签中配置了构造参数信息 * 条件4：getBean时，args有参数 * */ if (ctors != null || mbd.getResolvedAutowireMode() == AUTOWIRE_CONSTRUCTOR || mbd.hasConstructorArgumentValues() || !ObjectUtils.isEmpty(args)) &#123; return autowireConstructor(beanName, mbd, ctors, args); &#125; /*判断是否在beanDefinition里面配置偏好构造器，默认实现是null。*/ // Preferred constructors for default construction? ctors = mbd.getPreferredConstructors(); if (ctors != null) &#123; return autowireConstructor(beanName, mbd, ctors, null); &#125; /*大部分情况下都是走这里：未指定构造参数，未设定偏好，使用无参构造器创建bean实例。*/ // No special handling: simply use no-arg constructor. return instantiateBean(beanName, mbd);&#125; 解析bd中的bean类型 对类型做一些校验 判断如果配置了bean标签里面的**factoryMethod**属性，走利用工厂创建对象的逻辑。**instantiateUsingFactoryMethod()** 进行一些属性判断，其实主要就是创建相同bean对象的时候，没有必要全部重新来做，可以走一些捷径 如果已经解析过&amp;可以自动匹配构造方法，那就走自动匹配有参构造器的逻辑。**autowireConstructor()** 如果是已经解析过&amp;无参构造器的处理方法，那就走**instantiateBean()** 接下来是一个后置处理器的调用点 _**SmartInstantiationAwareBeanPostProcessor**_，这个后置处理器里面默认实现为空，可以自己扩展。 典型的应用就是 @Autowired 注解打在了构造器上，就会用到后置处理器_**AutowiredAnnotationBeanPostProcessor**_。 判断是否需要执行构造器自动注入的逻辑 **autowireConstructor()** 判断是否在beandefinition里面配置了偏好的构造器，默认实现是null，如果是的话，就走**autowireConstructor()**。 大部分情况下，其实会走**instantiateBean()**。（未指定构造参数，未设置偏好，使用无参构造器创建bean对象实例） \u0000这里主要关心三个方法**instantiateBean()**&amp;**autowireConstructor()**&amp;**instantiateUsingFactoryMethod()**。​ 5.1 instantiateBean()\u0000 12345678910111213141516171819protected BeanWrapper instantiateBean(String beanName, RootBeanDefinition mbd) &#123; try &#123; /*获取实例化策略，调用实例化方法创建bean实例*/ /** * 1.反射去创建man * 2.反射去创建women */ Object beanInstance = getInstantiationStrategy().instantiate(mbd, beanName, this); /*根据实例创建bean实例的包装器*/ BeanWrapper bw = new BeanWrapperImpl(beanInstance); /*对包装器进行一些参数，工具设置*/ initBeanWrapper(bw); return bw; &#125; catch (Throwable ex) &#123; throw new BeanCreationException( mbd.getResourceDescription(), beanName, &quot;Instantiation of bean failed&quot;, ex); &#125;&#125; 首先获取到实例化的策略，反射去创建对象，根据bean去创建对象的包装器，对包装器进行赋能。**instantiate()** 12345678910111213141516171819202122232425262728293031@Overridepublic Object instantiate(RootBeanDefinition bd, @Nullable String beanName, BeanFactory owner) &#123; // Don&#x27;t override the class with CGLIB if no overrides. /*不考虑方法覆盖*/ if (!bd.hasMethodOverrides()) &#123; Constructor&lt;?&gt; constructorToUse; synchronized (bd.constructorArgumentLock) &#123; constructorToUse = (Constructor&lt;?&gt;) bd.resolvedConstructorOrFactoryMethod; if (constructorToUse == null) &#123; final Class&lt;?&gt; clazz = bd.getBeanClass(); if (clazz.isInterface()) &#123; throw new BeanInstantiationException(clazz, &quot;Specified class is an interface&quot;); &#125; try &#123; constructorToUse = clazz.getDeclaredConstructor(); bd.resolvedConstructorOrFactoryMethod = constructorToUse; &#125; catch (Throwable ex) &#123; throw new BeanInstantiationException(clazz, &quot;No default constructor found&quot;, ex); &#125; &#125; &#125; /*实例化*/ return BeanUtils.instantiateClass(constructorToUse); &#125; else &#123; // Must generate CGLIB subclass. /*方法覆盖的逻辑*/ return instantiateWithMethodInjection(bd, beanName, owner); &#125;&#125; **instantiateClass()** 12345678910111213141516171819202122232425262728293031323334353637383940414243public static &lt;T&gt; T instantiateClass(Constructor&lt;T&gt; ctor, Object... args) throws BeanInstantiationException &#123; Assert.notNull(ctor, &quot;Constructor must not be null&quot;); try &#123; ReflectionUtils.makeAccessible(ctor); if (KotlinDetector.isKotlinReflectPresent() &amp;&amp; KotlinDetector.isKotlinType(ctor.getDeclaringClass())) &#123; return KotlinDelegate.instantiateClass(ctor, args); &#125; else &#123; Class&lt;?&gt;[] parameterTypes = ctor.getParameterTypes(); Assert.isTrue(args.length &lt;= parameterTypes.length, &quot;Can&#x27;t specify more arguments than constructor parameters&quot;); /*这里的参数数组是为了防止某个类型传过来的值是null，如果是null，就是用默认的值*/ Object[] argsWithDefaultValues = new Object[args.length]; for (int i = 0 ; i &lt; args.length; i++) &#123; if (args[i] == null) &#123; Class&lt;?&gt; parameterType = parameterTypes[i]; /*从这里就能看到是不是选用了默认值*/ /* * DEFAULT_TYPE_VALUES :存各种基本数据类型的默认值。 * */ argsWithDefaultValues[i] = (parameterType.isPrimitive() ? DEFAULT_TYPE_VALUES.get(parameterType) : null); &#125; else &#123; argsWithDefaultValues[i] = args[i]; &#125; &#125; /*通过构造器来进行反射实例化创建bean实例。*/ return ctor.newInstance(argsWithDefaultValues); &#125; &#125; catch (InstantiationException ex) &#123; throw new BeanInstantiationException(ctor, &quot;Is it an abstract class?&quot;, ex); &#125; catch (IllegalAccessException ex) &#123; throw new BeanInstantiationException(ctor, &quot;Is the constructor accessible?&quot;, ex); &#125; catch (IllegalArgumentException ex) &#123; throw new BeanInstantiationException(ctor, &quot;Illegal arguments for constructor&quot;, ex); &#125; catch (InvocationTargetException ex) &#123; throw new BeanInstantiationException(ctor, &quot;Constructor threw exception&quot;, ex.getTargetException()); &#125;&#125; 最终通过单实例bean的构造器反射创建对象。 5.2autowireConstructor()12345protected BeanWrapper autowireConstructor( String beanName, RootBeanDefinition mbd, @Nullable Constructor&lt;?&gt;[] ctors, @Nullable Object[] explicitArgs) &#123; /*通过构造函数解析器的autowireConstructor方法来创建bean实例*/ return new ConstructorResolver(this).autowireConstructor(beanName, mbd, ctors, explicitArgs);&#125; 自动装配构造函数行为。如果指定了显式的构造函数参数值，也可以，将所有剩余参数与来自bean工厂的bean匹配。这块对应于构造函数注入，在这种模式下spring的bean工厂能够承载期望基于构造函数的依赖项解析的组件。**autowireConstructor()** 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209public BeanWrapper autowireConstructor(String beanName, RootBeanDefinition mbd, @Nullable Constructor&lt;?&gt;[] chosenCtors, @Nullable Object[] explicitArgs) &#123; /*创建了一个bean包装器的实例*/ BeanWrapperImpl bw = new BeanWrapperImpl(); /*初始化bean包装器*/ this.beanFactory.initBeanWrapper(bw); /*最终反射调用的构造器*/ Constructor&lt;?&gt; constructorToUse = null; /*holder持有的是：实例化的时候，真正用到的参数*/ ArgumentsHolder argsHolderToUse = null; /*实例化的时候使用到的参数*/ Object[] argsToUse = null; /*如果getbean传的参数不为空，使用到的参数就是指定的参数*/ if (explicitArgs != null) &#123; argsToUse = explicitArgs; &#125; else &#123;/*getBean的时候没有指定参数*/ /*表示构造器参数需要做类型转换，参数引用*/ Object[] argsToResolve = null; synchronized (mbd.constructorArgumentLock) &#123; /*到缓存去看看有没有缓存过构造器*/ constructorToUse = (Constructor&lt;?&gt;) mbd.resolvedConstructorOrFactoryMethod; /*缓存有解析好的构造器并且构造器参数已经被解析过 说明不是第一次通过bd生成实例*/ if (constructorToUse != null &amp;&amp; mbd.constructorArgumentsResolved) &#123; // Found a cached constructor... argsToUse = mbd.resolvedConstructorArguments; if (argsToUse == null) &#123; argsToResolve = mbd.preparedConstructorArguments; &#125; &#125; &#125; /*条件成立说明：resolvedConstructorArguments==null*/ if (argsToResolve != null) &#123; /*解析构造参数*/ argsToUse = resolvePreparedArguments(beanName, mbd, bw, constructorToUse, argsToResolve); &#125; &#125; /*条件成立说明缓存机制失败，或者是第一次创建，需要匹配构造器。*/ if (constructorToUse == null || argsToUse == null) &#123; // Take specified constructors, if any. /*chosenCtors什么时候有值呢？构造器上有@Autowired注解的时候。*/ Constructor&lt;?&gt;[] candidates = chosenCtors; /*条件成立说明：外部程序调用方法的时候并没有指定可选用的构造器，需要通过class拿到构造器信息*/ if (candidates == null) &#123; Class&lt;?&gt; beanClass = mbd.getBeanClass(); try &#123; /*如果非公开的方法也允许访问就获取所有的构造器，否则获取可以访问的构造器。*/ candidates = (mbd.isNonPublicAccessAllowed() ? beanClass.getDeclaredConstructors() : beanClass.getConstructors()); &#125; catch (Throwable ex) &#123; throw new BeanCreationException(mbd.getResourceDescription(), beanName, &quot;Resolution of declared constructors on bean Class [&quot; + beanClass.getName() + &quot;] from ClassLoader [&quot; + beanClass.getClassLoader() + &quot;] failed&quot;, ex); &#125; &#125; /*执行到这里，可选用的构造器列表已经转备好了，但是还没确定具体是哪一个*/ /*如果只有一个构造器并且getBean没有指定构造器参数，并且bean定义信息里面没有参数，那就是用默认的无参构造器*/ if (candidates.length == 1 &amp;&amp; explicitArgs == null &amp;&amp; !mbd.hasConstructorArgumentValues()) &#123; Constructor&lt;?&gt; uniqueCandidate = candidates[0]; if (uniqueCandidate.getParameterCount() == 0) &#123; synchronized (mbd.constructorArgumentLock) &#123; mbd.resolvedConstructorOrFactoryMethod = uniqueCandidate; mbd.constructorArgumentsResolved = true; mbd.resolvedConstructorArguments = EMPTY_ARGS; &#125; bw.setBeanInstance(instantiate(beanName, mbd, uniqueCandidate, EMPTY_ARGS)); return bw; &#125; &#125; // 需要解析到底是用哪一个构造器 boolean autowiring = (chosenCtors != null || mbd.getResolvedAutowireMode() == AutowireCapableBeanFactory.AUTOWIRE_CONSTRUCTOR); /*完成解析后的构造器参数值列表*/ ConstructorArgumentValues resolvedValues = null; /*构造器参数的个数*/ int minNrOfArgs; if (explicitArgs != null) &#123; minNrOfArgs = explicitArgs.length; &#125; else &#123; /*从beanDefinition拿到构造器参数*/ ConstructorArgumentValues cargs = mbd.getConstructorArgumentValues(); resolvedValues = new ConstructorArgumentValues(); /*将此 bean 的构造函数参数解析为 resolveValues 对象。 这可能涉及查找其他 bean。 此方法也用于处理静态工厂方法的调用。，*/ minNrOfArgs = resolveConstructorArguments(beanName, mbd, bw, cargs, resolvedValues); &#125; /*给可选用的构造器数组排序，排序规则： public &gt; no public more args &gt; no args*/ AutowireUtils.sortConstructors(candidates); /*这个值越低，说明当前构造器参数列表类型和构造参数的匹配值越高*/ int minTypeDiffWeight = Integer.MAX_VALUE; /*尚未筛选或者筛选完还未确定的构造器*/ Set&lt;Constructor&lt;?&gt;&gt; ambiguousConstructors = null; Deque&lt;UnsatisfiedDependencyException&gt; causes = null; /*筛选可选的构造方法，找出一个匹配度最高的构造函数*/ for (Constructor&lt;?&gt; candidate : candidates) &#123; /*获取当前处理的构造器参数个数*/ int parameterCount = candidate.getParameterCount(); /*这里判断的指标都是上面循环筛选出来的东西 * 因为candidates是排过序的 排序规则：public &gt; no public &gt; 多参数的 &gt; 参数少的 * 当前筛选出来的构造器优先级一定是优先于后面的构造器的 * */ if (constructorToUse != null &amp;&amp; argsToUse != null &amp;&amp; argsToUse.length &gt; parameterCount) &#123; // Already found greedy constructor that can be satisfied -&gt; // do not look any further, there are only less greedy constructors left. break; &#125;/*表示当前构造器参数 小于 bd中配置了构造器参数个数，说明匹配不上。*/ if (parameterCount &lt; minNrOfArgs) &#123; continue; &#125; /*构造器参数的持有对象*/ ArgumentsHolder argsHolder; /*拿到当前构造器的参数类型数组*/ Class&lt;?&gt;[] paramTypes = candidate.getParameterTypes(); /*已经解析后的构造器参数值不为空，说明bd中是有参数的，需要做匹配逻辑*/ if (resolvedValues != null) &#123; try &#123; /*根据构造器参数注解@ConstructorProperties拿到所有参数的名字*/ String[] paramNames = ConstructorPropertiesChecker.evaluate(candidate, parameterCount); /*如果数组的名字是空,那么就都是默认的名字*/ if (paramNames == null) &#123; ParameterNameDiscoverer pnd = this.beanFactory.getParameterNameDiscoverer(); if (pnd != null) &#123; paramNames = pnd.getParameterNames(candidate); &#125; &#125; argsHolder = createArgumentArray(beanName, mbd, resolvedValues, bw, paramTypes, paramNames, getUserDeclaredConstructor(candidate), autowiring, candidates.length == 1); &#125; catch (UnsatisfiedDependencyException ex) &#123; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Ignoring constructor [&quot; + candidate + &quot;] of bean &#x27;&quot; + beanName + &quot;&#x27;: &quot; + ex); &#125; // Swallow and try next constructor. if (causes == null) &#123; causes = new ArrayDeque&lt;&gt;(1); &#125; causes.add(ex); continue; &#125; &#125; else &#123; // Explicit arguments given -&gt; arguments length must match exactly. if (parameterCount != explicitArgs.length) &#123; continue; &#125; argsHolder = new ArgumentsHolder(explicitArgs); &#125; /*typeDiffWeight数值越高说明构造器与参数的匹配度越低。 * 计算出当前构造器参数类型与当前构造器参数匹配度*/ int typeDiffWeight = (mbd.isLenientConstructorResolution() ? argsHolder.getTypeDifferenceWeight(paramTypes) : argsHolder.getAssignabilityWeight(paramTypes)); // Choose this constructor if it represents the closest match. /*条件成立说明：当前循环处理的构造器比上一次帅选出来的更合适*/ if (typeDiffWeight &lt; minTypeDiffWeight) &#123; constructorToUse = candidate; argsHolderToUse = argsHolder; argsToUse = argsHolder.arguments; minTypeDiffWeight = typeDiffWeight; ambiguousConstructors = null; &#125; /*条件成立说明：当前处理的构造器 他计算出来的typeDiffWeight值与上一次筛选出来的最优先的构造器的值一致，有模棱两可的情况。加入到模棱两可集合。*/ else if (constructorToUse != null &amp;&amp; typeDiffWeight == minTypeDiffWeight) &#123; if (ambiguousConstructors == null) &#123; ambiguousConstructors = new LinkedHashSet&lt;&gt;(); ambiguousConstructors.add(constructorToUse); &#125; ambiguousConstructors.add(candidate); &#125; &#125; /*条件成立：没找到可用构造器，所以直接报错。*/ if (constructorToUse == null) &#123; if (causes != null) &#123; UnsatisfiedDependencyException ex = causes.removeLast(); for (Exception cause : causes) &#123; this.beanFactory.onSuppressedException(cause); &#125; throw ex; &#125; throw new BeanCreationException(mbd.getResourceDescription(), beanName, &quot;Could not resolve matching constructor on bean class [&quot; + mbd.getBeanClassName() + &quot;] &quot; + &quot;(hint: specify index/type/name arguments for simple parameters to avoid type ambiguities)&quot;); &#125; /*条件成立：模棱两可的构造器集合有值，并且beanDefinition里面的构造器指定策略是狭窄策略，这个时候也要报错。*/ else if (ambiguousConstructors != null &amp;&amp; !mbd.isLenientConstructorResolution()) &#123; throw new BeanCreationException(mbd.getResourceDescription(), beanName, &quot;Ambiguous constructor matches found on bean class [&quot; + mbd.getBeanClassName() + &quot;] &quot; + &quot;(hint: specify index/type/name arguments for simple parameters to avoid type ambiguities): &quot; + ambiguousConstructors); &#125; /*条件成立：说明自动匹配成功了，需要进行缓存，方便后来者继续使用mergerdbeanDefinition来创建实例*/ if (explicitArgs == null &amp;&amp; argsHolderToUse != null) &#123; argsHolderToUse.storeCache(mbd, constructorToUse); &#125; &#125; Assert.state(argsToUse != null, &quot;Unresolved constructor arguments&quot;); /*根据上面选择的构造器和解析出来的参数，通过instantiate方法反射创建bean对象实例，最终将实例设置到beanWrapper的beanInstance实例里面。*/ bw.setBeanInstance(instantiate(beanName, mbd, constructorToUse, argsToUse)); return bw;&#125; 5.3 instantiateUsingFactoryMethod()12345protected BeanWrapper instantiateUsingFactoryMethod( String beanName, RootBeanDefinition mbd, @Nullable Object[] explicitArgs) &#123; //工厂模式创建对象的方法 return new ConstructorResolver(this).instantiateUsingFactoryMethod(beanName, mbd, explicitArgs);&#125; **instantiateUsingFactoryMethod()**​ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308public BeanWrapper instantiateUsingFactoryMethod( String beanName, RootBeanDefinition mbd, @Nullable Object[] explicitArgs) &#123; //创建一个空的bean实例的包装器 BeanWrapperImpl bw = new BeanWrapperImpl(); //进行一些参数设置，给包装器赋能 this.beanFactory.initBeanWrapper(bw); //工厂bean对象的引用 Object factoryBean; //工厂bean对象的类型 Class&lt;?&gt; factoryClass; //是不是静态工厂 boolean isStatic; //获取工厂bean的名字 String factoryBeanName = mbd.getFactoryBeanName(); //如果工厂bean的名字不为空 if (factoryBeanName != null) &#123; //如果工厂bean的名字和要创建的bean名字一样，那么说明出现问题了，这个时候要抛异常 if (factoryBeanName.equals(beanName)) &#123; throw new BeanDefinitionStoreException(mbd.getResourceDescription(), beanName, &quot;factory-bean reference points back to the same bean definition&quot;); &#125; //获取工厂bean对象 factoryBean = this.beanFactory.getBean(factoryBeanName); //如果mbd是单实例的 &amp;&amp; 当前一级缓存里面包含这个要创建的单实例bean //这个时候说明不正常，为啥呢？我们要通过工厂模式创建的bean，被以其他方式创建出来了，非正常途径得到的bean if (mbd.isSingleton() &amp;&amp; this.beanFactory.containsSingleton(beanName)) &#123; throw new ImplicitlyAppearedSingletonException(); &#125; //给当前要创建的bean实例对象注册一个依赖bean，这个依赖bean就是他的工厂bean this.beanFactory.registerDependentBean(factoryBeanName, beanName); factoryClass = factoryBean.getClass(); isStatic = false; &#125; //走到这里的情况就是工厂bean没有名字，什么情况会走到这里？通过静态工厂来创建对象 else &#123; // It&#x27;s a static factory method on the bean class. //如果依赖静态工厂来创建bean实例对象的话，那么必须要有工厂bean的类型，否则没法创建 if (!mbd.hasBeanClass()) &#123; throw new BeanDefinitionStoreException(mbd.getResourceDescription(), beanName, &quot;bean definition declares neither a bean class nor a factory-bean reference&quot;); &#125; factoryBean = null; factoryClass = mbd.getBeanClass(); isStatic = true; &#125; //创建bean对象的工厂方法 Method factoryMethodToUse = null; //持有创建对象需要的参数 ArgumentsHolder argsHolderToUse = null; //创建对象需要使用的参数 Object[] argsToUse = null; //getBean()方法传入的参数 if (explicitArgs != null) &#123; argsToUse = explicitArgs; &#125; else &#123;//这里的逻辑就是getBean()的时候没有参数传递进来 Object[] argsToResolve = null; synchronized (mbd.constructorArgumentLock) &#123; //获取已经解析过的工厂方法 factoryMethodToUse = (Method) mbd.resolvedConstructorOrFactoryMethod; //如果工厂方法已经解析过，&amp;&amp; mbd的构造器参数也已经解析过 if (factoryMethodToUse != null &amp;&amp; mbd.constructorArgumentsResolved) &#123; // Found a cached factory method... //这里应该就是判断构造器参数有没有解析过，没有解析过的话，就去设置解析状态为准备解析 argsToUse = mbd.resolvedConstructorArguments; if (argsToUse == null) &#123; argsToResolve = mbd.preparedConstructorArguments; &#125; &#125; &#125; //判断构造器参数尚未解析过，就去解析构造器参数 if (argsToResolve != null) &#123; argsToUse = resolvePreparedArguments(beanName, mbd, bw, factoryMethodToUse, argsToResolve); &#125; &#125; //如果工厂方法为空 || 需要使用的参数为空 if (factoryMethodToUse == null || argsToUse == null) &#123; // Need to determine the factory method... // Try all methods with this name to see if they match the given arguments. //需要一个一个方法尝试，去找到创建bean所需要的工厂方法 factoryClass = ClassUtils.getUserClass(factoryClass); List&lt;Method&gt; candidates = null; //如果工厂方法是唯一的 &amp;&amp; 当前尚未有指定工厂方法 if (mbd.isFactoryMethodUnique) &#123; if (factoryMethodToUse == null) &#123; //获取mbd里面已经解析过的工厂方法 factoryMethodToUse = mbd.getResolvedFactoryMethod(); &#125; //如果mbd里面有已经解析好的工厂方法，把他放到集合 if (factoryMethodToUse != null) &#123; candidates = Collections.singletonList(factoryMethodToUse); &#125; &#125; //如果方法列表为空，有什么情况？ //可能是上面mbd里面没有已经指定的工厂方法 //也可能是工厂方法不是唯一的，一个类有多个工厂方法，这个时候工厂方法的名字都是一样的，只是方法形参不同 if (candidates == null) &#123; candidates = new ArrayList&lt;&gt;(); //拿到工厂里面的所有的方法 Method[] rawCandidates = getCandidateMethods(factoryClass, mbd); //循环遍历，将满足条件的方法加入到集合 //满足什么条件？ 是静态的方法 &amp;&amp; 名字是指定的工厂方法的名字 for (Method candidate : rawCandidates) &#123; if (Modifier.isStatic(candidate.getModifiers()) == isStatic &amp;&amp; mbd.isFactoryMethod(candidate)) &#123; candidates.add(candidate); &#125; &#125; &#125; /*执行到这里，其实可选用的方法列表已经都帅选出来了，但是还没有具体确定是哪一个*/ //如果恰好只匹配到一个方法 &amp;&amp; 参数为空 &amp;&amp; 要创建的单实例bean 的构造参数也为空 ，那这里就是使用无参构造器的情况 if (candidates.size() == 1 &amp;&amp; explicitArgs == null &amp;&amp; !mbd.hasConstructorArgumentValues()) &#123; //将这个方法指定为唯一的工厂方法 Method uniqueCandidate = candidates.get(0); //如果工厂方法没有形参 if (uniqueCandidate.getParameterCount() == 0) &#123; //将mbd的工厂方法指定为这个方法 mbd.factoryMethodToIntrospect = uniqueCandidate; //设置一些参数，给工厂方法创建对象赋能 synchronized (mbd.constructorArgumentLock) &#123; mbd.resolvedConstructorOrFactoryMethod = uniqueCandidate; mbd.constructorArgumentsResolved = true; mbd.resolvedConstructorArguments = EMPTY_ARGS; &#125; //通过这个匹配到的工厂方法 ，利用 无参的工厂方法/构造器创建对象 //其实本质上还是利用反射调用工厂bean的工厂方法 bw.setBeanInstance(instantiate(beanName, mbd, factoryBean, uniqueCandidate, EMPTY_ARGS)); return bw; &#125; &#125; //如果匹配到了多个方法 ，对匹配到的方法进行一个排序 排序规则 ： public &gt; no public &amp;&amp; more args &gt; no args if (candidates.size() &gt; 1) &#123; // explicitly skip immutable singletonList candidates.sort(AutowireUtils.EXECUTABLE_COMPARATOR); &#125; //构造参数 ConstructorArgumentValues resolvedValues = null; //已解析的自动装配代码 == 指示自动装配可以满足的最贪婪的构造函数的常量 //需要解析到底是哪一个方法 boolean autowiring = (mbd.getResolvedAutowireMode() == AutowireCapableBeanFactory.AUTOWIRE_CONSTRUCTOR); //选择权重 值越低，表示匹配度越高 int minTypeDiffWeight = Integer.MAX_VALUE; //模棱两可的构造器集合 Set&lt;Method&gt; ambiguousFactoryMethods = null; //通过getBean()传入的参数的个数/构造器参数的个数 int minNrOfArgs; //如果通过getBean()传入的参数不为空 ，获取参数的个数 if (explicitArgs != null) &#123; minNrOfArgs = explicitArgs.length; &#125; else &#123; // // getBean()没有传入参数，这种情况下，我们就得自己去解析bd里面的构造器 if (mbd.hasConstructorArgumentValues()) &#123; //从bd去拿到构造器 ConstructorArgumentValues cargs = mbd.getConstructorArgumentValues(); resolvedValues = new ConstructorArgumentValues(); //将当前bean的方法参数或构造器参数解析 ，这里可能涉及到查找其他bean minNrOfArgs = resolveConstructorArguments(beanName, mbd, bw, cargs, resolvedValues); &#125; else &#123;//这就是无参构造器的情况 minNrOfArgs = 0; &#125; &#125; Deque&lt;UnsatisfiedDependencyException&gt; causes = null; //遍历匹配到的多个方法 for (Method candidate : candidates) &#123; //获取方法的参数个数 int parameterCount = candidate.getParameterCount(); /*这里判断的指标都是上面循环筛选出来的东西 * 因为candidates是排过序的 排序规则：public &gt; no public &gt; 多参数的 &gt; 参数少的 * 当前筛选出来的构造器优先级一定是优先于后面的构造器的 * */ if (parameterCount &gt;= minNrOfArgs) &#123; ArgumentsHolder argsHolder; //进行类型匹配 Class&lt;?&gt;[] paramTypes = candidate.getParameterTypes(); //如果显式参数不为空 if (explicitArgs != null) &#123; // bd中的参数个数 和 当前给定的参数个数匹配不上 ，直接淘汰当前方法 if (paramTypes.length != explicitArgs.length) &#123; continue; &#125; //将显式参数放到holder里面 argsHolder = new ArgumentsHolder(explicitArgs); &#125; else &#123;//这里是显式参数为空的情况 // 已解析的构造函数参数:需要类型转换和/或自动注入 try &#123; //存放方法的参数名数组 String[] paramNames = null; //参数解析器 ParameterNameDiscoverer pnd = this.beanFactory.getParameterNameDiscoverer(); if (pnd != null) &#123; //解析方法的参数名字 paramNames = pnd.getParameterNames(candidate); &#125; //对参数进行封装 argsHolder = createArgumentArray(beanName, mbd, resolvedValues, bw, paramTypes, paramNames, candidate, autowiring, candidates.size() == 1); &#125; catch (UnsatisfiedDependencyException ex) &#123; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Ignoring factory method [&quot; + candidate + &quot;] of bean &#x27;&quot; + beanName + &quot;&#x27;: &quot; + ex); &#125; // Swallow and try next overloaded factory method. if (causes == null) &#123; causes = new ArrayDeque&lt;&gt;(1); &#125; causes.add(ex); continue; &#125; &#125; //计算当前方法参数和当前方法的匹配度 int typeDiffWeight = (mbd.isLenientConstructorResolution() ? argsHolder.getTypeDifferenceWeight(paramTypes) : argsHolder.getAssignabilityWeight(paramTypes)); // 条件成立，说明本次的匹配度高于上一轮 if (typeDiffWeight &lt; minTypeDiffWeight) &#123; factoryMethodToUse = candidate; argsHolderToUse = argsHolder; argsToUse = argsHolder.arguments; minTypeDiffWeight = typeDiffWeight; ambiguousFactoryMethods = null; &#125; /* * 了解模糊性:对于具有相同参数数量的方法，如果存在相同类型的差异权重，则收集这样的候选项，并最终引发模糊性异常。 * 但是，只在非宽松的构造函数解析模式下执行该检查，并显式忽略重写的方法(具有相同的参数签名)。 * */ /*条件成立说明：当前处理的构造器 他计算出来的typeDiffWeight值与上一次筛选出来的最优先的构造器的值一致，有模棱两可的情况。加入到模棱两可集合*/ else if (factoryMethodToUse != null &amp;&amp; typeDiffWeight == minTypeDiffWeight &amp;&amp; !mbd.isLenientConstructorResolution() &amp;&amp; paramTypes.length == factoryMethodToUse.getParameterCount() &amp;&amp; !Arrays.equals(paramTypes, factoryMethodToUse.getParameterTypes())) &#123; //如果模棱两可的构造方法为空，就创建一个新的集合， 把满足条件的 构造器收集起来 if (ambiguousFactoryMethods == null) &#123; ambiguousFactoryMethods = new LinkedHashSet&lt;&gt;(); ambiguousFactoryMethods.add(factoryMethodToUse); &#125; //保存模棱两可的构造方法 ambiguousFactoryMethods.add(candidate); &#125; &#125; &#125; //如果要使用的工厂方法为空 或 要使用的参数为空，意思就是没找到创建对象的方法，那就得报错了； if (factoryMethodToUse == null || argsToUse == null) &#123; //如果有异常信息，记录异常信息 if (causes != null) &#123; UnsatisfiedDependencyException ex = causes.removeLast(); for (Exception cause : causes) &#123; this.beanFactory.onSuppressedException(cause); &#125; throw ex; &#125; List&lt;String&gt; argTypes = new ArrayList&lt;&gt;(minNrOfArgs); //走到这里就是可用的方法为空，参数不为空 if (explicitArgs != null) &#123; //保存参数类型 for (Object arg : explicitArgs) &#123; argTypes.add(arg != null ? arg.getClass().getSimpleName() : &quot;null&quot;); &#125; &#125; //如果构造器解析出的参数不为空，记录构造器参数的类型 else if (resolvedValues != null) &#123; Set&lt;ValueHolder&gt; valueHolders = new LinkedHashSet&lt;&gt;(resolvedValues.getArgumentCount()); valueHolders.addAll(resolvedValues.getIndexedArgumentValues().values()); valueHolders.addAll(resolvedValues.getGenericArgumentValues()); for (ValueHolder value : valueHolders) &#123; String argType = (value.getType() != null ? ClassUtils.getShortName(value.getType()) : (value.getValue() != null ? value.getValue().getClass().getSimpleName() : &quot;null&quot;)); argTypes.add(argType); &#125; &#125; //否则就要抛出异常了 String argDesc = StringUtils.collectionToCommaDelimitedString(argTypes); throw new BeanCreationException(mbd.getResourceDescription(), beanName, &quot;No matching factory method found on class [&quot; + factoryClass.getName() + &quot;]: &quot; + (mbd.getFactoryBeanName() != null ? &quot;factory bean &#x27;&quot; + mbd.getFactoryBeanName() + &quot;&#x27;; &quot; : &quot;&quot;) + &quot;factory method &#x27;&quot; + mbd.getFactoryMethodName() + &quot;(&quot; + argDesc + &quot;)&#x27;. &quot; + &quot;Check that a method with the specified name &quot; + (minNrOfArgs &gt; 0 ? &quot;and arguments &quot; : &quot;&quot;) + &quot;exists and that it is &quot; + (isStatic ? &quot;static&quot; : &quot;non-static&quot;) + &quot;.&quot;); &#125; //如果工厂方法没有返回值，那指定不对，抛异常 else if (void.class == factoryMethodToUse.getReturnType()) &#123; throw new BeanCreationException(mbd.getResourceDescription(), beanName, &quot;Invalid factory method &#x27;&quot; + mbd.getFactoryMethodName() + &quot;&#x27; on class [&quot; + factoryClass.getName() + &quot;]: needs to have a non-void return type!&quot;); &#125; //如果模棱两可的方法不为空，抛异常 else if (ambiguousFactoryMethods != null) &#123; throw new BeanCreationException(mbd.getResourceDescription(), beanName, &quot;Ambiguous factory method matches found on class [&quot; + factoryClass.getName() + &quot;] &quot; + &quot;(hint: specify index/type/name arguments for simple parameters to avoid type ambiguities): &quot; + ambiguousFactoryMethods); &#125; /*条件成立：说明自动匹配成功了，需要进行缓存，方便后来者继续使用mergerdbeanDefinition来创建实例*/ if (explicitArgs == null &amp;&amp; argsHolderToUse != null) &#123; mbd.factoryMethodToIntrospect = factoryMethodToUse; argsHolderToUse.storeCache(mbd, factoryMethodToUse); &#125; &#125; /*根据上面选择的构造器和解析出来的参数，通过instantiate方法反射创建bean对象实例，最终将实例设置到beanWrapper的beanInstance实例里面。*/ bw.setBeanInstance(instantiate(beanName, mbd, factoryBean, factoryMethodToUse, argsToUse)); return bw;&#125; 6.applyMergedBeanDefinitionPostProcessors()\u0000遍历所有的后置处理器进行方法调用，典型应用：如果开启了自动依赖注入，那么就会将相关的bean加入到集合中。​ 123456789protected void applyMergedBeanDefinitionPostProcessors(RootBeanDefinition mbd, Class&lt;?&gt; beanType, String beanName) &#123; for (MergedBeanDefinitionPostProcessor processor : getBeanPostProcessorCache().mergedDefinition) &#123; /*这里是后置处理器中的方法执行的逻辑 * 做了一件事情：提取出当前beanType类型整个继承体系内的@Autowired @Value @Inject 信息 并且包装成一个InjectionMetadata的一个对象 * 存放到 AutowiredAnnotationBeanPostProcessor 的缓存中，key是beanName。 * */ processor.postProcessMergedBeanDefinition(mbd, beanType, beanName); &#125;&#125; **MergedBeanDefinitionPostProcessor** 7.registerDisposableBeanIfNecessary()判断当前bean是否需要注册一个需要在容器关闭的时候执行的析构函数​ 12345678910111213141516171819202122232425protected void registerDisposableBeanIfNecessary(String beanName, Object bean, RootBeanDefinition mbd) &#123; /*条件一：原型模式的bean不会注册析构函数 * 条件二：判断当前bean是否需要注册析构函数*/ if (!mbd.isPrototype() &amp;&amp; requiresDestruction(bean, mbd)) &#123; /*如果当前bean对象时单例模式*/ if (mbd.isSingleton()) &#123; // Register a DisposableBean implementation that performs all destruction // work for the given bean: DestructionAwareBeanPostProcessors, // DisposableBean interface, custom destroy method. /*给当前单实例bean注册回调适配器。适配器内 根据当前bean实例是继承接口还是通过自定义来决定具体调用哪个方法，完成析构操作。*/ registerDisposableBean(beanName, new DisposableBeanAdapter( bean, beanName, mbd, getBeanPostProcessorCache().destructionAware)); &#125; else &#123; /*这是自定义作用域的逻辑，压根用不到*/ // A bean with a custom scope... Scope scope = this.scopes.get(mbd.getScope()); if (scope == null) &#123; throw new IllegalStateException(&quot;No Scope registered for scope name &#x27;&quot; + mbd.getScope() + &quot;&#x27;&quot;); &#125; scope.registerDestructionCallback(beanName, new DisposableBeanAdapter( bean, beanName, mbd, getBeanPostProcessorCache().destructionAware)); &#125; &#125;&#125; 首先会进行条件过滤，原型模式的bean排除，不需要注册的bean排除，我们只关心单实例bean的注册逻辑，其他作用域的处理逻辑不需要关注，判断如果是单实例bean对象，给当前的bean对象注册回调适配器。在适配器里面根据当前bean实例是继承接口还是通过自定义来决定具体调用哪个方法完成析构操作。​ **DisposableBeanAdapter**\u0000 123456789101112131415161718192021222324252627282930313233343536373839404142@Overridepublic void run() &#123; destroy();&#125;@Overridepublic void destroy() &#123; if (!CollectionUtils.isEmpty(this.beanPostProcessors)) &#123; for (DestructionAwareBeanPostProcessor processor : this.beanPostProcessors) &#123; processor.postProcessBeforeDestruction(this.bean, this.beanName); &#125; &#125; if (this.invokeDisposableBean) &#123; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Invoking destroy() on bean with name &#x27;&quot; + this.beanName + &quot;&#x27;&quot;); &#125; try &#123; ((DisposableBean) this.bean).destroy(); &#125; catch (Throwable ex) &#123; String msg = &quot;Invocation of destroy method failed on bean with name &#x27;&quot; + this.beanName + &quot;&#x27;&quot;; if (logger.isDebugEnabled()) &#123; logger.warn(msg, ex); &#125; else &#123; logger.warn(msg + &quot;: &quot; + ex); &#125; &#125; &#125; if (this.destroyMethod != null) &#123; invokeCustomDestroyMethod(this.destroyMethod); &#125; else if (this.destroyMethodName != null) &#123; Method methodToInvoke = determineDestroyMethod(this.destroyMethodName); if (methodToInvoke != null) &#123; invokeCustomDestroyMethod(ClassUtils.getInterfaceMethodIfPossible(methodToInvoke)); &#125; &#125;&#125; 总结一下，其实就是创建对象的时候分为三种情况：无参构造器，有参构造器，工厂方法，创建完对象之后，在执行对应的后置处理器(**MergedBeanDefinitionPostProcessor**),最终在判断创建的bean实例是不是需要注册一个析构函数，在容器关闭的时候回调，如果需要就创建并保存。","categories":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/tags/Spring/"}]},{"title":"Spring[四]getBean方法内幕","slug":"Spring/Spring[四]getBean方法内幕","date":"2022-01-11T05:59:14.265Z","updated":"2022-01-11T06:06:59.884Z","comments":true,"path":"2022/01/11/Spring/Spring[四]getBean方法内幕/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/Spring/Spring[%E5%9B%9B]getBean%E6%96%B9%E6%B3%95%E5%86%85%E5%B9%95/","excerpt":"","text":"回顾一下前面，在**AbstractApplicationContext**中，**refresh()**完成了整个IOC容器的刷新，上回我们分析到了初始化剩下所有的单实例bean，这里面有几个核心的地方，**getBean(),createBean(),populateBean()**,三级缓存与循环依赖，完成这些后，整个IOC容器的大体流程就分析完了。本篇我们来分析Spring的**getBean()**。 1.初始化所有的单实例bean对象**finishBeanFactoryInitialization()** 12345678910111213141516171819202122232425262728protected void finishBeanFactoryInitialization(ConfigurableListableBeanFactory beanFactory) &#123; // 为此上下文初始化转换服务 if (beanFactory.containsBean(CONVERSION_SERVICE_BEAN_NAME) &amp;&amp; beanFactory.isTypeMatch(CONVERSION_SERVICE_BEAN_NAME, ConversionService.class)) &#123; beanFactory.setConversionService( beanFactory.getBean(CONVERSION_SERVICE_BEAN_NAME, ConversionService.class)); &#125; // 如果容器里面没有字符串转换器，初始化一个字符串转换器放到容器中。 if (!beanFactory.hasEmbeddedValueResolver()) &#123; beanFactory.addEmbeddedValueResolver(strVal -&gt; getEnvironment().resolvePlaceholders(strVal)); &#125; // 尽早初始化LoadTimeWeaverAware beans，以便尽早注册它们的转换器。 String[] weaverAwareNames = beanFactory.getBeanNamesForType(LoadTimeWeaverAware.class, false, false); for (String weaverAwareName : weaverAwareNames) &#123; getBean(weaverAwareName); &#125; // 停止使用临时类加载器进行类型匹配 beanFactory.setTempClassLoader(null); // 允许缓存所有bean定义元数据，不期望进一步的更改，冻结bd信息，冻结之后就无法往bf注册bd了 beanFactory.freezeConfiguration(); // 实例化所有剩余的单实例bean beanFactory.preInstantiateSingletons();&#125; 冻结bd的信息实际上就是通过一个状态位来控制的，这里面最核心的一个方法或者说步骤就是实例化所有剩余的单实例bean对象。**beanFactory.preInstantiateSingletons()**​ 2.preInstantiateSingletons()12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364@Overridepublic void preInstantiateSingletons() throws BeansException &#123; //日志打印 if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Pre-instantiating singletons in &quot; + this); &#125; /*拿过来所有的beanDefinition names 信息*/ List&lt;String&gt; beanNames = new ArrayList&lt;&gt;(this.beanDefinitionNames); // 触发所有的非懒加载的单例bean的初始化 for (String beanName : beanNames) &#123; //获取bean的定义信息 RootBeanDefinition bd = getMergedLocalBeanDefinition(beanName); // bean不是抽象的 是单例的 不是懒加载的 if (!bd.isAbstract() &amp;&amp; bd.isSingleton() &amp;&amp; !bd.isLazyInit()) &#123; //如果是工厂bean if (isFactoryBean(beanName)) &#123; //通过getBean方法获取bean 前缀 &amp; 拿到的是工厂bean Object bean = getBean(FACTORY_BEAN_PREFIX + beanName); //如果拿到的bean确定是工厂bean if (bean instanceof FactoryBean) &#123; FactoryBean&lt;?&gt; factory = (FactoryBean&lt;?&gt;) bean; //判断这个工厂bean是否期望被初始化 /*判断逻辑：SmartFactoryBean里面有一个isEagerInit方法，这个方法为true就表示这个工厂bean是需要现在创建的*/ boolean isEagerInit = (factory instanceof SmartFactoryBean &amp;&amp; ((SmartFactoryBean&lt;?&gt;) factory).isEagerInit()); //如果期望被初始化 if (isEagerInit) &#123; //通过getBean走创建bean的逻辑 getBean(beanName); &#125; &#125; &#125; else &#123;//执行到这里，说明就是普通的单实例bean，不是工厂bean，直接通过getBean创建bean //三级缓存解决循环依赖的入口： /** * 重点 */ /** * eg：man 和 women 产生循环依赖 * 1.第一次来到这里是首先去创建man */ getBean(beanName); &#125; &#125; &#125; // 触发所有的单实例bean的初始化后的回调逻辑 for (String beanName : beanNames) &#123; //从一级缓存获取单实例bean Object singletonInstance = getSingleton(beanName); //类型断言，执行回调 /*SmartInitializingSingleton里面有一个方法 afterSingletonsInstantiated 这个方法需要在创建好单实例bean之后调用一下*/ if (singletonInstance instanceof SmartInitializingSingleton) &#123; StartupStep smartInitialize = this.getApplicationStartup().start(&quot;spring.beans.smart-initialize&quot;) .tag(&quot;beanName&quot;, beanName); SmartInitializingSingleton smartSingleton = (SmartInitializingSingleton) singletonInstance; //这里就是触发初始化后的回调逻辑 smartSingleton.afterSingletonsInstantiated(); smartInitialize.end(); &#125; &#125;&#125; 这里拿到所有的**beanDefinition**的名字，然后循环判断：【不是抽象的，单实例的，非懒加载的】，然后根据拿到的bean定义信息，再分为**FactoryBean**和**Bean**两种情况进行处理。 梳理一下这里的重点逻辑：​ 合并**beanDefinition**信息 **getMergedLocalBeanDefinition(beanName)** 获取bean对象 **getBean() ** **SmartInitializingSingleton**回调**afterSingletonsInstantiated() **触发单实例bean初始化后的回调逻辑 \u0000接下来先来分析，如何合并bean的定义信息。​ 3. getMergedLocalBeanDefinition(beanName)1234567891011protected RootBeanDefinition getMergedLocalBeanDefinition(String beanName) throws BeansException &#123; // Quick check on the concurrent map first, with minimal locking. /*从缓存获取*/ RootBeanDefinition mbd = this.mergedBeanDefinitions.get(beanName); /*缓存有，直接返回*/ if (mbd != null &amp;&amp; !mbd.stale) &#123; return mbd; &#125; /*真正的合并 bd 的逻辑*/ return getMergedBeanDefinition(beanName, getBeanDefinition(beanName));&#125; 首先会先尝试从缓存来获取**beanDefinition**的信息，如果缓存有，就直接返回，否则就要触发合并bd的逻辑。​ 12345protected RootBeanDefinition getMergedBeanDefinition(String beanName, BeanDefinition bd) throws BeanDefinitionStoreException &#123; return getMergedBeanDefinition(beanName, bd, null);&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293protected RootBeanDefinition getMergedBeanDefinition( String beanName, BeanDefinition bd, @Nullable BeanDefinition containingBd) throws BeanDefinitionStoreException &#123; synchronized (this.mergedBeanDefinitions) &#123; /*合并后的bd信息 * */ RootBeanDefinition mbd = null; /*表示当前beanname对应的过期的mbd信息*/ RootBeanDefinition previous = null; // Check with full lock now in order to enforce the same merged instance. /*null==null*/ if (containingBd == null) &#123; /*从缓存拿信息*/ mbd = this.mergedBeanDefinitions.get(beanName); &#125; /*条件成立说明mbd==null或者 过期...*/ if (mbd == null || mbd.stale) &#123; /*表示当前beanname对应的过期的mbd信息*/ previous = mbd; /*没有当前beanName 对应的 bd 没有使用继承 那么就不用处理继承 */ if (bd.getParentName() == null) &#123; // Use copy of given root bean definition. /*如果 mbd 是 root bean*/ if (bd instanceof RootBeanDefinition) &#123; /*克隆一份保存*/ mbd = ((RootBeanDefinition) bd).cloneBeanDefinition(); &#125; else &#123; /*否则直接创建一个root bean*/ mbd = new RootBeanDefinition(bd); &#125; &#125;/*说明当前beanName 对应的 beanDefinition 存在继承关系*/ else &#123; /*表示bd 的父信息*/ BeanDefinition pbd; try &#123; /*处理beanName 拿到处理了别名和&amp;的真是父bd beanName 名称*/ String parentBeanName = transformedBeanName(bd.getParentName()); /*条件成立 说明 子 bd 和父 bd 名称不一样，就是普通情况*/ if (!beanName.equals(parentBeanName)) &#123; /*递归当前方法，最终返回父 bd 信息*/ pbd = getMergedBeanDefinition(parentBeanName); &#125; else &#123;/*特殊：父子bd名称一样*/ BeanFactory parent = getParentBeanFactory(); if (parent instanceof ConfigurableBeanFactory) &#123; pbd = ((ConfigurableBeanFactory) parent).getMergedBeanDefinition(parentBeanName); &#125; else &#123; throw new NoSuchBeanDefinitionException(parentBeanName, &quot;Parent name &#x27;&quot; + parentBeanName + &quot;&#x27; is equal to bean name &#x27;&quot; + beanName + &quot;&#x27;: cannot be resolved without a ConfigurableBeanFactory parent&quot;); &#125; &#125; &#125; catch (NoSuchBeanDefinitionException ex) &#123; throw new BeanDefinitionStoreException(bd.getResourceDescription(), beanName, &quot;Could not resolve parent bean definition &#x27;&quot; + bd.getParentName() + &quot;&#x27;&quot;, ex); &#125; // 按照父bd信息创建 mbd 对象 mbd = new RootBeanDefinition(pbd); // 用子bd 覆盖mbd信息，以子bd 为基准 父 bd为辅 mbd.overrideFrom(bd); &#125; // Set default singleton scope, if not configured before. if (!StringUtils.hasLength(mbd.getScope())) &#123; mbd.setScope(SCOPE_SINGLETON); &#125; // A bean contained in a non-singleton bean cannot be a singleton itself. // Let&#x27;s correct this on the fly here, since this might be the result of // parent-child merging for the outer bean, in which case the original inner bean // definition will not have inherited the merged outer bean&#x27;s singleton status. if (containingBd != null &amp;&amp; !containingBd.isSingleton() &amp;&amp; mbd.isSingleton()) &#123; mbd.setScope(containingBd.getScope()); &#125; // Cache the merged bean definition for the time being // (it might still get re-merged later on in order to pick up metadata changes) if (containingBd == null &amp;&amp; isCacheBeanMetadata()) &#123; /*缓存合并后的 mbd信息*/ this.mergedBeanDefinitions.put(beanName, mbd); &#125; &#125; if (previous != null) &#123; copyRelevantMergedBeanDefinitionCaches(previous, mbd); &#125; return mbd; &#125;&#125; 这里面需要关注的点有两个：​ 如何处理bean的名字 **transformedBeanName()** 如何合并bd信息 **getMergedBeanDefinition()** ​ 4.处理bean的名字12345678910protected String transformedBeanName(String name) &#123; /* * 返回 BeanFactoryUtils.transformedBeanName(name) 处理完的bean name ，这里也可能是别名。 * spring的bean别名是通过aliasMap保存的。 * &#123;C:B,B:A&#125; a有一个别名叫做 b b有一个别名叫做 c * * 通过这个方法canonicalName 去处理 bean的名字 和别名之间的关系 ，最终返回的是 bean 的真实名字。 * */ return canonicalName(BeanFactoryUtils.transformedBeanName(name));&#125; 4.1 canonicalName()处理bean的名字与别名。​ 123456789101112131415161718public String canonicalName(String name) &#123; /*假设传入c*/ String canonicalName = name; // Handle aliasing... String resolvedName; do &#123; /*根据c会从map拿到b*/ resolvedName = this.aliasMap.get(canonicalName); /*如果获取到的结果不是null*/ if (resolvedName != null) &#123; /*赋值再次循环拿*/ canonicalName = resolvedName; &#125; &#125; /*当resolvedName是空的时候，那么此时的canonicalName一定是最终的名字，返回即可。*/ while (resolvedName != null); return canonicalName;&#125; 4.2 BeanFactoryUtils.transformedBeanName(name)1234567891011121314151617181920212223242526public static String transformedBeanName(String name) &#123; /*断言*/ Assert.notNull(name, &quot;&#x27;name&#x27; must not be null&quot;); /*如果当前bean对象不是 &amp; 开头 （说明是正常bean对象实例） 直接返回*/ if (!name.startsWith(BeanFactory.FACTORY_BEAN_PREFIX)) &#123; return name; &#125; /* * 这里是拿工厂bean的逻辑： * transformedBeanNameCache：缓存处理完&amp;开头的beanName，提升性能 * map.computeIfAbsent(k,v) 说明： * 当map中对应的k ==null || v ==null 这次写操作就会成功，并且返回 k v， * 否则就会失败，并且返回原有的k v。 */ return transformedBeanNameCache.computeIfAbsent(name, beanName -&gt; &#123; /** * 假设bean name = &amp;abc * 判断如果name不是以 &amp;开头 跳出循环 最终会返回 abc。 */ do &#123; beanName = beanName.substring(BeanFactory.FACTORY_BEAN_PREFIX.length()); &#125; while (beanName.startsWith(BeanFactory.FACTORY_BEAN_PREFIX)); return beanName; &#125;);&#125; 5.合并bd信息**getMergedBeanDefinition()**​ 1234567891011protected RootBeanDefinition getMergedLocalBeanDefinition(String beanName) throws BeansException &#123; // Quick check on the concurrent map first, with minimal locking. /*从缓存获取*/ RootBeanDefinition mbd = this.mergedBeanDefinitions.get(beanName); /*缓存有，直接返回*/ if (mbd != null &amp;&amp; !mbd.stale) &#123; return mbd; &#125; /*真正的合并 bd 的逻辑*/ return getMergedBeanDefinition(beanName, getBeanDefinition(beanName));&#125; 12345protected RootBeanDefinition getMergedBeanDefinition(String beanName, BeanDefinition bd) throws BeanDefinitionStoreException &#123; return getMergedBeanDefinition(beanName, bd, null);&#125; 再往下其实又回到了上面的**getMergedBeanDefinition()**，递归去处理bd信息。 合并完bd信息之后就是判断是否是工厂bean的逻辑，想要获取工厂bean，在前面提到过，需要在bean的名字前面加一个 &amp; 。​ 默认情况下，我们要看单实例bean的获取过程，此时我们去看 **getBean()**。​ 6.获取单实例bean对象**getBean()**​ 12345@Overridepublic Object getBean(String name) throws BeansException &#123; /*真正加载bean的方法*/ return doGetBean(name, null, null, false);&#125; **doGetBean()** 返回一个指定的bean实例，这个bean可以是共享的，也可以是单例的。​ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246protected &lt;T&gt; T doGetBean( String name, @Nullable Class&lt;T&gt; requiredType, @Nullable Object[] args, boolean typeCheckOnly) throws BeansException &#123; /* * 处理转换bean名字，可能是一个别名，也可能是一个带着&amp; 开头的beanName * 别名：重定向出来真的beanName * &amp;开头：说明要获取的bean实例对象实际上是一个工厂bean * FactoryBean ：如果某个bean的配置特别复杂，使用spring管理不容易...不够灵活，想要使用编码的形式去构建它， * 那么你就可以提供一个构建该bean实例的工厂，这个工厂就是factoryBean接口 * factoryBean接口的实现类还是需要spring来管理的。 * 这里就涉及到两种对象：一种是beanFactory实现类 另一个是FactoryBean内部管理的对象。 * * 如果要获取工厂bean，需要获取 &amp; * 如果要拿factoryBean内部管理的对象，直接传name 不需要带着 &amp; */ String beanName = transformedBeanName(name); //保留返回值的 Object beanInstance; /*到缓存中获取共享单实例 第一次去缓存拿*/ /** * 1.第一次经过这里是创建man的时候，首先去缓存获取，但是这个时候，man对象时第一次创建，所以什么都拿不到 * 2.此时man在第三级缓存，然后去拿women，这个时候women还没创建，所以锤子也拿不到 * 3.第三次经过这里的时候，实际上就是women创建完了，放到三级缓存了，然后发现属性赋值的时候需要man，又通过getBean来拿， * 这个时候，在第三级缓存拿到了早期对象的引用，然后并对man进行一个缓存升级 ，从 三级缓存升级到了二级 */ Object sharedInstance = getSingleton(beanName); /*此时的逻辑应该是从二级缓存拿到了早期暴露的bean实例，并且属性没有填充*/ /*这里如果是第一次创建bean实例，上面从一级缓存实际上是啥也没拿到，所以走到这里，实例是null。*/ if (sharedInstance != null &amp;&amp; args == null) &#123; if (logger.isTraceEnabled()) &#123; if (isSingletonCurrentlyInCreation(beanName)) &#123; logger.trace(&quot;Returning eagerly cached instance of singleton bean &#x27;&quot; + beanName + &quot;&#x27; that is not fully initialized yet - a consequence of a circular reference&quot;); &#125; else &#123; logger.trace(&quot;Returning cached instance of singleton bean &#x27;&quot; + beanName + &quot;&#x27;&quot;); &#125; &#125; /* * 这里为什么又要包装一层呢？ * 从ioc中拿到的对象可能是普通的单实例，也可能是FactoryBean实例 * 如果是FactoryBean实例，还要考虑进行处理 主要看name带不带 &amp; * 带&amp;说明这次getBean想要拿工厂bean，否则想要拿 FactoryBean 内部管理的 bean 实例 */ /** * 1.走到这里是什么时候？创建man，然后对man进行属性赋值，发现需要women，然后递归去创建women，然后对women进行属性赋值， * 然后发现需要man，然后从缓存拿，恰好此时从三级缓存拿到了，早期man对象的引用，这个时候，这里啥也没干，直接return */ beanInstance = getObjectForBeanInstance(sharedInstance, name, beanName, null); &#125; /* * 执行到这里的情况：sharedInstance == null || args != null * 1. 二级缓存未拿到bean实例 * 2. 二级缓存拿到了bean实例，但是属性已经填充完了 ！！！这是不可能的，这样的话就在一级缓存了。 * * 缓存没有想要的数据 * 1.原型循环依赖问题的判定 */ else &#123; /* * 原型循环依赖的判定 * eg： a(prototype) - b b - a (prototype) * 1.会像正在创建中的原型集合添加 a * 2.创建 a 早期对象 二级缓存 * 3.处理 a的依赖 发现 a 依赖 b 类型 * 4.触发 spring getBeab(b.class) 的操作 * 5.根据 b 的构造方法 反射创建 b 的早期实例 * 6.spring 处理 b 对象的依赖发现依赖了 a * 7.所以spring砖头回来再次获取 a getBean(a.class) * 8.程序再次来到这里会判断当前要获取的a对象是不是正在创建中 如果是循环依赖 会返回true ，最终抛出异常 结束了循环依赖注入 */ if (isPrototypeCurrentlyInCreation(beanName)) &#123; throw new BeanCurrentlyInCreationException(beanName); &#125; /*父子容器相关的处理逻辑*/ BeanFactory parentBeanFactory = getParentBeanFactory(); if (parentBeanFactory != null &amp;&amp; !containsBeanDefinition(beanName)) &#123; // Not found -&gt; check parent. String nameToLookup = originalBeanName(name); if (parentBeanFactory instanceof AbstractBeanFactory) &#123; return ((AbstractBeanFactory) parentBeanFactory).doGetBean( nameToLookup, requiredType, args, typeCheckOnly); &#125; else if (args != null) &#123; // Delegation to parent with explicit args. return (T) parentBeanFactory.getBean(nameToLookup, args); &#125; else if (requiredType != null) &#123; // No args -&gt; delegate to standard getBean method. return parentBeanFactory.getBean(nameToLookup, requiredType); &#125; else &#123; return (T) parentBeanFactory.getBean(nameToLookup); &#125; &#125; /*穿的是false就成立*/ if (!typeCheckOnly) &#123; /* * 将指定的 bean 标记为已创建（或即将创建）。 * 允许 bean 工厂优化其缓存以重复创建指定 bean。 */ /** * 1.第一次创建man的时候，经过这里，标记man正在创建中 * 2.第二次经过这里的时候，就是对man进行属性赋值的过程中，发现依赖women，所以去走getBean逻辑。这个时候去标记women也是正在被创建中 */ markBeanAsCreated(beanName); &#125; StartupStep beanCreation = this.applicationStartup.start(&quot;spring.beans.instantiate&quot;) .tag(&quot;beanName&quot;, name); try &#123; if (requiredType != null) &#123; beanCreation.tag(&quot;beanType&quot;, requiredType::toString); &#125; /*获取合并beanDefinition信息 * 为什么需要合并？ * bd 支持继承 子 会 继承 父亲 的所有 信息 * */ RootBeanDefinition mbd = getMergedLocalBeanDefinition(beanName); /*判断当前mbd是不是抽象的，如果是抽象的，需要抛出异常，因为抽象的bd不能创建实例，只能作为模板让子bd继承*/ checkMergedBeanDefinition(mbd, beanName, args); // Guarantee initialization of beans that the current bean depends on. /* * depends-on 属性处理 * &lt;bean name=&quot;A&quot; depends-on=&quot;B&quot; /&gt; * &lt;bean name=&quot;B&quot; /&gt; * * 循环依赖问题 b - a a - b * spring是处理不了这种情况的，需要报错 * spring需要发现这种情况的产生： * 如何发现？依靠两个map * 1.dependentBeanMap 记录依赖当前beanName的其他beanName * 2.dependenciesForBeanMap 记录当前bean依赖的其他bean集合 * */ String[] dependsOn = mbd.getDependsOn(); if (dependsOn != null) &#123; for (String dep : dependsOn) &#123; /*判断循环依赖*/ if (isDependent(beanName, dep)) &#123; throw new BeanCreationException(mbd.getResourceDescription(), beanName, &quot;Circular depends-on relationship between &#x27;&quot; + beanName + &quot;&#x27; and &#x27;&quot; + dep + &quot;&#x27;&quot;); &#125; /* * 注册依赖关系 * 1.记录a依赖了谁 * 2.记录谁依赖了b * */ registerDependentBean(dep, beanName); try &#123; getBean(dep); &#125; catch (NoSuchBeanDefinitionException ex) &#123; throw new BeanCreationException(mbd.getResourceDescription(), beanName, &quot;&#x27;&quot; + beanName + &quot;&#x27; depends on missing bean &#x27;&quot; + dep + &quot;&#x27;&quot;, ex); &#125; &#125; &#125; // 创建单实例 if (mbd.isSingleton()) &#123; /*第二个 getSingthon() 创建实例并返回*/ /** * 1.第一次经过这里是去创建man对象 * 2.创建women对象 */ sharedInstance = getSingleton(beanName, () -&gt; &#123; try &#123; return createBean(beanName, mbd, args); &#125; catch (BeansException ex) &#123; // Explicitly remove instance from singleton cache: It might have been put there // eagerly by the creation process, to allow for circular reference resolution. // Also remove any beans that received a temporary reference to the bean. destroySingleton(beanName); throw ex; &#125; &#125;); /* * 这里为什么不直接返回 ，还调用 这个方法？ * 创建出来的单实例bean也可能是工厂bean对象，所以需要根据名字判断到底返回 * bean对象还是工厂bean * */ beanInstance = getObjectForBeanInstance(sharedInstance, name, beanName, mbd); &#125; /*多例bean的创建*/ else if (mbd.isPrototype()) &#123; // It&#x27;s a prototype -&gt; create a new instance. Object prototypeInstance = null; try &#123; /*记录当前线程相关的正在创建的原型对象beanName*/ beforePrototypeCreation(beanName); /*创建对象*/ prototypeInstance = createBean(beanName, mbd, args); &#125; finally &#123; /*从正在创建中的集合中移除*/ afterPrototypeCreation(beanName); &#125; beanInstance = getObjectForBeanInstance(prototypeInstance, name, beanName, mbd); &#125; /*other作用域： 略 ....*/ else &#123; String scopeName = mbd.getScope(); if (!StringUtils.hasLength(scopeName)) &#123; throw new IllegalStateException(&quot;No scope name defined for bean ´&quot; + beanName + &quot;&#x27;&quot;); &#125; Scope scope = this.scopes.get(scopeName); if (scope == null) &#123; throw new IllegalStateException(&quot;No Scope registered for scope name &#x27;&quot; + scopeName + &quot;&#x27;&quot;); &#125; try &#123; Object scopedInstance = scope.get(beanName, () -&gt; &#123; beforePrototypeCreation(beanName); try &#123; return createBean(beanName, mbd, args); &#125; finally &#123; afterPrototypeCreation(beanName); &#125; &#125;); beanInstance = getObjectForBeanInstance(scopedInstance, name, beanName, mbd); &#125; catch (IllegalStateException ex) &#123; throw new ScopeNotActiveException(beanName, scopeName, ex); &#125; &#125; &#125; catch (BeansException ex) &#123; beanCreation.tag(&quot;exception&quot;, ex.getClass().toString()); beanCreation.tag(&quot;message&quot;, String.valueOf(ex.getMessage())); cleanupAfterBeanCreationFailure(beanName); throw ex; &#125; finally &#123; beanCreation.end(); &#125; &#125; /** * 1.走到这里的逻辑：对man进行属性赋值，递归创建women，对women进行属性赋值，然后从缓存拿到了man */ return adaptBeanInstance(name, beanInstance, requiredType);&#125; 这个方法的逻辑有点多，进行一个简单的梳理：​ 首先上来先处理bean的名字 **transformedBeanName(name)** 然后尝试通过 **getSingleton(beanName) **去一级缓存拿数据，但是这个时候这个bean对象还没有创建过，此时是拿不到数据的。 如果说拿到了bean对象，那么就再通过 **getObjectForBeanInstance() **对bean对象进行一层包装。 这里为什么又要包装一层呢？从IOC中拿到的对象可能是普通单实例bean对象，也可能是一个工厂bean对象。如果是工厂bean对象，还需要考虑是不是需要处理，主要是看名字里面带不带 &amp; 。如果带 &amp; ，就是想要拿工厂本身，如果不带 &amp; ，其实就是想拿工厂bean生产的对象。 正常第一次去获取bean对象的时候，是在缓存拿不到数据的，所以会走else的逻辑。 判断是不是原型模式的循环依赖，如果是的话，抛出异常。 然后就是护理父子容器相关的逻辑 如果是第一次创建bean，就会通过一个状态位来标记bean正在创建中 获取合并后的bd信息 判断当前bean对象是不是抽象的，如果是抽象的，需要跑出异常，因为抽象的bean是模板bean，不能创建实例 判断是不是发生互相依赖，注意是互相依赖，不是循环依赖，如果是发生了互相依赖，则抛出异常。 互相依赖：bean标签或者@Bean 注解里面配置了 **depends-on=&quot;B&quot; a-&gt;b , b-&gt;a**​ 循环依赖：a里面有个属性叫做b，b里面有个属性a\u0000 注册依赖关系，记录当前bean依赖了谁，谁依赖了当前bean 判断如果是单实例bean，这次就会通过**getSingleton()-&gt;createBean()**去创建bean对象 然后再通过**getObjectForBeanInstance() **对bean对象进行一层包装 如果是原型实例的bean对象，就会走多实例bean创建的流程，其实还是通过**createBean()**去创建对象，只是不会被一级缓存所缓存。 对于其他作用域的bean对象，则走他们的创建逻辑，再次不作为重点内容分析。 最终的逻辑**adaptBeanInstance()**其实就是对循环依赖的处理，对 A对象属性赋值的时候，发现需要B对象。然后从一级缓存拿发现没有，就走这里递归去创建B对象，从缓存拿到A对象，对B对象进行属性赋值。 7.getSingleton()看一下这个方法，从缓存来获取bean。这个方法在 **DefaultSingletonRegistry** 中。​ 123456@Override@Nullablepublic Object getSingleton(String beanName) &#123; /*方法重载 allowEarlyReference 是否允许拿到早期引用*/ return getSingleton(beanName, true);&#125; \u0000这里发生了方法重载。这个方法挺叼的，根据名称来返回单实例bean对象，能够解决循环依赖。​ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970@Nullableprotected Object getSingleton(String beanName, boolean allowEarlyReference) &#123; /*通过名字去一级缓存拿单实例bean对象*/ Object singletonObject = this.singletonObjects.get(beanName); /*如果拿到的对象实例是null，有几种情况？ * 1.单实例确实没创建呢 * 2. 当前正在创建中，发生了循环依赖了，这个时候实例其实在二级缓存 * * 循环依赖： * A-&gt;B B-&gt;A * 单实例的循环依赖有几种？ * 1.构造方法 无解 * 2.setter 有解 通过三级缓存 * * 三级缓存实际上如何解决的循环依赖？ * 利用bean的中间状态 ：已经实例化但是还未初始化 * A-B B-&gt;A setter依赖 * 1. 假设spring先实例化A，首先拿到A的构造方法，反射创建A的早期实例对象，这个早期对象被包装了一下， * 变成ObjectFactory对象，放到三级缓存。 * 2. 处理A的依赖数据，检查发现 A依赖B ，所以，spring 根据 B的类型去容器中去getBean(B.class) ,这里就是递归了 * 3. 首先拿到B的构造方法，反射创建B的早期实例对象，把B包装成ObjectFactory对象，放到三级缓存。 * 4. 处理Bde 依赖数据，检查发现，B依赖对象A，所以接下来，spring就会根据A类型去容器去getBean(A.class) 对象，这个时候又递归了 * 5. 程序还会走到当前方法getSingleton * 6. 条件一成立，条件二成立。 * */ if (singletonObject == null &amp;&amp; isSingletonCurrentlyInCreation(beanName)) &#123; /*从二级缓存拿数据*/ singletonObject = this.earlySingletonObjects.get(beanName); /*条件成立说明二级缓存没有 去三级缓存拿*/ if (singletonObject == null &amp;&amp; allowEarlyReference) &#123; synchronized (this.singletonObjects) &#123; /* * spring为什么需要有三级缓存，而不是只有二级缓存？ * AOP 靠什么实现呢？动态代理 jdk cglib * 代理：静态代理：需要手动写代码实现新的JAVA文件，这个JAV类需要和代理对象实现同一个接口，内部维护一个被代理对象 * 代理类在接口调用原生对象前后可以加一些逻辑。 * 代理对象和被代理对象是两个不同的内存地址，一定是不一样的 * 动态代理：... 不需要人为写代码了，而是依靠字节码框架动态生成字节码文件，然后jvm在进行加载，然后也是一样 * 也是去new代理对象，这个代理对象没啥特殊的，也是内部保留了原生对象，然后再调用原生对象前后实现的字节码增强。 * 两者共同点：代理对象和被代理对象实际上都不是同一内存地址 * * 三级缓存在这里有什么意义呢？ * 三级缓存里面保存的是对象工厂，这个对象工厂内部保留着原生对象的引用，ObjectFactory的实现类，getObject方法， * 需要考虑一个问题：到底是返回原生的，还是增强的？ * getObject会判断当前早期实例是否需要被增强，如果是 那么提前完成动态代理增强，返回代理对象，否则，返回原生对象。 * */ /*从一级缓存拿*/ singletonObject = this.singletonObjects.get(beanName); if (singletonObject == null) &#123; /*从二级缓存拿*/ singletonObject = this.earlySingletonObjects.get(beanName); if (singletonObject == null) &#123; /*从三级缓存拿*/ ObjectFactory&lt;?&gt; singletonFactory = this.singletonFactories.get(beanName); /*条件成立：说明第三级缓存有数据。这里就涉及到了缓存的升级 ，很简单 ，从三级挪到二级 ，再反手干掉三级的。*/ if (singletonFactory != null) &#123; singletonObject = singletonFactory.getObject(); this.earlySingletonObjects.put(beanName, singletonObject); this.singletonFactories.remove(beanName); &#125; &#125; &#125; &#125; &#125; &#125; return singletonObject;&#125; 梳理一下这个方法的大体逻辑：(我们现在走的是单实例bean创建的流程，所以假设我们正在创建一个单实例bean，执行到了这里，三级缓存与循环依赖的问题会在后续文章分析。)​ 先根据名字去一级缓存拿bean，此时是拿不到的。 判断当前bean，是不是单实例bean对象，是不是正在创建中？ 走这里的逻辑就是发生了循环依赖： 假设spring先实例化A，首先拿到A的构造方法，反射创建A的早期实例对象，这个早期对象被包装了一下，变成ObjectFactory对象，放到三级缓存。\u0000\u00002. 处理A的依赖数据，检查发现 A依赖B ，所以，spring 根据 B的类型去容器中去getBean(B.class) ,这里就是递归了\u0000 首先拿到B的构造方法，反射创建B的早期实例对象，把B包装成ObjectFactory对象，放到三级缓存。\u0000 处理Bde 依赖数据，检查发现，B依赖对象A，所以接下来，spring就会根据A类型去容器去getBean(A.class) 对象，这个时候又递归了\u0000 程序还会走到当前方法getSingleton\u0000 首先去二级缓存拿数据，这个时候二级缓存是拿不到数据的，所以会继续往下走 再次尝试从一级缓存和二级缓存拿，这个时候其实还是拿不到的，所以从三级缓存来拿 如果三级缓存拿到了数据，那就进行缓存的升级 把三级缓存的对象拿到二级缓存，三级缓存的对象干掉。 ​ 8.思考与沉淀spring为什么需要有三级缓存，而不是只有二级缓存？AOP 靠什么实现呢？动态代理 jdk cglib代理：静态代理：需要手动写代码实现新的JAVA文件，这个JAV类需要和代理对象实现同一个接口，内部维护一个被代理对象 代理类在接口调用原生对象前后可以加一些逻辑。 代理对象和被代理对象是两个不同的内存地址，一定是不一样的 动态代理：… 不需要人为写代码了，而是依靠字节码框架动态生成字节码文件，然后jvm在进行加载，然后也是一样 也是去new代理对象，这个代理对象没啥特殊的，也是内部保留了原生对象，然后再调用原生对象前后实现的字节码增强。两者共同点：代理对象和被代理对象实际上都不是同一内存地址​ 三级缓存在这里有什么意义呢？三级缓存里面保存的是对象工厂，这个对象工厂内部保留着原生对象的引用，ObjectFactory的实现类，getObject方法，需要考虑一个问题：到底是返回原生的，还是增强的？getObject会判断当前早期实例是否需要被增强，如果是 那么提前完成动态代理增强，返回代理对象，否则，返回原生对象。​ 9.getObjectForBeanInstance()这个方法是获取给定的bean实例对象，如果是工厂bean，则根据名字返回工厂本身或者其创建的对象。​ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162/** * 获取给定 bean 实例的对象，如果是 FactoryBean，则是 bean 实例本身或其创建的对象。 * @param beanInstance 共享单实例对象 * @param name 未处理 &amp; 的 name * @param beanName 处理过&amp; 和别名后的name * @param mbd the merged bean definition 合并过后的beanDefinition信息 * @return the object to expose for the bean */protected Object getObjectForBeanInstance( Object beanInstance, String name, String beanName, @Nullable RootBeanDefinition mbd) &#123; /*判断当前的name是不是&amp;开始的，条件成立，说明当前要获取的是工厂bean对象*/ if (BeanFactoryUtils.isFactoryDereference(name)) &#123; if (beanInstance instanceof NullBean) &#123; return beanInstance; &#125; /*条件成立：说明但实例对象不是工厂bean接口的实现类 直接报错*/ if (!(beanInstance instanceof FactoryBean)) &#123; throw new BeanIsNotAFactoryException(beanName, beanInstance.getClass()); &#125; /*打标 给当前bean对应的mbd打标，记录他表达的实例是一个工厂bean*/ if (mbd != null) &#123; mbd.isFactoryBean = true; &#125; return beanInstance; &#125; /* * 执行到这里有几种情况？ * 1.当前bean实例是普通单实例 * 2.当前bean实例是工厂bean接口实现类，但是当前要获取的是工厂内部管理的bean实例 */ if (!(beanInstance instanceof FactoryBean)) &#123; return beanInstance; &#125; /*保存工厂bean实例的getobject值得*/ Object object = null; if (mbd != null) &#123; mbd.isFactoryBean = true; &#125; else &#123; /*尝试到缓存获取工厂bean*/ object = getCachedObjectForFactoryBean(beanName); &#125; /*此时说明缓存没有，需要到工厂bean getObject 获取*/ if (object == null) &#123; // Return bean instance from factory. FactoryBean&lt;?&gt; factory = (FactoryBean&lt;?&gt;) beanInstance; // 条件一几乎恒成立 条件二：判断spring中当前是否有当前beanName的bd信息 if (mbd == null &amp;&amp; containsBeanDefinition(beanName)) &#123; /*拿到合并后的bd信息 * 为什么是合并后的呢？ * 因为bd支持继承的，合并后的bd信息是包含继承回来的bd * */ mbd = getMergedLocalBeanDefinition(beanName); &#125; /*synthetic默认值是false ，表示这是一个用户对象 如果是 true 表示是系统对象*/ boolean synthetic = (mbd != null &amp;&amp; mbd.isSynthetic()); /*到这里说明 说明真正的去执行 getBean() 的逻辑 time 【51：32】*/ object = getObjectFromFactoryBean(factory, beanName, !synthetic); &#125; return object;&#125; ​ 判断当前的name是不是 &amp; 开始的，条件成立，说明当前要获取的是工厂bean对象 如果当前bean独享没有实现工厂bean接口，直接报错 打标：给当前bean对应的mbd打标，记录他表达的实例是一个工厂bean对象 返回bean实例 判断如果当前bean没有实现工厂bean接口，（思考一下，走到这里的逻辑，其实要么是单实例bean，要么是获取工厂bean创建的对象），既然没有实现工厂bean接口，所以这里的逻辑就是处理单实例bean的。 直接返回就行了 走到这里的逻辑就是一种情况：获取工厂bean创建的bean对象。 在mbd打标，表示是一个工厂bean对象 如果缓存里面没有当前bean工厂生产的对象，需要通过工厂bean的**getBean()**去获取。 判断有没有bd信息，如果没有就合并bd信息得到mbd信息 然后再通过**getObjectFromFactoryBean() **根据mbd信息去获取bean对象 \u0000 9.1 getObjectFromFactoryBean()这个方法的逻辑上存在问题，但是不影响分析。​ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556protected Object getObjectFromFactoryBean(FactoryBean&lt;?&gt; factory, String beanName, boolean shouldPostProcess) &#123; /*如果是已经存在的单实例bean对象*/ if (factory.isSingleton() &amp;&amp; containsSingleton(beanName)) &#123; /*加锁 内部逻辑是串行化的，不存在并发*/ synchronized (getSingletonMutex()) &#123; /*先从缓存获取*/ Object object = this.factoryBeanObjectCache.get(beanName); if (object == null) &#123; /*这里已经是空了*/ object = doGetObjectFromFactoryBean(factory, beanName); // Only post-process and store if not put there already during getObject() call above // (e.g. because of circular reference processing triggered by custom getBean calls) Object alreadyThere = this.factoryBeanObjectCache.get(beanName); /*这里一定是空，因为是串行化的方法，所以逻辑有问题*/ if (alreadyThere != null) &#123; object = alreadyThere; &#125; else &#123; if (shouldPostProcess) &#123; /*判断当前实例是否被创建 逻辑有问题 明明上面就是没创建 */ if (isSingletonCurrentlyInCreation(beanName)) &#123; // Temporarily return non-post-processed object, not storing it yet.. return object; &#125; beforeSingletonCreation(beanName); try &#123; /*执行后置处理器的逻辑*/ object = postProcessObjectFromFactoryBean(object, beanName); &#125; catch (Throwable ex) &#123; throw new BeanCreationException(beanName, &quot;Post-processing of FactoryBean&#x27;s singleton object failed&quot;, ex); &#125; finally &#123; afterSingletonCreation(beanName); &#125; &#125; if (containsSingleton(beanName)) &#123; this.factoryBeanObjectCache.put(beanName, object); &#125; &#125; &#125; return object; &#125; /*工厂bean对象内部维护的对象不是单实例，每次都是一个新对象*/ &#125; else &#123; /*直接拿*/ Object object = doGetObjectFromFactoryBean(factory, beanName); if (shouldPostProcess) &#123; try &#123; /*后置处理器的逻辑*/ object = postProcessObjectFromFactoryBean(object, beanName); &#125; catch (Throwable ex) &#123; throw new BeanCreationException(beanName, &quot;Post-processing of FactoryBean&#x27;s object failed&quot;, ex); &#125; &#125; return object; &#125;&#125; if里面的判断是不会成立的，所以直接看else的逻辑。​ 通过委派模式委派给**doGetObjectFromFactoryBean()**去拿bean对象，然后再执行后置处理器的逻辑。​ 继续往下分析：​ 9.2 doGetObjectFromFactoryBean()1234567891011121314151617181920212223private Object doGetObjectFromFactoryBean(FactoryBean&lt;?&gt; factory, String beanName) throws BeanCreationException &#123; Object object; try &#123; object = factory.getObject(); &#125; catch (FactoryBeanNotInitializedException ex) &#123; throw new BeanCurrentlyInCreationException(beanName, ex.toString()); &#125; catch (Throwable ex) &#123; throw new BeanCreationException(beanName, &quot;FactoryBean threw exception on object creation&quot;, ex); &#125; // Do not accept a null value for a FactoryBean that&#x27;s not fully // initialized yet: Many FactoryBeans just return null then. if (object == null) &#123; if (isSingletonCurrentlyInCreation(beanName)) &#123; throw new BeanCurrentlyInCreationException( beanName, &quot;FactoryBean which is currently in creation returned null from getObject&quot;); &#125; object = new NullBean(); &#125; return object;&#125; 这里面的核心代码其实就是只有一句：调用工厂bean的**factory.getObject()** 获取bean对象。​ 剩余的**createBean()** &amp; **adaptBeanInstance()**，**createBean()**创建bean的逻辑会在下一篇中深入分析，**adaptBeanInstance()**循环依赖相关的逻辑会在后续的三级缓存与循环依赖篇中分析，与本次的创建单实例bean对象关系并不大。至此，整个**getBean()**就分析完了。​ 10.触发所有单实例bean初始化后的回调逻辑在**preInstantiateSingletons()**里面通过 getBean() 方法实例化完所有的单实例bean以后，就会触发所有 **SmartInitializingSingleton** 的 **afterSingletonsInstantiated()**。\u0000\u0000这个组件具体的作用，在前面的Spring组件与注解篇里面已经详细介绍过，在此不再赘述。​ 123456789101112131415// 触发所有的单实例bean的初始化后的回调逻辑for (String beanName : beanNames) &#123; //从一级缓存获取单实例bean Object singletonInstance = getSingleton(beanName); //类型断言，执行回调 /*SmartInitializingSingleton里面有一个方法 afterSingletonsInstantiated 这个方法需要在创建好单实例bean之后调用一下*/ if (singletonInstance instanceof SmartInitializingSingleton) &#123; StartupStep smartInitialize = this.getApplicationStartup().start(&quot;spring.beans.smart-initialize&quot;) .tag(&quot;beanName&quot;, beanName); SmartInitializingSingleton smartSingleton = (SmartInitializingSingleton) singletonInstance; //这里就是触发初始化后的回调逻辑 smartSingleton.afterSingletonsInstantiated(); smartInitialize.end(); &#125;&#125; 下一篇，我们将继续分析bean对象的创建逻辑 **createBean()**。","categories":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/tags/Spring/"}]},{"title":"Spring[三]解析配置文件","slug":"Spring/Spring[三]解析配置文件","date":"2022-01-11T05:59:03.018Z","updated":"2022-01-11T06:06:28.495Z","comments":true,"path":"2022/01/11/Spring/Spring[三]解析配置文件/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/Spring/Spring[%E4%B8%89]%E8%A7%A3%E6%9E%90%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/","excerpt":"","text":"本来这一篇是要写如何加载所有的单实例bean对象，但是回顾第二篇，发现对于解析xml文件加载bean定义信息的部分理解表达的并不是很好，所以在此补充一篇Spring解析配置文件，加载bean定义信息的文章。 ​ 1.以refresh()作为抓手**refresh()**作为容器的刷新方法，重要性不必多说，在里面的第二个方法**obtainFreshBeanFactory()**里面，解析了xml配置文件或注解，载入bean定义资源信息，返回了一个全新的bean工厂。接下来来分析**obtainFreshBeanFactory()**。​ 2.obtainFreshBeanFactory()解析xml文件或者注解，加载bean 的定义信息，创建一个全新的bean工厂。​ \u0000 123456protected ConfigurableListableBeanFactory obtainFreshBeanFactory() &#123; //刷新bean工厂 refreshBeanFactory(); /**返回bean工厂*/ return getBeanFactory();&#125; \u0000 3.refreshBeanFactory()这里面主要就是判断当前有没有bean工厂，如果有的话，就把工厂关了，重新造一个，如果没有的话，直接造一个。总之一定要造一个新的工厂。**createBeanFactory()** 12345678910111213141516171819202122232425@Overrideprotected final void refreshBeanFactory() throws BeansException &#123; /*如果已经有了bean工厂，通常情况下并不会，什么情况下会有？通过applicationContext直接调用refresh方法*/ if (hasBeanFactory()) &#123; /*销毁里面的所有bean*/ destroyBeans(); /*关闭bean工厂*/ closeBeanFactory(); &#125; try &#123; //不管上面是否已经有bean工厂存在，最终都会走到这里，去创建一个bean工厂 DefaultListableBeanFactory beanFactory = createBeanFactory(); /*设置序列化id*/ beanFactory.setSerializationId(getId()); /*对工厂进行一些定制化设置*/ customizeBeanFactory(beanFactory); /*加载bean的定义信息*/ loadBeanDefinitions(beanFactory); /*将当前类的bean工厂引用指向创建的bean工厂*/ this.beanFactory = beanFactory; &#125; catch (IOException ex) &#123; throw new ApplicationContextException(&quot;I/O error parsing bean definition source for &quot; + getDisplayName(), ex); &#125;&#125; **createBeanFactory()**返回一个全新的bean工厂。​ 123protected DefaultListableBeanFactory createBeanFactory() &#123; return new DefaultListableBeanFactory(getInternalParentBeanFactory());&#125; 每次容器刷新的时候尝试调用此方法。默认创建一个**DefaultListableBeanFactory**，并将此上下文父级的内部bean工厂作为父bean工厂。​ 可以再子类中覆盖，例如自定义**DefaultListableBeanFactory**的设置。​ 创建完工厂了，就要往工厂里面放东西，** loadBeanDefinitions(beanFactory)**。​ 4.loadBeanDefinitions(beanFactory)载入bean的定义信息 **beanDefinition**。​ 12345678910111213141516@Overrideprotected void loadBeanDefinitions(DefaultListableBeanFactory beanFactory) throws BeansException, IOException &#123; // 创建一个xml 的beanDefinition加载器 这个玩意里面持有一个beanFactory的引用 XmlBeanDefinitionReader beanDefinitionReader = new XmlBeanDefinitionReader(beanFactory); /*给beanDefinition 加载器设置上下文环境 资源加载器 实体解析器*/ /*这玩意我记得都是用的默认的*/ beanDefinitionReader.setEnvironment(this.getEnvironment()); beanDefinitionReader.setResourceLoader(this); beanDefinitionReader.setEntityResolver(new ResourceEntityResolver(this)); /*初始化beanDefinition加载器*/ initBeanDefinitionReader(beanDefinitionReader); /*使用beanDefinition加载器加载beanDefinitions*/ loadBeanDefinitions(beanDefinitionReader);&#125; 首先创建了一个bean定义信息的加载器 **XmlBeanDefinitionReader**，用来解析xml文件。​ 然后给**XmlBeanDefinitionReader**设置一些相关信息。​ 然后初始化 **XmlBeanDefinitionReader**。**initBeanDefinitionReader(beanDefinitionReader)**​ 最后使用**XmlBeanDefinitionReader**加载bean的定义信息。**loadBeanDefinitions(beanDefinitionReader)**\u0000 5.bean定义信息加载器**XmlBeanDefinitionReader**​ 123public XmlBeanDefinitionReader(BeanDefinitionRegistry registry) &#123; super(registry);&#125; 构造器里面传入了一个**BeanDefinitionRegistry**，给父类**AbstractBeanDefinitionReader**的属性赋值。​ ​ 6.Bean定义信息注册器**BeanDefinitionRegistry**​ 持有bean定义的注册表接口。例如_**RootBeanDefinition**__ 和 _**ChildBeanDefinition**_ 实例。_​ 通常由在内部使用_**AbstractBeanDefinition**_层次结构的**beanFactory**实现。​ 这是spring的bean工厂中唯一封装**BeanDefinitionRegistry**的接口。标准的**BeanFactory**接口仅仅涵盖对完全配置的工厂的工厂实例的访问。​ spring的**BeanDefinitionRegistry**期待作用于该接口的实现。spring中已经有的实现是_**DefaultListableBeanFactory**__ &amp; __**GenericApplicationContext**_。 1234567891011121314151617181920212223public interface BeanDefinitionRegistry extends AliasRegistry &#123; /*注册一个bean的定义信息，支持指定bean的名字*/ void registerBeanDefinition(String beanName, BeanDefinition beanDefinition) throws BeanDefinitionStoreException; /*根据bean的名字移除掉已经注册的beanDefinition*/ void removeBeanDefinition(String beanName) throws NoSuchBeanDefinitionException; /*根据名字查找BeanDefinition*/ BeanDefinition getBeanDefinition(String beanName) throws NoSuchBeanDefinitionException; /*判断是否包含给定名字的BeanDefinition信息*/ boolean containsBeanDefinition(String beanName); /*返回已经注册的BeanDefinition名字列表*/ String[] getBeanDefinitionNames(); /*返回已经注册的BeanDefinition数量*/ int getBeanDefinitionCount(); /*确定给定的bean名称是否已经在注册中心使用， 怎么判断是否使用呢？就是是否有别名注册在此名称下，或者已经有注册在这里的bean。*/ boolean isBeanNameInUse(String beanName);&#125; 7.初始化beanDefinition的加载器**initBeanDefinitionReader(beanDefinitionReader)**​ 123protected void initBeanDefinitionReader(XmlBeanDefinitionReader reader) &#123; reader.setValidating(this.validating);&#125; 初始化用于加载次上下文的beanDefinition的beanDefinitionReader。默认实现为空。可以再子类中覆盖，例如关闭xml验证或使用不同的_**XmlBeanDefinitionParser**_实现。​ 8.加载bean的定义信息**loadBeanDefinitions(beanDefinitionReader)**​ 使用给定的_**XmlBeanDefinitionReader**_加载**beanDefinition**。​ **BeanFactory**的生命周期由_**refreshBeanFactory() **_处理，因此此方法仅用于加载和注册**beanDefinition**。 123456789101112131415protected void loadBeanDefinitions(XmlBeanDefinitionReader reader) throws BeansException, IOException &#123; /*这里实际上是一个钩子方法，经典的模板模式，子类根据需要对方法进行重写，实际上加载xml的时候，这里锤子也没拿到*/ Resource[] configResources = getConfigResources(); /*如果资源不为空，走这里的逻辑，但是上面已经分析过，实际上锤子也没拿到，所以走下面的逻辑*/ if (configResources != null) &#123; reader.loadBeanDefinitions(configResources); &#125; /*获取配置文件位置*/ String[] configLocations = getConfigLocations(); /*此时读取到了我们在配置文件指定的配置文件 beans.xml*/ if (configLocations != null) &#123; reader.loadBeanDefinitions(configLocations); &#125;&#125; 在这里我们注意两个方法：​ **getConfigLocations()** 获取到配置文件的位置 **reader.loadBeanDefinitions(configLocations)** 使用bean定义信息的加载器加载**beanDefinition**。 ​ 1234@Nullableprotected String[] getConfigLocations() &#123; return (this.configLocations != null ? this.configLocations : getDefaultConfigLocations());&#125; 返回一个资源位置数组，指的是构建此上下文应用的时候使用的 xml配置文件。还可以包括位置模式，这将通过_**ResourcePatternResolver**_处理。默认实现返回null。子类可以重写这个方法用来提供一组资源位置来加载**beanDefinition**。​ 9.reader.loadBeanDefinitions(configLocations)用bean定义信息的加载器加载**beanDefinition**。\u0000 123456789101112@Overridepublic int loadBeanDefinitions(String... locations) throws BeanDefinitionStoreException &#123; /*断言 判空*/ Assert.notNull(locations, &quot;Location array must not be null&quot;); /*记录beanDefinition的数量*/ int count = 0; /*迭代加载beanDefinition*/ for (String location : locations) &#123; count += loadBeanDefinitions(location); &#125; return count;&#125; 这里是根据配置文件的位置数组进行循环迭代加载并记录**beanDefinition**的数量。​ 来到了**AbstractBeanDefinitionReader**的**loadBeanDefinitions()**。​ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public int loadBeanDefinitions(String location, @Nullable Set&lt;Resource&gt; actualResources) throws BeanDefinitionStoreException &#123; /*获取资源加载器*/ ResourceLoader resourceLoader = getResourceLoader(); /*如果资源加载器为空，抛异常*/ if (resourceLoader == null) &#123; throw new BeanDefinitionStoreException( &quot;Cannot load bean definitions from location [&quot; + location + &quot;]: no ResourceLoader available&quot;); &#125; /*如果资源加载器是资源模式解析器类型的*/ if (resourceLoader instanceof ResourcePatternResolver) &#123; // Resource pattern matching available. try &#123; /* 将xml配置文件加载到resources中，resource其实就是spring底层封装了很多的细节， 抽象出来的资源顶层接口 让开发人员不必专注于底层配置文件的加载细节 */ Resource[] resources = ((ResourcePatternResolver) resourceLoader).getResources(location); /*加载beanDefinition*/ int count = loadBeanDefinitions(resources); /*这玩意不知道是啥，反正是空，没啥锤子用*/ if (actualResources != null) &#123; Collections.addAll(actualResources, resources); &#125; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Loaded &quot; + count + &quot; bean definitions from location pattern [&quot; + location + &quot;]&quot;); &#125; return count; &#125; catch (IOException ex) &#123; throw new BeanDefinitionStoreException( &quot;Could not resolve bean definition resource pattern [&quot; + location + &quot;]&quot;, ex); &#125; &#125;/*走到这里说明资源加载器肯定不是资源模式解析器类型的*/ else &#123; // 只能通过绝对网址加载单个资源 Resource resource = resourceLoader.getResource(location); int count = loadBeanDefinitions(resource); if (actualResources != null) &#123; actualResources.add(resource); &#125; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Loaded &quot; + count + &quot; bean definitions from location [&quot; + location + &quot;]&quot;); &#125; return count; &#125;&#125; 首先获取到资源的加载器，如果加载器是空，那就说明程序无法往下执行了，直接抛异常。​ 如果资源加载器是资源解析器模式的，加载xml文件到**Resource**数组中，**Resource**是什么在上一篇中已经介绍过了，在此不再赘述。​ 此时在根据**Resource**数组去加载**beanDefinition**，最后返回加载的**beanDefinition**的数量。（注意：这里走的是else的逻辑，因为默认我们不是位置模式。）​ 注意这个时候思路已经很明确了，准备了这么多实际上到这里就分为了两步：​ 加载xml配置文件 **getResource(location)** 通过xml配置文件去加载**beanDefinition** **loadBeanDefinitions(resource)** ​ 10.加载xml配置文件来到**DefaultResourceLoader**。​ 12345678910111213141516171819202122232425262728293031@Overridepublic Resource getResource(String location) &#123; Assert.notNull(location, &quot;Location must not be null&quot;); //循环遍历使用解析器解析该location的资源，如果资源不为空，直接返回 for (ProtocolResolver protocolResolver : getProtocolResolvers()) &#123; Resource resource = protocolResolver.resolve(location, this); if (resource != null) &#123; return resource; &#125; &#125; /*以/开头那么根据path去寻找*/ if (location.startsWith(&quot;/&quot;)) &#123; return getResourceByPath(location); &#125; /*以classpath开头，那么抽象为ClassPathResource*/ else if (location.startsWith(CLASSPATH_URL_PREFIX)) &#123; return new ClassPathResource(location.substring(CLASSPATH_URL_PREFIX.length()), getClassLoader()); &#125; else &#123; try &#123; /*其他情况采用urlResource来加载*/ URL url = new URL(location); return (ResourceUtils.isFileURL(url) ? new FileUrlResource(url) : new UrlResource(url)); &#125; catch (MalformedURLException ex) &#123; // No URL -&gt; resolve as resource path. return getResourceByPath(location); &#125; &#125;&#125; 接下来就是加载**beanDefinition**信息。\u0000 11.加载beanDefinition**loadBeanDefinitions(resource)**​ 来到了 **XmlBeanDefinitionReader**。​ 123456@Overridepublic int loadBeanDefinitions(Resource resource) throws BeanDefinitionStoreException &#123; //1.将resource包装成带有编码格式的EncodedResource //2.重载调用loadBeanDefinitions() return loadBeanDefinitions(new EncodedResource(resource));&#125; 我们来到重载的方法。**loadBeanDefinitions(resource)**​ 1234567891011121314151617181920212223242526272829303132333435363738394041public int loadBeanDefinitions(EncodedResource encodedResource) throws BeanDefinitionStoreException &#123; //跳过断言 日志 Assert.notNull(encodedResource, &quot;EncodedResource must not be null&quot;); if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Loading XML bean definitions from &quot; + encodedResource); &#125; //获取引用：当前线程已经加载过的encodingResource资源 Set&lt;EncodedResource&gt; currentResources = this.resourcesCurrentlyBeingLoaded.get(); //将当前的encodingResource加入到threadlocal的set中，加入失败说明当前资源已经加载过了，不能重复加载，需要抛出异常 if (!currentResources.add(encodedResource)) &#123; throw new BeanDefinitionStoreException( &quot;Detected cyclic loading of &quot; + encodedResource + &quot; - check your import definitions!&quot;); &#125; //jdk新版本的语法糖 拿到资源的输入流 try (InputStream inputStream = encodedResource.getResource().getInputStream()) &#123; //因为接下来要使用 sax解析器，解析xml文件 ，所以需要将输入流包装成inputsource， //inputsource是sax中表示资源的对象 InputSource inputSource = new InputSource(inputStream); //设置字符编码 spring源码中判断逻辑特别多 ，稳定化的框架并不相信一切外部的输入 //这也是软件架构原则中的规范之一 稳定性体现 if (encodedResource.getEncoding() != null) &#123; inputSource.setEncoding(encodedResource.getEncoding()); &#125; //真正干活的逻辑 ，加载beanDefinition的入口 return doLoadBeanDefinitions(inputSource, encodedResource.getResource()); &#125; catch (IOException ex) &#123; throw new BeanDefinitionStoreException( &quot;IOException parsing XML document from &quot; + encodedResource.getResource(), ex); &#125; finally &#123; //因为resourcesCurrentlyBeingLoaded表示当前线程正在加载的redource //执行到这里说明资源已经加载完了或者失败了 //所以需要将当前资源移除出去 currentResources.remove(encodedResource); //set没有数据了，说明没啥乱用了，清理一下内存 防止threadlocal内存泄漏 if (currentResources.isEmpty()) &#123; this.resourcesCurrentlyBeingLoaded.remove(); &#125; &#125;&#125; 这里使用了一个委派模式，将真正干活的逻辑交给了**doLoadBeanDefinitions()**。\u0000 1234567891011121314151617181920protected int doLoadBeanDefinitions(InputSource inputSource, Resource resource) throws BeanDefinitionStoreException &#123; try &#123; //把 resource 转换成程序层面可以识别的有层次结构的document对象 Document doc = doLoadDocument(inputSource, resource); //解析文档对象，生成beanDefinition注册到beanFactory中，最终返回新注册到beanFactory的beanDefinition数量 int count = registerBeanDefinitions(doc, resource); //日志打印 if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Loaded &quot; + count + &quot; bean definitions from &quot; + resource); &#125; //返回新注册 bean定义信息的数量 return count; &#125; catch (BeanDefinitionStoreException ex) &#123; throw ex; &#125; //这里省略部分catch的逻辑&#125; 这里再次分成了两步：​ 将**Resource**转换成**Document**。 **doLoadDocument(inputSource, resource)** 解析文档对象，生成**beanDefinition**注册到**BeanFactory**。 **registerBeanDefinitions(doc, resource)** ​ 12.Resource转化成Document**doLoadDocument(inputSource, resource)**​ 1234567891011121314protected Document doLoadDocument(InputSource inputSource, Resource resource) throws Exception &#123; //这个方法就是将inputSource转化成可以识别的文档对象 //通过文档加载器来转化的 //1. getEntityResolver？ 这个实体解析器 //spring官网说明：如果sax应用程序中需要实现自定义处理外部实体，则必须实现此接口并使用setEntityResolver方法向sax驱动器注册一个实例 //也就是说，对于解析一个xml，sax首先读取xml文档上的声明，根据声明去寻找相应的DTD/XSD定义，以便对文档进行一个校验。 //默认的寻找校验规则，即通过网络来下载响应的DTD/XSD声明，在进行校验。并且下载的过程是一个漫长且不可控的过程，当下在失败后，这里还会抛出异常 //那么有什么办法可以避免直接从网络上下载呢？使用EntityResolver //EntityResolver的作用是项目本身可以提供一个如何寻找DTD/XSD声明的方法，即让程序来实现寻找定义声明的过程，比如我们将定义文件 //放到项目的某个地方，在实现时直接将此文件读取并返回给sax即可，这样避免了通过网络来寻找对应的声明。 //2.验证模式是怎么获取的？getValidationModeForResource() return this.documentLoader.loadDocument(inputSource, getEntityResolver(), this.errorHandler, getValidationModeForResource(resource), isNamespaceAware());&#125; **getValidationModeForResource(resource)**​ 123456789101112131415161718protected int getValidationModeForResource(Resource resource) &#123; //获取默认的validationMode int validationModeToUse = getValidationMode(); //条件成立：说明set过默认值，一般情况下，不会走这里，都是使用自动检测 if (validationModeToUse != VALIDATION_AUTO) &#123; return validationModeToUse; &#125; //自动检查xml使用的是哪种验证模式？由这个方法决定 int detectedMode = detectValidationMode(resource); if (detectedMode != VALIDATION_AUTO) &#123; return detectedMode; &#125; // Hmm, we didn&#x27;t get a clear indication... Let&#x27;s assume XSD, // since apparently no DTD declaration has been found up until // detection stopped (before finding the document&#x27;s root tag). return VALIDATION_XSD;&#125; **documentLoader.loadDocument()**​ 1234567891011@Overridepublic Document loadDocument(InputSource inputSource, EntityResolver entityResolver, ErrorHandler errorHandler, int validationMode, boolean namespaceAware) throws Exception &#123; DocumentBuilderFactory factory = createDocumentBuilderFactory(validationMode, namespaceAware); if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Using JAXP provider [&quot; + factory.getClass().getName() + &quot;]&quot;); &#125; DocumentBuilder builder = createDocumentBuilder(factory, entityResolver, errorHandler); return builder.parse(inputSource);&#125; **DocumentBuilder.parse() **就是真正的解析逻辑。​ 13.解析文档对象注册到BeanFactory**registerBeanDefinitions()** 解析文档对象，生成beanDefinition注册到beanFactory中，最终返回新注册到beanFactory的beanDefinition数量。\u0000 123456789101112public int registerBeanDefinitions(Document doc, Resource resource) throws BeanDefinitionStoreException &#123; //创建一个 BeanDefinitionDocumentReader 一对一处理 每个文档对象都会创建一个 BeanDefinitionDocumentReader 对象 BeanDefinitionDocumentReader documentReader = createBeanDefinitionDocumentReader(); //getRegistry 会返回程序创建的beanFactory实例 //countBefore 解析doc之前，bf中已经有的bd数量 int countBefore = getRegistry().getBeanDefinitionCount(); //解析文档，并且注册到bf中 //xmlReaderContext :包含最主要的参数是当前 this -&gt; xmlBeanDefinitionReader -&gt; bf documentReader.registerBeanDefinitions(doc, createReaderContext(resource)); //返回值 返回新注册的bd数量 最新的 - 注册之前的 return getRegistry().getBeanDefinitionCount() - countBefore;&#125; 看重载的方法​ 12345678@Overridepublic void registerBeanDefinitions(Document doc, XmlReaderContext readerContext) &#123; //引用上下文对象 this.readerContext = readerContext; //doc.getDocumentElement() 拿出文档代表的xml的顶层标签 &lt;beans&gt;&lt;/beans&gt; /*真正的解析xml的逻辑*/ doRegisterBeanDefinitions(doc.getDocumentElement());&#125; 又是一个委派模式，终于来到了解析xml的逻辑，这里的逻辑其实没有什么可以学习的点，追到这里主要是为了串起来整个解析xml文件的流程，具体解析xml的过程不做重点说明。​ 1234567891011121314151617181920212223242526272829303132333435protected void doRegisterBeanDefinitions(Element root) &#123; BeanDefinitionParserDelegate parent = this.delegate; //方法返回一个beans标签 解析器对象 this.delegate = createDelegate(getReaderContext(), root, parent); //解析器对象去判断是不是默认的命名空间 一般情况下 条件成立 if (this.delegate.isDefaultNamespace(root)) &#123; //获取 profile 属性， 环境 Dev prod test String profileSpec = root.getAttribute(PROFILE_ATTRIBUTE); //条件成立 说明 beans 标签上 有 profile 属性 if (StringUtils.hasText(profileSpec)) &#123; //将属性值按照,拆分成字符串数组 String[] specifiedProfiles = StringUtils.tokenizeToStringArray( profileSpec, BeanDefinitionParserDelegate.MULTI_VALUE_ATTRIBUTE_DELIMITERS); // We cannot use Profiles.of(...) since profile expressions are not supported // in XML config. See SPR-12458 for details. // Environment.acceptsProfiles(String [] args) 条件成立 ：说明beans 标签可以继续解析 bd //这里取反 ，所以 就是 if里面整个条件成立 ，说明该 beans 标签不在继续解析 ，直接返回 if (!getReaderContext().getEnvironment().acceptsProfiles(specifiedProfiles)) &#123; if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Skipped XML bean definition file due to specified profiles [&quot; + profileSpec + &quot;] not matching: &quot; + getReaderContext().getResource()); &#125; return; &#125; &#125; &#125; //这里是留给子类的扩展点 preProcessXml(root); parseBeanDefinitions(root, this.delegate); //这里也是留给子类扩展 体现的软件设计模式的开闭原则 postProcessXml(root); this.delegate = parent;&#125; 已经走到了这里，大概看一下里面的逻辑。​ **parseBeanDefinitions(root, this.delegate)**​ 12345678910111213141516171819202122232425protected void parseBeanDefinitions(Element root, BeanDefinitionParserDelegate delegate) &#123; /*条件成立说明root是spring缺省的命名空间*/ if (delegate.isDefaultNamespace(root)) &#123; /*这里获取的，大部分情况下，其实都是bean标签*/ NodeList nl = root.getChildNodes(); /*迭代处理每一个子标签*/ for (int i = 0; i &lt; nl.getLength(); i++) &#123; Node node = nl.item(i); if (node instanceof Element) &#123; Element ele = (Element) node; /*说明子标签也是默认的spring标签*/ if (delegate.isDefaultNamespace(ele)) &#123; /*默认标签解析逻辑 step into*/ parseDefaultElement(ele, delegate); &#125;/*自定义标签解析逻辑*/ else &#123; delegate.parseCustomElement(ele); &#125; &#125; &#125; &#125;/*root不是默认的命名空间，解析自定义标签逻辑*/ else &#123; delegate.parseCustomElement(root); &#125;&#125; 因为我们没有自定义标签，所以看默认标签的解析逻辑 **parseDefaultElement(ele, delegate)**。​ 1234567891011121314151617private void parseDefaultElement(Element ele, BeanDefinitionParserDelegate delegate) &#123; /*条件成立，说明此时是import标签*/ if (delegate.nodeNameEquals(ele, IMPORT_ELEMENT)) &#123; importBeanDefinitionResource(ele); &#125;/*条件成立说明是alias别名标签*/ else if (delegate.nodeNameEquals(ele, ALIAS_ELEMENT)) &#123; processAliasRegistration(ele); &#125;/*此时说明是bean标签*/ else if (delegate.nodeNameEquals(ele, BEAN_ELEMENT)) &#123; /*解析bean标签*/ processBeanDefinition(ele, delegate); &#125;/*说明是嵌套beans标签*/ else if (delegate.nodeNameEquals(ele, NESTED_BEANS_ELEMENT)) &#123; // 递归到上层重新来了 doRegisterBeanDefinitions(ele); &#125;&#125; \u0000继续看一下bean标签的解析逻辑**processBeanDefinition(ele, delegate)**。 1234567891011121314151617181920protected void processBeanDefinition(Element ele, BeanDefinitionParserDelegate delegate) &#123; /*用解析器对象解析标签 hodler里面包含三个属性：beanDefinition，beanName，Alias别名信息*/ BeanDefinitionHolder bdHolder = delegate.parseBeanDefinitionElement(ele); if (bdHolder != null) &#123; /*如果当前hodler需要被装饰，执行装饰逻辑 主要是处理自定义属性*/ bdHolder = delegate.decorateBeanDefinitionIfRequired(ele, bdHolder); try &#123; // Register the final decorated instance. /*注册当前bean倒容器中 通过readerContext拿到XMLBeanDefinition拿到beanFactory*/ BeanDefinitionReaderUtils.registerBeanDefinition(bdHolder, getReaderContext().getRegistry()); &#125; catch (BeanDefinitionStoreException ex) &#123; getReaderContext().error(&quot;Failed to register bean definition with name &#x27;&quot; + bdHolder.getBeanName() + &quot;&#x27;&quot;, ele, ex); &#125; // Send registration event. /*发送一个bean注册完成的事件*/ getReaderContext().fireComponentRegistered(new BeanComponentDefinition(bdHolder)); &#125;&#125; **BeanDefinitionReaderUtils.registerBeanDefinition()** 单实例bean的注册逻辑。​ 12345678910111213141516public static void registerBeanDefinition( BeanDefinitionHolder definitionHolder, BeanDefinitionRegistry registry) throws BeanDefinitionStoreException &#123; // Register bean definition under primary name. String beanName = definitionHolder.getBeanName(); registry.registerBeanDefinition(beanName, definitionHolder.getBeanDefinition()); // Register aliases for bean name, if any. String[] aliases = definitionHolder.getAliases(); if (aliases != null) &#123; for (String alias : aliases) &#123; registry.registerAlias(beanName, alias); &#125; &#125;&#125; 最终将**beanDefinition**放到了**beanDefinitionMap**里面，至此整个解析xml配置文件，加载**beanDefinition**并返回全新**BeanFactory**的逻辑结束了。​ ​ ​ ​","categories":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/tags/Spring/"}]},{"title":"Spring[二]IOC容器初始化","slug":"Spring/Spring[二]Ioc容器的初始化","date":"2022-01-11T05:58:52.126Z","updated":"2022-01-11T06:06:02.565Z","comments":true,"path":"2022/01/11/Spring/Spring[二]Ioc容器的初始化/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/Spring/Spring[%E4%BA%8C]Ioc%E5%AE%B9%E5%99%A8%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96/","excerpt":"","text":"从上层视角入手，观测Spring容器的初始化工作。 1.初始准备首先在Spring的源码工程里面创建一段代码，通过简单的运行代码，来追踪Spring ioc 容器的整个启动流程。 12345678910111213141516171819202122232425262728293031public class IocMainTest &#123; private static final String CONFIG_LOCATION = &quot;applicationContext.xml&quot;; public static void main(String[] args) &#123; //自定义 ioc 容器进行扩展 testDiyIoc(); &#125; private static void testDiyIoc() &#123; MyClassPathXmlApplicationContext ioc = new MyClassPathXmlApplicationContext(CONFIG_LOCATION); Person person = ioc.getBean(&quot;person&quot;, Person.class); System.out.println(&quot;person = &quot; + person); &#125; &#125;public class MyClassPathXmlApplicationContext extends ClassPathXmlApplicationContext &#123; public MyClassPathXmlApplicationContext(String...configLocations)&#123; super(configLocations); &#125; @Override protected void initPropertySources()&#123; System.out.println(&quot;这是我自己定义的ioc容器&quot;); //getEnvironment().setRequiredProperties(&quot;ES_HOME&quot;); &#125;&#125; 程序启动之后，首先会进入自定义的ioc容器，然后通过构造器显式调用**super()**方法来到**ClassPathXmlApplicationContext**。 1234567891011121314public ClassPathXmlApplicationContext( String[] configLocations, boolean refresh, @Nullable ApplicationContext parent) throws BeansException &#123; //调用父类的构造方法，AbstractApplicationContext /*模板方法模式 和 钩子方法 易于扩展 开闭原则*/ super(parent); //解析配置文件路径 setConfigLocations(configLocations); /*refresh 是表示一个容器是否刷新过得标识符，如果容器还没有刷新过就进行容器刷新，实际上这里是一个双端检测锁*/ if (refresh) &#123; /*spring ioc 容器刷新方法*/ refresh(); &#125;&#125; 这里面主要是做了两件事：​ 解析配置文件路径 **setConfigLocations()** 判断容器是否刷新过，如果尚未刷新，那么刷新ioc容器 **refresh()** ​ 2.解析配置文件1234567891011121314151617public void setConfigLocations(@Nullable String... locations) &#123; /*如果location！=null 也就是说配置文件不为空，spring扫描到了配置文件，那么我们就可以以xml的形式启动spring容器*/ if (locations != null) &#123; /*断言：配置文件所在位置不能为空*/ Assert.noNullElements(locations, &quot;Config locations must not be null&quot;); /*引用*/ this.configLocations = new String[locations.length]; /*迭代解析路径*/ for (int i = 0; i &lt; locations.length; i++) &#123; /*解析路径*/ this.configLocations[i] = resolvePath(locations[i]).trim(); &#125; &#125; else &#123; this.configLocations = null; &#125;&#125; 如果我们程序是有xml的spring配置文件，那么该文件被扫描到以后，（当然可能不止一个)，回去迭代解析配置文件的路径。 **resolvePath()**​ 123protected String resolvePath(String path) &#123; return getEnvironment().resolveRequiredPlaceholders(path);&#125; 解析给定的路径，如果有必要的话，用相应的环境属性值替换占位符，应用于配置位置。**resolveRequiredPlaceholders()**​ 12345@Overridepublic String resolveRequiredPlaceholders(String text) throws IllegalArgumentException &#123; /*占位符解析器解析并替换占位符*/ return this.propertyResolver.resolveRequiredPlaceholders(text);&#125; 这里其实是将解析的逻辑交给了属性解析器。**resolveRequiredPlaceholders()** 123456789@Overridepublic String resolveRequiredPlaceholders(String text) throws IllegalArgumentException &#123; if (this.strictHelper == null) &#123; /*如果占位符解析器为空，就创建一个占位符解析器*/ this.strictHelper = createPlaceholderHelper(false); &#125; /*真正去解析占位符的操作*/ return doResolvePlaceholders(text, this.strictHelper);&#125; 在这个抽象的解析器里面又通过委派模式委派给了真正的解析占位符操作的方法。 **doResolvePlaceholders()**​ 这里为什么要这么设计？这其实是两个扩展点，开发人员可以自己继承这两个抽象类，重写对应的方法，来实现定制化的解析占位符的逻辑。​ 1234private String doResolvePlaceholders(String text, PropertyPlaceholderHelper helper) &#123; /*使用传入的属性占位符解析器去解析并替换占位符*/ return helper.replacePlaceholders(text, this::getPropertyAsRawString);&#125; 这里交给了真正的属性解析器**PropertyPlaceholderHelper**去解析属性占位符。​ 12345public String replacePlaceholders(String value, PlaceholderResolver placeholderResolver) &#123; Assert.notNull(value, &quot;&#x27;value&#x27; must not be null&quot;); /*解析字符串类型的值*/ return parseStringValue(value, placeholderResolver, null);&#125; 将所有格式的**$&#123;name&#125;**占位符替换为从提供的**PropertyPlaceHolderHelper**，**PlaceHolderResolver**返回的值。​ 再往下就是具体的解析字符串的逻辑，没有什么继续的必要。​ 这里面主要体现的就是利用**抽象类** 和 **委派模式** 提供了默认的属性解析器实现，当然我们也可以提供自己的解析器，只需要继承相应的抽象类，实现相应的抽象方法。​ 3.容器刷新解析完配置文件路径以后，就是判断容器是否已经刷新过，如果尚未刷新，就会进行容器的刷新动作。 **AbstractApplicationContext.refresh()**​ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677@Overridepublic void refresh() throws BeansException, IllegalStateException &#123; //加锁是因为ioc容器只能有一个，防止重复创建 synchronized (this.startupShutdownMonitor) &#123; //准备开始容器刷新 StartupStep contextRefresh = this.applicationStartup.start(&quot;spring.context.refresh&quot;); // 获取当前系统的时间，给容器设置同步锁标识 prepareRefresh(); // 解析xml配置文件或注解 bean定义资源的载入，获取一个全新的bean工厂 ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // 给容器中注册一些组件，添加切面，类加载器，表达式解析器，注解解析器，事件处理器 prepareBeanFactory(beanFactory); try &#123; //为容器的某些子类指定后置处理器，子类可以通过重写这个方法， // 在bean工厂创建并预备完成以后做进一步的设置 /*可以在bd创建出实例之前，对bd信息做进一步的修改*/ postProcessBeanFactory(beanFactory); StartupStep beanPostProcess = this.applicationStartup.start(&quot;spring.context.beans.post-process&quot;); /** * 执行bean工厂的后置处理器，先执行bean定义注册后置处理器，在执行bean工厂的后置处理器。 * 执行顺序：先执行实现了优先级接口的，在执行带有order的，最后执行其他的。 */ invokeBeanFactoryPostProcessors(beanFactory); // 获取所有的后置处理器，先注册带优先级的，在注册带order的， // 在注册剩下的。注册一个ApplicationListenerDetector， // 在bean完成创建后检查是是否是ApplicationListener，如果是，添加到容器。 registerBeanPostProcessors(beanFactory); beanPostProcess.end(); // 做一些国际化相关的操作 initMessageSource(); // 初始化事件派发器：获取beanFactory， // 从beanFactory获取事件派发器，如果没有，创建一个放到容器中。 initApplicationEventMulticaster(); // 留给子容器，子容器可以再容器刷新的时候加入自己的逻辑。 onRefresh(); // 为事件派发器注册事件监听器，派发一些早期事件。 registerListeners(); // 初始化剩下的所有的非懒加载的单实例bean。 finishBeanFactoryInitialization(beanFactory); // 初始化容器的生命周期事件处理器， // 回调onRefresh()，并发布容器的生命周期事件。 finishRefresh(); &#125; catch (BeansException ex) &#123; if (logger.isWarnEnabled()) &#123; logger.warn(&quot;Exception encountered during context initialization - &quot; + &quot;cancelling refresh attempt: &quot; + ex); &#125; // Destroy already created singletons to avoid dangling resources. destroyBeans(); // Reset &#x27;active&#x27; flag. cancelRefresh(ex); // Propagate exception to caller. throw ex; &#125; finally &#123; // Reset common introspection caches in Spring&#x27;s core, since we // might not ever need metadata for singleton beans anymore... resetCommonCaches(); contextRefresh.end(); &#125; &#125;&#125; ​ 这个方法很长，也可以说是ioc容器的核心，这里面涉及到的关于Spring的额外的知识也比较繁多，在此会对相关内容进行一一列举。 4.属性资源设置与校验​ 首先会去获取当前系统的时间，给容器设置同步锁标识。 **prepareRefresh()**​ 12345678910111213141516171819202122232425262728293031323334353637protected void prepareRefresh() &#123; // 获取当前系统的时间 this.startupDate = System.currentTimeMillis(); //关闭状态设置为false this.closed.set(false); //激活状态设置为true this.active.set(true); //日志打印 if (logger.isDebugEnabled()) &#123; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Refreshing &quot; + this); &#125; else &#123; logger.debug(&quot;Refreshing &quot; + getDisplayName()); &#125; &#125; // 留给子类扩展：可以做一些资源初始化 initPropertySources(); // 检验必须有环境变量 ，结合上面的方法可以设置哪些属性是必填的 /*demo：MyClassPathXmlApplicationContext*/ getEnvironment().validateRequiredProperties(); // 获取容器早期的事件监听器 if (this.earlyApplicationListeners == null) &#123; this.earlyApplicationListeners = new LinkedHashSet&lt;&gt;(this.applicationListeners); &#125; else &#123; // 如果已经有了，就先清空 在push this.applicationListeners.clear(); this.applicationListeners.addAll(this.earlyApplicationListeners); &#125; // 允许这些早期事件在容器刷新之后发布出去 this.earlyApplicationEvents = new LinkedHashSet&lt;&gt;();&#125; \u0000这里面可以关注一下几个扩展点：​ 可以再子类做一些资源初始化的方法 **initPropertySources()** 对上一步设置的属性变量进行校验 **validateRequiredProperties()** ​ 随后会获取到容器早期的一些事件监听器，这些早期事件会在容器刷新成功之后发布出去。​ 5.解析配置文件这一步主要的操作就是解析Spring的配置文件或者注解标识的配置类，加载bean的定义信息 **BeanDefinition**，简称bd。最后创建一个**BeanFactory**，简称bf。​ **obtainFreshBeanFactory()**\u0000 123456protected ConfigurableListableBeanFactory obtainFreshBeanFactory() &#123; //刷新bean工厂 refreshBeanFactory(); /**返回bean工厂*/ return getBeanFactory();&#125; 这里面主要的逻辑就在于bean工厂的刷新。 **refreshBeanFactory()**​ 1234567891011121314151617181920212223242526@Overrideprotected final void refreshBeanFactory() throws BeansException &#123; /*如果已经有了bean工厂，通常情况下并不会，什么情况下会有？通过applicationContext直接调用refresh方法*/ if (hasBeanFactory()) &#123; /*销毁里面的所有bean*/ destroyBeans(); /*关闭bean工厂*/ closeBeanFactory(); &#125; try &#123; //不管上面是否已经有bean工厂存在，最终都会走到这里，去创建一个bean工厂 DefaultListableBeanFactory beanFactory = createBeanFactory(); /*设置序列化id*/ beanFactory.setSerializationId(getId()); /*对工厂进行一些定制化设置*/ customizeBeanFactory(beanFactory); /*加载bean的定义信息*/ loadBeanDefinitions(beanFactory); /*将当前类的bean工厂引用指向创建的bean工厂*/ this.beanFactory = beanFactory; &#125; catch (IOException ex) &#123; throw new ApplicationContextException(&quot;I/O error parsing bean definition source for &quot; + getDisplayName(), ex); &#125;&#125; 这里可以关注两点：​ 如何创建一个bean工厂 **createBeanFactory()** 加载bd的信息 **loadBeanDefinitions(beanFactory)** ​ 5.1 创建bean工厂123protected DefaultListableBeanFactory createBeanFactory() &#123; return new DefaultListableBeanFactory(getInternalParentBeanFactory());&#125; 为当前的容器上下文创建了一个内部bean工厂，每次刷新的时候会尝试调用当前方法。默认是创建了一个_**DefaultListableBeanFactory**_，并且将此上下文父级的内部bean作为父 bean工厂。可以再子类中覆盖，例如自定义_**DefaultListableBeanFactory**_的设置。​ 5.2 BeanFactory上面提到了一个**DefaultListableBeanFactory**，那么都有哪些bean工厂呢？为什么有这么多bean工厂呢？​ 来到**BeanFactory**的顶层接口，**BeanFactory** 作为顶层接口，提供了一些列操作工厂内对象的方法，有一点需要注意的。​ 123456/** * * 对FactoryBean的转义定义，因为如果使用bean的名字检索 FactoryBean 得到的对象 * 是工厂生成的对象，如果想要得到工厂本身，需要转义。 */String FACTORY_BEAN_PREFIX = &quot;&amp;&quot;; \u0000这是决定你获取当前工厂还是当前工厂内对象的一个重要标识符号，切记。**&amp;**​ 接下来再去看 **BeanFactory**的继承关系，作为顶层接口，我们自然是从上往下看。​ \u0000这里面列举了几个重要的Bean工厂，同时也可以看到，**DefaultListableBeanFactory**本身其实是bean工厂的重要实现类。​ 从图上的继承关系可以看出：**ListableBeanFactory**、**HierarchicalBeanFactory** 和 **AutowireCapableBeanFactory**。​ 为什么要定义这么多层次的接口呢？​ 主要是为了区分在Spring内部操作过程中对象的传递和转化过程，对对象的数据访问所做的一些限制。这三个接口共同定义了 Bean的集合，Bean之间关系，Bean行为。​ 在 **BeanFactory** 里只对 IOC 容器的基本行为作了定义，根本不关心你的 Bean 是如何定义怎样加载的。正如我们只关心工厂里得到什么的产品对象，至于工厂是怎么生产这些对象的，这个基本的接口不关心。要知道工厂是如何产生对象的，需要看具体的 IOC 容器实现，Spring 提供了许多 IOC 容器的 实 现 。 **ApplicationContext**是Spring提供的一个高级的ioc容器，他除了能够提供ioc容器的基本功能以外，还未用户提供了一些附加的服务：​ 支持信息源，可以实现国际化。 **MessageSource** 访问资源 **ResourcePatternResolver** 支持应用事件 **ApplicationEventPublisher** ​ 容器本身其实也是扩展点，支持我们通过类的继承来做一些定制化实现。​ 5.3加载BeanDefinition**loadBeanDefinitions(beanFactory)**在**AbstractApplicationContext**其实也是一个空方法，交给子类去实现。找到具体的实现**AbstractXmlApplicationContext**。​ 12345678910111213141516@Overrideprotected void loadBeanDefinitions(DefaultListableBeanFactory beanFactory) throws BeansException, IOException &#123; // 创建一个xml 的beanDefinition加载器 这个玩意里面持有一个beanFactory的引用 XmlBeanDefinitionReader beanDefinitionReader = new XmlBeanDefinitionReader(beanFactory); /*给beanDefinition 加载器设置上下文环境 资源加载器 实体解析器*/ /*这玩意我记得都是用的默认的*/ beanDefinitionReader.setEnvironment(this.getEnvironment()); beanDefinitionReader.setResourceLoader(this); beanDefinitionReader.setEntityResolver(new ResourceEntityResolver(this)); /*初始化beanDefinition加载器*/ initBeanDefinitionReader(beanDefinitionReader); /*使用beanDefinition加载器加载beanDefinitions*/ loadBeanDefinitions(beanDefinitionReader);&#125; 这里面需要关注两个方法： 初始化**beanDefinition**加载器 **initBeanDefinitionReader()** 使用**beanDefinition**加载器加载**beanDefinitions** **loadBeanDefinitions()** ​ 不过在分析这两个方法之前，先明确一下**BeanDefinition**。​ 5.4BeanDefinition什么是BeanDefinition？​ **BeanDefinition**作为定义Spring Bean 文件中bean的接口，可以说是bean的抽象的数据结构，它包括属性参数，构造器参数，以及其他具体的参数。​ 继承关系​ **BeanDefinition**继承了**AttributeAccessor**和**BeanMetaDataElement**接口，拥有了对元数据访问的功能。​ 看一下具体的代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107public interface BeanDefinition extends AttributeAccessor, BeanMetadataElement &#123; /*单例的作用域*/ String SCOPE_SINGLETON = ConfigurableBeanFactory.SCOPE_SINGLETON; /*多例作用域*/ String SCOPE_PROTOTYPE = ConfigurableBeanFactory.SCOPE_PROTOTYPE; /*用户*/ int ROLE_APPLICATION = 0; /*某些复杂的配置*/ int ROLE_SUPPORT = 1; /*完全内部使用*/ int ROLE_INFRASTRUCTURE = 2; /*设置父类的名字*/ void setParentName(@Nullable String parentName); /*获取父类的名字*/ @Nullable String getParentName(); /*设置class类型名*/ void setBeanClassName(@Nullable String beanClassName); /*返回当前bean的名字（并不是准确的名字，有些childBeanDefinition是继承自父bean的名字）*/ /*所以不能依靠该名字确定class的类型*/ @Nullable String getBeanClassName(); /*设置作用域*/ void setScope(@Nullable String scope); /*获取作用域*/ @Nullable String getScope(); /*设置懒加载*/ void setLazyInit(boolean lazyInit); /*判断是否懒加载*/ boolean isLazyInit(); /*设置依赖的bean*/ void setDependsOn(@Nullable String... dependsOn); /*依赖的bean*/ @Nullable String[] getDependsOn(); /*设置优先注入其他bean*/ void setAutowireCandidate(boolean autowireCandidate); /*是否优先注入其他bean*/ boolean isAutowireCandidate(); /*设置优先自动装配*/ void setPrimary(boolean primary); /*这个bean是否优先自动装配*/ boolean isPrimary(); /*设置工厂bean的名字*/ void setFactoryBeanName(@Nullable String factoryBeanName); /*返回工厂bean的名字*/ @Nullable String getFactoryBeanName(); /*设置工厂方法名*/ void setFactoryMethodName(@Nullable String factoryMethodName); /*返回工厂方法名*/ @Nullable String getFactoryMethodName(); /*获取构造函数的值*/ ConstructorArgumentValues getConstructorArgumentValues(); /*判断是否有构造器参数*/ default boolean hasConstructorArgumentValues() &#123; return !getConstructorArgumentValues().isEmpty(); &#125; /*获取参数的值*/ MutablePropertyValues getPropertyValues(); /*判断是否有属性参数*/ default boolean hasPropertyValues() &#123; return !getPropertyValues().isEmpty(); &#125; /*设置初始化方法*/ void setInitMethodName(@Nullable String initMethodName); /*获取初始化方法的名字*/ @Nullable String getInitMethodName(); /*设置销毁方法的名字*/ void setDestroyMethodName(@Nullable String destroyMethodName); /*获取销毁方法的名字*/ @Nullable String getDestroyMethodName(); /*设置角色*/ void setRole(int role); /*获取角色*/ int getRole(); /*设置描述信息*/ void setDescription(@Nullable String description); /*获取描述信息*/ @Nullable String getDescription(); // Read-only attributes ResolvableType getResolvableType(); /*是否是单例的*/ boolean isSingleton(); /*是否是多例的*/ boolean isPrototype(); /*是否是抽象bean*/ boolean isAbstract(); /*获取资源描述*/ @Nullable String getResourceDescription(); /*有些beanDeFInition会使用beanDefinitionResource进行包装，将beanDefinition描述为一个资源*/ @Nullable BeanDefinition getOriginatingBeanDefinition();&#125; 具体实现​ **RootBeanDefinition**、**GenericBeanDefinition**、**ChildBeanDefinition** **RootBeanDefinition**是最常用的实现类，它对应一般性的元素标签，**GenericBeanDefinition**是自2.5以后新加入的bean文件配置属性定义类，是一站式服务类。在配置文件中可以定义父和子，父用**RootBeanDefinition**表示，而子用**ChildBeanDefiniton**表示，而没有父的就使用**RootBeanDefinition**表示。 **AnnotatedGenericBeanDefinition** 以**@Configuration**注解标记的会解析为**AnnotatedGenericBeanDefinition**。 **ConfigurationClassBeanDefinition** 以**@Bean**注解标记的会解析为**ConfigurationClassBeanDefinition**。 **ScannedGenericBeanDefinition** 以**@Component**注解标记的会解析为**ScannedGenericBeanDefinition**。​ ​ 总结一下：如果把ioc容器看成是一个飞机场，那么里面的bean对象就是一个个的飞机，bd则是对应的造飞机用的图纸，我们首先要拿到图纸，根据图纸才能造飞机。​ 5.5 初始化BeanDefinition加载器**initBeanDefinitionReader()** 123protected void initBeanDefinitionReader(XmlBeanDefinitionReader reader) &#123; reader.setValidating(this.validating);&#125; 初始化加载此上下文的bean定义信息的bean定义读取器，默认实现为空。可以在子类中覆盖，例如关闭xml验证或者使用不同的_**XmlBeanDefinitionParser**__ 实现。_​ 5.6 加载bean定义信息**loadBeanDefinitions()\u0000** 1234567891011121314protected void loadBeanDefinitions(XmlBeanDefinitionReader reader) throws BeansException, IOException &#123; /*这里实际上是一个钩子方法，经典的模板模式，子类根据需要对方法进行重写，实际上加载xml的时候，这里锤子也没拿到*/ Resource[] configResources = getConfigResources(); /*如果资源不为空，走这里的逻辑，但是上面已经分析过，实际上锤子也没拿到，所以走下面的逻辑*/ if (configResources != null) &#123; reader.loadBeanDefinitions(configResources); &#125; /*获取配置文件位置*/ String[] configLocations = getConfigLocations(); /*此时读取到了我们在配置文件指定的配置文件 beans.xml*/ if (configLocations != null) &#123; reader.loadBeanDefinitions(configLocations); &#125;&#125; 1234567891011121314151617public int loadBeanDefinitions(String... locations) throws BeanDefinitionStoreException &#123; /*断言 判空*/ Assert.notNull(locations, &quot;Location array must not be null&quot;); /*记录beanDefinition的数量*/ int count = 0; /*迭代加载beanDefinition*/ for (String location : locations) &#123; count += loadBeanDefinitions(location); &#125; return count;&#125;@Overridepublic int loadBeanDefinitions(String location) throws BeanDefinitionStoreException &#123; /*方法重载*/ return loadBeanDefinitions(location, null);&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public int loadBeanDefinitions(String location, @Nullable Set&lt;Resource&gt; actualResources) throws BeanDefinitionStoreException &#123; /*获取资源加载器*/ ResourceLoader resourceLoader = getResourceLoader(); /*如果资源加载器为空，抛异常*/ if (resourceLoader == null) &#123; throw new BeanDefinitionStoreException( &quot;Cannot load bean definitions from location [&quot; + location + &quot;]: no ResourceLoader available&quot;); &#125; /*如果资源加载器是资源模式解析器类型的*/ if (resourceLoader instanceof ResourcePatternResolver) &#123; // Resource pattern matching available. try &#123; /* 将xml配置文件加载到resources中，resource其实就是spring底层封装了很多的细节， 抽象出来的资源顶层接口 让开发人员不必专注于底层配置文件的加载细节 */ Resource[] resources = ((ResourcePatternResolver) resourceLoader).getResources(location); /*加载beanDefinition*/ int count = loadBeanDefinitions(resources); /*这玩意不知道是啥，反正是空，没啥锤子用*/ if (actualResources != null) &#123; Collections.addAll(actualResources, resources); &#125; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Loaded &quot; + count + &quot; bean definitions from location pattern [&quot; + location + &quot;]&quot;); &#125; return count; &#125; catch (IOException ex) &#123; throw new BeanDefinitionStoreException( &quot;Could not resolve bean definition resource pattern [&quot; + location + &quot;]&quot;, ex); &#125; &#125;/*走到这里说明资源加载器肯定不是资源模式解析器类型的*/ else &#123; // 只能通过绝对网址加载单个资源 Resource resource = resourceLoader.getResource(location); int count = loadBeanDefinitions(resource); if (actualResources != null) &#123; actualResources.add(resource); &#125; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Loaded &quot; + count + &quot; bean definitions from location [&quot; + location + &quot;]&quot;); &#125; return count; &#125;&#125; 1234567891011@Overridepublic int loadBeanDefinitions(Resource... resources) throws BeanDefinitionStoreException &#123; /*断言*/ Assert.notNull(resources, &quot;Resource array must not be null&quot;); int count = 0; /*迭代遍历加载*/ for (Resource resource : resources) &#123; count += loadBeanDefinitions(resource); &#125; return count;&#125; 最终将加载到的所有**beanDefinition**信息注册到**BeanDefinitionRegistry**。这个**BeanDefinitionRegistry**被**AbstractBeanDefinitionReader**持有。 5.7 Resource这里面涉及到了一个对象，**Resource**。​ Spring把其资源做了一个抽象，底层使用统一的资源访问接口来访问Spring的所有资源。即：不管什么格式的文件，也不管文件在哪里，到Spring底层，都只有一个访问接口，**Resource**。​ 类结构图 类和接口的分析 可以看到有四个比较重要的接口：**InputStreamSource**、**Resource**、**WritableResource**、**ContextResource**。 **InputStreamSource** 12345public interface InputStreamSource &#123; InputStream getInputStream() throws IOException;&#125; **Resource**接口 ​ 接口中定义了对于资源的判断、对资源的获取、对资源描述的获取。通过该接口可以对资源进行有效的操作。但是**Resource**接口注重于对资源的读取。 123456789101112131415161718192021222324252627282930313233343536373839404142/** * 接口中定义了对于资源的判断、对资源的获取、对资源描述的获取。通过该接口可以对资源进行有效的操作。但是Resource接口注重于对资源的读取。 */public interface Resource extends InputStreamSource &#123; /*判断是否存在*/ boolean exists(); /*判断是否可读*/ default boolean isReadable() &#123; return exists(); &#125; /*判断是否可以重复读取，如果为true表示不可以重复读取，在读取完成之后，需要关闭流*/ default boolean isOpen() &#123; return false; &#125; /*判断是否是文件*/ default boolean isFile() &#123; return false; &#125; /*获取URL地址*/ URL getURL() throws IOException; /*获取URL地址*/ URI getURI() throws IOException; /*获取文件*/ File getFile() throws IOException; /*默认通过输入流获取nio的只读字节流管道*/ default ReadableByteChannel readableChannel() throws IOException &#123; return Channels.newChannel(getInputStream()); &#125; /*资源长度*/ long contentLength() throws IOException; /*上次更新时间*/ long lastModified() throws IOException; /*根据资源的当前位置，获取相对位置的其他资源*/ Resource createRelative(String relativePath) throws IOException; /*返回资源名称*/ @Nullable String getFilename(); /*返回资源描述*/ String getDescription();&#125; **WritableResource** 因为Resource接口主要是注重对资源的读取，当我们对资源进行写入的时候，需要获取对应的判断和输出流。WritableResource接口主要定义了对写入的支持。 12345678910111213public interface WritableResource extends Resource &#123; /*返回资源是否可以被写入*/ default boolean isWritable() &#123; return true; &#125; /*获取资源的写入流*/ OutputStream getOutputStream() throws IOException; /*默认提供的支持写的nio字节管道*/ default WritableByteChannel writableChannel() throws IOException &#123; return Channels.newChannel(getOutputStream()); &#125;&#125; **ContextResource** 有些资源是相对于当前容器的，用来获取容器中的资源。 12345public interface ContextResource extends Resource &#123; String getPathWithinContext();&#125; 存在一个**AbstractResource**的抽象类，所有的对于资源获取都继承自**AbstractResource**抽象类。 其余的都是具体的实现类，用来加载指定的资源。 ​ 对资源的加载​ Spring框架为了更方便的获取资源，尽量弱化程序员对各个**Resource**接口的实现类的感知，定义了另一个**ResourceLoader**接口。接口有一个特别重要的方法：**Resource getResource(String location);** 返回**Resource**实例。因此程序猿在使用spring容器的时候，可以不去过于计较比较底层的**Resource**实现，也不需要自己创建**Resource**的实现类，而是直接使用**ResourceLoader**获取到bean容器本身的**Resource**，进而获取到相关的资源信息。​ **ResourceLoader** 只能对**classpath**路径下面的资源进行加载，并且只会加载指定的文件 123456789101112131415161718/** * Spring框架为了更方便的获取资源，尽量弱化程序员对各个Resource接口的实现类的感知，定义了另一个ResourceLoader接口。 * 接口有一个特别重要的方法：Resource getResource(String location)，返回Resource实例。因此程序员在使用Spring容器时， * 可以不去过于计较底层Resource的实现，也不需要自己创建Resource实现类，而是直接使用ReourceLoader，获取到bean容器本身的Resource， * 进而取到相关的资源信息。 */public interface ResourceLoader &#123; /** Pseudo URL prefix for loading from the class path: &quot;classpath:&quot;. */ String CLASSPATH_URL_PREFIX = ResourceUtils.CLASSPATH_URL_PREFIX; /*用来根据location来获取指定的资源*/ Resource getResource(String location); /*获取类加载器*/ @Nullable ClassLoader getClassLoader();&#125; **ResourcePatternResolver** 表示会加载所有路径下面的文件，包括jar包中的文件。同时**locationPattern**可以设置为表达式来加载对应的文件。 123456789101112131415/*表示会加载所有路径下面的文件，包括jar包中的文件。同时locationPattern可以设置为表达式来加载对应的文件。*/public interface ResourcePatternResolver extends ResourceLoader &#123; /** * 表示会加载所有路径下面的文件，包括jar包中 */ String CLASSPATH_ALL_URL_PREFIX = &quot;classpath*:&quot;; /** * 根据 */ Resource[] getResources(String locationPattern) throws IOException;&#125; 区别：​ **classpath**: ：表示从类路径中加载资源，**classpath**:和**classpath**:/是等价的，都是相对于类的根路径。资源文件库标准的在文件系统中，也可以在JAR或ZIP的类包中。​ **classpath**:*：假设多个JAR包或文件系统类路径都有一个相同的配置文件，**classpath**:只会在第一个加载的类路径下查找，而**classpath***:会扫描所有这些JAR包及类路径下出现的同名文件。​ **DefaultResourceLoader** spring实现的默认的加载器，一般其他的加载器会继承该类，并重写**getResourceByPath**方法。 123456789101112131415161718192021222324252627282930@Overridepublic Resource getResource(String location) &#123; Assert.notNull(location, &quot;Location must not be null&quot;); //循环遍历使用解析器解析该location的资源，如果资源不为空，直接返回 for (ProtocolResolver protocolResolver : getProtocolResolvers()) &#123; Resource resource = protocolResolver.resolve(location, this); if (resource != null) &#123; return resource; &#125; &#125; /*以/开头那么根据path去寻找*/ if (location.startsWith(&quot;/&quot;)) &#123; return getResourceByPath(location); &#125; /*以classpath开头，那么抽象为ClassPathResource*/ else if (location.startsWith(CLASSPATH_URL_PREFIX)) &#123; return new ClassPathResource(location.substring(CLASSPATH_URL_PREFIX.length()), getClassLoader()); &#125; else &#123; try &#123; /*其他情况采用urlResource来加载*/ URL url = new URL(location); return (ResourceUtils.isFileURL(url) ? new FileUrlResource(url) : new UrlResource(url)); &#125; catch (MalformedURLException ex) &#123; // No URL -&gt; resolve as resource path. return getResourceByPath(location); &#125; &#125;&#125; **PathMatchingResourcePatternResolver**​ Spring提供了一个**ResourcePatternResolver**实现**PathMatchingResourcePatternResolver**，它是基于模式匹配的，默认使用**AntPathMatcher**进行路径匹配，它除了支持**ResourceLoader**支持的前缀外，还额外支持“**classpath**:”用于加载所有匹配的类路径**Resource**，**ResourceLoader**不支持前缀“**classpath**:”。​ 6.prepareBeanFactory(beanFactory)给容器中注册一些组件，添加切面，类加载器，表达式解析器，注解解析器，事件处理器。 **prepareBeanFactory()**​ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152protected void prepareBeanFactory(ConfigurableListableBeanFactory beanFactory) &#123; // Tell the internal bean factory to use the context&#x27;s class loader etc. /*类加载器*/ beanFactory.setBeanClassLoader(getClassLoader()); if (!shouldIgnoreSpel) &#123; /*表达式解析器*/ beanFactory.setBeanExpressionResolver(new StandardBeanExpressionResolver(beanFactory.getBeanClassLoader())); &#125; /*属性编辑器*/ beanFactory.addPropertyEditorRegistrar(new ResourceEditorRegistrar(this, getEnvironment())); // 给当前工厂设置上下文回调 beanFactory.addBeanPostProcessor(new ApplicationContextAwareProcessor(this)); beanFactory.ignoreDependencyInterface(EnvironmentAware.class); beanFactory.ignoreDependencyInterface(EmbeddedValueResolverAware.class); beanFactory.ignoreDependencyInterface(ResourceLoaderAware.class); beanFactory.ignoreDependencyInterface(ApplicationEventPublisherAware.class); beanFactory.ignoreDependencyInterface(MessageSourceAware.class); beanFactory.ignoreDependencyInterface(ApplicationContextAware.class); beanFactory.ignoreDependencyInterface(ApplicationStartupAware.class); // BeanFactory接口未在普通工厂中注册为可解析类型。 // 注册消息资源的解析器 beanFactory.registerResolvableDependency(BeanFactory.class, beanFactory); beanFactory.registerResolvableDependency(ResourceLoader.class, this); beanFactory.registerResolvableDependency(ApplicationEventPublisher.class, this); beanFactory.registerResolvableDependency(ApplicationContext.class, this); // 将用于检测内部beans的早期后置处理器注册为ApplicationListeners。 beanFactory.addBeanPostProcessor(new ApplicationListenerDetector(this)); // Detect a LoadTimeWeaver and prepare for weaving, if found. if (!NativeDetector.inNativeImage() &amp;&amp; beanFactory.containsBean(LOAD_TIME_WEAVER_BEAN_NAME)) &#123; beanFactory.addBeanPostProcessor(new LoadTimeWeaverAwareProcessor(beanFactory)); // Set a temporary ClassLoader for type matching. beanFactory.setTempClassLoader(new ContextTypeMatchClassLoader(beanFactory.getBeanClassLoader())); &#125; // Register default environment beans. if (!beanFactory.containsLocalBean(ENVIRONMENT_BEAN_NAME)) &#123; beanFactory.registerSingleton(ENVIRONMENT_BEAN_NAME, getEnvironment()); &#125; if (!beanFactory.containsLocalBean(SYSTEM_PROPERTIES_BEAN_NAME)) &#123; beanFactory.registerSingleton(SYSTEM_PROPERTIES_BEAN_NAME, getEnvironment().getSystemProperties()); &#125; if (!beanFactory.containsLocalBean(SYSTEM_ENVIRONMENT_BEAN_NAME)) &#123; beanFactory.registerSingleton(SYSTEM_ENVIRONMENT_BEAN_NAME, getEnvironment().getSystemEnvironment()); &#125; if (!beanFactory.containsLocalBean(APPLICATION_STARTUP_BEAN_NAME)) &#123; beanFactory.registerSingleton(APPLICATION_STARTUP_BEAN_NAME, getApplicationStartup()); &#125;&#125; 7.postProcessBeanFactory(beanFactory)​ 为容器的某些子类指定后置处理器，子类可以通过重写这个方法，在bean工厂创建并预备完成以后做进一步的设置，可以在bd创建出实例对象之前，对bd信息做进一步修改。**postProcessBeanFactory()** 这也是spring提供的一个扩展点。​ 12protected void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) &#123;&#125; 8.执行bean工厂的后置处理器执行bean工厂的后置处理器，先执行bean定义注册后置处理器，在执行bean工厂的后置处理器。执行顺序：先执行实现了优先级接口的，在执行带有order的，最后执行其他的。​ **invokeBeanFactoryPostProcessors(beanFactory)**​ 1234567891011protected void invokeBeanFactoryPostProcessors(ConfigurableListableBeanFactory beanFactory) &#123; //执行bean工厂的后置处理器 PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(beanFactory, getBeanFactoryPostProcessors()); // Detect a LoadTimeWeaver and prepare for weaving, if found in the meantime // (e.g. through an @Bean method registered by ConfigurationClassPostProcessor) if (!NativeDetector.inNativeImage() &amp;&amp; beanFactory.getTempClassLoader() == null &amp;&amp; beanFactory.containsBean(LOAD_TIME_WEAVER_BEAN_NAME)) &#123; beanFactory.addBeanPostProcessor(new LoadTimeWeaverAwareProcessor(beanFactory)); beanFactory.setTempClassLoader(new ContextTypeMatchClassLoader(beanFactory.getBeanClassLoader())); &#125;&#125; **\u0000invokeBeanFactoryPostProcessors()** 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171public static void invokeBeanFactoryPostProcessors( ConfigurableListableBeanFactory beanFactory, List&lt;BeanFactoryPostProcessor&gt; beanFactoryPostProcessors) &#123; //初始化一个set列表，存储已经执行过的beanName Set&lt;String&gt; processedBeans = new HashSet&lt;&gt;(); //类型断言：如果是BeanDefinitionRegistry类型的 //意思就是：当前 beanFactory 是 beanDefinition 的注册中心 ， bd 全部注册到 bf。 if (beanFactory instanceof BeanDefinitionRegistry) &#123; //类型转换 bf -&gt; bd注册中心 BeanDefinitionRegistry registry = (BeanDefinitionRegistry) beanFactory; //存储BeanFactoryPostProcessor List&lt;BeanFactoryPostProcessor&gt; regularPostProcessors = new ArrayList&lt;&gt;(); /*存储BeanDefinitionRegistryPostProcessor*/ List&lt;BeanDefinitionRegistryPostProcessor&gt; registryProcessors = new ArrayList&lt;&gt;(); /** * BeanFactoryPostProcessor 和 BeanDefinitionRegistryPostProcessor 之间的关系 ？ * BeanDefinitionRegistryPostProcessor 是 BeanFactoryPostProcessor 的子类。 * 它里面 搞了一个新的方法 postProcessBeanDefinitionRegistry ，可以往容器中注册更多的bd信息。 * 扩展点： * ①BeanFactoryPostProcessor 对bd信息进行修改 * ②postProcessBeanDefinitionRegistry 添加更多的bd信息 */ //处理ApplicationContext里面硬编码注册的beanFactoryPostProcessor for (BeanFactoryPostProcessor postProcessor : beanFactoryPostProcessors) &#123; /*如果当前后置处理器的类型是BeanDefinitionRegistryPostProcessor*/ if (postProcessor instanceof BeanDefinitionRegistryPostProcessor) &#123; /*将后置处理器转换为BeanDefinitionRegistryPostProcessor类型*/ BeanDefinitionRegistryPostProcessor registryProcessor = (BeanDefinitionRegistryPostProcessor) postProcessor; /*执行后置处理器的方法 这里又是一个后置处理器的调用点*/ registryProcessor.postProcessBeanDefinitionRegistry(registry); /*将执行完的后置处理器加入到集合中*/ registryProcessors.add(registryProcessor); &#125; else &#123;/*此时说明后置处理器是一个BeanFactoryPostProcessor，直接加入到BeanFactoryPostProcessor的集合中。后续统一执行。*/ regularPostProcessors.add(postProcessor); &#125; &#125; // 这里不要初始化FactoryBeans:我们需要保留所有常规bean的未初始化状态， // 让bean工厂的后处理器应用于它们！在实现优先级的bean definition registry后处理器、 // Ordered、Ordered等之间进行分离。 /*当前阶段的registry后置处理器集合*/ List&lt;BeanDefinitionRegistryPostProcessor&gt; currentRegistryProcessors = new ArrayList&lt;&gt;(); // 首先执行实现了主排序接口的后置处理器 PriorityOrdered String[] postProcessorNames = beanFactory.getBeanNamesForType(BeanDefinitionRegistryPostProcessor.class, true, false); for (String ppName : postProcessorNames) &#123; /*判断对应的bean是否实现了主排序接口，如果实现了，就让该后置处理器添加到集合。*/ if (beanFactory.isTypeMatch(ppName, PriorityOrdered.class)) &#123; currentRegistryProcessors.add(beanFactory.getBean(ppName, BeanDefinitionRegistryPostProcessor.class)); /*已经执行的后置处理器名字集合，因为接下来马上要执行这些后置处理器了。*/ processedBeans.add(ppName); &#125; &#125; /*对后置处理器进行排序 根据 getOrder() 返回的值进行升序排序*/ sortPostProcessors(currentRegistryProcessors, beanFactory); /*注册所有的后置处理器*/ registryProcessors.addAll(currentRegistryProcessors); /*调用当前后置处理器的相关接口方法 执行 BeanDefinitionRegistryPostProcessor 的 postProcessBeanDefinitionRegistry()*/ invokeBeanDefinitionRegistryPostProcessors(currentRegistryProcessors, registry, beanFactory.getApplicationStartup()); /*清空当前阶段临时的集合*/ currentRegistryProcessors.clear(); // 接着执行实现了Ordered接口的后置处理器 这里的后置处理器是普通排序后置处理器 postProcessorNames = beanFactory.getBeanNamesForType(BeanDefinitionRegistryPostProcessor.class, true, false); for (String ppName : postProcessorNames) &#123; /*确保每一个后置处理器只执行一次*/ if (!processedBeans.contains(ppName) &amp;&amp; beanFactory.isTypeMatch(ppName, Ordered.class)) &#123; currentRegistryProcessors.add(beanFactory.getBean(ppName, BeanDefinitionRegistryPostProcessor.class)); processedBeans.add(ppName); &#125; &#125; sortPostProcessors(currentRegistryProcessors, beanFactory); registryProcessors.addAll(currentRegistryProcessors); invokeBeanDefinitionRegistryPostProcessors(currentRegistryProcessors, registry, beanFactory.getApplicationStartup()); currentRegistryProcessors.clear(); // 最后将剩下的后置处理器逐个执行 /*这个变量控制是否需要循环*/ boolean reiterate = true; while (reiterate) &#123; reiterate = false; /*从bean工厂拿BeanDefinitionRegistryPostProcessor的后置处理器*/ postProcessorNames = beanFactory.getBeanNamesForType(BeanDefinitionRegistryPostProcessor.class, true, false); /*循环判断执行没执行过得*/ for (String ppName : postProcessorNames) &#123; if (!processedBeans.contains(ppName)) &#123; currentRegistryProcessors.add(beanFactory.getBean(ppName, BeanDefinitionRegistryPostProcessor.class)); processedBeans.add(ppName); /*这里为什么吧变量设置为true？因为新注册的后置处理器，可能也是往容器中注册的 registry 类型的后置处理器*/ reiterate = true; &#125; &#125; sortPostProcessors(currentRegistryProcessors, beanFactory); registryProcessors.addAll(currentRegistryProcessors); invokeBeanDefinitionRegistryPostProcessors(currentRegistryProcessors, registry, beanFactory.getApplicationStartup()); currentRegistryProcessors.clear(); &#125; /*这里是执行BeanDefinitionRegistryPostProcessor 的父类 BeanFactoryPostProcessor 的方法*/ // 上面重复三遍是为了执行 BeanDefinitionRegistryPostProcessor 的方法， // 但是，BeanDefinitionRegistryPostProcessor 还继承了 BeanFactoryPostProcessor // 这里是为了执行所有后置处理器的 // （BeanFactoryPostProcessor）的 postProcessBeanFactory方法。 invokeBeanFactoryPostProcessors(registryProcessors, beanFactory); /*这个方法就是执行BeanFactoryPostProcessor的方法*/ invokeBeanFactoryPostProcessors(regularPostProcessors, beanFactory); &#125; /*处理不是 BeanDefinitionRegistry 类型的后置处理器*/ else &#123; // 调用上下文实例注册的后置处理器 invokeBeanFactoryPostProcessors(beanFactoryPostProcessors, beanFactory); &#125; /** * 上面处理的是硬编码的BeanDefinitionRegistry和BeanFactoryPostProcessor的后置处理器。 * 下面处理器的是普通的后置处理器。 */ // 不要在这里初始化FactoryBeans:我们需要保留所有常规beans未初始化以让bean工厂后处理器应用于它们！ /*获取容器内注册的所有BeanFactoryPostProcessor集合。还是处理主排序，普通排序，为排序的后置处理器集合*/ String[] postProcessorNames = beanFactory.getBeanNamesForType(BeanFactoryPostProcessor.class, true, false); // 在实现优先级有序、有序和其他的BeanFactoryPostProcessors之间进行分离。 List&lt;BeanFactoryPostProcessor&gt; priorityOrderedPostProcessors = new ArrayList&lt;&gt;(); List&lt;String&gt; orderedPostProcessorNames = new ArrayList&lt;&gt;(); List&lt;String&gt; nonOrderedPostProcessorNames = new ArrayList&lt;&gt;(); for (String ppName : postProcessorNames) &#123; /*包含说明已经执行过了，直接啥也不做跳过即可*/ if (processedBeans.contains(ppName)) &#123; // skip - already processed in first phase above &#125; else if (beanFactory.isTypeMatch(ppName, PriorityOrdered.class)) &#123; priorityOrderedPostProcessors.add(beanFactory.getBean(ppName, BeanFactoryPostProcessor.class)); &#125; else if (beanFactory.isTypeMatch(ppName, Ordered.class)) &#123; orderedPostProcessorNames.add(ppName); &#125; else &#123; nonOrderedPostProcessorNames.add(ppName); &#125; &#125; // 首先，执行带有优先级接口的 sortPostProcessors(priorityOrderedPostProcessors, beanFactory); invokeBeanFactoryPostProcessors(priorityOrderedPostProcessors, beanFactory); // 接着执行实现了order接口的 List&lt;BeanFactoryPostProcessor&gt; orderedPostProcessors = new ArrayList&lt;&gt;(orderedPostProcessorNames.size()); for (String postProcessorName : orderedPostProcessorNames) &#123; orderedPostProcessors.add(beanFactory.getBean(postProcessorName, BeanFactoryPostProcessor.class)); &#125; sortPostProcessors(orderedPostProcessors, beanFactory); invokeBeanFactoryPostProcessors(orderedPostProcessors, beanFactory); // 最后执行其他剩余的 List&lt;BeanFactoryPostProcessor&gt; nonOrderedPostProcessors = new ArrayList&lt;&gt;(nonOrderedPostProcessorNames.size()); for (String postProcessorName : nonOrderedPostProcessorNames) &#123; nonOrderedPostProcessors.add(beanFactory.getBean(postProcessorName, BeanFactoryPostProcessor.class)); &#125; invokeBeanFactoryPostProcessors(nonOrderedPostProcessors, beanFactory); // 内存清理，帮助GC beanFactory.clearMetadataCache(); /*思考：为什么spring要把这里代码逻辑写的这么麻烦？为了框架的健壮性，提供更好的扩展性，方便在不同阶段，针对不同的需求进行扩展。*/&#125; 9.注册后置处理器获取所有的后置处理器，先注册实现主排序接口的，在注册实现Order接口的，最后注册剩下的。​ 注册一个**ApplicationListenerDetector**，在bean完成创建后检查是否是**ApplicationListener**，如果是，添加到容器。​ 12345678/** * 实例化并注册所有 BeanPostProcessor bean，如果给出，则遵守显式顺序。 * 必须在应用程序 bean 的任何实例化之前调用。 */protected void registerBeanPostProcessors(ConfigurableListableBeanFactory beanFactory) &#123; //注册所有的后置处理器 PostProcessorRegistrationDelegate.registerBeanPostProcessors(beanFactory, this);&#125; **\u0000registerBeanPostProcessors()** 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374public static void registerBeanPostProcessors( ConfigurableListableBeanFactory beanFactory, AbstractApplicationContext applicationContext) &#123; //获取到所有的BeanPostProcessor后置处理器的beanName数组 String[] postProcessorNames = beanFactory.getBeanNamesForType(BeanPostProcessor.class, true, false); // 注册bean后置处理器检查器，当bean在bean后置处理器实例化期间创建时， // 即当bean不适合被所有bean后置处理器处理时，它会记录一条信息。 /*后置处理器数量，计算方式beanFactory已经有的数量+1+后注册的。1？下面一行手动硬加的*/ int beanProcessorTargetCount = beanFactory.getBeanPostProcessorCount() + 1 + postProcessorNames.length; /*BeanPostProcessorChecker？ step into 检查创建bean实例的时候，后置处理器是否已经全部注册完毕，如果未完毕，日志提示。*/ beanFactory.addBeanPostProcessor(new BeanPostProcessorChecker(beanFactory, beanProcessorTargetCount)); // 分离 实现了主排序接口的后置处理器 List&lt;BeanPostProcessor&gt; priorityOrderedPostProcessors = new ArrayList&lt;&gt;(); /*存放mergedBeanDefinition后置处理器*/ List&lt;BeanPostProcessor&gt; internalPostProcessors = new ArrayList&lt;&gt;(); /*普通排序接口的*/ List&lt;String&gt; orderedPostProcessorNames = new ArrayList&lt;&gt;(); /*没有实现排序接口的*/ List&lt;String&gt; nonOrderedPostProcessorNames = new ArrayList&lt;&gt;(); /*循环将所有的后置处理器进行分离存放*/ for (String ppName : postProcessorNames) &#123; if (beanFactory.isTypeMatch(ppName, PriorityOrdered.class)) &#123; BeanPostProcessor pp = beanFactory.getBean(ppName, BeanPostProcessor.class); priorityOrderedPostProcessors.add(pp); if (pp instanceof MergedBeanDefinitionPostProcessor) &#123; internalPostProcessors.add(pp); &#125; &#125; else if (beanFactory.isTypeMatch(ppName, Ordered.class)) &#123; orderedPostProcessorNames.add(ppName); &#125; else &#123; nonOrderedPostProcessorNames.add(ppName); &#125; &#125; // 首先注册实现了优先级接口的后置处理器 sortPostProcessors(priorityOrderedPostProcessors, beanFactory); registerBeanPostProcessors(beanFactory, priorityOrderedPostProcessors); // 其次，注册实现了order接口的后置处理器 List&lt;BeanPostProcessor&gt; orderedPostProcessors = new ArrayList&lt;&gt;(orderedPostProcessorNames.size()); for (String ppName : orderedPostProcessorNames) &#123; BeanPostProcessor pp = beanFactory.getBean(ppName, BeanPostProcessor.class); orderedPostProcessors.add(pp); if (pp instanceof MergedBeanDefinitionPostProcessor) &#123; internalPostProcessors.add(pp); &#125; &#125; sortPostProcessors(orderedPostProcessors, beanFactory); registerBeanPostProcessors(beanFactory, orderedPostProcessors); // 最终，注册所有剩下的所有后置处理器 List&lt;BeanPostProcessor&gt; nonOrderedPostProcessors = new ArrayList&lt;&gt;(nonOrderedPostProcessorNames.size()); for (String ppName : nonOrderedPostProcessorNames) &#123; BeanPostProcessor pp = beanFactory.getBean(ppName, BeanPostProcessor.class); nonOrderedPostProcessors.add(pp); if (pp instanceof MergedBeanDefinitionPostProcessor) &#123; internalPostProcessors.add(pp); &#125; &#125; //注册后置处理器的逻辑 step into registerBeanPostProcessors(beanFactory, nonOrderedPostProcessors); // 最后，再次处理internalPostProcessors，确保存放mergedBeanDefinition后置处理器在链表的末尾 sortPostProcessors(internalPostProcessors, beanFactory); registerBeanPostProcessors(beanFactory, internalPostProcessors); //重新注册后处理器时，将内部beans检测为ApplicationListeners，将其移动到处理器链的末端(用于获取代理等)。 beanFactory.addBeanPostProcessor(new ApplicationListenerDetector(applicationContext));&#125; 10.initMessageSource()\u0000做一些国际化相关的操作。**initMessageSource()**​ 123456789101112131415161718192021222324252627protected void initMessageSource() &#123; ConfigurableListableBeanFactory beanFactory = getBeanFactory(); if (beanFactory.containsLocalBean(MESSAGE_SOURCE_BEAN_NAME)) &#123; this.messageSource = beanFactory.getBean(MESSAGE_SOURCE_BEAN_NAME, MessageSource.class); // Make MessageSource aware of parent MessageSource. if (this.parent != null &amp;&amp; this.messageSource instanceof HierarchicalMessageSource) &#123; HierarchicalMessageSource hms = (HierarchicalMessageSource) this.messageSource; if (hms.getParentMessageSource() == null) &#123; // Only set parent context as parent MessageSource if no parent MessageSource // registered already. hms.setParentMessageSource(getInternalParentMessageSource()); &#125; &#125; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Using MessageSource [&quot; + this.messageSource + &quot;]&quot;); &#125; &#125; else &#123; // Use empty MessageSource to be able to accept getMessage calls. DelegatingMessageSource dms = new DelegatingMessageSource(); dms.setParentMessageSource(getInternalParentMessageSource()); this.messageSource = dms; beanFactory.registerSingleton(MESSAGE_SOURCE_BEAN_NAME, this.messageSource); if (logger.isTraceEnabled()) &#123; logger.trace(&quot;No &#x27;&quot; + MESSAGE_SOURCE_BEAN_NAME + &quot;&#x27; bean, using [&quot; + this.messageSource + &quot;]&quot;); &#125; &#125;&#125; 11.初始化事件派发器**initApplicationEventMulticaster()**\u0000初始化事件派发器，获取bean工厂，从bean工厂获取事件派发器，如果没有，创建一个放到容器中。​ 1234567891011121314151617181920212223protected void initApplicationEventMulticaster() &#123; //获取bean工厂 ConfigurableListableBeanFactory beanFactory = getBeanFactory(); //判断容器是否有事件派发器 if (beanFactory.containsLocalBean(APPLICATION_EVENT_MULTICASTER_BEAN_NAME)) &#123; //如果有就获取到引用 （用户自定义了一个事件多播器） /*可以实现APPLICATION_EVENT_MULTICASTER接口来自己实现一个事件派发器,通过bean的方式传递给spring*/ this.applicationEventMulticaster = beanFactory.getBean(APPLICATION_EVENT_MULTICASTER_BEAN_NAME, ApplicationEventMulticaster.class); if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Using ApplicationEventMulticaster [&quot; + this.applicationEventMulticaster + &quot;]&quot;); &#125; &#125; else &#123;//走到这里的前提是容器中没有事件派发器,需要使用spring框架默认提供的事件派发器。 //直接造一个事件派发器 this.applicationEventMulticaster = new SimpleApplicationEventMulticaster(beanFactory); //将事件派发器注册到一级缓存，事件派发器是单例的全局通用的。 beanFactory.registerSingleton(APPLICATION_EVENT_MULTICASTER_BEAN_NAME, this.applicationEventMulticaster); if (logger.isTraceEnabled()) &#123; logger.trace(&quot;No &#x27;&quot; + APPLICATION_EVENT_MULTICASTER_BEAN_NAME + &quot;&#x27; bean, using &quot; + &quot;[&quot; + this.applicationEventMulticaster.getClass().getSimpleName() + &quot;]&quot;); &#125; &#125;&#125; 12.onRefresh()\u0000留给子容器，子容器可以在容器刷新的时候加入自己的逻辑。**onRefresh()**​ 123protected void onRefresh() throws BeansException &#123; // For subclasses: do nothing by default.&#125; 13.注册监听器为事件派发器注册事件监听器，派发一些早期事件。**registerListeners()**​ 123456789101112131415161718192021protected void registerListeners() &#123; // 将所有的事件监听器（硬编码的）注册到事件派发器 for (ApplicationListener&lt;?&gt; listener : getApplicationListeners()) &#123; getApplicationEventMulticaster().addApplicationListener(listener); &#125; // 注册通过bean配置提供的监听器 ，用户可以通过 bd 的方式 提供listener String[] listenerBeanNames = getBeanNamesForType(ApplicationListener.class, true, false); for (String listenerBeanName : listenerBeanNames) &#123; getApplicationEventMulticaster().addApplicationListenerBean(listenerBeanName); &#125; // 通过事件派发器派发容器早期应用程序事件 这里可以通过重写onRefresh方法直接往earlyApplicationEvents的set集合里面加。 Set&lt;ApplicationEvent&gt; earlyEventsToProcess = this.earlyApplicationEvents; this.earlyApplicationEvents = null; if (!CollectionUtils.isEmpty(earlyEventsToProcess)) &#123; for (ApplicationEvent earlyEvent : earlyEventsToProcess) &#123; getApplicationEventMulticaster().multicastEvent(earlyEvent); &#125; &#125;&#125; \u0000关注一下：**ApplicationEventMulticaster.multicastEvent(earlyEvent)** 派发事件。​ 123456789101112131415161718192021@Overridepublic void multicastEvent(ApplicationEvent event) &#123; multicastEvent(event, resolveDefaultEventType(event));&#125;@Overridepublic void multicastEvent(final ApplicationEvent event, @Nullable ResolvableType eventType) &#123; /*事件类型为空就是用默认的事件类型解析器*/ ResolvableType type = (eventType != null ? eventType : resolveDefaultEventType(event)); /*获取事件多波器的线程池*/ Executor executor = getTaskExecutor(); /*拿到当前事件的所有监听器，伦轮训所有的监听器，对事件进行派发。*/ for (ApplicationListener&lt;?&gt; listener : getApplicationListeners(event, type)) &#123; if (executor != null) &#123; executor.execute(() -&gt; invokeListener(listener, event)); &#125; else &#123; invokeListener(listener, event); &#125; &#125;&#125; ​ **\u0000invokeListener()**​ 12345678910111213141516protected void invokeListener(ApplicationListener&lt;?&gt; listener, ApplicationEvent event) &#123; /*获取到错误处理器的引用*/ ErrorHandler errorHandler = getErrorHandler(); if (errorHandler != null) &#123; try &#123; /*真正执行派发事件的逻辑*/ doInvokeListener(listener, event); &#125; catch (Throwable err) &#123; errorHandler.handleError(err); &#125; &#125; else &#123; doInvokeListener(listener, event); &#125;&#125; **doInvokeListener()**​ 1234567891011121314151617181920212223242526private void doInvokeListener(ApplicationListener listener, ApplicationEvent event) &#123; try &#123; /*监听器发布事件*/ listener.onApplicationEvent(event); &#125; catch (ClassCastException ex) &#123; String msg = ex.getMessage(); if (msg == null || matchesClassCastMessage(msg, event.getClass()) || (event instanceof PayloadApplicationEvent &amp;&amp; matchesClassCastMessage(msg, ((PayloadApplicationEvent) event).getPayload().getClass()))) &#123; // Possibly a lambda-defined listener which we could not resolve the generic event type for // -&gt; let&#x27;s suppress the exception. Log loggerToUse = this.lazyLogger; if (loggerToUse == null) &#123; loggerToUse = LogFactory.getLog(getClass()); this.lazyLogger = loggerToUse; &#125; if (loggerToUse.isTraceEnabled()) &#123; loggerToUse.trace(&quot;Non-matching event type for listener: &quot; + listener, ex); &#125; &#125; else &#123; throw ex; &#125; &#125;&#125; 13.1 ApplicationListenerApplicationListener &amp;&amp; ApplicationEvent通过自定义不同类型的事件，使用不同的监听器监听不同类型的事件，做到jvm进程内的消息队列，事件驱动，解耦。​ 1234567891011121314151617public class MyEvent extends ApplicationEvent &#123; String message; public MyEvent(Object source) &#123; super(source); &#125; public MyEvent(Object source,String message) &#123; super(source); this.message=message; &#125; public void print()&#123; System.out.println(&quot;发布了一个事件：&quot;+message); &#125;&#125; 12345678public class MyListener implements ApplicationListener&lt;MyEvent&gt; &#123; @Override public void onApplicationEvent(MyEvent event) &#123; event.print(); &#125;&#125; 1234567/** * 测试spring 的 ioc 容器的事件发布 */private static void testPublishEvent() &#123; ApplicationContext ioc = new ClassPathXmlApplicationContext(CONFIG_LOCATION); ioc.publishEvent(new MyEvent(&quot;&quot;, &quot;这是我自定义的一个事件&quot;));&#125; @EventListener对上面写法的一个优化，更加简洁，开发量更少，懒人必备神器。​ 12345678private static void testEventListener() &#123; ioc.publishEvent(new ApplicationEvent(&quot;hello，spring&quot;) &#123; @Override public Object getSource() &#123; return super.getSource(); &#125; &#125;);&#125; 12345678@Componentpublic class MyEventListener &#123; @EventListener(classes = ApplicationEvent.class) public void listener(ApplicationEvent event)&#123; System.out.println(&quot;event = &quot; + event); &#125;&#125; 14.初始化剩下所有的单实例bean**finishBeanFactoryInitialization(beanFactory)**\u0000 12345678910111213141516171819202122232425262728protected void finishBeanFactoryInitialization(ConfigurableListableBeanFactory beanFactory) &#123; // 为此上下文初始化转换服务 if (beanFactory.containsBean(CONVERSION_SERVICE_BEAN_NAME) &amp;&amp; beanFactory.isTypeMatch(CONVERSION_SERVICE_BEAN_NAME, ConversionService.class)) &#123; beanFactory.setConversionService( beanFactory.getBean(CONVERSION_SERVICE_BEAN_NAME, ConversionService.class)); &#125; // 如果容器里面没有字符串转换器，初始化一个字符串转换器放到容器中。 if (!beanFactory.hasEmbeddedValueResolver()) &#123; beanFactory.addEmbeddedValueResolver(strVal -&gt; getEnvironment().resolvePlaceholders(strVal)); &#125; // 尽早初始化LoadTimeWeaverAware beans，以便尽早注册它们的转换器。 String[] weaverAwareNames = beanFactory.getBeanNamesForType(LoadTimeWeaverAware.class, false, false); for (String weaverAwareName : weaverAwareNames) &#123; getBean(weaverAwareName); &#125; // 停止使用临时类加载器进行类型匹配 beanFactory.setTempClassLoader(null); // 允许缓存所有bean定义元数据，不期望进一步的更改，冻结bd信息，冻结之后就无法往bf注册bd了 beanFactory.freezeConfiguration(); // 实例化所有剩余的单实例bean beanFactory.preInstantiateSingletons();&#125; 如何冻结bd信息的？**beanFactory.freezeConfiguration()**​ 123456@Overridepublic void freezeConfiguration() &#123; /*这个字段如果是true，name就不能往容器中添加新的beanDefinition了*/ this.configurationFrozen = true; this.frozenBeanDefinitionNames = StringUtils.toStringArray(this.beanDefinitionNames);&#125; **beanFactory.preInstantiateSingletons()**​ 这个方法内容过多且偏核心内容，留在下一篇分析，跳过这里，加载完所有的单实例bean之后，看还要做什么操作。​ 15.finishRefresh()初始化容器的生命周期事件处理器，回调**onRefresh()**,并发布容器的生命周期事件。**finishRefresh() **\u0000 1234567891011121314151617181920212223protected void finishRefresh() &#123; // 清除上下文级资源缓存(例如来自扫描的ASM元数据)。 clearResourceCaches(); // 为此上下文初始化生命周期处理器。 /*案例演示：lifecycle package * 两者的区别： * smart：正常情况下就会执行 * 普通的：必须显示调用容器.start()/.stop() * */ initLifecycleProcessor(); // 首先将刷新传播到生命周期处理器。回调 onRefresh getLifecycleProcessor().onRefresh(); // 发布容器创建完成事件 publishEvent(new ContextRefreshedEvent(this)); // 注册容器上下文 if (!NativeDetector.inNativeImage()) &#123; LiveBeansView.registerApplicationContext(this); &#125;&#125; 15.1 为此上下文初始化生命周期事件处理器**initLifecycleProcessor()**\u0000 1234567891011121314151617181920212223protected void initLifecycleProcessor() &#123; /*获取bean工厂*/ ConfigurableListableBeanFactory beanFactory = getBeanFactory(); /*判断工厂是否已经有生命周期处理器，有的话直接获取引用*/ if (beanFactory.containsLocalBean(LIFECYCLE_PROCESSOR_BEAN_NAME)) &#123; this.lifecycleProcessor = beanFactory.getBean(LIFECYCLE_PROCESSOR_BEAN_NAME, LifecycleProcessor.class); if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Using LifecycleProcessor [&quot; + this.lifecycleProcessor + &quot;]&quot;); &#125; &#125; else &#123; /*此时就是工厂目前还没有的逻辑，那么就需要自动创建一个*/ DefaultLifecycleProcessor defaultProcessor = new DefaultLifecycleProcessor(); defaultProcessor.setBeanFactory(beanFactory); this.lifecycleProcessor = defaultProcessor; /*注册到一级缓存*/ beanFactory.registerSingleton(LIFECYCLE_PROCESSOR_BEAN_NAME, this.lifecycleProcessor); if (logger.isTraceEnabled()) &#123; logger.trace(&quot;No &#x27;&quot; + LIFECYCLE_PROCESSOR_BEAN_NAME + &quot;&#x27; bean, using &quot; + &quot;[&quot; + this.lifecycleProcessor.getClass().getSimpleName() + &quot;]&quot;); &#125; &#125;&#125; 15.2 生命周期事件处理器**Lifecycle** &amp;&amp; **SmartLifecycle**​ 容器创建完成之后的回调，传递的参数_**autoStartUpOnly**_是干嘛的？​ 表示只启动_**SmartLifeCycle**_生命周期对象，并且启动的对象_**autoStartUpOnly**_必须是true，不会启动普通的生命周期对象，false的时候，会启动全部的生命周期对象。​ 1234567891011121314151617181920public class DemoLifeCycle implements Lifecycle &#123; private boolean running =false; @Override public void start() &#123; this.running=true; System.out.println(&quot;demo one start!&quot;); &#125; @Override public void stop() &#123; this.running=false; System.out.println(&quot;demo one stop!&quot;); &#125; @Override public boolean isRunning() &#123; return running; &#125;&#125; 1234567891011121314151617181920public class DemoSmartLifeCycle implements SmartLifecycle &#123; private boolean running = false; @Override public void start() &#123; this.running=true; System.out.println(&quot;demo two start!&quot;); &#125; @Override public void stop() &#123; this.running=false; System.out.println(&quot;demo two stop!&quot;); &#125; @Override public boolean isRunning() &#123; return running; &#125;&#125; 15.3 回调onRefresh()**onRefresh()** 123456789@Overridepublic void onRefresh() &#123; /*传递的参数autoStartUpOnly是干嘛的？ * 表示只启动SmartLifeCycle生命周期对象，并且启动的对象autoStartUpOnly必须是true， * 不会启动普通的生命周期对象， * false的时候，会启动全部的生命周期对象。*/ startBeans(true); this.running = true;&#125; **startBeans(true)**​ 12345678910111213141516171819202122232425private void startBeans(boolean autoStartupOnly) &#123; /*获取到所有实现了生命周期接口的对象，包装到map内 k：beanName v: 生命周期对象*/ Map&lt;String, Lifecycle&gt; lifecycleBeans = getLifecycleBeans(); /*因为生命周期对象可能依赖其他生命周期对象的执行结果，所以需要执行顺序，依靠 smart生命周期接口实现的另一个接口 ，方法返回值越低，优先级越高 */ Map&lt;Integer, LifecycleGroup&gt; phases = new TreeMap&lt;&gt;(); /*遍历判断满足条件加入到集合 * 条件1：SmartLifecycle * 条件2：自启动 * */ lifecycleBeans.forEach((beanName, bean) -&gt; &#123; if (!autoStartupOnly || (bean instanceof SmartLifecycle &amp;&amp; ((SmartLifecycle) bean).isAutoStartup())) &#123; /*排序值*/ int phase = getPhase(bean); phases.computeIfAbsent( phase, p -&gt; new LifecycleGroup(phase, this.timeoutPerShutdownPhase, lifecycleBeans, autoStartupOnly) ).add(beanName, bean); &#125; &#125;); /*轮训启动所有的生命周期处理器*/ if (!phases.isEmpty()) &#123; phases.values().forEach(LifecycleGroup::start); &#125;&#125; **start()** 123456789101112131415public void start() &#123; /*members？看上面的add方法，相当于判断生命周期事件处理器是不是空，如果是空的话就没必要向下执行了。*/ if (this.members.isEmpty()) &#123; return; &#125; if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Starting beans in phase &quot; + this.phase); &#125; /*对生命周期事件处理器进行一个排序*/ Collections.sort(this.members); for (LifecycleGroupMember member : this.members) &#123; /*真正执行启动的逻辑*/ doStart(this.lifecycleBeans, member.name, this.autoStartupOnly); &#125;&#125; **doStart()** 123456789101112131415161718192021222324252627282930313233343536/** * 将指定的 bean 作为给定的 Lifecycle bean 集的一部分启动，确保首先启动它所依赖的任何 bean。 * @param lifecycleBeans a Map with bean name as key and Lifecycle instance as value * @param beanName the name of the bean to start */private void doStart(Map&lt;String, ? extends Lifecycle&gt; lifecycleBeans, String beanName, boolean autoStartupOnly) &#123; /*确保每一个生命周期处理器智只能被启动一次，在一个分组内被启动，其他分组内就看不到这个生命周期处理器*/ Lifecycle bean = lifecycleBeans.remove(beanName); if (bean != null &amp;&amp; bean != this) &#123; /*获取当前即将要被启动的生命周期处理器锁依赖的其他beanName*/ String[] dependenciesForBean = getBeanFactory().getDependenciesForBean(beanName); /*先启动当前生命周期处理器所依赖的其他lifeCycle*/ for (String dependency : dependenciesForBean) &#123; doStart(lifecycleBeans, dependency, autoStartupOnly); &#125; /* * 条件成立：执行start方法 * */ if (!bean.isRunning() &amp;&amp; (!autoStartupOnly || !(bean instanceof SmartLifecycle) || ((SmartLifecycle) bean).isAutoStartup())) &#123; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Starting bean &#x27;&quot; + beanName + &quot;&#x27; of type [&quot; + bean.getClass().getName() + &quot;]&quot;); &#125; try &#123; /*启动当前lifeCycle*/ bean.start(); &#125; catch (Throwable ex) &#123; throw new ApplicationContextException(&quot;Failed to start bean &#x27;&quot; + beanName + &quot;&#x27;&quot;, ex); &#125; if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Successfully started bean &#x27;&quot; + beanName + &quot;&#x27;&quot;); &#125; &#125; &#125;&#125; 看完了启动，再看一下停止。​ **stop()** 12345678910111213/** * 停止所有注册的bean实现Lifecycle和当前正在运行。 * 任何实现SmartLifecycle bean 都将在其“阶段”内停止， * 并且所有阶段都将从最高值到最低值排序。 * 所有未实现SmartLifecycle将在默认阶段 0 中停止。 * 声明为依赖另一个 bean 的 bean 将在依赖 bean 之前停止，无论声明的阶段如何。 */@Overridepublic void stop() &#123; /*停止所有的bean*/ stopBeans(); this.running = false;&#125; **stopBeans()** 1234567891011121314151617181920212223private void stopBeans() &#123; /*这里其实和启动的时候的逻辑是一样的。*/ Map&lt;String, Lifecycle&gt; lifecycleBeans = getLifecycleBeans(); Map&lt;Integer, LifecycleGroup&gt; phases = new HashMap&lt;&gt;(); lifecycleBeans.forEach((beanName, bean) -&gt; &#123; int shutdownPhase = getPhase(bean); LifecycleGroup group = phases.get(shutdownPhase); if (group == null) &#123; group = new LifecycleGroup(shutdownPhase, this.timeoutPerShutdownPhase, lifecycleBeans, false); phases.put(shutdownPhase, group); &#125; group.add(beanName, bean); &#125;); if (!phases.isEmpty()) &#123; List&lt;Integer&gt; keys = new ArrayList&lt;&gt;(phases.keySet()); /*注意这里是反向排序，启动的时候 1 2 3 4 5 关闭的时候就是 5 4 3 2 1*/ keys.sort(Collections.reverseOrder()); for (Integer key : keys) &#123; /*真正的核心逻辑在这里*/ phases.get(key).stop(); &#125; &#125;&#125; **stop()** 123456789101112131415161718192021222324252627282930313233343536373839404142 public void stop() &#123; /*如果为空，说明没有必要往下走了，直接返回。*/ if (this.members.isEmpty()) &#123; return; &#125; if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Stopping beans in phase &quot; + this.phase); &#125; /*反向排序*/ this.members.sort(Collections.reverseOrder()); /*并发关闭smartLifeCycle*/ CountDownLatch latch = new CountDownLatch(this.smartMemberCount); /*保存当前正处于关闭中的smartLifeCycle*/ Set&lt;String&gt; countDownBeanNames = Collections.synchronizedSet(new LinkedHashSet&lt;&gt;()); /*all 处理器*/ Set&lt;String&gt; lifecycleBeanNames = new HashSet&lt;&gt;(this.lifecycleBeans.keySet()); /*处理本分组内需要关闭的生命周期处理器*/ for (LifecycleGroupMember member : this.members) &#123; if (lifecycleBeanNames.contains(member.name)) &#123; /*真正执行关闭的逻辑*/ doStop(this.lifecycleBeans, member.name, latch, countDownBeanNames); &#125; else if (member.bean instanceof SmartLifecycle) &#123; // Already removed: must have been a dependent bean from another phase latch.countDown(); &#125; &#125; try &#123; /*关闭主线程会在这里等待所有异步关闭的线程关闭完*/ latch.await(this.timeout, TimeUnit.MILLISECONDS); if (latch.getCount() &gt; 0 &amp;&amp; !countDownBeanNames.isEmpty() &amp;&amp; logger.isInfoEnabled()) &#123; logger.info(&quot;Failed to shut down &quot; + countDownBeanNames.size() + &quot; bean&quot; + (countDownBeanNames.size() &gt; 1 ? &quot;s&quot; : &quot;&quot;) + &quot; with phase value &quot; + this.phase + &quot; within timeout of &quot; + this.timeout + &quot;ms: &quot; + countDownBeanNames); &#125; &#125; catch (InterruptedException ex) &#123; Thread.currentThread().interrupt(); &#125; &#125;&#125; **doStop()** 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354private void doStop(Map&lt;String, ? extends Lifecycle&gt; lifecycleBeans, final String beanName, final CountDownLatch latch, final Set&lt;String&gt; countDownBeanNames) &#123; /*吧要关闭的处理器从全局的集合移除掉并返回*/ Lifecycle bean = lifecycleBeans.remove(beanName); if (bean != null) &#123; /*获取依赖当前bean的处理器*/ String[] dependentBeans = getBeanFactory().getDependentBeans(beanName); /*递归关闭依赖的bean*/ for (String dependentBean : dependentBeans) &#123; doStop(lifecycleBeans, dependentBean, latch, countDownBeanNames); &#125; try &#123; if (bean.isRunning()) &#123; if (bean instanceof SmartLifecycle) &#123; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Asking bean &#x27;&quot; + beanName + &quot;&#x27; of type [&quot; + bean.getClass().getName() + &quot;] to stop&quot;); &#125; /*将当前的处理器名添加到countDownBeanNames内，这个集合表示正在关闭的smart bean*/ countDownBeanNames.add(beanName); /*执行smart bean的关闭逻辑，看这个stop可以异步关闭，回调，999*/ ((SmartLifecycle) bean).stop(() -&gt; &#123; latch.countDown(); countDownBeanNames.remove(beanName); if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Bean &#x27;&quot; + beanName + &quot;&#x27; completed its stop procedure&quot;); &#125; &#125;); &#125; else &#123; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Stopping bean &#x27;&quot; + beanName + &quot;&#x27; of type [&quot; + bean.getClass().getName() + &quot;]&quot;); &#125; /*普通生命周期处理器的关闭逻辑*/ bean.stop(); if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Successfully stopped bean &#x27;&quot; + beanName + &quot;&#x27;&quot;); &#125; &#125; &#125; /*如果是已经关闭的smart bean，直接跳过了，相当于...*/ else if (bean instanceof SmartLifecycle) &#123; // Don&#x27;t wait for beans that aren&#x27;t running... latch.countDown(); &#125; &#125; catch (Throwable ex) &#123; if (logger.isWarnEnabled()) &#123; logger.warn(&quot;Failed to stop bean &#x27;&quot; + beanName + &quot;&#x27;&quot;, ex); &#125; &#125; &#125;&#125; 15.4 发布容器创建完成事件**publishEvent(new ContextRefreshedEvent(this))**\u0000 123456789101112131415161718192021222324252627282930protected void publishEvent(Object event, @Nullable ResolvableType eventType) &#123; Assert.notNull(event, &quot;Event must not be null&quot;); // Decorate event as an ApplicationEvent if necessary ApplicationEvent applicationEvent; if (event instanceof ApplicationEvent) &#123; applicationEvent = (ApplicationEvent) event; &#125; else &#123; applicationEvent = new PayloadApplicationEvent&lt;&gt;(this, event); if (eventType == null) &#123; eventType = ((PayloadApplicationEvent&lt;?&gt;) applicationEvent).getResolvableType(); &#125; &#125; // Multicast right now if possible - or lazily once the multicaster is initialized if (this.earlyApplicationEvents != null) &#123; this.earlyApplicationEvents.add(applicationEvent); &#125; else &#123; getApplicationEventMulticaster().multicastEvent(applicationEvent, eventType); &#125; // Publish event via parent context as well... if (this.parent != null) &#123; if (this.parent instanceof AbstractApplicationContext) &#123; ((AbstractApplicationContext) this.parent).publishEvent(event, eventType); &#125; else &#123; this.parent.publishEvent(event); &#125; &#125;&#125; 15.5 注册容器上下文**LiveBeansView.**_**registerApplicationContext**_**(this)**​ 12345678910111213141516171819static void registerApplicationContext(ConfigurableApplicationContext applicationContext) &#123; String mbeanDomain = applicationContext.getEnvironment().getProperty(MBEAN_DOMAIN_PROPERTY_NAME); if (mbeanDomain != null) &#123; synchronized (applicationContexts) &#123; if (applicationContexts.isEmpty()) &#123; try &#123; MBeanServer server = ManagementFactory.getPlatformMBeanServer(); applicationName = applicationContext.getApplicationName(); server.registerMBean(new LiveBeansView(), new ObjectName(mbeanDomain, MBEAN_APPLICATION_KEY, applicationName)); &#125; catch (Throwable ex) &#123; throw new ApplicationContextException(&quot;Failed to register LiveBeansView MBean&quot;, ex); &#125; &#125; applicationContexts.add(applicationContext); &#125; &#125;&#125; \u0000至此，整个**refresh()**的主流程已经完成。对于初始化所有的单实例bean，我将会在下一篇尽量做一个完整的分析。此外针对AOP，事务，三级缓存与循环依赖，加载spring mvc的组件，web环境下一个请求的执行流程，我都会在后续逐行分析源代码，做一些归纳总结。Spring发展至今，已经不单单是一个Java的框架，更是一个生态。","categories":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/tags/Spring/"}]},{"title":"Spring[一]初始化WEB组件","slug":"Spring/Spring[一]初始化Web三大组件","date":"2022-01-11T05:58:37.923Z","updated":"2022-01-11T06:05:07.133Z","comments":true,"path":"2022/01/11/Spring/Spring[一]初始化Web三大组件/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/Spring/Spring[%E4%B8%80]%E5%88%9D%E5%A7%8B%E5%8C%96Web%E4%B8%89%E5%A4%A7%E7%BB%84%E4%BB%B6/","excerpt":"","text":"SpringMVC最后是通过Tomcat来进行部署的。当在Servlet中进行进行应用部署时，主要步骤为:When a web application is deployed into a container, the following steps must be performed, in this order, before the web application begins processing client requests. Instantiate an instance of each event listener identified by a element in the deployment descriptor. For instantiated listener instances that implement ServletContextListener , call the contextInitialized() method. Instantiate an instance of each filter identified by a element in the deployment descriptor and call each filter instance’s init() method. Instantiate an instance of each servlet identified by a element that includes a element in the order defined by the load-on-startup element values, and call each servlet instance’s init() method. 当应用部署到容器时，在应用相应客户的请求之前，需要执行以下步骤： 创建并初始化由元素标记的事件监听器。 对于事件监听器，如果实现了ServletContextListener接口，那么调用其contextInitialized()方法。 创建和初始化由元素标记的过滤器，并调用其init()方法。 根据中定义的顺序创建和初始化由元素标记的servlet，并调用其init()方法。所以在Tomcat下部署的应用，会先初始化listener，然后初始化filter，最后初始化servlet。 现在根据配置文件和Oracle所说的初始化流程分析一下，SpringMVC是如何来一步步启动容器，并加载相关信息的。 配置文件12345678910111213141516171819202122232425262728293031323334353637383940414243444546&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;web-app xmlns=&quot;http://xmlns.jcp.org/xml/ns/javaee&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_3_1.xsd&quot; version=&quot;3.1&quot;&gt; &lt;!--告诉加载器，去这个位置去加载spring的相关配置--&gt; &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:spring-mvc.xml&lt;/param-value&gt; &lt;/context-param&gt; &lt;!--配置前端控制器--&gt; &lt;servlet&gt; &lt;servlet-name&gt;springMvc&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;!--SpringMVC配置文件--&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:spring-mvc.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;0&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;springMvc&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt; &lt;/listener&gt; &lt;!--解决乱码问题的filter--&gt; &lt;filter&gt; &lt;filter-name&gt;CharacterEncodingFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;utf-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;CharacterEncodingFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt;&lt;/web-app&gt; 1.初始化Listener我们在配置文件定义的Listener类是ContextLoaderListener. 继承关系：ContextLoaderListener 类继承了 ContextLoader 类并且实现了 ServletContextListener 接口，按照启动程序，会调用其 contextInitialized() 方法。 123456789/** * 这个方法：初始化应用上下文 * @param event */@Overridepublic void contextInitialized(ServletContextEvent event) &#123; //这里就进入了初始化web容器的核心 initWebApplicationContext(event.getServletContext());&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475/** * 初始化web应用的上下文 * ServletContext官方叫servlet上下文。服务器会为每一个工程创建一个对象，这个对象就是ServletContext对象。 * 这个对象全局唯一，而且工程内部的所有servlet都共享这个对象。所以叫全局应用程序共享对象。 * @param servletContext servlet 上下文 * @return */public WebApplicationContext initWebApplicationContext(ServletContext servletContext) &#123; /* * 首先通过 WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE 这个String类型的静态变量获取一个Root ioc 容器， * 根据 ioc 容器作为全局变量存储在 application 对象中，如果存在则有且只能有一个 * * 如果在初始化 Root WebApplicationContext 即， Root ioc 容器的时候发现已经存在，则直接抛出异常， * 因此web.xml中只允许存在一个ContextLoader类或其子类对象。 * */ if (servletContext.getAttribute(WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE) != null) &#123; throw new IllegalStateException( &quot;Cannot initialize context because there is already a root application context present - &quot; + &quot;check whether you have multiple ContextLoader* definitions in your web.xml!&quot;); &#125; servletContext.log(&quot;Initializing Spring root WebApplicationContext&quot;); Log logger = LogFactory.getLog(ContextLoader.class); if (logger.isInfoEnabled()) &#123; logger.info(&quot;Root WebApplicationContext: initialization started&quot;); &#125; long startTime = System.currentTimeMillis(); try &#123; /*如果context不存在，则直接进行创建*/ if (this.context == null) &#123; /*step into*/ this.context = createWebApplicationContext(servletContext); &#125; if (this.context instanceof ConfigurableWebApplicationContext) &#123; ConfigurableWebApplicationContext cwac = (ConfigurableWebApplicationContext) this.context; if (!cwac.isActive()) &#123; // The context has not yet been refreshed -&gt; provide services such as // setting the parent context, setting the application context id, etc if (cwac.getParent() == null) &#123; // The context instance was injected without an explicit parent -&gt; // determine parent for root web application context, if any. ApplicationContext parent = loadParentContext(servletContext); cwac.setParent(parent); &#125; /*配置并刷新应用的root ioc 容器，这里会进行bean的创建和初始化工作， * 这里最终会调用 AbstractApplicationContext的refresh方法。 * 并且ioc容器中的bean类会被放在application中。*/ configureAndRefreshWebApplicationContext(cwac, servletContext); &#125; &#125; //以属性的配置方式将application配置servletContext中，因为servletContext是整个应用唯一的，所以可以根据key值获取到application，从而能够获取到应用的所有信息 servletContext.setAttribute(WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE, this.context); ClassLoader ccl = Thread.currentThread().getContextClassLoader(); if (ccl == ContextLoader.class.getClassLoader()) &#123; currentContext = this.context; &#125; else if (ccl != null) &#123; currentContextPerThread.put(ccl, this.context); &#125; if (logger.isInfoEnabled()) &#123; long elapsedTime = System.currentTimeMillis() - startTime; logger.info(&quot;Root WebApplicationContext initialized in &quot; + elapsedTime + &quot; ms&quot;); &#125; return this.context; &#125; catch (RuntimeException | Error ex) &#123; logger.error(&quot;Context initialization failed&quot;, ex); servletContext.setAttribute(WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE, ex); throw ex; &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435protected void configureAndRefreshWebApplicationContext(ConfigurableWebApplicationContext wac, ServletContext sc) &#123; if (ObjectUtils.identityToString(wac).equals(wac.getId())) &#123; // The application context id is still set to its original default value // -&gt; assign a more useful id based on available information String idParam = sc.getInitParameter(CONTEXT_ID_PARAM); if (idParam != null) &#123; wac.setId(idParam); &#125; else &#123; // Generate default id... wac.setId(ConfigurableWebApplicationContext.APPLICATION_CONTEXT_ID_PREFIX + ObjectUtils.getDisplayString(sc.getContextPath())); &#125; &#125; //将ServletContext设置到application的属性中 wac.setServletContext(sc); //获取web.xml中配置的contextConfigLocation参数值 String configLocationParam = sc.getInitParameter(CONFIG_LOCATION_PARAM); if (configLocationParam != null) &#123; wac.setConfigLocation(configLocationParam); &#125; // The wac environment&#x27;s #initPropertySources will be called in any case when the context // is refreshed; do it eagerly here to ensure servlet property sources are in place for // use in any post-processing or initialization that occurs below prior to #refresh ConfigurableEnvironment env = wac.getEnvironment(); if (env instanceof ConfigurableWebEnvironment) &#123; ((ConfigurableWebEnvironment) env).initPropertySources(sc, null); &#125; customizeContext(sc, wac); //调用应用的refresh方法，进行IOC容器的装载 wac.refresh();&#125; 思考与沉淀我们在web.xml配置文件定义的Listener类是ContextLoaderListener类.继承关系：ContextLoaderListener 类继承了 ContextLoader 类并且实现了 ServletContextListener 接口，按照启动程序，会调用其 contextInitialized() 方法。这个方法主要是调用了initWebApplicationContext()来初始化web应用的上下文，再通过configureAndRefreshWebApplicationContext()加载web.xml里面指定的配置文件，然后调用ioc容器的刷新方法。 2.初始化Filter在完成了对于 listener 的初始化操作以后，会进行 filter 的创建和初始化操作。我们这里使用的是 CharacterEncodingFilter 。我们先看一下这个类的具体类图信息 ctrl+H。 因为其实现了 Filter 接口，所以会调用其对应的 init(FilterConfig filterConfig) 方法。在其父类 GenericFilterBean 中有该方法的实现。 123456789101112131415161718192021222324252627282930313233343536373839404142/** * filter的初始化方法 * @param filterConfig * @throws ServletException */@Overridepublic final void init(FilterConfig filterConfig) throws ServletException &#123; Assert.notNull(filterConfig, &quot;FilterConfig must not be null&quot;); this.filterConfig = filterConfig; // 将设置的初始化参数信息设置到pvs中 PropertyValues pvs = new FilterConfigPropertyValues(filterConfig, this.requiredProperties); if (!pvs.isEmpty()) &#123; try &#123; //将具体的filter类进行包装 BeanWrapper bw = PropertyAccessorFactory.forBeanPropertyAccess(this); //创建对应的资源加载器 ResourceLoader resourceLoader = new ServletContextResourceLoader(filterConfig.getServletContext()); Environment env = this.environment; if (env == null) &#123; env = new StandardServletEnvironment(); &#125; bw.registerCustomEditor(Resource.class, new ResourceEditor(resourceLoader, env)); initBeanWrapper(bw); bw.setPropertyValues(pvs, true); &#125; catch (BeansException ex) &#123; String msg = &quot;Failed to set bean properties on filter &#x27;&quot; + filterConfig.getFilterName() + &quot;&#x27;: &quot; + ex.getMessage(); logger.error(msg, ex); throw new NestedServletException(msg, ex); &#125; &#125; // 交给子类实现 initFilterBean(); if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Filter &#x27;&quot; + filterConfig.getFilterName() + &quot;&#x27; configured for use&quot;); &#125;&#125; 123protected void initFilterBean() throws ServletException &#123; //这里并未进行任何的初始化操作。其实Filter的主要作用还是在有请求过来时，进行的 doFilter() 中的处理，在启动阶段，处理比较少。&#125; 思考与沉淀在完成了对于 listener 的初始化操作以后，会进行 filter 的创建和初始化操作。通常在web.xml文件配置的是：CharacterEncodingFilter 。通过这个类的继承关系，再结合实际源码分析：因为其实现了 Filter 接口，所以会调用其对应的 init(FilterConfig filterConfig) 方法。在其父类 GenericFilterBean 中有该方法的实现。然后又交给了子类，但是实际上啥也没做，其实Filter的主要作用还是在有请求过来时，进行的 doFilter() 中的处理，在启动阶段，处理比较少。 3.初始化Servletweb应用启动的最后一个步骤就是创建和初始化 Servlet ，我们就从我们使用的 DispatcherServlet 这个类来进行分析，这个类是前端控制器，主要用于分发用户请求到具体的实现类，并返回具体的响应信息。 这里面有一个看源码的技巧，当分析到某一个类的时候，不知道如何再往下分析了，也就是缺少相应的抓手，怎么办？ 1.看类的继承关系，从类的继承关系触发，寻找相应的方法 2.看初始化这种的方法 ctrl+H 查看类的继承关系，DispatcherServlet 实现了Servlet 接口，所以按照加载过程，最终会调用其 init(ServletConfig config)方法。从DispatcherServlet中寻找init方法发现没有，这个时候怎么办？按照类的继承关系，逐步向上寻找。最终定位到 GenericServlet 中，但是这里什么也没有做，只是交给了子类去实现，那就继续寻找，最终定位到抓手：HttpServletBean。 123456789101112131415161718192021222324252627282930313233/** * 这个方法几乎是web-ioc容器的核心入口 * 当前类是DispatcherServlet的爷爷类 * 加载DispatcherServlet的时候，他的爷爷类会先加载的并且 init方法会执行 * @throws ServletException */@Overridepublic final void init() throws ServletException &#123; // 设置属性信息 PropertyValues pvs = new ServletConfigPropertyValues(getServletConfig(), this.requiredProperties); if (!pvs.isEmpty()) &#123; try &#123; //使用装饰器模式，对具体的实现类进行一个包装 BeanWrapper bw = PropertyAccessorFactory.forBeanPropertyAccess(this); ResourceLoader resourceLoader = new ServletContextResourceLoader(getServletContext()); bw.registerCustomEditor(Resource.class, new ResourceEditor(resourceLoader, getEnvironment())); //将web.xml里面的属性信息设置到 bw 里面。 initBeanWrapper(bw); bw.setPropertyValues(pvs, true); &#125; catch (BeansException ex) &#123; if (logger.isErrorEnabled()) &#123; logger.error(&quot;Failed to set bean properties on servlet &#x27;&quot; + getServletName() + &quot;&#x27;&quot;, ex); &#125; throw ex; &#125; &#125; // Let subclasses do whatever initialization they like. //交给子类来实现 initServletBean();&#125; 来到子类：FrameworkServlet。 12345678910111213141516171819202122232425262728293031@Overrideprotected final void initServletBean() throws ServletException &#123; getServletContext().log(&quot;Initializing Spring &quot; + getClass().getSimpleName() + &quot; &#x27;&quot; + getServletName() + &quot;&#x27;&quot;); if (logger.isInfoEnabled()) &#123; logger.info(&quot;Initializing Servlet &#x27;&quot; + getServletName() + &quot;&#x27;&quot;); &#125; long startTime = System.currentTimeMillis(); try &#123; //初始化web应用容器 this.webApplicationContext = initWebApplicationContext(); //初始化框架servlet initFrameworkServlet(); &#125; catch (ServletException | RuntimeException ex) &#123; logger.error(&quot;Context initialization failed&quot;, ex); throw ex; &#125; if (logger.isDebugEnabled()) &#123; String value = this.enableLoggingRequestDetails ? &quot;shown which may lead to unsafe logging of potentially sensitive data&quot; : &quot;masked to prevent unsafe logging of potentially sensitive data&quot;; logger.debug(&quot;enableLoggingRequestDetails=&#x27;&quot; + this.enableLoggingRequestDetails + &quot;&#x27;: request parameters and headers will be &quot; + value); &#125; if (logger.isInfoEnabled()) &#123; logger.info(&quot;Completed initialization in &quot; + (System.currentTimeMillis() - startTime) + &quot; ms&quot;); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061/** * 这里主要是进行了web应用容器上下文的创建，并进行了初始化工作。 * 跟踪一下初始化的具体流程 * @return */protected WebApplicationContext initWebApplicationContext() &#123; //获取到 root ioc 容器 WebApplicationContext rootContext = WebApplicationContextUtils.getWebApplicationContext(getServletContext()); WebApplicationContext wac = null; if (this.webApplicationContext != null) &#123; // A context instance was injected at construction time -&gt; use it wac = this.webApplicationContext; if (wac instanceof ConfigurableWebApplicationContext) &#123; ConfigurableWebApplicationContext cwac = (ConfigurableWebApplicationContext) wac; if (!cwac.isActive()) &#123; // The context has not yet been refreshed -&gt; provide services such as // setting the parent context, setting the application context id, etc if (cwac.getParent() == null) &#123; //将 root ioc 容器设置成 servlet 的ioc容器的父类 //如果当前servlet存在一个 WebApplicationContext 即：子IOC容器 //并且上下文获取的root ioc 容器存在，则将root ioc 容器作为子 ioc 容器的父容器 cwac.setParent(rootContext); &#125; //配置并刷新子容器，加载子容器中对应的bean实体类 configureAndRefreshWebApplicationContext(cwac); &#125; &#125; &#125; //如果当前servlet中不存在子ioc容器，则去查找 if (wac == null) &#123; // No context instance was injected at construction time -&gt; see if one // has been registered in the servlet context. If one exists, it is assumed // that the parent context (if any) has already been set and that the // user has performed any initialization such as setting the context id wac = findWebApplicationContext(); &#125; if (wac == null) &#123; // No context instance is defined for this servlet -&gt; create a local one //如果查找不到，就去创建一个 wac = createWebApplicationContext(rootContext); &#125; if (!this.refreshEventReceived) &#123; // Either the context is not a ConfigurableApplicationContext with refresh // support or the context injected at construction time had already been // refreshed -&gt; trigger initial onRefresh manually here. synchronized (this.onRefreshMonitor) &#123; onRefresh(wac); &#125; &#125; if (this.publishContext) &#123; // Publish the context as a servlet context attribute. String attrName = getServletContextAttributeName(); getServletContext().setAttribute(attrName, wac); &#125; return wac;&#125; 12345678910111213141516171819202122232425protected WebApplicationContext createWebApplicationContext(@Nullable ApplicationContext parent) &#123; Class&lt;?&gt; contextClass = getContextClass(); if (!ConfigurableWebApplicationContext.class.isAssignableFrom(contextClass)) &#123; throw new ApplicationContextException( &quot;Fatal initialization error in servlet with name &#x27;&quot; + getServletName() + &quot;&#x27;: custom WebApplicationContext class [&quot; + contextClass.getName() + &quot;] is not of type ConfigurableWebApplicationContext&quot;); &#125; //根据类信息初始化一个ConfigurableWebApplicationContext对象 ConfigurableWebApplicationContext wac = (ConfigurableWebApplicationContext) BeanUtils.instantiateClass(contextClass); //设置web上下文环境信息 wac.setEnvironment(getEnvironment()); //设置其父类为root ioc 容器，root ioc 容器是整个应用唯一的。 wac.setParent(parent); //设置其具体的配置信息的位置，这里是 classpath:spring-mvc.xml String configLocation = getContextConfigLocation(); if (configLocation != null) &#123; wac.setConfigLocation(configLocation); &#125; //配置并刷新web应用的ioc容器 configureAndRefreshWebApplicationContext(wac); return wac;&#125; 123456789101112131415161718192021222324252627282930313233343536protected void configureAndRefreshWebApplicationContext(ConfigurableWebApplicationContext wac) &#123; if (ObjectUtils.identityToString(wac).equals(wac.getId())) &#123; // The application context id is still set to its original default value // -&gt; assign a more useful id based on available information if (this.contextId != null) &#123; wac.setId(this.contextId); &#125; else &#123; // Generate default id... wac.setId(ConfigurableWebApplicationContext.APPLICATION_CONTEXT_ID_PREFIX + ObjectUtils.getDisplayString(getServletContext().getContextPath()) + &#x27;/&#x27; + getServletName()); &#125; &#125; //配置容器的相关信息 wac.setServletContext(getServletContext()); wac.setServletConfig(getServletConfig()); wac.setNamespace(getNamespace()); //配置容器应用加载的监听器 wac.addApplicationListener(new SourceFilteringListener(wac, new ContextRefreshListener())); // The wac environment&#x27;s #initPropertySources will be called in any case when the context // is refreshed; do it eagerly here to ensure servlet property sources are in place for // use in any post-processing or initialization that occurs below prior to #refresh ConfigurableEnvironment env = wac.getEnvironment(); if (env instanceof ConfigurableWebEnvironment) &#123; ((ConfigurableWebEnvironment) env).initPropertySources(getServletContext(), getServletConfig()); &#125; postProcessWebApplicationContext(wac); //应用初始化信息 applyInitializers(wac); //刷新加载里面的bean实体类 wac.refresh();&#125; 这里其实可以看到，在这个其实主要是根据配置文件信息进行类加载的工作，并且配置了一个容器加载信息的监听器 SourceFilteringListener。在最后通过 refresh 方法进行了容器中实体类的加载过程。这个refresh方法和我们在listener中实现类的初始化过程使用的是同一个方法。到此为止，在我们应用中配置的所有的类都能够扫描到，并且配置了我们的ioc容器中。因为我们配置了相关的容器加载的监听器，在refresh方法中调用了 finishRefresh 方法时，发送对应的容器加载完成广播信息，从而能够调用我们所注册的监听器 SourceFilteringListener。看一下里面的逻辑~ 12345678protected void onApplicationEventInternal(ApplicationEvent event) &#123; if (this.delegate == null) &#123; throw new IllegalStateException( &quot;Must specify a delegate object or override the onApplicationEventInternal method&quot;); &#125; //这里的delegate，是传入的具体的代理类，所以在此回到了我们的FrameworkServlet this.delegate.onApplicationEvent(event);&#125; 1234567891011public void onApplicationEvent(ContextRefreshedEvent event) &#123; this.refreshEventReceived = true; synchronized (this.onRefreshMonitor) &#123; //最终调用了FrameworkServlet的onApplicationEvent方法 onRefresh(event.getApplicationContext()); &#125;&#125;protected void onRefresh(ApplicationContext context) &#123; // 又是一个扩展点，交给子类去实现&#125; 终于来到了我们的DispatcherServlet。 12345@Overrideprotected void onRefresh(ApplicationContext context) &#123; //通过重写父类的扩展点来到这里 initStrategies(context);&#125; 123456789101112131415161718192021222324252627282930/** * 初始化servlet使用的策略信息，子类可以通过覆写该方法类增加更多的呃策略方法 * Initialize the strategy objects that this servlet uses. * &lt;p&gt;May be overridden in subclasses in order to initialize further strategy objects. */protected void initStrategies(ApplicationContext context) &#123; //初始化MultipartResolver,可以支持文件的上传 initMultipartResolver(context); //初始化本地解析器 initLocaleResolver(context); //初始化主题解析器 initThemeResolver(context); //处理器映射器，将请求和方法进行映射关联 initHandlerMappings(context); //处理器适配器 initHandlerAdapters(context); //处理器异常解析器 initHandlerExceptionResolvers(context); //从请求到视图名的转换器 initRequestToViewNameTranslator(context); //视图解析器 initViewResolvers(context); //FlashMap管理器 initFlashMapManager(context); /* * 可以看到里面主要是初始化了我们的所使用到的一些解析器和处理器等。 * 当接收到请求后，就可以根据这些解析器来进行请求的解析处理、方法的调用、异常的处理等等。 * 到此为止，Servlet的初始化工作就整个完成了。想当的复杂，主要是将很多的方法实现在父类中进行了处理。层级比较复杂，需要一点点跟踪分析。 */&#125; 思考与沉淀web应用启动的最后一个步骤就是创建和初始化 Servlet ，我们就从我们使用的 DispatcherServlet 这个类来进行分析，这个类是前端控制器，主要用于分发用户请求到具体的实现类，并返回具体的响应信息。看这个类的继承关系：在HttpServlet（他的爷爷类）中，实现了servlet的init方法（其实再往里追源码，实际是从源码里面实现的，算是一个扩展点吧），这里主要是委派给了子类FrameworkServlet（dispatcherServlet的父类）的initServletBean()，这个方法主要是初始化web应用。通过initWebApplicationContext()配置并刷新子容器，这里其实可以看到，在这个其实主要是根据配置文件信息进行类加载的工作，并且配置了一个容器加载信息的监听器 SourceFilteringListener。在最后通过 refresh 方法进行了容器中实体类的加载过程。这个refresh方法和我们在listener中实现类的初始化过程使用的是同一个方法。到此为止，在我们应用中配置的所有的类都能够扫描到，并且配置了我们的ioc容器中。因为我们配置了相关的容器加载的监听器，在refresh方法中调用了 finishRefresh 方法时，发送对应的容器加载完成广播信息，从而能够调用我们所注册的监听器 SourceFilteringListener。看一下里面的逻辑~这里面实际上又调用了传入的代理类的onRefresh()。这个代理类是谁？其实就是DispatcherServlet。他的onRefresh()实际上就是调用了initStrategies()，可以看到里面主要是初始化了我们的所使用到的一些解析器和处理器等。当接收到请求后，就可以根据这些解析器来进行请求的解析处理、方法的调用、异常的处理等等。到此为止，Servlet的初始化工作就整个完成了。","categories":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/tags/Spring/"}]},{"title":"MySQL[十一]高性能MySQL调优实战","slug":"MySQL/MySQL[十一]高性能MySQL调优实战","date":"2022-01-10T16:00:00.000Z","updated":"2022-01-12T00:42:24.249Z","comments":true,"path":"2022/01/11/MySQL/MySQL[十一]高性能MySQL调优实战/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/MySQL/MySQL[%E5%8D%81%E4%B8%80]%E9%AB%98%E6%80%A7%E8%83%BDMySQL%E8%B0%83%E4%BC%98%E5%AE%9E%E6%88%98/","excerpt":"","text":"一，数据库应该如何优化数据库优化有很多层面。 1.SQL与索引因为 SQL 语句是在我们的应用端编写的，所以第一步，我们可以在程序中对 SQL 语句进行优化，最终的目标是用到索引。这个是容易的也是最常用的优化手段。 2.表与存储引擎数据是存放在表里面的，表又是以不同的格式存放在存储引擎中的，所以我们可以选用特定的存储引擎，或者对表进行分区，对表结构进行拆分或者冗余处理，或者对表结构比如字段的定义进行优化。 3.架构对于数据库的服务，我们可以对它的架构进行优化。如果只有一台数据库的服务器，我们可以运行多个实例，做集群的方案，做负载均衡。或者基于主从复制实现读写分离，让写的服务都访问 master 服务器，读的请求都访问从服务器，slave 服务器自动 master 主服务器同步数据。或者在数据库前面加一层缓存，达到减少数据库的压力，提升访问速度的目的。为了分散数据库服务的存储压力和访问压力，我们也可以把不同的数据分布到不同的服务节点，这个就是分库分表（scale out）。 注意主从（replicate）和分片（shard）的区别： 主从通过数据冗余实现高可用，和实现读写分离。 分片通过拆分数据分散存储和访问压力。 4.配置数据库配置的优化，比如连接数，缓冲区大小等等，优化配置的目的都是为了更高效地利用硬件。 5.操作系统与硬件从上往下，成本收益比慢慢地在增加。所以肯定不是查询一慢就堆硬件，堆硬件叫做向上的扩展（scale up）。 二，慢日志查询1.概述MySQL的慢查询日志是MySQL提供的一种日志记录，它用来记录在MySQL中响应时间超过阀值的语句，具体指运行时间超过long_query_time值的SQL，则会被记录到慢查询日志中。long_query_time的默认值为10，意思是运行10秒以上的语句。由他来查看哪些SQL超出了我们的最大忍耐时间值，比如一条sql执行超过5秒钟，我们就算慢SQL，希望能收集超过5秒的sql，结合explain进行全面分析。 2.实操默认情况下，MySQL数据库没有开启慢查询日志，需要我们手动来设置这个参数。 当然，如果不是调优需要的话，一般不建议启动该参数，因为开启慢查询日志会或多或少带来一定的性能影响。慢查询日志支持将日志记录写入文件。 2.1查看及开启①日志1SHOW VARIABLES LIKE &#x27;%slow_query_log%&#x27;; 默认情况下slow_query_log的值为OFF，表示慢查询日志是禁用的。 1set global slow_query_log=1; 只对窗口生效，重启服务失效。 ②时间1SHOW VARIABLES LIKE &#x27;%long_query_time%&#x27;; 1SET GLOBAL long_query_time=0.1; 全局变量设置，对所有客户端有效。但必须是设置后进行登录的客户端。 1SET SESSION long_query_time=0.1; #session可省略 对当前会话连接立即生效，对其他客户端无效。 假如运行时间正好等于long_query_time的情况，并不会被记录下来。也就是说，在mysql源码里是判断大于long_query_time，而非大于等于。 ③永久生效修改配置文件my.cnf（其它系统变量也是如此） [mysqld]下增加或修改参数 slow_query_log 和slow_query_log_file后，然后重启MySQL服务器。也即将如下两行配置进my.cnf文件 [1] 1234slow_query_log =1slow_query_log_file=/var/lib/mysql/yhd-slow.log long_query_time=3log_output=FILE 2.2Case记录慢SQL并后续分析 查询当前系统中有多少条慢查询记录 1SHOW GLOBAL STATUS LIKE &#x27;%Slow_queries%&#x27;; 3.日志分析工具-mysqldumpslow在生产环境中，如果要手工分析日志，查找、分析SQL，显然是个体力活，MySQL提供了日志分析工具mysqldumpslow。 查看mysqldumpslow的帮助信息（windows下需要安装perl环境） 1mysqldumpslow --help -a: 不将数字抽象成N，字符串抽象成S-s: 是表示按照何种方式排序；c: 访问次数l: 锁定时间r: 返回记录t: 查询时间al:平均锁定时间ar:平均返回记录数at:平均查询时间-t: 即为返回前面多少条的数据；-g: 后边搭配一个正则匹配模式，大小写不敏感的； 3.1常用SQL12345678得到返回记录集最多的10个SQLmysqldumpslow -s r -t 10 /var/lib/mysql/yhd-slow.log得到访问次数最多的10个SQLmysqldumpslow -s c -t 10 /var/lib/mysql/yhd-slow.log得到按照时间排序的前10条里面含有左连接的查询语句mysqldumpslow -s t -t 10 -g &quot;left join&quot; /var/lib/mysql/yhd-slow.log另外建议在使用这些命令时结合 | 和more 使用 ，否则有可能出现爆屏情况mysqldumpslow -s r -t 10 /var/lib/mysql/yhd-slow.log | more 4.SHOW PROCESSLIST作用：查询所有用户正在干什么。 如果出现不顺眼的：kill [id] 三，EXPLAIN调优实战1.准备数据员工表插入500w数据，部门表插入10w数据。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586CREATE TABLE `dept`( `id` INT(11) NOT NULL AUTO_INCREMENT, `deptName` VARCHAR(30) DEFAULT NULL, `address` VARCHAR(40) DEFAULT NULL, `ceo` INT NULL, PRIMARY KEY (`id`)) ENGINE = INNODB AUTO_INCREMENT = 1 DEFAULT CHARSET = utf8;CREATE TABLE `emp`( `id` INT(11) NOT NULL AUTO_INCREMENT, `empno` INT NOT NULL, `name` VARCHAR(20) DEFAULT NULL, `age` INT(3) DEFAULT NULL, `deptId` INT(11) DEFAULT NULL, PRIMARY KEY (`id`) #CONSTRAINT `fk_dept_id` FOREIGN KEY (`deptId`) REFERENCES `t_dept` (`id`)) ENGINE = INNODB AUTO_INCREMENT = 1 DEFAULT CHARSET = utf8;#生成随机字符串DELIMITER $$CREATE FUNCTION rand_string(n INT) RETURNS VARCHAR(255)BEGIN DECLARE chars_str VARCHAR(100) DEFAULT &#x27;abcdefghijklmnopqrstuvwxyzABCDEFJHIJKLMNOPQRSTUVWXYZ&#x27;; DECLARE return_str VARCHAR(255) DEFAULT &#x27;&#x27;; DECLARE i INT DEFAULT 0; WHILE i &lt; n DO SET return_str = CONCAT(return_str, SUBSTRING(chars_str, FLOOR(1 + RAND() * 52), 1)); SET i = i + 1; END WHILE; RETURN return_str;END $$#用于随机产生多少到多少的编号DELIMITER $$CREATE FUNCTION rand_num(from_num INT, to_num INT) RETURNS INT(11)BEGIN DECLARE i INT DEFAULT 0; SET i = FLOOR(from_num + RAND() * (to_num - from_num + 1)); RETURN i;END$$#创建往emp表中插入数据的存储过程DELIMITER $$CREATE PROCEDURE insert_emp(START INT, max_num INT)BEGIN DECLARE i INT DEFAULT 0; SET autocommit = 0; #设置手动提交事务 REPEAT #循环 SET i = i + 1; #赋值 INSERT INTO emp (empno, NAME, age, deptid) VALUES ((START + i), rand_string(6), rand_num(30, 50), rand_num(1, 10000)); UNTIL i = max_num END REPEAT; COMMIT; #提交事务END$$#创建往dept表中插入数据的存储过程DELIMITER $$CREATE PROCEDURE `insert_dept`(max_num INT)BEGIN DECLARE i INT DEFAULT 0; SET autocommit = 0; REPEAT SET i = i + 1; INSERT INTO dept (deptname, address, ceo) VALUES (rand_string(8), rand_string(10), rand_num(1, 500000)); UNTIL i = max_num END REPEAT; COMMIT;END$$#执行存储过程，往dept表添加10万条数据CALL insert_dept(100000);#执行存储过程，往emp表添加500万条数据CALL insert_emp(100000, 5000000); 2.批量删除索引建立好的索引在哪里？ 12SHOW INDEX FROM t_emp ; -- 只能查看索引，但不能删除。information_schema.STATISTICS -- 存储索引的表（元数据库，统计表），我们可以对表数据进行删除操作。 知识点 删除某一个索引 1DROP INDEX idx_xxx ON emp 查出该表有哪些索引，索引名–&gt;集合 12345678SHOW INDEX FROM t_emp-- 元数据：meta DATA 描述数据的数据SELECT index_nameFROM information_schema.STATISTICSWHERE table_name = &#x27;t_emp&#x27; AND table_schema = &#x27;mydb&#x27; AND index_name &lt;&gt; &#x27;PRIMARY&#x27; AND seq_in_index = 1 3.单表使用索引建立索引 12CREATE INDEX idx_age_deptid_name ON emp(age,deptid,NAME);CREATE INDEX idx_name ON emp(NAME); 3.1 全值匹配1234567891011121314151617181920# 单表查询-全值匹配EXPLAINSELECT SQL_NO_CACHE *FROM empWHERE emp.age = 30;EXPLAINSELECT SQL_NO_CACHE *FROM empWHERE emp.age = 30 and deptid = 4;EXPLAINSELECT SQL_NO_CACHE *FROM empWHERE emp.age = 30 and deptid = 4 AND emp.name = &#x27;abcd&#x27;; 3.2 最左前缀法则12345678# 单表查询-左前缀法则EXPLAIN SELECT * FROM emp WHERE age=1 AND deptid=1 AND NAME=&#x27;aaa&#x27;;EXPLAIN SELECT * FROM emp WHERE age=1 AND deptid=1;EXPLAIN SELECT * FROM emp WHERE age=1 AND NAME=&#x27;aaa&#x27; AND deptid=1;EXPLAIN SELECT * FROM emp WHERE deptid=1 AND NAME =&#x27;aaa&#x27;; 过滤条件要使用索引必须按照索引建立时的顺序，依次满足，一旦跳过某个字段，索引后面的字段都无法被使用。 3.3 索引列上计算/函数导致索引失效1234# 单表查询-操作索引列导致索引失效EXPLAIN SELECT SQL_NO_CACHE * FROM emp WHERE emp.name LIKE &#x27;abc%&#x27;;EXPLAIN SELECT SQL_NO_CACHE * FROM emp WHERE LEFT(emp.name,3) = &#x27;abc&#x27;; 3.4 范围查询导致的索引失效12345678EXPLAIN SELECT SQL_NO_CACHE * FROM emp WHERE emp.name = &#x27;abc&#x27; AND emp.deptId &gt; 20 AND emp.age = 30 ; 应用开发中范围查询，例如： 金额查询，日期查询往往都是范围查询。应将查询条件放置where语句最后。 3.5 不等于(!= 或者&lt;&gt;)索引失效1EXPLAIN SELECT SQL_NO_CACHE * FROM emp WHERE emp.name &lt;&gt; &#x27;abc&#x27; ; 3.6 is not null无法使用索引，is null可使用索引1234EXPLAIN SELECT SQL_NO_CACHE * FROM emp WHERE age IS NULL;#用到索引 EXPLAIN SELECT SQL_NO_CACHE * FROM emp WHERE age IS NOT NULL;#未用到索引 3.7 like以%开头索引失效1EXPLAIN SELECT SQL_NO_CACHE * FROM emp WHERE NAME LIKE &#x27;%aaa&#x27;; 3.8 类型转换导致索引失效1EXPLAIN SELECT SQL_NO_CACHE * FROM emp WHERE NAME=123; 设计实体类属性时，一定要与数据库字段类型相对应，否则会出现类型转换的情况，导致索引失效。 4. 关联查询优化4.1 左外连接1explain select * from emp left join dept on emp.deptId=dept.id; 这种情况下，驱动表无法避免全表扫描，但是因为被驱动表的主键存在索引并且是两张表关联查询的关联条件，所以可以避免被驱动表的全表扫描。 4.2 内连接(TODO)内连接MySQL会自动为我们选择驱动表。 123456explain select * from dept straight_join emp on emp.deptId=dept.id;## 1. dept 全表扫描 10w## 2. emp deptid refexplain select * from dept join emp on emp.deptId=dept.id;## 1. emp 500w## 2. dept id ref 保证被驱动表的join字段被索引 left join 时，选择小表作为驱动表，大表作为被驱动表 inner join 时，mysql会自动将小结果集的表选为驱动表。选择相信mysql优化策略。 子查询尽量不要放在被驱动表，衍生表建不了索引。 能够直接多表关联的尽量直接关联，不用子查询。 两张表的连接查询，比方说 left join right、inner join 等，他们的连表方式是什么？ 连表查询一共三种算法：nlj bnl bka 算法 。 right join 底层，会给你转化为left join。 4.3 子查询优化1234567891011121314151617181920212223#①不推荐explain SELECT *FROM empWHERE emp.id NOT IN -- not in 导致无法对in进行优化，用不了exists (SELECT dept.ceo FROM dept WHERE dept.ceo IS NOT NULL) ; -- is not null 导致索引失效#②推荐explain SELECT emp.*FROM emp LEFT JOIN dept ON emp.id = dept.ceo -- 如果ceo没有索引，两张表都是全表扫描，如果ceo有索引，被驱动表就是ref级别WHERE dept.id IS NULL ;# 尝试在ceo创建索引后，确实是 create index idx_ceo on dept(ceo); 尽量不要使用not in 或者 not exists，会使索引失效。 MySQL自动做出的子查询优化，物化子查询，转为半连接 物化子查询：把子查询的结果查出来后，建立一个临时表，“物化”-&gt;变成一张内存临时表 半连接：把子查询转化为类似连接查询的方式，但又不是真正的连接查询，所以叫 半 连接优化 5.排序分组优化5.1 无过滤，不索引1234EXPLAIN SELECT SQL_NO_CACHE * FROM emp ORDER BY age,deptid; #没用上索引，Using filesort EXPLAIN SELECT SQL_NO_CACHE * FROM emp ORDER BY age,deptid LIMIT 10; #使用上索引 null 因为order by的字段顺序和索引的顺序一样，所以此时会先尝试内存排序，但是因为上面的sql没有limit，导致内存放不下，使用了文件排序（文件系统级别，相当于在磁盘做排序），所以第一条sql效率更低。 order后面的字段想要使用索引，必须要有过滤条件，limit也行。 5.2顺序错，必排序1234567891011121314EXPLAIN SELECT * FROM emp WHERE age=45 ORDER BY deptid;# Using index conditionEXPLAIN SELECT * FROM emp WHERE age=45 ORDER BY deptid,NAME;# Using index conditionEXPLAIN SELECT * FROM emp WHERE age=45 ORDER BY deptid,empno;# Using index condition; Using filesortEXPLAIN SELECT * FROM emp WHERE age=45 ORDER BY NAME,deptid;# Using index condition; Using filesortEXPLAIN SELECT * FROM emp WHERE deptid=45 ORDER BY age;# Using where; Using filesort 在SQL语句中的顺序一定要和定义索引中的字段顺序完全一致。 5.3 方向反，必排序1234EXPLAIN SELECT * FROM emp WHERE age=45 ORDER BY deptid DESC, NAME DESC ;#Using whereEXPLAIN SELECT * FROM emp WHERE age=45 ORDER BY deptid ASC, NAME DESC ;#Using index condition; Using filesort ORDER BY子句，尽量使用Index方式排序，避免使用FileSort方式排序 要么全升序、要么全降序。有升有降无法使用索引。 5.4 索引的选择 两个索引同时存在，mysql自动选择最优的方案，但是，随着数据量的变化，选择的索引也会随之变化的。 所有的排序都是在条件过滤之后才执行的，所以，如果条件过滤掉大部分数据的话，剩下几百几千条数据进行排序其实并不是很消耗性能，即使索引优化了排序，但实际提升性能很有限。 当【范围条件】和【group by 或者 order by】的字段出现二选一时，优先观察条件字段的过滤数量，如果过滤的数据足够多，而需要排序的数据并不多时，优先把索引放在范围（过滤条件）字段上。反之，亦然。 扫描行数的多少，就是explain里的rows，可以说明一个需要扫描的行数多，一个扫描行数少，扫描行数多，代表成本高，扫描行数少代表成本少。优化器最终是对比成本值的大小来选取索引的。准确的说，是MySQL基于成本，优化器是在server层。 有时候优化器会选择错索引为什么？ 主要是出在优化器预估行数上，这个涉及到了一条sql的执行流程，语法分析，词法分析之后，进入优化阶段，由优化器进行优化，在优化阶段，会尽可能的生成全部的执行计划，然后对比一下哪一个成本值最低，就选它，所以优化器有一个选择索引，选择表的连接顺序的过程，索引不同，成本不同，读表顺序不同，成本不同，索引的选取，需要存储引擎提供统计信息，innodb中，统计信息是随机采样，随机选取8个索引页，取平均值，当做该索引的全部情况，也就是部分代表整体，也就是最终导致rows那里是个预估值，而不是准确的。所以有时候MySQL选错了索引，有一定概率，是由于这个随机采样造成的。而随机采样的不准确，是由于数据不断添加导致索引页的分裂，导致有些页内数据较少。 解决方案： 执行一下alter table +表名 就可以使统计信息稍微准确点，他会重新构建索引，使索引页保持紧凑，这个就是B+树的分裂。 调整参数，加大InnoDB采样的页数，页数越大越精确，但性能消耗更高。一般不建议这么干。 在优化阶段，会对表中所有索引进行对比，优化器基于成本的原因，选择成本最低的索引，所以会错过最佳索引。带来的问题便是，执行速度很慢。 解决方案： 通过explain查看执行计划，结合sql条件查看可以利用哪些索引。 使用 force index(indexName)强制走指定索引。弊端就是后期若索引名发生改变，或索引被删除，该sql语句需要调整。 5.5 双路排序&amp;单路排序如果不在索引列上，filesort有两种算法： mysql就要启动双路排序和单路排序。 双路排序 MySQL 4.1之前是使用双路排序，字面意思就是两次扫描磁盘，最终得到数据， 读取行指针和order by列，对他们进行排序，然后扫描已经排序好的列表，按照列表中的值重新从列表中读取对应的数据输出 从磁盘取排序字段，在buffer进行排序，再从磁盘取其他字段。 取一批数据，要对磁盘进行两次扫描，众所周知，I\\O是很耗时的，所以在mysql4.1之后，出现了第二种改进的算法，就是单路排序。 单路排序 从磁盘读取查询需要的所有列，按照order by列在buffer对它们进行排序，然后扫描排序后的列表进行输出， 它的效率更快一些，避免了第二次读取数据。并且把随机IO变成了顺序IO，但是它会使用更多的空间， 因为它把每一行都保存在内存中了。 结论 由于单路是后出的，总体而言好过双路。 但是用单路有问题： 在sort_buffer中，单路比多路要多占用很多空间，因为单路是把所有字段都取出, 所以有可能取出的数据的总大小超出了sort_buffer的容量，导致每次只能取sort_buffer容量大小的数据，进行排序（创建tmp文件，多路合并），排完再取sort_buffer容量大小，再排……从而多次I/O。 单路本来想省一次I/O操作，反而导致了大量的I/O操作，反而得不偿失。 优化策略 增大sort_buffer_size参数的设置 增大max_length_for_sort_data参数的设置 减少select 后面的查询的字段。 提高order by的速度 Order by时select * 是一个大忌。只Query需要的字段， 这点非常重要。 当Query的字段大小总和小于max_length_for_sort_data 而且排序字段不是 TEXT|BLOB 类型时，会用改进后的算法——单路排序， 否则用老算法——多路排序。 两种算法的数据都有可能超出sort_buffer的容量，超出之后，会创建tmp文件进行合并排序，导致多次I/O，但是用单路排序算法的风险会更大一些，所以要提高sort_buffer_size。 尝试提高 sort_buffer_size 不管用哪种算法，提高这个参数都会提高效率，当然，要根据系统的能力去提高，因为这个参数是针对每个进程（connection）的 1M-8M之间调整。 MySQL5.7，InnoDB存储引擎默认值是1048576字节，1MB。 1SHOW VARIABLES LIKE &#x27;%sort_buffer_size%&#x27;; 尝试提高 max_length_for_sort_data 提高这个参数， 会增加用改进算法的概率。 但是如果设的太高，数据总容量超出sort_buffer_size的概率就增大，明显症状是高的磁盘I/O活动和低的处理器使用率。如果需要返回的列的总长度大于max_length_for_sort_data，使用双路算法，否则使用单路算法。1024-8192字节之间调整。 1SHOW VARIABLES LIKE &#x27;%max_length_for_sort_data%&#x27;; #默认1024字节 5.6 分组优化group by 使用索引的原则几乎跟order by一致 ，唯一区别： group by 先排序再分组，遵照索引建的最佳左前缀法则 当无法使用索引列，增大max_length_for_sort_data和sort_buffer_size参数的设置 where高于having,能写在where限定的条件就不要写在having中了 group by没有过滤条件，也可以用上索引。Order By 必须有过滤条件才能使用上索引。 6. 覆盖索引禁止使用select *，禁止查询与业务无关字段，尽量使用覆盖索引，防止回表。 覆盖索引减少了 IO 次数，减少了数据的访问量，可以大大地提升查询效率。 四，追踪优化器前面的原理篇详细分析过，在此不再赘述。 五， 分库分表从维度来说分成两种，一种是垂直，一种是水平。 垂直切分：基于表或字段划分，表结构不同。我们有单库的分表，也有多库的分库。 水平切分：基于数据划分，表结构相同，数据不同，也有同库的水平切分和多库的切分。 1.垂直切分垂直分表有两种，一种是单库的，一种是多库的。 1.1 单库垂直分表单库分表，比如：商户信息表，拆分成基本信息表，联系方式表，结算信息表，附件表等等。 可以考虑根据冷热点字段拆分，是否经常发生修改操作拆分，根据字段功能拆分。 1.2 多库垂直分表多库垂直分表就是把原来存储在一个库的不同的表，拆分到不同的数据库。 比如电商平台的消费系统：一开始，商品表，商品详情表，订单表，用户表，支付记录表，库存表，风控表都在一个库里面，随着数据的增长和业务的扩张，可以考虑将商品和商品详情表单独放到一个库，订单表单独放到一个库，支付记录单独放到一个库，库存表单独放到一个库，风控表单独放到一个库。 当我们对原来的一张表做了分库的处理，如果某些业务系统的数据还是有一个非常快的增长速度，比如说订单数据库的订单表，数据量达到了几个亿，这个时候硬件限制导致的性能问题还是会出现，所以从这个角度来说垂直切分并没有从根本上解决单库单表数据量过大的问题。在这个时候，我们还需要对我们的数据做一个水平的切分。 2.水平拆分当我们的客户表数量已经到达数千万甚至上亿的时候，单表的存储容量和查询效率都会出现问题，我们需要进一步对单张表的数据进行水平切分。水平切分的每个数据库的表结构都是一样的，只是存储的数据不一样，比如每个库存储 1000 万的数据。 水平切分也可以分成两种，一种是单库的，一种是多库的。 2.1 单库水平分表银行的交易流水表，所有进出的交易都需要登记这张表，因为绝大部分时候客户都是查询当天的交易和一个月以内的交易数据，所以我们根据使用频率把这张表拆分成三张表： 当天表：只存储当天的数据。 当月表：在夜间运行一个定时任务，前一天的数据，全部迁移到当月表。用的是 insert into select，然后 delete。 历史表：同样是通过定时任务，把登记时间超过 30 天的数据，迁移到 history历史表（历史表的数据非常大，我们按照月度，每个月建立分区）。 跟分区一样，这种方式虽然可以一定程度解决单表查询性能的问题，但是并不能解决单机存储瓶颈的问题。 2.2 多库水平分表比如客户表，我们拆分到多个库存储，表结构是完全一样的。 一般我们说的分库分表都是跨库的分表。 3. 分库分表带来的问题3.1 跨库关联查询比如查询合同信息的时候要关联客户数据，由于是合同数据和客户数据是在不同的数据库，那么我们肯定不能直接使用 join 的这种方式去做关联查询。 解决方案 ①字段冗余比如我们查询合同库的合同表的时候需要关联客户库的客户表，我们可以直接把一些经常关联查询的客户字段放到合同表，通过这种方式避免跨库关联查询的问题。 ②数据同步比如商户系统要查询产品系统的产品表，我们干脆在商户系统创建一张产品表，通过 ETL 或者其他方式定时同步产品数据。 ③全局表（广播表）比如行名行号信息被很多业务系统用到，如果我们放在核心系统，每个系统都要去关联查询，这个时候我们可以在所有的数据库都存储相同的基础数据。 ④ER表我们有些表的数据是存在逻辑的主外键关系的，比如订单表 order_info，存的是汇总的商品数，商品金额；订单明细表 order_detail，是每个商品的价格，个数等等。或者叫做从属关系，父表和子表的关系。他们之间会经常有关联查询的操作，如果父表的数据和子表的数据分别存储在不同的数据库，跨库关联查询也比较麻烦。所以我们能不能把父表的数据和从属于父表的数据落到一个节点上呢？ 比如 order_id=1001 的数据在 node1，它所有的明细数据也放到 node1；order_id=1002 的数据在 node2，它所有的明细数据都放到 node2，这样在关联查询的时候依然是在一个数据库。 上面的思路都是通过合理的数据分布避免跨库关联查询，实际上在我们的业务中，也是尽量不要用跨库关联查询，如果出现了这种情况，就要分析一下业务或者数据拆分是不是合理。如果还是出现了需要跨库关联的情况，那我们就只能用最后一种办法。 ⑤系统层组装在不同的数据库节点把符合条件数据的数据查询出来，然后重新组装，返回给客户端。 3.2 分布式事务具体分布式事务会单独写一篇文章 3.3 排序，翻页，函数计算问题跨节点多库进行查询时，会出现 limit 分页，order by 排序的问题。比如有两个节点，节点 1 存的是奇数 id=1,3,5,7,9……；节点 2 存的是偶数 id=2,4,6,8,10…… 执行 select * from user_info order by id limit 0,10 需要在两个节点上各取出 10 条，然后合并数据，重新排序。 max、min、sum、count 之类的函数在进行计算的时候，也需要先在每个分片上执行相应的函数，然后将各个分片的结果集进行汇总和再次计算，最终将结果返回。 3.4 全局主键避重MySQL 的数据库里面字段有一个自增的属性，Oracle 也有 Sequence 序列。如果是一个数据库，那么可以保证 ID 是不重复的，但是水平分表以后，每个表都按照自己的规律自增，肯定会出现 ID 重复的问题，这个时候我们就不能用本地自增的方式了。 解决方案 ①UUIDUUID 标准形式包含 32 个 16 进制数字，分为 5 段，形式为 8-4-4-4-12 的 36 个字符，例如：c4e7956c-03e7-472c-8909-d733803e79a9。 UUID 是主键是最简单的方案，本地生成，性能高，没有网络耗时。但缺点也很明显，由于 UUID 非常长，会占用大量的存储空间；另外，作为主键建立索引和基于索引进行查询时都会存在性能问题，在 InnoDB 中，UUID 的无序性会引起数据位置频繁变动，导致分页。 ②数据库把序号维护在数据库的一张表中。这张表记录了全局主键的类型、位数、起始值，当前值。当其他应用需要获得全局 ID 时，先 for update 锁行，取到值+1 后并且更新后返回。并发性比较差。 ③redis基于 Redis 的 INT 自增的特性，使用批量的方式降低数据库的写压力，每次获取一段区间的 ID 号段，用完之后再去数据库获取，可以大大减轻数据库的压力。 ④雪花算法优点：毫秒数在高位，生成的 ID 整体上按时间趋势递增；不依赖第三方系统，稳定性和效率较高，理论上 QPS 约为 409.6w/s(1000*2^12)，并且整个分布式系统内不会产生 ID 碰撞；可根据自身业务灵活分配 bit 位。 不足就在于：强依赖机器时钟，如果时钟回拨，则可能导致生成 ID 重复。 4. 多数据源/读写数据源的解决方案分析一下 SQL 执行经过的流程： DAO——Mapper（ORM）——JDBC——代理——数据库服务 4.1 客户端DAO 层在我们连接到某一个数据源之前，我们先根据配置的分片规则，判断需要连接到哪些节点，再建立连接。 Spring 中提供了一个抽象类 AbstractRoutingDataSource，可以实现数据源的动态切换。 123456789101）aplication.properties 定义多个数据源2）创建@TargetDataSource 注解3）创建 DynamicDataSource 继承 AbstractRoutingDataSource4）多数据源配置类 DynamicDataSourceConfig5）创建切面类 DataSourceAspect，对添加了@TargetDataSource 注解的类进行拦截设置数据源。6）在 启 动 类 上 自 动 装 配 数 据 源 配 置@Import(&#123;DynamicDataSourceConfig.class&#125;)7）在 实 现 类 上 加 上 注 解 ， 如 @TargetDataSource(name =DataSourceNames.SECOND)，调用 在 DAO 层实现的优势：不需要依赖 ORM 框架，即使替换了 ORM 框架也不受影响。实现简单（不需要解析 SQL 和路由规则），可以灵活地定制。 缺点：不能复用，不能跨语言。 4.2 ORM框架层比如我们用 MyBatis 连接数据库，也可以指定数据源。我们可以基于 MyBatis 插件的拦截机制（拦截 query 和 update 方法），实现数据源的选择。 4.3 驱动层不管是MyBatis还是Hibernate，还是Spring的JdbcTemplate，本质上都是对JDBC的封装，所以第三层就是驱动层。比如 Sharding-JDBC，就是对 JDBC 的对象进行了封装。JDBC 的核心对象： DataSource：数据源 Connection：数据库连接 Statement：语句对象 ResultSet：结果集 那我们只要对这几个对象进行封装或者拦截或者代理，就可以实现分片的操作。 4.4 代理层前面三种都是在客户端实现的，也就是说不同的项目都要做同样的改动，不同的编程语言也有不同的实现，所以我们能不能把这种选择数据源和实现路由的逻辑提取出来，做成一个公共的服务给所有的客户端使用呢？ 这个就是第四层，代理层。比如 Mycat 和 Sharding-Proxy，都是属于这一层。 4.5 数据库服务某些特定的数据库或者数据库的特定版本可以实现这个功能。 六，主从复制1. 基本原理 MySQL复制过程分成三步： master将改变记录到二进制日志（binary log）。这些记录过程叫做二进制日志事件，binary log events； slave将master的binary log events拷贝到它的中继日志（relay log）； slave重做中继日志中的事件，将改变应用到自己的数据库中。 MySQL复制是异步的且串行化的，slave会从master读取binlog来进行数据同步。 2.与Redis主从复制的差别 redis主从复制是将主机的所有数据都拷贝给从机，并且是近乎实时的。 mysql主从复制不会将建立连接以前的数据发送给从机，并且是异步，且串行化的。 3.复制的基本原则每个slave只有一个master 每个slave只能有一个唯一的服务器ID 每个master可以有多个salve 4.复制的最大问题延时 全同步可以避免，但性能会极差，正常情况下半同步，且容忍一部分数据不一致。如果不容忍数据不一致，只有强制读主。 5.一主一从常见配置 MySQL版本一致且后台以服务运行 主从配置都在【mysqld】节点下，且全部小写 主机修改my.ini文件 主服务器唯一ID server-id=1 启用二进制日志 设置不要复制的数据库 设置需要复制的数据库 设置logbin格式 log-bin=自己的本地路径/data/mysqlbin binlog-ignore-db=mysql binlog-do-db=需要复制的主数据库名字 binlog_fromat=STATEMENT(默认) 七，硬件层面的配置1.选择合适的CPU数据库分为两大类，在线事务处理和在线分析处理。 InnoDB储存引擎一般应用于OLTP的数据库应用，这种应用的特点如下： 用户操作的并发量大 事务处理时间一般比较短 查询的语句较为简单，一般都走索引 复杂查询比较少 在当前的MySQL数据库版本中，一条SQL语句只能在一个CPU工作，并不支持多CPU。若cpu支持多核，innodb版本应该选择1.1或者更高。另外如果是多核cpu，可以通过修改参数innodb_read_io_threads和innodb_write_io_threads来增大IO的线程，这样也可以更充分的利用cpu的多核性能。 2.内存的重要性内存大小直接反映数据库的性能。Innodb存储引擎既缓存数据，又缓存索引，并且将它们缓存于一个很大的缓冲池中，即InnoDB Buffer Pool。因此，内存的大小直接影像数据库的性能。 3.磁盘对数据库性能的影响4.合理设置RAID类型5.操作系统的选择6.文件系统的选择","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/tags/MySQL/"}]},{"title":"MySQL[十]Explain&optimizer trace","slug":"MySQL/MySQL[十]Explain&optimizer trace","date":"2022-01-09T16:00:00.000Z","updated":"2022-01-12T00:42:16.008Z","comments":true,"path":"2022/01/10/MySQL/MySQL[十]Explain&optimizer trace/","link":"","permalink":"https://yinhuidong.github.io/2022/01/10/MySQL/MySQL[%E5%8D%81]Explain&optimizer%20trace/","excerpt":"","text":"一，Explain一条查询语句在经过MySQL查询优化器的各种基于成本和规则的优化会后生成一个所谓的执行计划，这个执行计划展示了接下来具体执行查询的方式，比如多表连接的顺序是什么，对于每个表采用什么访问方法来具体执行查询等等。MySQL为我们提供了EXPLAIN语句来帮助我们查看某个查询语句的具体执行计划，如果我们想看看某个查询的执行计划的话，可以在具体的查询语句前边加一个EXPLAIN，就像这样： 其实除了以SELECT开头的查询语句，其余的DELETE、INSERT、REPLACE以及UPDATE语句前边都可以加上EXPLAIN这个词儿，用来查看这些语句的执行计划，不过我们这里对SELECT语句更感兴趣，所以后边只会以SELECT语句为例来描述EXPLAIN语句的用法。我们先把EXPLAIN语句输出的各个列的作用先大致罗列一下： 列名 描述 id 在一个大的查询语句中每个SELECT关键字都对应一个唯一的id select_type SELECT关键字对应的那个查询的类型 table 表名 partitions 匹配的分区信息 type 针对单表的访问方法 possible_keys 可能用到的索引 key 实际上使用的索引 key_len 实际使用到的索引长度 ref 当使用索引列等值查询时，与索引列进行等值匹配的对象信息 rows 预估的需要读取的记录条数 filtered 某个表经过搜索条件过滤后剩余记录条数的百分比 Extra 一些额外的信息 我们前面创建过一张single_table表： 123456789101112131415CREATE TABLE single_table ( id INT NOT NULL AUTO_INCREMENT, key1 VARCHAR(100), key2 INT, key3 VARCHAR(100), key_part1 VARCHAR(100), key_part2 VARCHAR(100), key_part3 VARCHAR(100), common_field VARCHAR(100), PRIMARY KEY (id), KEY idx_key1 (key1), UNIQUE KEY idx_key2 (key2), KEY idx_key3 (key3), KEY idx_key_part(key_part1, key_part2, key_part3)) Engine=InnoDB CHARSET=utf8; 1.执行计划输出中各列详解table不论我们的查询语句有多复杂，里边儿包含了多少个表，到最后也是需要对每个表进行单表访问的，所以MySQL规定EXPLAIN语句输出的每条记录都对应着某个单表的访问方法，该条记录的table列代表着该表的表名。所以我们看一条比较简单的查询语句： 这个查询语句只涉及对s1表的单表查询，所以EXPLAIN输出中只有一条记录，其中的table列的值是s1，表明这条记录是用来说明对s1表的单表访问方法的。 下边我们看一下一个连接查询的执行计划： 可以看到这个连接查询的执行计划中有两条记录，这两条记录的table列分别是s1和s2，这两条记录用来分别说明对s1表和s2表的访问方法是什么。 id我们知道我们写的查询语句一般都以SELECT关键字开头，比较简单的查询语句里只有一个SELECT关键字，比如下边这个查询语句： 1SELECT * FROM s1 WHERE key1 = &#x27;a&#x27;; 稍微复杂一点的连接查询中也只有一个SELECT关键字，比如： 123SELECT * FROM s1 INNER JOIN s2 ON s1.key1 = s2.key1 WHERE s1.common_field = &#x27;a&#x27;; 但是下边两种情况下在一条查询语句中会出现多个SELECT关键字： 查询中包含子查询的情况比如下边这个查询语句中就包含2个SELECT关键字： 12SELECT * FROM s1 WHERE key1 IN (SELECT key3 FROM s2); 查询中包含UNION语句的情况比如下边这个查询语句中也包含2个SELECT关键字： 1SELECT * FROM s1 UNION SELECT * FROM s2; 查询语句中每出现一个SELECT关键字，MySQL就会为它分配一个唯一的id值。这个id值就是EXPLAIN语句的第一个列，比如下边这个查询中只有一个SELECT关键字，所以EXPLAIN的结果中也就只有一条id列为1的记录： 对于连接查询来说，一个SELECT关键字后边的FROM子句中可以跟随多个表，所以在连接查询的执行计划中，每个表都会对应一条记录，但是这些记录的id值都是相同的，比如： 可以看到，上述连接查询中参与连接的s1和s2表分别对应一条记录，但是这两条记录对应的id值都是1。在连接查询的执行计划中，每个表都会对应一条记录，这些记录的id列的值是相同的，出现在前边的表表示驱动表，出现在后边的表表示被驱动表。所以从上边的EXPLAIN输出中我们可以看出，查询优化器准备让s1表作为驱动表，让s2表作为被驱动表来执行查询。 对于包含子查询的查询语句来说，就可能涉及多个SELECT关键字，所以在包含子查询的查询语句的执行计划中，每个SELECT关键字都会对应一个唯一的id值，比如这样： 从输出结果中我们可以看到，s1表在外层查询中，外层查询有一个独立的SELECT关键字，所以第一条记录的id值就是1，s2表在子查询中，子查询有一个独立的SELECT关键字，所以第二条记录的id值就是2。 查询优化器可能对涉及子查询的查询语句进行重写，从而转换为连接查询。所以如果我们想知道查询优化器对某个包含子查询的语句是否进行了重写，直接查看执行计划就好了，比如说： 虽然我们的查询语句是一个子查询，但是执行计划中s1和s2表对应的记录的id值全部是1，这就表明了查询优化器将子查询转换为了连接查询。 对于包含UNION子句的查询语句来说，每个SELECT关键字对应一个id值也是没错的，不过还是有点儿特别，比方说下边这个查询： 这个语句的执行计划的第三条记录是个什么？为什么id值是NULL，而且table名也不大对？UNION子句会把多个查询的结果集合并起来并对结果集中的记录进行去重，怎么去重呢？MySQL使用的是内部的临时表。正如上边的查询计划中所示，UNION子句是为了把id为1的查询和id为2的查询的结果集合并起来并去重，所以在内部创建了一个临时表（就是执行计划第三条记录的table列的名称），id为NULL表明这个临时表是为了合并两个查询的结果集而创建的。 跟UNION对比起来，UNION ALL就不需要为最终的结果集进行去重，它只是单纯的把多个查询的结果集中的记录合并成一个并返回给用户，所以也就不需要使用临时表。所以在包含UNION ALL子句的查询的执行计划中，就没有那个id为NULL的记录，如下所示： select_type通过上边的内容我们知道，一条大的查询语句里边可以包含若干个SELECT关键字，每个SELECT关键字代表着一个小的查询语句，而每个SELECT关键字的FROM子句中都可以包含若干张表（这些表用来做连接查询），每一张表都对应着执行计划输出中的一条记录，对于在同一个SELECT关键字中的表来说，它们的id值是相同的。 MySQL为每一个SELECT关键字代表的小查询都定义了一个称之为select_type的属性，意思是我们只要知道了某个小查询的select_type属性，就知道了这个小查询在整个大查询中有什么作用。 名称 描述 SIMPLE Simple SELECT (not using UNION or subqueries) PRIMARY Outermost SELECT UNION Second or later SELECT statement in a UNION UNION RESULT Result of a UNION SUBQUERY First SELECT in subquery DEPENDENT SUBQUERY First SELECT in subquery, dependent on outer query DEPENDENT UNION Second or later SELECT statement in a UNION, dependent on outer query DERIVED Derived table MATERIALIZED Materialized subquery UNCACHEABLE SUBQUERY A subquery for which the result cannot be cached and must be re-evaluated for each row of the outer query UNCACHEABLE UNION The second or later select in a UNION that belongs to an uncacheable subquery (see UNCACHEABLE SUBQUERY) SIMPLE查询语句中不包含UNION或者子查询的查询都算作是SIMPLE类型，比方说下边这个单表查询的select_type的值就是SIMPLE：当然，连接查询也算是SIMPLE类型，比如： PRIMARY对于包含UNION、UNION ALL或者子查询的大查询来说，它是由几个小查询组成的，其中最左边的那个查询的select_type值就是PRIMARY，比方说：从结果中可以看到，最左边的小查询SELECT * FROM s1对应的是执行计划中的第一条记录，它的select_type值就是PRIMARY。 UNION对于包含UNION或者UNION ALL的大查询来说，它是由几个小查询组成的，其中除了最左边的那个小查询以外，其余的小查询的select_type值就是UNION，可以对比上一个例子的效果，这就不多举例子了。 UNION RESULTMySQL选择使用临时表来完成UNION查询的去重工作，针对该临时表的查询的select_type就是UNION RESULT，例子上边有，就不赘述了。 SUBQUERY如果包含子查询的查询语句不能够转为对应的semi-join的形式，并且该子查询是不相关子查询，并且查询优化器决定采用将该子查询物化的方案来执行该子查询时，该子查询的第一个SELECT关键字代表的那个查询的select_type就是SUBQUERY，比如下边这个查询：可以看到，外层查询的select_type就是PRIMARY，子查询的select_type就是SUBQUERY。由于select_type为SUBQUERY的子查询会被物化，所以只需要执行一遍。 DEPENDENT SUBQUERY如果包含子查询的查询语句不能够转为对应的semi-join的形式，并且该子查询是相关子查询，则该子查询的第一个SELECT关键字代表的那个查询的select_type就是DEPENDENT SUBQUERY，比如下边这个查询： select_type为DEPENDENT SUBQUERY的查询可能会被执行多次。 DEPENDENT UNION在包含UNION或者UNION ALL的大查询中，如果各个小查询都依赖于外层查询的话，那除了最左边的那个小查询之外，其余的小查询的select_type的值就是DEPENDENT UNION。说的有些绕哈，比方说下边这个查询：这个查询比较复杂，大查询里包含了一个子查询，子查询里又是由UNION连起来的两个小查询。从执行计划中可以看出来，SELECT key1 FROM s2 WHERE key1 = &#39;a&#39;这个小查询由于是子查询中第一个查询，所以它的select_type是DEPENDENT SUBQUERY，而SELECT key1 FROM s1 WHERE key1 = &#39;b&#39;这个查询的select_type就是DEPENDENT UNION。 DERIVED对于采用物化的方式执行的包含派生表的查询，该派生表对应的子查询的select_type就是DERIVED，比方说下边这个查询：从执行计划中可以看出，id为2的记录就代表子查询的执行方式，它的select_type是DERIVED，说明该子查询是以物化的方式执行的。id为1的记录代表外层查询，它的table列显示的是``，表示该查询是针对将派生表物化之后的表进行查询的。 如果派生表可以通过和外层查询合并的方式执行的话，执行计划又是另一番景象。 MATERIALIZED当查询优化器在执行包含子查询的语句时，选择将子查询物化之后与外层查询进行连接查询时，该子查询对应的select_type属性就是MATERIALIZED，比如下边这个查询：执行计划的第三条记录的id值为2，说明该条记录对应的是一个单表查询，从它的select_type值为MATERIALIZED可以看出，查询优化器是要把子查询先转换成物化表。然后看执行计划的前两条记录的id值都为1，说明这两条记录对应的表进行连接查询，需要注意的是第二条记录的table列的值是``，说明该表其实就是id为2对应的子查询执行之后产生的物化表，然后将s1和该物化表进行连接查询。 UNCACHEABLE SUBQUERY不常用 UNCACHEABLE UNION不常用 partitions一般情况下我们的查询语句的执行计划的partitions列的值都是NULL。 type执行计划的一条记录就代表着MySQL对某个表的执行查询时的访问方法，其中的type列就表明了这个访问方法是什么，比方说下边这个查询： 可以看到type列的值是ref，表明MySQL即将使用ref访问方法来执行对s1表的查询。但是我们之前只分析过对使用InnoDB存储引擎的表进行单表访问的一些访问方法，完整的访问方法如下：system，const，eq_ref，ref，fulltext，ref_or_null，index_merge，unique_subquery，index_subquery，range，index，ALL。接下来我们详细看一下： system当表中只有一条记录并且该表使用的存储引擎的统计数据是精确的，比如MyISAM、Memory，那么对该表的访问方法就是system。比方说我们新建一个MyISAM表，并为其插入一条记录：然后我们看一下查询这个表的执行计划：可以看到type列的值就是system了。 12345mysql&gt; CREATE TABLE t(i int) Engine=MyISAM;Query OK, 0 rows affected (0.05 sec)mysql&gt; INSERT INTO t VALUES(1);Query OK, 1 row affected (0.01 sec) 把表改成使用InnoDB存储引擎，执行计划的type列是ALL。 const当我们根据主键或者唯一二级索引列与常数进行等值匹配时，对单表的访问方法就是const，比如： eq_ref在连接查询时，如果被驱动表是通过主键或者唯一二级索引列等值匹配的方式进行访问的（如果该主键或者唯一二级索引是联合索引的话，所有的索引列都必须进行等值比较），则对该被驱动表的访问方法就是eq_ref，比方说：从执行计划的结果中可以看出，MySQL打算将s1作为驱动表，s2作为被驱动表，重点关注s2的访问方法是eq_ref，表明在访问s2表的时候可以通过主键的等值匹配来进行访问。 ref当通过普通的二级索引列与常量进行等值匹配时来查询某个表，那么对该表的访问方法就可能是ref。 fulltext全文索引,意义不大。 ref_or_null当对普通二级索引进行等值匹配查询，该索引列的值也可以是NULL值时，那么对该表的访问方法就可能是ref_or_null，比如说： index_merge在某些场景下可以使用Intersection、Union、Sort-Union这三种索引合并的方式来执行查询，我们看一下执行计划中是怎么体现MySQL使用索引合并的方式来对某个表执行查询的：从执行计划的type列的值是index_merge就可以看出，MySQL打算使用索引合并的方式来执行对s1表的查询。 unique_subquery类似于两表连接中被驱动表的eq_ref访问方法，unique_subquery是针对在一些包含IN子查询的查询语句中，如果查询优化器决定将IN子查询转换为EXISTS子查询，而且子查询可以使用到主键进行等值匹配的话，那么该子查询执行计划的type列的值就是unique_subquery，比如下边的这个查询语句：可以看到执行计划的第二条记录的type值就是unique_subquery，说明在执行子查询时会使用到id列的索引。 index_subqueryindex_subquery与unique_subquery类似，只不过访问子查询中的表时使用的是普通的索引，比如这样： range如果使用索引获取某些范围区间的记录，那么就可能使用到range访问方法，比如下边的这个查询：或者： index当我们可以使用索引覆盖，但需要扫描全部的索引记录时，该表的访问方法就是index，比如这样：上述查询中的搜索列表中只有key_part2一个列，而且搜索条件中也只有key_part3一个列，这两个列又恰好包含在idx_key_part这个索引中，可是搜索条件key_part3不能直接使用该索引进行ref或者range方式的访问，只能扫描整个idx_key_part索引的记录，所以查询计划的type列的值就是index。 对于使用InnoDB存储引擎的表来说，二级索引的记录只包含索引列和主键列的值，而聚簇索引中包含用户定义的全部列以及一些隐藏列，所以扫描二级索引的代价比直接全表扫描，也就是扫描聚簇索引的代价更低一些。 ALL最熟悉的全表扫描直接看例子： 一般来说，这些访问方法按照我们介绍它们的顺序性能依次变差。其中除了All这个访问方法外，其余的访问方法都能用到索引，除了index_merge访问方法外，其余的访问方法都最多只能用到一个索引。 possible_keys和key在EXPLAIN语句输出的执行计划中，possible_keys列表示在某个查询语句中，对某个表执行单表查询时可能用到的索引有哪些，key列表示实际用到的索引有哪些，比方说下边这个查询： 上述执行计划的possible_keys列的值是idx_key1,idx_key3，表示该查询可能使用到idx_key1,idx_key3两个索引，然后key列的值是idx_key3，表示经过查询优化器计算使用不同索引的成本后，最后决定使用idx_key3来执行查询比较划算。 不过有一点比较特别，就是在使用index访问方法来查询某个表时，possible_keys列是空的，而key列展示的是实际使用到的索引，比如这样： 另外需要注意的一点是，possible_keys列中的值并不是越多越好，可能使用的索引越多，查询优化器计算查询成本时就得花费更长时间，所以如果可以的话，尽量删除那些用不到的索引。 key_lenkey_len列表示当优化器决定使用某个索引执行查询时，该索引记录的最大长度，它是由这三个部分构成的： 对于使用固定长度类型的索引列来说，它实际占用的存储空间的最大长度就是该固定值，对于指定字符集的变长类型的索引列来说，比如某个索引列的类型是VARCHAR(100)，使用的字符集是utf8，那么该列实际占用的最大存储空间就是100 × 3 = 300个字节。 如果该索引列可以存储NULL值，则key_len比不可以存储NULL值时多1个字节。 对于变长字段来说，都会有2个字节的空间来存储该变长列的实际长度。 比如下边这个查询： 由于id列的类型是INT，并且不可以存储NULL值，所以在使用该列的索引时key_len大小就是4。当索引列可以存储NULL值时，比如： 可以看到key_len列就变成了5，比使用id列的索引时多了1。 对于可变长度的索引列来说，比如下边这个查询： 由于key1列的类型是VARCHAR(100)，所以该列实际最多占用的存储空间就是300字节，又因为该列允许存储NULL值，所以key_len需要加1，又因为该列是可变长度列，所以key_len需要加2，所以最后ken_len的值就是303。 这里需要强调的一点是，执行计划的生成是在MySQL server层中的功能，并不是针对具体某个存储引擎的功能，MySQL在执行计划中输出key_len列主要是为了让我们区分某个使用联合索引的查询具体用了几个索引列，而不是为了准确的说明针对某个具体存储引擎存储变长字段的实际长度占用的空间到底是占用1个字节还是2个字节。比方说下边这个使用到联合索引idx_key_part的查询： 我们可以从执行计划的key_len列中看到值是303，这意味着MySQL在执行上述查询中只能用到idx_key_part索引的一个索引列，而下边这个查询： 这个查询的执行计划的ken_len列的值是606，说明执行这个查询的时候可以用到联合索引idx_key_part的两个索引列。 ref当使用索引列等值匹配的条件去执行查询时，也就是在访问方法是const、eq_ref、ref、ref_or_null、unique_subquery、index_subquery其中之一时，ref列展示的就是与索引列作等值匹配的东东是个啥，比如只是一个常数或者是某个列。 可以看到ref列的值是const，表明在使用idx_key1索引执行查询时，与key1列作等值匹配的对象是一个常数，当然有时候更复杂一点： 可以看到对被驱动表s2的访问方法是eq_ref，而对应的ref列的值是yhd.s1.id，这说明在对被驱动表进行访问时会用到PRIMARY索引，也就是聚簇索引与一个列进行等值匹配的条件，于s2表的id作等值匹配的对象就是yhd.s1.id列（注意这里把数据库名也写出来了）。 有的时候与索引列进行等值匹配的对象是一个函数，比方说下边这个查询： 我们看执行计划的第二条记录，可以看到对s2表采用ref访问方法执行查询，然后在查询计划的ref列里输出的是func，说明与s2表的key1列进行等值匹配的对象是一个函数。 rows如果查询优化器决定使用全表扫描的方式对某个表执行查询时，执行计划的rows列就代表预计需要扫描的行数，如果使用索引来执行查询时，执行计划的rows列就代表预计扫描的索引记录行数。比如下边这个查询： 我们看到执行计划的rows列的值是1，这意味着查询优化器在经过分析使用idx_key1进行查询的成本之后，觉得满足key1 &gt; &#39;z&#39;这个条件的记录只有1条。 filtered之前在分析连接查询的成本时提出过一个condition filtering的概念，就是MySQL在计算驱动表扇出时采用的一个策略： 如果使用的是全表扫描的方式执行的单表查询，那么计算驱动表扇出时需要估计出满足搜索条件的记录到底有多少条。 如果使用的是索引执行的单表扫描，那么计算驱动表扇出的时候需要估计出满足除使用到对应索引的搜索条件外的其他搜索条件的记录有多少条。 比方说下边这个查询： 从执行计划的key列中可以看出来，该查询使用idx_key1索引来执行查询，从rows列可以看出满足key1 &gt; &#39;z&#39;的记录有1条。执行计划的filtered列就代表查询优化器预测在这1条记录中，有多少条记录满足其余的搜索条件，也就是common_field = &#39;a&#39;这个条件的百分比。此处filtered列的值是10.00，说明查询优化器预测在1条记录中有10.00%的记录满足common_field = &#39;a&#39;这个条件。 对于单表查询来说，这个filtered列的值没什么意义，我们更关注在连接查询中驱动表对应的执行计划记录的filtered值，比方说下边这个查询： 从执行计划中可以看出来，查询优化器打算把s1当作驱动表，s2当作被驱动表。我们可以看到驱动表s1表的执行计划的rows列为997219， filtered列为10.00，这意味着驱动表s1的扇出值就是997219 × 10.00% = 99721.9，这说明还要对被驱动表执行大约99721.9次查询。 Extra顾名思义，Extra列是用来说明一些额外信息的，我们可以通过这些额外信息来更准确的理解MySQL到底将如何执行给定的查询语句。 No tables used当查询语句的没有FROM子句时将会提示该额外信息，比如： Impossible WHERE查询语句的WHERE子句永远为FALSE时将会提示该额外信息，比方说： No matching min/max row当查询列表处有MIN或者MAX聚集函数，但是并没有符合WHERE子句中的搜索条件的记录时，将会提示该额外信息，比方说： Using index当我们的查询列表以及搜索条件中只包含属于某个索引的列，也就是在可以使用索引覆盖的情况下，在Extra列将会提示该额外信息。比方说下边这个查询中只需要用到idx_key1而不需要回表操作： Using index condition有些搜索条件中虽然出现了索引列，但却不能使用到索引，比如下边这个查询：其中的key1 &gt; &#39;z&#39;可以使用到索引，但是key1 LIKE &#39;%a&#39;却无法使用到索引，在以前版本的MySQL中，是按照下边步骤来执行这个查询的： 1SELECT * FROM s1 WHERE key1 &gt; &#x27;z&#x27; AND key1 LIKE &#x27;%a&#x27;; 先根据key1 &gt; &#39;z&#39;这个条件，从二级索引idx_key1中获取到对应的二级索引记录。 根据上一步骤得到的二级索引记录中的主键值进行回表，找到完整的用户记录再检测该记录是否符合key1 LIKE &#39;%a&#39;这个条件，将符合条件的记录加入到最后的结果集。 但是虽然key1 LIKE &#39;%a&#39;不能组成范围区间参与range访问方法的执行，但这个条件毕竟只涉及到了key1列，所以MySQL把上边的步骤改进了一下： 先根据key1 &gt; &#39;z&#39;这个条件，定位到二级索引idx_key1中对应的二级索引记录。 对于指定的二级索引记录，先不着急回表，而是先检测一下该记录是否满足key1 LIKE &#39;%a&#39;这个条件，如果这个条件不满足，则该二级索引记录压根儿就没必要回表。 对于满足key1 LIKE &#39;%a&#39;这个条件的二级索引记录执行回表操作。 我们说回表操作其实是一个随机IO，比较耗时，所以上述修改虽然只改进了一点点，但是可以省去好多回表操作的成本。MySQL把他们的这个改进称之为索引条件下推（英文名：Index Condition Pushdown）。如果在查询语句的执行过程中将要使用索引条件下推这个特性，在Extra列中将会显示Using index condition，比如这样： Using where当我们使用全表扫描来执行对某个表的查询，并且该语句的WHERE子句中有针对该表的搜索条件时，在Extra列中会提示上述额外信息。比如下边这个查询：当使用索引访问来执行对某个表的查询，并且该语句的WHERE子句中有除了该索引包含的列之外的其他搜索条件时，在Extra列中也会提示上述额外信息。比如下边这个查询虽然使用idx_key1索引执行查询，但是搜索条件中除了包含key1的搜索条件key1 = &#39;a&#39;，还有包含common_field的搜索条件，所以Extra列会显示Using where的提示： Using join buffer (Block Nested Loop)在连接查询执行过程中，当被驱动表不能有效的利用索引加快访问速度，MySQL一般会为其分配一块名叫join buffer的内存块来加快查询速度，也就是我们所讲的基于块的嵌套循环算法，比如下边这个查询语句：可以在对s2表的执行计划的Extra列显示了两个提示： Using join buffer (Block Nested Loop)：这是因为对表s2的访问不能有效利用索引，只好退而求其次，使用join buffer来减少对s2表的访问次数，从而提高性能。 Using where：可以看到查询语句中有一个s1.common_field = s2.common_field条件，因为s1是驱动表，s2是被驱动表，所以在访问s2表时，s1.common_field的值已经确定下来了，所以实际上查询s2表的条件就是s2.common_field = 一个常数，所以提示了Using where额外信息。 Not exists当我们使用左（外）连接时，如果WHERE子句中包含要求被驱动表的某个列等于NULL值的搜索条件，而且那个列又是不允许存储NULL值的，那么在该表的执行计划的Extra列就会提示Not exists额外信息，比如这样：上述查询中s1表是驱动表，s2表是被驱动表，s2.id列是不允许存储NULL值的，而WHERE子句中又包含s2.id IS NULL的搜索条件，这意味着必定是驱动表的记录在被驱动表中找不到匹配ON子句条件的记录才会把该驱动表的记录加入到最终的结果集，所以对于某条驱动表中的记录来说，如果能在被驱动表中找到1条符合ON子句条件的记录，那么该驱动表的记录就不会被加入到最终的结果集，也就是说我们没有必要到被驱动表中找到全部符合ON子句条件的记录，这样可以稍微节省一点性能。 右（外）连接可以被转换为左（外）连接，所以就不提右（外）连接的情况了。 Using intersect(...)、Using union(...)和Using sort_union(...)如果执行计划的Extra列出现了Using intersect(...)提示，说明准备使用Intersect索引合并的方式执行查询，括号中的...表示需要进行索引合并的索引名称；如果出现了Using union(...)提示，说明准备使用Union索引合并的方式执行查询；出现了Using sort_union(...)提示，说明准备使用Sort-Union索引合并的方式执行查询。比如这个查询的执行计划：其中Extra列就显示了Using intersect(idx_key3,idx_key1)，表明MySQL即将使用idx_key3和idx_key1这两个索引进行Intersect索引合并的方式执行查询。 Zero limit当我们的LIMIT子句的参数为0时，表示不打算从表中读出任何记录，将会提示该额外信息，比如这样： Using filesort有一些情况下对结果集中的记录进行排序是可以使用到索引的，比如下边这个查询：这个查询语句可以利用idx_key1索引直接取出key1列的10条记录，然后再进行回表操作就好了。但是很多情况下排序操作无法使用到索引，只能在内存中（记录较少的时候）或者磁盘中（记录较多的时候）进行排序，MySQL把这种在内存中或者磁盘上进行排序的方式统称为文件排序（英文名：filesort）。如果某个查询需要使用文件排序的方式执行查询，就会在执行计划的Extra列中显示Using filesort提示，比如这样：需要注意的是，如果查询中需要使用filesort的方式进行排序的记录非常多，那么这个过程是很耗费性能的，我们最好想办法将使用文件排序的执行方式改为使用索引进行排序。 Using temporary在许多查询的执行过程中，MySQL可能会借助临时表来完成一些功能，比如去重、排序之类的，比如我们在执行许多包含DISTINCT、GROUP BY、UNION等子句的查询过程中，如果不能有效利用索引来完成查询，MySQL很有可能寻求通过建立内部的临时表来执行查询。如果查询中使用到了内部的临时表，在执行计划的Extra列将会显示Using temporary提示，比方说这样：再比如：不知道大家注意到没有，上述执行计划的Extra列不仅仅包含Using temporary提示，还包含Using filesort提示，可是我们的查询语句中明明没有写ORDER BY子句呀？这是因为MySQL会在包含GROUP BY子句的查询中默认添加上ORDER BY子句，也就是说上述查询其实和下边这个查询等价： 1EXPLAIN SELECT common_field, COUNT(*) AS amount FROM s1 GROUP BY common_field ORDER BY common_field; 如果我们并不想为包含GROUP BY子句的查询进行排序，需要我们显式的写上ORDER BY NULL，就像这样：​ ​ 这回执行计划中就没有Using filesort的提示了，也就意味着执行查询时可以省去对记录进行文件排序的成本了。​ 另外，执行计划中出现Using temporary并不是一个好的征兆，因为建立与维护临时表要付出很大成本的，所以我们最好能使用索引来替代掉使用临时表，比方说下边这个包含GROUP BY子句的查询就不需要使用临时表：​ ​ 从Extra的Using index的提示里我们可以看出，上述查询只需要扫描idx_key1索引就可以搞定了，不再需要临时表了。 Start temporary, End temporary查询优化器会优先尝试将IN子查询转换成semi-join，而semi-join又有好多种执行策略，当执行策略为DuplicateWeedout时，也就是通过建立临时表来实现为外层查询中的记录进行去重操作时，驱动表查询执行计划的Extra列将显示Start temporary提示，被驱动表查询执行计划的Extra列将显示End temporary提示，就是这样： LooseScan在将In子查询转为semi-join时，如果采用的是LooseScan执行策略，则在驱动表执行计划的Extra列就是显示LooseScan提示，比如这样： FirstMatch(tbl_name)在将In子查询转为semi-join时，如果采用的是FirstMatch执行策略，则在被驱动表执行计划的Extra列就是显示FirstMatch(tbl_name)提示，比如这样： 2.Json格式的执行计划我们上边介绍的EXPLAIN语句输出中缺少了一个衡量执行计划好坏的重要属性 —— 成本。不过MySQL贴心的为我们提供了一种查看某个执行计划花费的成本的方式： 在EXPLAIN单词和真正的查询语句中间加上FORMAT=JSON。 这样我们就可以得到一个json格式的执行计划，里边儿包含该计划花费的成本，比如这样： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586mysql&gt; EXPLAIN FORMAT=JSON SELECT * FROM s1 INNER JOIN s2 ON s1.key1 = s2.key2 WHERE s1.common_field = &#x27;a&#x27;\\G*************************** 1. row ***************************EXPLAIN: &#123; &quot;query_block&quot;: &#123; &quot;select_id&quot;: 1, # 整个查询语句只有1个SELECT关键字，该关键字对应的id号为1 &quot;cost_info&quot;: &#123; &quot;query_cost&quot;: &quot;3197.16&quot; # 整个查询的执行成本预计为3197.16 &#125;, &quot;nested_loop&quot;: [ # 几个表之间采用嵌套循环连接算法执行 # 以下是参与嵌套循环连接算法的各个表的信息 &#123; &quot;table&quot;: &#123; &quot;table_name&quot;: &quot;s1&quot;, # s1表是驱动表 &quot;access_type&quot;: &quot;ALL&quot;, # 访问方法为ALL，意味着使用全表扫描访问 &quot;possible_keys&quot;: [ # 可能使用的索引 &quot;idx_key1&quot; ], &quot;rows_examined_per_scan&quot;: 9688, # 查询一次s1表大致需要扫描9688条记录 &quot;rows_produced_per_join&quot;: 968, # 驱动表s1的扇出是968 &quot;filtered&quot;: &quot;10.00&quot;, # condition filtering代表的百分比 &quot;cost_info&quot;: &#123; &quot;read_cost&quot;: &quot;1840.84&quot;, # 稍后解释 &quot;eval_cost&quot;: &quot;193.76&quot;, # 稍后解释 &quot;prefix_cost&quot;: &quot;2034.60&quot;, # 单次查询s1表总共的成本 &quot;data_read_per_join&quot;: &quot;1M&quot; # 读取的数据量 &#125;, &quot;used_columns&quot;: [ # 执行查询中涉及到的列 &quot;id&quot;, &quot;key1&quot;, &quot;key2&quot;, &quot;key3&quot;, &quot;key_part1&quot;, &quot;key_part2&quot;, &quot;key_part3&quot;, &quot;common_field&quot; ], # 对s1表访问时针对单表查询的条件 &quot;attached_condition&quot;: &quot;((`xiaohaizi`.`s1`.`common_field` = &#x27;a&#x27;) and (`xiaohaizi`.`s1`.`key1` is not null))&quot; &#125; &#125;, &#123; &quot;table&quot;: &#123; &quot;table_name&quot;: &quot;s2&quot;, # s2表是被驱动表 &quot;access_type&quot;: &quot;ref&quot;, # 访问方法为ref，意味着使用索引等值匹配的方式访问 &quot;possible_keys&quot;: [ # 可能使用的索引 &quot;idx_key2&quot; ], &quot;key&quot;: &quot;idx_key2&quot;, # 实际使用的索引 &quot;used_key_parts&quot;: [ # 使用到的索引列 &quot;key2&quot; ], &quot;key_length&quot;: &quot;5&quot;, # key_len &quot;ref&quot;: [ # 与key2列进行等值匹配的对象 &quot;xiaohaizi.s1.key1&quot; ], &quot;rows_examined_per_scan&quot;: 1, # 查询一次s2表大致需要扫描1条记录 &quot;rows_produced_per_join&quot;: 968, # 被驱动表s2的扇出是968（由于后边没有多余的表进行连接，所以这个值也没啥用） &quot;filtered&quot;: &quot;100.00&quot;, # condition filtering代表的百分比 # s2表使用索引进行查询的搜索条件 &quot;index_condition&quot;: &quot;(`xiaohaizi`.`s1`.`key1` = `xiaohaizi`.`s2`.`key2`)&quot;, &quot;cost_info&quot;: &#123; &quot;read_cost&quot;: &quot;968.80&quot;, # 稍后解释 &quot;eval_cost&quot;: &quot;193.76&quot;, # 稍后解释 &quot;prefix_cost&quot;: &quot;3197.16&quot;, # 单次查询s1、多次查询s2表总共的成本 &quot;data_read_per_join&quot;: &quot;1M&quot; # 读取的数据量 &#125;, &quot;used_columns&quot;: [ # 执行查询中涉及到的列 &quot;id&quot;, &quot;key1&quot;, &quot;key2&quot;, &quot;key3&quot;, &quot;key_part1&quot;, &quot;key_part2&quot;, &quot;key_part3&quot;, &quot;common_field&quot; ] &#125; &#125; ] &#125;&#125;1 row in set, 2 warnings (0.00 sec) &quot;cost_info&quot;里边的成本是怎么计算出来的？先看s1表的&quot;cost_info&quot;部分： 123456&quot;cost_info&quot;: &#123; &quot;read_cost&quot;: &quot;1840.84&quot;, &quot;eval_cost&quot;: &quot;193.76&quot;, &quot;prefix_cost&quot;: &quot;2034.60&quot;, &quot;data_read_per_join&quot;: &quot;1M&quot;&#125; read_cost是由下边这两部分组成的： IO成本 检测rows × (1 - filter)条记录的CPU成本 rows和filter都是我们前边介绍执行计划的输出列，在JSON格式的执行计划中，rows相当于rows_examined_per_scan，filtered名称不变。 eval_cost是这样计算的：检测 rows × filter条记录的成本。 prefix_cost就是单独查询s1表的成本，也就是：read_cost + eval_cost data_read_per_join表示在此次查询中需要读取的数据量，我们就不多唠叨这个了。 其实没必要关注MySQL为啥使用这么古怪的方式计算出read_cost和eval_cost，关注prefix_cost是查询s1表的成本就好了。 对于s2表的&quot;cost_info&quot;部分是这样的： 123456&quot;cost_info&quot;: &#123; &quot;read_cost&quot;: &quot;968.80&quot;, &quot;eval_cost&quot;: &quot;193.76&quot;, &quot;prefix_cost&quot;: &quot;3197.16&quot;, &quot;data_read_per_join&quot;: &quot;1M&quot;&#125; 由于s2表是被驱动表，所以可能被读取多次，这里的read_cost和eval_cost是访问多次s2表后累加起来的值，主要关注里边儿的prefix_cost的值代表的是整个连接查询预计的成本，也就是单次查询s1表和多次查询s2表后的成本的和，也就是： 1968.80 + 193.76 + 2034.60 = 3197.16 3.Extented EXPLAIN在我们使用EXPLAIN语句查看了某个查询的执行计划后，紧接着还可以使用SHOW WARNINGS语句查看与这个查询的执行计划有关的一些扩展信息，比如这样： 可以看到SHOW WARNINGS展示出来的信息有三个字段，分别是Level、Code、Message。我们最常见的就是Code为1003的信息，当Code值为1003时，Message字段展示的信息类似于查询优化器将我们的查询语句重写后的语句。比如我们上边的查询本来是一个左（外）连接查询，但是有一个s2.common_field IS NOT NULL的条件，着就会导致查询优化器把左（外）连接查询优化为内连接查询，从SHOW WARNINGS的Message字段也可以看出来，原本的LEFT JOIN已经变成了JOIN。 我们说Message字段展示的信息类似于查询优化器将我们的查询语句重写后的语句，并不是等价于，也就是说Message字段展示的信息并不是标准的查询语句，在很多情况下并不能直接运行，它只能作为帮助我们理解查MySQL将如何执行查询语句的一个参考依据而已。 二，optimizer trace对于MySQL 5.6以及之前的版本来说，查询优化器就像是一个黑盒子一样，只能通过EXPLAIN语句查看到最后优化器决定使用的执行计划，却无法知道它为什么做这个决策。 在MySQL 5.6以及之后的版本中，MySQL提出了一个optimizer trace的功能，这个功能可以让我们方便的查看优化器生成执行计划的整个过程，这个功能的开启与关闭由系统变量optimizer_trace决定，我们看一下： 1234567mysql&gt; SHOW VARIABLES LIKE &#x27;optimizer_trace&#x27;;+-----------------+--------------------------+| Variable_name | Value |+-----------------+--------------------------+| optimizer_trace | enabled=off,one_line=off |+-----------------+--------------------------+1 row in set (0.02 sec) 可以看到enabled值为off，表明这个功能默认是关闭的。 one_line的值是控制输出格式的，如果为on那么所有输出都将在一行中展示，不适合人阅读，所以我们就保持其默认值为off吧。 如果想打开这个功能，必须首先把enabled的值改为on，就像这样： 12mysql&gt; SET optimizer_trace=&quot;enabled=on&quot;;Query OK, 0 rows affected (0.00 sec) 然后我们就可以输入我们想要查看优化过程的查询语句，当该查询语句执行完成后，就可以到information_schema数据库下的OPTIMIZER_TRACE表中查看完整的优化过程。这个OPTIMIZER_TRACE表有4个列，分别是： QUERY：表示我们的查询语句。 TRACE：表示优化过程的JSON格式文本。 MISSING_BYTES_BEYOND_MAX_MEM_SIZE：由于优化过程可能会输出很多，如果超过某个限制时，多余的文本将不会被显示，这个字段展示了被忽略的文本字节数。 INSUFFICIENT_PRIVILEGES：表示是否没有权限查看优化过程，默认值是0，只有某些特殊情况下才会是1，我们暂时不关心这个字段的值。 完整的使用optimizer trace功能的步骤总结如下： 1234567891011121314# 1. 打开optimizer trace功能 (默认情况下它是关闭的):SET optimizer_trace=&quot;enabled=on&quot;;# 2. 这里输入查询语句SELECT ...; # 3. 从OPTIMIZER_TRACE表中查看上一个查询的优化过程SELECT * FROM information_schema.OPTIMIZER_TRACE;# 4. 可能还要观察其他语句执行的优化过程，重复上边的第2、3步...# 5. 当停止查看语句的优化过程时，把optimizer trace功能关闭SET optimizer_trace=&quot;enabled=off&quot;; 现在我们有一个搜索条件比较多的查询语句，它的执行计划如下： 可以看到该查询可能使用到的索引有3个，那么为什么优化器最终选择了idx_key2而不选择其他的索引或者直接全表扫描呢？这时候就可以通过otpimzer trace功能来查看优化器的具体工作过程： 123456789SET optimizer_trace=&quot;enabled=on&quot;;SELECT * FROM s1 WHERE key1 &gt; &#x27;z&#x27; AND key2 &lt; 1000000 AND key3 IN (&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;) AND common_field = &#x27;abc&#x27;; SELECT * FROM information_schema.OPTIMIZER_TRACE\\G 直接看一下通过查询OPTIMIZER_TRACE表得到的输出： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281*************************** 1. row ***************************# 分析的查询语句是什么QUERY: SELECT * FROM s1 WHERE key1 &gt; &#x27;z&#x27; AND key2 &lt; 1000000 AND key3 IN (&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;) AND common_field = &#x27;abc&#x27;# 优化的具体过程TRACE: &#123; &quot;steps&quot;: [ &#123; &quot;join_preparation&quot;: &#123; # prepare阶段 &quot;select#&quot;: 1, &quot;steps&quot;: [ &#123; &quot;IN_uses_bisection&quot;: true &#125;, &#123; &quot;expanded_query&quot;: &quot;/* select#1 */ select `s1`.`id` AS `id`,`s1`.`key1` AS `key1`,`s1`.`key2` AS `key2`,`s1`.`key3` AS `key3`,`s1`.`key_part1` AS `key_part1`,`s1`.`key_part2` AS `key_part2`,`s1`.`key_part3` AS `key_part3`,`s1`.`common_field` AS `common_field` from `s1` where ((`s1`.`key1` &gt; &#x27;z&#x27;) and (`s1`.`key2` &lt; 1000000) and (`s1`.`key3` in (&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;)) and (`s1`.`common_field` = &#x27;abc&#x27;))&quot; &#125; ] /* steps */ &#125; /* join_preparation */ &#125;, &#123; &quot;join_optimization&quot;: &#123; # optimize阶段 &quot;select#&quot;: 1, &quot;steps&quot;: [ &#123; &quot;condition_processing&quot;: &#123; # 处理搜索条件 &quot;condition&quot;: &quot;WHERE&quot;, # 原始搜索条件 &quot;original_condition&quot;: &quot;((`s1`.`key1` &gt; &#x27;z&#x27;) and (`s1`.`key2` &lt; 1000000) and (`s1`.`key3` in (&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;)) and (`s1`.`common_field` = &#x27;abc&#x27;))&quot;, &quot;steps&quot;: [ &#123; # 等值传递转换 &quot;transformation&quot;: &quot;equality_propagation&quot;, &quot;resulting_condition&quot;: &quot;((`s1`.`key1` &gt; &#x27;z&#x27;) and (`s1`.`key2` &lt; 1000000) and (`s1`.`key3` in (&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;)) and (`s1`.`common_field` = &#x27;abc&#x27;))&quot; &#125;, &#123; # 常量传递转换 &quot;transformation&quot;: &quot;constant_propagation&quot;, &quot;resulting_condition&quot;: &quot;((`s1`.`key1` &gt; &#x27;z&#x27;) and (`s1`.`key2` &lt; 1000000) and (`s1`.`key3` in (&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;)) and (`s1`.`common_field` = &#x27;abc&#x27;))&quot; &#125;, &#123; # 去除没用的条件 &quot;transformation&quot;: &quot;trivial_condition_removal&quot;, &quot;resulting_condition&quot;: &quot;((`s1`.`key1` &gt; &#x27;z&#x27;) and (`s1`.`key2` &lt; 1000000) and (`s1`.`key3` in (&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;)) and (`s1`.`common_field` = &#x27;abc&#x27;))&quot; &#125; ] /* steps */ &#125; /* condition_processing */ &#125;, &#123; # 替换虚拟生成列 &quot;substitute_generated_columns&quot;: &#123; &#125; /* substitute_generated_columns */ &#125;, &#123; # 表的依赖信息 &quot;table_dependencies&quot;: [ &#123; &quot;table&quot;: &quot;`s1`&quot;, &quot;row_may_be_null&quot;: false, &quot;map_bit&quot;: 0, &quot;depends_on_map_bits&quot;: [ ] /* depends_on_map_bits */ &#125; ] /* table_dependencies */ &#125;, &#123; &quot;ref_optimizer_key_uses&quot;: [ ] /* ref_optimizer_key_uses */ &#125;, &#123; # 预估不同单表访问方法的访问成本 &quot;rows_estimation&quot;: [ &#123; &quot;table&quot;: &quot;`s1`&quot;, &quot;range_analysis&quot;: &#123; &quot;table_scan&quot;: &#123; # 全表扫描的行数以及成本 &quot;rows&quot;: 9688, &quot;cost&quot;: 2036.7 &#125; /* table_scan */, # 分析可能使用的索引 &quot;potential_range_indexes&quot;: [ &#123; &quot;index&quot;: &quot;PRIMARY&quot;, # 主键不可用 &quot;usable&quot;: false, &quot;cause&quot;: &quot;not_applicable&quot; &#125;, &#123; &quot;index&quot;: &quot;idx_key2&quot;, # idx_key2可能被使用 &quot;usable&quot;: true, &quot;key_parts&quot;: [ &quot;key2&quot; ] /* key_parts */ &#125;, &#123; &quot;index&quot;: &quot;idx_key1&quot;, # idx_key1可能被使用 &quot;usable&quot;: true, &quot;key_parts&quot;: [ &quot;key1&quot;, &quot;id&quot; ] /* key_parts */ &#125;, &#123; &quot;index&quot;: &quot;idx_key3&quot;, # idx_key3可能被使用 &quot;usable&quot;: true, &quot;key_parts&quot;: [ &quot;key3&quot;, &quot;id&quot; ] /* key_parts */ &#125;, &#123; &quot;index&quot;: &quot;idx_key_part&quot;, # idx_keypart不可用 &quot;usable&quot;: false, &quot;cause&quot;: &quot;not_applicable&quot; &#125; ] /* potential_range_indexes */, &quot;setup_range_conditions&quot;: [ ] /* setup_range_conditions */, &quot;group_index_range&quot;: &#123; &quot;chosen&quot;: false, &quot;cause&quot;: &quot;not_group_by_or_distinct&quot; &#125; /* group_index_range */, # 分析各种可能使用的索引的成本 &quot;analyzing_range_alternatives&quot;: &#123; &quot;range_scan_alternatives&quot;: [ &#123; # 使用idx_key2的成本分析 &quot;index&quot;: &quot;idx_key2&quot;, # 使用idx_key2的范围区间 &quot;ranges&quot;: [ &quot;NULL &lt; key2 &lt; 1000000&quot; ] /* ranges */, &quot;index_dives_for_eq_ranges&quot;: true, # 是否使用index dive &quot;rowid_ordered&quot;: false, # 使用该索引获取的记录是否按照主键排序 &quot;using_mrr&quot;: false, # 是否使用mrr &quot;index_only&quot;: false, # 是否是索引覆盖访问 &quot;rows&quot;: 12, # 使用该索引获取的记录条数 &quot;cost&quot;: 15.41, # 使用该索引的成本 &quot;chosen&quot;: true # 是否选择该索引 &#125;, &#123; # 使用idx_key1的成本分析 &quot;index&quot;: &quot;idx_key1&quot;, # 使用idx_key1的范围区间 &quot;ranges&quot;: [ &quot;z &lt; key1&quot; ] /* ranges */, &quot;index_dives_for_eq_ranges&quot;: true, # 同上 &quot;rowid_ordered&quot;: false, # 同上 &quot;using_mrr&quot;: false, # 同上 &quot;index_only&quot;: false, # 同上 &quot;rows&quot;: 266, # 同上 &quot;cost&quot;: 320.21, # 同上 &quot;chosen&quot;: false, # 同上 &quot;cause&quot;: &quot;cost&quot; # 因为成本太大所以不选择该索引 &#125;, &#123; # 使用idx_key3的成本分析 &quot;index&quot;: &quot;idx_key3&quot;, # 使用idx_key3的范围区间 &quot;ranges&quot;: [ &quot;a &lt;= key3 &lt;= a&quot;, &quot;b &lt;= key3 &lt;= b&quot;, &quot;c &lt;= key3 &lt;= c&quot; ] /* ranges */, &quot;index_dives_for_eq_ranges&quot;: true, # 同上 &quot;rowid_ordered&quot;: false, # 同上 &quot;using_mrr&quot;: false, # 同上 &quot;index_only&quot;: false, # 同上 &quot;rows&quot;: 21, # 同上 &quot;cost&quot;: 28.21, # 同上 &quot;chosen&quot;: false, # 同上 &quot;cause&quot;: &quot;cost&quot; # 同上 &#125; ] /* range_scan_alternatives */, # 分析使用索引合并的成本 &quot;analyzing_roworder_intersect&quot;: &#123; &quot;usable&quot;: false, &quot;cause&quot;: &quot;too_few_roworder_scans&quot; &#125; /* analyzing_roworder_intersect */ &#125; /* analyzing_range_alternatives */, # 对于上述单表查询s1最优的访问方法 &quot;chosen_range_access_summary&quot;: &#123; &quot;range_access_plan&quot;: &#123; &quot;type&quot;: &quot;range_scan&quot;, &quot;index&quot;: &quot;idx_key2&quot;, &quot;rows&quot;: 12, &quot;ranges&quot;: [ &quot;NULL &lt; key2 &lt; 1000000&quot; ] /* ranges */ &#125; /* range_access_plan */, &quot;rows_for_plan&quot;: 12, &quot;cost_for_plan&quot;: 15.41, &quot;chosen&quot;: true &#125; /* chosen_range_access_summary */ &#125; /* range_analysis */ &#125; ] /* rows_estimation */ &#125;, &#123; # 分析各种可能的执行计划 #（对多表查询这可能有很多种不同的方案，单表查询的方案上边已经分析过了，直接选取idx_key2就好） &quot;considered_execution_plans&quot;: [ &#123; &quot;plan_prefix&quot;: [ ] /* plan_prefix */, &quot;table&quot;: &quot;`s1`&quot;, &quot;best_access_path&quot;: &#123; &quot;considered_access_paths&quot;: [ &#123; &quot;rows_to_scan&quot;: 12, &quot;access_type&quot;: &quot;range&quot;, &quot;range_details&quot;: &#123; &quot;used_index&quot;: &quot;idx_key2&quot; &#125; /* range_details */, &quot;resulting_rows&quot;: 12, &quot;cost&quot;: 17.81, &quot;chosen&quot;: true &#125; ] /* considered_access_paths */ &#125; /* best_access_path */, &quot;condition_filtering_pct&quot;: 100, &quot;rows_for_plan&quot;: 12, &quot;cost_for_plan&quot;: 17.81, &quot;chosen&quot;: true &#125; ] /* considered_execution_plans */ &#125;, &#123; # 尝试给查询添加一些其他的查询条件 &quot;attaching_conditions_to_tables&quot;: &#123; &quot;original_condition&quot;: &quot;((`s1`.`key1` &gt; &#x27;z&#x27;) and (`s1`.`key2` &lt; 1000000) and (`s1`.`key3` in (&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;)) and (`s1`.`common_field` = &#x27;abc&#x27;))&quot;, &quot;attached_conditions_computation&quot;: [ ] /* attached_conditions_computation */, &quot;attached_conditions_summary&quot;: [ &#123; &quot;table&quot;: &quot;`s1`&quot;, &quot;attached&quot;: &quot;((`s1`.`key1` &gt; &#x27;z&#x27;) and (`s1`.`key2` &lt; 1000000) and (`s1`.`key3` in (&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;)) and (`s1`.`common_field` = &#x27;abc&#x27;))&quot; &#125; ] /* attached_conditions_summary */ &#125; /* attaching_conditions_to_tables */ &#125;, &#123; # 再稍稍的改进一下执行计划 &quot;refine_plan&quot;: [ &#123; &quot;table&quot;: &quot;`s1`&quot;, &quot;pushed_index_condition&quot;: &quot;(`s1`.`key2` &lt; 1000000)&quot;, &quot;table_condition_attached&quot;: &quot;((`s1`.`key1` &gt; &#x27;z&#x27;) and (`s1`.`key3` in (&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;)) and (`s1`.`common_field` = &#x27;abc&#x27;))&quot; &#125; ] /* refine_plan */ &#125; ] /* steps */ &#125; /* join_optimization */ &#125;, &#123; &quot;join_execution&quot;: &#123; # execute阶段 &quot;select#&quot;: 1, &quot;steps&quot;: [ ] /* steps */ &#125; /* join_execution */ &#125; ] /* steps */&#125;# 因优化过程文本太多而丢弃的文本字节大小，值为0时表示并没有丢弃MISSING_BYTES_BEYOND_MAX_MEM_SIZE: 0# 权限字段INSUFFICIENT_PRIVILEGES: 01 row in set (0.00 sec) 这只是优化器执行过程中的一小部分，MySQL可能会在之后的版本中添加更多的优化过程信息。不过杂乱之中其实还是蛮有规律的，优化过程大致分为了三个阶段： prepare阶段 optimize阶段 execute阶段 我们所说的基于成本的优化主要集中在optimize阶段，对于单表查询来说，我们主要关注optimize阶段的&quot;rows_estimation&quot;这个过程，这个过程深入分析了对单表查询的各种执行方案的成本；对于多表连接查询来说，我们更多需要关注&quot;considered_execution_plans&quot;这个过程，这个过程里会写明各种不同的连接方式所对应的成本。反正优化器最终会选择成本最低的那种方案来作为最终的执行计划，也就是我们使用EXPLAIN语句所展现出的那种方案。 如果对使用EXPLAIN语句展示出的对某个查询的执行计划很不理解，可以尝试使用optimizer trace功能来详细了解每一种执行方案对应的成本。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/tags/MySQL/"}]},{"title":"MySQL[九]基于规则的优化&子查询优化","slug":"MySQL/MySQL[九]基于规则的优化&子查询优化","date":"2022-01-08T16:00:00.000Z","updated":"2022-01-12T00:41:59.912Z","comments":true,"path":"2022/01/09/MySQL/MySQL[九]基于规则的优化&子查询优化/","link":"","permalink":"https://yinhuidong.github.io/2022/01/09/MySQL/MySQL[%E4%B9%9D]%E5%9F%BA%E4%BA%8E%E8%A7%84%E5%88%99%E7%9A%84%E4%BC%98%E5%8C%96&%E5%AD%90%E6%9F%A5%E8%AF%A2%E4%BC%98%E5%8C%96/","excerpt":"","text":"MySQL依据一些规则，把语句转换成某种可以比较高效执行的形式，这个过程也可以被称作查询重写。 1.条件化简我们编写的查询语句的搜索条件本质上是一个表达式，这些表达式可能比较繁杂，或者不能高效的执行，MySQL的查询优化器会为我们简化这些表达式。 1.1移除不必要的括号有时候表达式里有许多无用的括号，比如这样： 1((a = 5 AND b = c) OR ((a &gt; c) AND (c &lt; 5))) 优化器会把那些用不到的括号给干掉，就是这样： 1(a = 5 and b = c) OR (a &gt; c AND c &lt; 5) 1.2常量传递有时候某个表达式是某个列和某个常量做等值匹配，比如这样： 1a = 5 当这个表达式和其他涉及列a的表达式使用AND连接起来时，可以将其他表达式中的a的值替换为5，比如这样： 1a = 5 AND b &gt; a 就可以被转换为： 1a = 5 AND b &gt; 5 用OR的表达式不能进行能量传递是因为OR两边的条件是取并集的，或者说互不相关。 1.3 等值传递有时候多个列之间存在等值匹配的关系，比如这样： 1a = b and b = c and c = 5 这个表达式可以被简化为： 1a = 5 and b = 5 and c = 5 1.4移除没用的条件对于一些明显永远为TRUE或者FALSE的表达式，优化器会移除掉它们，比如这个表达式： 1(a &lt; 1 and b = b) OR (a = 6 OR 5 != 5) 很明显，b = b这个表达式永远为TRUE，5 != 5这个表达式永远为FALSE，所以简化后的表达式就是这样的： 1(a &lt; 1 and TRUE) OR (a = 6 OR FALSE) 可以继续被简化为 1a &lt; 1 OR a = 6 1.5表达式计算在查询开始执行之前，如果表达式中只包含常量的话，它的值会被先计算出来，比如这个： 1a = 5 + 1 因为5 + 1这个表达式只包含常量，所以就会被化简成： 1a = 6 但是这里需要注意的是，如果某个列并不是以单独的形式作为表达式的操作数时，比如出现在函数中，出现在某个更复杂表达式中，就像这样： 1ABS(a) &gt; 5 或者： 1-a &lt; -8 优化器是不会尝试对这些表达式进行化简的。我们前边说过只有搜索条件中索引列和常数使用某些运算符连接起来才可能使用到索引，所以如果可以的话，最好让索引列以单独的形式出现在表达式中。 1.6HAVING&amp;WHERE子句的合并如果查询语句中没有出现诸如SUM、MAX等等的聚集函数以及GROUP BY子句，优化器就把HAVING子句和WHERE子句合并起来。 1.7常量表检测MySQL觉得下边这两种查询运行的特别快： 查询的表中一条记录没有，或者只有一条记录。 使用主键等值匹配或者唯一二级索引列等值匹配作为搜索条件来查询某个表。 MySQL觉得这两种查询花费的时间特别少，少到可以忽略，所以也把通过这两种方式查询的表称之为常量表（英文名：constant tables）。优化器在分析一个查询语句时，先首先执行常量表查询，然后把查询中涉及到该表的条件全部替换成常数，最后再分析其余表的查询成本，比方说这个查询语句： 123SELECT * FROM table1 INNER JOIN table2 ON table1.column1 = table2.column2 WHERE table1.primary_key = 1; 很明显，这个查询可以使用主键和常量值的等值匹配来查询table1表，也就是在这个查询中table1表相当于常量表，在分析对table2表的查询成本之前，就会执行对table1表的查询，并把查询中涉及table1表的条件都替换掉，也就是上边的语句会被转换成这样： 12SELECT table1表记录的各个字段的常量值, table2.* FROM table1 INNER JOIN table2 ON table1表column1列的常量值 = table2.column2; 2.外连接消除内连接的驱动表和被驱动表的位置可以相互转换，而左（外）连接和右（外）连接的驱动表和被驱动表是固定的。这就导致内连接可能通过优化表的连接顺序来降低整体的查询成本，而外连接却无法优化表的连接顺序。我在之前的文章创建了两个表： 123456789CREATE TABLE t1 ( m1 int, n1 char(1)) Engine=InnoDB, CHARSET=utf8;CREATE TABLE t2 ( m2 int, n2 char(1)) Engine=InnoDB, CHARSET=utf8; 再看一下表的数据 12345678910111213141516171819mysql&gt; SELECT * FROM t1;+------+------+| m1 | n1 |+------+------+| 1 | a || 2 | b || 3 | c |+------+------+3 rows in set (0.00 sec)mysql&gt; SELECT * FROM t2;+------+------+| m2 | n2 |+------+------+| 2 | b || 3 | c || 4 | d |+------+------+3 rows in set (0.00 sec) 外连接和内连接的本质区别就是：对于外连接的驱动表的记录来说，如果无法在被驱动表中找到匹配ON子句中的过滤条件的记录，那么该记录仍然会被加入到结果集中，对应的被驱动表记录的各个字段使用NULL值填充；而内连接的驱动表的记录如果无法在被驱动表中找到匹配ON子句中的过滤条件的记录，那么该记录会被舍弃。查询效果就是这样： 123456789101112131415161718mysql&gt; SELECT * FROM t1 INNER JOIN t2 ON t1.m1 = t2.m2;+------+------+------+------+| m1 | n1 | m2 | n2 |+------+------+------+------+| 2 | b | 2 | b || 3 | c | 3 | c |+------+------+------+------+2 rows in set (0.00 sec)mysql&gt; SELECT * FROM t1 LEFT JOIN t2 ON t1.m1 = t2.m2;+------+------+------+------+| m1 | n1 | m2 | n2 |+------+------+------+------+| 2 | b | 2 | b || 3 | c | 3 | c || 1 | a | NULL | NULL |+------+------+------+------+3 rows in set (0.00 sec) 对于上边例子中的（左）外连接来说，由于驱动表t1中m1=1, n1=&#39;a&#39;的记录无法在被驱动表t2中找到符合ON子句条件t1.m1 = t2.m2的记录，所以就直接把这条记录加入到结果集，对应的t2表的m2和n2列的值都设置为NULL。 右（外）连接和左（外）连接其实只在驱动表的选取方式上是不同的，其余方面都是一样的，所以优化器会首先把右（外）连接查询转换成左（外）连接查询。 凡是不符合WHERE子句中条件的记录都不会参与连接。只要我们在搜索条件中指定关于被驱动表相关列的值不为NULL，那么外连接中在被驱动表中找不到符合ON子句条件的驱动表记录也就被排除出最后的结果集了，也就是说：在这种情况下：外连接和内连接也就没有什么区别了！比方说这个查询： 12345678mysql&gt; SELECT * FROM t1 LEFT JOIN t2 ON t1.m1 = t2.m2 WHERE t2.n2 IS NOT NULL;+------+------+------+------+| m1 | n1 | m2 | n2 |+------+------+------+------+| 2 | b | 2 | b || 3 | c | 3 | c |+------+------+------+------+2 rows in set (0.01 sec) 由于指定了被驱动表t2的n2列不允许为NULL，所以上边的t1和t2表的左（外）连接查询和内连接查询是一样一样的。当然，我们也可以不用显式的指定被驱动表的某个列IS NOT NULL，只要隐含的有这个意思就行了，比方说这样： 1234567mysql&gt; SELECT * FROM t1 LEFT JOIN t2 ON t1.m1 = t2.m2 WHERE t2.m2 = 2;+------+------+------+------+| m1 | n1 | m2 | n2 |+------+------+------+------+| 2 | b | 2 | b |+------+------+------+------+1 row in set (0.00 sec) 在这个例子中，我们在WHERE子句中指定了被驱动表t2的m2列等于2，也就相当于间接的指定了m2列不为NULL值，所以上边的这个左（外）连接查询其实和下边这个内连接查询是等价的： 1234567mysql&gt; SELECT * FROM t1 INNER JOIN t2 ON t1.m1 = t2.m2 WHERE t2.m2 = 2;+------+------+------+------+| m1 | n1 | m2 | n2 |+------+------+------+------+| 2 | b | 2 | b |+------+------+------+------+1 row in set (0.00 sec) 我们把这种在外连接查询中，指定的WHERE子句中包含被驱动表中的列不为NULL值的条件称之为空值拒绝（英文名：reject-NULL）。在被驱动表的WHERE子句符合空值拒绝的条件后，外连接和内连接可以相互转换。这种转换带来的好处就是查询优化器可以通过评估表的不同连接顺序的成本，选出成本最低的那种连接顺序来执行查询。 3.子查询优化3.1子查询语法3.1.1按返回的结果集区分子查询因为子查询本身也算是一个查询，所以可以按照它们返回的不同结果集类型而把这些子查询分为不同的类型： 标量子查询那些只返回一个单一值的子查询称之为标量子查询，比如这样：或者这样：这两个查询语句中的子查询都返回一个单一的值，也就是一个标量。这些标量子查询可以作为一个单一值或者表达式的一部分出现在查询语句的各个地方。 1SELECT (SELECT m1 FROM t1 LIMIT 1); 1SELECT * FROM t1 WHERE m1 = (SELECT MIN(m2) FROM t2); 行子查询顾名思义，就是返回一条记录的子查询，不过这条记录需要包含多个列（只包含一个列就成了标量子查询了）。比如这样：其中的(SELECT m2, n2 FROM t2 LIMIT 1)就是一个行子查询，整条语句的含义就是要从t1表中找一些记录，这些记录的m1和n1列分别等于子查询结果中的m2和n2列。 1SELECT * FROM t1 WHERE (m1, n1) = (SELECT m2, n2 FROM t2 LIMIT 1); 列子查询列子查询自然就是查询出一个列的数据喽，不过这个列的数据需要包含多条记录（只包含一条记录就成了标量子查询了）。比如这样：其中的(SELECT m2 FROM t2)就是一个列子查询，表明查询出t2表的m2列的值作为外层查询IN语句的参数。 1SELECT * FROM t1 WHERE m1 IN (SELECT m2 FROM t2); 表子查询顾名思义，就是子查询的结果既包含很多条记录，又包含很多个列，比如这样：其中的(SELECT m2, n2 FROM t2)就是一个表子查询，这里需要和行子查询对比一下，行子查询中我们用了LIMIT 1来保证子查询的结果只有一条记录，表子查询中不需要这个限制。 1SELECT * FROM t1 WHERE (m1, n1) IN (SELECT m2, n2 FROM t2); 3.1.2按与外层查询关系来区分子查询 不相关子查询如果子查询可以单独运行出结果，而不依赖于外层查询的值，我们就可以把这个子查询称之为不相关子查询。我们前边介绍的那些子查询全部都可以看作不相关子查询，所以也就不举例子了哈。 相关子查询如果子查询的执行需要依赖于外层查询的值，我们就可以把这个子查询称之为相关子查询。比如：例子中的子查询是(SELECT m2 FROM t2 WHERE n1 = n2)，可是这个查询中有一个搜索条件是n1 = n2，别忘了n1是表t1的列，也就是外层查询的列，也就是说子查询的执行需要依赖于外层查询的值，所以这个子查询就是一个相关子查询。 1SELECT * FROM t1 WHERE m1 IN (SELECT m2 FROM t2 WHERE n1 = n2); 3.1.3 子查询语法注意事项 子查询必须用小括号扩起来。 在SELECT子句中的子查询必须是标量子查询。 在想要得到标量子查询或者行子查询，但又不能保证子查询的结果集只有一条记录时，应该使用LIMIT 1语句来限制记录数量。 对于[NOT] IN/ANY/SOME/ALL子查询来说，子查询中不允许有LIMIT语句。比如这样是非法的：因为[NOT] IN/ANY/SOME/ALL子查询不支持LIMIT语句，所以子查询中的这些语句也就是多余的了： 1mysql&gt; SELECT * FROM t1 WHERE m1 IN (SELECT * FROM t2 LIMIT 2); ORDER BY子句：子查询的结果其实就相当于一个集合，集合里的值排不排序一点儿都不重要。 DISTINCT语句：集合里的值去不去重也没啥意义。 没有聚集函数以及HAVING子句的GROUP BY子句。 对于这些冗余的语句，查询优化器在一开始就把它们给干掉了。 不允许在一条语句中增删改某个表的记录时同时还对该表进行子查询。 1mysql&gt; DELETE FROM t1 WHERE m1 &lt; (SELECT MAX(m1) FROM t1); 3.2 子查询的执行还是复用前面的表： 123456789101112131415CREATE TABLE single_table ( id INT NOT NULL AUTO_INCREMENT, key1 VARCHAR(100), key2 INT, key3 VARCHAR(100), key_part1 VARCHAR(100), key_part2 VARCHAR(100), key_part3 VARCHAR(100), common_field VARCHAR(100), PRIMARY KEY (id), KEY idx_key1 (key1), UNIQUE KEY idx_key2 (key2), KEY idx_key3 (key3), KEY idx_key_part(key_part1, key_part2, key_part3)) Engine=InnoDB CHARSET=utf8; 我们假设有两个表s1、s2与这个single_table表的构造是相同的，而且这两个表里边儿有10000条记录。 3.2.1 标量/行子查询的执行方式我们经常在下边两个场景中使用到标量子查询或者行子查询： SELECT子句中，我们前边说过的在查询列表中的子查询必须是标量子查询。 子查询使用=、&gt;、&lt;、&gt;=、&lt;=、&lt;&gt;、!=、&lt;=&gt;等操作符和某个操作数组成一个布尔表达式，这样的子查询必须是标量子查询或者行子查询。 对于上述两种场景中的不相关标量子查询或者行子查询来说，它们的执行方式是简单的，比方说下边这个查询语句： 12SELECT * FROM s1 WHERE key1 = (SELECT common_field FROM s2 WHERE key3 = &#x27;a&#x27; LIMIT 1); 先单独执行(SELECT common_field FROM s2 WHERE key3 = &#39;a&#39; LIMIT 1)这个子查询。 然后在将上一步子查询得到的结果当作外层查询的参数再执行外层查询SELECT * FROM s1 WHERE key1 = ...。 也就是说，对于包含不相关的标量子查询或者行子查询的查询语句来说，MySQL会分别独立的执行外层查询和子查询，就当作两个单表查询就好了。 对于相关的标量子查询或者行子查询来说，比如下边这个查询： 12SELECT * FROM s1 WHERE key1 = (SELECT common_field FROM s2 WHERE s1.key3 = s2.key3 LIMIT 1); 3.2.2 IN子查询优化① 物化表的提出12SELECT * FROM s1 WHERE key1 IN (SELECT common_field FROM s2 WHERE key3 = &#x27;a&#x27;); 对于不相关的IN子查询来说，如果子查询的结果集中的记录条数很少，那么把子查询和外层查询分别看成两个单独的单表查询效率还行，但是如果单独执行子查询后的结果集太多的话，就会导致这些问题： 结果集太多，可能内存中都放不下。 对于外层查询来说，如果子查询的结果集太多，那就意味着IN子句中的参数特别多，这就导致： 无法有效的使用索引，只能对外层查询进行全表扫描。 在对外层查询执行全表扫描时，由于IN子句中的参数太多，这会导致检测一条记录是否符合和IN子句中的参数匹配花费的时间太长。 所以MySQL并不直接将不相关子查询的结果集当作外层查询的参数，而是将该结果集写入一个临时表里。 该临时表的列就是子查询结果集中的列。 写入临时表的记录会被去重。我们说IN语句是判断某个操作数在不在某个集合中，集合中的值重不重复对整个IN语句的结果并没有影响，所以我们在将结果集写入临时表时对记录进行去重可以让临时表变得更小。 临时表如何对记录进行去重？只要为表中记录的所有列建立主键或者唯一索引就好了嘛～ 一般情况下子查询结果集不会大的离谱，所以会为它建立基于内存的使用Memory存储引擎的临时表，而且会为该表建立哈希索引。如果子查询的结果集非常大，超过了系统变量tmp_table_size或者max_heap_table_size，临时表会转而使用基于磁盘的存储引擎来保存结果集中的记录，索引类型也对应转变为B+树索引。 IN语句的本质就是判断某个操作数在不在某个集合里，如果集合中的数据建立了哈希索引，那么这个匹配的过程就是超级快的。 MySQL把这个将子查询结果集中的记录保存到临时表的过程称之为物化。为了方便起见，我们就把那个存储子查询结果集的临时表称之为物化表。正因为物化表中的记录都建立了索引（基于内存的物化表有哈希索引，基于磁盘的有B+树索引），通过索引执行IN语句判断某个操作数在不在子查询结果集中变得非常快，从而提升了子查询语句的性能。 ② 物化表转连接再看一下最开始的那个查询语句： 12SELECT * FROM s1 WHERE key1 IN (SELECT common_field FROM s2 WHERE key3 = &#x27;a&#x27;); 当我们把子查询进行物化之后，假设子查询物化表的名称为materialized_table，该物化表存储的子查询结果集的列为m_val，那么这个查询其实可以从下边两种角度来看待： 从表s1的角度来看待，整个查询的意思其实是：对于s1表中的每条记录来说，如果该记录的key1列的值在子查询对应的物化表中，则该记录会被加入最终的结果集。画个图表示一下就是这样： 从子查询物化表的角度来看待，整个查询的意思其实是：对于子查询物化表的每个值来说，如果能在s1表中找到对应的key1列的值与该值相等的记录，那么就把这些记录加入到最终的结果集。画个图表示一下就是这样： 也就是说其实上边的查询就相当于表s1和子查询物化表materialized_table进行内连接： 1SELECT s1.* FROM s1 INNER JOIN materialized_table ON key1 = m_val; 转化成内连接之后就有意思了，查询优化器可以评估不同连接顺序需要的成本是多少，选取成本最低的那种查询方式执行查询。我们分析一下上述查询中使用外层查询的表s1和物化表materialized_table进行内连接的成本都是由哪几部分组成的： 如果使用s1表作为驱动表的话，总查询成本由下边几个部分组成： 物化子查询时需要的成本 扫描s1表时的成本 s1表中的记录数量 × 通过m_val = xxx对materialized_table表进行单表访问的成本（我们前边说过物化表中的记录是不重复的，并且为物化表中的列建立了索引，所以这个步骤显然是非常快的）。 如果使用materialized_table表作为驱动表的话，总查询成本由下边几个部分组成： 物化子查询时需要的成本 扫描物化表时的成本 物化表中的记录数量 × 通过key1 = xxx对s1表进行单表访问的成本（非常庆幸key1列上建立了索引，所以这个步骤是非常快的）。 MySQL查询优化器会通过运算来选择上述成本更低的方案来执行查询。 ③ 将子查询转换为semi-join虽然将子查询进行物化之后再执行查询都会有建立临时表的成本，但是不管怎么说，我们见识到了将子查询转换为连接的强大作用，能不能不进行物化操作直接把子查询转换为连接呢？ 12SELECT * FROM s1 WHERE key1 IN (SELECT common_field FROM s2 WHERE key3 = &#x27;a&#x27;); 我们可以把这个查询理解成：对于s1表中的某条记录，如果我们能在s2表（准确的说是执行完WHERE s2.key3 = &#39;a&#39;之后的结果集）中找到一条或多条记录，这些记录的common_field的值等于s1表记录的key1列的值，那么该条s1表的记录就会被加入到最终的结果集。这个过程其实和把s1和s2两个表连接起来的效果很像： 123SELECT s1.* FROM s1 INNER JOIN s2 ON s1.key1 = s2.common_field WHERE s2.key3 = &#x27;a&#x27;; 只不过我们不能保证对于s1表的某条记录来说，在s2表（准确的说是执行完WHERE s2.key3 = &#39;a&#39;之后的结果集）中有多少条记录满足s1.key1 = s2.common_field这个条件，不过我们可以分三种情况讨论： 情况一：对于s1表的某条记录来说，s2表中没有任何记录满足s1.key1 = s2.common_field这个条件，那么该记录自然也不会加入到最后的结果集。 情况二：对于s1表的某条记录来说，s2表中有且只有1条记录满足s1.key1 = s2.common_field这个条件，那么该记录会被加入最终的结果集。 情况三：对于s1表的某条记录来说，s2表中至少有2条记录满足s1.key1 = s2.common_field这个条件，那么该记录会被多次加入最终的结果集。 对于s1表的某条记录来说，由于我们只关心s2表中是否存在记录满足s1.key1 = s2.common_field这个条件，而不关心具体有多少条记录与之匹配，又因为有情况三的存在，我们上边所说的IN子查询和两表连接之间并不完全等价。但是将子查询转换为连接又真的可以充分发挥优化器的作用，所以MySQL在这里提出了一个新概念 — 半连接。将s1表和s2表进行半连接的意思就是：对于s1表的某条记录来说，我们只关心在s2表中是否存在与之匹配的记录，而不关心具体有多少条记录与之匹配，最终的结果集中只保留s1表的记录。我们假设MySQL内部是这么改写上边的子查询的： 123SELECT s1.* FROM s1 SEMI JOIN s2 ON s1.key1 = s2.common_field WHERE key3 = &#x27;a&#x27;; semi-join只是在MySQL内部采用的一种执行子查询的方式，MySQL并没有提供面向用户的semi-join语法，所以我们不需要，也不能尝试把上边这个语句放到mysql客户端执行。 怎么实现这种所谓的半连接呢？ Table pullout （子查询中的表上拉）当子查询的查询列表处只有主键或者唯一索引列时，可以直接把子查询中的表上拉到外层查询的FROM子句中，并把子查询中的搜索条件合并到外层查询的搜索条件中，比如这个由于key2列是s2表的唯一二级索引列，所以我们可以直接把s2表上拉到外层查询的FROM子句中，并且把子查询中的搜索条件合并到外层查询的搜索条件中，上拉之后的查询就是这样的：为什么当子查询的查询列表处只有主键或者唯一索引列时，就可以直接将子查询转换为连接查询呢？主键或者唯一索引列中的数据本身就是不重复的，所以对于同一条s1表中的记录，不可能找到两条以上的符合s1.key2 = s2.key2的记录。 12SELECT * FROM s1 WHERE key2 IN (SELECT key2 FROM s2 WHERE key3 = &#x27;a&#x27;); 123SELECT s1.* FROM s1 INNER JOIN s2 ON s1.key2 = s2.key2 WHERE s2.key3 = &#x27;a&#x27;; DuplicateWeedout execution strategy （重复值消除）对于这个查询来说：转换为半连接查询后，s1表中的某条记录可能在s2表中有多条匹配的记录，所以该条记录可能多次被添加到最后的结果集中，为了消除重复，我们可以建立一个临时表，比方说这个临时表长这样：这样在执行连接查询的过程中，每当某条s1表中的记录要加入结果集时，就首先把这条记录的id值加入到这个临时表里，如果添加成功，说明之前这条s1表中的记录并没有加入最终的结果集，现在把该记录添加到最终的结果集；如果添加失败，说明之前这条s1表中的记录已经加入过最终的结果集，这里直接把它丢弃就好了，这种使用临时表消除semi-join结果集中的重复值的方式称之为DuplicateWeedout。 12SELECT * FROM s1 WHERE key1 IN (SELECT common_field FROM s2 WHERE key3 = &#x27;a&#x27;); 123CREATE TABLE tmp ( id PRIMARY KEY); LooseScan execution strategy （松散扫描）大家看这个查询：在子查询中，对于s2表的访问可以使用到key1列的索引，而恰好子查询的查询列表处就是key1列，这样在将该查询转换为半连接查询后，如果将s2作为驱动表执行查询的话，那么执行过程就是这样：如图所示，在s2表的idx_key1索引中，值为&#39;aa&#39;的二级索引记录一共有3条，那么只需要取第一条的值到s1表中查找s1.key3 = &#39;aa&#39;的记录，如果能在s1表中找到对应的记录，那么就把对应的记录加入到结果集。依此类推，其他值相同的二级索引记录，也只需要取第一条记录的值到s1表中找匹配的记录，这种虽然是扫描索引，但只取值相同的记录的第一条去做匹配操作的方式称之为松散扫描。 12SELECT * FROM s1 WHERE key3 IN (SELECT key1 FROM s2 WHERE key1 &gt; &#x27;a&#x27; AND key1 &lt; &#x27;b&#x27;); Semi-join Materialization execution strategy我们之前介绍的先把外层查询的IN子句中的不相关子查询进行物化，然后再进行外层查询的表和物化表的连接本质上也算是一种semi-join，只不过由于物化表中没有重复的记录，所以可以直接将子查询转为连接查询。 FirstMatch execution strategy （首次匹配）FirstMatch是一种最原始的半连接执行方式，先取一条外层查询的中的记录，然后到子查询的表中寻找符合匹配条件的记录，如果能找到一条，则将该外层查询的记录放入最终的结果集并且停止查找更多匹配的记录，如果找不到则把该外层查询的记录丢弃掉；然后再开始取下一条外层查询中的记录，重复上边这个过程。 对于某些使用IN语句的相关子查询，比方这个查询： 12SELECT * FROM s1 WHERE key1 IN (SELECT common_field FROM s2 WHERE s1.key3 = s2.key3); 它也可以很方便的转为半连接，转换后的语句类似这样： 12SELECT s1.* FROM s1 SEMI JOIN s2 ON s1.key1 = s2.common_field AND s1.key3 = s2.key3; 然后就可以使用我们上边介绍过的DuplicateWeedout、LooseScan、FirstMatch等半连接执行策略来执行查询，当然，如果子查询的查询列表处只有主键或者唯一二级索引列，还可以直接使用table pullout的策略来执行查询，但是，由于相关子查询并不是一个独立的查询，所以不能转换为物化表来执行查询。 ④ semi-join的适用条件当然，并不是所有包含IN子查询的查询语句都可以转换为semi-join，只有形如这样的查询才可以被转换为semi-join： 12SELECT ... FROM outer_tables WHERE expr IN (SELECT ... FROM inner_tables ...) AND ... 或者这样的形式也可以： 12SELECT ... FROM outer_tables WHERE (oe1, oe2, ...) IN (SELECT ie1, ie2, ... FROM inner_tables ...) AND ... 用文字总结一下，只有符合下边这些条件的子查询才可以被转换为semi-join： 该子查询必须是和IN语句组成的布尔表达式，并且在外层查询的WHERE或者ON子句中出现。 外层查询也可以有其他的搜索条件，只不过和IN子查询的搜索条件必须使用AND连接起来。 该子查询必须是一个单一的查询，不能是由若干查询由UNION连接起来的形式。 该子查询不能包含GROUP BY或者HAVING语句或者聚集函数。 … 还有一些条件比较少见…. ⑤ 不适用于semi-join的情况对于一些不能将子查询转位semi-join的情况，典型的比如下边这几种： 外层查询的WHERE条件中有其他搜索条件与IN子查询组成的布尔表达式使用OR连接起来 123SELECT * FROM s1 WHERE key1 IN (SELECT common_field FROM s2 WHERE key3 = &#x27;a&#x27;) OR key2 &gt; 100; 使用NOT IN而不是IN的情况 12SELECT * FROM s1 WHERE key1 NOT IN (SELECT common_field FROM s2 WHERE key3 = &#x27;a&#x27;) 在SELECT子句中的IN子查询的情况 1SELECT key1 IN (SELECT common_field FROM s2 WHERE key3 = &#x27;a&#x27;) FROM s1 ; 子查询中包含GROUP BY、HAVING或者聚集函数的情况 12SELECT * FROM s1 WHERE key2 IN (SELECT COUNT(*) FROM s2 GROUP BY key1); 子查询中包含UNION的情况 12345SELECT * FROM s1 WHERE key1 IN ( SELECT common_field FROM s2 WHERE key3 = &#x27;a&#x27; UNION SELECT common_field FROM s2 WHERE key3 = &#x27;b&#x27;); MySQL仍然会尝试优化不能转为semi-join查询的子查询，那就是： 对于不相关子查询来说，可以尝试把它们物化之后再参与查询比如我们上边提到的这个查询：先将子查询物化，然后再判断key1是否在物化表的结果集中可以加快查询执行的速度。 12SELECT * FROM s1 WHERE key1 NOT IN (SELECT common_field FROM s2 WHERE key3 = &#x27;a&#x27;) 这里将子查询物化之后不能转为和外层查询的表的连接，只能是先扫描s1表，然后对s1表的某条记录来说，判断该记录的key1值在不在物化表中。 不管子查询是相关的还是不相关的，都可以把IN子查询尝试转为EXISTS子查询其实对于任意一个IN子查询来说，都可以被转为EXISTS子查询，通用的例子如下：可以被转换为：当然这个过程中有一些特殊情况，比如在outer_expr或者inner_expr值为NULL的情况下就比较特殊。因为有NULL值作为操作数的表达式结果往往是NULL，比方说：而EXISTS子查询的结果肯定是TRUE或者FASLE：但是，我们大部分使用IN子查询的场景是把它放在WHERE或者ON子句中，而WHERE或者ON子句是不区分NULL和FALSE的，比方说：所以只要我们的IN子查询是放在WHERE或者ON子句中的，那么IN -&gt; EXISTS的转换就是没问题的。说了这么多，为啥要转换呢？这是因为不转换的话可能用不到索引，比方说下边这个查询：这个查询中的子查询是一个相关子查询，而且子查询执行的时候不能使用到索引，但是将它转为EXISTS子查询后却可以使用到索引：转为EXISTS子查询时便可以使用到s2表的idx_key3索引了。需要注意的是，如果IN子查询不满足转换为semi-join的条件，又不能转换为物化表或者转换为物化表的成本太大，那么它就会被转换为EXISTS查询。 1outer_expr IN (SELECT inner_expr FROM ... WHERE subquery_where) 1EXISTS (SELECT inner_expr FROM ... WHERE subquery_where AND outer_expr=inner_expr) 1234567891011121314151617181920212223mysql&gt; SELECT NULL IN (1, 2, 3);+-------------------+| NULL IN (1, 2, 3) |+-------------------+| NULL |+-------------------+1 row in set (0.00 sec)mysql&gt; SELECT 1 IN (1, 2, 3);+----------------+| 1 IN (1, 2, 3) |+----------------+| 1 |+----------------+1 row in set (0.00 sec)mysql&gt; SELECT NULL IN (NULL);+----------------+| NULL IN (NULL) |+----------------+| NULL |+----------------+1 row in set (0.00 sec) 1234567891011121314151617181920212223mysql&gt; SELECT EXISTS (SELECT 1 FROM s1 WHERE NULL = 1);+------------------------------------------+| EXISTS (SELECT 1 FROM s1 WHERE NULL = 1) |+------------------------------------------+| 0 |+------------------------------------------+1 row in set (0.01 sec)mysql&gt; SELECT EXISTS (SELECT 1 FROM s1 WHERE 1 = NULL);+------------------------------------------+| EXISTS (SELECT 1 FROM s1 WHERE 1 = NULL) |+------------------------------------------+| 0 |+------------------------------------------+1 row in set (0.00 sec)mysql&gt; SELECT EXISTS (SELECT 1 FROM s1 WHERE NULL = NULL);+---------------------------------------------+| EXISTS (SELECT 1 FROM s1 WHERE NULL = NULL) |+---------------------------------------------+| 0 |+---------------------------------------------+1 row in set (0.00 sec) 12345mysql&gt; SELECT 1 FROM s1 WHERE NULL;Empty set (0.00 sec)mysql&gt; SELECT 1 FROM s1 WHERE FALSE;Empty set (0.00 sec) 123SELECT * FROM s1 WHERE key1 IN (SELECT key3 FROM s2 where s1.common_field = s2.common_field) OR key2 &gt; 1000; 123SELECT * FROM s1 WHERE EXISTS (SELECT 1 FROM s2 where s1.common_field = s2.common_field AND s2.key3 = s1.key1) OR key2 &gt; 1000; 在MySQL5.5以及之前的版本没有引进semi-join和物化的方式优化子查询时，优化器都会把IN子查询转换为EXISTS子查询。 ⑥ 阶段梳理 如果IN子查询符合转换为semi-join的条件，查询优化器会优先把该子查询转换为semi-join，然后再考虑下边5种执行半连接的策略中哪个成本最低： Table pullout DuplicateWeedout LooseScan Materialization FirstMatch 选择成本最低的那种执行策略来执行子查询。 如果IN子查询不符合转换为semi-join的条件，那么查询优化器会从下边两种策略中找出一种成本更低的方式执行子查询： 先将子查询物化之后再执行查询 执行IN to EXISTS转换。 3.2.3 ANY/ALL子查询优化如果ANY/ALL子查询是不相关子查询的话，它们在很多场合都能转换成我们熟悉的方式去执行，比方说： 原始表达式 转换为 &lt; ANY (SELECT inner_expr …) &lt; (SELECT MAX(inner_expr) …) &gt; ANY (SELECT inner_expr …) &gt; (SELECT MIN(inner_expr) …) &lt; ALL (SELECT inner_expr …) &lt; (SELECT MIN(inner_expr) …) &gt; ALL (SELECT inner_expr …) &gt; (SELECT MAX(inner_expr) …) 3.2.4 [NOT] EXISTS子查询的执行如果[NOT] EXISTS子查询是不相关子查询，可以先执行子查询，得出该[NOT] EXISTS子查询的结果是TRUE还是FALSE，并重写原先的查询语句，比如对这个查询来说： 123SELECT * FROM s1 WHERE EXISTS (SELECT 1 FROM s2 WHERE key1 = &#x27;a&#x27;) OR key2 &gt; 100; 因为这个语句里的子查询是不相关子查询，所以优化器会首先执行该子查询，假设该EXISTS子查询的结果为TRUE，那么接着优化器会重写查询为： 12SELECT * FROM s1 WHERE TRUE OR key2 &gt; 100; 进一步简化后就变成了： 12SELECT * FROM s1 WHERE TRUE; 对于相关的[NOT] EXISTS子查询来说，比如这个查询： 12SELECT * FROM s1 WHERE EXISTS (SELECT 1 FROM s2 WHERE s1.common_field = s2.common_field); 这个查询只能按照普通的那种执行相关子查询的方式来执行。不过如果[NOT] EXISTS子查询中如果可以使用索引的话，那查询速度也会加快不少，比如： 12SELECT * FROM s1 WHERE EXISTS (SELECT 1 FROM s2 WHERE s1.common_field = s2.key1); 上边这个EXISTS子查询中可以使用idx_key1来加快查询速度。 3.2.5 对于派生表的优化我们前边说过把子查询放在外层查询的FROM子句后，那么这个子查询的结果相当于一个派生表，比如下边这个查询： 123SELECT * FROM ( SELECT id AS d_id, key3 AS d_key3 FROM s2 WHERE key1 = &#x27;a&#x27; ) AS derived_s1 WHERE d_key3 = &#x27;a&#x27;; 子查询( SELECT id AS d_id, key3 AS d_key3 FROM s2 WHERE key1 = &#39;a&#39;)的结果就相当于一个派生表，这个表的名称是derived_s1，该表有两个列，分别是d_id和d_key3。 对于含有派生表的查询，MySQL提供了两种执行策略： 把派生表物化我们可以将派生表的结果集写到一个内部的临时表中，然后就把这个物化表当作普通表一样参与查询。当然，在对派生表进行物化时，MySQL使用了一种称为延迟物化的策略，也就是在查询中真正使用到派生表时才会去尝试物化派生表，而不是还没开始执行查询就把派生表物化掉。比方说对于下边这个含有派生表的查询来说：如果采用物化派生表的方式来执行这个查询的话，那么执行时首先会到s2表中找出满足s2.key2 = 1的记录，如果找不到，说明参与连接的s2表记录就是空的，所以整个查询的结果集就是空的，所以也就没有必要去物化查询中的派生表了。 12345SELECT * FROM ( SELECT * FROM s1 WHERE key1 = &#x27;a&#x27; ) AS derived_s1 INNER JOIN s2 ON derived_s1.key1 = s2.key1 WHERE s2.key2 = 1; 将派生表和外层的表合并，也就是将查询重写为没有派生表的形式我们来看这个包含派生表的查询：这个查询本质上就是想查看s1表中满足key1 = &#39;a&#39;条件的的全部记录，所以和下边这个语句是等价的：对于一些稍微复杂的包含派生表的语句，比如我们上边提到的那个：我们可以将派生表与外层查询的表合并，然后将派生表中的搜索条件放到外层查询的搜索条件中，就像这样：这样通过将外层查询和派生表合并的方式成功的消除了派生表，也就意味着我们没必要再付出创建和访问临时表的成本了。可是并不是所有带有派生表的查询都能被成功的和外层查询合并，当派生表中有这些语句就不可以和外层查询合并： 1SELECT * FROM (SELECT * FROM s1 WHERE key1 = &#x27;a&#x27;) AS derived_s1; 1SELECT * FROM s1 WHERE key1 = &#x27;a&#x27;; 12345SELECT * FROM ( SELECT * FROM s1 WHERE key1 = &#x27;a&#x27; ) AS derived_s1 INNER JOIN s2 ON derived_s1.key1 = s2.key1 WHERE s2.key2 = 1; 123SELECT * FROM s1 INNER JOIN s2 ON s1.key1 = s2.key1 WHERE s1.key1 = &#x27;a&#x27; AND s2.key2 = 1; 聚集函数，比如MAX()、MIN()、SUM()… DISTINCT GROUP BY HAVING LIMIT UNION 或者 UNION ALL 派生表对应的子查询的SELECT子句中含有另一个子查询 … 还有些不常用的情况… 所以MySQL在执行带有派生表的时候，优先尝试把派生表和外层查询合并掉，如果不行的话，再把派生表物化掉执行查询。 4.总结MySQL会对用户编写的SQL语句进行重写操作，比如： 移除不必要的括号 常量传递 移除没用的条件 表达式计算 HAVING&amp;WHERE子句的合并 常量表检测 在被驱动表的WHERE子句符合空值拒绝条件的时候，外连接&amp;内连接可以相互转换。 子查询可以按照不同维度进行不同分类，比如按照子查询返回的结果集分类： 标量子查询 行子查询 列子查询 表子查询 按照与外层查询的关系来分类： 不相关子查询 相关子查询 MySQL对in查询进行了很多优化。如果in子查询符合转换为半连接的条件，查询优化器会优先把该子查询转换为半连接，然后再考虑下面五种执行半连接查询的策略中哪个成本最低，最后选择成本最低的执行策略来执行子查询。 table pullout duplicate weedout looseScan Semj-join Materialization FirstMatch 如果IN子查询不符合转换为半连接的条件，查询优化器会从下面的两种策略里面找出一种成本更低的方式去执行子查询： 先将子查询物化，在执行子查询 执行in到exists的转换 MySQL在处理带有派生表的语句的时候，优先尝试把派生表和外层查询进行合并；如果不行，再把派生表物化掉，然后执行查询。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/tags/MySQL/"}]},{"title":"MySQL[八]InnoDB统计数据收集原理","slug":"MySQL/MySQL[八]InnoDB统计数据收集原理","date":"2022-01-07T16:00:00.000Z","updated":"2022-01-12T00:41:47.959Z","comments":true,"path":"2022/01/08/MySQL/MySQL[八]InnoDB统计数据收集原理/","link":"","permalink":"https://yinhuidong.github.io/2022/01/08/MySQL/MySQL[%E5%85%AB]InnoDB%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%E5%8E%9F%E7%90%86/","excerpt":"","text":"本篇我们来分析下InnoDB存储引擎的统计数据收集策略。 1.统计数据的存储方式InnoDB提供了两种存储统计数据的方式： 永久性的统计数据这种统计数据存储在磁盘上，也就是服务器重启之后这些统计数据还在。 非永久性的统计数据这种统计数据存储在内存中，当服务器关闭时这些这些统计数据就都被清除掉了，等到服务器重启之后，在某些适当的场景下才会重新收集这些统计数据。 MySQL提供了系统变量innodb_stats_persistent来控制到底采用哪种方式去存储统计数据。在MySQL 5.6.6之前，innodb_stats_persistent的值默认是OFF，也就是说InnoDB的统计数据默认是存储到内存的，之后的版本中innodb_stats_persistent的值默认是ON，也就是统计数据默认被存储到磁盘中。 不过InnoDB默认是以表为单位来收集和存储统计数据的，也就是说我们可以把某些表的统计数据（以及该表的索引统计数据）存储在磁盘上，把另一些表的统计数据存储在内存中。 我们可以在创建和修改表的时候通过指定STATS_PERSISTENT属性来指明该表的统计数据存储方式： 123CREATE TABLE 表名 (...) Engine=InnoDB, STATS_PERSISTENT = (1|0);ALTER TABLE 表名 Engine=InnoDB, STATS_PERSISTENT = (1|0); 当STATS_PERSISTENT=1时，表明我们想把该表的统计数据永久的存储到磁盘上，当STATS_PERSISTENT=0时，表明我们想把该表的统计数据临时的存储到内存中。如果我们在创建表时未指定STATS_PERSISTENT属性，那默认采用系统变量innodb_stats_persistent的值作为该属性的值。 2.永久性统计数据当我们选择把某个表以及该表索引的统计数据存放到磁盘上时，实际上是把这些统计数据存储到了两个表里： 12345678mysql&gt; SHOW TABLES FROM mysql LIKE &#x27;innodb%&#x27;;+---------------------------+| Tables_in_mysql (innodb%) |+---------------------------+| innodb_index_stats || innodb_table_stats |+---------------------------+2 rows in set (0.01 sec) 可以看到，这两个表都位于mysql系统数据库下边，其中： innodb_table_stats存储了关于表的统计数据，每一条记录对应着一个表的统计数据。 innodb_index_stats存储了关于索引的统计数据，每一条记录对应着一个索引的一个统计项的统计数据。 看一下这两个表里边都有什么以及表里的数据是如何生成的。 2.1 innodb_table_stats直接看一下这个innodb_table_stats表中的各个列都是干嘛的： 字段名 描述 database_name 数据库名 table_name 表名 last_update 本条记录最后更新时间 n_rows 表中记录的条数 clustered_index_size 表的聚簇索引占用的页面数量 sum_of_other_index_sizes 表的其他索引占用的页面数量 注意这个表的主键是(database_name,table_name)，也就是innodb_table_stats表的每条记录代表着一个表的统计信息。 12345678910mysql&gt; SELECT * FROM mysql.innodb_table_stats;+---------------+---------------+---------------------+--------+----------------------+--------------------------+| database_name | table_name | last_update | n_rows | clustered_index_size | sum_of_other_index_sizes |+---------------+---------------+---------------------+--------+----------------------+--------------------------+| mysql | gtid_executed | 2021-12-14 00:00:08 | 0 | 1 | 0 || sys | sys_config | 2021-12-14 00:00:09 | 6 | 1 | 0 || yhd | person_info | 2021-12-19 00:27:39 | 0 | 1 | 1 || yhd | single_table | 2021-12-19 02:13:19 | 910545 | 3109 | 5836 |+---------------+---------------+---------------------+--------+----------------------+--------------------------+4 rows in set (0.00 sec) 可以看到single_table表的统计信息就对应着mysql.innodb_table_stats的第三条记录。几个重要统计信息项的值如下： n_rows的值是9693，表明single_table表中大约有9693条记录，注意这个数据是估计值。 clustered_index_size的值是97，表明single_table表的聚簇索引占用97个页面，这个值是也是一个估计值。 sum_of_other_index_sizes的值是175，表明single_table表的其他索引一共占用175个页面，这个值是也是一个估计值。 2.1.1 n_rows统计项的收集为啥n_rows这个统计项的值是估计值呢？InnoDB`统计一个表中有多少行记录的套路是这样的： 按照一定算法（并不是纯粹随机的）选取几个叶子节点页面，计算每个页面中主键值记录数量，然后计算平均一个页面中主键值的记录数量乘以全部叶子节点的数量就算是该表的n_rows值。 真实的计算过程比这个稍微复杂一些，不过大致上就是这样。 可以看出来这个n_rows值精确与否取决于统计时采样的页面数量，MySQL为我们准备了一个名为innodb_stats_persistent_sample_pages的系统变量来控制使用永久性的统计数据时，计算统计数据时采样的页面数量。该值设置的越大，统计出的n_rows值越精确，但是统计耗时也就最久；该值设置的越小，统计出的n_rows值越不精确，但是统计耗时特别少。所以在实际使用是需要我们去权衡利弊，该系统变量的默认值是20。 我们前边说过，不过InnoDB默认是以表为单位来收集和存储统计数据的，我们也可以单独设置某个表的采样页面的数量，设置方式就是在创建或修改表的时候通过指定STATS_SAMPLE_PAGES属性来指明该表的统计数据存储方式： 123CREATE TABLE 表名 (...) Engine=InnoDB, STATS_SAMPLE_PAGES = 具体的采样页面数量;ALTER TABLE 表名 Engine=InnoDB, STATS_SAMPLE_PAGES = 具体的采样页面数量; 如果我们在创建表的语句中并没有指定STATS_SAMPLE_PAGES属性的话，将默认使用系统变量innodb_stats_persistent_sample_pages的值作为该属性的值。 2.1.2 clustered_index_size和sum_of_other_index_sizes统计项的收集这两个统计项的收集过程如下： 从数据字典里找到表的各个索引对应的根页面位置。系统表SYS_INDEXES里存储了各个索引对应的根页面信息。 从根页面的Page Header里找到叶子节点段和非叶子节点段对应的Segment Header。在每个索引的根页面的Page Header部分都有两个字段： PAGE_BTR_SEG_LEAF：表示B+树叶子段的Segment Header信息。 PAGE_BTR_SEG_TOP：表示B+树非叶子段的Segment Header信息。 从叶子节点段和非叶子节点段的Segment Header中找到这两个段对应的INODE Entry结构。这个是Segment Header结构： 从对应的INODE Entry结构中可以找到该段对应所有零散的页面地址以及FREE、NOT_FULL、FULL链表的基节点。这个是INODE Entry结构： 直接统计零散的页面有多少个，然后从那三个链表的List Length字段中读出该段占用的区的大小，每个区占用64个页，所以就可以统计出整个段占用的页面。这个是链表基节点的示意图： 分别计算聚簇索引的叶子结点段和非叶子节点段占用的页面数，它们的和就是clustered_index_size的值，按照同样的套路把其余索引占用的页面数都算出来，加起来之后就是sum_of_other_index_sizes的值。 注意，我们说一个段的数据在非常多时（超过32个页面），会以区为单位来申请空间，这里头的问题是以区为单位申请空间中有一些页可能并没有使用，但是在统计clustered_index_size和sum_of_other_index_sizes时都把它们算进去了，所以说聚簇索引和其他的索引占用的页面数可能比这两个值要小一些。 2.2 innodb_index_stats直接看一下这个innodb_index_stats表中的各个列： 字段名 描述 database_name 数据库名 table_name 表名 index_name 索引名 last_update 本条记录最后更新时间 stat_name 统计项的名称 stat_value 对应的统计项的值 sample_size 为生成统计数据而采样的页面数量 stat_description 对应的统计项的描述 注意这个表的主键是(database_name,table_name,index_name,stat_name)，其中的stat_name是指统计项的名称，也就是说innodb_index_stats表的每条记录代表着一个索引的一个统计项。我们直接看一下关于single_table表的索引统计数据都有些什么： 1234567891011121314151617181920212223242526mysql&gt; SELECT * FROM mysql.innodb_index_stats WHERE table_name = &#x27;single_table&#x27;;+---------------+--------------+--------------+---------------------+--------------+------------+-------------+-----------------------------------+| database_name | table_name | index_name | last_update | stat_name | stat_value | sample_size | stat_description |+---------------+--------------+--------------+---------------------+--------------+------------+-------------+-----------------------------------+| yhd | single_table | PRIMARY | 2021-12-19 02:13:19 | n_diff_pfx01 | 910518 | 20 | id || yhd | single_table | PRIMARY | 2021-12-19 02:13:19 | n_leaf_pages | 3097 | NULL | Number of leaf pages in the index || yhd | single_table | PRIMARY | 2021-12-19 02:13:19 | size | 3109 | NULL | Number of pages in the index || yhd | single_table | idx_key1 | 2021-12-19 02:13:19 | n_diff_pfx01 | 8 | 10 | key1 || yhd | single_table | idx_key1 | 2021-12-19 02:13:19 | n_diff_pfx02 | 882192 | 20 | key1,id || yhd | single_table | idx_key1 | 2021-12-19 02:13:19 | n_leaf_pages | 828 | NULL | Number of leaf pages in the index || yhd | single_table | idx_key1 | 2021-12-19 02:13:19 | size | 993 | NULL | Number of pages in the index || yhd | single_table | idx_key3 | 2021-12-19 02:13:19 | n_diff_pfx01 | 8 | 10 | key3 || yhd | single_table | idx_key3 | 2021-12-19 02:13:19 | n_diff_pfx02 | 910072 | 20 | key3,id || yhd | single_table | idx_key3 | 2021-12-19 02:13:19 | n_leaf_pages | 827 | NULL | Number of leaf pages in the index || yhd | single_table | idx_key3 | 2021-12-19 02:13:19 | size | 993 | NULL | Number of pages in the index || yhd | single_table | idx_key_part | 2021-12-19 02:13:19 | n_diff_pfx01 | 8 | 10 | key_part1 || yhd | single_table | idx_key_part | 2021-12-19 02:13:19 | n_diff_pfx02 | 82 | 20 | key_part1,key_part2 || yhd | single_table | idx_key_part | 2021-12-19 02:13:19 | n_diff_pfx03 | 730 | 20 | key_part1,key_part2,key_part3 || yhd | single_table | idx_key_part | 2021-12-19 02:13:19 | n_diff_pfx04 | 1028534 | 20 | key_part1,key_part2,key_part3,id || yhd | single_table | idx_key_part | 2021-12-19 02:13:19 | n_leaf_pages | 2556 | NULL | Number of leaf pages in the index || yhd | single_table | idx_key_part | 2021-12-19 02:13:19 | size | 2985 | NULL | Number of pages in the index || yhd | single_table | uk_key2 | 2021-12-19 02:13:19 | n_diff_pfx01 | 913104 | 20 | key2 || yhd | single_table | uk_key2 | 2021-12-19 02:13:19 | n_leaf_pages | 816 | NULL | Number of leaf pages in the index || yhd | single_table | uk_key2 | 2021-12-19 02:13:19 | size | 865 | NULL | Number of pages in the index |+---------------+--------------+--------------+---------------------+--------------+------------+-------------+-----------------------------------+20 rows in set (0.01 sec) 这个结果有点儿多，正确查看这个结果的方式是这样的： 先查看index_name列，这个列说明该记录是哪个索引的统计信息，从结果中我们可以看出来，PRIMARY索引（也就是主键）占了3条记录，idx_key_part索引占了6条记录。 针对index_name列相同的记录，stat_name表示针对该索引的统计项名称，stat_value展示的是该索引在该统计项上的值，stat_description指的是来描述该统计项的含义的。我们来具体看一下一个索引都有哪些统计项： n_leaf_pages：表示该索引的叶子节点占用多少页面。 size：表示该索引共占用多少页面。 n_diff_pfx**NN**：表示对应的索引列不重复的值有多少。其实NN可以被替换为01、02、03… 这样的数字。比如对于idx_key_part来说： 1. n_diff_pfx01表示的是统计key_part1这单单一个列不重复的值有多少。 1. n_diff_pfx02表示的是统计key_part1、key_part2这两个列组合起来不重复的值有多少。 1. n_diff_pfx03表示的是统计key_part1、key_part2、key_part3这三个列组合起来不重复的值有多少。 1. n_diff_pfx04表示的是统计key_part1、key_part2、key_part3、id这四个列组合起来不重复的值有多少。 注意：对于普通的二级索引，并不能保证它的索引列值是唯一的，比如对于idx_key1来说，key1列就可能有很多值重复的记录。此时只有在索引列上加上主键值才可以区分两条索引列值都一样的二级索引记录。对于主键和唯一二级索引则没有这个问题，它们本身就可以保证索引列值的不重复，所以也不需要再统计一遍在索引列后加上主键值的不重复值有多少。比如上边的idx_key1有n_diff_pfx01、n_diff_pfx02两个统计项，而idx_key2却只有n_diff_pfx01一个统计项。 在计算某些索引列中包含多少不重复值时，需要对一些叶子节点页面进行采样，sample_size列就表明了采样的页面数量是多少。 注意：对于有多个列的联合索引来说，采样的页面数量是：innodb_stats_persistent_sample_pages × 索引列的个数。当需要采样的页面数量大于该索引的叶子节点数量的话，就直接采用全表扫描来统计索引列的不重复值数量了。所以在查询结果中看到不同索引对应的size列的值可能是不同的。 2.3定期更新统计数据随着我们不断的对表进行增删改操作，表中的数据也一直在变化，innodb_table_stats和innodb_index_stats表里的统计数据是要变的，不变的话MySQL查询优化器计算的成本就会有问题。MySQL提供了如下两种更新统计数据的方式： 开启innodb_stats_auto_recalc。系统变量innodb_stats_auto_recalc决定着服务器是否自动重新计算统计数据，它的默认值是ON，也就是该功能默认是开启的。每个表都维护了一个变量，该变量记录着对该表进行增删改的记录条数，如果发生变动的记录数量超过了表大小的10%，并且自动重新计算统计数据的功能是打开的，那么服务器会重新进行一次统计数据的计算，并且更新innodb_table_stats和innodb_index_stats表。不过自动重新计算统计数据的过程是异步发生的，也就是即使表中变动的记录数超过了10%，自动重新计算统计数据也不会立即发生，可能会延迟几秒才会进行计算。InnoDB默认是以表为单位来收集和存储统计数据的，我们也可以单独为某个表设置是否自动重新计算统计数的属性，设置方式就是在创建或修改表的时候通过指定STATS_AUTO_RECALC属性来指明该表的统计数据存储方式：当STATS_AUTO_RECALC=1时，表明我们想让该表自动重新计算统计数据，当STATS_AUTO_RECALC=0时，表明不想让该表自动重新计算统计数据。如果我们在创建表时未指定STATS_AUTO_RECALC属性，那默认采用系统变量innodb_stats_auto_recalc的值作为该属性的值。 123CREATE TABLE 表名 (...) Engine=InnoDB, STATS_AUTO_RECALC = (1|0);ALTER TABLE 表名 Engine=InnoDB, STATS_AUTO_RECALC = (1|0); 手动调用ANALYZE TABLE语句来更新统计信息如果innodb_stats_auto_recalc系统变量的值为OFF的话，我们也可以手动调用ANALYZE TABLE语句来重新计算统计数据，比如我们可以这样更新关于single_table表的统计数据：需要注意的是，ANALYZE TABLE语句会立即重新计算统计数据，也就是这个过程是同步的，在表中索引多或者采样页面特别多时这个过程可能会特别慢，请不要没事儿就运行一下ANALYZE TABLE语句，最好在业务不是很繁忙的时候再运行。 1234567mysql&gt; ANALYZE TABLE single_table;+------------------------+---------+----------+----------+| Table | Op | Msg_type | Msg_text |+------------------------+---------+----------+----------+| yhd.single_table | analyze | status | OK |+------------------------+---------+----------+----------+1 row in set (0.08 sec) 2.4手动更新统计数据其实innodb_table_stats和innodb_index_stats表就相当于一个普通的表一样，我们能对它们做增删改查操作。这也就意味着我们可以手动更新某个表或者索引的统计数据。比如说我们想把single_table表关于行数的统计数据更改一下可以这么做： 步骤一：更新innodb_table_stats表。 123UPDATE innodb_table_stats SET n_rows = 1 WHERE table_name = &#x27;single_table&#x27;; 步骤二：让MySQL查询优化器重新加载我们更改过的数据。更新完innodb_table_stats只是单纯的修改了一个表的数据，需要让MySQL查询优化器重新加载我们更改过的数据，运行下边的命令就可以了： 1FLUSH TABLE single_table; 之后我们使用SHOW TABLE STATUS语句查看表的统计数据时就看到Rows行变为了1。 3.非永久性统计数据当我们把系统变量innodb_stats_persistent的值设置为OFF时，之后创建的表的统计数据默认就都是非永久性的了，或者我们直接在创建表或修改表时设置STATS_PERSISTENT属性的值为0，那么该表的统计数据就是非永久性的了。 与永久性的统计数据不同，非永久性的统计数据采样的页面数量是由innodb_stats_transient_sample_pages控制的，这个系统变量的默认值是8。 另外，由于非永久性的统计数据经常更新，所以导致MySQL查询优化器计算查询成本的时候依赖的是经常变化的统计数据，也就会生成经常变化的执行计划。 4.innodb_stats_method索引列不重复的值的数量这个统计数据对于MySQL查询优化器十分重要，因为通过它可以计算出在索引列中平均一个值重复多少行，它的应用场景主要有两个： 单表查询中单点区间太多，比方说这样：当IN里的参数数量过多时，采用index dive的方式直接访问B+树索引去统计每个单点区间对应的记录的数量就太耗费性能了，所以直接依赖统计数据中的平均一个值重复多少行来计算单点区间对应的记录数量。 1SELECT * FROM tbl_name WHERE key IN (&#x27;xx1&#x27;, &#x27;xx2&#x27;, ..., &#x27;xxn&#x27;); 连接查询时，如果有涉及两个表的等值匹配连接条件，该连接条件对应的被驱动表中的列又拥有索引时，则可以使用ref访问方法来对被驱动表进行查询，比方说这样：在真正执行对t2表的查询前，t1.comumn的值是不确定的，所以我们也不能通过index dive的方式直接访问B+树索引去统计每个单点区间对应的记录的数量，所以也只能依赖统计数据中的平均一个值重复多少行来计算单点区间对应的记录数量。 1SELECT * FROM t1 JOIN t2 ON t1.column = t2.key WHERE ...; 在统计索引列不重复的值的数量时，有一个比较烦的问题就是索引列中出现NULL值怎么办，比方说某个索引列的内容是这样： 12345678+------+| col |+------+| 1 || 2 || NULL || NULL |+------+ 此时计算这个col列中不重复的值的数量就有下边的分歧： 有的人认为NULL值代表一个未确定的值，所以MySQL认为任何和NULL值做比较的表达式的值都为NULL，就是这样：所以每一个NULL值都是独一无二的，也就是说统计索引列不重复的值的数量时，应该把NULL值当作一个独立的值，所以col列的不重复的值的数量就是：4（分别是1、2、NULL、NULL这四个值）。 12345678910111213141516171819202122232425262728293031mysql&gt; SELECT 1 = NULL;+----------+| 1 = NULL |+----------+| NULL |+----------+1 row in set (0.00 sec)mysql&gt; SELECT 1 != NULL;+-----------+| 1 != NULL |+-----------+| NULL |+-----------+1 row in set (0.00 sec)mysql&gt; SELECT NULL = NULL;+-------------+| NULL = NULL |+-------------+| NULL |+-------------+1 row in set (0.00 sec)mysql&gt; SELECT NULL != NULL;+--------------+| NULL != NULL |+--------------+| NULL |+--------------+1 row in set (0.00 sec) 有的人认为其实NULL值在业务上就是代表没有，所有的NULL值代表的意义是一样的，所以col列不重复的值的数量就是：3（分别是1、2、NULL这三个值）。 有的人认为这NULL完全没有意义嘛，所以在统计索引列不重复的值的数量时压根儿不能把它们算进来，所以col列不重复的值的数量就是：2（分别是1、2这两个值）。 MySQL提供了一个名为innodb_stats_method的系统变量，我们可以自己来设置，这个系统变量有三个候选值： nulls_equal：认为所有NULL值都是相等的。这个值也是innodb_stats_method的默认值。如果某个索引列中NULL值特别多的话，这种统计方式会让优化器认为某个列中平均一个值重复次数特别多，所以倾向于不使用索引进行访问。 nulls_unequal：认为所有NULL值都是不相等的。如果某个索引列中NULL值特别多的话，这种统计方式会让优化器认为某个列中平均一个值重复次数特别少，所以倾向于使用索引进行访问。 nulls_ignored：直接把NULL值忽略掉。 5.总结 InnoDB以表为单位来收集统计数据，这些统计数据可以是基于磁盘的永久性统计数据，也可以是基于内存的非永久性统计数据。 innodb_stats_persistent控制着使用永久性统计数据还是非永久性统计数据；innodb_stats_persistent_sample_pages控制着永久性统计数据的采样页面数量；innodb_stats_transient_sample_pages控制着非永久性统计数据的采样页面数量；innodb_stats_auto_recalc控制着是否自动重新计算统计数据。 我们可以针对某个具体的表，在创建和修改表时通过指定STATS_PERSISTENT、STATS_AUTO_RECALC、STATS_SAMPLE_PAGES的值来控制相关统计数据属性。 innodb_stats_method决定着在统计某个索引列不重复值的数量时如何对待NULL值。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/tags/MySQL/"}]},{"title":"MySQL[七]基于成本的优化","slug":"MySQL/MySQL[七]基于成本的优化","date":"2022-01-06T16:00:00.000Z","updated":"2022-01-12T00:41:37.540Z","comments":true,"path":"2022/01/07/MySQL/MySQL[七]基于成本的优化/","link":"","permalink":"https://yinhuidong.github.io/2022/01/07/MySQL/MySQL[%E4%B8%83]%E5%9F%BA%E4%BA%8E%E6%88%90%E6%9C%AC%E7%9A%84%E4%BC%98%E5%8C%96/","excerpt":"","text":"1.什么是成本？在MySQL中一条查询语句的执行成本是由下边这两个方面组成的： I/O成本我们的表经常使用的MyISAM、InnoDB存储引擎都是将数据和索引都存储到磁盘上的，当我们想查询表中的记录时，需要先把数据或者索引加载到内存中然后再操作。这个从磁盘到内存这个加载的过程损耗的时间称之为I/O成本。 CPU成本读取以及检测记录是否满足对应的搜索条件、对结果集进行排序等这些操作损耗的时间称之为CPU成本。 对于InnoDB存储引擎来说，页是磁盘和内存之间交互的基本单位，MySQL规定读取一个页面花费的成本默认是1.0，读取以及检测一条记录是否符合搜索条件的成本默认是0.2。1.0、0.2这些数字称之为成本常数。 不管读取记录时需不需要检测是否满足搜索条件，其成本都算是0.2。 2.单表查询的成本2.1 准备工作把之前用到的single_table表搬来。 123456789101112131415CREATE TABLE single_table ( id INT NOT NULL AUTO_INCREMENT, key1 VARCHAR(100), key2 INT, key3 VARCHAR(100), key_part1 VARCHAR(100), key_part2 VARCHAR(100), key_part3 VARCHAR(100), common_field VARCHAR(100), PRIMARY KEY (id), KEY idx_key1 (key1), UNIQUE KEY idx_key2 (key2), KEY idx_key3 (key3), KEY idx_key_part(key_part1, key_part2, key_part3)) Engine=InnoDB CHARSET=utf8; 2.2 基于成本的优化步骤在一条单表查询语句真正执行之前，MySQL的查询优化器会找出执行该语句所有可能使用的方案，对比之后找出成本最低的方案，这个成本最低的方案就是所谓的执行计划，之后才会调用存储引擎提供的接口真正的执行查询，这个过程总结一下就是这样： 根据搜索条件，找出所有可能使用的索引 计算全表扫描的代价 计算使用不同索引执行查询的代价 对比各种执行方案的代价，找出成本最低的那一个 下边我们就以一个实例来分析一下这些步骤，单表查询语句如下： 123456SELECT * FROM single_table WHERE key1 IN (&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;) AND key2 &gt; 10 AND key2 &lt; 1000 AND key3 &gt; key2 AND key_part1 LIKE &#x27;%hello%&#x27; AND common_field = &#x27;123&#x27;; 2.2.1 根据搜索条件，找出所有可能使用的索引对于B+树索引来说，只要索引列和常数使用=、&lt;=&gt;、IN、NOT IN、IS NULL、IS NOT NULL、&gt;、&lt;、&gt;=、&lt;=、BETWEEN、!=（不等于也可以写成&lt;&gt;）或者LIKE操作符连接起来，就可以产生一个所谓的范围区间（LIKE匹配字符串前缀也行），也就是说这些搜索条件都可能使用到索引，MySQL把一个查询中可能使用到的索引称之为possible keys。 我们分析一下上边查询中涉及到的几个搜索条件： key1 IN (&#39;a&#39;, &#39;b&#39;, &#39;c&#39;)，这个搜索条件可以使用二级索引idx_key1。 key2 &gt; 10 AND key2 &lt; 1000，这个搜索条件可以使用二级索引idx_key2。 key3 &gt; key2，这个搜索条件的索引列由于没有和常数比较，所以并不能使用到索引。 key_part1 LIKE &#39;%hello%&#39;，key_part1通过LIKE操作符和以通配符开头的字符串做比较，不可以适用索引。 common_field = &#39;123&#39;，由于该列上压根儿没有索引，所以不会用到索引。 综上所述，上边的查询语句可能用到的索引，也就是possible keys只有idx_key1和idx_key2。 2.2.2 计算全表扫描的代价对于InnoDB存储引擎来说，全表扫描的意思就是把聚簇索引中的记录都依次和给定的搜索条件做一下比较，把符合搜索条件的记录加入到结果集，所以需要将聚簇索引对应的页面加载到内存中，然后再检测记录是否符合搜索条件。由于查询成本=I/O成本+CPU成本，所以计算全表扫描的代价需要两个信息： 聚簇索引占用的页面数 该表中的记录数 这两个信息从哪来呢？MySQL为每个表维护了一系列的统计信息，关于这些统计信息是如何收集起来的后面再谈，现在看看怎么查看这些统计信息。MySQL给我们提供了SHOW TABLE STATUS语句来查看表的统计信息，如果要看指定的某个表的统计信息，在该语句后加对应的LIKE语句就好了，比方说我们要查看single_table这个表的统计信息可以这么写： 1SHOW TABLE STATUS LIKE &#x27;single_table&#x27; 虽然出现了很多统计选项，但我们目前只关心两个： Rows本选项表示表中的记录条数。对于使用MyISAM存储引擎的表来说，该值是准确的，对于使用InnoDB存储引擎的表来说，该值是一个估计值。从查询结果我们也可以看出来，由于我们的single_table表是使用InnoDB存储引擎的，所以虽然实际上表中有10000条记录，但是SHOW TABLE STATUS显示的Rows值只有9693条记录。 Data_length本选项表示表占用的存储空间字节数。使用MyISAM存储引擎的表来说，该值就是数据文件的大小，对于使用InnoDB存储引擎的表来说，该值就相当于聚簇索引占用的存储空间大小，也就是说可以这样计算该值的大小：我们的single_table使用默认16KB的页面大小，而上边查询结果显示Data_length的值是1589248，所以我们可以反向来推导出聚簇索引的页面数量： 1Data_length = 聚簇索引的页面数量 x 每个页面的大小 1聚簇索引的页面数量 = 1589248 ÷ 16 ÷ 1024 = 97 我们现在已经得到了聚簇索引占用的页面数量以及该表记录数的估计值，所以就可以计算全表扫描成本了，但是MySQL在真实计算成本时会进行一些微调，这些微调的值是直接硬编码到代码里的，这些微调的值十分的小，并不影响我们分析。现在可以看一下全表扫描成本的计算过程： I/O成本97指的是聚簇索引占用的页面数，1.0指的是加载一个页面的成本常数，后边的1.1是一个微调值，我们不用在意。 197 x 1.0 + 1.1 = 98.1 CPU成本：9693指的是统计数据中表的记录数，对于InnoDB存储引擎来说是一个估计值，0.2指的是访问一条记录所需的成本常数，后边的1.0是一个微调值，我们不用在意。 19693 x 0.2 + 1.0 = 1939.6 总成本： 198.1 + 1939.6 = 2037.7 综上所述，对于single_table的全表扫描所需的总成本就是2037.7。 表中的记录其实都存储在聚簇索引对应B+树的叶子节点中，所以只要我们通过根节点获得了最左边的叶子节点，就可以沿着叶子节点组成的双向链表把所有记录都查看一遍。也就是说全表扫描这个过程其实有的B+树内节点是不需要访问的，但是MySQL在计算全表扫描成本时直接使用聚簇索引占用的页面数作为计算I/O成本的依据，是不区分内节点和叶子节点的。 2.2.3 计算使用不同索引执行查询的代价从第1步分析我们得到，上述查询可能使用到idx_key1和idx_key2这两个索引，我们需要分别分析单独使用这些索引执行查询的成本，最后还要分析是否可能使用到索引合并。这里需要提一点的是，MySQL查询优化器先分析使用唯一二级索引的成本，再分析使用普通索引的成本，所以我们也先分析idx_key2的成本，然后再看使用idx_key1的成本。 ①使用idx_key2执行查询的成本分析idx_key2对应的搜索条件是：key2 &gt; 10 AND key2 &lt; 1000，也就是说对应的范围区间就是：(10, 1000)，使用idx_key2搜索的示意图就是这样子： 对于使用二级索引 + 回表方式的查询，MySQL计算这种查询的成本依赖两个方面的数据： 范围区间数量不论某个范围区间的二级索引到底占用了多少页面，查询优化器粗暴的认为读取索引的一个范围区间的I/O成本和读取一个页面是相同的。本例中使用idx_key2的范围区间只有一个：(10, 1000)，所以相当于访问这个范围区间的二级索引付出的I/O成本就是： 11 x 1.0 = 1.0 需要回表的记录数优化器需要计算二级索引的某个范围区间到底包含多少条记录，对于本例来说就是要计算idx_key2在(10, 1000)这个范围区间中包含多少二级索引记录，计算过程是这样的： 步骤1：先根据key2 &gt; 10这个条件访问一下idx_key2对应的B+树索引，找到满足key2 &gt; 10这个条件的第一条记录，我们把这条记录称之为区间最左记录。在B+数树中定位一条记录的过程是贼快的，是常数级别的，所以这个过程的性能消耗是可以忽略不计的。 步骤2：然后再根据key2 &lt; 1000这个条件继续从idx_key2对应的B+树索引中找出最后一条满足这个条件的记录，我们把这条记录称之为区间最右记录，这个过程的性能消耗也可以忽略不计的。 步骤3：如果区间最左记录和区间最右记录相隔不太远（在MySQL 5.7.21这个版本里，只要相隔不大于10个页面即可），那就可以精确统计出满足key2 &gt; 10 AND key2 &lt; 1000条件的二级索引记录条数。否则只沿着区间最左记录向右读10个页面，计算平均每个页面中包含多少记录，然后用这个平均值乘以区间最左记录和区间最右记录之间的页面数量就可以了。那么问题又来了，怎么估计区间最左记录和区间最右记录之间有多少个页面呢？解决这个问题还得回到B+树索引的结构中来： 如图，我们假设区间最左记录在页b中，区间最右记录在页c中，那么我们想计算区间最左记录和区间最右记录之间的页面数量就相当于计算页b和页c之间有多少页面，而每一条目录项记录都对应一个数据页，所以计算页b和页c之间有多少页面就相当于计算它们父节点（也就是页a）中对应的目录项记录之间隔着几条记录。在一个页面中统计两条记录之间有几条记录的成本就贼小了。如果页b和页c之间的页面实在太多，以至于页b和页c对应的目录项记录都不在一个页面中，继续递归，也就是再统计页b和页c对应的目录项记录所在页之间有多少个页面。过一个B+树有4层高已经很了不得了，所以这个统计过程也不是很耗费性能。知道了如何统计二级索引某个范围区间的记录数之后，就需要回到现实问题中来，根据上述算法测得idx_key2在区间(10, 1000)之间大约有95条记录。读取这95条二级索引记录需要付出的CPU成本就是： 195 x 0.2 + 0.01 = 19.01 其中95是需要读取的二级索引记录条数，0.2是读取一条记录成本常数，0.01是微调。在通过二级索引获取到记录之后，还需要干两件事儿： 根据这些记录里的主键值到聚簇索引中做回表操作MySQL认为每次回表操作都相当于访问一个页面，也就是说二级索引范围区间有多少记录，就需要进行多少次回表操作，也就是需要进行多少次页面I/O。我们上边统计了使用idx_key2二级索引执行查询时，预计有95条二级索引记录需要进行回表操作，所以回表操作带来的I/O成本就是：其中95是预计的二级索引记录数，1.0是一个页面的I/O成本常数。 195 x 1.0 = 95.0 回表操作后得到的完整用户记录，然后再检测其他搜索条件是否成立回表操作的本质就是通过二级索引记录的主键值到聚簇索引中找到完整的用户记录，然后再检测除key2 &gt; 10 AND key2 &lt; 1000这个搜索条件以外的搜索条件是否成立。因为我们通过范围区间获取到二级索引记录共95条，也就对应着聚簇索引中95条完整的用户记录，读取并检测这些完整的用户记录是否符合其余的搜索条件的CPU成本如下：MySQL只计算这个查找过程所需的I/O成本，也就是我们上一步骤中得到的95.0，在内存中的定位完整用户记录的过程的成本是忽略不计的。在定位到这些完整的用户记录后，需要检测除key2 &gt; 10 AND key2 &lt; 1000这个搜索条件以外的搜索条件是否成立，这个比较过程花费的CPU成本就是：其中95是待检测记录的条数，0.2是检测一条记录是否符合给定的搜索条件的成本常数。 195 x 0.2 = 19.0 所以本例中使用idx_key2执行查询的成本就如下所示： I/O成本： 11.0 + 95 x 1.0 = 96.0 (范围区间的数量 + 预估的二级索引记录条数) CPU成本： 195 x 0.2 + 0.01 + 95 x 0.2 = 38.01 （读取二级索引记录的成本 + 读取并检测回表后聚簇索引记录的成本） 综上所述，使用idx_key2执行查询的总成本就是： 196.0 + 38.01 = 134.01 ②使用idx_key1执行查询的成本分析idx_key1对应的搜索条件是：key1 IN (&#39;a&#39;, &#39;b&#39;, &#39;c&#39;)，也就是说相当于3个单点区间： [&#39;a&#39;, &#39;a&#39;] [&#39;b&#39;, &#39;b&#39;] [&#39;c&#39;, &#39;c&#39;] 使用idx_key1搜索的示意图就是这样子： 与使用idx_key2的情况类似，我们也需要计算使用idx_key1时需要访问的范围区间数量以及需要回表的记录数： 范围区间数量使用idx_key1执行查询时很显然有3个单点区间，所以访问这3个范围区间的二级索引付出的I/O成本就是： 13 x 1.0 = 3.0 需要回表的记录数由于使用idx_key1时有3个单点区间，所以每个单点区间都需要查找一遍对应的二级索引记录数： 查找单点区间[&#39;a&#39;, &#39;a&#39;]对应的二级索引记录数计算单点区间对应的二级索引记录数和计算连续范围区间对应的二级索引记录数是一样的，都是先计算区间最左记录和区间最右记录，然后再计算它们之间的记录数，最后计算得到单点区间[&#39;a&#39;, &#39;a&#39;]对应的二级索引记录数是：35。 查找单点区间[&#39;b&#39;, &#39;b&#39;]对应的二级索引记录数与上同理，计算得到本单点区间对应的记录数是：44。 查找单点区间[&#39;c&#39;, &#39;c&#39;]对应的二级索引记录数与上同理，计算得到本单点区间对应的记录数是：39。 所以，这三个单点区间总共需要回表的记录数就是： 135 + 44 + 39 = 118 读取这些二级索引记录的CPU成本就是： 1118 x 0.2 + 0.01 = 23.61 得到总共需要回表的记录数之后，就要考虑： 根据这些记录里的主键值到聚簇索引中做回表操作所需的I/O成本就是： 1118 x 1.0 = 118.0 回表操作后得到的完整用户记录，然后再比较其他搜索条件是否成立此步骤对应的CPU成本就是： 1118 x 0.2 = 23.6 所以本例中使用idx_key1执行查询的成本就如下所示： I/O成本： 13.0 + 118 x 1.0 = 121.0 (范围区间的数量 + 预估的二级索引记录条数) CPU成本： 1118 x 0.2 + 0.01 + 118 x 0.2 = 47.21 （读取二级索引记录的成本 + 读取并检测回表后聚簇索引记录的成本） 综上所述，使用idx_key1执行查询的总成本就是： 1121.0 + 47.21 = 168.21 ③是否有可能使用索引合并（Index Merge）本例中有关key1和key2的搜索条件是使用AND连接起来的，而对于idx_key1和idx_key2都是范围查询，也就是说查找到的二级索引记录并不是按照主键值进行排序的，并不满足使用Intersection索引合并的条件，所以并不会使用索引合并。 2.2.4 对比各种执行方案的代价，找出成本最低的那一个下边把执行本例中的查询的各种可执行方案以及它们对应的成本列出来： 全表扫描的成本：2037.7 使用idx_key2的成本：134.01 使用idx_key1的成本：168.21 很显然，使用idx_key2的成本最低，所以选择idx_key2来执行查询。 2.3 基于索引统计数据的成本计算有时候使用索引执行查询时会有许多单点区间，比如使用IN语句就很容易产生非常多的单点区间，比如下边这个查询（下边查询语句中的...表示还有很多参数）： 1SELECT * FROM single_table WHERE key1 IN (&#x27;aa1&#x27;, &#x27;aa2&#x27;, &#x27;aa3&#x27;, ... , &#x27;zzz&#x27;); 很显然，这个查询可能使用到的索引就是idx_key1，由于这个索引并不是唯一二级索引，所以并不能确定一个单点区间对应的二级索引记录的条数有多少，需要我们去计算。计算方式就是先获取索引对应的B+树的区间最左记录和区间最右记录，然后再计算这两条记录之间有多少记录（记录条数少的时候可以做到精确计算，多的时候只能估算）。MySQL把这种通过直接访问索引对应的B+树来计算某个范围区间对应的索引记录条数的方式称之为index dive。 index dive就是直接利用索引对应的B+树来计算某个范围区间对应的记录条数。 有几个单点区间的话，使用index dive的方式去计算这些单点区间对应的记录数也不是什么问题，可是如果很多的话，这就意味着MySQL的查询优化器为了计算这些单点区间对应的索引记录条数，要进行20000次index dive操作，这性能损耗可就大了，搞不好计算这些单点区间对应的索引记录条数的成本比直接全表扫描的成本都大了。MySQL提供了一个系统变量eq_range_index_dive_limit，我们看一下在MySQL 5.7.21中这个系统变量的默认值： 1234567mysql&gt; SHOW VARIABLES LIKE &#x27;%dive%&#x27;;+---------------------------+-------+| Variable_name | Value |+---------------------------+-------+| eq_range_index_dive_limit | 200 |+---------------------------+-------+1 row in set (0.08 sec) 也就是说如果我们的IN语句中的参数个数小于200个的话，将使用index dive的方式计算各个单点区间对应的记录条数，如果大于或等于200个的话，可就不能使用index dive了，要使用所谓的索引统计数据来进行估算。 像会为每个表维护一份统计数据一样，MySQL也会为表中的每一个索引维护一份统计数据，查看某个表中索引的统计数据可以使用SHOW INDEX FROM 表名的语法，比如我们查看一下single_table的各个索引的统计数据可以这么写： 12345678910111213mysql&gt; SHOW INDEX FROM single_table;+--------------+------------+--------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+| Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation | Cardinality | Sub_part | Packed | Null | Index_type | Comment | Index_comment |+--------------+------------+--------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+| single_table | 0 | PRIMARY | 1 | id | A | 9693 | NULL | NULL | | BTREE | | || single_table | 0 | idx_key2 | 1 | key2 | A | 9693 | NULL | NULL | YES | BTREE | | || single_table | 1 | idx_key1 | 1 | key1 | A | 968 | NULL | NULL | YES | BTREE | | || single_table | 1 | idx_key3 | 1 | key3 | A | 799 | NULL | NULL | YES | BTREE | | || single_table | 1 | idx_key_part | 1 | key_part1 | A | 9673 | NULL | NULL | YES | BTREE | | || single_table | 1 | idx_key_part | 2 | key_part2 | A | 9999 | NULL | NULL | YES | BTREE | | || single_table | 1 | idx_key_part | 3 | key_part3 | A | 10000 | NULL | NULL | YES | BTREE | | |+--------------+------------+--------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+7 rows in set (0.01 sec) 属性名 描述 Table 索引所属表的名称。 Non_unique 索引列的值是否是唯一的，聚簇索引和唯一二级索引的该列值为0 ，普通二级索引该列值为1 。 Key_name 索引的名称。 Seq_in_index 索引列在索引中的位置，从1开始计数。比如对于联合索引idx_key_part ，来说，key_part1 、key_part2 和key_part3 对应的位置分别是1、2、3。 Column_name 索引列的名称。 Collation 索引列中的值是按照何种排序方式存放的，值为A 时代表升序存放，为NULL 时代表降序存放。 Cardinality 索引列中不重复值的数量。后边我们会重点看这个属性的。 Sub_part 对于存储字符串或者字节串的列来说，有时候我们只想对这些串的前n 个字符或字节建立索引，这个属性表示的就是那个n 值。如果对完整的列建立索引的话，该属性的值就是NULL 。 Packed 索引列如何被压缩，NULL 值表示未被压缩。这个属性我们暂时不了解，可以先忽略掉。 Null 该索引列是否允许存储NULL 值。 Index_type 使用索引的类型，我们最常见的就是BTREE ，其实也就是B+ 树索引。 Comment 索引列注释信息。 Index_comment 索引注释信息。 上述属性其实我们现在最在意的是Cardinality属性，Cardinality直译过来就是基数的意思，表示索引列中不重复值的个数。比如对于一个一万行记录的表来说，某个索引列的Cardinality属性是10000，那意味着该列中没有重复的值，如果Cardinality属性是1的话，就意味着该列的值全部是重复的。不过需要注意的是，对于InnoDB存储引擎来说，使用SHOW INDEX语句展示出来的某个索引列的Cardinality属性是一个估计值，并不是精确的。 前边说道，当IN语句中的参数个数大于或等于系统变量eq_range_index_dive_limit的值的话，就不会使用index dive的方式计算各个单点区间对应的索引记录条数，而是使用索引统计数据，这里所指的索引统计数据指的是这两个值： 使用SHOW TABLE STATUS展示出的Rows值，也就是一个表中有多少条记录。 使用SHOW INDEX语句展示出的Cardinality属性。结合上一个Rows统计数据，我们可以针对索引列，计算出平均一个值重复多少次。 1一个值的重复次数 ≈ Rows ÷ Cardinality 以single_table表的idx_key1索引为例，它的Rows值是9693，它对应索引列key1的Cardinality值是968，所以我们可以计算key1列平均单个值的重复次数就是： 19693 ÷ 968 ≈ 10（条） 此时再看上边那条查询语句： 1SELECT * FROM single_table WHERE key1 IN (&#x27;aa1&#x27;, &#x27;aa2&#x27;, &#x27;aa3&#x27;, ... , &#x27;zzz&#x27;); 假设IN语句中有20000个参数的话，就直接使用统计数据来估算这些参数需要单点区间对应的记录条数了，每个参数大约对应10条记录，所以总共需要回表的记录数就是： 120000 x 10 = 200000 使用统计数据来计算单点区间对应的索引记录条数可比index dive的方式简单多了，但是不精确！。使用统计数据算出来的查询成本与实际所需的成本可能相差非常大。 在MySQL 5.7.3以及之前的版本中，eq_range_index_dive_limit的默认值为10，之后的版本默认值为200。所以如果采用的是5.7.3以及之前的版本的话，很容易采用索引统计数据而不是index dive的方式来计算查询成本。当查询中使用到了IN查询，但是却实际没有用到索引，就应该考虑一下是不是由于 eq_range_index_dive_limit 值太小导致的。 3. 连接查询的成本我们直接构造一个和single_table表一模一样的single_table2表。为了简便起见，我们把single_table表称为s1表，把single_table2表称为s2表。 3.1 条件过滤我们前边说过，MySQL中连接查询采用的是嵌套循环连接算法，驱动表会被访问一次，被驱动表可能会被访问多次，所以对于两表连接查询来说，它的查询成本由下边两个部分构成： 单次查询驱动表的成本 多次查询被驱动表的成本（具体查询多少次取决于对驱动表查询的结果集中有多少条记录） 我们把对驱动表进行查询后得到的记录条数称之为驱动表的扇出（英文名：fanout）。很显然驱动表的扇出值越小，对被驱动表的查询次数也就越少，连接查询的总成本也就越低。当查询优化器想计算整个连接查询所使用的成本时，就需要计算出驱动表的扇出值，有的时候扇出值的计算是很容易的，比如下边这两个查询： 查询一：假设使用s1表作为驱动表，很显然对驱动表的单表查询只能使用全表扫描的方式执行，驱动表的扇出值也很明确，那就是驱动表中有多少记录，扇出值就是多少。我们前边说过，统计数据中s1表的记录行数是9693，也就是说优化器就直接会把9693当作在s1表的扇出值。 1SELECT * FROM single_table AS s1 INNER JOIN single_table2 AS s2; 查询二：仍然假设s1表是驱动表的话，很显然对驱动表的单表查询可以使用idx_key2索引执行查询。此时idx_key2的范围区间(10, 1000)中有多少条记录，那么扇出值就是多少。我们前边计算过，满足idx_key2的范围区间(10, 1000)的记录数是95条，也就是说本查询中优化器会把95当作驱动表s1的扇出值。 12SELECT * FROM single_table AS s1 INNER JOIN single_table2 AS s2 WHERE s1.key2 &gt;10 AND s1.key2 &lt; 1000; 有的时候扇出值的计算就变得很棘手，比方说下边几个查询： 查询三：本查询和查询一类似，只不过对于驱动表s1多了一个common_field &gt; &#39;xyz&#39;的搜索条件。查询优化器又不会真正的去执行查询，所以它只能猜这9693记录里有多少条记录满足common_field &gt; &#39;xyz&#39;条件。 12SELECT * FROM single_table AS s1 INNER JOIN single_table2 AS s2 WHERE s1.common_field &gt; &#x27;xyz&#x27;; 查询四：本查询和查询二类似，只不过对于驱动表s1也多了一个common_field &gt; &#39;xyz&#39;的搜索条件。不过因为本查询可以使用idx_key2索引，所以只需要从符合二级索引范围区间的记录中猜有多少条记录符合common_field &gt; &#39;xyz&#39;条件，也就是只需要猜在95条记录中有多少符合common_field &gt; &#39;xyz&#39;条件。 123SELECT * FROM single_table AS s1 INNER JOIN single_table2 AS s2 WHERE s1.key2 &gt; 10 AND s1.key2 &lt; 1000 AND s1.common_field &gt; &#x27;xyz&#x27;; 查询五：本查询和查询二类似，不过在驱动表s1选取idx_key2索引执行查询后，优化器需要从符合二级索引范围区间的记录中猜有多少条记录符合下边两个条件： 1234SELECT * FROM single_table AS s1 INNER JOIN single_table2 AS s2 WHERE s1.key2 &gt; 10 AND s1.key2 &lt; 1000 AND s1.key1 IN (&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;) AND s1.common_field &gt; &#x27;xyz&#x27;; key1 IN (&#39;a&#39;, &#39;b&#39;, &#39;c&#39;) common_field &gt; &#39;xyz&#39; 也就是优化器需要猜在95条记录中有多少符合上述两个条件的。 说了这么多，其实就是想表达在这两种情况下计算驱动表扇出值时需要靠猜： 如果使用的是全表扫描的方式执行的单表查询，那么计算驱动表扇出时需要猜满足搜索条件的记录到底有多少条。 如果使用的是索引执行的单表扫描，那么计算驱动表扇出的时候需要猜满足除使用到对应索引的搜索条件外的其他搜索条件的记录有多少条。 MySQL把这个猜的过程称之为condition filtering。当然，这个过程可能会使用到索引，也可能使用到统计数据，也可能就是MySQL单纯的瞎猜。 在MySQL 5.7之前的版本中，查询优化器在计算驱动表扇出时，如果是使用全表扫描的话，就直接使用表中记录的数量作为扇出值，如果使用索引的话，就直接使用满足范围条件的索引记录条数作为扇出值。在MySQL 5.7中，MySQL引入了这个condition filtering的功能，就是还要猜一猜剩余的那些搜索条件能把驱动表中的记录再过滤多少条，其实本质上就是为了让成本估算更精确。 MySQL称之为启发式规则（heuristic）。 3.2 两表连接成本分析连接查询的成本计算公式是这样的： 1连接查询总成本 = 单次访问驱动表的成本 + 驱动表扇出数 x 单次访问被驱动表的成本 对于左（外）连接和右（外）连接查询来说，它们的驱动表是固定的，所以想要得到最优的查询方案只需要： 分别为驱动表和被驱动表选择成本最低的访问方法。 可是对于内连接来说，驱动表和被驱动表的位置是可以互换的，所以需要考虑两个方面的问题： 不同的表作为驱动表最终的查询成本可能是不同的，也就是需要考虑最优的表连接顺序。 然后分别为驱动表和被驱动表选择成本最低的访问方法。 很显然，计算内连接查询成本的方式更麻烦一些，下边我们就以内连接为例来看看如何计算出最优的连接查询方案。 左（外）连接和右（外）连接查询在某些特殊情况下可以被优化为内连接查询。 比如对于下边这个查询来说： 1234SELECT * FROM single_table AS s1 INNER JOIN single_table2 AS s2 ON s1.key1 = s2.common_field WHERE s1.key2 &gt; 10 AND s1.key2 &lt; 1000 AND s2.key2 &gt; 1000 AND s2.key2 &lt; 2000; 可以选择的连接顺序有两种： s1连接s2，也就是s1作为驱动表，s2作为被驱动表。 s2连接s1，也就是s2作为驱动表，s1作为被驱动表。 查询优化器需要分别考虑这两种情况下的最优查询成本，然后选取那个成本更低的连接顺序以及该连接顺序下各个表的最优访问方法作为最终的查询计划。我们分别来看一下（定性的分析一下，不像分析单表查询那样定量的分析了）： 使用s1作为驱动表的情况 分析对于驱动表的成本最低的执行方案首先看一下涉及s1表单表的搜索条件有哪些： s1.key2 &gt; 10 AND s1.key2 &lt; 1000 所以这个查询可能使用到idx_key2索引，从全表扫描和使用idx_key2这两个方案中选出成本最低的那个，很显然使用idx_key2执行查询的成本更低些。 然后分析对于被驱动表的成本最低的执行方案此时涉及被驱动表s2的搜索条件就是： s2.common_field = 常数（这是因为对驱动表s1结果集中的每一条记录，都需要进行一次被驱动表s2的访问，此时那些涉及两表的条件现在相当于只涉及被驱动表s2了。） s2.key2 &gt; 1000 AND s2.key2 &lt; 2000 很显然，第一个条件由于common_field没有用到索引，此时访问s2表时可用的方案也是全表扫描和使用idx_key2两种，假设使用idx_key2的成本更小。所以此时使用s1作为驱动表时的总成本就是（暂时不考虑使用join buffer对成本的影响）： 1使用idx_key2访问s1的成本 + s1的扇出 × 使用idx_key2访问s2的成本 使用s2作为驱动表的情况 分析对于驱动表的成本最低的执行方案首先看一下涉及s2表单表的搜索条件有哪些： s2.key2 &gt; 1000 AND s2.key2 &lt; 2000 所以这个查询可能使用到idx_key2索引，从全表扫描和使用idx_key2这两个方案中选出成本最低的那个，假设使用idx_key2执行查询的成本更低些。 然后分析对于被驱动表的成本最低的执行方案此时涉及被驱动表s1的搜索条件就是： s1.key1 = 常数 s1.key2 &gt; 10 AND s1.key2 &lt; 2000 这时就很有趣了，使用idx_key1可以进行ref方式的访问，使用idx_key2可以使用range方式的访问。这时优化器需要从全表扫描、使用idx_key1、使用idx_key2这几个方案里选出一个成本最低的方案。这里有个问题，因为idx_key2的范围区间是确定的：(10, 1000)，怎么计算使用idx_key2的成本我们上边已经说过了，可是在没有真正执行查询前，s1.key1 = 常数中的常数值我们并不知道，怎么衡量使用idx_key1执行查询的成本呢？其实很简单，直接使用索引统计数据就好了（就是索引列平均一个值重复多少次）。一般情况下，ref的访问方式要比range成本更低，这里假设使用idx_key1进行对s1的访问。所以此时使用s2作为驱动表时的总成本就是： 1使用idx_key2访问s2的成本 + s2的扇出 × 使用idx_key1访问s1的成本 最后优化器会比较这两种方式的最优访问成本，选取那个成本更低的连接顺序去真正的执行查询。从上边的计算过程也可以看出来，连接查询成本占大头的其实是驱动表扇出数 x 单次访问被驱动表的成本，所以我们的优化重点其实是下边这两个部分： 尽量减少驱动表的扇出 对被驱动表的访问成本尽量低这一点对于我们实际书写连接查询语句时十分有用，我们需要尽量在被驱动表的连接列上建立索引，这样就可以使用ref访问方法来降低访问被驱动表的成本了。如果可以，被驱动表的连接列最好是该表的主键或者唯一二级索引列，这样就可以把访问被驱动表的成本降到更低了。 3.3 多表连接的成本分析首先要考虑一下多表连接时可能产生出多少种连接顺序： 对于两表连接，比如表A和表B连接只有 AB、BA这两种连接顺序。其实相当于2 × 1 = 2种连接顺序。 对于三表连接，比如表A、表B、表C进行连接有ABC、ACB、BAC、BCA、CAB、CBA这么6种连接顺序。其实相当于3 × 2 × 1 = 6种连接顺序。 对于四表连接的话，则会有4 × 3 × 2 × 1 = 24种连接顺序。 对于n表连接的话，则有 n × (n-1) × (n-2) × ··· × 1种连接顺序，就是n的阶乘种连接顺序，也就是n!。 有n个表进行连接，MySQL查询优化器要每一种连接顺序的成本都计算一遍，不过MySQL想了很多办法减少计算非常多种连接顺序的成本的方法： 提前结束某种顺序的成本评估MySQL在计算各种链接顺序的成本之前，会维护一个全局的变量，这个变量表示当前最小的连接查询成本。如果在分析某个连接顺序的成本时，该成本已经超过当前最小的连接查询成本，那就不对该连接顺序继续往下分析了。比方说A、B、C三个表进行连接，已经得到连接顺序ABC是当前的最小连接成本，比方说10.0，在计算连接顺序BCA时，发现B和C的连接成本就已经大于10.0时，就不再继续往后分析BCA这个连接顺序的成本了。 系统变量optimizer_search_depth为了防止无穷无尽的分析各种连接顺序的成本，MySQL提出了optimizer_search_depth系统变量，如果连接表的个数小于该值，那么就继续穷举分析每一种连接顺序的成本，否则只对与optimizer_search_depth值相同数量的表进行穷举分析。很显然，该值越大，成本分析的越精确，越容易得到好的执行计划，但是消耗的时间也就越长，否则得到不是很好的执行计划，但可以省掉很多分析连接成本的时间。 根据某些规则压根儿就不考虑某些连接顺序即使是有上边两条规则的限制，但是分析多个表不同连接顺序成本花费的时间还是会很长，所以MySQL干脆提出了一些所谓的启发式规则（就是根据以往经验指定的一些规则），凡是不满足这些规则的连接顺序压根儿就不分析，这样可以极大的减少需要分析的连接顺序的数量，但是也可能造成错失最优的执行计划。他们提供了一个系统变量optimizer_prune_level来控制到底是不是用这些启发式规则。 4. 调节成本常数我们前边已经介绍了两个成本常数： 读取一个页面花费的成本默认是1.0 检测一条记录是否符合搜索条件的成本默认是0.2 其实除了这两个成本常数，MySQL还支持好多，它们被存储到了mysql数据库（这是一个系统数据库）的两个表中： 12345678mysql&gt; SHOW TABLES FROM mysql LIKE &#x27;%cost%&#x27;;+--------------------------+| Tables_in_mysql (%cost%) |+--------------------------+| engine_cost || server_cost |+--------------------------+2 rows in set (0.00 sec) 一条语句的执行其实是分为两层的： server层 存储引擎层 在server层进行连接管理、查询缓存、语法解析、查询优化等操作，在存储引擎层执行具体的数据存取操作。也就是说一条语句在server层中执行的成本是和它操作的表使用的存储引擎是没关系的，所以关于这些操作对应的成本常数就存储在了server_cost表中，而依赖于存储引擎的一些操作对应的成本常数就存储在了engine_cost表中。 4.1mysql.server_cost表server_cost表中在server层进行的一些操作对应的成本常数，具体内容如下： 123456789101112mysql&gt; SELECT * FROM mysql.server_cost;+------------------------------+------------+---------------------+---------+| cost_name | cost_value | last_update | comment |+------------------------------+------------+---------------------+---------+| disk_temptable_create_cost | NULL | 2018-01-20 12:03:21 | NULL || disk_temptable_row_cost | NULL | 2018-01-20 12:03:21 | NULL || key_compare_cost | NULL | 2018-01-20 12:03:21 | NULL || memory_temptable_create_cost | NULL | 2018-01-20 12:03:21 | NULL || memory_temptable_row_cost | NULL | 2018-01-20 12:03:21 | NULL || row_evaluate_cost | NULL | 2018-01-20 12:03:21 | NULL |+------------------------------+------------+---------------------+---------+6 rows in set (0.05 sec) 我们先看一下server_cost各个列都分别是什么意思： cost_name表示成本常数的名称。 cost_value表示成本常数对应的值。如果该列的值为NULL的话，意味着对应的成本常数会采用默认值。 last_update表示最后更新记录的时间。 comment注释。 从server_cost中的内容可以看出来，目前在server层的一些操作对应的成本常数有以下几种： 成本常数名称 默认值 描述 disk_temptable_create_cost 40.0 创建基于磁盘的临时表的成本，如果增大这个值的话会让优化器尽量少的创建基于磁盘的临时表。 disk_temptable_row_cost 1.0 向基于磁盘的临时表写入或读取一条记录的成本，如果增大这个值的话会让优化器尽量少的创建基于磁盘的临时表。 key_compare_cost 0.1 两条记录做比较操作的成本，多用在排序操作上，如果增大这个值的话会提升filesort 的成本，让优化器可能更倾向于使用索引完成排序而不是filesort 。 memory_temptable_create_cost 2.0 创建基于内存的临时表的成本，如果增大这个值的话会让优化器尽量少的创建基于内存的临时表。 memory_temptable_row_cost 0.2 向基于内存的临时表写入或读取一条记录的成本，如果增大这个值的话会让优化器尽量少的创建基于内存的临时表。 row_evaluate_cost 0.2 这个就是我们之前一直使用的检测一条记录是否符合搜索条件的成本，增大这个值可能让优化器更倾向于使用索引而不是直接全表扫描。 MySQL在执行诸如DISTINCT查询、分组查询、Union查询以及某些特殊条件下的排序查询都可能在内部先创建一个临时表，使用这个临时表来辅助完成查询（比如对于DISTINCT查询可以建一个带有UNIQUE索引的临时表，直接把需要去重的记录插入到这个临时表中，插入完成之后的记录就是结果集了）。在数据量大的情况下可能创建基于磁盘的临时表，也就是为该临时表使用MyISAM、InnoDB等存储引擎，在数据量不大时可能创建基于内存的临时表，也就是使用Memory存储引擎。创建临时表和对这个临时表进行写入和读取的操作代价还是很高的。 这些成本常数在server_cost中的初始值都是NULL，意味着优化器会使用它们的默认值来计算某个操作的成本，如果我们想修改某个成本常数的值的话，需要做两个步骤： 对我们感兴趣的成本常数做更新操作比方说我们想把检测一条记录是否符合搜索条件的成本增大到0.4，那么就可以这样写更新语句： 123UPDATE mysql.server_cost SET cost_value = 0.4 WHERE cost_name = &#x27;row_evaluate_cost&#x27;; 让系统重新加载这个表的值。使用下边语句即可： 1FLUSH OPTIMIZER_COSTS; 当然，在你修改完某个成本常数后想把它们再改回默认值的话，可以直接把cost_value的值设置为NULL，再使用FLUSH OPTIMIZER_COSTS语句让系统重新加载它就好了。 4.2mysql.engine_cost表engine_cost表表中在存储引擎层进行的一些操作对应的成本常数，具体内容如下： 12345678mysql&gt; SELECT * FROM mysql.engine_cost;+-------------+-------------+------------------------+------------+---------------------+---------+| engine_name | device_type | cost_name | cost_value | last_update | comment |+-------------+-------------+------------------------+------------+---------------------+---------+| default | 0 | io_block_read_cost | NULL | 2018-01-20 12:03:21 | NULL || default | 0 | memory_block_read_cost | NULL | 2018-01-20 12:03:21 | NULL |+-------------+-------------+------------------------+------------+---------------------+---------+2 rows in set (0.05 sec) 与server_cost相比，engine_cost多了两个列： engine_name列指成本常数适用的存储引擎名称。如果该值为default，意味着对应的成本常数适用于所有的存储引擎。 device_type列指存储引擎使用的设备类型，这主要是为了区分常规的机械硬盘和固态硬盘，不过在MySQL 5.7.21这个版本中并没有对机械硬盘的成本和固态硬盘的成本作区分，所以该值默认是0。 我们从engine_cost表中的内容可以看出来，目前支持的存储引擎成本常数只有两个： 成本常数名称 默认值 描述 io_block_read_cost 1.0 从磁盘上读取一个块对应的成本。请注意我使用的是块 ，而不是页 这个词儿。对于InnoDB 存储引擎来说，一个页 就是一个块，不过对于MyISAM 存储引擎来说，默认是以4096 字节作为一个块的。增大这个值会加重I/O 成本，可能让优化器更倾向于选择使用索引执行查询而不是执行全表扫描。 memory_block_read_cost 1.0 与上一个参数类似，只不过衡量的是从内存中读取一个块对应的成本。 怎么从内存中和从磁盘上读取一个块的默认成本是一样的？这主要是因为在MySQL目前的实现中，并不能准确预测某个查询需要访问的块中有哪些块已经加载到内存中，有哪些块还停留在磁盘上，所以MySQL认为不管这个块有没有加载到内存中，使用的成本都是1.0，不过随着MySQL的发展，等到可以准确预测哪些块在磁盘上，那些块在内存中的那一天，这两个成本常数的默认值可能会改。 与更新server_cost表中的记录一样，我们也可以通过更新engine_cost表中的记录来更改关于存储引擎的成本常数，我们也可以通过为engine_cost表插入新记录的方式来添加只针对某种存储引擎的成本常数： 插入针对某个存储引擎的成本常数比如我们想增大InnoDB存储引擎页面I/O的成本，书写正常的插入语句即可： 123INSERT INTO mysql.engine_cost VALUES (&#x27;InnoDB&#x27;, 0, &#x27;io_block_read_cost&#x27;, 2.0, CURRENT_TIMESTAMP, &#x27;increase Innodb I/O cost&#x27;); 让系统重新加载这个表的值。使用下边语句即可： 1FLUSH OPTIMIZER_COSTS; 5. 总结在MySQL中，一个查询的执行成本是由IO成本和CPU成本组成的。对于InnoDB存储引擎来说，读取一个页面的默认IO成本是1.0，读取以及检测一条记录是否符合搜索条件的成本默认是0.2。 在单表查询中，优化器生成执行计划的步骤如下： 根据搜索条件，找出所有可能使用的索引 计算全表扫描的代价 计算使用不同索引执行查询的代价 对比各种执行方案的代价，找出成本最低的那个方案 在优化器生成执行计划过程中，需要依赖一些数据。这些数据可能是使用下面两种方式得到的： index dive：通过直接访问索引对应的B+树来获取数据 索引统计数据：直接依赖对表或者索引的统计数据 为了更准确的计算连接查询的成本，MySQL提出了条件过滤的概念，也就是采用了某些规则来预测驱动表的扇出值。 对于内连接来说，为了生成成本最低的执行计划，需要考虑两方面的事情： 选择最优的表连接顺序 为驱动表和被驱动表选择成本最低的访问方法 我们可以通过手动修改MySQL数据库下engine_cost &amp; server_cost表中的某些成本常数，更精确的控制在生成执行计划时的成本计算过程。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/tags/MySQL/"}]},{"title":"MySQL[六]单表查询&连接查询原理","slug":"MySQL/MySQL[六]单表查询&连接查询原理","date":"2022-01-05T16:00:00.000Z","updated":"2022-01-12T00:41:29.503Z","comments":true,"path":"2022/01/06/MySQL/MySQL[六]单表查询&连接查询原理/","link":"","permalink":"https://yinhuidong.github.io/2022/01/06/MySQL/MySQL[%E5%85%AD]%E5%8D%95%E8%A1%A8%E6%9F%A5%E8%AF%A2&%E8%BF%9E%E6%8E%A5%E6%9F%A5%E8%AF%A2%E5%8E%9F%E7%90%86/","excerpt":"","text":"一，单表查询不会走之前不要跑，在学SQL优化之前，我们先来分析下SQL是怎么执行的。 前面说过，MySQL Server有一个称为查询优化器的模块，一条查询语句进行语法解析之后就会被交给查询优化器来进行优化，优化的结果就是生成一个所谓的执行计划，这个执行计划表明了应该使用哪些索引进行查询，表之间的连接顺序是啥样的，最后会按照执行计划中的步骤调用存储引擎提供的方法来真正的执行查询，并将查询结果返回给用户。 如果觉得我这篇博客讲的看不懂，回头看看我前面的几篇，MySQL是一个很复杂的东西，尽量不要跳着学，要静下心系统的来学习，之前我都是四处看帖子看博客，一直觉得自己MySQL迷迷糊糊，甚至成了痛点，所以决心写个MySQL专栏，系统的学习下。 我们前面创建过一张表，现在拿来复用下。 123456789101112131415CREATE TABLE single_table ( id INT NOT NULL AUTO_INCREMENT, key1 VARCHAR(100), key2 INT, key3 VARCHAR(100), key_part1 VARCHAR(100), key_part2 VARCHAR(100), key_part3 VARCHAR(100), common_field VARCHAR(100), PRIMARY KEY (id), KEY idx_key1 (key1), UNIQUE KEY idx_key2 (key2), KEY idx_key3 (key3), KEY idx_key_part(key_part1, key_part2, key_part3)) Engine=InnoDB CHARSET=utf8; 我们为这个single_table表建立了1个聚簇索引和4个二级索引，分别是： 为id列建立的聚簇索引。 为key1列建立的idx_key1二级索引。 为key2列建立的idx_key2二级索引，而且该索引是唯一二级索引。 为key3列建立的idx_key3二级索引。 为key_part1、key_part2、key_part3列建立的idx_key_part二级索引，这也是一个联合索引。 这张表我插入了一百万数据，用来做实验。 1.访问方法对于单个表的查询来说，MySQL把查询的执行方式大致分为下边两种： 使用全表扫描进行查询这种执行方式很好理解，就是把表的每一行记录都扫一遍嘛，把符合搜索条件的记录加入到结果集就完了。不管是啥查询都可以使用这种方式执行，当然，这种也是最笨的执行方式。 使用索引进行查询因为直接使用全表扫描的方式执行查询要遍历好多记录，所以代价可能太大了。如果查询语句中的搜索条件可以使用到某个索引，那直接使用索引来执行查询可能会加快查询执行的时间。使用索引来执行查询的方式五花八门，又可以细分为许多种类： 针对主键或唯一二级索引的等值查询 针对普通二级索引的等值查询 针对索引列的范围查询 直接扫描整个索引 MySQL把MySQL执行查询语句的方式称之为访问方法或者访问类型。同一个查询语句可能可以使用多种不同的访问方法来执行，虽然最后的查询结果都是一样的，但是执行的时间可能相差很多。 2.const1SELECT * FROM single_table WHERE id = 1438; MySQL会直接利用主键值在聚簇索引中定位对应的用户记录。 **B+**树叶子节点中的记录是按照索引列排序的，对于的聚簇索引来说，它对应的**B+**树叶子节点中的记录就是按照**id**列排序的。所以这样根据主键值定位一条记录的速度贼快。类似的，我们根据唯一二级索引列来定位一条记录的速度也是贼快的，比如下边这个查询： 1SELECT * FROM single_table WHERE key2 = 3841; 这个查询的执行过程的示意图就是这样： 这个查询的执行分两步： 先从idx_key2对应的B+树索引中根据key2列与常数的等值比较条件定位到一条二级索引记录 再根据该记录的id值到聚簇索引中获取到完整的用户记录 MySQL认为通过主键或者唯一二级索引列与常数的等值比较来定位一条记录非常快，所以他们把这种通过主键或者唯一二级索引列来定位一条记录的访问方法定义为：const，意思是常数级别的，代价是可以忽略不计的。不过这种const访问方法只能在主键列或者唯一二级索引列和一个常数进行等值比较时才有效，如果主键或者唯一二级索引是由多个列构成的话，索引中的每一个列都需要与常数进行等值比较，这个const访问方法才有效（这是因为只有该索引中全部列都采用等值比较才可以定位唯一的一条记录）。 对于唯一二级索引来说，查询该列为NULL值的情况比较特殊，比如这样： 1SELECT * FROM single_table WHERE key2 IS NULL; 因为唯一二级索引列并不限制 NULL 值的数量，所以上述语句可能访问到多条记录，也就是说 上边这个语句不可以使用const访问方法来执行。 3.ref有时候我们对某个普通的二级索引列与常数进行等值比较，比如这样： 1SELECT * FROM single_table WHERE key1 = &#x27;abc&#x27;; 对于这个查询，我们当然可以选择全表扫描来逐一对比搜索条件是否满足要求，我们也可以先使用二级索引找到对应记录的id值，然后再回表到聚簇索引中查找完整的用户记录。由于普通二级索引并不限制索引列值的唯一性，所以可能找到多条对应的记录，也就是说使用二级索引来执行查询的代价取决于等值匹配到的二级索引记录条数。如果匹配的记录较少，则回表的代价还是比较低的，所以MySQL可能选择使用索引而不是全表扫描的方式来执行查询。MySQL把这种搜索条件为二级索引列与常数等值比较，采用二级索引来执行查询的访问方法称为：ref。我们看一下采用ref访问方法执行查询的图示： 对于普通的二级索引来说，通过索引列进行等值比较后可能匹配到多条连续的记录，而不是像主键或者唯一二级索引那样最多只能匹配1条记录，所以这种ref访问方法比const差了那么一点，但是在二级索引等值比较时匹配的记录数较少时的效率还是很高的（如果匹配的二级索引记录太多那么回表的成本就太大了）。 有两种特殊情况： 二级索引列值为NULL的情况不论是普通的二级索引，还是唯一二级索引，它们的索引列对包含NULL值的数量并不限制，所以我们采用key IS NULL这种形式的搜索条件最多只能使用ref的访问方法，而不是const的访问方法。 对于某个包含多个索引列的二级索引来说，只要是最左边的连续索引列是与常数的等值比较就可能采用ref的访问方法，比方说下边这几个查询： 12345SELECT * FROM single_table WHERE key_part1 = &#x27;god like&#x27;;SELECT * FROM single_table WHERE key_part1 = &#x27;god like&#x27; AND key_part2 = &#x27;legendary&#x27;;SELECT * FROM single_table WHERE key_part1 = &#x27;god like&#x27; AND key_part2 = &#x27;legendary&#x27; AND key_part3 = &#x27;penta kill&#x27;; 但是如果最左边的连续索引列并不全部是等值比较的话，它的访问方法就不能称为ref了，比方说这样： 1SELECT * FROM single_table WHERE key_part1 = &#x27;god like&#x27; AND key_part2 &gt; &#x27;legendary&#x27;; 4.ref_or_null有时候我们不仅想找出某个二级索引列的值等于某个常数的记录，还想把该列的值为NULL的记录也找出来，就像下边这个查询： 1SELECT * FROM single_table WHERE key1 = &#x27;abc&#x27; OR key1 IS NULL; 当使用二级索引而不是全表扫描的方式执行该查询时，这种类型的查询使用的访问方法就称为ref_or_null，这个ref_or_null访问方法的执行过程如下： 上边的查询相当于先分别从idx_key1索引对应的B+树中找出key1 IS NULL和key1 = &#39;abc&#39;的两个连续的记录范围，然后根据这些二级索引记录中的id值再回表查找完整的用户记录。 5.range1SELECT * FROM single_table WHERE key2 IN (1438, 6328) OR (key2 &gt;= 38 AND key2 &lt;= 79); 如果采用二级索引 + 回表的方式来执行的话，那么此时的搜索条件就不只是要求索引列与常数的等值匹配了，而是索引列需要匹配某个或某些范围的值，在本查询中key2列的值只要匹配下列3个范围中的任何一个就算是匹配成功了： key2的值是1438 key2的值是6328 key2的值在38和79之间。 MySQL把这种利用索引进行范围匹配的访问方法称之为：range。 此处所说的使用索引进行范围匹配中的 索引 可以是聚簇索引，也可以是二级索引。 我们可以把那种索引列等值匹配的情况称之为单点区间，上边所说的范围1和范围2都可以被称为单点区间，像范围3这种的我们可以称为连续范围区间。 6.index1SELECT key_part1, key_part2, key_part3 FROM single_table WHERE key_part2 = &#x27;abc&#x27;; 由于key_part2并不是联合索引idx_key_part最左索引列，所以我们无法使用ref或者range访问方法来执行这个语句。但是这个查询符合下边这两个条件： 它的查询列表只有3个列：key_part1, key_part2, key_part3，而索引idx_key_part又包含这三个列。 搜索条件中只有key_part2列。这个列也包含在索引idx_key_part中。 也就是说我们可以直接通过遍历idx_key_part索引的叶子节点的记录来比较key_part2 = &#39;abc&#39;这个条件是否成立，把匹配成功的二级索引记录的key_part1, key_part2, key_part3列的值直接加到结果集中就行了。由于二级索引记录比聚簇索记录小的多（聚簇索引记录要存储所有用户定义的列以及所谓的隐藏列，而二级索引记录只需要存放索引列和主键），而且这个过程也不用进行回表操作，所以直接遍历二级索引比直接遍历聚簇索引的成本要小很多，MySQL就把这种采用遍历二级索引记录的执行方式称之为：index。 7.all全表扫描 8.注意8.1 二级索引 + 回表一般情况下只能利用单个二级索引执行查询，比方说下边的这个查询： 1SELECT * FROM single_table WHERE key1 = &#x27;abc&#x27; AND key2 &gt; 1000; 查询优化器会识别到这个查询中的两个搜索条件： key1 = &#39;abc&#39; key2 &gt; 1000 优化器一般会根据single_table表的统计数据来判断到底使用哪个条件到对应的二级索引中查询扫描的行数会更少，选择那个扫描行数较少的条件到对应的二级索引中查询。然后将从该二级索引中查询到的结果经过回表得到完整的用户记录后再根据其余的WHERE条件过滤记录。一般来说，等值查找比范围查找需要扫描的行数更少（也就是ref的访问方法一般比range好，但这也不总是一定的，也可能采用ref访问方法的那个索引列的值为特定值的行数特别多），所以这里假设优化器决定使用idx_key1索引进行查询，那么整个查询过程可以分为两个步骤： 使用二级索引定位记录的阶段，也就是根据条件key1 = &#39;abc&#39;从idx_key1索引代表的B+树中找到对应的二级索引记录。 回表阶段，也就是根据上一步骤中找到的记录的主键值进行回表操作，也就是到聚簇索引中找到对应的完整的用户记录，再根据条件key2 &gt; 1000到完整的用户记录继续过滤。将最终符合过滤条件的记录返回给用户。 注意，因为二级索引的节点中的记录只包含索引列和主键，所以在步骤1中使用idx_key1索引进行查询时只会用到与key1列有关的搜索条件，其余条件，比如key2 &gt; 1000这个条件在步骤1中是用不到的，只有在步骤2完成回表操作后才能继续针对完整的用户记录中继续过滤。 一般情况下执行一个查询只会用到单个二级索引，不过还是有特殊情况的。 从上文可以看出，每次从二级索引中读取到一条记录后，就会根据该记录的主键值执行回表操作。而在某个扫描区间中的二级索引记录的主键值是无序的，也就是说这些二级索引记录对应的聚簇索引记录所在的页面的页号是无序的。每次执行回表操作时都相当于要随机读取一个聚簇索引页面，而这些随机I/O带来的性能开销比较大。于是MySQL提出了一个名为Disk-S weep Multi-Range Read(MRR，多范围读取)的优化措施，即先读取一部分二级索引记录，将它们的主键值排好序之后再统一执行回表操作。相对于每读取一条二级索引记录 就立即执行回表操作，这样会节省一些I/0开销。当然使用这个MRR优化措施的条件比较苛刻，我们之前的讨论中没有涉及MRR 之后的讨论中也将忽略这项优化措施，直接认为每读取一条二级索引记录就立即执行回表操作。 8.2 range访问方法使用的范围区间其实对于B+树索引来说，只要索引列和常数使用=、&lt;=&gt;、IN、NOT IN、IS NULL、IS NOT NULL、&gt;、&lt;、&gt;=、&lt;=、BETWEEN、!=（不等于也可以写成&lt;&gt;）或者LIKE操作符连接起来，就可以产生一个所谓的区间。 LIKE操作符比较特殊，只有在匹配完整字符串或者匹配字符串前缀时才可以利用索引。 IN操作符的效果和若干个等值匹配操作符=之间用OR连接起来是一样的，也就是说会产生多个单点区间，比如下边这两个语句的效果是一样的： 123SELECT * FROM single_table WHERE key2 IN (1438, 6328); SELECT * FROM single_table WHERE key2 = 1438 OR key2 = 6328; 在日常的工作中，一个查询的WHERE子句可能有很多个小的搜索条件，这些搜索条件需要使用AND或者OR操作符连接起来。当我们想使用range访问方法来执行一个查询语句时，重点就是找出该查询可用的索引以及这些索引对应的范围区间。 9.索引合并MySQL在一般情况下执行一个查询时最多只会用到单个二级索引，但是还有特殊情况，在这些特殊情况下也可能在一个查询中使用到多个二级索引，MySQL把这种使用到多个索引来完成一次查询的执行方法称之为：index merge，具体的索引合并算法有下边三种。 9.1 Intersection合并Intersection翻译过来的意思是交集。这里是说某个查询可以使用多个二级索引，将从多个二级索引中查询到的结果取交集，比方说下边这个查询： 1SELECT * FROM single_table WHERE key1 = &#x27;a&#x27; AND key3 = &#x27;b&#x27;; 假设这个查询使用Intersection合并的方式执行的话，那这个过程就是这样的： 从idx_key1二级索引对应的B+树中取出key1 = &#39;a&#39;的相关记录。 从idx_key3二级索引对应的B+树中取出key3 = &#39;b&#39;的相关记录。 二级索引的记录都是由索引列 + 主键构成的，所以我们可以计算出这两个结果集中id值的交集。 按照上一步生成的id值列表进行回表操作，也就是从聚簇索引中把指定id值的完整用户记录取出来，返回给用户。 为啥不直接使用idx_key1或者idx_key3只根据某个搜索条件去读取一个二级索引，然后回表后再过滤另外一个搜索条件呢？这里要分析一下两种查询执行方式之间需要的成本代价。 只读取一个二级索引的成本： 按照某个搜索条件读取一个二级索引 根据从该二级索引得到的主键值进行回表操作，然后再过滤其他的搜索条件 读取多个二级索引之后取交集成本： 按照不同的搜索条件分别读取不同的二级索引 将从多个二级索引得到的主键值取交集，然后进行回表操作 虽然读取多个二级索引比读取一个二级索引消耗性能，但是读取二级索引的操作是顺序I/O，而回表操作是随机I/O，所以如果只读取一个二级索引时需要回表的记录数特别多，而读取多个二级索引之后取交集的记录数非常少，当节省的因为回表而造成的性能损耗比访问多个二级索引带来的性能损耗更高时，读取多个二级索引后取交集比只读取一个二级索引的成本更低。 MySQL在某些特定的情况下才可能会使用到Intersection索引合并： 情况一：二级索引列是等值匹配的情况，对于联合索引来说，在联合索引中的每个列都必须等值匹配，不能出现只匹配部分列的情况。比方说下边这个查询可能用到idx_key1和idx_key_part这两个二级索引进行Intersection索引合并的操作： 1SELECT * FROM single_table WHERE key1 = &#x27;a&#x27; AND key_part1 = &#x27;a&#x27; AND key_part2 = &#x27;b&#x27; AND key_part3 = &#x27;c&#x27;; 而下边这两个查询就不能进行Intersection索引合并： 123SELECT * FROM single_table WHERE key1 &gt; &#x27;a&#x27; AND key_part1 = &#x27;a&#x27; AND key_part2 = &#x27;b&#x27; AND key_part3 = &#x27;c&#x27;;SELECT * FROM single_table WHERE key1 = &#x27;a&#x27; AND key_part1 = &#x27;a&#x27;; 第一个查询是因为对key1进行了范围匹配，第二个查询是因为联合索引idx_key_part中的key_part2和key_part3列并没有出现在搜索条件中，所以这两个查询不能进行Intersection索引合并。 情况二：主键列可以是范围匹配比方说下边这个查询可能用到主键和idx_key1进行Intersection索引合并的操作： 1SELECT * FROM single_table WHERE id &gt; 100 AND key1 = &#x27;a&#x27;; 对于InnoDB的二级索引来说，记录先是按照索引列进行排序，如果该二级索引是一个联合索引，那么会按照联合索引中的各个列依次排序。而二级索引的用户记录是由索引列 + 主键构成的，二级索引列的值相同的记录可能会有好多条，这些索引列的值相同的记录又是按照主键的值进行排序的。所以在二级索引列都是等值匹配的情况下才可能使用Intersection索引合并，是因为只有在这种情况下根据二级索引查询出的结果集是按照主键值排序的。 根据二级索引查询出的结果集是按照主键值排序的对使用**Intersection**索引合并的好处？Intersection索引合并会把从多个二级索引中查询出的主键值求交集，如果从各个二级索引中查询的到的结果集本身就是已经按照主键排好序的，那么求交集的过程就很简单。假设某个查询使用Intersection索引合并的方式从idx_key1和idx_key2这两个二级索引中获取到的主键值分别是： 从idx_key1中获取到已经排好序的主键值：1、3、5 从idx_key2中获取到已经排好序的主键值：2、3、4 那么求交集的过程就是这样：逐个取出这两个结果集中最小的主键值，如果两个值相等，则加入最后的交集结果中，否则丢弃当前较小的主键值，再取该丢弃的主键值所在结果集的后一个主键值来比较，直到某个结果集中的主键值用完了： 先取出这两个结果集中较小的主键值做比较，因为1 &lt; 2，所以把idx_key1的结果集的主键值1丢弃，取出后边的3来比较。 因为3 &gt; 2，所以把idx_key2的结果集的主键值2丢弃，取出后边的3来比较。 因为3 = 3，所以把3加入到最后的交集结果中，继续两个结果集后边的主键值来比较。 后边的主键值也不相等，所以最后的交集结果中只包含主键值3。 这个过程其实很快，时间复杂度是O(n)，但是如果从各个二级索引中查询出的结果集并不是按照主键排序的话，那就要先把结果集中的主键值排序完再来做上边的那个过程，就比较耗时了。 按照有序的主键值去回表取记录有个专有名词儿，叫：Rowid Ordered Retrieval，简称ROR。 另外，不仅是多个二级索引之间可以采用Intersection索引合并，索引合并也可以有聚簇索引参加，也就是我们上边写的情况二：在搜索条件中有主键的范围匹配的情况下也可以使用Intersection索引合并索引合并。 1SELECT * FROM single_table WHERE key1 = &#x27;a&#x27; AND id &gt; 100; 假设这个查询可以采用Intersection索引合并，我们理所当然的以为这个查询会分别按照id &gt; 100这个条件从聚簇索引中获取一些记录，在通过key1 = &#39;a&#39;这个条件从idx_key1二级索引中获取一些记录，然后再求交集，其实这样就把问题复杂化了，没必要从聚簇索引中获取一次记录。二级索引的记录中都带有主键值的，所以可以在从idx_key1中获取到的主键值上直接运用条件id &gt; 100过滤就行了。所以涉及主键的搜索条件只不过是为了从别的二级索引得到的结果集中过滤记录罢了，是不是等值匹配不重要。 当然，上边说的情况一和情况二只是发生Intersection索引合并的必要条件，不是充分条件。也就是说即使情况一、情况二成立，也不一定发生Intersection索引合并，这得看优化器的心情。优化器只有在单独根据搜索条件从某个二级索引中获取的记录数太多，导致回表开销太大，而通过Intersection索引合并后需要回表的记录数大大减少时才会使用Intersection索引合并。 9.2 Union合并有时候OR关系的不同搜索条件会使用到不同的索引。 1SELECT * FROM single_table WHERE key1 = &#x27;a&#x27; OR key3 = &#x27;b&#x27; Intersection是交集的意思，这适用于使用不同索引的搜索条件之间使用AND连接起来的情况；Union是并集的意思，适用于使用不同索引的搜索条件之间使用OR连接起来的情况。与Intersection索引合并类似，MySQL在某些特定的情况下才可能会使用到Union索引合并： 情况一：二级索引列是等值匹配的情况，对于联合索引来说，在联合索引中的每个列都必须等值匹配，不能出现只出现匹配部分列的情况。比方说下边这个查询可能用到idx_key1和idx_key_part这两个二级索引进行Union索引合并的操作： 1SELECT * FROM single_table WHERE key1 = &#x27;a&#x27; OR ( key_part1 = &#x27;a&#x27; AND key_part2 = &#x27;b&#x27; AND key_part3 = &#x27;c&#x27;); 而下边这两个查询就不能进行Union索引合并： 123SELECT * FROM single_table WHERE key1 &gt; &#x27;a&#x27; OR (key_part1 = &#x27;a&#x27; AND key_part2 = &#x27;b&#x27; AND key_part3 = &#x27;c&#x27;);SELECT * FROM single_table WHERE key1 = &#x27;a&#x27; OR key_part1 = &#x27;a&#x27;; 第一个查询是因为对key1进行了范围匹配，第二个查询是因为联合索引idx_key_part中的key_part2和key_part3列并没有出现在搜索条件中，所以这两个查询不能进行Union索引合并。 情况二：主键列可以是范围匹配 情况三：使用Intersection索引合并的搜索条件 这种情况其实就是搜索条件的某些部分使用Intersection索引合并的方式得到的主键集合和其他方式得到的主键集合取交集，比方说这个查询： 1SELECT * FROM single_table WHERE key_part1 = &#x27;a&#x27; AND key_part2 = &#x27;b&#x27; AND key_part3 = &#x27;c&#x27; OR (key1 = &#x27;a&#x27; AND key3 = &#x27;b&#x27;); 优化器可能采用这样的方式来执行这个查询： 先按照搜索条件key1 = &#39;a&#39; AND key3 = &#39;b&#39;从索引idx_key1和idx_key3中使用Intersection索引合并的方式得到一个主键集合。 再按照搜索条件key_part1 = &#39;a&#39; AND key_part2 = &#39;b&#39; AND key_part3 = &#39;c&#39;从联合索引idx_key_part中得到另一个主键集合。 采用Union索引合并的方式把上述两个主键集合取并集，然后进行回表操作，将结果返回给用户。 当然，查询条件符合了这些情况也不一定就会采用Union索引合并，也得看优化器的心情。优化器只有在单独根据搜索条件从某个二级索引中获取的记录数比较少，通过Union索引合并后进行访问的代价比全表扫描更小时才会使用Union索引合并。 9.3 Sort-Union合并Union索引合并的使用条件太苛刻，必须保证各个二级索引列在进行等值匹配的条件下才可能被用到，比方说下边这个查询就无法使用到Union索引合并： 1SELECT * FROM single_table WHERE key1 &lt; &#x27;a&#x27; OR key3 &gt; &#x27;z&#x27; 这是因为根据key1 &lt; &#39;a&#39;从idx_key1索引中获取的二级索引记录的主键值不是排好序的，根据key3 &gt; &#39;z&#39;从idx_key3索引中获取的二级索引记录的主键值也不是排好序的，但是key1 &lt; &#39;a&#39;和key3 &gt; &#39;z&#39;这两个条件又特别让我们动心，所以我们可以这样： 先根据key1 &lt; &#39;a&#39;条件从idx_key1二级索引中获取记录，并按照记录的主键值进行排序 再根据key3 &gt; &#39;z&#39;条件从idx_key3二级索引中获取记录，并按照记录的主键值进行排序 因为上述的两个二级索引主键值都是排好序的，剩下的操作和Union索引合并方式就一样了。 我们把上述这种先按照二级索引记录的主键值进行排序，之后按照Union索引合并方式执行的方式称之为Sort-Union索引合并，很显然，这种Sort-Union索引合并比单纯的Union索引合并多了一步对二级索引记录的主键值排序的过程。 为啥有Sort-Union索引合并，就没有Sort-Intersection索引合并么？是的，的确没有Sort-Intersection索引合并这么一说， Sort-Union的适用场景是单独根据搜索条件从某个二级索引中获取的记录数比较少，这样即使对这些二级索引记录按照主键值进行排序的成本也不会太高 而Intersection索引合并的适用场景是单独根据搜索条件从某个二级索引中获取的记录数太多，导致回表开销太大，合并后可以明显降低回表开销，但是如果加入Sort-Intersection后，就需要为大量的二级索引记录按照主键值进行排序，这个成本可能比回表查询都高了，所以也就没有引入Sort-Intersection。 9.4 索引合并注意事项联合索引替代Intersection索引合并1SELECT * FROM single_table WHERE key1 = &#x27;a&#x27; AND key3 = &#x27;b&#x27;; 这个查询之所以可能使用Intersection索引合并的方式执行，还不是因为idx_key1和idx_key3是两个单独的B+树索引，要是把这两个列搞一个联合索引，那直接使用这个联合索引就可以了： 1ALTER TABLE single_table drop index idx_key1, idx_key3, add index idx_key1_key3(key1, key3); 这样我们把没用的idx_key1、idx_key3都干掉，再添加一个联合索引idx_key1_key3，使用这个联合索引进行查询效果更好，既不用多读一棵B+树，也不用合并结果。 不过如果有单独对key3列进行查询的业务场景，这样子不得不再把key3列的单独索引给加上。具体还得以业务为准。 二，连接查询原理1. 连接简介1.1 连接的本质我们先建立两个简单的表并给它们填充一点数据： 1234567CREATE TABLE t1 (m1 int, n1 char(1));CREATE TABLE t2 (m2 int, n2 char(1));INSERT INTO t1 VALUES(1, &#x27;a&#x27;), (2, &#x27;b&#x27;), (3, &#x27;c&#x27;);INSERT INTO t2 VALUES(2, &#x27;b&#x27;), (3, &#x27;c&#x27;), (4, &#x27;d&#x27;); 我们成功建立了t1、t2两个表，这两个表都有两个列，一个是INT类型的，一个是CHAR(1)类型的。 12345678910111213141516171819mysql&gt; SELECT * FROM t1;+------+------+| m1 | n1 |+------+------+| 1 | a || 2 | b || 3 | c |+------+------+3 rows in set (0.00 sec)mysql&gt; SELECT * FROM t2;+------+------+| m2 | n2 |+------+------+| 2 | b || 3 | c || 4 | d |+------+------+3 rows in set (0.00 sec) 连接的本质就是把各个连接表中的记录都取出来依次匹配的组合加入结果集并返回给用户。所以我们把t1和t2两个表连接起来的过程如下图所示： 这个过程看起来就是把t1表的记录和t2的记录连起来组成新的更大的记录，所以这个查询过程称之为连接查询。连接查询的结果集中包含一个表中的每一条记录与另一个表中的每一条记录相互匹配的组合，像这样的结果集就可以称之为笛卡尔积。因为表t1中有3条记录，表t2中也有3条记录，所以这两个表连接之后的笛卡尔积就有3×3=9行记录。在MySQL中，连接查询的语法很简单，只要在FROM语句后边跟多个表名就好了，比如我们把t1表和t2表连接起来的查询语句可以写成这样： 1SELECT * FROM t1, t2; 1.2 连接过程简介在连接查询中的过滤条件可以分成两种： 涉及单表的条件这种只涉及单表的过滤条件我们之前都提到过一万遍了，我们之前也一直称为搜索条件，比如t1.m1 &gt; 1是只针对t1表的过滤条件，t2.n2 &lt; &#39;d&#39;是只针对t2表的过滤条件。 涉及两表的条件这种过滤条件我们之前没见过，比如t1.m1 = t2.m2、t1.n1 &gt; t2.n2等，这些条件中涉及到了两个表。 我们看一下携带过滤条件的连接查询的大致执行过程： 1SELECT * FROM t1, t2 WHERE t1.m1 &gt; 1 AND t1.m1 = t2.m2 AND t2.n2 &lt; &#x27;d&#x27;; 在这个查询中我们指明了这三个过滤条件： t1.m1 &gt; 1 t1.m1 = t2.m2 t2.n2 &lt; &#39;d&#39; 这个连接查询的大致执行过程如下： 首先确定第一个需要查询的表，这个表称之为驱动表。此处假设使用t1作为驱动表，那么就需要到t1表中找满足t1.m1 &gt; 1的记录，因为表中的数据太少，我们也没在表上建立二级索引，所以此处查询t1表的访问方法就设定为all吧，也就是采用全表扫描的方式执行单表查询，所以查询过程就如下图所示：我们可以看到，t1表中符合t1.m1 &gt; 1的记录有两条。 针对上一步骤中从驱动表产生的结果集中的每一条记录，分别需要到t2表中查找匹配的记录，所谓匹配的记录，指的是符合过滤条件的记录。因为是根据t1表中的记录去找t2表中的记录，所以t2表也可以被称之为被驱动表。上一步骤从驱动表中得到了2条记录，所以需要查询2次t2表。此时涉及两个表的列的过滤条件t1.m1 = t2.m2就派上用场了： 当t1.m1 = 2时，过滤条件t1.m1 = t2.m2就相当于t2.m2 = 2，所以此时t2表相当于有了t2.m2 = 2、t2.n2 &lt; &#39;d&#39;这两个过滤条件，然后到t2表中执行单表查询。 当t1.m1 = 3时，过滤条件t1.m1 = t2.m2就相当于t2.m2 = 3，所以此时t2表相当于有了t2.m2 = 3、t2.n2 &lt; &#39;d&#39;这两个过滤条件，然后到t2表中执行单表查询。 所以整个连接查询的执行过程就如下图所示： 也就是说整个连接查询最后的结果只有两条符合过滤条件的记录： 123456+------+------+------+------+| m1 | n1 | m2 | n2 |+------+------+------+------+| 2 | b | 2 | b || 3 | c | 3 | c |+------+------+------+------+ 这个两表连接查询共需要查询1次t1表，2次t2表。当然这是在特定的过滤条件下的结果，如果我们把t1.m1 &gt; 1这个条件去掉，那么从t1表中查出的记录就有3条，就需要查询3次t2表了。也就是说在两表连接查询中，驱动表只需要访问一次，被驱动表可能被访问多次。 1.3 内连接和外连接我们先创建两个有现实意义的表。 12345678910111213CREATE TABLE student ( number INT NOT NULL AUTO_INCREMENT COMMENT &#x27;学号&#x27;, name VARCHAR(5) COMMENT &#x27;姓名&#x27;, major VARCHAR(30) COMMENT &#x27;专业&#x27;, PRIMARY KEY (number)) Engine=InnoDB CHARSET=utf8 COMMENT &#x27;学生信息表&#x27;;CREATE TABLE score ( number INT COMMENT &#x27;学号&#x27;, subject VARCHAR(30) COMMENT &#x27;科目&#x27;, score TINYINT COMMENT &#x27;成绩&#x27;, PRIMARY KEY (number, subject)) Engine=InnoDB CHARSET=utf8 COMMENT &#x27;学生成绩表&#x27;; 我们新建了一个学生信息表，一个学生成绩表，然后我们向上述两个表中插入一些数据： 1234567891011121314151617181920mysql&gt; SELECT * FROM student;+----------+-----------+--------------------------+| number | name | major |+----------+-----------+--------------------------+| 20180101 | 杜子腾 | 软件学院 || 20180102 | 范统 | 计算机科学与工程 || 20180103 | 史珍香 | 计算机科学与工程 |+----------+-----------+--------------------------+3 rows in set (0.00 sec)mysql&gt; SELECT * FROM score;+----------+-----------------------------+-------+| number | subject | score |+----------+-----------------------------+-------+| 20180101 | 母猪的产后护理 | 78 || 20180101 | 论萨达姆的战争准备 | 88 || 20180102 | 论萨达姆的战争准备 | 98 || 20180102 | 母猪的产后护理 | 100 |+----------+-----------------------------+-------+4 rows in set (0.00 sec) 现在我们想把每个学生的考试成绩都查询出来就需要进行两表连接了（因为score中没有姓名信息，所以不能单纯只查询score表）。连接过程就是从student表中取出记录，在score表中查找number相同的成绩记录，所以过滤条件就是student.number = socre.number，整个查询语句就是这样： 1SELECT * FROM student, score WHERE student.number = score.number; 从上述查询结果中我们可以看到，各个同学对应的各科成绩就都被查出来了，可是有个问题，史珍香同学，也就是学号为20180103的同学因为某些原因没有参加考试，所以在score表中没有对应的成绩记录。那如果老师想查看所有同学的考试成绩，即使是缺考的同学也应该展示出来，但是到目前为止我们介绍的连接查询是无法完成这样的需求的。 这个需求的本质是：驱动表中的记录即使在被驱动表中没有匹配的记录，也仍然需要加入到结果集。为了解决这个问题，就有了内连接和外连接的概念。 对于内连接的两个表，驱动表中的记录在被驱动表中找不到匹配的记录，该记录不会加入到最后的结果集，我们上边提到的连接都是所谓的内连接。 对于外连接的两个表，驱动表中的记录即使在被驱动表中没有匹配的记录，也仍然需要加入到结果集。在MySQL中，根据选取驱动表的不同，外连接仍然可以细分为2种： 左外连接：选取左侧的表为驱动表。 右外连接：选取右侧的表为驱动表。 对于外连接来说，有时候我们也并不想把驱动表的全部记录都加入到最后的结果集。把过滤条件分为两种就可以了，所以放在不同地方的过滤条件是有不同语义的： WHERE子句中的过滤条件WHERE子句中的过滤条件就是我们平时见的那种，不论是内连接还是外连接，凡是不符合WHERE子句中的过滤条件的记录都不会被加入最后的结果集。 ON子句中的过滤条件对于外连接的驱动表的记录来说，如果无法在被驱动表中找到匹配ON子句中的过滤条件的记录，那么该记录仍然会被加入到结果集中，对应的被驱动表记录的各个字段使用NULL值填充。 需要注意的是，这个ON子句是专门为外连接驱动表中的记录在被驱动表找不到匹配记录时应不应该把该记录加入结果集这个场景下提出的，所以如果把ON子句放到内连接中，MySQL会把它和WHERE子句一样对待，也就是说：内连接中的WHERE子句和ON子句是等价的。 一般情况下，我们都把只涉及单表的过滤条件放到WHERE子句中，把涉及两表的过滤条件都放到ON子句中，我们也一般把放到ON子句中的过滤条件也称之为连接条件。 1.3.1 左（外）连接的语法比如我们要把t1表和t2表进行左外连接查询可以这么写： 1SELECT * FROM t1 LEFT [OUTER] JOIN t2 ON 连接条件 [WHERE 普通过滤条件]; 其中中括号里的OUTER单词是可以省略的。对于LEFT JOIN类型的连接来说，我们把放在左边的表称之为外表或者驱动表，右边的表称之为内表或者被驱动表。所以上述例子中t1就是外表或者驱动表，t2就是内表或者被驱动表。需要注意的是，对于左（外）连接和右（外）连接来说，必须使用ON子句来指出连接条件。 回到我们上边那个现实问题中来，看看怎样写查询语句才能把所有的学生的成绩信息都查询出来，即使是缺考的考生也应该被放到结果集中： 1SELECT s1.number, s1.name, s2.subject, s2.score FROM student AS s1 LEFT JOIN score AS s2 ON s1.number = s2.number; 从结果集中可以看出来，虽然史珍香并没有对应的成绩记录，但是由于采用的是连接类型为左（外）连接，所以仍然把她放到了结果集中，只不过在对应的成绩记录的各列使用NULL值填充而已。 1.3.2 右（外）连接的语法右（外）连接和左（外）连接的原理是一样一样的，语法也只是把LEFT换成RIGHT而已： 1SELECT * FROM t1 RIGHT [OUTER] JOIN t2 ON 连接条件 [WHERE 普通过滤条件]; 只不过驱动表是右边的表，被驱动表是左边的表。 1.3.3 内连接的语法内连接和外连接的根本区别就是在驱动表中的记录不符合ON子句中的连接条件时不会把该记录加入到最后的结果集。 一种最简单的内连接语法，就是直接把需要连接的多个表都放到FROM子句后边。其实针对内连接，MySQL提供了好多不同的语法，我们以t1和t2表为例： 1SELECT * FROM t1 [INNER | CROSS] JOIN t2 [ON 连接条件] [WHERE 普通过滤条件]; 也就是说在MySQL中，下边这几种内连接的写法都是等价的： SELECT * FROM t1 JOIN t2; SELECT * FROM t1 INNER JOIN t2; SELECT * FROM t1 CROSS JOIN t2; 上边的这些写法和直接把需要连接的表名放到FROM语句之后，用逗号,分隔开的写法是等价的： 1SELECT * FROM t1, t2; 在内连接中ON子句和WHERE子句是等价的，所以内连接中不要求强制写明ON子句。 前边说过，连接的本质就是把各个连接表中的记录都取出来依次匹配的组合加入结果集并返回给用户。不论哪个表作为驱动表，两表连接产生的笛卡尔积肯定是一样的。而对于内连接来说，由于凡是不符合ON子句或WHERE子句中的条件的记录都会被过滤掉，其实也就相当于从两表连接的笛卡尔积中把不符合过滤条件的记录给踢出去，所以对于内连接来说，驱动表和被驱动表是可以互换的，并不会影响最后的查询结果。但是对于外连接来说，由于驱动表中的记录即使在被驱动表中找不到符合ON子句条件的记录时也要将其加入到结果集，所以此时驱动表和被驱动表的关系就很重要了，也就是说左外连接和右外连接的驱动表和被驱动表不能轻易互换。 2.连接的原理接下来看一下MySQL采用了什么样的算法来进行表与表之间的连接。 2.1 嵌套循环连接对于两表连接来说，驱动表只会被访问一遍，但被驱动表却要被访问到好多遍，具体访问几遍取决于对驱动表执行单表查询后的结果集中的记录条数。对于内连接来说，选取哪个表为驱动表都没关系，而外连接的驱动表是固定的，也就是说左（外）连接的驱动表就是左边的那个表，右（外）连接的驱动表就是右边的那个表。 再来看一下t1表和t2表执行内连接查询的大致过程： 步骤1：选取驱动表，使用与驱动表相关的过滤条件，选取代价最低的单表访问方法来执行对驱动表的单表查询。 步骤2：对上一步骤中查询驱动表得到的结果集中每一条记录，都分别到被驱动表中查找匹配的记录。 通用的两表连接过程如下图所示： 如果有3个表进行连接的话，那么步骤2中得到的结果集就像是新的驱动表，然后第三个表就成为了被驱动表，重复上边过程，也就是步骤2中得到的结果集中的每一条记录都需要到t3表中找一找有没有匹配的记录，用伪代码表示一下这个过程就是这样： 123456789for each row in t1 &#123; #此处表示遍历满足对t1单表查询结果集中的每一条记录 for each row in t2 &#123; #此处表示对于某条t1表的记录来说，遍历满足对t2单表查询结果集中的每一条记录 for each row in t3 &#123; #此处表示对于某条t1和t2表的记录组合来说，对t3表进行单表查询 if row satisfies join conditions, send to client &#125; &#125;&#125; 这个过程就像是一个嵌套的循环，所以这种驱动表只访问一次，但被驱动表却可能被多次访问，访问次数取决于对驱动表执行单表查询后的结果集中的记录条数的连接执行方式称之为嵌套循环连接（Nested-Loop Join），这是最简单，也是最笨拙的一种连接查询算法。 2.2 使用索引加快连接速度在嵌套循环连接的步骤2中可能需要访问多次被驱动表，如果访问被驱动表的方式都是全表扫描的话，要查很多次。但是查询t2表其实就相当于一次单表扫描，我们可以利用索引来加快查询速度。回到最开始的t1表和t2表进行内连接的例子： 1SELECT * FROM t1, t2 WHERE t1.m1 &gt; 1 AND t1.m1 = t2.m2 AND t2.n2 &lt; &#x27;d&#x27;; 查询驱动表t1后的结果集中有两条记录，嵌套循环连接算法需要对被驱动表查询2次： 当t1.m1 = 2时，去查询一遍t2表，对t2表的查询语句相当于： 1SELECT * FROM t2 WHERE t2.m2 = 2 AND t2.n2 &lt; &#x27;d&#x27;; 当t1.m1 = 3时，再去查询一遍t2表，此时对t2表的查询语句相当于： 1SELECT * FROM t2 WHERE t2.m2 = 3 AND t2.n2 &lt; &#x27;d&#x27;; 可以看到，原来的t1.m1 = t2.m2这个涉及两个表的过滤条件在针对t2表做查询时关于t1表的条件就已经确定了，所以我们只需要单单优化对t2表的查询了，上述两个对t2表的查询语句中利用到的列是m2和n2列，我们可以： 在m2列上建立索引，因为对m2列的条件是等值查找，比如t2.m2 = 2、t2.m2 = 3等，所以可能使用到ref的访问方法，假设使用ref的访问方法去执行对t2表的查询的话，需要回表之后再判断t2.n2 &lt; d这个条件是否成立。这里有一个比较特殊的情况，就是假设m2列是t2表的主键或者唯一二级索引列，那么使用t2.m2 = 常数值这样的条件从t2表中查找记录的过程的代价就是常数级别的。我们知道在单表中使用主键值或者唯一二级索引列的值进行等值查找的方式称之为const，而MySQL把在连接查询中对被驱动表使用主键值或者唯一二级索引列的值进行等值查找的查询执行方式称之为：eq_ref。 在n2列上建立索引，涉及到的条件是t2.n2 &lt; &#39;d&#39;，可能用到range的访问方法，假设使用range的访问方法对t2表的查询的话，需要回表之后再判断在m2列上的条件是否成立。 假设m2和n2列上都存在索引的话，那么就需要从这两个里边儿挑一个代价更低的去执行对t2表的查询。当然，建立了索引不一定使用索引，只有在二级索引 + 回表的代价比全表扫描的代价更低时才会使用索引。 另外，有时候连接查询的查询列表和过滤条件中可能只涉及被驱动表的部分列，而这些列都是某个索引的一部分，这种情况下即使不能使用eq_ref、ref、ref_or_null或者range这些访问方法执行对被驱动表的查询的话，也可以使用索引扫描，也就是index的访问方法来查询被驱动表。所以我们建议在真实工作中最好不要使用*作为查询列表，最好把真实用到的列作为查询列表。 2.3 基于块的嵌套循环连接扫描一个表的过程其实是先把这个表从磁盘上加载到内存中，然后从内存中比较匹配条件是否满足。现实生活中的表可不像t1、t2这种只有3条记录，成千上万条记录都是少的，几百万、几千万甚至几亿条记录的表到处都是。内存里可能并不能完全存放的下表中所有的记录，所以在扫描表前边记录的时候后边的记录可能还在磁盘上，等扫描到后边记录的时候可能内存不足，所以需要把前边的记录从内存中释放掉。我们前边又说过，采用嵌套循环连接算法的两表连接过程中，被驱动表可是要被访问好多次的，如果这个被驱动表中的数据特别多而且不能使用索引进行访问，那就相当于要从磁盘上读好几次这个表，这个I/O代价就非常大了，所以我们得想办法：尽量减少访问被驱动表的次数。 当被驱动表中的数据非常多时，每次访问被驱动表，被驱动表的记录会被加载到内存中，在内存中的每一条记录只会和驱动表结果集的一条记录做匹配，之后就会被从内存中清除掉。然后再从驱动表结果集中拿出另一条记录，再一次把被驱动表的记录加载到内存中一遍，周而复始，驱动表结果集中有多少条记录，就得把被驱动表从磁盘上加载到内存中多少次。 如果在把被驱动表的记录加载到内存的时候，一次性和多条驱动表中的记录做匹配，这样就可以大大减少重复从磁盘上加载被驱动表的代价。 MySQL提出了一个join buffer的概念，join buffer就是执行连接查询前申请的一块固定大小的内存，先把若干条驱动表结果集中的记录装在这个join buffer中，然后开始扫描被驱动表，每一条被驱动表的记录一次性和join buffer中的多条驱动表记录做匹配，因为匹配的过程都是在内存中完成的，所以这样可以显著减少被驱动表的I/O代价。使用join buffer的过程如下图所示： 最好的情况是join buffer足够大，能容纳驱动表结果集中的所有记录，这样只需要访问一次被驱动表就可以完成连接操作了。MySQL把这种加入了join buffer的嵌套循环连接算法称之为基于块的嵌套连接（Block Nested-Loop Join）算法。 这个join buffer的大小是可以通过启动参数或者系统变量join_buffer_size进行配置，默认大小为262144字节（也就是256KB），最小可以设置为128字节。当然，对于优化被驱动表的查询来说，最好是为被驱动表加上效率高的索引，如果实在不能使用索引，并且自己的机器的内存也比较大可以尝试调大join_buffer_size的值来对连接查询进行优化。 另外需要注意的是，驱动表的记录并不是所有列都会被放到join buffer中，只有查询列表中的列和过滤条件中的列才会被放到join buffer中，所以再次提醒我们，最好不要把*作为查询列表，只需要把我们关心的列放到查询列表就好了，这样还可以在join buffer中放置更多的记录。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/tags/MySQL/"}]},{"title":"MySQL[五]InnoDb表空间","slug":"MySQL/MySQL[五]InnoDb表空间","date":"2022-01-04T16:00:00.000Z","updated":"2022-01-12T00:41:20.706Z","comments":true,"path":"2022/01/05/MySQL/MySQL[五]InnoDb表空间/","link":"","permalink":"https://yinhuidong.github.io/2022/01/05/MySQL/MySQL[%E4%BA%94]InnoDb%E8%A1%A8%E7%A9%BA%E9%97%B4/","excerpt":"","text":"在前面的两篇文章已经对InnoDB索引的结构，页存储结构，行格式做了十分细致的分析，也详细阐述了为什么你的SQL会慢，索引命中的原理，接下来我要继续深入学习MySQL。在此之前还要先来补充一下MySQL的一些基础知识。 一，MySQL的数据目录1. 数据库和文件系统的关系InnoDB,MyISAM这样的存储引擎都是把表存储在磁盘上，而操作系统是使用文件系统来管理磁盘的。【像InnoDB,MyISAM这样的存储引擎都是把数据存储在文件系统上的。】当我们想读取数据的时候，这些存储引擎会从文件系统中把数据读出来返回给我们，当我们想写入数据的时候，这些存储引擎又会把数据写回到文件系统。 本小节主要就是分析下InnoDB,MyISAM两个存储引擎的数据是如何在文件系统中存储的。 我的MySQL版本是5.7.28，所以接下来的操作和分析都是基于这个小版本的。其他版本可能会有细微的差异。 2.MySQL数据目录MySQL服务器程序在启动时，会到文件系统的某个目录下加载一些数据，之后再运行过程中产生的数据也会存储到这个目录下的某些文件中。这个目录就是数据目录。 2.1 数据目录和安装目录的区别MySQL的安装目录是在安装MySQL的时候指定的安装位置，下面有个很重要的bin目录，里面存储着控制客户端程序与服务器程序的命令。 MySQL的数据目录是用来存储MySQL在运行过程中产生的数据。 2.2 MySQL的数据目录在哪里数据目录对应着一个系统变量datadir，在使用客户端与服务器建立连接以后，查看这个系统变量的值就知道了： 1show variables like &#x27;datadir&#x27;; 结果如下： 1234567mysql&gt; show variables like &#x27;datadir&#x27;;+---------------+--------------------+| Variable_name | Value |+---------------+--------------------+| datadir | C:\\yhd\\mysql\\Data\\ |+---------------+--------------------+1 row in set, 1 warning (0.00 sec) 3.数据目录的结构3.1 数据库在文件系统中的表示每个数据库都对应着数据目录下的一个子目录，或者说对应着一个文件夹。当我们创建一个新的数据库的时候，MySQL会帮助我们做两件事： 在数据目录下创建一个与数据库同名的文件目录 在该子目录下创建一个db.opt文件。这个文件中包含了数据库的一些属性，比如该数据库的字符集和比较规则。 下面来看一下我的MySQL中的数据库： 1234567891011mysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || mysql || performance_schema || sys || yhd |+--------------------+5 rows in set (0.00 sec) 再从数据目录里看一下： 123456789101112131415161718192021222324252627282930313233C:\\yhd\\mysql\\Data&gt;dir 驱动器 C 中的卷没有标签。 卷的序列号是 CA5F-90F5 C:\\yhd\\mysql\\Data 的目录2021/12/19 00:27 &lt;DIR&gt; .2021/12/19 00:27 &lt;DIR&gt; ..2021/12/14 00:00 56 auto.cnf2021/12/14 00:00 1,703 ca-key.pem2021/12/14 00:00 1,131 ca.pem2021/12/14 00:00 1,131 client-cert.pem2021/12/14 00:00 1,707 client-key.pem2021/12/14 00:12 696 DESKTOP-NJIMTJP-slow.log2021/12/18 01:15 25,345 DESKTOP-NJIMTJP.err2021/12/18 01:15 5 DESKTOP-NJIMTJP.pid2021/12/19 02:18 79,691,776 ibdata12021/12/18 01:15 12,582,912 ibtmp12021/12/18 01:15 356 ib_buffer_pool2021/12/19 02:18 50,331,648 ib_logfile02021/12/19 02:18 50,331,648 ib_logfile12021/12/14 00:00 &lt;DIR&gt; mysql2021/12/14 00:00 &lt;DIR&gt; performance_schema2021/12/14 00:00 1,707 private_key.pem2021/12/14 00:00 461 public_key.pem2021/12/14 00:00 1,131 server-cert.pem2021/12/14 00:00 1,707 server-key.pem2021/12/14 00:00 &lt;DIR&gt; sys2021/12/19 00:56 &lt;DIR&gt; yhd 17 个文件 192,975,120 字节 6 个目录 189,156,126,720 可用字节C:\\yhd\\mysql\\Data&gt; 仔细看会发现，除了information_schema这个数据库以外，其他的数据库都对应一个文件目录，这个数据库有点特殊，后面在具体分析。 3.2 表在文件系统中的表示我们的数据其实是以记录的形式插入到表中的。每个表的信息其实可以分为两种。 表结构信息 表数据信息 为了保存表结构信息，InnoDB,MyISAM这两种存储引擎都会在数据目录下对应的数据库子目录中创建一个专门用于描述表结构的文件，文件名是表名.frm。这个文件是二进制格式的，直接打开会乱码。 我们知道不同的存储引擎对于表中的数据存储是不一样的，接下来我们分别来看一下InnoDB,MyISAM是如何存储表中的数据的。 InnoDb是如何存储数据的我们再来回顾下上一篇的知识： innoDB其实是使用页来作为基本单位管理存储空间的，默认大小16KB。 对于InnoDB存储引擎来说，每个索引都对应一颗B+树，该B+树的每个结点都是一个数据页。数据页之间没有必要是物理连续的，因为数据页之间有双向链表来维护这些页的顺序。 InnoDB的聚簇索引的叶子结点存储了完整的用户记录，也就是所谓的索引即数据，数据即索引。 为了更好的管理这些页，InnoDB提出了表空间或者文件空间的概念。这个表空间是一个抽象的概念，他可以对应文件系统上一个或者多个真实文件（不同表空间对应的文件数量可能不同）。每一个表空间可以被划分为很多个页，表数据被存放在某个表空间下的某些页中。InnoDB将表空间划分几种不同的类型，我们一个个分析一下子。 系统表空间 这个系统表空间可以对应文件系统上一个或者多个实际的文件。在默认情况下，InnoDB会在数据目录下创建一个名为ibdata1，大小为12MB的文件，这个文件就是对应的系统表空间在文件系统上的表示。怎么才12MB？这是因为这个文件是自扩展文件，也就是当不够用的时候会自己增加文件大小。 当然，如果想让系统表空间对应文件系统上的多个实际文件，或者仅仅觉得原来的ibdata1这个文件名难听，那么可以在MySQL服务启动的时候，配置对应的文件路径以及他们的大小。比如像下面这样修改配置文件： 12[server]innodb_data_file_path=data1:512M;data2:512M:autoextend 这样，在MySQL启动之后会创建data1和data2这两个各自512MB大小的文件作为系统表空间。其中的autoextend表明，如果这两个文件不够用，则会自动扩展data2文件的大小。 我们也可以把系统表空间对应的文件路径不配置到数据目录下，甚至可以配置到单独的磁盘分区上，涉及到的启动参数就是innodb_data_file_path和innodb_data_home_dir。 需要注意的一点是，在一个MySQL服务器中，系统表空间只有一份。从MySQL5.5.7到MySQL5.6.6之间的各个版本中，我们表中的数据都会被默认存储到这个 **系统表空间**。 独立表空间 在MySQL5.6.6以及以后的版本中，InnoDB不在默认把各个表的数据存储到系统表空间，而是为每一个表建立一个独立的表空间，也就是说，创建多少张表就会对应多少个表空间。在使用独立表空间来存储表数据的时候，会在该表所属的数据库对应的子目录下创建一个表示该独立表空间的文件，其文件名和表名相同，只不过添加了一个.ibd扩展名。所以完整的文件名称：表名.ibd。 假如我们使用独立表空间来存储yhd数据库下的person_info表，那么在该数据库所对应的yhd文件目录下会为person_info表创建下面两个文件：person_info.frm,person_info.ibd。 其中ibd文件用来存储表中的数据。当然也可以自己指定是使用系统表空间还是独立表空间来存储数据。 其他类型表空间 除了上述两种表空间之外，还有一些不同类型的表空间，比如通用表空间，undo表空间，临时表空间。 MyISAM是如何存储数据的索引和数据在InnoDB是一回事，但是MyISAM中的索引相当于全部都是二级索引，该存储引擎的数据和索引是分开存放的。所以在文件系统中也是使用不同的文件来存储数据文件和索引文件，而且与InnoDB不同的是，MyISAM并没有什么表空间一说，表的数据和索引都存放到对应的数据库子目录下。 假设我们person_info表使用的是MyISAM存储引擎，那么在它所在数据库对应的yhd文件目录下会为person_info创建三个文件：person_info.frm,person_info.MYD,person_info.MYI。 其中person_info.MYD表示表的数据文件，也就是插入的用户记录，person_info.MYI表示表的索引文件，我们为该表创建的索引都会放到这个文件中。 3.3 其他的文件除了我们上边说的这些用户自己存储的数据以外，数据目录下还包括为了更好运行程序的一些额外文件，主要包括这几种类型的文件： 服务器进程文件。我们知道每运行一个MySQL服务器程序，都意味着启动一个进程。MySQL服务器会把自己的进程ID写入到一个文件中。 服务器日志文件。在服务器运行过程中，会产生各种各样的日志，比如常规的查询日志、错误日志、二进制日志、redo日志等各种日志，这些日志各有各的用途，现在先了解一下就可以了。 默认/自动生成的SSL和RSA证书和密钥文件。主要是为了客户端和服务器安全通信而创建的一些文件 4.文件系统对数据库的影响因为MySQL的数据都是存在文件系统中的，就不得不受到文件系统的一些制约，这在数据库和表的命名、表的大小和性能方面体现的比较明显，比如下边这些方面： 数据库名称和表名称不得超过文件系统所允许的最大长度。每个数据库都对应数据目录的一个子目录，数据库名称就是这个子目录的名称；每个表都会在数据库子目录下产生一个和表名同名的.frm文件，如果是InnoDB的独立表空间或者使用MyISAM引擎还会有别的文件的名称与表名一致。这些目录或文件名的长度都受限于文件系统所允许的长度～ 特殊字符的问题为了避免因为数据库名和表名出现某些特殊字符而造成文件系统不支持的情况，MySQL会把数据库名和表名中所有除数字和拉丁字母以外的所有字符在文件名里都映射成 @+编码值的形式作为文件名。比方说我们创建的表的名称为&#39;test?&#39;，由于?不属于数字或者拉丁字母，所以会被映射成编码值，所以这个表对应的.frm文件的名称就变成了test@003f.frm。 文件长度受文件系统最大长度限制对于InnoDB的独立表空间来说，每个表的数据都会被存储到一个与表名同名的.ibd文件中；对于MyISAM存储引擎来说，数据和索引会分别存放到与表同名的.MYD和.MYI文件中。这些文件会随着表中记录的增加而增大，它们的大小受限于文件系统支持的最大文件大小。 5.MySQL系统数据库简介我们前边提到了MySQL的几个系统数据库，这几个数据库包含了MySQL服务器运行过程中所需的一些信息以及一些运行状态信息，我们现在稍微了解一下。 mysql这个数据库贼核心，它存储了MySQL的用户账户和权限信息，一些存储过程、事件的定义信息，一些运行过程中产生的日志信息，一些帮助信息以及时区信息等。 information_schema这个数据库保存着MySQL服务器维护的所有其他数据库的信息，比如有哪些表、哪些视图、哪些触发器、哪些列、哪些索引等等。这些信息并不是真实的用户数据，而是一些描述性信息，有时候也称之为元数据。 performance_schema这个数据库里主要保存MySQL服务器运行过程中的一些状态信息，算是对MySQL服务器的一个性能监控。包括统计最近执行了哪些语句，在执行过程的每个阶段都花费了多长时间，内存的使用情况等等信息。 sys这个数据库主要是通过视图的形式把information_schema和performance_schema结合起来，让程序员可以更方便的了解MySQL服务器的一些性能信息。 二，回顾前面数据页（也就是Index类型的页）由7部分组成，其中有两个部分是所有类型的页面都通用的。 所有类型的页都会包含下面两个部分。 File Header：记录页面的一些通用信息 File Trailer: 校验页是否完整，保证页面在从内存刷新到磁盘后内容是相同的 名称 占用空间大小（字节） 描述 FIL_PAGE_SPACE_OR_CHKSUM 4 页的校验和（checksum值） FIL_PAGE_OFFSET 4 页号 FIL_PAGE_PREV 4 上一个页的页号 FIL_PAGE_NEXT 4 下一个页的页号 FIL_PAGE_LSN 8 页面被最后修改时对应的日志序列位置 FIL_PAGE_TYPE 2 该页的类型 FIL_PAGE_FILE_FLUSH_LSN 8 仅仅在系统表空间的一个页中定义，代表文件至少被刷新到了对应的LSN值 FIL_PAGE_ARCH_LOG_NO_OR_SPACE_ID 4 页属于哪个表空间 表空间中的每一个页都对应着一个页号，也就是FIL_PAGE_OFFSET，这个页号由4个字节组成，也就是32个比特位，所以一个表空间最多可以拥有2³²个页，如果按照页的默认大小16KB来算，一个表空间最多支持64TB的数据。表空间的第一个页的页号为0，之后的页号分别是1，2，3…依此类推 某些类型的页可以组成链表，链表中的页可以不按照物理顺序存储，而是根据FIL_PAGE_PREV和FIL_PAGE_NEXT来存储上一个页和下一个页的页号。需要注意的是，这两个字段主要是为了INDEX类型的页，也就是我们之前一直说的数据页建立B+树后，为每层节点建立双向链表用的，一般类型的页是不使用这两个字段的。 每个页的类型由FIL_PAGE_TYPE表示，比如像数据页的该字段的值就是0x45BF，不同类型的页在该字段上的值是不同的。 InnoDB支持许多种类型的表空间，我们暂时重点关注系统表空间和独立表空间的结构。他们结构比较相似，但是由于系统表空间中额外包含了一些关于整个系统的信息，所以我们先分析独立表空间，再说系统表空间。 三，独立表空间1.区的概念为了更好的管理表中的页，InnoDB提出了区的概念。对于16KB的页来说，连续的64个页就是一个区。也就是说一个区默认占用1M空间。不论是系统表空间还是独立表空间，都可以看成是由若干区组成的，每256个区划分为一个组。 为什么要有区的概念？ 从理论上来讲，不引入区的概念只使用页的概念对存储引擎的运行并没有任何影响，但是我们来分析下： 我们每向表中插入一条记录，本质上就是向该表的聚簇索引以及所有二级索引代表的B+树的节点中插入数据。而B+树的每一层中的页都会形成一个双向链表，如果是以页为单位来分配存储空间的话，双向链表相邻的两个页之间的物理位置可能离得非常远。 B+树的范围查询只需要定位到最左边的记录和最右边的记录，然后沿着双向链表一直扫描就可以了，而如果链表中相邻的两个页物理位置离得非常远，就是所谓的随机I/O。磁盘的速度和内存的速度差了好几个数量级，随机I/O是非常慢的，所以我们应该尽量让链表中相邻的页的物理位置也相邻，这样进行范围查询的时候才可以使用所谓的顺序I/O。 所以才引入了区（extent）的概念，一个区就是在物理位置上连续的64个页。在表中数据量大的时候，为某个索引分配空间的时候就不再按照页为单位分配了，而是按照区为单位分配，甚至在表中的数据十分非常特别多的时候，可以一次性分配多个连续的区。虽然可能造成一点点空间的浪费（数据不足填充满整个区），但是从性能角度看，可以消除很多的随机I/O。 2.段的概念范围查询，其实是对B+树叶子节点中的记录进行顺序扫描，而如果不区分叶子节点和非叶子节点，统统把节点代表的页面放到申请到的区中的话，进行范围扫描的效果就大打折扣了。InnoDB对B+树的叶子节点和非叶子节点进行了区别对待：叶子节点有自己独有的区，非叶子节点也有自己独有的区。存放叶子节点的区的集合就算是一个段（segment），存放非叶子节点的区的集合也算是一个段。也就是说一个索引会生成2个段，一个叶子节点段，一个非叶子节点段。 默认情况下一个使用InnoDB存储引擎的表只有一个聚簇索引，一个索引会生成2个段，而段是以区为单位申请存储空间的，一个区默认占用1M存储空间，所以默认情况下一个只存了几条记录的小表也需要2M的存储空间么？以后每次添加一个索引都要多申请2M的存储空间么？ 为了考虑以完整的区为单位分配给某个段对于数据量较小的表太浪费存储空间的这种情况，InnoDB提出了碎片区的概念，也就是在一个碎片区中，并不是所有的页都是为了存储同一个段的数据而存在的，而是碎片区中的页可以用于不同的目的，比如有些页用于段A，有些页用于段B，有些页甚至哪个段都不属于。碎片区直属于表空间，并不属于任何一个段。所以此后为某个段分配存储空间的策略是这样的： 在刚开始向表中插入数据的时候，段是从某个碎片区以单个页面为单位来分配存储空间的 当某个段已经占用了32个碎片区页面之后，就会以完整的区为单位来分配存储空间 所以现在段不能仅定义为是某些区的集合，更精确的应该是某些零散的页面以及一些完整的区的集合。除了索引的叶子节点段和非叶子节点段之外，InnoDB中还有为存储一些特殊的数据而定义的段，比如回滚段，当然我们现在并不关心别的类型的段，现在只需要知道段是一些零散的页面以及一些完整的区的集合就好了。 有的时候处于不同的阶段，对于某个概念的定义或者理解是不同的，随着知识水平的提升后续再来逐渐完善，就像小学的时候老师会告诉你最小的数是0，中学又告诉你最小的数是负无穷一样。 3.区的分类每个区都对应一个XDES Entry结构，这个结构中存储了一些与这个区有关的属性。这些区可以被分为下面四种类型。 空闲的区：现在还没有用到这个区中的任何页面，这些区会被加入到FREE链表。 有剩余空间的碎片区：表示碎片区中还有可用的页面，这些区会被加入到FREE_FRAG链表。 没有剩余空间的碎片区：表示碎片区中的所有页面都被使用，没有空闲页面；这些区会被加入到FULL_FRAG链表。 附属于某个段的区。每一个索引都可以分为叶子节点段和非叶子节点段，除此之外InnoDB还会另外定义一些特殊作用的段，在这些段中的数据量很大时将使用区来作为基本的分配单位；每个段所属的区又会被组织成下面几种链表。 FREE链表：在同一个段中，所有页面都是空闲页面的区对应的XDES Entry结构会被加入到这个链表。 NOT_FULL链表：在同一个段中，仍有空闲页面的区对应的XDES Entry结构会被加入到这个链表。 FULL链表：在同一个段中，已经没有空闲页面的区对应的XDES Entry结构会被加入到这个链表。 这四种类型的区也被叫做区的四种状态。 状态名 含义 FREE 空闲的区 FREE_FRAG 有剩余空间的碎片区 FULL_FRAG 没有剩余空间的碎片区 FSEG 附属于某个段的区 处于**FREE**、**FREE_FRAG**以及**FULL_FRAG**这三种状态的区都是独立的，算是直属于表空间；而处于**FSEG**状态的区是附属于某个段的。 每个段都会对应一个INODE Entry结构，该结构中存储了一些与这个段有关的属性。 表空间中第一个页面的类型为FSP_HDR，它存储了表空间的一些整体属性以及第一个组内256个区对应的XDES Entry结构。 除了表空间的第一个组以外，其余组的第一个页面的类型为XDES，这种页面的结构和FSP_HDR类型的页面对比，除了少了File Space header（记录表空间整体属性的部分）部分之外，其余部分是一样的。 每个组的第二个页面类型为IBUF_BITMAP，存储了一些关于Change Buffer的信息。 表空间中第一个组的第三个页面的类型是INODE，他是为了存储INODE Entry结构而设计的，这种类型的页面会组织成下面两个链表。 SEG_INODES_FULL链表：在该链表中，INODE类型的页面中已经没有空闲空间来存储额外的INODE Entry结构。 SEG_INODES_FREE链表：在该链表中，INODE类型的页面中还有空闲空间来存储额外的INODE Entry结构。 4. Segment Header一个索引会产生两个段，分别是叶子节点段和非叶子节点段，而每个段都会对应一个INODE Entry结构，那我们怎么知道某个段对应哪个**INODE Entry**结构呢？所以得找个地方记下来这个对应关系。INDEX类型的页时有一个Page Header部分，其中的PAGE_BTR_SEG_LEAF和PAGE_BTR_SEG_TOP都占用10个字节，它们其实对应一个叫Segment Header的结构，该结构图示如下： 各个部分的具体释义如下： 名称 占用字节数 描述 Space ID of the INODE Entry 4 INODE Entry结构所在的表空间ID Page Number of the INODE Entry 4 INODE Entry结构所在的页面页号 Byte Offset of the INODE Ent 2 INODE Entry结构在该页面中的偏移量 PAGE_BTR_SEG_LEAF记录着叶子节点段对应的INODE Entry结构的地址是哪个表空间的哪个页面的哪个偏移量，PAGE_BTR_SEG_TOP记录着非叶子节点段对应的INODE Entry结构的地址是哪个表空间的哪个页面的哪个偏移量。这样子索引和其对应的段的关系就建立起来了。不过需要注意的一点是，因为一个索引只对应两个段，所以只需要在索引的根页面中记录这两个结构即可。 其实Segment Header的作用就是记录哪个段对应哪个INODE Entry结构的。 5. 真实表空间对应的文件大小一个新建的表对应的.ibd文件只占用了96KB，才6个页的大小。刚开始的时候，表空间占用空间自然很小，因为表里面没有数据。不过，ibd文件是自扩展文件，随着数据的增多文件也在逐渐增大。 四，系统表空间系统表空间的结构和独立表空间基本类似，只不过由于整个MySQL进程只有一个系统表空间，在系统表空间中会额外记录一些有关整个系统信息的页面，所以会比独立表空间多出一些记录这些信息的页面。因为这个系统表空间相当于是表空间之首，所以它的表空间 ID（Space ID）是0。 1.系统表空间的整体结构系统表空间与独立表空间的一个非常明显的不同之处就是在表空间开头有许多记录整个系统属性的页面。 可以看到，系统表空间和独立表空间的前三个页面（页号分别为0、1、2，类型分别是FSP_HDR、IBUF_BITMAP、INODE）的类型是一致的，只是页号为3～7的页面是系统表空间特有的，我们来看一下这些多出来的页面都是干啥使的： 页号 页面类型 英文描述 描述 3 SYS Insert Buffer Header 存储Insert Buffer的头部信息 4 INDEX Insert Buffer Root 存储Insert Buffer的根页面 5 TRX_SYS Transaction System 事务系统的相关信息 6 SYS First Rollback Segment 第一个回滚段的页面 7 SYS Data Dictionary Header 数据字典头部信息 除了这几个记录系统属性的页面之外，系统表空间的extent 1和extent 2这两个区，也就是页号从64~`191这128个页面被称为Doublewrite buffer`，也就是双写缓冲区。不过上述的大部分知识都涉及到了事务和多版本控制的问题，现在我们只分析有关InnoDB数据字典的知识，其余的概念在后边再看。 1.1 InnoDB数据字典每当我们向一个表中插入一条记录的时候，MySQL先要校验一下插入语句对应的表存不存在，插入的列和表中的列是否符合，如果语法没有问题的话，还需要知道该表的聚簇索引和所有二级索引对应的根页面是哪个表空间的哪个页面，然后把记录插入对应索引的B+树中。所以说，MySQL除了保存着我们插入的用户数据之外，还需要保存许多额外的信息，比方说： 某个表属于哪个表空间，表里边有多少列 表对应的每一个列的类型是什么 该表有多少索引，每个索引对应哪几个字段，该索引对应的根页面在哪个表空间的哪个页面 该表有哪些外键，外键对应哪个表的哪些列 某个表空间对应文件系统上文件路径是什么 上述这些数据并不是我们使用INSERT语句插入的用户数据，实际上是为了更好的管理我们这些用户数据而不得已引入的一些额外数据，这些数据也称为元数据。InnoDB存储引擎特意定义了一些列的内部系统表（internal system table）来记录这些这些元数据： 表名 描述 SYS_TABLES 整个InnoDB存储引擎中所有的表的信息 SYS_COLUMNS 整个InnoDB存储引擎中所有的列的信息 SYS_INDEXES 整个InnoDB存储引擎中所有的索引的信息 SYS_FIELDS 整个InnoDB存储引擎中所有的索引对应的列的信息 SYS_FOREIGN 整个InnoDB存储引擎中所有的外键的信息 SYS_FOREIGN_COLS 整个InnoDB存储引擎中所有的外键对应列的信息 SYS_TABLESPACES 整个InnoDB存储引擎中所有的表空间信息 SYS_DATAFILES 整个InnoDB存储引擎中所有的表空间对应文件系统的文件路径信息 SYS_VIRTUAL 整个InnoDB存储引擎中所有的虚拟生成列的信息 这些系统表也被称为数据字典，它们都是以B+树的形式保存在系统表空间的某些页面中，其中SYS_TABLES、SYS_COLUMNS、SYS_INDEXES、SYS_FIELDS这四个表尤其重要，称之为基本系统表（basic system tables），我们先看看这4个表的结构： 1.2 SYS_TABLES表 列名 描述 name 表的名称 id InnoDB存储引擎每一张表都有一个唯一的ID n_cols 该表拥有的列的个数 type 表的类型，记录了一些文件格式，行格式，压缩等信息 Mix_id 已经过时，忽略 Mix_len 表的一些额外属性 Cluster_id 未使用，忽略 Space 该表所属空间的ID 这个SYS_TABLES表有两个索引： 以NAME列为主键的聚簇索引 以ID列建立的二级索引 1.3 SYS_COLUMNS表 列名 描述 TABLE_ID 该列所属表对应的ID POS 该列在表中是第几列 NAME 该列的名称 MTYPE main data type，主数据类型，就是那堆INT、CHAR、VARCHAR、FLOAT、DOUBLE等 PRTYPE precise type，精确数据类型，就是修饰主数据类型的那堆东东，比如是否允许NULL值，是否允许负数啥的 LEN 该列最多占用存储空间的字节数 PREC 该列的精度，不过这列貌似都没有使用，默认值都是0 这个SYS_COLUMNS表只有一个聚集索引： 以(TABLE_ID, POS)列为主键的聚簇索引 1.4 SYS_INDEXES表 列名 描述 TABLE_ID 该索引所属表对应的ID ID InnoDB存储引擎中每个索引都有一个唯一的ID NAME 该索引的名称 N_FIELDS 该索引包含列的个数 TYPE 该索引的类型，比如聚簇索引、唯一索引、更改缓冲区的索引、全文索引、普通的二级索引等等各种类型 SPACE 该索引根页面所在的表空间ID PAGE_NO 该索引根页面所在的页面号 MERGE_THRESHOLD 如果页面中的记录被删除到某个比例，就把该页面和相邻页面合并，这个值就是这个比例 这个SYS_INDEXES表只有一个聚集索引： 以(TABLE_ID, ID)列为主键的聚簇索引 1.5 SYS_FIELDS表 列名 描述 INDEX_ID 该索引列所属的索引的ID POS 该索引列在某个索引中是第几列 COL_NAME 该索引列的名称 这个SYS_FIELDS表只有一个聚集索引： 以(INDEX_ID, POS)列为主键的聚簇索引 1.6 Data Dictionary Header页面只要有了上述4个基本系统表，也就意味着可以获取其他系统表以及用户定义的表的所有元数据。比方说我们想看看SYS_TABLESPACES这个系统表里存储了哪些表空间以及表空间对应的属性，那就可以： 到SYS_TABLES表中根据表名定位到具体的记录，就可以获取到SYS_TABLESPACES表的TABLE_ID 使用这个TABLE_ID到SYS_COLUMNS表中就可以获取到属于该表的所有列的信息。 使用这个TABLE_ID还可以到SYS_INDEXES表中获取所有的索引的信息，索引的信息中包括对应的INDEX_ID，还记录着该索引对应的B+数根页面是哪个表空间的哪个页面。 使用INDEX_ID就可以到SYS_FIELDS表中获取所有索引列的信息。 这4个表的元数据去哪里获取呢？这4个表的元数据，就是它们有哪些列、哪些索引等信息是硬编码到代码中的，InnoDB用一个固定的页面来记录这4个表的聚簇索引和二级索引对应的B+树位置，这个页面就是页号为7的页面，类型为SYS，记录了Data Dictionary Header，也就是数据字典的头部信息。除了这4个表的5个索引的根页面信息外，这个页号为7的页面还记录了整个InnoDB存储引擎的一些全局属性。 这个页面由下边几个部分组成： 名称 中文名 占用空间（字节） 简单描述 File Header 文件头部 38 页的一些通用信息 Data Dictionary Header 数据字典头部信息 56 记录一些基本系统表的根页面位置以及InnoDB存储引擎的一些全局信息 Segment Header 段头部信息 10 记录本页面所在段对应的INODE Entry位置信息 Empty Space 尚未使用空间 16272 用于页结构的填充，没啥实际意义 File Trailer 文件尾部 8 校验页是否完整 这个页面里有Segment Header部分，意味着InnoDB把这些有关数据字典的信息当成一个段来分配存储空间，我们称之为数据字典段。由于目前我们需要记录的数据字典信息非常少（可以看到Data Dictionary Header部分仅占用了56字节），所以该段只有一个碎片页，也就是页号为7的这个页。 接下来我们需要看一下Data Dictionary Header部分的各个字段： Max Row ID：如果我们不显式的为表定义主键，而且表中也没有UNIQUE索引，那么InnoDB存储引擎会默认为我们生成一个名为row_id的列作为主键。因为它是主键，所以每条记录的row_id列的值不能重复。原则上只要一个表中的row_id列不重复就可以了，也就是说表a和表b拥有一样的row_id列也没啥关系，不过InnoDB只提供了这个Max Row ID字段，不论哪个拥有row_id列的表插入一条记录时，该记录的row_id列的值就是Max Row ID对应的值，然后再把Max Row ID对应的值加1，也就是说这个Max Row ID是全局共享的。 Max Table ID：InnoDB存储引擎中的所有的表都对应一个唯一的ID，每次新建一个表时，就会把本字段的值作为该表的ID，然后自增本字段的值。 Max Index ID：InnoDB存储引擎中的所有的索引都对应一个唯一的ID，每次新建一个索引时，就会把本字段的值作为该索引的ID，然后自增本字段的值。 Max Space ID：InnoDB存储引擎中的所有的表空间都对应一个唯一的ID，每次新建一个表空间时，就会把本字段的值作为该表空间的ID，然后自增本字段的值。 Mix ID Low(Unused)：这个字段没啥用，跳过。 Root of SYS_TABLES clust index：本字段代表SYS_TABLES表聚簇索引的根页面的页号。 Root of SYS_TABLE_IDS sec index：本字段代表SYS_TABLES表为ID列建立的二级索引的根页面的页号。 Root of SYS_COLUMNS clust index：本字段代表SYS_COLUMNS表聚簇索引的根页面的页号。 Root of SYS_INDEXES clust index本字段代表SYS_INDEXES表聚簇索引的根页面的页号。 Root of SYS_FIELDS clust index：本字段代表SYS_FIELDS表聚簇索引的根页面的页号。 Unused：这4个字节没用，跳过。 以上就是页号为7的页面的全部内容。 1.7 information_schema系统数据库用户是不能直接访问InnoDB的这些内部系统表的，除非你直接去解析系统表空间对应文件系统上的文件。不过InnoDB考虑到查看这些表的内容可能有助于大家分析问题，所以在系统数据库information_schema中提供了一些以innodb_sys开头的表： 12345678910111213141516171819mysql&gt; USE information_schema;Database changedmysql&gt; SHOW TABLES LIKE &#x27;innodb_sys%&#x27;;+--------------------------------------------+| Tables_in_information_schema (innodb_sys%) |+--------------------------------------------+| INNODB_SYS_DATAFILES || INNODB_SYS_VIRTUAL || INNODB_SYS_INDEXES || INNODB_SYS_TABLES || INNODB_SYS_FIELDS || INNODB_SYS_TABLESPACES || INNODB_SYS_FOREIGN_COLS || INNODB_SYS_COLUMNS || INNODB_SYS_FOREIGN || INNODB_SYS_TABLESTATS |+--------------------------------------------+10 rows in set (0.00 sec) 在information_schema数据库中的这些以INNODB_SYS开头的表并不是真正的内部系统表，而是在存储引擎启动时读取这些以SYS开头的系统表，然后填充到这些以INNODB_SYS开头的表中。以INNODB_SYS开头的表和以SYS开头的表中的字段并不完全一样。​ 补充一张表空间完整结构图","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/tags/MySQL/"}]},{"title":"MySQL[四]索引命中原理","slug":"MySQL/MySQL[四]索引命中原理","date":"2022-01-03T16:00:00.000Z","updated":"2022-01-12T00:41:11.938Z","comments":true,"path":"2022/01/04/MySQL/MySQL[四]索引命中原理/","link":"","permalink":"https://yinhuidong.github.io/2022/01/04/MySQL/MySQL[%E5%9B%9B]%E7%B4%A2%E5%BC%95%E5%91%BD%E4%B8%AD%E5%8E%9F%E7%90%86/","excerpt":"","text":"上一篇分析了InnoDB存储引擎的B+树索引，现在来进行一个简单的回顾。 每个索引都对应一颗B+树，B+树分为好多层，最下边一层是叶子结点，其余的是内结点。所有的用户记录都存储在B+树的叶子结点，所有目录项记录都存储在内节点。 InnoDB存储引擎会自动为主键建立聚簇索引，聚簇索引的叶子结点包含完整的用户记录。 我们可以为指定的列建立二级索引，二级索引的叶子结点包含的用户记录由索引列+主键组成，所以如果项通过二级索引来查找完整的用户记录的话，需要通过回表操作，也就是在通过二级索引找到主键值之后再到聚簇索引中查找完整的用户记录。 B+树中每层结点都是按照索引列值从大到小的顺序排序而组成了双向链表，而且每个页内的记录(不论是用户记录还是目录项记录)都是按照索引列的值从小到大的顺序而形成了一个单链表。如果是联合索引的话，则页面和记录先按照联合索引前边的列排序，如果该列值相同，在按照联合索引后边的列排序。 通过索引查找记录是从B+树的根节点开始，一层一层乡下搜索。由于每个页面都按照索引列的值建立了页目录，所以在这些页面中查找非常快。 一，做一些前置的准备为了这篇文章的演示，需要先建立一张简单的表，用来演示索引执行过程中出现的一些情况。 123456789101112131415161718192021create table single_table( # 主键索引 id int primary key auto_increment, key1 varchar(100), key2 int, key3 varchar(100), key_part1 varchar(100), key_part2 varchar(100), key_part3 varchar(100), common_field varchar(100), # 索引列key1 key idx_key1 (key1), # 唯一索引：索引列key2 unique key uk_key2 (key2), # 索引列key3 key idx_key3 (key3), # 联合索引：索引列：key_part1, key_part2, key_part3 key idx_key_part (key_part1, key_part2, key_part3)) engine = innodb charset = utf8; 再往表中插入100w条数据，具体的插入过程不在演示。 二，索引的代价凡事都是有利有弊的，索引可以加快查询的速度，但是同样的，他也有相应的缺点。 空间上 一个索引对应一个B+树，每一个B+树的每一个节点都是一个数据页。一个数据页大小默认是16kb，所以一张表的索引越多，占用的空间其实越大，特别是在数据量大的时候，所以一般我们建立索引，默认每张表不要超过5个。 时间上 在对表进行增删改操作的时候，要对所有索引对应的B+树进行修改。而且上一篇分析过，B+树的每一层节点都按照索引列的值从小到大的顺序排序组成了双向链表。页中的记录都按照索引列的值从小到大的顺序形成了一个单向链表。而增删改操作可能会对结点和记录的排序造成破坏，所以存储引擎需要额外的时间进行页面分裂，页回收等操作，好维护结点和记录的排序。索引越多，维护的时间成本越高。 还有一点就是执行查询语句之前，会先生成执行计划。一般情况下一条语句再一次执行过程中只会使用一个二级索引(有特殊的，后面会分析)，在生成执行计划的时候需要计算使用不同索引执行查询时所需成本，最后选取成本最低的索引执行查询。如果索引太多，分析成本就会很高，耗时严重，从而影响查询语句的执行性能。 为了合理的建立索引，一方面加快我们的查询速度，一方面又不会过分的占用我们的时间和空间，我们需要了解索引在查询执行期间到底是如何发挥作用的。 三，使用B+树索引1.扫描区间和边界条件先说什么是全表扫描？就是从头到尾依次遍历所有结点，再依次遍历结点中的所有记录。全表扫描虽然效率很低，但是却是一种万能的解决方案，所有查询都可以使用这种方案兜底。 我们可以利用B+树查找索引列值等于某个值的记录，这样可以明显减少需要扫描的记录数量。由于B+树叶子节点中的记录是按照索引列值从小到大的顺序排序的，所以只扫描某个区间或某些区间中的记录也是很快的，比如下面这个查询语句： 1explain select * from single_table where id&gt;=2 and id&lt;=100; 这个时候其实是走了主键索引，这个语句其实是想查找id值在区间【2，100】内的所有聚簇索引记录，我们可以通过主键索引先定位到id=2的记录，然后顺着这条记录的单向链表往后找就行了。 与全表扫描的100w数据相比，扫描这个区间的成本简直太小了，所以提升了查询效率。我们把这个案例中待扫描的id值所在区间称为扫描区间，把形成这个扫描区间的搜索条件称为形成这个扫描区间的边界条件。 其实对于全表扫描来讲，就是在【-∞，+∞】的区间进行扫描而已。 再来看一条查询语句： 12345explainselect *from single_tablewhere key2 in (1438, 6328) or (key2 &gt;= 38 and key2 &lt;= 79); 这个查询的搜索条件涉及到key2列，我们正好在key2列上建立了唯一索引。如果使用唯一索引执行这个查询，实际上相当于从三个区间获取二级索引的记录。 【1438，1438】 【6328，6328】 【38，79】 类似前面两个区间这种，只有一个值的区间，我们称为单点扫描区间，把类似第三个区间这样存在多个值的叫做范围扫描区间，另外，由于我们的查询列是*，导致从上述的区间每次获取到一条二级索引记录，就需要根据二级索引记录的id列的值取回表一次。 当然，并不是所有的条件都可以称为边界条件，比如下面的查询语句： 123456explainselect *from single_tablewhere key1 &gt; &#x27;aaa&#x27; and key3 &lt; &#x27;zzz&#x27; and common_field = &#x27;aaa&#x27;; 如果使用idx_key1执行查询，那么相应的扫描区间就变成了【’aaa’,+∞】，后面的条件就是普通搜索条件，这些普通的搜索条件需要在获取到idx_key1的二级索引记录后，在执行回表操作，在获取到完整的用户记录后才能去判断他们是否成立。 如果使用idx_key3执行查询，那么扫描区间就是【-∞,’zzz’】,其余的条件就是普通搜索条件，这些普通的搜索条件需要在获取到idx_key3的二级索引记录后，在执行回表操作，在获取到完整的用户记录后才能去判断他们是否成立。 在使用某个索引执行查询的时候，关键的问题就是通过搜索条件找出合适的区间，然后再去对应的B+树中扫描索引列值在这些扫描区间的记录，对于每一个区间来说，只需要定位到第一条，就可以沿着单链表一直往后扫符合条件的记录。 其实对于B+树索引来说： = &lt;=&gt; in not in is null is not null &gt; &lt; &gt;= &lt;= between != like 都会进行区间扫描，只不过区间扫描大小不同导致效率不同。 不过也有一些需要注意的点： in和多个 = 用or连接起来的效果其实是一样的，都会产生多个单点扫描区间 不等于 产生的区间比较操蛋： 1select * from single_table where key1 != &#x27;aaa&#x27; 这个时候idx_key1 执行查询的时候对应的扫描区间就是【-∞,’aaa’】和【’aaa’,+∞】。 like操作比较特殊，只有在匹配完整的字符串或者匹配字符串前缀的时候才会产生合适的扫描区间 比较字符串的大小其实就是逐个比较每个字符的大小。字符串的比较过程如下： 先比较字符串的第一个字符，第一个字符串小的字符就比较小 如果第一个字符一样的话就按照上面的规则比较第二个，以此类推。 对于某个索引列来说，字符串前缀相同的记录在由记录组成的单向链表中肯定是相邻的。比如我们有一个搜索条件是key1 like &#39;a%&#39;，对于二级索引idx_key1来说，所有字符串前缀为a的二级索引记录肯定是相邻的。这也就意味着我们只要定位到key1值得字符串前缀为a的第一条记录，就可以依次往后扫描，直到某条二级索引记录的字符串不是a为止。 很显然，key1 like &#39;a%&#39;形成的扫描区间相当于【’a’,’b’】。 在执行一个查询语句的时候，首先需要找出所有可用的索引以及使用他们时对应的扫描区间。接下来我们分析下怎么从包含若干个and或者or的复杂搜索条件中提取出正确的扫描区间。 1.1 所有搜索条件都可以生成合适的扫描区间的情况在使用某个索引执行查询的时候，有时每个小的搜索条件都可以生成一个合适的扫描区间来减少需要扫描的记录数量。 12345explainselect *from single_tablewhere key2 &gt; 100 and key2 &gt; 200; 在使用唯一索引进行查询的时候，这两个条件都可以形成一个扫描区间【100，+∞】，【200，+∞】。因为这两个条件是用and连接的，所以最终就是两个区间取交集【200，+∞】。 我们把sql改一改： 12345explainselect *from single_tablewhere key2 &gt; 100 or key2 &gt; 200; 这个时候因为是使用or进行两个条件的连接，所以两个条件的区间应该取并集：【100，+∞】。 1.2 有的搜索条件不能生成合适的扫描区间的情况在使用某个索引进行查询的时候，有些小的搜索条件并不能生成合适的扫描区间来减少需要扫描的行数。 12345explainselect *from single_tablewhere key2 &gt; 100 and common_field =&#x27;abc&#x27;; 在使用唯一索引进行查询的时候，第一个条件会定位出区间【100，+∞】，但是第二个条件是一个普通条件，相当于【-∞，+∞】，因为两个条件使用and连接的，所以最终取交集之后的区间就是【100，+∞】。 其实在使用唯一索引进行查询的时候，在寻找对应的扫描区间的过程中，搜索条件common_field =&#39;abc&#39;没有起到任何作用，我们可以直接把这个条件进行一个等价替换【TRUE】(true对应的扫描区间也是【-∞，+∞】)。 12345explainselect *from single_tablewhere key2 &gt; 100 and true; 在进行化简之后就变成： 1234explainselect *from single_tablewhere key2 &gt; 100 也就是说上面的查询语句在使用唯一索引进行查询的时候对应的扫描区间就是【100，+∞】。 再来看一下使用OR的情况： 12345explainselect *from single_tablewhere key2 &gt; 100 or common_field =&#x27;abc&#x27;; 同样进行化简 12345explainselect *from single_tablewhere key2 &gt; 100 or true; 继续化简 1234explainselect *from single_tablewhere true 可见如果此时强制使用唯一索引进行查询，对应的扫描区间就是【-∞，+∞】，再加上这是二级索引，每次匹配到一条都要进行回表，所以这个查询的代价甚至比全表扫描还大，这个时候再使用唯一索引就没意义了。 1.3从复杂的搜索条件中找出扫描区间来一个复杂点的条件： 12345select *from single_tablewhere (key1 &gt; &#x27;aaa&#x27; and key2 &gt; 748) or (key1 &lt; &#x27;eee&#x27; and key1 &gt; &#x27;ccc&#x27;) or (key1 like &#x27;%f&#x27; and key1 &gt; &#x27;aaa&#x27; and (key2 &lt; 8000 or common_field = &#x27;aaa&#x27;)); 这无语的SQL怎么搞？ 先看where子句里面都涉及到了哪些列，以及我们为哪些列建立了索引 对于可以用到的索引，我们来分析索引的扫描区间 1.3.1 使用idx_key1查询先把不能形成合适扫描区间的搜索条件干掉，怎么干掉？直接把他们替换成TRUE。 替换之后的效果： 12345select *from single_tablewhere (key1 &gt; &#x27;aaa&#x27; and TRUE) or (key1 &lt; &#x27;eee&#x27; and key1 &gt; &#x27;ccc&#x27;) or (TRUE and key1 &gt; &#x27;aaa&#x27; and (TRUE or TRUE)); 化简之后的结果： 1234select *from single_tablewhere (key1 &gt; &#x27;aaa&#x27; ) -- 【aaa,+∞】 or (key1 &lt; &#x27;eee&#x27; and key1 &gt; &#x27;ccc&#x27;) -- 【&#x27;ccc&#x27;,&#x27;eee&#x27;】 因为这两个条件之间是用OR连接起来的，所以我们应该取并集，最终：【aaa,+∞】。 也就是需要把所有key1在这个区间内的所有二级索引记录都取出来，针对获取到的每一条二级索引记录进行一次回表，在得到完整的用户记录之后在使用其他的搜索条件进行过滤。 1.3.2 使用唯一二级索引查询我们还是进行化简 12345select *from single_tablewhere (TRUE and key2 = 748) or (TRUE and TRUE) or (TRUE and TRUE and (key2 &lt; 8000 or common_field = &#x27;aaa&#x27;)); 再继续化简 1234select *from single_tablewhere key2 = 748 or TRUE 因为两个条件使用OR连接的，所以最终的结果就是【-∞，+∞】。 也就是需要把所有key2所有二级索引记录都取出来，针对获取到的每一条二级索引记录进行一次回表，在得到完整的用户记录之后在使用其他的搜索条件进行过滤，比全表扫描还耗时，所以这个时候我们是不会走唯一二级索引的。 在使用idx_key1执行上述查询的时候，搜索条件 key1 like &#39;%f&#39; 比较特殊。虽然他不能作为形成扫描区间的边界条件，但是idx_key1的二级索引记录是包含key1列的。因此我们可以先判断获取到的二级索引记录是否符合这个条件。如果符合在执行回表操作，如果不符合就不用回表了。这样就可以较少因为回表带来的性能损耗，这就是索引下推。 1.4使用联合索引执行查询时对应的扫描区间联合索引的索引列包含多个列，B+树中的每一层页面以及每一个页中的记录采用的排序规则比较复杂。以上面的表为例，idx_key_part (key_part1, key_part2, key_part3) 采用的排序规则如下： 先按照key_part1进行排序 key_part1相同按照key_part2进行排序，以此类推 1.4.1全值匹配原理对于下面这条查询语句来讲： 123select *from single_tablewhere key_part1 =&#x27;a&#x27;; 因为二级索引记录先按照key_part1进行值排序的，所以符合条件的所有记录肯定是相邻的。我们可以先定位到符合条件的第一条记录，沿着链表顺序往下扫描知道不符合条件为止（当然，对于获取到的每一条二级索引记录都需要进行回表）。此时的扫描区间【’a’,’a’】。 在看一条查询语句： 1234select *from single_tablewhere key_part1 = &#x27;a&#x27; and key_part2 = &#x27;b&#x27;; 按照联合索引的排序规则，最终的扫描区间其实就是【(‘a’,’b’),(‘a’,’b’)】。 在看一条SQL： 123select *from single_tablewhere key_part1 &lt;&#x27;a&#x27;; 因为二级索引记录先按照key_part1进行值排序的，所以符合条件的所有记录肯定是相邻的。我们可以先定位到符合条件的第一条记录，然后顺着单向链表继续往后扫描，直到遇到不符合规则的记录就停止。【-∞,’a’】 1.4.2最佳左前缀匹配原理在看一条SQL： 123select *from single_tablewhere key_part2 = &#x27;a&#x27;; 由于二级索引记录不是直接按照key_part2列的值进行排序的，所以符合条件的二级索引记录可能并不相邻，也就意味着我们不能通过搜索条件来减少需要扫描的行数，这种情况下，我们是不会使用这个索引的。 在看一条SQL： 1234select *from single_tablewhere key_part1 = &#x27;a&#x27; and key_part3 = &#x27;c&#x27;; 这个时候，其实是可以按照key_part1进行过滤的，但是因为接下来是按照key_part2进行排序的，所以满足搜索条件 key_part3 = &#39;c&#39;的二级索引值记录可能并不相邻，这个时候扫描区间其实就是【’a’,’a’】。因为第二个条件走不了索引。 针对获取到的每一条二级索引记录，如果没有开启索引条件下推的特性，则必须先回表获取完整的记录在来判断 key_part3 = &#39;c&#39;条件是否成立，如果开启了索引下推特性，可以判断完 key_part3 = &#39;c&#39;是否成立后在进行回表操作，索引下推是在MySQL5.6引入的，默认开启。 在看一条SQL： 1234select *from single_tablewhere key_part1 &lt; &#x27;a&#x27; and key_part2 = &#x27;c&#x27;; 因为二级索引记录先按照key_part1进行值排序的，所以符合条件的所有记录肯定是相邻的。但是对于key_part1 &lt; &#39;a&#39;条件的二级索引记录来说，并不是直接按照key_part2进行排序的，也就是说我们不能根据key_part2 = &#39;c&#39;来进一步减少扫描的行数。那么，如果使用当前索引执行查询，可以定位到符合key_part1 &lt; &#39;a&#39;的第一条记录，然后沿着单链表往后扫描，一直到不符合key_part1 &lt; &#39;a&#39;为止。 所以在使用当前索引执行SQL的时候，对应的扫描区间其实就是【-∞,’a’】。 在看一条SQL： 1234select *from single_tablewhere key_part1 &lt;= &#x27;a&#x27; and key_part2 = &#x27;c&#x27;; 这条SQL和上一条SQL很像，唯一的区别就是从小于变成了小于等于。很显然符合key_part1 &lt;= &#39;a&#39;的索引值记录是连续的，但是对于符合key_part1 &lt;= &#39;a&#39;条件的二级索引记录来说，并不是直接按照key_part2列排序的。但是，对于符合key_part1 = &#39;a&#39;的二级索引记录来说，是按照key_part2的值进行排序的。那么再确定需要扫描的二级索引记录的范围时，当二级索引记录的key_part1 = &#39;a&#39;时，也可以通过key_part2 = &#39;c&#39;来减少扫描行数，也就是说，当扫描到不符合key_part1 &lt;= &#39;a&#39; and key_part2 = &#39;c&#39;的第一条记录的时候，就可以结束扫描，而不需要将所有的key_part1 = &#39;a&#39;的记录全部扫描完。 2. 索引用于排序我们在写查询语句的时候经常需要对查询出来的记录通过ORDER BY子句按照某种规则进行排序。一般情况下，我们只能把记录都加载到内存中，再用一些排序算法，比如快速排序、归并排序等等在内存中对这些记录进行排序，有的时候可能查询的结果集太大以至于不能在内存中进行排序的话，还可能暂时借助磁盘的空间来存放中间结果，排序操作完成后再把排好序的结果集返回到客户端。在MySQL中，把这种在内存中或者磁盘上进行排序的方式统称为文件排序（英文名：filesort），跟文件这个词儿一沾边儿，就显得这些排序操作非常慢了（磁盘和内存的速度比起来，就像是飞机和蜗牛的对比）。但是如果ORDER BY子句里使用到了我们的索引列，就有可能省去在内存或文件中排序的步骤，比如下边这个简单的查询语句： 1SELECT * FROM single_table ORDER BY key_part1, key_part2, key_part3 LIMIT 10; 这个查询的结果集需要先按照key_part1值排序，如果记录的key_part1值相同，则需要按照key_part2来排序，如果key_part2的值相同，则需要按照key_part3排序。因为这个B+树索引本身就是按照上述规则排好序的，所以直接从索引中提取数据，然后进行回表操作取出该索引中不包含的列就好了。 2.1使用联合索引进行排序注意事项对于联合索引有个问题需要注意，ORDER BY的子句后边的列的顺序也必须按照索引列的顺序给出，如果给出ORDER BY key_part1, key_part3, key_part2的顺序，那也是用不了B+树索引，这种颠倒顺序就不能使用索引的原因我们上边详细说过了，这就不赘述了。 同理，ORDER BY key_part1、ORDER BY key_part1, key_part2这种匹配索引左边的列的形式可以使用部分的B+树索引。当联合索引左边列的值为常量，也可以使用后边的列进行排序，比如这样： 1SELECT * FROM single_table WHERE key_part1 = &#x27;A&#x27; ORDER BY key_part2, key_part3 LIMIT 10; 这个查询能使用联合索引进行排序是因为key_part1列的值相同的记录是按照key_part2, key_part3排序的。 2.2不可以使用索引进行排序的几种情况2.2.1ASC、DESC混用对于使用联合索引进行排序的场景，我们要求各个排序列的排序顺序是一致的，也就是要么各个列都是ASC规则排序，要么都是DESC规则排序。 ORDER BY子句后的列如果不加ASC或者DESC默认是按照ASC排序规则排序的，也就是升序排序的。 为啥会有这种规定呢？这个还得回头想想这个idx_key_part联合索引中记录的结构： 先按照记录的key_part1列的值进行升序排列。 如果记录的key_part1列的值相同，再按照key_part2列的值进行升序排列。 如果记录的key_part2列的值相同，再按照key_part3列的值进行升序排列。 如果查询中的各个排序列的排序顺序是一致的，比方说下边这两种情况： ORDER BY key_part1, key_part2 LIMIT 10这种情况直接从索引的最左边开始往右读10行记录就可以了。 ORDER BY key_part1 DESC, key_part2 DESC LIMIT 10，这种情况直接从索引的最右边开始往左读10行记录就可以了。 但是如果我们查询的需求是先按照key_part1列进行升序排列，再按照key_part2列进行降序排列的话，比如说这样的查询语句： 1SELECT * FROM single_table ORDER BY key_part1, key_part2 DESC LIMIT 10; 这样如果使用索引排序的话过程就是这样的： 先从索引的最左边确定key_part1列最小的值，然后找到key_part1列等于该值的所有记录，然后从name列等于该值的最右边的那条记录开始往左找10条记录。 如果key_part1列等于最小的值的记录不足10条，再继续往右找key_part1值第二小的记录，重复上边那个过程，直到找到10条记录为止。 这样不能高效使用索引，而要采取更复杂的算法去从索引中取数据，所以就规定使用联合索引的各个排序列的排序顺序必须是一致的。 2.2.2排序列包含非同一个索引的列有时候用来排序的多个列不是一个索引里的，这种情况也不能使用索引进行排序，比方说： 1SELECT * FROM single_table ORDER BY key_part1, common_field LIMIT 10; key_part1和common_field并不属于一个联合索引中的列，所以无法使用索引进行排序。 2.2.3排序列使用了复杂的表达式要想使用索引进行排序操作，必须保证索引列是以单独列的形式出现，而不是修饰过的形式，比方说这样： 1SELECT * FROM single_table ORDER BY UPPER(key_part1) LIMIT 10; 使用了UPPER函数修饰过的列就不是单独的列了，这样就无法使用索引进行排序了。 3. 索引用于分组有时候我们为了方便统计表中的一些信息，会把表中的记录按照某些列进行分组。比如下边这个分组查询： 1SELECT key_part1, key_part2, key_part3, COUNT(*) FROM single_table GROUP BY key_part1, key_part2, key_part3 这个查询语句相当于做了3次分组操作： 先把记录按照key_part1值进行分组，所有key_part1值相同的记录划分为一组。 将每个key_part1值相同的分组里的记录再按照key_part2的值进行分组，将key_part3值相同的记录放到一个小分组里，所以看起来就像在一个大分组里又化分了好多小分组。 再将上一步中产生的小分组按照key_part3的值分成更小的分组，所以整体上看起来就像是先把记录分成一个大分组，然后把大分组分成若干个小分组，然后把若干个小分组再细分成更多的小小分组。 然后针对那些小小分组进行统计，比如在我们这个查询语句中就是统计每个小小分组包含的记录条数。如果没有索引的话，这个分组过程全部需要在内存里实现，而如果有了索引的话，恰巧这个分组顺序又和我们的B+树中的索引列的顺序是一致的，而我们的B+树索引又是按照索引列排好序的，这不正好么，所以可以直接使用B+树索引进行分组。 和使用B+树索引进行排序是一个道理，分组列的顺序也需要和索引列的顺序一致，也可以只使用索引列中左边的列进行分组。 四，回表的代价看下边这个查询： 1SELECT * FROM single_table WHERE key_part1 &gt; &#x27;aaa&#x27; AND key_part1 &lt; &#x27;zzz&#x27;; 在使用idx_key_part索引进行查询时大致可以分为这两个步骤： 从索引idx_key_part对应的B+树中取出key_part1值在aaa～zzz之间的用户记录。 由于索引idx_key_part对应的B+树用户记录中只包含key_part1、key_part2、key_part3、id这4个字段，而查询列表是*，意味着要查询表中所有字段，也就是还要包括其他字段。这时需要把从上一步中获取到的每一条记录的id字段都到聚簇索引对应的B+树中找到完整的用户记录，也就是我们通常所说的回表，然后把完整的用户记录返回给查询用户。 由于索引idx_key_part对应的B+树中的记录首先会按照key_part1列的值进行排序，所以值在aaa～zzz之间的记录在磁盘中的存储是相连的，集中分布在一个或几个数据页中，我们可以很快的把这些连着的记录从磁盘中读出来，这种读取方式我们也可以称为顺序I/O。根据第1步中获取到的记录的id字段的值可能并不相连，而在聚簇索引中记录是根据id（也就是主键）的顺序排列的，所以根据这些并不连续的id值到聚簇索引中访问完整的用户记录可能分布在不同的数据页中，这样读取完整的用户记录可能要访问更多的数据页，这种读取方式我们也可以称为随机I/O。一般情况下，顺序I/O比随机I/O的性能高很多，所以步骤1的执行可能很快，而步骤2就慢一些。所以这个使用索引idx_key_part的查询有这么两个特点： 会使用到两个B+树索引，一个二级索引，一个聚簇索引。 访问二级索引使用顺序I/O，访问聚簇索引使用随机I/O。 需要回表的记录越多，使用二级索引的性能就越低，甚至让某些查询宁愿使用全表扫描也不使用二级索引。比方说key_part1值在aaa～zzz之间的用户记录数量占全部记录数量90%以上，那么如果使用idx_key_part索引的话，有90%多的id值需要回表，还不如直接去扫描聚簇索引（也就是全表扫描）。 那什么时候采用全表扫描的方式，什么时候使用采用二级索引 + 回表的方式去执行查询呢？这个就是查询优化器做的工作，查询优化器会事先对表中的记录计算一些统计数据，然后再利用这些统计数据根据查询的条件来计算一下需要回表的记录数，需要回表的记录数越多，就越倾向于使用全表扫描，反之倾向于使用二级索引 + 回表的方式。当然优化器做的分析工作不仅仅是这么简单，但是大致上是这个过程。一般情况下，限制查询获取较少的记录数会让优化器更倾向于选择使用二级索引 + 回表的方式进行查询，因为回表的记录越少，性能提升就越高，比方说上边的查询可以改写成这样： 1SELECT * FROM single_table WHERE key_part1 &gt; &#x27;aaa&#x27; AND key_part1 &lt; &#x27;zzz&#x27; LIMIT 10; 添加了LIMIT 10的查询更容易让优化器采用二级索引 + 回表的方式进行查询。 对于有排序需求的查询，上边讨论的采用全表扫描还是二级索引 + 回表的方式进行查询的条件也是成立的，比方说下边这个查询： 1SELECT * FROM single_table ORDER BY key_part1, key_part2, key_part3; 由于查询列表是*，所以如果使用二级索引进行排序的话，需要把排序完的二级索引记录全部进行回表操作，这样操作的成本还不如直接遍历聚簇索引然后再进行文件排序（filesort）低，所以优化器会倾向于使用全表扫描的方式执行查询。如果我们加了LIMIT子句，比如这样： 1SELECT * FROM single_table ORDER BY key_part1, key_part2, key_part3 LIMIT 10; 这样需要回表的记录特别少，优化器就会倾向于使用二级索引 + 回表的方式执行查询。 五，更好的创建和使用索引1. 只为了用于搜索，排序&amp;分组的列创建索引也就是说，只为出现在WHERE子句中的列、连接子句中的连接列，或者出现在ORDER BY或GROUP BY子句中的列创建索引。而出现在查询列表中的列就没必要建立索引了： 1SELECT key_part1, key_part2 FROM single_table WHERE key_part3 = &#x27;Ashburn&#x27;; 像查询列表中的key_part1、key_part2这两个列就不需要建立索引，我们只需要为出现在WHERE子句中的key_part3列创建索引就可以了。 2. 考虑列的基数列的基数指的是某一列中不重复数据的个数，比方说某个列包含值2, 5, 8, 2, 5, 8, 2, 5, 8，虽然有9条记录，但该列的基数却是3。也就是说，在记录行数一定的情况下，列的基数越大，该列中的值越分散，列的基数越小，该列中的值越集中。这个列的基数指标非常重要，直接影响我们是否能有效的利用索引。假设某个列的基数为1，也就是所有记录在该列中的值都一样，那为该列建立索引是没有用的，因为所有值都一样就无法排序，无法进行快速查找了～ 而且如果某个建立了二级索引的列的重复值特别多，那么使用这个二级索引查出的记录还可能要做回表操作，这样性能损耗就更大了。所以结论就是：最好为那些列的基数大的列建立索引，为基数太小列的建立索引效果可能不好。 3. 索引列的类型尽量小我们在定义表结构的时候要显式的指定列的类型，以整数类型为例，有TINYINT、MEDIUMINT、INT、BIGINT这么几种，它们占用的存储空间依次递增，我们这里所说的类型大小指的就是该类型表示的数据范围的大小。能表示的整数范围当然也是依次递增，如果我们想要对某个整数列建立索引的话，在表示的整数范围允许的情况下，尽量让索引列使用较小的类型，比如我们能使用INT就不要使用BIGINT，能使用MEDIUMINT就不要使用INT～ 这是因为： 数据类型越小，在查询时进行的比较操作越快（这是CPU层次的东西） 数据类型越小，索引占用的存储空间就越少，在一个数据页内就可以放下更多的记录，从而减少磁盘I/O带来的性能损耗，也就意味着可以把更多的数据页缓存在内存中，从而加快读写效率。 这个建议对于表的主键来说更加适用，因为不仅是聚簇索引中会存储主键值，其他所有的二级索引的节点处都会存储一份记录的主键值，如果主键适用更小的数据类型，也就意味着节省更多的存储空间和更高效的I/O。 4. 为列前缀建立索引一个字符串其实是由若干个字符组成，如果我们在MySQL中使用utf8字符集去存储字符串的话，编码一个字符需要占用1~3个字节。假设我们的字符串很长，那存储一个字符串就需要占用很大的存储空间。在我们需要为这个字符串列建立索引时，那就意味着在对应的B+树中有这么两个问题： B+树索引中的记录需要把该列的完整字符串存储起来，而且字符串越长，在索引中占用的存储空间越大。 如果B+树索引中索引列存储的字符串很长，那在做字符串比较时会占用更多的时间。 索引列的字符串前缀其实也是排好序的，所以索引的设计者提出了个方案 — 只对字符串的前几个字符进行索引也就是说在二级索引的记录中只保留字符串前几个字符。这样在查找记录时虽然不能精确的定位到记录的位置，但是能定位到相应前缀所在的位置，然后根据前缀相同的记录的主键值回表查询完整的字符串值，再对比就好了。这样只在B+树中存储字符串的前几个字符的编码，既节约空间，又减少了字符串的比较时间，还大概能解决排序的问题。 5. 覆盖索引为了彻底告别回表操作带来的性能损耗，建议：最好在查询列表里只包含索引列，比如这样： 1SELECT key_part1, key_part2 FROM single_table WHERE key_part3 = &#x27;Ashburn&#x27;; 因为我们只查询key_part1, key_part2, 这2个索引列的值，所以在通过idx_key_part索引得到结果后就不必到聚簇索引中再查找记录的剩余列，这样就省去了回表操作带来的性能损耗。我们把这种只需要用到索引的查询方式称为索引覆盖。排序操作也优先使用覆盖索引的方式进行查询，比方说这个查询： 1SELECT key_part1, key_part2, key_part3 FROM person_info ORDER BYkey_part1, key_part2, key_part3; 虽然这个查询中没有LIMIT子句，但是采用了覆盖索引，所以查询优化器就会直接使用idx_key_part索引进行排序而不需要回表操作了。 当然，如果业务需要查询出索引以外的列，那还是以保证业务需求为重。但是尽量不要用用*号作为查询列表，最好把需要查询的列依次标明。 6.不要乱动列名假设表中有一个整数列my_col，我们为这个列建立了索引。下边的两个WHERE子句虽然语义是一致的，但是在效率上却有差别： WHERE my_col * 2 &lt; 4 WHERE my_col &lt; 4/2 第1个WHERE子句中my_col列并不是以单独列的形式出现的，而是以my_col * 2这样的表达式的形式出现的，存储引擎会依次遍历所有的记录，计算这个表达式的值是不是小于4，所以这种情况下是使用不到为my_col列建立的B+树索引的。而第2个WHERE子句中my_col列并是以单独列的形式出现的，这样的情况可以直接使用B+树索引。 所以结论就是：如果索引列在比较表达式中不是以单独列的形式出现，而是以某个表达式，或者函数调用形式出现的话，是用不到索引的。 7. 尽量维持有序插入对于一个使用InnoDB存储引擎的表来说，在我们没有显式的创建索引时，表中的数据实际上都是存储在聚簇索引的叶子节点的。而记录又是存储在数据页中的，数据页和记录又是按照记录主键值从小到大的顺序进行排序，所以如果我们插入的记录的主键值是依次增大的话，那我们每插满一个数据页就换到下一个数据页继续插，而如果我们插入的主键值忽大忽小的话，这就比较麻烦了，假设某个数据页存储的记录已经满了，它存储的主键值在1~100之间： 如果此时再插入一条主键值为9的记录，那它插入的位置就如下图： 可这个数据页已经满了，再插进来咋办呢？我们需要把当前页面分裂成两个页面，把本页中的一些记录移动到新创建的这个页中。页面分裂和记录移位意味着什么？意味着：性能损耗！所以如果我们想尽量避免这样无谓的性能损耗，最好让插入的记录的主键值依次递增，这样就不会发生这样的性能损耗了。所以我们建议：让主键具有**AUTO_INCREMENT**，让存储引擎自己为表生成主键，而不是我们手动插入 。 8.冗余和重复索引我们知道，通过idx_key_part索引就可以对key_part1列进行快速搜索，再创建一个专门针对key_part1列的索引就算是一个冗余索引，维护这个索引只会增加维护的成本，并不会对搜索有什么好处。 至此，索引命中的原理和我们在建立索引的时候应该注意什么就分析完了，好家伙，又是一个通宵。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/tags/MySQL/"}]},{"title":"MySQL[三]InnoDB索引结构","slug":"MySQL/MySQL[三]InnoDB索引结构","date":"2022-01-02T16:00:00.000Z","updated":"2022-01-12T00:41:01.638Z","comments":true,"path":"2022/01/03/MySQL/MySQL[三]InnoDB索引结构/","link":"","permalink":"https://yinhuidong.github.io/2022/01/03/MySQL/MySQL[%E4%B8%89]InnoDB%E7%B4%A2%E5%BC%95%E7%BB%93%E6%9E%84/","excerpt":"","text":"索引其实就是对数据按照某种格式进行存储的文件。就InnoDB来讲，索引文件里面会有很多的基本单元【页】。​ 为什么有页的概念？​ 查询数据的时候直接交互磁盘，效率显然又会很慢，所以真正处理数据的过程其实是在内存中，这样就需要把磁盘的数据加载到内存，如果是写操作，可能还要将内存的数据再次刷新到磁盘。如果内存与磁盘的数据交互过程是基于一条条记录来进行的，显然又会很慢，所以InnoDB采取的方式是将数据划分为若干个页，以页来作为内存和磁盘交互的基本单位，默认大小为16KB。 ​ 数据或者叫记录，其实是以【行】的格式存储在页里面的，可以简单的理解成页里面的一行对应一条记录。​ 当然索引文件里面肯定不光只有页，还会有其余的东西，页里面也不光只有行格式，也会有额外的信息，这个下面我们会详细分析，至此我们仅仅需要明确一下索引的概念和层级关系。 明确了这个层级关系之后，接下来我们来从最基础的行格式来进行分析。 一，行格式我们平时都是以记录为单位向表中插入数据的，这些记录在磁盘上的存储形式被称为行格式或者记录格式，截至目前，一共有4种行格式。分别是 compact redundant dynamic compressed，MySQL5.7默认的行格式为dynamic。 1. 如何指定行格式123CREATE TABLE 表名 (列的信息) ROW_FORMAT=行格式名称 ALTER TABLE 表名 ROW_FORMAT=行格式名称 比如我们创建一张表来指定行格式： 12345678create table record_format( c1 varchar(10), c2 varchar(10) not null, c3 char(10), c4 varchar(10))charset=ascii row_format=compact;INSERT INTO record_format_demo(c1, c2, c3, c4) VALUES(&#x27;aaaa&#x27;, &#x27;bbb&#x27;, &#x27;cc&#x27;, &#x27;d&#x27;), (&#x27;eeee&#x27;, &#x27;fff&#x27;, NULL, NULL); 2.compact 行格式首先我们来看Compact行格式。 一条完整的行格式可以被分为两个部分：记录额外信息的部分&amp;记录真实数据的部分。 2.1 额外的信息额外的信息实包含三部分：变长字段的长度列表，NULL值列表和记录头信息。 2.1.1 变长字段长度列表MySQL支持很多的变长字段，我们就以最经典的varchar来进行举例，变长字段的数据存储多少字节其实是不固定的，所以在存储真实的数据的时候，要记录一下真实数据的字节数，这样的话，一个变长字段列实际上就占用了两部分的空间来存储：【真实数据】&amp;【真实数据占用字节数】。 注意：对于一个列varchar(100)，我们实际上存储一个10字节的数据，当在内存中为这个列的数据分配内存空间的时候，实际上会分配100字节，但是这个列的数据在磁盘上，实际上只会分配10字节。 在Compact行格式中，会把所有的变长字段占用的真实长度全部逆序存储在记录的开头位置，形成一个变长字段长度列表。 比如我们刚才创建的那张表，我们来分析一下： c1,c2,c4三个列都是变长字段，所以这三个列的值的长度其实都需要保存到变长字段长度列表，因为这张表的字符集的ASCII，所以每个字符实际只占用1字节来进行编码： 列名 储存内容 内容长度(十进制表示) 内容长度(十六进制表示) C1 ‘aaaa’ 4 0x04 C2 ‘bbb’ 3 0x03 C4 ‘d’ 1 0x01 因为这些长度是按照逆序来存放的，所以最终变长字段长度列表的字节串用十六进制表示的效果就是【010304】。 因为我们演示的这条记录中，c1,c2,c4列中的字符串都比较短，所以真实的数据占用的字节数就比较小，真实数据的长度用一个字节就可以表示，但是如果变长列的内容占用字节数比较多，可能就需要用2个字节来表示。对此InnoDB的规定是： 【W】：某个字符集中表示一个字符最多需要使用的字节数 【M】：当前列类型最多能存储的字符数(比如varchar(100),M=100),如果换算成字节数就是W*M 【L】：真实占用的字节数 如果M*W&lt;=255,那么使用1字节来表示字符串实际用到的字节数。 InnoDB在读记录的变长字段长度列表的时候会先去查看表结构，判断用几个字节去存储的。 如果M*W&gt;=255,这个时候再次分为两种情况： 如果L&lt;=127，那就用1个字节表示 否则就用2个字节表示 如果某个变长字段允许存储的最大字节数大于255的时候，怎么区分他正在读取的字节是一个单独的字段长度还是半个字段长度呢？ InnoDB用该字节的第一个二进制为作为标志位，0：单独的字段长度，1：半个字段长度。 对于一些占用字节数特别多的字段，单个页都无法存储的时候，InnoDB会把一部分数据放到所谓的溢出页，在变长字段长度列表中只会记录当前页的字段长度，所以用两个字节也可以存的下。 此外，变长字段的长度列表中只存储真实数据值为非NULL的列占用的长度，真实数据为NULL的列的长度是不存储的。 也并不是所有的记录都会有变长字段长度列表，假如表中的列要是没有变长字段，或者记录中的变长字段值都是NULL，那就没有变长字段长度列表了。 2.1.2 NULL值列表如果一条记录有多个字段的真实值为NULL，不统一管理的话就会比较占用空间，所以抽取出来了NULL值列表。 当然如果这个表的所有字段都是NOT NULL约束的，就不会有NULL值列表。 看一下处理过程： 首先统计出表中允许存储NULL的字段 如果表中没有NULL字段的列，那就没必要再往下了，否则将每个允许存储NULL的列对应的一个二进制位按照列的顺序逆序排列。1：NULL，0：不是NULL。 MySQL规定NULL值必须用整数个字节的位表示，如果使用的二进制位个数不是整数个字节，则在字节的高位补0。 以此类推，如果一个表中有9个字段允许为NULL，那么这个记录的NULL值列表部分就需要2个字节来表示。 这个时候再来看我们上面创建的表中的记录。 2.1.3 记录头信息由五个固定的字节组成，换算成二进制就是40位，每一部分代表不同的信息。 名称 大小(bit) 描述 预留位1 1 没有使用 预留位2 1 没有使用 delete_mask 1 标记该记录是否被删除 min_rec_mask 1 B+树的每层非叶子节点中的最小记录都会添加该标记 n_owned 4 表示当前记录拥有的记录数 heap_no 13 表示当前记录在记录堆的位置信息 record_type 3 表示当前记录的类型 0 ：普通记录，1：B+树非页节点记录，2：最小记录，3：最大记录 next_record 16 下一条记录的相对位置 接下来来看记录的真实数据。 2.2 真实数据除了表中显式定义的列，MySQL会往我们的表中放一些隐藏列。 列名 是否必须 占用空间 描述 row_id 否 6字节 行ID，唯一标识一条记录 transaction_id 是 6字节 事务ID roll_pointer 是 7字节 回滚指针 【row_id】：这个玩意，跟主键的选择有关，如果我们显式定义了表的主键，就不会有它，如果我们没显式定义主键，那么会去选择一个unique的列作为主键，如果unique的列也没有，那么就会生成一个row_id列作为隐藏的主键。 【transaction_id】&amp;【roll_pointer】和一致性非锁读(MVCC)有关,后面遇到的时候我会在分析介绍。 在完善下我们开头创建的那张表的记录形象。 至此，其实就剩下我们显式插入数据库的真实记录了，但是还有一个特殊的类型需要说明一下。 2.2.1 CHAR 也是变长的？在Compact行格式下只会把变长类型的列的长度逆序记录到变长字段长度列表，但是这其实和我们的字符集有关系，上面我们创建的表显式指定为ASCII字符集，这个时候一个字符只会用一个字节表示，但是假如我们指定的是其它字符集，比如utf8，这个时候一个字符用几个字节表示就不确定了，所以CHAR列的真实字节长度也会被记录到变长字段长度列表。 另外，变长字符集的CHAR(M)类型的列要求至少占用M个字节，而VARCHAR(M)就没有这个要求。 对于使用utf8字符集的CHAR(10)的列来说，该列存储的数据字节长度的范围是10～30个字节。即使我们向该列中存储一个空字符串也会占用10个字节，这是怕将来更新该列的值的字节长度大于原有值的字节长度而小于10个字节时，可以在该记录处直接更新，而不是在存储空间中重新分配一个新的记录空间，导致原有的记录空间成为所谓的碎片。 3. 行溢出上面提到了，如果一条记录的真实字节数太大，就会导致行溢出，把超出的一部分数据存储到其他行或者页。 3.1 varchar(M)最多能存储的数据varchar(M)的列最多可以占用65535个字节。其中M代表该类型最多存储的字符数量。 实际上，MySQL对一条记录占用的最大存储空间是有限制的，除了BLOB，TEXT类型的列之外，其他所有的列(不包含隐藏列和记录头信息)占用的字节长度加起来不能超过65535个字节。这个65535个字节除了列本身的数据之外，还包括一些其他的数据，比如说我们为了存储一个varchar列，其实还需要占用3部分空间。 真实数据 真实数据占用的字节长度 NULL值标识，如果该列有NOT_NULL属性则可以没有这部分存储空间 如果该varchar类型的列没有NOT NULL属性那最多只能存储65532个字节的数据，因为真实数据的长度可能占用2个字节，NULL值标识需要占用1个字节。 如果VARCHAR类型的列有NOT NULL属性，那最多只能存储65533个字节的数据，因为真实数据的长度可能占用2个字节，不需要NULL值标识。 如果VARCHAR(M)类型的列使用的不是ascii字符集，那会怎么样呢？ 如果VARCHAR(M)类型的列使用的不是ascii字符集，那M的最大取值取决于该字符集表示一个字符最多需要的字节数。在列的值允许为NULL的情况下，gbk字符集表示一个字符最多需要2个字节，那在该字符集下，M的最大取值就是32766（也就是：65532/2），也就是说最多能存储32766个字符；utf8字符集表示一个字符最多需要3个字节，那在该字符集下，M的最大取值就是21844，就是说最多能存储21844（也就是：65532/3）个字符。 上述所言在列的值允许为NULL的情况下，gbk字符集下M的最大取值就是32766，utf8字符集下M的最大取值就是21844，这都是在表中只有一个字段的情况下说的，一定要记住一个行中的所有列（不包括隐藏列和记录头信息）占用的字节长度加起来不能超过65535个字节！ 3.2 记录中的数据太多产生溢出MySQL中磁盘和内存交互的基本单位是页，也就是说MySQL是以页为基本单位来管理存储空间的，我们的记录都会被分配到某个页中存储。而一个页的大小一般是16KB，也就是16384字节，而一个VARCHAR(M)类型的列就最多可以存储65532个字节，这样就可能造成一个页存放不了一条记录的尴尬情况。 在Compact和Redundant行格式中，对于占用存储空间非常大的列，在记录的真实数据处只会存储该列的一部分数据，把剩余的数据分散存储在几个其他的页中，然后记录的真实数据处用20个字节存储指向这些页的地址（当然这20个字节中还包括这些分散在其他页面中的数据的占用的字节数），从而可以找到剩余数据所在的页。 从图中可以看出来，对于Compact和Redundant行格式来说，如果某一列中的数据非常多的话，在本记录的真实数据处只会存储该列的前768个字节的数据和一个指向其他页的地址，然后把剩下的数据存放到其他页中，这个过程也叫做行溢出，存储超出768字节的那些页面也被称为溢出页。画一个简图就是这样： 不只是 VARCHAR(M)类型的列，其他的 TEXT、BLOB 类型的列在存储数据非常多的时候也会发生行溢出。 3.3 行溢出的临界点发生行溢出的临界点是什么呢？也就是说在列存储多少字节的数据时就会发生行溢出？ MySQL中规定一个页中至少存放两行记录，至于为什么这么规定我们之后再说，现在看一下这个规定造成的影响。我们往表中插入亮条记录，每条记录最少插入多少字节的数据才会行溢出呢？ 分析一下页空间是如何利用的 每个页除了存放我们的记录以外，也需要存储一些额外的信息，乱七八糟的额外信息加起来需要132个字节的空间（现在只要知道这个数字就好了），其他的空间都可以被用来存储记录。 每个记录需要的额外信息是27字节。这27个字节包括下边这些部分： 内容 大小(字节) 真实数据的长度 2 列是否是NULL值 1 头信息 5 row_id 6 transaction_id 6 roll_pointer 7 因为表中具体有多少列不确定，所以没法确定具体的临界点，只需要知道插入的字段数据长度很大就会导致行溢出的现象。 4.Dynamic &amp; Compressed 行格式这俩行格式和Compact行格式挺像，只不过在处理行溢出数据时有点儿分歧，它们不会在记录的真实数据处存储字段真实数据的前768个字节，而是把所有的字节都存储到其他页面中，只在记录的真实数据处存储其他页面的地址，就像这样： Compressed行格式和Dynamic不同的一点是，Compressed行格式会采用压缩算法对页面进行压缩，以节省空间。 至此，行格式就分析的差不多了，接下来我们来看页的存储结构。 二，页的存储结构InnoDB为了不同的目的设计了许多种页，比如存放表空间头部信息的页，存放 Insert Buffer信息的页，存放Innode信息的页，存放undo日志信息的页等等。 本节分析存放表中记录的页，官方成为索引页，为了分析方便，我们暂且叫做数据页。 系统变量innodb_page_size表明了InnoDB存储引擎中的页大小，默认值是16384字节，也就是16kb。 该变量只能在第一次初始化MySQL数据目录时指定，之后就再也不能更改了。 数据页代表的这块16kb的存储空间被划分为多个部分，不同部分有不同的功能。 从图中可以看出，一个InnoDB数据页的存储空间大致被划分为了7个部分，有的部分占用的字节数是确定的，有的占用的字节数不是确定的。 名称 中文名 占用空间大小（字节） 简单描述 File Header 文件头部 38 页的一些通用信息 Page Header 页面头部 56 数据页专有的一些信息 Infifmum + Supremum 最小记录和最大记录 26 两个虚拟的行记录 User Records 用户记录 不确定 实际存储的行记录内容 Free Space 空闲空间 不确定 页中尚未使用的空间 Page Directory 页面目录 不确定 页中某些记录的相对位置 File Trailer 文件尾部 8 校验页是否完整 1. 记录在页中的存储我们先来创建一张表 1234567mysql&gt; create table page_demo( -&gt; c1 int , -&gt; c2 int , -&gt; c3 varchar(10000), -&gt; primary key(c1) -&gt; ) charset=ascii row_format=Compact;Query OK, 0 rows affected (0.03 sec) 因为我们指定了主键，所以存储实际数据的列里面不会有隐藏的row_id,我们来看一下他的行格式。 再次回顾下记录头中5个字节表示的数据。 名称 大小(bit) 描述 预留位1 1 没有使用 预留位2 1 没有使用 delete_mask 1 标记该记录是否被删除 min_rec_mask 1 B+树的每层非叶子节点中的最小记录都会添加该标记 n_owned 4 表示当前记录拥有的记录数 heap_no 13 表示当前记录在记录堆的位置信息 record_type 3 表示当前记录的类型 0 ：普通记录，1：B+树非页节点记录，2：最小记录，3：最大记录 next_record 16 下一条记录的相对位置 针对当前这个表的行格式简化图： 接下来我们往表中插入几条数据： 1INSERT INTO page_demo VALUES(1, 100, &#x27;aaaa&#x27;), (2, 200, &#x27;bbbb&#x27;), (3, 300, &#x27;cccc&#x27;), (4, 400, &#x27;dddd&#x27;); 为了分析这些记录在页的User Records 部分中是怎么表示的，把记录头信息和实际的列数据都用十进制表示出来了（其实是一堆二进制位），所以这些记录的示意图就是： 分析一下头信息中的每个属性是什么意思。 1.1 delete_mask标记当前记录是否被删除，占用1个二进制位，0：未删除，1：删除。 被删除的记录不会立即从磁盘上删除，因为删除他们之后吧其他的记录在磁盘上重新排列需要性能消耗，所以只是打一个删除标记，所有被删掉的数据会组成一个垃圾链表，在这个链表中的记录占用的空间成为可重用空间，之后如果有新的记录插入到表中，可能会把这些删除的记录覆盖掉。 将delete_mask 设置为1 和 将被删除的记录加入到垃圾链表中其实是两个阶段。 1.2 min_rec_maskB+树的每层非叶子节点中的最小记录都会添加该标记，如果这个字段的值是0，意味着不是B+树的非叶子节点中的最小记录。 1.3 n_owned1.4 heap_no这个属性表示当前记录在本页中的位置，我们插入的四条记录在本页中的位置分别是 2，3，4 ，5 。为什么不见 0 和 1 的记录呢？ 这是因为InnoDB自动给每个页里边加了两个记录，由于这两个记录并不是我们自己插入的，所以有时候也称为虚拟记录。这两个伪记录一个代表最小记录，一个代表最大记录。 记录是如何比较大小的？对于一条完整的记录来说，比较记录大小就是比较主键的大小。比方说我们插入的4行记录的主键值分别为1，2，3，4，这也就意味着这四条记录的大小从大到小递增。 但是不管我们往页中插入了多少自己的记录，InnoDB都规定他们定义的两条伪记录分别为最小记录和最大记录。这两条记录的构造十分简单，都是由5字节大小的记录头信息和8字节大小的一个固定的部分组成的。 由于这两条记录不是我们自己定义的记录，所以他们并不存放在页的User Records部分，他们被单独放在一个称为Infimum+Supremum的部分。 从图中我们可以看出来，最小记录和最大记录的heap_no值分别是0 和 1 ， 也就是说他们的位置最靠前。 1.5 record_type这个属性表示当前记录的类型。0：普通记录，1：B+树非叶子节点记录，2：最小记录，3：最大记录。 我们自己插入的记录是普通记录 0 ， 而最大记录和最小记录record_type 分别为 2 和 3。 1.6 next_record表示从当前记录的真实数据到下一条记录的真实数据的地址偏移量。这其实是一条链表，可以通过一条记录找到他的下一条记录，但是下一条记录指的并不是按照我们插入顺序的下一条记录，而是按照主键值由小到大的顺序的下一条记录。而且规定 infimum记录 的下一条记录就是本页主键值最小的用户记录，而本页中主键最大的用户记录的下一条记录就是supremum记录。 如果从中删除一条记录，这个链表也是会跟着变化的，假如现在删除第二条记录 1delete from page_demo where c1 =2 ; 删除第二条记录以后： 发生的变化： 第二条记录并没有从存储空间中移除，而是把该记录的delete_mask设置为1 第二条记录的next_records值变成了0，意味着该记录没有下一条记录了 第一条记录的next record指向了第三条记录 最大记录的 n_owned 值从5 变成了4 所以，不论我们怎么对页中的记录做增删改查操作，InnoDB始终会维护一条记录的单链表，链表中各个节点是按照主键值由小到大的顺序连接起来的。 next_records 为啥要指向记录头信息和真实数据之间的位置呢？为啥不干脆指向整条记录的开头位置，也就是记录的额外信息开头的位置呢？ 因为这个位置刚刚好，向左读取就是记录头信息，向右读取就是真实数据。我们前边还说过变长字段长度列表，null值列表中的信息都是逆序存放的，这样可以使记录中位置靠前的字段和他们对应的字段长度信息在内存中的距离更近，可能会提高高速缓存的命中率。 因为主键值为2的记录已经被我们删除了，但是存储空间并没有回收，如果再次把这条记录插入到表中，会发生什么？ 1INSERT INTO page_demo VALUES(2, 200, &#x27;bbbb&#x27;); 从图中可以看到，InnoDB并没有因为新记录的插入而为他申请新的存储空间，而是直接复用了原来删除的记录的存储空间。 2. Page Directory（页目录）如果我们想根据主键值查找页中某条记录该咋办？ 1select * from page_demo where c1 = 3; 将所有正常的记录(包括两条隐藏记录但是不包括已经标记为删除的记录)划分为几组 每个组的最后一条记录（也就是组内最大的那条记录）的头信息中的n_owned属性表示该组拥有多少条记录 将每个组的最后一条记录的地址偏移量单独提取出来按照顺序存储到靠近页的尾部的地方，这个地方就是所谓的【Page Directory】,也就是页目录。页目录中的这些地址偏移量被称为槽，所以页目录就是由槽组成的 比方说刚才创建的表中正常的记录由6条，InnoDB会把他们分成两组，第一组中只有一条最小记录，第二组中是剩余的5条记录。 现在页目录部分中有两个槽，也就意味着我们的记录被分成了两个组，槽1中的值为112，代表最大记录的地址偏移量；槽0的值为99，代表最小记录的地址偏移量。 注意最大和最小记录的头信息的n_owned属性： 最小记录中的n_owned值为1，这就代表着以最小记录结尾的这个分组中只有1条记录，也就是最小记录本身 最大记录中的n_owned值为5，这就代表着以最大记录结尾的这个分组中只有5条记录，包括最大记录本身还有我们自己插入的4条记录 【99】&amp;【112】这样的地址偏移量很不直观，我们用箭头指向的方式替代数字。 InnoDB对每个分组中的记录条数是有规定的：对于最小记录所在的分组只能有 1 条记录，最大记录所在的分组拥有的记录条数只能在 1~8 条之间，剩下的分组中记录的条数范围只能在是 4~8 条之间。所以分组是按照下边的步骤进行的： 初始情况下一个数据页里只有最小记录和最大记录两条记录，它们分属于两个分组。 之后每插入一条记录，都会从页目录中找到主键值比本记录的主键值大并且差值最小的槽，然后把该槽对应的记录的n_owned值加1，表示本组内又添加了一条记录，直到该组中的记录数等于8个。 在一个组中的记录数等于8个后再插入一条记录时，会将组中的记录拆分成两个组，一个组中4条记录，另一个5条记录。这个过程会在页目录中新增一个槽来记录这个新增分组中最大的那条记录的偏移量。 由于现在page_demo表中的记录太少，无法演示添加了页目录之后加快查找速度的过程，所以再往page_demo表中添加一些记录： 1INSERT INTO page_demo VALUES(5, 500, &#x27;eeee&#x27;), (6, 600, &#x27;ffff&#x27;), (7, 700, &#x27;gggg&#x27;), (8, 800, &#x27;hhhh&#x27;), (9, 900, &#x27;iiii&#x27;), (10, 1000, &#x27;jjjj&#x27;), (11, 1100, &#x27;kkkk&#x27;), (12, 1200, &#x27;llll&#x27;), (13, 1300, &#x27;mmmm&#x27;), (14, 1400, &#x27;nnnn&#x27;), (15, 1500, &#x27;oooo&#x27;), (16, 1600, &#x27;pppp&#x27;); 现在看怎么从这个页目录中查找记录。因为各个槽代表的记录的主键值都是从小到大排序的，所以我们可以使用所谓的二分法来进行快速查找。5个槽的编号分别是：0、1、2、3、4，所以初始情况下最低的槽就是low=0，最高的槽就是high=4。比方说我们想找主键值为6的记录，过程是这样的： 计算中间槽的位置：(0+4)/2=2，所以查看槽2对应记录的主键值为8，又因为8 &gt; 6，所以设置high=2，low保持不变。 重新计算中间槽的位置：(0+2)/2=1，所以查看槽1对应的主键值为4，又因为4 &lt; 6，所以设置low=1，high保持不变。 因为high - low的值为1，所以确定主键值为6的记录在槽2对应的组中。此刻我们需要找到槽2中主键值最小的那条记录，然后沿着单向链表遍历槽2中的记录。但是我们前边又说过，每个槽对应的记录都是该组中主键值最大的记录，这里槽2对应的记录是主键值为8的记录，怎么定位一个组中最小的记录呢？别忘了各个槽都是挨着的，我们可以很轻易的拿到槽1对应的记录（主键值为4），该条记录的下一条记录就是槽2中主键值最小的记录，该记录的主键值为5。所以我们可以从这条主键值为5的记录出发，遍历槽2中的各条记录，直到找到主键值为6的那条记录即可。由于一个组中包含的记录条数只能是1~8条，所以遍历一个组中的记录的代价是很小的。 所以在一个数据页中查找指定主键值的记录的过程分为两步： 通过二分法确定该记录所在的槽，并找到该槽所在分组中主键值最小的那条记录。 通过记录的next_record属性遍历该槽所在的组中的各个记录。 3.Page Header（页面头部）为了能得到一个数据页中存储的记录的状态信息，比如本页中已经存储了多少条记录，第一条记录的地址是什么，页目录中存储了多少个槽等等，特意在页中定义了一个叫Page Header的部分，它是页结构的第二部分，这个部分占用固定的56个字节，专门存储各种状态信息。 名称 占用空间大小（字节） 描述 PAGE_N_DIR_SLOTS 2 在页目录中的槽数量 PAGE_HEAP_TOP 2 还未使用的空间最小地址，也就是说从该地址之后就是Free Space PAGE_N_HEAP 2 本页中的记录的数量（包括最小和最大记录以及标记为删除的记录） PAGE_FREE 2 第一个已经标记为删除的记录地址（各个已删除的记录通过next_record也会组成一个单链表，这个单链表中的记录可以被重新利用） PAGE_GARBAGE 2 已删除记录占用的字节数 PAGE_LAST_INSERT 2 最后插入记录的位置 PAGE_DIRECTION 2 记录插入的方向 PAGE_N_DIRECTION 2 一个方向连续插入的记录数量 PAGE_N_RECS 2 该页中记录的数量（不包括最小和最大记录以及被标记为删除的记录） PAGE_MAX_TRX_ID 8 修改当前页的最大事务ID，该值仅在二级索引中定义 PAGE_LEVEL 2 当前页在B+树中所处的层级 PAGE_INDEX_ID 8 索引ID，表示当前页属于哪个索引 PAGE_BTR_SEG_LEAF 10 B+树叶子段的头部信息，仅在B+树的Root页定义 PAGE_BTR_SEG_TOP 10 B+树非叶子段的头部信息，仅在B+树的Root页定义 4.File Header（文件头部）File Header针对各种类型的页都通用，也就是说不同类型的页都会以File Header作为第一个组成部分，它描述了一些针对各种页都通用的一些信息，比方说这个页的编号是多少，它的上一个页、下一个页是谁，这个部分占用固定的38个字节。 名称 占用空间大小（字节） 描述 FIL_PAGE_SPACE_OR_CHKSUM 4 页的校验和（checksum值） FIL_PAGE_OFFSET 4 页号 FIL_PAGE_PREV 4 上一个页的页号 FIL_PAGE_NEXT 4 下一个页的页号 FIL_PAGE_LSN 8 页面被最后修改时对应的日志序列位置（英文名是：Log Sequence Number） FIL_PAGE_TYPE 2 该页的类型 FIL_PAGE_FILE_FLUSH_LSN 8 仅在系统表空间的一个页中定义，代表文件至少被刷新到了对应的LSN值 FIL_PAGE_ARCH_LOG_NO_OR_SPACE_ID 4 页属于哪个表空间 InnoDB为了不同的目的而把页分为不同的类型，我们上边介绍的其实都是存储记录的数据页，其实还有很多别的类型的页，具体如下表： 类型名称 十六进制 描述 FIL_PAGE_TYPE_ALLOCATED 0x0000 最新分配，还没使用 FIL_PAGE_UNDO_LOG 0x0002 Undo日志页 FIL_PAGE_INODE 0x0003 段信息节点 FIL_PAGE_IBUF_FREE_LIST 0x0004 Insert Buffer空闲列表 FIL_PAGE_IBUF_BITMAP 0x0005 Insert Buffer位图 FIL_PAGE_TYPE_SYS 0x0006 系统页 FIL_PAGE_TYPE_TRX_SYS 0x0007 事务系统数据 FIL_PAGE_TYPE_FSP_HDR 0x0008 表空间头部信息 FIL_PAGE_TYPE_XDES 0x0009 扩展描述页 FIL_PAGE_TYPE_BLOB 0x000A 溢出页 FIL_PAGE_INDEX 0x45BF 索引页，也就是我们所说的数据页 我们存放记录的数据页的类型其实是FIL_PAGE_INDEX，也就是所谓的索引页。 有时候我们存放某种类型的数据占用的空间非常大（比方说一张表中可以有成千上万条记录），InnoDB可能不可以一次性为这么多数据分配一个非常大的存储空间，如果分散到多个不连续的页中存储的话需要把这些页关联起来，FIL_PAGE_PREV和FIL_PAGE_NEXT就分别代表本页的上一个和下一个页的页号。这样通过建立一个双向链表把许许多多的页就都串联起来了，而无需这些页在物理上真正连着。需要注意的是，并不是所有类型的页都有上一个和下一个页的属性，不过我们现在分析的数据页（也就是类型为FIL_PAGE_INDEX的页）是有这两个属性的，所以所有的数据页其实是一个双链表。 5.File Trailer(文件尾部)如果页中的数据在内存中被修改了，那么在修改后的某个时间需要把数据同步到磁盘中。但是在同步了一半的时候中断电了咋办？ 为了检测一个页是否完整，在每个页的尾部都加了一个File Trailer部分，这个部分由8个字节组成，可以分成2个小部分： 前四个字节代表校验和 后四个字节代表页面被最后修改时对应的日志序列位置 这个File Trailer &amp; File Header 类似，都是所有类型的页通用的。 至此，整个数据页的结构我们也基本上分析完了，现在在回头看一下开头我们那张恐怖的图，是不是感觉清晰很多了呢？接下来，我们来分析索引的结构。 三，索引1.假如没有索引我们先来看看没有索引的情况下，我们进行数据的查找(毕竟没有对比就没有伤害)。 1.1 在一个页中查找假设表中的记录很少，所有的记录仅仅用一个页就存放下了，这个时候按照不同的搜索条件其实可以分为两种情况讨论： 【以主键为搜索的条件】：可以再页目录中根据二分查找快速定位到槽，在根据槽定位到该组的最小索引记录，然后进行遍历匹配查找。 【以其他列作为搜索条件】：在数据页中并没有为非主键列建立所谓的页目录，所以无法通过二分法快速定位相应的槽。在这种情况下，只能从最小记录开始依次往后遍历单链表中的每条记录，然后对比每条记录是否符合搜索条件，显然，效率很低。 1.2 在很多页中查找很多时候，表的记录一个页都是存储不下的，这个时候的查找其实分为两个步骤： 【定位到记录所在的页】 【从所在的页内查找相应的记录】 因为我们不能快速的定位到所在的页，所以只能从第一页开始沿着双链表往后遍历定位页，定位到页以后在根据在一个页中的查找方式进行匹配查找，显而易见，这个时候效率低的可怕。 有了痛点，就会有大牛去思考整个生命周期，完善逻辑和资源倾斜，形成一套自己的方法论，想办法为快速查找赋能。 2. 索引我们先创建一张表： 1234567mysql&gt; CREATE TABLE index_demo( -&gt; c1 INT, -&gt; c2 INT, -&gt; c3 CHAR(1), -&gt; PRIMARY KEY(c1) -&gt; ) ROW_FORMAT = Compact;Query OK, 0 rows affected (0.03 sec) 2.1 一个简单的索引方案我们在根据某个搜索条件查找一些记录时为什么要遍历所有的数据页呢？ 因为各个页中的记录并没有规律，我们并不知道我们的搜索条件匹配哪些页中的记录，所以 不得不 依次遍历所有的数据页。 如果我们想快速的定位到需要查找的记录在哪些数据页中该咋办？ 对比根据主键值快速定位一条记录从而在页中的位置建立页目录，我们也可以想办法为快速定位记录所在的数据页而建立一个别的目录。 【下一个数据页中用户记录的主键值必须大于上一个页中用户记录的主键值。】 假设我们现在每一页只能放三条记录，现在已经放了主键为1,3,5的三条记录。这个时候我们再添加一条主键为4的记录，我们不得不为他分配一个新的页。 注意：新分配的数据页编号可能和原来并不是连续的，也就是说我们使用的这些页在存储空间里可能并不挨着。他们只是通过维护着上一页和下一页的编号而建立了链表关系。 原来页中主键最大的值为5，现在我们新插入一条记录，如果直接放在新页里面，那就会有问题，这不符合下一个数据页中用户记录的主键值必须大于上一个页中用户记录的主键值得要求，所以在插入主键值为4 的记录的时候需要伴随一次记录的移动，也就是把主键值为5 的记录移动到新分配的页中，然后把主键值为4 的记录插入到原来的页中。 这个过程表明了在对页中的记录进行增删改操作的过程中，我们必须通过一些诸如记录移动的操作来始终保证这个状态一直成立：下一个数据页中用户记录的主键值必须大于上一个页中用户记录的主键值。这个过程我们也可以称为页分裂。 【给所有的页建立一个目录项。】 由于数据页的编号可能并不是连续的，所以在向index_demo表中插入许多条记录后，可能是这样的效果： 因为这些16KB的页在物理存储上可能并不挨着，所以如果想从这么多页中根据主键值快速定位某些记录所在的页，我们需要给它们做个目录，每个页对应一个目录项，每个目录项包括下边两个部分： 页的用户记录中最小的主键值，我们用key来表示。 页号，我们用page_no表示。 以页28为例，它对应目录项2，这个目录项中包含着该页的页号28以及该页中用户记录的最小主键值5。我们只需要把几个目录项在物理存储器上连续存储，比如把他们放到一个数组里，就可以实现根据主键值快速查找某条记录的功能了。比方说我们想找主键值为20的记录，具体查找过程分两步： 先从目录项中根据二分法快速确定出主键值为20的记录在目录项3中（因为 12 &lt; 20 &lt; 209），它对应的页是页9。 再根据前边说的在页中查找记录的方式去页9中定位具体的记录。 至此，针对数据页做的简易目录就搞定了。这个目录其实就是【索引】。 2.2 InnoDB中的索引方案上面的方案存在什么样的问题？ InnoDB是使用页来作为管理存储空间的基本单位，也就是最多能保证16KB的连续存储空间，而随着表中记录数量的增多，需要非常大的连续的存储空间才能把所有的目录项都放下，这对记录数量非常多的表是不现实的。 我们时常会对记录进行增删，假设我们把页28中的记录都删除了，页28也就没有存在的必要了，那意味着目录项2也就没有存在的必要了，这就需要把目录项2后的目录项都向前移动一下。 InnoDB复用了之前存储用户记录的数据页来存储目录项，为了和用户记录做一下区分，我们把这些用来表示目录项的记录称为目录项记录。 那InnoDB怎么区分一条记录是普通的用户记录还是目录项记录呢？通过记录头信息里的record_type属性，它的各个取值代表的意思如下： 0：普通的用户记录 **1**：目录项记录 2：最小记录 3：最大记录 把前边使用到的目录项放到数据页中的样子就是这样： 从图中可以看出来，我们新分配了一个编号为30的页来专门存储目录项记录。这里再次强调一遍目录项记录和普通的用户记录的不同点： 目录项记录的record_type值是1，而普通用户记录的record_type值是0。 目录项记录只有主键值和页的编号两个列，而普通的用户记录的列是用户自己定义的，可能包含很多列，另外还有InnoDB自己添加的隐藏列。 头信息里面有一个叫min_rec_mask的属性，只有在存储目录项记录的页中的主键值最小的目录项记录的min_rec_mask值为1，其他别的记录的min_rec_mask值都是0。 除此之外，两者就没有区别了，页的组成结构也是一样一样的（就是我们前边介绍过的7个部分），都会为主键值生成Page Directory（页目录），从而在按照主键值进行查找时可以使用二分法来加快查询速度。 现在以查找主键为20的记录为例，根据某个主键值去查找记录的步骤就可以大致拆分成下边两步： 先到存储目录项记录的页，也就是页30中通过二分法快速定位到对应目录项，因为12 &lt; 20 &lt; 209，所以定位到对应的记录所在的页就是页9。 再到存储用户记录的页9中根据二分法快速定位到主键值为20的用户记录。 虽然说目录项记录中只存储主键值和对应的页号，比用户记录需要的存储空间小多了，但是不论怎么说一个页只有16KB大小，能存放的目录项记录也是有限的，那如果表中的数据太多，以至于一个数据页不足以存放所有的目录项记录，该咋办呢？ 当然是再多整一个存储**目录项记录**的页。 从图中可以看出，我们插入了一条主键值为320的用户记录之后需要两个新的数据页： 为存储该用户记录而新生成了页31。 因为原先存储目录项记录的页30的容量已满（我们前边假设只能存储4条目录项记录），所以不得不需要一个新的页32来存放页31对应的目录项。 现在因为存储目录项记录的页不止一个，所以如果我们想根据主键值查找一条用户记录大致需要3个步骤，以查找主键值为20的记录为例： 确定目录项记录页 我们现在的存储目录项记录的页有两个，即页30和页32，又因为页30表示的目录项的主键值的范围是[1, 320)，页32表示的目录项的主键值不小于320，所以主键值为20的记录对应的目录项记录在页30中。 通过目录项记录页确定用户记录真实所在的页。 在真实存储用户记录的页中定位到具体的记录。 那么问题来了，在这个查询步骤的第1步中我们需要定位存储目录项记录的页，但是这些页在存储空间中也可能不挨着，如果我们表中的数据非常多则会产生很多存储目录项记录的页，那我们怎么根据主键值快速定位一个存储目录项记录的页呢？ 为这些存储目录项记录的页再生成一个更高级的目录，就像是一个多级目录一样，大目录里嵌套小目录，小目录里才是实际的数据，所以现在各个页的示意图就是这样子： 随着表中记录的增加，这个目录的层级会继续增加，如果简化一下，那么我们可以用下边这个图来描述它： 其实这是一种组织数据的形式，或者说是一种数据结构，它的名称是B+树。 不论是存放用户记录的数据页，还是存放目录项记录的数据页，我们都把它们存放到B+树这个数据结构中了，所以我们也称这些数据页为节点。从图中可以看出来，我们的实际用户记录其实都存放在B+树的最底层的节点上，这些节点也被称为叶子节点或叶节点，其余用来存放目录项的节点称为非叶子节点或者内节点，其中B+树最上边的那个节点也称为根节点。 从图中可以看出来，一个B+树的节点其实可以分成好多层，InnoDB规定最下边的那层，也就是存放我们用户记录的那层为第0层，之后依次往上加。之前的分析我们做了一个非常极端的假设：存放用户记录的页最多存放3条记录，存放目录项记录的页最多存放4条记录。其实真实环境中一个页存放的记录数量是非常大的，假设所有存放用户记录的叶子节点代表的数据页可以存放100条用户记录，所有存放目录项记录的内节点代表的数据页可以存放1000条目录项记录，那么： 如果B+树只有1层，也就是只有1个用于存放用户记录的节点，最多能存放100条记录。 如果B+树有2层，最多能存放1000×100=100000条记录。 如果B+树有3层，最多能存放1000×1000×100=100000000条记录。 如果B+树有4层，最多能存放1000×1000×1000×100=100000000000条记录。 一般情况下，我们用到的B+树都不会超过4层，那我们通过主键值去查找某条记录最多只需要做4个页面内的查找（查找3个目录项页和一个用户记录页），又因为在每个页面内有所谓的Page Directory（页目录），所以在页面内也可以通过二分法实现快速定位记录。 2.3 聚簇索引上边介绍的B+树本身就是一个目录，或者说本身就是一个索引。它有两个特点： 使用记录主键值的大小进行记录和页的排序，这包括三个方面的含义： 页内的记录是按照主键的大小顺序排成一个单向链表。 各个存放用户记录的页也是根据页中用户记录的主键大小顺序排成一个双向链表。 存放目录项记录的页分为不同的层次，在同一层次中的页也是根据页中目录项记录的主键大小顺序排成一个双向链表。 B+树的叶子节点存储的是完整的用户记录。所谓完整的用户记录，就是指这个记录中存储了所有列的值（包括隐藏列）。 我们把具有这两种特性的B+树称为聚簇索引，所有完整的用户记录都存放在这个聚簇索引的叶子节点处。这种聚簇索引并不需要我们在MySQL语句中显式的使用INDEX语句去创建，InnoDB存储引擎会自动的为我们创建聚簇索引。另外，在InnoDB存储引擎中，聚簇索引就是数据的存储方式（所有的用户记录都存储在了叶子节点），也就是所谓的索引即数据，数据即索引。 2.4 二级索引聚簇索引只能在搜索条件是主键值时才能发挥作用，因为B+树中的数据都是按照主键进行排序的。那如果我们想以别的列作为搜索条件怎么办？ 我们可以多建几棵B+树，不同的B+树中的数据采用不同的排序规则。比方说我们用c2列的大小作为数据页、页中记录的排序规则，再建一棵B+树，效果如下图所示： 这个B+树与上边介绍的聚簇索引有几处不同： 使用记录c2列的大小进行记录和页的排序，这包括三个方面的含义： 页内的记录是按照c2列的大小顺序排成一个单向链表。 各个存放用户记录的页也是根据页中记录的c2列大小顺序排成一个双向链表。 存放目录项记录的页分为不同的层次，在同一层次中的页也是根据页中目录项记录的c2列大小顺序排成一个双向链表。 B+树的叶子节点存储的并不是完整的用户记录，而只是c2列+主键这两个列的值。 目录项记录中不再是主键+页号的搭配，而变成了c2列+页号的搭配。 所以如果我们现在想通过c2列的值查找某些记录的话就可以使用我们刚刚建好的这个B+树了。以查找c2列的值为4的记录为例，查找过程如下： 确定目录项记录页 根据根页面，也就是页44，可以快速定位到目录项记录所在的页为页42（因为2 &lt; 4 &lt; 9）。 通过目录项记录页确定用户记录真实所在的页。 在页42中可以快速定位到实际存储用户记录的页，但是由于c2列并没有唯一性约束，所以c2列值为4的记录可能分布在多个数据页中，又因为2 &lt; 4 ≤ 4，所以确定实际存储用户记录的页在页34和页35中。 在真实存储用户记录的页中定位到具体的记录. 到页34和页35中定位到具体的记录。 但是这个B+树的叶子节点中的记录只存储了c2和c1（也就是主键）两个列，所以我们必须再根据主键值去聚簇索引中再查找一遍完整的用户记录。 我们根据这个以**c2**列大小排序的**B+**树只能确定我们要查找记录的主键值，所以如果我们想根据**c2**列的值查找到完整的用户记录的话，仍然需要到**聚簇索引**中再查一遍，这个过程也被称为**回表**。也就是根据c2列的值查询一条完整的用户记录需要使用到2棵B+树！！！ 为什么我们还需要一次回表操作呢？直接把完整的用户记录放到叶子节点不就好了么？ 如果把完整的用户记录放到叶子节点是可以不用回表，相当于每建立一棵B+树都需要把所有的用户记录再都拷贝一遍，这就有点太浪费存储空间了。因为这种按照非主键列建立的B+树需要一次回表操作才可以定位到完整的用户记录，所以这种B+树也被称为二级索引（英文名secondary index），或者辅助索引。由于我们使用的是c2列的大小作为B+树的排序规则，所以我们也称这个B+树为为c2列建立的索引。 假设我们的查询结果是十条，那就是要进行10次回表，那这样的话，效率不是又慢了？ 在MySQL5.6对这种情况进行了优化，如果发现查询结果会导致多次回表，那么就会进行IO合并，拿到所有的主键再去进行回表。 2.5 联合索引我们也可以同时以多个列的大小作为排序规则，也就是同时为多个列建立索引，比方说我们想让B+树按照c2和c3列的大小进行排序，这个包含两层含义： 先把各个记录和页按照c2列进行排序。 在记录的c2列相同的情况下，采用c3列进行排序 为c2和c3列建立的索引的示意图如下： 3. InnoDB的B+树索引的注意事项3.1 跟页面永远固定不动前边介绍B+树索引的时候，为了理解上的方便，先把存储用户记录的叶子节点都画出来，然后接着画存储目录项记录的内节点，实际上B+树的形成过程是这样的： 每当为某个表创建一个B+树索引（聚簇索引不是人为创建的，默认就有）的时候，都会为这个索引创建一个根节点页面。最开始表中没有数据的时候，每个B+树索引对应的根节点中既没有用户记录，也没有目录项记录。 随后向表中插入用户记录时，先把用户记录存储到这个根节点中。 当根节点中的可用空间用完时继续插入记录，此时会将根节点中的所有记录复制到一个新分配的页，比如页a中，然后对这个新页进行页分裂的操作，得到另一个新页，比如页b。这时新插入的记录根据键值（也就是聚簇索引中的主键值，二级索引中对应的索引列的值）的大小就会被分配到页a或者页b中，而根节点便升级为存储目录项记录的页。 这个过程需要特别注意的是：一个B+树索引的根节点自诞生之日起，便不会再移动。这样只要我们对某个表建立一个索引，那么它的根节点的页号便会被记录到某个地方，然后凡是InnoDB存储引擎需要用到这个索引的时候，都会从那个固定的地方取出根节点的页号，从而来访问这个索引。 3.2 内节点中目录项记录的唯一性我们知道B+树索引的内节点中目录项记录的内容是索引列 + 页号的搭配，但是这个搭配对于二级索引来说有点儿不严谨。假设表中的数据是这样的： c1 c2 c3 1 1 ‘u’ 3 1 ‘d’ 5 1 ‘y’ 7 1 ‘a’ 如果二级索引中目录项记录的内容只是索引列 + 页号的搭配的话，那么为c2列建立索引后的B+树应该长这样： 如果我们想新插入一行记录，其中c1、c2、c3的值分别是：9、1、&#39;c&#39;，那么在修改这个为c2列建立的二级索引对应的B+树时便碰到了个大问题：由于页3中存储的目录项记录是由c2列 + 页号的值构成的，页3中的两条目录项记录对应的c2列的值都是1，而我们新插入的这条记录的c2列的值也是1，那我们这条新插入的记录到底应该放到页4中，还是应该放到页5中? 为了让新插入记录能找到自己在那个页里，我们需要保证在B+树的同一层内节点的目录项记录除页号这个字段以外是唯一的。所以对于二级索引的内节点的目录项记录的内容实际上是由三个部分构成的： 索引列的值 主键值 页号 也就是我们把主键值也添加到二级索引内节点中的目录项记录了，这样就能保证B+树每一层节点中各条目录项记录除页号这个字段外是唯一的。 这样我们再插入记录(9, 1, &#39;c&#39;)时，由于页3中存储的目录项记录是由c2列 + 主键 + 页号的值构成的，可以先把新记录的c2列的值和页3中各目录项记录的c2列的值作比较，如果c2列的值相同的话，可以接着比较主键值，因为B+树同一层中不同目录项记录的c2列 + 主键的值肯定是不一样的，所以最后肯定能定位唯一的一条目录项记录，在本例中最后确定新记录应该被插入到页5中。 3.3 一个页面最少存储2条记录B+树只需要很少的层级就可以轻松存储数亿条记录，这是因为B+树本质上就是一个大的多层级目录，每经过一个目录时都会过滤掉许多无效的子目录，直到最后访问到存储真实数据的目录。那如果一个大的目录中只存放一个子目录会怎么样？那就是目录层级非常多，而且最后的那个存放真实数据的目录中只能存放一条记录，会导致效率很低。 其实让B+数的叶子结点值存储一条记录，让内节点存储多条记录，也还是可以发挥B+数的作用的。但是InnoDB为了避免数的层级过高，要求所有的数据页都至少可以容纳两条记录。 4. MyISAM中的索引方案简单介绍MyISAM的索引方案虽然也使用树形结构，但是却将索引和数据分开存储： 将表中的记录按照记录的插入顺序单独存储在一个文件中，称之为数据文件。这个文件并不划分为若干个数据页，有多少记录就往这个文件中塞多少记录就成了。我们可以通过行号而快速访问到一条记录。 使用MyISAM存储引擎的表会把索引信息另外存储到一个称为索引文件的另一个文件中。MyISAM会单独为表的主键创建一个索引，只不过在索引的叶子节点中存储的不是完整的用户记录，而是主键值 + 行号的组合。也就是先通过索引找到对应的行号，再通过行号去找对应的记录！这一点和InnoDB是完全不相同的，在InnoDB存储引擎中，我们只需要根据主键值对聚簇索引进行一次查找就能找到对应的记录，而在MyISAM中却需要进行一次回表操作，意味着MyISAM中建立的索引相当于全部都是二级索引！ 如果有需要的话，我们也可以对其它的列分别建立索引或者建立联合索引，原理和InnoDB中的索引差不多，不过在叶子节点处存储的是相应的列 + 行号。这些索引也全部都是二级索引。 由于在插入数据的时候并没有刻意按照主键大小排序，所以我们并不能在MyIsaM数据上使用二分法进行查找。 5. 创建和删除索引的语句InnoDB和MyISAM会自动为主键或者声明为UNIQUE的列去自动建立B+树索引，但是如果我们想为其他的列建立索引就需要我们显式的去指明。 我们可以在创建表的时候指定需要建立索引的单个列或者建立联合索引的多个列： 1234CREATE TALBE 表名 ( 各种列的信息 ··· , [KEY|INDEX] 索引名 (需要被索引的单个列或多个列)) 我们也可以在修改表结构的时候添加索引： 1ALTER TABLE 表名 ADD [INDEX|KEY] 索引名 (需要被索引的单个列或多个列); 也可以在修改表结构的时候删除索引： 1ALTER TABLE 表名 DROP [INDEX|KEY] 索引名; 至此，整个索引相关的结构我们就都分析完了。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/tags/MySQL/"}]},{"title":"MySQL[二]概述","slug":"MySQL/MySQL[二]概述","date":"2022-01-01T16:00:00.000Z","updated":"2022-01-12T01:56:10.918Z","comments":true,"path":"2022/01/02/MySQL/MySQL[二]概述/","link":"","permalink":"https://yinhuidong.github.io/2022/01/02/MySQL/MySQL[%E4%BA%8C]%E6%A6%82%E8%BF%B0/","excerpt":"","text":"一，一条SQL的查询流程 去连接池获取连接 查询缓存，命中返回，否则继续向下 词法解析&amp;预处理 词法解析拆分SQL，语法分析检查SQL的正确性生成一颗解析树，预处理检查表名，列名，生成一颗解析树。 优化器优化，优化计划，查询计划 执行引擎生成执行计划 存储引擎查询SQL，加入缓存，返回结果。 1.获取连接MySQL支持多种通信协议，可以使用同步/异步的方式，支持长连接，短连接。​ 1.1 通信类型​ 一般来说，连接数据库都是同步连接。​ 同步连接：依赖于被调用方，受限制于被调用方的性能；一般只能一对一。 异步连接：避免阻塞，但不能节省SQL的执行时间，并发情况下，每个SQL的执行都要单独建立连接，占用大量CPU资源；异步连接必须使用连接池减少线程创建销毁的开销。 1.2 连接方式MySQL长短连接都支持，一般我们会在连接池中使用长连接。保持长连接会消耗内存，长时间不活动的连接，MySQL服务器会断开。​ 12show global variables like &#x27;wait_timeout&#x27;; -- 非交互式超时时间，如 JDBC 程序show global variables like &#x27;interactive_timeout&#x27;; -- 交互式超时时间，如数据库工具 默认长连接断开时间是8小时。 可以使用 show status;查看当前MySQL有多少个连接。 1show global status like &#x27;Thread%&#x27;; Threads_cached 缓存中的线程连接数 Threads_connected 当前打开的连接数 Threads_created 为处理连接创建的线程数 Threads_running 非睡眠状态的连接数，通常指并发连接数 每产生一个连接或者会话，服务端就会创建一个线程来处理。杀死会话本质就是kill 线程。​ 可以使用SHOW PROCESSLIST; （root 用户）查看 SQL 的执行状态。​ +—-+——+———–+——+———+——+———-+| Id | User | Host | db | Command | Time | State | Info |+—-+——+———–+——+———+——+———-+| 11 | root | localhost | NULL | Query | 0 | starting | show processlist |+—-+——+———–+——+———+——+———-+ 状态 含义 Sleep 线程正在等待客户端，以向它发送一个新语句 Query 线程正在执行查询或往客户端发送数据 Locked 该查询被其它查询锁定 Copying to tmp table on disk 临时结果集合大于 tmp_table_size。线程把临时表从存储器内部格式改变为磁盘模式，以节约存储器 Sending data 线程正在为 SELECT 语句处理行，同时正在向客户端发送数据 Sorting for group 线程正在进行分类，以满足 GROUP BY 要求 Sorting for order 线程正在进行分类，以满足 ORDER BY 要求 在5.7版本，MySQL的默认连接数是151个，我们最大可以修改为16384个 （214）。​ 123show variables like &#x27;max_connections&#x27;;set [global | session] max_connections =10000; 1.3 通信协议​ 编程语言的连接模块都是用 TCP 协议连接到 MySQL 服务器的，比如mysql-connector-java-x.x.xx.jar。 类unix系统上，支持 Socket套接字文件进行进程间通信。/tmp/mysql.sock windows系统上还支持命名管道和共享内存。 ​ 1.4 通信方式MySQL使用了半双工通信，所以客户端发送SQL语句给服务端的时候，不管SQL有多大，都是一次发过去的。​ 比如我们用MyBatis动态SQL生成了一个批量插入的语句，插入10万条数据，values后面跟了一长串的内容，或者 where 条件 in 里面的值太多，会出现问题。这个时候我们必须要调整 MySQL 服务器配置 max_allowed_packet 参数的值（默认是 4M），把它调大，否则就会报错。 对于服务端来说，也是一次性发送所有的数据，不能因为你已经取到了想要的数据就中断操作，这个时候会对网络和内存产生大量消耗。在程序里面避免不带 limit 的这种操作，比如一次把所有满足条件的数据全部查出来，一定要先 count 一下。如果数据量的话，可以分批查询。 2.查询缓存MySQL 的缓存默认是关闭的。​ 1show variables like &#x27;query_cache%&#x27;; MySQL不推荐使用自带的缓存，命中条件过于苛刻。且表里数据发生变化，整张表的缓存全部失效，MySQL8移除掉了缓存。 3.语法解析&amp;预处理3.1 词法解析词法分析就是把一个完整的 SQL 语句打碎成一个个的单词。​ 1select name from user where id =1; 它会打碎成 8 个符号，每个符号是什么类型，从哪里开始到哪里结束。​ 3.2 语法解析语法分析会对 SQL 做一些语法检查，比如单引号有没有闭合，然后根据 MySQL 定义的语法规则，根据 SQL 语句生成一个数据结构。这个数据结构我们把它叫做解析树（select_lex）。​ 任何数据库的中间件，比如 Mycat，Sharding-JDBC（用到了 Druid Parser），都必须要有词法和语法分析功能。 3.3 预处理​ 如果写了一个词法和语法都正确的 SQL，但是表名或者字段不存在，会在哪里报错？是在数据库的执行层还是解析器？​ 实际上还是在解析的时候报错，解析 SQL 的环节里面有个预处理器。它会检查生成的解析树，解决解析器无法解析的语义。比如，它会检查表和列名是否存在，检查名字和别名，保证没有歧义。预处理之后得到一个新的解析树。​ 4.查询优化&amp;查询执行计划一条SQL语句的执行方式有很多种，但是最终返回的结果都是相同的。查询优化器的目的就是根据解析树生成不同的执行计划（Execution Plan），然后选择一种最优的执行计划，MySQL 里面使用的是基于开销（cost）的优化器，那种执行计划开销最小，就用哪种。 12# 查看查询的开销show status like &#x27;Last_query_cost&#x27;; 4.1 优化器的作用 多表联查，以哪张表为基准表 用不用索引，用哪个索引 。。。。 ​ 4.2 优化器是怎么得到执行计划的 首先我们要启用优化器的追踪（默认是关闭的）。 开启这开关是会消耗性能的，因为它要把优化分析的结果写到表里面，所以不要轻易开启，或者查看完之后关闭它（改成 off）。 接着执行一个 SQL 语句，优化器会生成执行计划： 这个时候优化器分析的过程已经记录到系统表里面了，我们可以查询： 它是一个 JSON 类型的数据，主要分成三部分，准备阶段、优化阶段和执行阶段。 expanded_query 是优化后的 SQL 语句。 considered_execution_plans 里面列出了所有的执行计划。 分析完记得关掉它 通过追踪优化器，可以看到优化器对sql的初始优化，表的读取顺序，为什么采用了这种读取顺序。为什么采用了某个索引或者采用了全表查询。 4.3 优化器得到的结果​ 优化器最终会把解析树变成一个查询执行计划，查询执行计划是一个数据结构。 当然，这个执行计划是不是一定是最优的执行计划呢？不一定，因为 MySQL 也有可能覆盖不到所有的执行计划。​ MySQL 提供了一个执行计划的工具。我们在 SQL 语句前面加上 EXPLAIN，就可以看到执行计划的信息。​ Explain 的结果也不一定最终执行的方式。​ 4.4 选错索引这里错误决定分两类，第一，彻底错误。第二，基于成本最低，但执行速度不是最快。 由于InnoDB的 MVCC 功能和随机采样方式，默认随机采取几个数据页，当做总体数据。以部分代表整体，本来就有错误的风险。加上数据不断地添加过程中，索引树可能会分裂，结果更加不准确。 执行 ANALYZE TABLE ,可以重新构建索引，使索引树不过于分裂。 调整参数，加大InnoDB采样的页数，页数越大越精确，但性能消耗更高。一般不建议这么干。 在优化阶段，会对表中所有索引进行对比，优化器基于成本的原因，选择成本最低的索引，所以会错过最佳索引。带来的问题便是，执行速度很慢。 通过explain查看执行计划，结合sql条件查看可以利用哪些索引。 使用 force index(indexName)强制走指定索引。弊端就是后期若索引名发生改变，或索引被删除，该sql语句需要调整。 5. 存储引擎得到执行计划以后，SQL 语句是不是终于可以执行了？ 从逻辑的角度来说，我们的数据是放在哪里的，或者说放在一个什么结构里面？ 执行计划在哪里执行？是谁去执行？ 表在存储数据的同时，还要组织数据的存储结构，这个存储结构就是由存储引擎决定的，所以也可以把存储引擎叫做表类型。 在 MySQL 里面，支持多种存储引擎，他们是可以替换的，所以叫做插件式的存储引擎。​ 5.1 查看存储引擎我们数据库里面已经存在的表，我们怎么查看它们的存储引擎呢？​ 1show table status from `数据库名`; 或者通过 DDL 建表语句来查看。​ 在 MySQL 里面，我们创建的每一张表都可以指定它的存储引擎，而不是一个数据库只能使用一个存储引擎。存储引擎的使用是以表为单位的。而且，创建表之后还可以修改存储引擎。​ 一张表使用的存储引擎决定存储数据的结构，那在服务器上它们是怎么存储的呢？先要找到数据库存放数据的路径：默认情况下，每个数据库有一个自己文件夹，以 yhd数据库为例。任何一个存储引擎都有一个 frm 文件，这个是表结构定义文件。不同的存储引擎存放数据的方式不一样，产生的文件也不一样，innodb 是 1 个，memory 没有，myisam 是两个。 5.2 存储引擎比较①常见存储引擎MyISAM 和 InnoDB 是我们用得最多的两个存储引擎，在 MySQL 5.5 版本之前，默认的存储引擎是 MyISAM，它是 MySQL 自带的。 5.5 版本之后默认的存储引擎改成了 InnoDB，最主要的原因还是 InnoDB 支持事务，支持行级别的锁，对于业务一致性要求高的场景来说更适合。 ②数据库支持的存储引擎可以用这个命令查看数据库对存储引擎的支持情况： 1show engines ; 其中有存储引擎的描述和对事务、XA 协议和 Savepoints 的支持。 XA 协议用来实现分布式事务（分为本地资源管理器，事务管理器）。 Savepoints 用来实现子事务（嵌套事务）。创建了一个 Savepoints 之后，事务就可以回滚到这个点，不会影响到创建 Savepoints 之前的操作。 ③MyISAM（3 个文件）应用范围比较小。表级锁定限制了读/写的性能，因此在 Web 和数据仓库配置中，它通常用于只读或以读为主的工作。 特点 支持表级别的锁（插入和更新会锁表）。不支持事务。 拥有较高的插入（insert）和查询（select）速度。 存储了表的行数（count 速度更快）。 适合：只读之类的数据分析的项目。 ④InnoDB（2个文件）mysql 5.7 中的默认存储引擎。InnoDB 是一个事务安全（与 ACID 兼容）的 MySQL存储引擎，它具有提交、回滚和崩溃恢复功能来保护用户数据。InnoDB 行级锁（不升级为更粗粒度的锁）和 Oracle 风格的一致非锁读提高了多用户并发性和性能。InnoDB 将用户数据存储在聚集索引中，以减少基于主键的常见查询的 I/O。为了保持数据完整性，InnoDB 还支持外键引用完整性约束。 特点 支持事务，支持外键，因此数据的完整性、一致性更高。 支持行级别的锁和表级别的锁。 支持读写并发，写不阻塞读（MVCC）。 特殊的索引存放方式，可以减少 IO，提升查询效率。 适合：经常更新的表，存在并发读写或者有事务处理的业务系统。 ⑤Memory(1个文件)基于内存的存储引擎。 特征： 基于内存的表，服务器重启后，表结构会被保留，但表中的数据会被清空。 不需要进行磁盘IO，比 MYISAM 快了一个数量级。 表级锁，故并发插入性能较低。 每一行是固定的，VARCHAR 列在 memory 存储引擎中会变成 CHAR，可能导致内存浪费。 不支持 BLOB 或 TEXT 列，如果sql返回的结果列中包含 BLOB 或 TEXT，就直接采用 MYISAM 存储引擎，在磁盘上建临时表 支持哈希索引，B+树索引 MEMORY 存储引擎在很多地方可以发挥很好的作用： 用于查找或映射表，例如邮编和州名的映射表 用于缓存周期性聚合数据的结果 用于保存数据分析中产生的中间结果。即SQL执行过程中用到的临时表 监控MySQL内存中的执行情况，例如：information_schema 库下的表基本都是 memory 存储引擎，监控InnoDB缓冲池中page(INNODB_BUFFER_PAGE表)，InnoDB缓冲池状态(INNODB_BUFFER_POOL_STATS表)、InnoDB缓存页淘汰记录(INNODB_BUFFER_PAGE_LRU表)、InnoDB锁等待(INNODB_LOCK_WAITS表)、InnoDB锁信息(INNODB_LOCKS表)、InnoDB中正在执行的事务(INNODB_TRX表)等。 MEMORY 存储引擎默认 hash 索引，故等值查询特别快。同时也支持B+树索引。虽然查询速度特别快，但依旧无法取代传统的磁盘建表。 ⑥CSV(3个文件)它的表实际上是带有逗号分隔值的文本文件。csv表允许以csv格式导入或转储数据，以便与读写相同格式的脚本和应用程序交换数据。因为 csv 表没有索引，所以通常在正常操作期间将数据保存在 innodb 表中，并且只在导入或导出阶段使用 csv 表。 特点 不允许空行，不支持索引。格式通用，可以直接编辑，适合在不同数据库之间导入导出。​ 5.3 如何选择存储引擎 如果对数据一致性要求比较高，需要事务支持，可以选择 InnoDB。 如果数据查询多更新少，对查询性能要求比较高，可以选择 MyISAM。 如果需要一个用于查询的临时表，可以选择 Memory。 ​ 6.执行引擎执行引擎，它利用存储引擎提供的相应的 API 来完成操作。 为什么我们修改了表的存储引擎，操作方式不需要做任何改变？因为不同功能的存储引擎实现的 API 是相同的。 最后把数据返回给客户端，即使没有结果也要返回。​ 二，一条SQL的更新流程更新和查询很多地方并没有区别，仅仅在于拿到数据之后的操作。 1.内存结构InnnoDB 的数据都是放在磁盘上的，InnoDB 操作数据有一个最小的逻辑单位，叫做页（索引页和数据页）。我们对于数据的操作，不是每次都直接操作磁盘，因为磁盘的速度太慢了。InnoDB 使用了一种缓冲池的技术，也就是把磁盘读到的页放到一块内存区域里面。这个内存区域就叫 Buffer Pool。​ 下一次读取相同的页，先判断是不是在缓冲池里面，如果是，就直接读取，不用再次访问磁盘。 修改数据的时候，先修改缓冲池里面的页。内存的数据页和磁盘数据不一致的时候，我们把它叫做脏页。InnoDB 里面有专门的后台线程把 Buffer Pool 的数据写入到磁盘，每隔一段时间就一次性地把多个修改写入磁盘，这个动作就叫做刷脏。 Buffer Pool 是 InnoDB 里面非常重要的一个结构，主要分为 3 个部分： Buffer Pool、Change Buffer、Adaptive HashIndex，另外还有一个（redo）log buffer。​ 1.1 buffer poolBuffer Pool 缓存的是页信息，包括数据页、索引页，默认大小是 128M（134217728 字节），可以调整。 查看服务器状态，里面有很多跟 Buffer Pool 相关的信息： 1SHOW STATUS LIKE &#x27;%innodb_buffer_pool%&#x27;; 查看参数（系统变量）： 1SHOW VARIABLES like &#x27;%innodb_buffer_pool%&#x27;; 内存的缓冲池写满了怎么办？（Redis 设置的内存满了怎么办？）InnoDB 用 LRU算法来管理缓冲池（链表实现，不是传统的 LRU，分成了 young 和 old），经过淘汰的数据就是热点数据。 内存缓冲区对于提升读写性能有很大的作用。当需要更新一个数据页时，如果数据页在 Buffer Pool 中存在，那么就直接更新好了。否则的话就需要从磁盘加载到内存，再对内存的数据页进行操作。也就是说，如果没有命中缓冲池，至少要产生一次磁盘 IO。​ 1.2 ChangeBuffer写缓冲如果这个数据页不是唯一索引，不存在数据重复的情况，也就不需要从磁盘加载索引页判断数据是不是重复（唯一性检查）。这种情况下可以先把修改记录在内存的缓冲池中，从而提升更新语句（Insert、Delete、Update）的执行速度。 这一块区域就是 Change Buffer。5.5 之前叫 Insert Buffer 插入缓冲，现在也能支持 delete 和 update。 最后把 Change Buffer 记录到数据页的操作叫做 merge。什么时候发生 merge？有几种情况：在访问这个数据页的时候，或者通过后台线程、或者数据库 shut down、redo log 写满时触发。 如果数据库大部分索引都是非唯一索引，并且业务是写多读少，不会在写数据后立刻读取，就可以使用 Change Buffer（写缓冲）。写多读少的业务，调大这个值： 1SHOW VARIABLES LIKE &#x27;innodb_change_buffer_max_size&#x27;; 代表 Change Buffer 占 Buffer Pool 的比例，默认 25%。​ 1.3 Adaptive Hash Index当我们需要访问某个页中的数据时，就会把该页从磁盘加载到Buffer Pool中，如果该页已经在Buffer Pool中的话直接使用就可以了。那么问题也就来了，我们怎么知道该页在不在Buffer Pool中呢？ 我们其实是根据表空间号 + 页号来定位一个页的，也就相当于表空间号 + 页号是一个key，缓存页就是对应的value，怎么通过一个key来快速找着一个value呢？那肯定是哈希表。 所以我们可以用表空间号 + 页号作为key，缓存页作为value创建一个哈希表，在需要访问某个页的数据时，先从哈希表中根据表空间号 + 页号看看有没有对应的缓存页，如果有，直接使用该缓存页就好，如果没有，那就从free链表中选一个空闲的缓存页，然后把磁盘中对应的页加载到该缓存页的位置。​ 1.4 （redo）Log Buffer​ 如果 Buffer Pool 里面的脏页还没有刷入磁盘时，数据库宕机或者重启，这些数据丢失。如果写操作写到一半，甚至可能会破坏数据文件导致数据库不可用。为了避免这个问题，InnoDB 把所有对页面的修改操作专门写入一个日志文件，并且在数据库启动时从这个文件进行恢复操作（实现 crash-safe）——用它来实现事务的持久性。​ 这个文件就是磁盘的 redo log（叫做重做日志），对应于/var/lib/mysql/目录下的ib_logfile0 和 ib_logfile1，每个 48M。这 种 日 志 和 磁 盘 配 合 的 整 个 过 程 ， 其 实 就 是 MySQL 里 的 WAL 技 术（Write-Ahead Logging），它的关键点就是先写日志，再写磁盘。 1show variables like &#x27;innodb_log%&#x27;; 值 含义 innodb_log_file_size 指定每个文件的大小，默认 48M innodb_log_files_in_group 指定文件的数量，默认为 2 innodb_log_group_home_dir 指定文件所在路径，相对或绝对。如果不指定，则为datadir 路径。 同样是写磁盘，为什么不直接写到 db file 里面去？为什么先写日志再写磁盘？ 磁盘的最小组成单元是扇区，通常是 512 个字节。操作系统和内存打交道，最小的单位是页 Page。操作系统和磁盘打交道，读写磁盘，最小的单位是块 Block。​ 如果我们所需要的数据是随机分散在不同页的不同扇区中，那么找到相应的数据需要等到磁臂旋转到指定的页，然后盘片寻找到对应的扇区，才能找到我们所需要的一块数据，依次进行此过程直到找完所有数据，这个就是随机 IO，读取数据速度较慢。 假设我们已经找到了第一块数据，并且其他所需的数据就在这一块数据后边，那么就不需要重新寻址，可以依次拿到我们所需的数据，这个就叫顺序 IO。 刷盘是随机 I/O，而记录日志是顺序 I/O，顺序 I/O 效率更高。因此先把修改写入日志，可以延迟刷盘时机，进而提升系统吞吐。 当然 redo log 也不是每一次都直接写入磁盘，在 Buffer Pool 里面有一块内存区域（Log Buffer）专门用来保存即将要写入日志文件的数据，默认 16M，它一样可以节省磁盘 IO。​ 1SHOW VARIABLES LIKE &#x27;innodb_log_buffer_size&#x27;; redo log 的内容主要是用于崩溃恢复。磁盘的数据文件，数据来自 buffer pool。redo log 写入磁盘，不是写入数据文件。 那么，Log Buffer 什么时候写入 log file？ 在我们写入数据到磁盘的时候，操作系统本身是有缓存的。flush 就是把操作系统缓冲区写入到磁盘。 log buffer 写入磁盘的时机，由一个参数控制，默认是 1。 1SHOW VARIABLES LIKE &#x27;innodb_flush_log_at_trx_commit&#x27;; 值 含义 0（延迟写） log buffer 将每秒一次地写入 log file 中，并且 log file 的 flush 操作同时进行。该模式下，在事务提交的时候，不会主动触发写入磁盘的操作。 1（默认，实时写，实时刷） 每次事务提交时 MySQL 都会把 log buffer 的数据写入 log file，并且刷到磁盘中去。 2（实时写，延迟刷） 每次事务提交时 MySQL 都会把 log buffer 的数据写入 log file。但是 flush 操作并不会同时进行。该模式下，MySQL 会每秒执行一次 flush 操作。 redo log，它又分成内存和磁盘两部分。redo log 有什么特点？ redo log 是 InnoDB 存储引擎实现的，并不是所有存储引擎都有。 不是记录数据页更新之后的状态，而是记录这个页做了什么改动，属于物理日志。（redo log 记录的是执行的结果） redo log 的大小是固定的，前面的内容会被覆盖。 check point 是当前要覆盖的位置。如果 write pos 跟 check point 重叠，说明 redolog 已经写满，这时候需要同步 redo log 到磁盘中。 这是 MySQL 的内存结构，总结一下，分为：Buffer pool、change buffer、Adaptive Hash Index、 log buffer。 磁盘结构里面主要是各种各样的表空间，叫做 Table space。 1.5 缓存的疑问缓存（cache）是在读取硬盘中的数据时，把最常用的数据保存在内存的缓存区中，再次读取该数据时，就不去硬盘中读取了，而在缓存中读取。缓冲（buffer）是在向硬盘写入数据时，先把数据放入缓冲区,然后再一起向硬盘写入，把分散的写操作集中进行，减少磁盘碎片和硬盘的反复寻道，从而提高系统性能。 然后，InnoDB架构中，有非常重要的一个部分——缓冲池。该缓冲池需要占用服务器内存，且专用于MySQL的服务器，建议把80%的内存交给MySQL。 缓冲池有一个缓存的功能。这个缓存，是InnoDB自带的，而且经常会用到。该缓存功能并不是MySQL架构中的缓存组件。这是两者最大的区别。 MySQL组件中的缓存 所处位置：MySQL架构中的缓存组件 缓存内容：缓存的是SQL 和 该SQL的查询结果。如果SQL的大小写，格式，注释不一致，则被认为是不同的SQL，重新查询数据库，并缓存一份数据。 可否关闭：是可以手动关闭，并卸载该组件的。 InnoDB中的缓存 所处位置：InnoDB架构中的缓冲池 缓存内容：缓存的是所有需要查找的数据，所在的数据页。 可否关闭：是InnoDB缓冲池自带的功能，无法关闭，无法卸载。如果InnoDB的缓冲池被关闭或卸载，则InnoDB直接瘫痪。所以说缓冲池是InnoDB的最重要的一部分。 不建议使用MySQL的缓存是指，不建议使用MySQL架构中的缓存组件，并不是同时否定了InnoDB中的缓存功能。​ 2.磁盘结构表空间可以看做是 InnoDB 存储引擎逻辑结构的最高层，所有的数据都存放在表空间中。InnoDB 的表空间分为 5 大类。​ 2.1 系统表空间在默认情况下 InnoDB 存储引擎有一个共享表空间（对应文件/var/lib/mysql/ibdata1），也叫系统表空间。 InnoDB 系统表空间包含 InnoDB 数据字典和双写缓冲区，（Change Buffer 和 UndoLogs），如果没有指定 file-per-table，也包含用户创建的表和索引数据。 undo 在后面介绍，因为有独立的表空间。 数据字典：由内部系统表组成，存储表和索引的元数据（定义信息）。 双写缓冲（InnoDB 的一大特性） InnoDB 的页和操作系统的页大小不一致，InnoDB 页大小一般为 16K，操作系统页大小为 4K，InnoDB 的页写入到磁盘时，一个页需要分 4 次写。 如果存储引擎正在写入页的数据到磁盘时发生了宕机，可能出现页只写了一部分的情况，比如只写了 4K，就宕机了，这种情况叫做部分写失效（partial page write），可能会导致数据丢失。 1show variables like &#x27;innodb_doublewrite&#x27;; 如果这个页本身已经损坏了，用它来做崩溃恢复是没有意义的。所以在对于应用 redo log 之前，需要一个页的副本。如果出现了写入失效，就用页的副本来还原这个页，然后再应用 redo log。这个页的副本就是 double write，InnoDB 的双写技术。通过它实现了数据页的可靠性。 跟 redo log 一样，double write 由两部分组成，一部分是内存的 double write，一个部分是磁盘上的 double write。因为 double write 是顺序写入的，不会带来很大的开销。 在MySQL5.7之前，所有的表共享一个系统表空间，这个文件会越来越大，而且它的空间不会收缩。​ 2.2 独占表空间我们可以让每张表独占一个表空间。这个开关通过 innodb_file_per_table 设置，默认开启。 1SHOW VARIABLES LIKE &#x27;innodb_file_per_table&#x27;; 开启后，则每张表会开辟一个表空间，这个文件就是数据目录下的 ibd 文件，存放表的索引和数据。但是其他类的数据，如回滚（undo）信息，插入缓冲索引页、系统事务信息，二次写缓冲（Double write buffer）等还是存放在原来的共享表空间内。​ 2.3 通用表空间通用表空间也是一种共享的表空间，跟 ibdata1 类似。 可以创建一个通用的表空间，用来存储不同数据库的表，数据路径和文件可以自定义。语法： 1create tablespace ts2673 add datafile &#x27;/var/lib/mysql/ts2673.ibd&#x27; file_block_size=16K engine=innodb; 在创建表的时候可以指定表空间，用 ALTER 修改表空间可以转移表空间。 1create table t2673(id integer) tablespace ts2673; 不同表空间的数据是可以移动的。删除表空间需要先删除里面的所有表： 12drop table t2673;drop tablespace ts2673; 2.4 临时表空间存储临时表的数据，包括用户创建的临时表，和磁盘的内部临时表。对应数据目录下的 ibtmp1 文件。当数据服务器正常关闭时，该表空间被删除，下次重新产生。 memory向template的过渡，还有磁盘上简历临时表用的什么存储引擎？ 8.0之前，内存临时表用Memory引擎创建，但假如字段中有BLOB或TEXT,或结果太大，就会转用MYISM在磁盘上建表，8.0之后内存临时表由MEMORY引擎更改为TempTable引擎，相比于前者，后者支持以变长方式存储VARCHAR，VARBINARY等变长字段。从MySQL 8.0.13开始，TempTable引擎支持BLOB字段。如果超过内存表大小，则用InnoDB建表。 2.5 redo log2.6 undo log 表空间undo log（撤销日志或回滚日志）记录了事务发生之前的数据状态（不包括 select）。 如果修改数据时出现异常，可以用 undo log 来实现回滚操作（保持原子性）。 在执行 undo 的时候，仅仅是将数据从逻辑上恢复至事务之前的状态，而不是从物理页面上操作实现的，属于逻辑格式的日志(记录操作)。 redo Log 和 undo Log 与事务密切相关，统称为事务日志。 undo Log 的数据默认在系统表空间 ibdata1 文件中，因为共享表空间不会自动收缩，也可以单独创建一个 undo 表空间。 1show global variables like &#x27;%undo%&#x27;; 2.7 一条SQL的更新流程12# id =1 的记录原 name = &#x27;yhd&#x27;update user set name = &#x27;二十&#x27; where id=1; 事务开始，从内存或者磁盘取到这条数据，返回给server的执行器 执行器修改这一行数据的值为二十 记录name =yhd 到undo log 记录name = 二十 到redo log 调用存储引擎接口，在buffer pool 中修改 name =二十 事务提交 ​ 内存和磁盘之间，工作着很多后台线程。 3.后台线程后台线程的主要作用是负责刷新内存池中的数据和把修改的数据页刷新到磁盘。后台线程分为：master线程，IO 线程，purge 线程，page cleaner 线程。​ 3.1 Master 线程Master Thread是InnoDB存储引擎非常核心的一个后台线程，主要负责将缓冲池中的数据异步刷新到磁盘，保证数据的一致性，包括脏页的刷新、合并插入缓冲、UNDO页的回收等。​ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546void master_thread()&#123; loop: for(int i = 0; i &lt; 10; ++i)&#123; thread_sleep(1); // sleep 1秒 do log buffer flush to disk; if(last_one_second_ios &lt; 5) do merge at most 5 insert buffer; if(buf_get_modified_ratio_pct &gt; innodb_max_dirty_pages_pct) // 如果缓冲池中的脏页比例大于innodb_max_dirty_pages_pct(默认是75时) do buffer pool flush 100 dirty page; // 刷新100脏页到磁盘 if(no user activity) goto backgroud loop; &#125; if(last_ten_second_ios &lt; 200) // 如果过去10内磁盘IO次数小于设置的innodb_io_capacity的值（默认是200） do buffer pool flush 100 dirty page; do merge at most 5 insert buffer; // 合并插入缓冲是innodb_io_capacity的5%（10）（总是） do log buffer flush to disk; do full purge; if(buf_get_modified_ratio_pct &gt; 70%) do buffer pool flush 100 dirty page; else buffer pool flush 10 dirty page; backgroud loop： // 后台循环 do full purge // 删除无用的undo页 （总是） do merge 20 insert buffer; // 合并插入缓冲是innodb_io_capacity的5%（10）（总是） if not idle // 如果不空闲，就跳回主循环，如果空闲就跳入flush loop goto loop: // 跳到主循环 else goto flush loop flush loop: // 刷新循环 do buffer pool flush 100 dirty page; if(buf_get_modified_ratio_pct &gt; innodb_max_dirty_pages_pct) // 如果缓冲池中的脏页比例大于innodb_max_dirty_pages_pct的值（默认75%） goto flush loop; // 跳到刷新循环，不断刷新脏页，直到符合条件 goto suspend loop; // 完成刷新脏页的任务后，跳入suspend loop suspend loop: suspend_thread(); //master线程挂起，等待事件发生 waiting event; goto loop;&#125; Master Thread具有最高的线程优先级别。内部由多个循环组成：主循环（loop）、后台循环（backgroup loop）、刷新循环（flush loop）、暂停循环（suspend loop）。Master Thread会根据数据库运行的状态在loop、backgroup loop、flush loop和suspend loop中进行切换。loop是主循环，大多数的操作都在这个循环中，主要有两大部分的操作——每秒钟的操作和每10秒钟的操作。​ ①每秒钟的操作​ ​日志缓冲刷新到磁盘，即使这个事务还没有提交（总是）；即使某个事务还没有提交，InnoDB存储引擎仍然每秒会将重做日志缓冲中的内容刷新到重做日志文件。这也解释了为什么再大的事务提交的时间也是很短的。 合并插入缓冲（可能）；合并插入缓冲并不是每秒都会发生的。InnoDB存储引擎会判断当前一秒内发生的IO次数是否小于5次，如果小于5次，InnoDB存储引擎认为当前的IO压力很小，可以执行合并插入缓冲的操作； 至多刷新100个InnoDB的缓冲池中的脏页到磁盘（可能）； 刷新100个脏页也不是每秒都会发生的，InnoDB存储引擎通过判断当前缓冲池中脏页的比例(buf_get_modified_ratio_pct)是否超过了配置文件中 innodb_max_dirty_pages_pct这个参数（默认是75，代表75%），如果超过了这个值，InnoDB存储引擎则认为需要做磁盘同步的操作，将100个脏页写入磁盘中。 如果当前没有用户活动，则切换到background loop(可能)。 ​ ②每十秒的操作 刷新100个脏页到磁盘（可能） InnoDB存储引擎会先判断过去10秒之内磁盘的IO操作是否小于200次，如果是，InnoDB存储引擎认为当前有足够的磁盘IO能力，因此将100个脏页刷新到磁盘。 合并至多5个插入缓冲（总是） 将日志缓冲刷新到磁盘（总是） 删除无用的Undo页（总是） 刷新100个或者10个脏页到磁盘（总是） InnoDB存储引擎会执行full purge操作，即删除无用的Undo页。对表进行update，delete这类的操作时，原先的行被标记为删除，但是因为一致性读的关系，需要保留这些行版本的信息。但是在full purge过程中，InnoDB存储引擎会判断当前事务系统中已被删除的行是否可以删除，比如有时候可能还有查询操作需要读取之前版本的undo信息，如果可以删除，InnoDB存储引擎会立即将其删除。从源代码中可以看出，InnoDB存储引擎在执行full purge 操作时，每次最多尝试回收20个undo页。然后，InnoDB存储引擎会判断缓冲池中脏页的比例（buf_get_modified_ratio_pct）,如果有超过70%的脏页，则刷新100个脏页到磁盘，如果脏页的比例小于70%,则只需刷新10%的脏页到磁盘。 ​ 如果当前没有用户活动（数据库空闲）或者数据库关系，就会切换到backgroud loop这个循环。 backgroud loop会执行以下操作： 删除无用的Undo页（总是） 合并20个插入缓冲（总是） 跳回到主循环（总是） 不断刷新100个页直到符合条件（可能，需要跳转到flush loop中完成） 如果flush loop中也没有什么事情可以做了，InnoDB存储引擎会切换到suspend_loop，将Master Thread挂起，等待事件的发生。若用户启用了InnoDB存储引擎，却没有使用任何InnoDB存储引擎的表，那么Master Thread总是处于挂起的状态。​ 1.0.x版本中，InnoDB存储引擎最多只会刷新100个脏页到磁盘，合并20个插入缓冲。如果是在写入密集的应用程序中，每秒可能会产生大于100个的脏页，如果是产生大于20个插入缓冲的情况，那么可能会来不及刷新所有的脏页以及合并插入缓冲。后来，InnoDB存储引擎提供了参数innodb_io_capacity，用来表示磁盘IO的吞吐量，默认值为200。​ 对于刷新到磁盘的页的数量，会按照innodb_io_capacity的百分比来进行控制。规则如下： 在合并插入缓冲时，合并插入缓冲的数量为innodb_io_capacity值的5%; 在从缓冲区刷新脏页时，刷新脏页的数量为innodb_io_capacity; 如果用户使用的是SSD类的磁盘，可以将innodb_io_capacity的值调高，直到符合磁盘IO的吞吐量为止； 另一个问题是参数innodb_max_dirty_pages_pct的默认值，在1.0.x版本之前，该值的默认值是90，意味着脏页占缓冲池的90%。InnoDB存储引擎在每秒刷新缓冲池和flush loop时会判断这个值，如果该值大于innodb_max_dirty_pages_pct,才会刷新100个脏页，如果有很大的内存，或者数据库服务器的压力很大，这时刷新脏页的速度反而会降低。 后来将innodb_max_dirty_pages_pct的默认值改为了75。这样既可以加快刷新脏页的频率，又能够保证磁盘IO的负载。​ 还有一个新的参数是innodb_adaptive_flushing(自适应地刷新)，该值影响每秒刷新脏页的数量。原来的刷新规则是：脏页在缓冲池所占的比例小于innodb_max_dirty_pages_pct时，不刷新脏页；大于innodb_max_dirty_pages_pct时，刷新100个脏页。随着innodb_adaptive_flushing参数的引入，InnoDB通过一个名为buf_flush_get_desired_flush_rate的函数来判断需要刷新脏页最合适的数量。buf_flush_get_desired_flush_rate函数通过判断产生重做日志的速率来决定最合适的刷新脏页数量。 之前每次进行full purge 操作时，最多回收20个Undo页，从InnoDB 1.0.x版本开始引入了参数innodb_purge_batch_size,该参数可以控制每次full purge回收的Undo页的数量。该参数的默认值为20，并可以动态地对其进行修改。​ 1.2.x版本中再次对Master Thread进行了优化，对于刷新脏页的操作，从Master Thread线程分离到一个单独的Page Cleaner Thread，从而减轻了Master Thread的工作，同时进一步提高了系统的并发性。​ 3.2 IO 线程InnoDB中大量使用AIO (Async IO) 来处理IO请求。IO Thread的作用，是负责这些 IO 请求的回调（call back）。​ 3.3 Purge 线程事务被提交后，其所使用的undo log可能不在需要。因此，需要purge thread来回收已经使用并分配的undo页。以前Master Thread来完成释放undo log，InnoDB1.1独立出来，分担主线程压力。​ 3.4 Page Cleaner 线程​ 负责将脏页刷新到磁盘。以前Master Thread来刷新脏页，InnoDB1.2独立出来，分担主线程压力。​ 除了 InnoDB 架构中的日志文件，MySQL 的 Server 层也有一个日志文件，叫做binlog，它可以被所有的存储引擎使用。 4.binlogbinlog 以事件的形式记录了所有的DDL 和DML 语句（因为它记录的是操作而不是数据值，属于逻辑日志），可以用来做主从复制和数据恢复。跟redo log不一样，它的文件内容是可以追加的，没有固定大小限制。在开启了 binlog 功能的情况下，我们可以把 binlog 导出成 SQL 语句，把所有的操作重放一遍，来实现数据的恢复。binlog 的另一个功能就是用来实现主从复制，它的原理就是从服务器读取主服务器的 binlog，然后执行一遍。​ 有了这两个日志之后，来看一下一条更新语句是怎么执行的：​ 12# id =1 的记录原 name = &#x27;yhd&#x27;update user set name = &#x27;二十&#x27; where id=1; ​ 事务开始，从内存或者磁盘取到这条数据所在的数据页，返回给server的执行器 执行器修改这一行数据的值为二十 记录name =yhd 到undo log 在buffer pool 中修改 name =二十，此时该页变成脏页 记录name = 二十 到redo log buffer，redo log buffer每秒刷盘。 redo log 进入prepare状态，然后告诉执行器，执行完成了，可以随时提交 写入binlog 事务提交，并回写最终状态到redo log里，代表该事务已经提交 ​ 事务开始之后就产生redo log，redo log的落盘并不是随着事务的提交才写入的，而是在事务的执行过程中，便不断写入redo log文件中。一般情况下，每次事务commit时，必须调用 fsync 操作，将redo日志缓冲同步写到磁盘。另外，每次事务提交时同步写到磁盘bin log中。 那么就有了一个谁先谁后的问题：redo log 先，bin log 后。 两阶段提交的内容：**事务提交时，redo log处于 pre状态 -&gt; 写入bin log -&gt; 事务真正提交。 ** 当发生崩溃恢复时，查看的是bin log是否完整，如果bin log完整，则代表事务已经提交。 如果在两阶段提交过程中，bin log写入失败，则事务无法终止提交，崩溃恢复时就不需要重做。如果bin log写完的一瞬间，服务器宕机了，事务都来不及提交，此时bin log并不是完整的，缺少了最终的commit标记。因此也是提交失败。 简单说，redo log和bin log都可以用于表示事务的提交状态，而两阶段提交就是让这两个状态保持逻辑上的一致。 三，MySQL中支持的字符集和排序规则1.MySQL中的utf8和utf8mb4utf8字符集表示一个字符需要使用1～4个字节，但是我们常用的一些字符使用1～3个字节就可以表示了。而在MySQL中字符集表示一个字符所用最大字节长度在某些方面会影响系统的存储和性能，所以设计MySQL的大叔偷偷的定义了两个概念： utf8mb3：阉割过的utf8字符集，只使用1～3个字节表示字符。 utf8mb4：正宗的utf8字符集，使用1～4个字节表示字符。 在MySQL中utf8是utf8mb3的别名，所以之后在MySQL中提到utf8就意味着使用1~3个字节来表示一个字符，如果有使用4字节编码一个字符的情况，比如存储一些emoji表情啥的，那请使用utf8mb4。 查看字符集：SHOW (CHARACTER SET|CHARSET)。 2.字符集&amp;比较规则的应用2.1 各级别的字符集和比较规则MySQL有4个级别的字符集和比较规则，分别是： 服务器级别 数据库级别 表级别 列级别 接下来仔细看一下怎么设置和查看这几个级别的字符集和比较规则。 服务器级别123456789101112131415mysql&gt; SHOW VARIABLES LIKE &#x27;character_set_server&#x27;;+----------------------+-------+| Variable_name | Value |+----------------------+-------+| character_set_server | utf8 |+----------------------+-------+1 row in set (0.00 sec)mysql&gt; SHOW VARIABLES LIKE &#x27;collation_server&#x27;;+------------------+-----------------+| Variable_name | Value |+------------------+-----------------+| collation_server | utf8_general_ci |+------------------+-----------------+1 row in set (0.00 sec) 可以在启动服务器程序时通过启动选项或者在服务器程序运行过程中使用SET语句修改这两个变量的值。比如我们可以在配置文件中这样写： 123[server]character_set_server=gbkcollation_server=gbk_chinese_ci 当服务器启动的时候读取这个配置文件后这两个系统变量的值便修改了。 数据库级别我们在创建和修改数据库的时候可以指定该数据库的字符集和比较规则，具体语法如下： 1234567CREATE DATABASE 数据库名 [[DEFAULT] CHARACTER SET 字符集名称] [[DEFAULT] COLLATE 比较规则名称];ALTER DATABASE 数据库名 [[DEFAULT] CHARACTER SET 字符集名称] [[DEFAULT] COLLATE 比较规则名称]; 其中的DEFAULT可以省略，并不影响语句的语义。比方说我们新创建一个名叫charset_demo_db的数据库，在创建的时候指定它使用的字符集为gb2312，比较规则为gb2312_chinese_ci： 1234mysql&gt; CREATE DATABASE charset_demo_db -&gt; CHARACTER SET gb2312 -&gt; COLLATE gb2312_chinese_ci;Query OK, 1 row affected (0.01 sec) 查看 1234567891011121314151617181920mysql&gt; USE charset_demo_db;Database changedmysql&gt; SHOW VARIABLES LIKE &#x27;character_set_database&#x27;;+------------------------+--------+| Variable_name | Value |+------------------------+--------+| character_set_database | gb2312 |+------------------------+--------+1 row in set (0.00 sec)mysql&gt; SHOW VARIABLES LIKE &#x27;collation_database&#x27;;+--------------------+-------------------+| Variable_name | Value |+--------------------+-------------------+| collation_database | gb2312_chinese_ci |+--------------------+-------------------+1 row in set (0.00 sec)mysql&gt; 可以看到这个charset_demo_db数据库的字符集和比较规则就是我们在创建语句中指定的。需要注意的一点是： character_set_database 和 _collation_database_ 这两个系统变量是只读的，我们不能通过修改这两个变量的值而改变当前数据库的字符集和比较规则。 表级别我们也可以在创建和修改表的时候指定表的字符集和比较规则，语法如下： 1234567CREATE TABLE 表名 (列的信息) [[DEFAULT] CHARACTER SET 字符集名称] [COLLATE 比较规则名称]]ALTER TABLE 表名 [[DEFAULT] CHARACTER SET 字符集名称] [COLLATE 比较规则名称] 比方说我们在刚刚创建的charset_demo_db数据库中创建一个名为t的表，并指定这个表的字符集和比较规则： 1234mysql&gt; CREATE TABLE t( -&gt; col VARCHAR(10) -&gt; ) CHARACTER SET utf8 COLLATE utf8_general_ci;Query OK, 0 rows affected (0.03 sec) 如果创建和修改表的语句中没有指明字符集和比较规则，将使用该表所在数据库的字符集和比较规则作为该表的字符集和比较规则。 列级别需要注意的是，对于存储字符串的列，同一个表中的不同的列也可以有不同的字符集和比较规则。我们在创建和修改列定义的时候可以指定该列的字符集和比较规则，语法如下： 123456CREATE TABLE 表名( 列名 字符串类型 [CHARACTER SET 字符集名称] [COLLATE 比较规则名称], 其他列...);ALTER TABLE 表名 MODIFY 列名 字符串类型 [CHARACTER SET 字符集名称] [COLLATE 比较规则名称]; 比如我们修改一下表t中列col的字符集和比较规则可以这么写： 12345mysql&gt; ALTER TABLE t MODIFY col VARCHAR(10) CHARACTER SET gbk COLLATE gbk_chinese_ci;Query OK, 0 rows affected (0.04 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; 对于某个列来说，如果在创建和修改的语句中没有指明字符集和比较规则，将使用该列所在表的字符集和比较规则作为该列的字符集和比较规则。 在转换列的字符集时需要注意，如果转换前列中存储的数据不能用转换后的字符集进行表示会发生错误。比方说原先列使用的字符集是utf8，列中存储了一些汉字，现在把列的字符集转换为ascii的话就会出错，因为ascii字符集并不能表示汉字字符。 2.2 客户端和服务器通信中的字符集编码和解码使用的字符集不一致的后果如果对于同一个字符串编码和解码使用的字符集不一样，会产生意想不到的结果，作为人类的我们看上去就像是产生了乱码一样。 从发送请求到接收结果过程中发生的字符集转换 客户端使用操作系统的字符集编码请求字符串，向服务器发送的是经过编码的一个字节串。 服务器将客户端发送来的字节串采用character_set_client代表的字符集进行解码，将解码后的字符串再按照character_set_connection代表的字符集进行编码。 如果character_set_connection代表的字符集和具体操作的列使用的字符集一致，则直接进行相应操作，否则的话需要将请求中的字符串从character_set_connection代表的字符集转换为具体操作的列使用的字符集之后再进行操作。 将从某个列获取到的字节串从该列使用的字符集转换为character_set_results代表的字符集后发送到客户端。 客户端使用操作系统的字符集解析收到的结果集字节串。 在这个过程中各个系统变量的含义如下： 系统变量 描述 character_set_client 服务器解码请求时使用的字符集 character_set_connection 服务器处理请求时会把请求字符串从character_set_client转为character_set_connection character_set_results 服务器向客户端返回数据时使用的字符集 一般情况下要使用保持这三个变量的值和客户端使用的字符集相同。 比较规则的作用通常体现比较字符串大小的表达式以及对某个字符串列进行排序中。​","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/tags/MySQL/"}]},{"title":"MySQL[一]入门","slug":"MySQL/MySQL[一]入门","date":"2021-12-31T16:00:00.000Z","updated":"2022-01-12T00:40:35.348Z","comments":true,"path":"2022/01/01/MySQL/MySQL[一]入门/","link":"","permalink":"https://yinhuidong.github.io/2022/01/01/MySQL/MySQL[%E4%B8%80]%E5%85%A5%E9%97%A8/","excerpt":"","text":"一，MYSQL入门1.数据库相关概念123DB：数据库：存储数据的仓库，保存了一系列有组织的数据。DBMS：数据库管理系统：数据库是通过DBMS创建和操作的容器。SQL：结构化查询语言：专门用来与数据库通信的语言。 2.数据库的好处121.可以持久化数据到本地2.可以实现结构化查询，方便管理 3.数据库存储数据特点12345671.将数据放到表中，表放到库中。2.一个数据库有多张表，每个表都有一个名字，用来标识自己。表名具有唯一性。3.表具有一些特性，这些特性定义了数据在表中如何存储，类似Java中类的设计。4.表有列组成，我们也称为字段。所有表都是由一个列或多个列组成的，每一列类似Java中的属性。5.表中的数据按照行来存储，每一行类似于Java中的对象。 4.mysql的安装与使用参照mysql安装文档 5.Mysql常用命令12345678910111213141516171819显示数据库-----&gt;show Databases;使用数据库-----&gt;use 数据库名；显示表----&gt;show tables;显式指定数据库的表----&gt;show tables from 数据库名；查看位于那个数据库----&gt;select database();显示表结构---&gt;desc 表名；查看数据库版本：---&gt;select version();查看数据库版本2:-----&gt;Dos:mysql --version;查看数据库信息-----&gt;show CREATE DATABASE mydb1;查看服务器中的数据库，并把mydb1的字符集修改为utf-8-----&gt;ALTER DATABASE mydb1character set utf8;删除数据库-----&gt;drop database mydb1;表中增加一栏信息-----&gt;alter table student add image blob;删除表-----&gt;drop table student;修改地址-----&gt;alter table student modify address varchar(100);删除一个属性-----&gt; alter table student drop image;修改表名-----&gt;rename table student to students;查看表的创建细节-----&gt;show create table students;修改表的字符集为 gbk-----&gt;alter table students character set gbk;列名name修改为studentname-----&gt;alter table students change name studentname varchar(100); 6.mysql语法规范12345671.不区分大小写，建议关键字大写，表名列名小写。2.每条命令最好用分号结尾。3.每条语句可以缩进，换行。4.注释单行注释：#注释文字 -- 注释文字多行注释：/* */ 二，DQL查询语言1.基础查询12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758**语法： select 查询列表 from 表名****查询列表：表中的字段，常量，表达式，函数****查询的结果是张虚拟的表格**1.查询表中的单个字段select last_name from employee;2.查询表中的多个字段select last_name,salary,email from employee;3.查询表中的所有字段select * from employee;4.查询常量值select 100;select &#x27;john&#x27;;5.查询表达式select 100*98;6.查询函数select version();7.起别名select last_name as name from employee;select last_name name from employee;8.去重查询员工表中涉及到的所有的部门编号select distinct department_id from employee;9.+的作用#运算符：两个操作数都为数值型，则做加法运算；#其中一方为字符型，试图将字符型数值转换成数值型，#如果转换成功，继续做加法运算；否则，将字符型数值#转换为0；10.使用concat实现连接#案例：查询员工名和性连接成一个字段SELECT CONCAT(username,PASSWORD) FROM USER;#任何数与null做运算结果都为null 2.条件查询语法： select 查询列表 from 表名 where 筛选条件 分类： ①按照条件表达式筛选条件运算符：&gt;,&lt;,=,!=,&gt;=,&lt;= 123456查询员工工资&gt;1w2的员工信息select * from employee where salary &gt;12000;查询部门编号！=90号的员工名和部门编号select name, dep_id from employee where dep_id 1=90； ②按照逻辑表达式筛选逻辑运算符：&amp;&amp;,||,!,AND,OR,NOT 1234查询工资在一万到两万之见的员工名，工资以及奖金。select name,salary ,jiangjin where salary between 10000 and 20000;查询部门编号不在90-110之间，或者工资高于15000的员工信息。select * from employee where department&lt;90||department&gt;110 ||salary &gt;15000; ③模糊查询like：一般和通配符搭配使用通配符：%任意多个字符，包含0个字符_任意单个字符BETWEEN AND:包含临界值IN:判断某个字段的值是否属于in列表中的某一项IS NULL,IS NOT NULL:=或者！=不能用来判断null安全等于&lt;=&gt;可以判断null 123456789101112131415查询员工名中包含a的员工信息select * from emp where name like %a%;查询员工名中第三个字符为e第五个字符为a的员工名和工资select name ,salary from emp where name like %__e_a%;员工名中第二个字符为_的员工名select name from emp where name like %_\\_%;查询员工编号在100到120之间的所有员工信息select * from emp where id between 100 and 120;查询员工的工种编号是IT_PRIG,AD_PRES,AD_VP中的一个员工名和工种编号；select name , id from emp where id in(IT_PRIG,AD_PRES,AD_VP);查询没有奖金的员工名和奖金率select salary , jjl from emp where salary is Null;查询有奖金的员工名和奖金率select salary ,jjl from emp where salary is not null; ④IF null的使用：123查询员工号为176的员工的姓名和部门号和年薪SELECT last_name ,department_id , salary*(1+IFNULL(commission_pct,0))*12 &#x27;年薪&#x27;FROM employees WHERE employee_id =176; 3.排序查询语法： select 查询列表 from 表 where 筛选条件 order by 排序列表 asc 或desc （升序或者降序，默认为升序） 12345678910查询员工信息，要求工资从高到低排序select * from emp order by salary desc;查询部门编号大于等于90的员工信息，按照入职时间先后排序select * from emp where dep_id &gt;=90 order by createtime asc;按照员工年薪的高低显示员工的信息和年薪select * ,年薪 from emp order by salary*(1+if null(jjl,0))*12 as 年薪 desc;按姓名长度显示员工的姓名和工资select name ,salary from emp order by length(name) asc;查询员工信息，先按照工资排序，再按照员工编号排序select * from emp order by salary asc,id asc; 4.常见函数功能：类似Java中的方法分类：单行函数分组函数 1.单行函数1.字符函数12345678910111213141516171819202122232425262728293031323334353637381.length 获取参数值的字节个数select * from emp order by length(name);2.concat 拼接字符串select concat(last_name,first_name) as 姓名 from emp;3.upper，lower 大小写转换函数案例：将姓变大写，名字变小写，然后拼接SELECT CONCAT(UPPER(last_name),LOWER(first_name))FROM employees;4.substr,SUBSTRING 截取字符串SELECT SUBSTR(&#x27;李莫愁&#x27;,2);SELECT SUBSTR(&#x27;李莫愁&#x27;,2,3);案例：姓名中首字符大写，其他的小写然后用_拼接显示出来SELECT CONCAT( UPPER(SUBSTR(last_name, 1, 1)), &#x27;_&#x27;, LOWER(SUBSTR(last_name, 2)) ) output FROM employees ;5.instr:返回字串第一次出现的索引，如果找不到返回0SELECT INSTR(&#x27;风急天高猿啸哀&#x27;,&#x27;天&#x27;) AS out_put;6.trim :去掉前后空格或前后指定字符SELECT LENGTH(TRIM(&#x27; 张三丰 &#x27;)) AS out_put;SELECT TRIM(&#x27;a&#x27; FROM &#x27;aaaa1aa2aaa3aaa&#x27;) AS out_put;7.lpad :用指定字符填满指定长度（左填充）SELECT LPAD(&#x27;苍老师&#x27;,10,&#x27;*&#x27;);8.rpad:用指定字符填满指定长度（右填充）SELECT RPAD(&#x27;苍老师&#x27;,10,&#x27;*&#x27;);9.replace 替换SELECT REPLACE(&#x27;千锋培训机构&#x27;,&#x27;千锋&#x27;,&#x27;尚硅谷&#x27;); 2.数学函数12345678910111.round:四舍五入SELECT ROUND(1.666);SELECT ROUND(1.567,2);2.ceil 向上取整SELECT CEIL(1.52);3.floor 向下取整SELECT FLOOR(1.52);4.truncate:截断（小数点后保留几位）SELECT TRUNCATE(1.65,2);5.mod:取余SELECT MOD(10,3); 3.日期函数123456789101112131415161718192021222324252627281.now:返回当前系统日期时间SELECT NOW();2.curdate:返回当前系统日期SELECT CURDATE();3.curtime:返回当前时间SELECT CURTIME();4.获取指定部分的年月日时分秒SELECT YEAR(NOW());SELECT YEAR(hiredate) FROM employees;5.str_to_date将字符通过指定的格式转化成日期SELECT STR_TO_DATE(&#x27;1998-3-2&#x27;,&#x27;%Y-%c-%d&#x27;) AS out_put;案例：查询入职时间为1992-4-3的员工信息SELECT * FROM employeesWHERE hiredate=STR_TO_DATE(&#x27;2016-3-3&#x27;,&#x27;%Y-%c-%d&#x27;);6.date_format 将日期转换成字符SELECT DATE_FORMAT(NOW(),&#x27;%y年%m月%d日&#x27;) AS 日期;案例：查询有奖金的员工名和入职日期（xx月/xx日 xx年）SELECT last_name, DATE_FORMAT(hiredate, &#x27;%c月/%d日 %y&#x27;) FROM employees WHERE commission_pct IS NOT NULL ; 4.其他函数123SELECT VERSION();SELECT DATABASE();SELECT USER(); 5.流程控制函数1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465661.if:IF else效果SELECT IF(10&gt;5,&#x27;true&#x27;,&#x27;false&#x27;);案例：查询如果有奖金就备注有，没有就备注没有。SELECT last_name, commission_pct, IF( commission_pct IS NULL, &#x27;没奖金&#x27;, &#x27;有奖金&#x27; ) AS 备注 FROM employees ;2.case函数1)switch-CASE语法:CASE 要判断的字段或者表达式WHEN 常量1 THEN 要显示的值1或者语句1WHEN 常量2 THEN 要显示的值2或者语句2...ELSE 要显示的值n或者语句n；案例：查询员工的工资，要求部门号==30，显示的工资为1.1倍，部门号==40，显示的工资为1.2倍，部门号==50，显示的工资为1.3倍，其他部门，显示原有工资。SELECT salary AS 原始工资, department_id , CASE department_id WHEN 30 THEN salary * 1.1 WHEN 40 THEN salary * 1.2 WHEN 50 THEN salary * 1.3 ELSE salary END AS 新工资 FROM employees ;2)CASE 使用2：语法：CASE WHEN 条件1 THEN 要显示的值1或语句1WHEN 条件2 THEN 要显示的值2或语句2...ELSE 要显示的值n或语句nEND案例：查询员工的工资情况如果&gt;2w，显示A如果&gt;1.5w，显示B如果&gt;1w，显示C否则，显示DSELECT salary, CASE WHEN salary &gt; 20000 THEN &#x27;A&#x27; WHEN salary &gt; 15000 THEN &#x27;B&#x27; WHEN salary &gt; 10000 THEN &#x27;C&#x27; ELSE &#x27;D&#x27; END AS 工资等级 FROM employees ; 2.分组函数功能：用作统计使用 123456789101112131415161718192021222324251.sum :求和SELECT SUM(salary) FROM employees;2.avg：平均值SELECT AVG(salary) FROM employees;3.max：最大值SELECT MAX(salary) FROM employees;4.min：最小值SELECT MIN(salary) FROM employees;5.count：计算个数SELECT COUNT(salary) FROM employees;总结①.sum,avg一般用于处理数值类型②.max，min，count用来处理任何类型③.以上分组函数都忽略null值④.可以和distinct搭配SELECT SUM(DISTINCT salary) 纯净,SUM(salary) FROM employees;6.count的详细介绍①select COUNT(*) FROM employees;②select COUNT(1) FROM employees;③和分组函数一同查询的字段要求是group by后的字段。 5.分组查询GROUP BY 和分组函数对应分组查询中分组条件分为两类 数据源 位置 关键字 分组前筛选 原始表 GROUP BY 子句的前面 WHERE 分组后筛选 分组后的结果集 GROUP BY 子句的后面 HAVING 分组函数做条件肯定是放在having子句中。group BY 子句支持单个字段分组，多个字段分组（多个字段之间用逗号隔开没有顺序要求），表达式或函数。也可以添加排序，放在整个分组查询的最后。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869案例：查询每个工种的最高工资SELECT MAX(salary), job_id FROM employees GROUP BY job_id ORDER BY MAX(salary) ASC ;案例：查询邮箱中包含a字符的，每个部门的平均工资SELECT AVG(salary), department_id FROM employees WHERE email LIKE &#x27;%a%&#x27; GROUP BY department_id ;#select Avg(salary),dep_id from employee where email like %a% group by dep_id ;案例：查询有奖金的每个领导手下员工的最高工资SELECT MAX(salary), manager_id FROM employees WHERE commission_pct IS NOT NULL GROUP BY manager_id ;#select max(salary) ,manage_id from employees where commission_pct is not null group by manager_id;案例：哪个部门的员工个数大于二？SELECT COUNT(*), department_id FROM employees GROUP BY department_id HAVING COUNT(*) &gt; 2 ;#select dep_id from emp group by dep_id having count(*)&gt;2;案例：查询每个工种有奖金的员工的最高工资&gt;12000的工种编号和最高工资SELECT MAX(salary), job_id FROM employees WHERE commission_pct IS NOT NULL GROUP BY job_id HAVING MAX(salary) &gt; 12000 ;#select job_id ,max(salary) from emp where commission_pct IS NOT NULL group by job_id having max(salary)&gt;12000;案例：查询领导编号&gt;102的每个领导手下的最低工资&gt;5000的领导编号是哪个？SELECT manager_id ,MIN(salary)FROM employeesWHERE manager_id&gt;102GROUP BY manager_idHAVING MIN(salary)&gt;5000;#select manager_id from emp where manager_id&gt;102 group by manager_id having min(salary)&gt;5000;#按照员工姓名的长度分组，查询每一组的员工个数，筛选员工个数&gt;5的有哪些？SELECT COUNT(*) AS cFROM employeesGROUP BY LENGTH(last_name) HAVING c&gt;5;# select count(*) from emp group by length(name) having count(*)&gt;5;#查询每个部门每个工种的员工的平均工资SELECT AVG(salary),job_idFROM employeesGROUP BY department_id,job_id;#select avg(salary) from emp group by dep_id,job_id;#查询每个部门每个工种的员工的平均工资并且按照平均工资的高低显示SELECT AVG(salary),job_idFROM employeesGROUP BY department_id,job_idORDER BY AVG(salary) ASC;#select avg(salary) from emp group by dep_id,job_id order by avg(salary) asc; 6.连接查询又称为多表查询，当查询的字段来自多个表时，就会用到连接查询。**笛卡尔乘积现象：表1有m行，表2有n行，结果：m_n行_发生原因：没有有效的连接条件 分类 ①按年代分类sql92:仅仅支持内连接sql99：不支持全外连接 ②按功能分类 内连接 外连接 交叉连接 等值连接 左外连接 非等值连接 右外连接 自连接 全外连接 1.等值连接①多表等值连接的结果为多表的交集部分②n表连接，至少需要n-1个连接条件③多表的顺序没有要求④一般需要为表起别名⑤可以搭配前面介绍的所有子句使用，比如排序，分组，筛选 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192#案例一：查询女优名对应的男优名SELECT NAME, boyName FROM beauty, boys WHERE beauty.boyfriend_id = boys.`id` ;#select name, boyname from girl ,boy where girl.boyfriend_id=boy.id;#案例：查询员工名和对应的部门名SELECT last_name, department_name FROM employees, departments WHERE employees.`department_id` = departments.`department_id` ;#select name ,dep_name from emp e,dep d where e.dep.id= d.id;#案例：查询员工名，工种号，工种名。SELECT last_name, emp.`job_id`, job_title FROM employees emp, jobs job WHERE emp.`job_id` = job.`job_id` ;#select name , e.job_id,job_title from emp e,job j where e.job_id=j.id;#案例：查询有奖金的员工名和部门名SELECT last_name, department_name FROM employees emp, departments dep WHERE commission_pct IS NOT NULL &amp;&amp; emp.`department_id` = dep.`department_id` ;#select name ,dep_name from emp e ,dep d where e.dep_id =d.id &amp;&amp;e.salary_pct is not null;#案例：查询城市名第二个字符为o的部门SELECT department_name FROM locations l, departments d WHERE l.`location_id` = d.`location_id` AND l.`city` LIKE &#x27;_o%&#x27; ;#select dep_name from location l , dep d where l.city like %_o% &amp;&amp; l.id =d.location_id;#案例：查询每个城市的部门个数SELECT COUNT(*), city FROM locations l, departments d WHERE l.`location_id` = d.`location_id` GROUP BY l.`city` ;#select count(*),city from loca l,dep d where l.loc_id=d.loc_id group by count(*) asc;#案例：查询有奖金的每个部门的部门名和部门的领导编号和该部门的最低工资SELECT d.`department_name`, d.manager_id, MIN(salary) FROM employees e, departments d WHERE e.`department_id` = d.`department_id` AND e.`commission_pct` IS NOT NULL GROUP BY d.`department_id`, d.`department_name` ;#select dep_name ,d.manager_id ,min(salary) from emp e ,dep d where e.`department_id` = d.`department_id` AND e.`commission_pct` IS NOT NULL GROUP BY d.`department_id`,d.`department_name` ;#案例：查询每个工种的工种名和员工的个数，并且按照员工个数降序排序SELECT j.job_title, COUNT(*) FROM jobs j, employees e WHERE j.`job_id` = e.`job_id` GROUP BY e.`job_id`, j.`job_title` ORDER BY COUNT(*) DESC ;#案例：查询员工名，部门名和所在城市SELECT last_name, department_name, city FROM employees e, departments d, locations l WHERE e.`department_id` = d.`department_id` AND d.`location_id` = l.`location_id` ; 2.非等值连接123456789#案例：查询员工的工资和工资级别SELECT DISTINCT salary, grade_level FROM employees e, job_grades j WHERE e.salary &gt;= j.lowest_sal &amp;&amp; e.salary &lt;= j.highest_sal ORDER BY salary ASC ; 3.自连接12345678#案例：查询员工名和上级的名称SELECT e.last_name, m.last_name FROM employees e, employees m WHERE e.manager_id = m.employee_id ; 4.内连接INNER 可以省略 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475#查询员工名，部门名SELECT last_name, department_name FROM employees e INNER JOIN departments d ON e.department_id = d.department_id ;#查询名字中包含e的员工名和工种名SELECT last_name, job_title FROM employees e INNER JOIN jobs j ON e.job_id = j.job_id WHERE last_name LIKE &#x27;%e%&#x27; ;#查询部门个数&gt;3的城市名和部门个数SELECT city, COUNT(*) FROM departments d INNER JOIN locations l ON d.`location_id` = l.`location_id` GROUP BY cityHAVING COUNT(*) &gt; 3 ;#查询哪个部门的部门员工个数&gt;3的部门名和员工个数，并按照个数降序排序SELECT department_name, COUNT(*) FROM employees e INNER JOIN departments d ON e.`department_id` = d.`department_id` GROUP BY e.department_id HAVING COUNT(*) &gt; 3 ORDER BY COUNT(*) DESC ;#查询员工名，部门名，工种名，并按照部门名降序排序SELECT last_name, department_name, job_title FROM employees e INNER JOIN departments d ON e.`department_id` = d.`department_id` INNER JOIN jobs j ON e.`job_id` = j.`job_id` ORDER BY department_name DESC ;#查询员工工资级别SELECT grade_level, salaryFROM job_grades j INNER JOIN employees e ON e.`salary` BETWEEN j.`lowest_sal` AND j.`highest_sal` ;#查询每个工资级别的个数，并且降序排序SELECT grade_level,COUNT(*)FROM employees eINNER JOIN job_grades jON e.`salary` BETWEEN j.`lowest_sal` AND j.`highest_sal` GROUP BY grade_levelORDER BY COUNT(*) DESC;#查询员工的名字和上级的名字SELECT e1.last_name, e2.last_nameFROM employees e1INNER JOIN employees e2ON e1.`employee_id`=e2.`manager_id`; 5.左外连接语法：SELECT 查询列表FROM 表1 【连接类型】JOIN 表2ON 连接条件WHERE 筛选条件GROUP BY 分组HAVING 筛选条件ORDER BY 排序条件连接类型：内连接：inner左外连接：left右外连接：right全外连接：full交叉连接：cross外连接用于查询一个表中有，另一个表中没有的数据左外连接，left左边是主表右外连接，right右边是主表Mysql不支持全外连接 1234567#没有男朋友的女生SELECT g.`name`,b.`boyName`FROM beauty gLEFT JOIN boys bON g.`boyfriend_id`=b.`id`WHERE b.`boyName` IS NULL; 6.交叉连接笛卡尔乘积 7.子查询出现在其它语句中的select语句，称为子查询或内查询外部的查询语句，称为主查询或外查询分类： ①按照子查询出现的位置： select后面 from后面 where或having后面 exists后面 仅仅支持标量子查询 支持表子查询 标量子查询，列子查询 表子查询 ②按照结果集的行列数不同： 标量子查询 列子查询 行子查询 表子查询 结果只有一行一列 结果一列多行 一行多列 多行多列 1）where或having后面特点：子查询一般放在小括号内子查询一般放在条件的右边标量子查询，一般搭配着单行操作符列子查询：一般搭配多行操作符使用 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211.标量子查询#谁的工资比Abel高SELECT last_name FROM employees WHERE salary &gt; (SELECT salary FROM employees WHERE last_name = &#x27;Abel&#x27;) ;#返回job_id于141号员工相同，salary比143号员工多的员工 姓名，job_id和工资SELECT last_name, job_id, salary FROM employees WHERE job_id = (SELECT job_id FROM employees WHERE employee_id = 141) AND salary &gt; (SELECT salary FROM employees WHERE employee_id = 143)#返回公司工资工资最少的员工的姓名，job_id,salarySELECT last_name, job_id, salary FROM employees WHERE salary = (SELECT MIN(salary) FROM employees);#查询最低工资大于50号部门最低工资的部门id和其最低工资SELECT department_id, MIN(salary) FROM employees GROUP BY department_id HAVING MIN(salary) &gt; (SELECT MIN(salary) FROM employees WHERE department_id = 50) ;2.列子查询多行操作符：IN / NOT in：等于列表中的任意一个ANY / SOME ：和子查询返回的某一个值比较ALL ：和子查询返回的所有值比较#返回location_id是1400或者1700的部门中的所有员工姓名SELECT last_name FROM employees WHERE department_id IN (SELECT DISTINCT department_id FROM departments WHERE location_id IN (1400, 1700)) ;#返回其他工种中比job_id为IT_PROG部门任意工资低的员工#工号，姓名，job_id以及salarySELECT employee_id, last_name, job_id, salary FROM employees WHERE salary &lt; (SELECT MAX(salary) FROM employees WHERE job_id = &#x27;IT_PROG&#x27;) AND job_id !=&#x27;IT_PROG&#x27;;#返回其他工种中比job_id为IT_PROG部门所有工资低的员工#工号，姓名，job_id以及salarySELECT employee_id, last_name, job_id, salary FROM employees WHERE salary &lt; (SELECT MIN(salary) FROM employees WHERE job_id = &#x27;IT_PROG&#x27;) AND job_id !=&#x27;IT_PROG&#x27;;*********************************3.行子查询#查询员工编号最小并且工资最高的员工信息SELECT * FROM employees WHERE employee_id = (SELECT MIN(employee_id) FROM employees) AND salary = (SELECT MAX(salary) FROM employees) 2）SELECT 后面123456789101112131415161718192021#查询每个部门的员工个数SELECT d.*, (SELECT COUNT(*) FROM employees e WHERE e.department_id = d.department_id) FROM departments d ;#查询员工号等于102的部门名SELECT department_name FROM departments WHERE department_id = (SELECT department_id FROM employees WHERE employee_id = 102) ; 3）FROM 后面1234567891011121314#查询每个部门平均工资的工资等级SELECT grade_level ,aa.department_idFROM (SELECT AVG(salary) ag, department_id FROM employees GROUP BY department_id) aa INNER JOIN job_grades j ON aa.ag BETWEEN lowest_sal AND highest_sal ; 4）exists后面（相关子查询）12345678#查询有员工的部门名SELECT department_name FROM departments dWHERE EXISTS(SELECT * FROM employees e WHERE d.department_id=e.department_id);#查询没有女朋友的男生信息SELECT bo.* FROM boys bo WHEREbo.`id` NOT IN(SELECT boyfriend_id FROM beauty); 5）子查询经典案例祥讲1234567891011121314151617181920212223242526272829303132331.查询工资最低的员工信息：last_name,salarySELECT last_name,salary FROM employeesWHERE salary=(SELECT MIN(salary) FROM employees);2.查询平均工资最低的部门信息SELECT * FROM departments WHERE department_id=(SELECT department_id FROM employees GROUP BY department_id ORDER BY AVG(salary)LIMIT 1)3.查询平均工资最低的部门信息和该部门的平均工资SELECT d.*,a1.ag FROM departments d JOIN (SELECT AVG(salary) ag,department_id FROM employees GROUP BY department_id ORDER BY AVG(salary)LIMIT 1) a1ON d.department_id=a1.department_id4.查询平均工资最高的job信息SELECT j.* FROM jobs j WHERE j.job_id=(SELECT job_id FROM employeesGROUP BY job_id ORDER BY AVG(salary) DESC LIMIT 1)5.查询平均工资高于公司平均工资的部门有哪些SELECT department_id FROM (SELECT department_id ,AVG(salary) AS avg1 FROM employees GROUP BY department_id) e1WHERE e1.avg1&gt;(SELECT AVG(salary) AS avg2 FROM employees) 6.查询出公司中所有manager的详细信息SELECT * FROM employeesWHERE employee_id IN(SELECT DISTINCT manager_id FROM employees);7.各个部门中，最高工资中最低的那个部门的最低工资是多少SELECT MIN(salary) FROM employees GROUP BY department_idHAVING department_id=(SELECT department_id FROM employees GROUP BY department_id ORDER BY MAX(salary)LIMIT 1)8.查询平均工资最高的部门的manager的详细信息：last_name,department_id,email,salarySELECT last_name,department_id,email,salary FROM employees WHERE employee_id=(SELECT manager_id FROM departments WHERE department_id=(SELECT department_id FROM employees GROUP BY department_id ORDER BY AVG(salary)DESC LIMIT 1)) 8.分页查询**语法：limit(currentPage-1)size,size 123456789#查询前五条员工信息SELECT * FROM employees LIMIT 0,5;#查询第11-25条员工信息SELECT * FROM employees LIMIT 10,15;#查询有奖金的员工，并且工资最高的前十名显示出来SELECT * FROM employeesWHERE commission_pct IS NOT NULLORDER BY salary DESC LIMIT 0 ,10; 9.联合查询要查询的结果来自于多个表，且多个表没有直接的连接关系，单查询的信息一致时特点：1.要求多条查询语句的查询列数是一致的2.要求多条查询语句的查询的每一列的类型和顺序最好一致3.union关键字默认去重，如果使用union all 可以不去除重复项 1234案例：查询员工部门编号大于90或邮箱包含a的员工信息SELECT * FROM employees WHERE department_id&gt;90UNIONSELECT * FROM employees WHERE email LIKE &#x27;%a%&#x27;; 三，DML数据操作语言插入insert 1234567891011121314151617一：插入语句#插入beauty一行数据INSERT INTO beauty(NAME,sex,borndate,phone,photo,boyfriend_id)VALUES(&#x27;波多野吉依&#x27;,&#x27;女&#x27;,&#x27;1998-11-11&#x27;,&#x27;13342969497&#x27;,NULL,10)#可以为null的列如何不插入值直接写null，或列名少写一列INSERT INTO beauty(NAME,sex,borndate,phone,photo,boyfriend_id)VALUES(&#x27;小泽玛利亚&#x27;,&#x27;女&#x27;,&#x27;1999-11-11&#x27;,&#x27;13342456497&#x27;,NULL,11)INSERT INTO beauty VALUES(15,&#x27;马蓉&#x27;,&#x27;女&#x27;,&#x27;1989-11-11&#x27;,&#x27;13342456123&#x27;,NULL,12);INSERT INTO beauty SET id=16,NAME=&#x27;刘亦菲&#x27;, sex=&#x27;女&#x27;,borndate=&#x27;1989-10-01&#x27;,phone=&#x27;15945231056&#x27;,boyfriend_id=16;#insert 嵌套子查询，将一个表的数据插入另一张表INSERT INTO beauty (NAME,sex,borndate,phone,boyfriend_id)SELECT &#x27;妲己&#x27;,&#x27;女&#x27;,&#x27;1111-11-11&#x27;,&#x27;13146587954&#x27;,0; 修改update 1234567891011121314151617181920二，修改 UPDATE beauty SET phone=&#x27;110&#x27; WHERE id=16;多表修改：sql99UPDATE 表1 别名INNER|LEFT|RIGHT JOIN 表2 别名ON 连接条件SET 列=值WHERE 筛选条件#修改张无忌的女朋友手机号为114UPDATE beauty gINNER JOIN boys bON g.boyfriend_id=b.idSET g.phone=&#x27;114&#x27;WHERE b.boyName=&#x27;张无忌&#x27;;#修改没有男朋友的女生的男朋友编号都为4号UPDATE beauty gLEFT JOIN boys bON g.`boyfriend_id`=b.idSET g.`boyfriend_id`=4WHERE b.id=NULL; 删除delete 1234567891011121314151617181920212223三，删除DELETE 和 TRUNCATE 的区别：1.delete可以加where条件，truncate不行2.truncate删除效率高3.加入要删除的表中有自增列，用delete删除整个表后在插入数据，从断点处开始插入用truncate删除后在插入数据，从1开始。4.truncate删除没有返回值，delete有返回值5.truncate删除不能回滚，delete删除可以回滚DELETE FROM beauty WHERE id=17;语法：truncate TABLE 表名;#删除张无忌的女朋友的信息DELETE g FROM beauty gINNER JOIN boys bON g.boyfriend_id=b.idWHERE b.id=1;#删除黄晓明以及他女朋友的信息DELETE b,g FROM beauty gINNER JOIN boys bON b.`id`=g.`boyfriend_id`WHERE b.`boyName`=&#x27;黄晓明&#x27;;多表删除 :TRUNCATETRUNCATE TABLE boys 四，DDL数据定义语言1.库和表的管理12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849一，库的管理创建 CREATECREATE DATABASE IF NOT EXISTS mydb1 ;修改 ALTER1.更改字符集ALTER DATABASE mydb1 CHARACTER SET utf8;删除 DROPDROP DATABASE IF EXISTS school;二，表的管理创建 CREATECREATE TABLE book(id INT PRIMARY KEY,b_name VARCHAR(30),price DOUBLE,author_id INT ,publishDate DATE);DESC book ;CREATE TABLE author(id INT PRIMARY KEY ,au_name VARCHAR(20),nation VARCHAR(10));DESC author;修改 ALTER1.修改列名ALTER TABLE book CHANGE COLUMN publishDate pub_date DATETIME;2.修改列的类型或约束ALTER TABLE book MODIFY COLUMN pub_date DATE;3.添加新列ALTER TABLE author ADD COLUMN annual DOUBLE;4.删除新列ALTER TABLE author DROP COLUMN annual;5.修改表名ALTER TABLE author RENAME TO book_author;删除 DROPDROP TABLE IF EXISTS my_employee;SHOW TABLES;复制1.仅仅复制表的结构CREATE TABLE copy LIKE book_author;2.复制表的结构加数据CREATE TABLE copy2SELECT * FROM book_author;3.复制部分结构CREATE TABLE copy3 SELECT id,au_nameFROM book_authorWHERE id=0; 2.数据类型数值型1.整型 TINYINT SMALLINT MEDIUMINT INT/INTEGER BIGINT 1 2 3 4 8 12345如何设置无符号和有符号(默认有符号)DROP TABLE tab_int;CREATE TABLE tab_int(t1 INT,t2 INT UNSIGNED);INSERT INTO tab_int(t1,t2) VALUES(-1,1);DESC tab_int; 1）如果插入的数值超出了整形的范围，会报out of range异常，并且插入临界值。2）如果不设置长度，会有默认的长度。长度代表了显示的最大宽度，如果不够会用0在左边填充，但必须搭配zerofill使用。2.小数①定点数dec（M,D）②浮点数float（4） ，double（8）M，D的意思：M指定一共多少位，D指定小数几位，超出会四舍五入。MD都可以省略，如果是dec，则M默认为10，D默认为0如果是浮点数，则会根据插入数值的精度改变精度定点型精度相对较高。3.字符型①较短的文本CHAR(M)默认为1,VARCHAR(M)M:字符数char：固定长度字符，比较耗费空间，但是效率高。varchar：可变长度字符 123456789ENUM 枚举类CREATE TABLE tab_char( t1 ENUM(&#x27;a&#x27;,&#x27;c&#x27;,&#x27;b&#x27;));SET 集合CREATE TABLE tab_set(s1 SET(&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;,&#x27;d&#x27;));INSERT INTO tab_set(s1) VALUES(&#x27;a,b&#x27;); BINARY:保存较短的二进制。②较长的文本text（文本）,BLOB(较大的二进制)4.日期型DATE:日期DATETIME:日期加时间，8字节timestamp：跟时区有关系，建议使用，4字节time：时间year：年 123456789CREATE TABLE tab_date(t1 DATETIME,t2 TIMESTAMP);INSERT INTO tab_date(t1,t2)VALUES(NOW(),NOW());SELECT * FROM tab_date;SET time_zone=&#x27;+9:00&#x27;;#设置时区为东9区 3.常见约束含义：一种限制，用于限制表中的数据，保证数据的一致性。 NOT NULL DEFAULT PRIMARY KEY 唯一，且不为空 UNIQUE 唯一，可以为空 CHECK Mysql不支持 FOREIGN KEY 外键约束，用于限制两个表的关系，用于保证该字段的值必须来自于主表的关联列的值。约束的分类：列级约束：除外键约束表级约束：除了非空，默认。CREATE TABLE 表名(字段1 字段类型 列级约束,字段2 字段类型 列级约束,表级约束); 1234567891011121314151617181920212223242526#创建表时添加列级约束DROP TABLE tab_test;CREATE TABLE tab_test(id INT PRIMARY KEY,stu_name VARCHAR(20) NOT NULL,gender CHAR DEFAULT &#x27;男&#x27;,seat_id INT UNIQUE, major_id INT REFERENCES tab_major(id) );CREATE TABLE tab_major(id INT PRIMARY KEY ,major_name VARCHAR(20) NOT NULL);DESC tab_test;SHOW INDEX FROM tab_test;#查看索引信息#添加表级约束CREATE TABLE tab_test(id INT PRIMARY KEY AUTO_INCREMENT,stu_name VARCHAR(20) NOT NULL,gender CHAR DEFAULT &#x27;男&#x27;,seat_id INT UNIQUE, major_id INT ,CONSTRAINT m_id FOREIGN KEY(major_id) REFERENCES tab_major(id) );CONSTRAINT m_id 可以省略 面试题：主键约束和唯一约束的区别：都可以保证唯一性，主键不能为空 ，unique 能为空，但是只能有一个null。主键只能有1个，unique可以有多个。都允许两个列组合成一个约束。面试题：外键：要求在从表设置外键关系从表的外键列类型和主表的关联列类型一致，名称无要求要求主表的关联列必须是主键或者唯一键插入数据应该先插入主表再插入从表删除数据应该先删除从表，在删除主表二，修改表时添加约束 1234567891011CREATE TABLE tab_test2(id INT ,stu_name VARCHAR(20) ,gender CHAR ,seat_id INT , major_id INT );ALTER TABLE tab_test2 MODIFY COLUMN stu_name VARCHAR(20) NOT NULL ;ALTER TABLE tab_test2 MODIFY COLUMN id INT PRIMARY KEY AUTO_INCREMENT;#添加外键ALTER TABLE tab_test2 ADD FOREIGN KEY(major_id) REFERENCES tab_major(id); 4.标识列自增长列 AUTO_INCREMENT特点：1.表示必须和一个key搭配2.一个表最多一个标识列3.标识列类型只能是数值型4.标识列可以通过set auto_increment_increment=3;设置步长 1234CREATE tab_auto(id INT PRIMARY KEY AUTO_INCREMENT,NAME VARCHAR(20) NOT NULL); 五，TCL语言：事务控制语言事务：一个或一组sql语句组成的执行单元， 要么全部执行,要么都不执行。存储引擎:在MySQL中的数据用各种不同的技术存储在文件中。通过show ENGINES;来查看mysql支持的存储引擎。innodb引擎支持事务。事务的ACID属性：1.原子性:事务是一个不可分割的工作单位，要么都发生，要么都不发生。2.一致性：事务必须使数据库从一个一致性状态变为另一个一致性状态。3.隔离性：一个事务的执行不能被另一个事务干扰。4.持久性：事务一旦被提交，对数据库事务的改变就是永久性的。 DELETE 和 TRUNCATE 在事务中的区别： 1234567891011演示deleteSET autocommit=0;START TRANSACTION;DELETE FROM tab_teacher;ROLLBACK;演示 TRUNCATESET autocommit=0;START TRANSACTION;TRUNCATE TABLE tab_teacher;ROLLBACK;DELETE 是直接删除表中数据，truncate是江表删除，创建一张与原来一样的空表。 六，视图含义：虚拟表，和普通表格一样使用通过表动态生成的数据 1.创建视图语法：CREATE VIEW 视图名AS查询语句 ; 12345678910111213141516171819202122# 案例：查询姓名中包含a字符的员工名，部门名和工种信息create view view1 as select e.last_name,d.department_name ,j.job_title from employees einner join departments d on e.department_id = d.department_id inner join jobs j on e.job_id = j.job_idwhere e.last_name like &#x27;%a%&#x27;;select * from view1;# 案例：查询各个部门的平均工资级别create view view2 asselect j.grade_level ,aa.department_id from job_grades jinner join (select avg(salary) avg_s,department_id from employees group by department_id) aa on aa.avg_s between j.lowest_sal and j.highest_sal;select * from view2;# 案例：查询平均工资最低的部门信息create view view3 asselect avg(salary) avg_s ,department_idfrom employeesgroup by department_idorder by avg_s asclimit 1;select * from view3; 2.视图修改①create OR REPLACE VIEW 视图名 AS 查询语句;②alter VIEW 视图名 AS 查询语句; 3.删除视图DROP VIEW v1,v2; 4.查看视图DESC v1; 12345678910#创建视图emp_v1，要求查询电话号码以011开头的员工姓名和工资，邮箱CREATE VIEW emp_v1 ASSELECT last_name ,salary,email FROM employees WHEREphone_number LIKE &#x27;%011&#x27;;#创建视图emp_v2,要求查询部门的最高工资高于12000的部门信息CREATE VIEW v4 ASSELECT department_id FROM employees GROUP BY department_idHAVING MAX(salary)&gt; 12000;CREATE VIEW emp_v2 ASSELECT * FROM departments WHERE department_id IN(SELECT * FROM v4); 5.视图的更新视图的可更新性和视图中查询的定义有关，以下类型的视图是不能更新的。1.包含以下关键字的sql语句：分组函数，distinct，group by，having，union2.常量视图3.select中包含子查询的4.join5.from 一个不能更新的视图6.where子句的子查询引用了from子句的表 6.视图和表的对比： 创建语法的关键字 是否实际占用物理空间 使用 视图 CREATE VIEW 只是保存了sql逻辑 增删改查，一般不能增删改 表 CREATE TABLE 占用 增删改查 七，变量系统变量 ：变量由系统提供，不是用户自定义，属于服务器层面。查看系统所有变量：show GLOBAL VARIABLES;查看满足条件的部分系统变量： SHOW GLOBAL VARIABLES LIKE ‘%char%’;查看指定的某个系统变量的值： SELECT @@global.autocommit;为某个系统变量赋值：set @@global.系统变量名=值;全局变量:GLOBAL作用域：服务器每次启动将为所有的全局变量赋初始值，针对于所有的会话有效，但不能跨重启。会话变量:SESSION作用域：针对当前的会话有效。用户自定义变量用户变量声明： SET/SELECT @用户变量名 :=值;赋值：通过 SELECT 字段 INTO 变量名;或 SET/SELECT @用户变量名 :=值;使用：select @用户变量名;应用在任何地方。作用域：针对当前会话和连接有效。局部变量作用域：作用在定义它的begin END 块中。声明： DECLARE 变量名 类型 （default 值）;赋值：通过 SELECT 字段 INTO 变量名;或 SET/SELECT @变量名 :=值;使用：select @变量名;只能放在begin END 中的第一句话 八，存储过程和函数存储过程：一组预先定义好的sql语句集合，理解成批处理语句。1.提高代码的重用性2.简化操作3.减少了编译次数并且减少了和数据库服务器的连接次数，提高了效率。 1.创建语法：CREATE PROCEDURE 存储过程名（参数列表）BEGIN一组合法的sql语句;END参数列表：参数模式 参数名 参数类型 参数模式：in：该参数可以作为输入，也就是该参数需要调用方传入值OUT ：该参数可以作为输出，也就是该参数可以作为返回值inout：该参数既可以作为输入又可以作为输出 如果存储过程只有一句话，begin END 可以省略 存储过程体中的每条sql语句的结尾需要必须加分号，存储过程的结尾可以使用 DELIMITER 重新设置。 2.调用CALL 存储过程名（实参列表）; 3.案例1234567891011121314151617181920212223242526272829303132333435363738394041424344#插入到admin表中五条记录DELIMITER $CREATE PROCEDURE my_a()BEGININSERT INTO admin(username,PASSWORD) VALUES(&#x27;yin&#x27;,&#x27;666&#x27;);INSERT INTO admin(username,PASSWORD) VALUES(&#x27;aa&#x27;,&#x27;123&#x27;);INSERT INTO admin(username,PASSWORD) VALUES(&#x27;bb&#x27;,&#x27;666&#x27;);INSERT INTO admin(username,PASSWORD) VALUES(&#x27;cc&#x27;,&#x27;123&#x27;);INSERT INTO admin(username,PASSWORD) VALUES(&#x27;dd&#x27;,&#x27;666&#x27;);END $#创建存储过程实现 根据女生名查询对应的男生信息DELIMITER $CREATE PROCEDURE my_b(IN beauty_name VARCHAR(20))BEGIN SELECT bo.* FROM boys bo RIGHT JOIN beauty b ON bo.id=b.boyfriend_id WHERE b.name=beauty_name;END $CALL my_b(&#x27;热巴&#x27;);#根据女生名返回他的男朋友名DELIMITER $CREATE PROCEDURE my_d(IN beautyName VARCHAR(20),OUT boyName VARCHAR(20))BEGIN SELECT bo.boyName INTO boyName FROM boys bo INNER JOIN beauty b ON bo.id=b.boyfriend_id WHERE b.name=beautyName;END $CALL my_d(&#x27;小昭&#x27;,@b_name);SELECT @b_name;#传入两个值a，b，最终翻倍返回a和bDELIMITER $CREATE PROCEDURE my_e(INOUT a INT ,INOUT b INT )BEGIN SET a=a*2; SET b=b*2;END $SET @m=10;SET @n=20;CALL my_e(@m,@n);SELECT @m,@n; 4.删除存储过程12DROP PROCEDURE 存储过程名DROP PROCEDURE my_a; 5.查看存储过程的信息1SHOW CREATE PROCEDURE my_b; 函数存储过程可以有0/n个返回值：适合批量增删改函数有且仅有一个返回值：适合查询 1.创建12345DELIMITER $CREATE FUNCTION 函数名(参数列表) RETURNS 返回类型BEGINEND 注意：参数列表：参数名，参数类型一定会有return语句 2.使用SELECT 函数名(参数列表) 12345678910111213141516171819#返回公司员工个数DELIMITER $CREATE FUNCTION my_f1() RETURNS INTBEGINDECLARE c INT DEFAULT 0 ; SELECT COUNT(*) INTO c FROM employees; RETURN c;END $SELECT my_f1();#根据员工名返回他的工资DELIMITER $CREATE FUNCTION my_f2(NAME VARCHAR(20)) RETURNS DOUBLEBEGIN DECLARE c DOUBLE; SELECT salary INTO c FROM employees WHERE last_name=NAME; RETURN c;END $SET @a=&#x27;Hunold&#x27;;SELECT my_f2(@a); 3.查看1SHOW CREATE FUNCTION my_f2; 4.删除1DROP FUNCTION my_f2; 九，流程控制分支结构1.if （表达式1，表达式2，表达式3）如果表达式1成立，就返回表达式2的值，否则返回表达式3的值。应用在任何地方 2.case1)switch-CASE语法:CASE 要判断的字段或者表达式WHEN 常量1 THEN 要显示的值1或者语句1WHEN 常量2 THEN 要显示的值2或者语句2…ELSE 要显示的值n或者语句n； 1234567891011121314151617181920案例：查询员工的工资，要求部门号==30，显示的工资为1.1倍，部门号==40，显示的工资为1.2倍，部门号==50，显示的工资为1.3倍，其他部门，显示原有工资。SELECT salary AS 原始工资, department_id , CASE department_id WHEN 30 THEN salary * 1.1 WHEN 40 THEN salary * 1.2 WHEN 50 THEN salary * 1.3 ELSE salary END AS 新工资 FROM employees ; 2)CASE 使用2：语法：CASEWHEN 条件1 THEN 要显示的值1或语句1WHEN 条件2 THEN 要显示的值2或语句2…ELSE 要显示的值n或语句nEND 12345678910111213141516171819202122232425262728293031323334案例：查询员工的工资情况如果&gt;2w，显示A如果&gt;1.5w，显示B如果&gt;1w，显示C否则，显示DSELECT salary, CASE WHEN salary &gt; 20000 THEN &#x27;A&#x27; WHEN salary &gt; 15000 THEN &#x27;B&#x27; WHEN salary &gt; 10000 THEN &#x27;C&#x27; ELSE &#x27;D&#x27; END AS 工资等级 FROM employees 可以放在任何地方 #创建存储过程，根据传入的成绩，显示等级，90A,80B，70C，60D ，F DELIMITER $ CREATE PROCEDURE my_1(IN score INT) BEGIN CASE WHEN score BETWEEN 90 AND 100 THEN SELECT &#x27;A&#x27;; WHEN score BETWEEN 80 AND 90 THEN SELECT &#x27;B&#x27;; WHEN score BETWEEN 70 AND 80 THEN SELECT &#x27;C&#x27;; WHEN score BETWEEN 70 AND 60 THEN SELECT &#x27;D&#x27;; ELSE SELECT &#x27;E&#x27;; END CASE; END $CALL my_1(95); 3.if语法：IF 条件1 THEN 语句1;ELSEIF 条件2 THEN 语句2;…ELSE 语句n;END IF;只能用在begin end中 12345678910111213#创建存储过程，根据传入的成绩，返回等级，90A,80B，70C，60D ，FDELIMITER $ CREATE FUNCTION my_2( score INT) RETURNS CHAR BEGIN IF score &gt;=90 THEN RETURN&#x27;A&#x27;; ELSEIF score &gt;=80 THEN RETURN&#x27;B&#x27;; ELSEIF score &gt;=70 THEN RETURN&#x27;C&#x27;; ELSEIF score &gt;=60 THEN RETURN&#x27;D&#x27;; ELSE RETURN&#x27;E&#x27;; END IF; END $ SELECT my_2(85); 循环结构在存储过程或函数里面使用 1.while语法：标签:WHILE 循环条件 DO循环体;END WHILE 标签;循环控制和标签搭配使用 2.loop语法：标签： LOOP循环体;END LOOP 标签; 3.repeat语法：标签： REPEAT循环体;UNTIL 结束循环的条件END REPEAT 标签; 循环控制ITERATE 类似continueLEAVE 类似break left join==left outer join a left join b 就是取a和b的交集加a剩下的部分 inner join a inner join b就是取交集","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/tags/MySQL/"}]}],"categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/categories/MySQL/"},{"name":"领域驱动设计","slug":"领域驱动设计","permalink":"https://yinhuidong.github.io/categories/%E9%A2%86%E5%9F%9F%E9%A9%B1%E5%8A%A8%E8%AE%BE%E8%AE%A1/"},{"name":"云计算","slug":"云计算","permalink":"https://yinhuidong.github.io/categories/%E4%BA%91%E8%AE%A1%E7%AE%97/"},{"name":"MongoDB","slug":"MongoDB","permalink":"https://yinhuidong.github.io/categories/MongoDB/"},{"name":"JVM","slug":"JVM","permalink":"https://yinhuidong.github.io/categories/JVM/"},{"name":"JAVA基础","slug":"JAVA基础","permalink":"https://yinhuidong.github.io/categories/JAVA%E5%9F%BA%E7%A1%80/"},{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"https://yinhuidong.github.io/categories/Elasticsearch/"},{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"ClickHouse","slug":"ClickHouse","permalink":"https://yinhuidong.github.io/categories/ClickHouse/"},{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"},{"name":"消息队列","slug":"消息队列","permalink":"https://yinhuidong.github.io/categories/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"name":"MyBatis","slug":"MyBatis","permalink":"https://yinhuidong.github.io/categories/MyBatis/"},{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/categories/Spring/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/tags/MySQL/"},{"name":"DDD","slug":"DDD","permalink":"https://yinhuidong.github.io/tags/DDD/"},{"name":"云计算","slug":"云计算","permalink":"https://yinhuidong.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"},{"name":"MongoDB","slug":"MongoDB","permalink":"https://yinhuidong.github.io/tags/MongoDB/"},{"name":"JVM","slug":"JVM","permalink":"https://yinhuidong.github.io/tags/JVM/"},{"name":"JAVA基础","slug":"JAVA基础","permalink":"https://yinhuidong.github.io/tags/JAVA%E5%9F%BA%E7%A1%80/"},{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"https://yinhuidong.github.io/tags/Elasticsearch/"},{"name":"软件架构","slug":"软件架构","permalink":"https://yinhuidong.github.io/tags/%E8%BD%AF%E4%BB%B6%E6%9E%B6%E6%9E%84/"},{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"ClickHouse","slug":"ClickHouse","permalink":"https://yinhuidong.github.io/tags/ClickHouse/"},{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"},{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"https://yinhuidong.github.io/tags/RabbitMQ/"},{"name":"MyBatis","slug":"MyBatis","permalink":"https://yinhuidong.github.io/tags/MyBatis/"},{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/tags/Spring/"}]}