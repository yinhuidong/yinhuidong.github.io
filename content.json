{"meta":{"title":"二十","subtitle":"卷就完了","description":"欢迎来卷","author":"二十","url":"https://yinhuidong.github.io","root":"/"},"pages":[{"title":"分类","date":"2022-01-10T12:46:21.418Z","updated":"2021-12-27T12:21:34.000Z","comments":true,"path":"categories/index.html","permalink":"https://yinhuidong.github.io/categories/index.html","excerpt":"","text":""},{"title":"","date":"2022-01-10T12:46:21.418Z","updated":"2021-12-27T12:21:35.000Z","comments":true,"path":"js/chocolate.js","permalink":"https://yinhuidong.github.io/js/chocolate.js","excerpt":"","text":"/* * @Author: tzy1997 * @Date: 2020-12-15 20:55:25 * @LastEditors: tzy1997 * @LastEditTime: 2021-01-12 19:02:25 */ // 友情链接页面 头像找不到时 替换图片 if (location.href.indexOf(\"link\") !== -1) { var imgObj = document.getElementsByTagName(\"img\"); for (i = 0; i < imgObj.length; i++) { imgObj[i].onerror = function() { this.src = \"https://cdn.jsdelivr.net/gh/tzy13755126023/BLOG_SOURCE/theme_f/friend_404.gif\" } } } $(function() { // 气泡 function bubble() { $('#page-header').circleMagic({ radius: 10, density: .2, color: 'rgba(255,255,255,.4)', clearOffset: 0.99 }); }! function(p) { p.fn.circleMagic = function(t) { var o, a, n, r, e = !0, i = [], d = p.extend({ color: \"rgba(255,0,0,.5)\", radius: 10, density: .3, clearOffset: .2 }, t), l = this[0]; function c() { e = !(document.body.scrollTop > a) } function s() { o = l.clientWidth, a = l.clientHeight, l.height = a + \"px\", n.width = o, n.height = a } function h() { if (e) for (var t in r.clearRect(0, 0, o, a), i) i[t].draw(); requestAnimationFrame(h) } function f() { var t = this; function e() { t.pos.x = Math.random() * o, t.pos.y = a + 100 * Math.random(), t.alpha = .1 + Math.random() * d.clearOffset, t.scale = .1 + .3 * Math.random(), t.speed = Math.random(), \"random\" === d.color ? t.color = \"rgba(\" + Math.floor(255 * Math.random()) + \", \" + Math.floor(0 * Math.random()) + \", \" + Math.floor(0 * Math.random()) + \", \" + Math.random().toPrecision(2) + \")\" : t.color = d.color } t.pos = {}, e(), this.draw = function() { t.alpha"},{"title":"留言板","date":"2022-01-10T12:46:21.418Z","updated":"2021-12-27T12:21:35.000Z","comments":true,"path":"message/index.html","permalink":"https://yinhuidong.github.io/message/index.html","excerpt":"","text":"本页面还在开发中……"},{"title":"标签","date":"2022-01-10T12:46:21.418Z","updated":"2021-12-27T12:21:35.000Z","comments":true,"path":"tags/index.html","permalink":"https://yinhuidong.github.io/tags/index.html","excerpt":"","text":""},{"title":"关于我","date":"2022-01-11T02:40:01.624Z","updated":"2022-01-11T02:40:01.624Z","comments":true,"path":"关于我/index.html","permalink":"https://yinhuidong.github.io/%E5%85%B3%E4%BA%8E%E6%88%91/index.html","excerpt":"","text":"关于我十年生死两茫茫,写程序，到天亮。千行代码，Bug何处藏。纵使上线又怎样，朝令改，夕断肠。领导每天新想法，天天改，日日忙。 相顾无言，惟有泪千行。每晚灯火阑珊处，程序员，又加班，工作狂~ 🤣🤣🤣 基本信息 类别 信息 出生年月 1998年11月 现居地 北京市 籍贯 大庆市 邮箱 &#49;&#x39;&#x37;&#50;&#x30;&#51;&#x39;&#x37;&#x37;&#x33;&#64;&#x71;&#x71;&#46;&#99;&#111;&#109; 教育经历 时间 学校 专业 备注 2017.09~2021.07 齐齐哈尔大学 计算机科学与技术 统招本科 工作经历 时间 公司 职位 2021.06~至今 mi Java 开发工程师 专业技能 Java 基础扎实、掌握 JVM 原理、多线程、网络原理、设计模式、常用的数据结构和算法 熟悉 Windows、Mac、Linux 操作系统，熟练使用 linux 常用操作指令 熟练使用 IntelliJ IDEA 开发工具(及各种插件)、熟练使用 Git 版本同步工具 阅读过 Spring、SpringMVC、等开源框架源码，理解其设计原理及底层架构，具备框架定制开发能力 理解 Redis 线程模型，Netty 线程模型，掌握基于响应式的异步非阻塞模型的基本原理，了解 Webflux 熟练掌握分布式缓存 Redis、Elaticsearch，对分布式锁，幂等等常见问题有深入研究及多年实战经验 熟悉常见消息中间件的使用，有多年 RabbitMQ 的实战开发经验，对高级消息队列有深入理解 熟练掌握 Mysql 事务，索引，锁，SQL 优化相关知识，可根据业务场景给出详细及高性能设计方案 熟练使用数据库操作框架 Mybatis、Mybatsi-plus 进行高效业务功能开发 掌握 springCloud 相关框架，对 SpringBoot、SpringCloud 原理有一定了解，有成熟项目经验 熟悉定时任务及延迟任务等业务相关设计，如 xxl-job，延迟消息等相关技术有多年开发经验 熟悉微服务思想，MVC 分层，DDD 理论，服务拆分，治理，监控，服务熔断，降级等相关能力 熟悉 jvm 原理，熟悉垃圾回收以 jvm 性能调优技术，有过线上服务器性能监测及调优经验 熟悉多线程及线程池使用，有多年多线程业务处理经验，封装过多线程批处理工具类等公用组建 了解 Mysql 分库分表相关原理，如 Sardingsphere、Mycat 等框架有相关使用经验 了解操作系统底层原理以及 C、C++程序开发，对计算机底层原理有初步了解 了解前端开发，了解 html，css，js，vue 等前端技术，对前端开发有一定的了解 研究过单片机等硬件开发，喜欢科技产品，喜欢软件，喜欢折腾各种电子产品以及软件 项目经验​ 自我评价NB。"},{"title":"","date":"2022-01-10T12:46:21.415Z","updated":"2021-12-27T12:21:34.000Z","comments":true,"path":"css/custom.css","permalink":"https://yinhuidong.github.io/css/custom.css","excerpt":"","text":"/* 文章页H1-H6图标样式效果 */ h1::before, h2::before, h3::before, h4::before, h5::before, h6::before { -webkit-animation: ccc 1.6s linear infinite ; animation: ccc 1.6s linear infinite ; } @-webkit-keyframes ccc { 0% { -webkit-transform: rotate(0deg); transform: rotate(0deg) } to { -webkit-transform: rotate(-1turn); transform: rotate(-1turn) } } @keyframes ccc { 0% { -webkit-transform: rotate(0deg); transform: rotate(0deg) } to { -webkit-transform: rotate(-1turn); transform: rotate(-1turn) } } #content-inner.layout h1::before { color: #ef50a8 ; margin-left: -1.55rem; font-size: 1.3rem; margin-top: -0.23rem; } #content-inner.layout h2::before { color: #fb7061 ; margin-left: -1.35rem; font-size: 1.1rem; margin-top: -0.12rem; } #content-inner.layout h3::before { color: #ffbf00 ; margin-left: -1.22rem; font-size: 0.95rem; margin-top: -0.09rem; } #content-inner.layout h4::before { color: #a9e000 ; margin-left: -1.05rem; font-size: 0.8rem; margin-top: -0.09rem; } #content-inner.layout h5::before { color: #57c850 ; margin-left: -0.9rem; font-size: 0.7rem; margin-top: 0.0rem; } #content-inner.layout h6::before { color: #5ec1e0 ; margin-left: -0.9rem; font-size: 0.66rem; margin-top: 0.0rem; } #content-inner.layout h1:hover, #content-inner.layout h2:hover, #content-inner.layout h3:hover, #content-inner.layout h4:hover, #content-inner.layout h5:hover, #content-inner.layout h6:hover { color: #49b1f5 ; } #content-inner.layout h1:hover::before, #content-inner.layout h2:hover::before, #content-inner.layout h3:hover::before, #content-inner.layout h4:hover::before, #content-inner.layout h5:hover::before, #content-inner.layout h6:hover::before { color: #49b1f5 ; -webkit-animation: ccc 3.2s linear infinite ; animation: ccc 3.2s linear infinite ; } /* 页面设置icon转动速度调整 */ #rightside_config i.fas.fa-cog.fa-spin { animation: fa-spin 5s linear infinite ; } /*--------更换字体------------*/ @font-face { font-family: 'tzy'; /* 字体名自定义即可 */ src: url('https://cdn.jsdelivr.net/gh/tzy13755126023/BLOG_SOURCE/font/ZhuZiAWan.woff2'); /* 字体文件路径 */ font-display: swap; } body, .gitcalendar { font-family: tzy !important; } .categoryBar-list { max-height: 400px; } .clock-row { overflow: hidden; text-overflow: ellipsis; } /*3s为加载动画的时间，1为加载动画的次数，ease-in-out为动画效果*/ #page-header, #web_bg { -webkit-animation: imgblur 2s 1 ease-in-out; animation: imgblur 2s 1 ease-in-out; } @keyframes imgblur { 0% { filter: blur(5px); } 100% { filter: blur(0px); } } /*适配使用-webkit内核的浏览器 */ @-webkit-keyframes imgblur { 0% { -webkit-filter: blur(5px); } 100% { -webkit-filter: blur(0px); } } .table-wrap img { margin: .6rem auto .1rem !important; } /* 标签外挂 网站卡片 start */ .site-card-group img { margin: 0 auto .1rem !important; } .site-card-group .info a img { margin-right: 10px !important; } [data-theme='dark'] .site-card-group .site-card .info .title { color: #f0f0f0 !important; } [data-theme='dark'] .site-card-group .site-card .info .desc { color: rgba(255, 255, 255, .7) !important; } .site-card-group .info .desc { margin-top: 4px !important; } /* 代码块颜色 */ figure.highlight pre .addition { color: #00bf03 !important; }"},{"title":"技术笔记","date":"2022-01-10T12:46:21.418Z","updated":"2021-12-27T12:21:35.000Z","comments":true,"path":"技术笔记/index.html","permalink":"https://yinhuidong.github.io/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/index.html","excerpt":"","text":"我是技术笔记"}],"posts":[{"title":"MySQL[三]InnoDB索引结构","slug":"MySQL/MySQL[三]InnoDB索引结构","date":"2022-01-11T03:08:18.570Z","updated":"2022-01-11T03:10:34.434Z","comments":true,"path":"2022/01/11/MySQL/MySQL[三]InnoDB索引结构/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/MySQL/MySQL[%E4%B8%89]InnoDB%E7%B4%A2%E5%BC%95%E7%BB%93%E6%9E%84/","excerpt":"","text":"索引其实就是对数据按照某种格式进行存储的文件。就InnoDB来讲，索引文件里面会有很多的基本单元【页】。​ 为什么有页的概念？​ 查询数据的时候直接交互磁盘，效率显然又会很慢，所以真正处理数据的过程其实是在内存中，这样就需要把磁盘的数据加载到内存，如果是写操作，可能还要将内存的数据再次刷新到磁盘。如果内存与磁盘的数据交互过程是基于一条条记录来进行的，显然又会很慢，所以InnoDB采取的方式是将数据划分为若干个页，以页来作为内存和磁盘交互的基本单位，默认大小为16KB。 ​ 数据或者叫记录，其实是以【行】的格式存储在页里面的，可以简单的理解成页里面的一行对应一条记录。​ 当然索引文件里面肯定不光只有页，还会有其余的东西，页里面也不光只有行格式，也会有额外的信息，这个下面我们会详细分析，至此我们仅仅需要明确一下索引的概念和层级关系。 明确了这个层级关系之后，接下来我们来从最基础的行格式来进行分析。 一，行格式我们平时都是以记录为单位向表中插入数据的，这些记录在磁盘上的存储形式被称为行格式或者记录格式，截至目前，一共有4种行格式。分别是 compact redundant dynamic compressed，MySQL5.7默认的行格式为dynamic。 1. 如何指定行格式123CREATE TABLE 表名 (列的信息) ROW_FORMAT=行格式名称 ALTER TABLE 表名 ROW_FORMAT=行格式名称 比如我们创建一张表来指定行格式： 12345678create table record_format( c1 varchar(10), c2 varchar(10) not null, c3 char(10), c4 varchar(10))charset=ascii row_format=compact;INSERT INTO record_format_demo(c1, c2, c3, c4) VALUES(&#x27;aaaa&#x27;, &#x27;bbb&#x27;, &#x27;cc&#x27;, &#x27;d&#x27;), (&#x27;eeee&#x27;, &#x27;fff&#x27;, NULL, NULL); 2.compact 行格式首先我们来看Compact行格式。 一条完整的行格式可以被分为两个部分：记录额外信息的部分&amp;记录真实数据的部分。 2.1 额外的信息额外的信息实包含三部分：变长字段的长度列表，NULL值列表和记录头信息。 2.1.1 变长字段长度列表MySQL支持很多的变长字段，我们就以最经典的varchar来进行举例，变长字段的数据存储多少字节其实是不固定的，所以在存储真实的数据的时候，要记录一下真实数据的字节数，这样的话，一个变长字段列实际上就占用了两部分的空间来存储：【真实数据】&amp;【真实数据占用字节数】。 注意：对于一个列varchar(100)，我们实际上存储一个10字节的数据，当在内存中为这个列的数据分配内存空间的时候，实际上会分配100字节，但是这个列的数据在磁盘上，实际上只会分配10字节。 在Compact行格式中，会把所有的变长字段占用的真实长度全部逆序存储在记录的开头位置，形成一个变长字段长度列表。 比如我们刚才创建的那张表，我们来分析一下： c1,c2,c4三个列都是变长字段，所以这三个列的值的长度其实都需要保存到变长字段长度列表，因为这张表的字符集的ASCII，所以每个字符实际只占用1字节来进行编码： 列名 储存内容 内容长度(十进制表示) 内容长度(十六进制表示) C1 ‘aaaa’ 4 0x04 C2 ‘bbb’ 3 0x03 C4 ‘d’ 1 0x01 因为这些长度是按照逆序来存放的，所以最终变长字段长度列表的字节串用十六进制表示的效果就是【010304】。 因为我们演示的这条记录中，c1,c2,c4列中的字符串都比较短，所以真实的数据占用的字节数就比较小，真实数据的长度用一个字节就可以表示，但是如果变长列的内容占用字节数比较多，可能就需要用2个字节来表示。对此InnoDB的规定是： 【W】：某个字符集中表示一个字符最多需要使用的字节数 【M】：当前列类型最多能存储的字符数(比如varchar(100),M=100),如果换算成字节数就是W*M 【L】：真实占用的字节数 如果M*W&lt;=255,那么使用1字节来表示字符串实际用到的字节数。 InnoDB在读记录的变长字段长度列表的时候会先去查看表结构，判断用几个字节去存储的。 如果M*W&gt;=255,这个时候再次分为两种情况： 如果L&lt;=127，那就用1个字节表示 否则就用2个字节表示 如果某个变长字段允许存储的最大字节数大于255的时候，怎么区分他正在读取的字节是一个单独的字段长度还是半个字段长度呢？ InnoDB用该字节的第一个二进制为作为标志位，0：单独的字段长度，1：半个字段长度。 对于一些占用字节数特别多的字段，单个页都无法存储的时候，InnoDB会把一部分数据放到所谓的溢出页，在变长字段长度列表中只会记录当前页的字段长度，所以用两个字节也可以存的下。 此外，变长字段的长度列表中只存储真实数据值为非NULL的列占用的长度，真实数据为NULL的列的长度是不存储的。 也并不是所有的记录都会有变长字段长度列表，假如表中的列要是没有变长字段，或者记录中的变长字段值都是NULL，那就没有变长字段长度列表了。 2.1.2 NULL值列表如果一条记录有多个字段的真实值为NULL，不统一管理的话就会比较占用空间，所以抽取出来了NULL值列表。 当然如果这个表的所有字段都是NOT NULL约束的，就不会有NULL值列表。 看一下处理过程： 首先统计出表中允许存储NULL的字段 如果表中没有NULL字段的列，那就没必要再往下了，否则将每个允许存储NULL的列对应的一个二进制位按照列的顺序逆序排列。1：NULL，0：不是NULL。 MySQL规定NULL值必须用整数个字节的位表示，如果使用的二进制位个数不是整数个字节，则在字节的高位补0。 以此类推，如果一个表中有9个字段允许为NULL，那么这个记录的NULL值列表部分就需要2个字节来表示。 这个时候再来看我们上面创建的表中的记录。 2.1.3 记录头信息由五个固定的字节组成，换算成二进制就是40位，每一部分代表不同的信息。 名称 大小(bit) 描述 预留位1 1 没有使用 预留位2 1 没有使用 delete_mask 1 标记该记录是否被删除 min_rec_mask 1 B+树的每层非叶子节点中的最小记录都会添加该标记 n_owned 4 表示当前记录拥有的记录数 heap_no 13 表示当前记录在记录堆的位置信息 record_type 3 表示当前记录的类型 0 ：普通记录，1：B+树非页节点记录，2：最小记录，3：最大记录 next_record 16 下一条记录的相对位置 接下来来看记录的真实数据。 2.2 真实数据除了表中显式定义的列，MySQL会往我们的表中放一些隐藏列。 列名 是否必须 占用空间 描述 row_id 否 6字节 行ID，唯一标识一条记录 transaction_id 是 6字节 事务ID roll_pointer 是 7字节 回滚指针 【row_id】：这个玩意，跟主键的选择有关，如果我们显式定义了表的主键，就不会有它，如果我们没显式定义主键，那么会去选择一个unique的列作为主键，如果unique的列也没有，那么就会生成一个row_id列作为隐藏的主键。 【transaction_id】&amp;【roll_pointer】和一致性非锁读(MVCC)有关,后面遇到的时候我会在分析介绍。 在完善下我们开头创建的那张表的记录形象。 至此，其实就剩下我们显式插入数据库的真实记录了，但是还有一个特殊的类型需要说明一下。 2.2.1 CHAR 也是变长的？在Compact行格式下只会把变长类型的列的长度逆序记录到变长字段长度列表，但是这其实和我们的字符集有关系，上面我们创建的表显式指定为ASCII字符集，这个时候一个字符只会用一个字节表示，但是假如我们指定的是其它字符集，比如utf8，这个时候一个字符用几个字节表示就不确定了，所以CHAR列的真实字节长度也会被记录到变长字段长度列表。 另外，变长字符集的CHAR(M)类型的列要求至少占用M个字节，而VARCHAR(M)就没有这个要求。 对于使用utf8字符集的CHAR(10)的列来说，该列存储的数据字节长度的范围是10～30个字节。即使我们向该列中存储一个空字符串也会占用10个字节，这是怕将来更新该列的值的字节长度大于原有值的字节长度而小于10个字节时，可以在该记录处直接更新，而不是在存储空间中重新分配一个新的记录空间，导致原有的记录空间成为所谓的碎片。 3. 行溢出上面提到了，如果一条记录的真实字节数太大，就会导致行溢出，把超出的一部分数据存储到其他行或者页。 3.1 varchar(M)最多能存储的数据varchar(M)的列最多可以占用65535个字节。其中M代表该类型最多存储的字符数量。 实际上，MySQL对一条记录占用的最大存储空间是有限制的，除了BLOB，TEXT类型的列之外，其他所有的列(不包含隐藏列和记录头信息)占用的字节长度加起来不能超过65535个字节。这个65535个字节除了列本身的数据之外，还包括一些其他的数据，比如说我们为了存储一个varchar列，其实还需要占用3部分空间。 真实数据 真实数据占用的字节长度 NULL值标识，如果该列有NOT_NULL属性则可以没有这部分存储空间 如果该varchar类型的列没有NOT NULL属性那最多只能存储65532个字节的数据，因为真实数据的长度可能占用2个字节，NULL值标识需要占用1个字节。 如果VARCHAR类型的列有NOT NULL属性，那最多只能存储65533个字节的数据，因为真实数据的长度可能占用2个字节，不需要NULL值标识。 如果VARCHAR(M)类型的列使用的不是ascii字符集，那会怎么样呢？ 如果VARCHAR(M)类型的列使用的不是ascii字符集，那M的最大取值取决于该字符集表示一个字符最多需要的字节数。在列的值允许为NULL的情况下，gbk字符集表示一个字符最多需要2个字节，那在该字符集下，M的最大取值就是32766（也就是：65532/2），也就是说最多能存储32766个字符；utf8字符集表示一个字符最多需要3个字节，那在该字符集下，M的最大取值就是21844，就是说最多能存储21844（也就是：65532/3）个字符。 上述所言在列的值允许为NULL的情况下，gbk字符集下M的最大取值就是32766，utf8字符集下M的最大取值就是21844，这都是在表中只有一个字段的情况下说的，一定要记住一个行中的所有列（不包括隐藏列和记录头信息）占用的字节长度加起来不能超过65535个字节！ 3.2 记录中的数据太多产生溢出MySQL中磁盘和内存交互的基本单位是页，也就是说MySQL是以页为基本单位来管理存储空间的，我们的记录都会被分配到某个页中存储。而一个页的大小一般是16KB，也就是16384字节，而一个VARCHAR(M)类型的列就最多可以存储65532个字节，这样就可能造成一个页存放不了一条记录的尴尬情况。 在Compact和Redundant行格式中，对于占用存储空间非常大的列，在记录的真实数据处只会存储该列的一部分数据，把剩余的数据分散存储在几个其他的页中，然后记录的真实数据处用20个字节存储指向这些页的地址（当然这20个字节中还包括这些分散在其他页面中的数据的占用的字节数），从而可以找到剩余数据所在的页。 从图中可以看出来，对于Compact和Redundant行格式来说，如果某一列中的数据非常多的话，在本记录的真实数据处只会存储该列的前768个字节的数据和一个指向其他页的地址，然后把剩下的数据存放到其他页中，这个过程也叫做行溢出，存储超出768字节的那些页面也被称为溢出页。画一个简图就是这样： 不只是 VARCHAR(M)类型的列，其他的 TEXT、BLOB 类型的列在存储数据非常多的时候也会发生行溢出。 3.3 行溢出的临界点发生行溢出的临界点是什么呢？也就是说在列存储多少字节的数据时就会发生行溢出？ MySQL中规定一个页中至少存放两行记录，至于为什么这么规定我们之后再说，现在看一下这个规定造成的影响。我们往表中插入亮条记录，每条记录最少插入多少字节的数据才会行溢出呢？ 分析一下页空间是如何利用的 每个页除了存放我们的记录以外，也需要存储一些额外的信息，乱七八糟的额外信息加起来需要132个字节的空间（现在只要知道这个数字就好了），其他的空间都可以被用来存储记录。 每个记录需要的额外信息是27字节。这27个字节包括下边这些部分： 内容 大小(字节) 真实数据的长度 2 列是否是NULL值 1 头信息 5 row_id 6 transaction_id 6 roll_pointer 7 因为表中具体有多少列不确定，所以没法确定具体的临界点，只需要知道插入的字段数据长度很大就会导致行溢出的现象。 4.Dynamic &amp; Compressed 行格式这俩行格式和Compact行格式挺像，只不过在处理行溢出数据时有点儿分歧，它们不会在记录的真实数据处存储字段真实数据的前768个字节，而是把所有的字节都存储到其他页面中，只在记录的真实数据处存储其他页面的地址，就像这样： Compressed行格式和Dynamic不同的一点是，Compressed行格式会采用压缩算法对页面进行压缩，以节省空间。 至此，行格式就分析的差不多了，接下来我们来看页的存储结构。 二，页的存储结构InnoDB为了不同的目的设计了许多种页，比如存放表空间头部信息的页，存放 Insert Buffer信息的页，存放Innode信息的页，存放undo日志信息的页等等。 本节分析存放表中记录的页，官方成为索引页，为了分析方便，我们暂且叫做数据页。 系统变量innodb_page_size表明了InnoDB存储引擎中的页大小，默认值是16384字节，也就是16kb。 该变量只能在第一次初始化MySQL数据目录时指定，之后就再也不能更改了。 数据页代表的这块16kb的存储空间被划分为多个部分，不同部分有不同的功能。 从图中可以看出，一个InnoDB数据页的存储空间大致被划分为了7个部分，有的部分占用的字节数是确定的，有的占用的字节数不是确定的。 名称 中文名 占用空间大小（字节） 简单描述 File Header 文件头部 38 页的一些通用信息 Page Header 页面头部 56 数据页专有的一些信息 Infifmum + Supremum 最小记录和最大记录 26 两个虚拟的行记录 User Records 用户记录 不确定 实际存储的行记录内容 Free Space 空闲空间 不确定 页中尚未使用的空间 Page Directory 页面目录 不确定 页中某些记录的相对位置 File Trailer 文件尾部 8 校验页是否完整 1. 记录在页中的存储我们先来创建一张表 1234567mysql&gt; create table page_demo( -&gt; c1 int , -&gt; c2 int , -&gt; c3 varchar(10000), -&gt; primary key(c1) -&gt; ) charset=ascii row_format=Compact;Query OK, 0 rows affected (0.03 sec) 因为我们指定了主键，所以存储实际数据的列里面不会有隐藏的row_id,我们来看一下他的行格式。 再次回顾下记录头中5个字节表示的数据。 名称 大小(bit) 描述 预留位1 1 没有使用 预留位2 1 没有使用 delete_mask 1 标记该记录是否被删除 min_rec_mask 1 B+树的每层非叶子节点中的最小记录都会添加该标记 n_owned 4 表示当前记录拥有的记录数 heap_no 13 表示当前记录在记录堆的位置信息 record_type 3 表示当前记录的类型 0 ：普通记录，1：B+树非页节点记录，2：最小记录，3：最大记录 next_record 16 下一条记录的相对位置 针对当前这个表的行格式简化图： 接下来我们往表中插入几条数据： 1INSERT INTO page_demo VALUES(1, 100, &#x27;aaaa&#x27;), (2, 200, &#x27;bbbb&#x27;), (3, 300, &#x27;cccc&#x27;), (4, 400, &#x27;dddd&#x27;); 为了分析这些记录在页的User Records 部分中是怎么表示的，把记录头信息和实际的列数据都用十进制表示出来了（其实是一堆二进制位），所以这些记录的示意图就是： 分析一下头信息中的每个属性是什么意思。 1.1 delete_mask标记当前记录是否被删除，占用1个二进制位，0：未删除，1：删除。 被删除的记录不会立即从磁盘上删除，因为删除他们之后吧其他的记录在磁盘上重新排列需要性能消耗，所以只是打一个删除标记，所有被删掉的数据会组成一个垃圾链表，在这个链表中的记录占用的空间成为可重用空间，之后如果有新的记录插入到表中，可能会把这些删除的记录覆盖掉。 将delete_mask 设置为1 和 将被删除的记录加入到垃圾链表中其实是两个阶段。 1.2 min_rec_maskB+树的每层非叶子节点中的最小记录都会添加该标记，如果这个字段的值是0，意味着不是B+树的非叶子节点中的最小记录。 1.3 n_owned1.4 heap_no这个属性表示当前记录在本页中的位置，我们插入的四条记录在本页中的位置分别是 2，3，4 ，5 。为什么不见 0 和 1 的记录呢？ 这是因为InnoDB自动给每个页里边加了两个记录，由于这两个记录并不是我们自己插入的，所以有时候也称为虚拟记录。这两个伪记录一个代表最小记录，一个代表最大记录。 记录是如何比较大小的？对于一条完整的记录来说，比较记录大小就是比较主键的大小。比方说我们插入的4行记录的主键值分别为1，2，3，4，这也就意味着这四条记录的大小从大到小递增。 但是不管我们往页中插入了多少自己的记录，InnoDB都规定他们定义的两条伪记录分别为最小记录和最大记录。这两条记录的构造十分简单，都是由5字节大小的记录头信息和8字节大小的一个固定的部分组成的。 由于这两条记录不是我们自己定义的记录，所以他们并不存放在页的User Records部分，他们被单独放在一个称为Infimum+Supremum的部分。 从图中我们可以看出来，最小记录和最大记录的heap_no值分别是0 和 1 ， 也就是说他们的位置最靠前。 1.5 record_type这个属性表示当前记录的类型。0：普通记录，1：B+树非叶子节点记录，2：最小记录，3：最大记录。 我们自己插入的记录是普通记录 0 ， 而最大记录和最小记录record_type 分别为 2 和 3。 1.6 next_record表示从当前记录的真实数据到下一条记录的真实数据的地址偏移量。这其实是一条链表，可以通过一条记录找到他的下一条记录，但是下一条记录指的并不是按照我们插入顺序的下一条记录，而是按照主键值由小到大的顺序的下一条记录。而且规定 infimum记录 的下一条记录就是本页主键值最小的用户记录，而本页中主键最大的用户记录的下一条记录就是supremum记录。 如果从中删除一条记录，这个链表也是会跟着变化的，假如现在删除第二条记录 1delete from page_demo where c1 =2 ; 删除第二条记录以后： 发生的变化： 第二条记录并没有从存储空间中移除，而是把该记录的delete_mask设置为1 第二条记录的next_records值变成了0，意味着该记录没有下一条记录了 第一条记录的next record指向了第三条记录 最大记录的 n_owned 值从5 变成了4 所以，不论我们怎么对页中的记录做增删改查操作，InnoDB始终会维护一条记录的单链表，链表中各个节点是按照主键值由小到大的顺序连接起来的。 next_records 为啥要指向记录头信息和真实数据之间的位置呢？为啥不干脆指向整条记录的开头位置，也就是记录的额外信息开头的位置呢？ 因为这个位置刚刚好，向左读取就是记录头信息，向右读取就是真实数据。我们前边还说过变长字段长度列表，null值列表中的信息都是逆序存放的，这样可以使记录中位置靠前的字段和他们对应的字段长度信息在内存中的距离更近，可能会提高高速缓存的命中率。 因为主键值为2的记录已经被我们删除了，但是存储空间并没有回收，如果再次把这条记录插入到表中，会发生什么？ 1INSERT INTO page_demo VALUES(2, 200, &#x27;bbbb&#x27;); 从图中可以看到，InnoDB并没有因为新记录的插入而为他申请新的存储空间，而是直接复用了原来删除的记录的存储空间。 2. Page Directory（页目录）如果我们想根据主键值查找页中某条记录该咋办？ 1select * from page_demo where c1 = 3; 将所有正常的记录(包括两条隐藏记录但是不包括已经标记为删除的记录)划分为几组 每个组的最后一条记录（也就是组内最大的那条记录）的头信息中的n_owned属性表示该组拥有多少条记录 将每个组的最后一条记录的地址偏移量单独提取出来按照顺序存储到靠近页的尾部的地方，这个地方就是所谓的【Page Directory】,也就是页目录。页目录中的这些地址偏移量被称为槽，所以页目录就是由槽组成的 比方说刚才创建的表中正常的记录由6条，InnoDB会把他们分成两组，第一组中只有一条最小记录，第二组中是剩余的5条记录。 现在页目录部分中有两个槽，也就意味着我们的记录被分成了两个组，槽1中的值为112，代表最大记录的地址偏移量；槽0的值为99，代表最小记录的地址偏移量。 注意最大和最小记录的头信息的n_owned属性： 最小记录中的n_owned值为1，这就代表着以最小记录结尾的这个分组中只有1条记录，也就是最小记录本身 最大记录中的n_owned值为5，这就代表着以最大记录结尾的这个分组中只有5条记录，包括最大记录本身还有我们自己插入的4条记录 【99】&amp;【112】这样的地址偏移量很不直观，我们用箭头指向的方式替代数字。 InnoDB对每个分组中的记录条数是有规定的：对于最小记录所在的分组只能有 1 条记录，最大记录所在的分组拥有的记录条数只能在 1~8 条之间，剩下的分组中记录的条数范围只能在是 4~8 条之间。所以分组是按照下边的步骤进行的： 初始情况下一个数据页里只有最小记录和最大记录两条记录，它们分属于两个分组。 之后每插入一条记录，都会从页目录中找到主键值比本记录的主键值大并且差值最小的槽，然后把该槽对应的记录的n_owned值加1，表示本组内又添加了一条记录，直到该组中的记录数等于8个。 在一个组中的记录数等于8个后再插入一条记录时，会将组中的记录拆分成两个组，一个组中4条记录，另一个5条记录。这个过程会在页目录中新增一个槽来记录这个新增分组中最大的那条记录的偏移量。 由于现在page_demo表中的记录太少，无法演示添加了页目录之后加快查找速度的过程，所以再往page_demo表中添加一些记录： 1INSERT INTO page_demo VALUES(5, 500, &#x27;eeee&#x27;), (6, 600, &#x27;ffff&#x27;), (7, 700, &#x27;gggg&#x27;), (8, 800, &#x27;hhhh&#x27;), (9, 900, &#x27;iiii&#x27;), (10, 1000, &#x27;jjjj&#x27;), (11, 1100, &#x27;kkkk&#x27;), (12, 1200, &#x27;llll&#x27;), (13, 1300, &#x27;mmmm&#x27;), (14, 1400, &#x27;nnnn&#x27;), (15, 1500, &#x27;oooo&#x27;), (16, 1600, &#x27;pppp&#x27;); 现在看怎么从这个页目录中查找记录。因为各个槽代表的记录的主键值都是从小到大排序的，所以我们可以使用所谓的二分法来进行快速查找。5个槽的编号分别是：0、1、2、3、4，所以初始情况下最低的槽就是low=0，最高的槽就是high=4。比方说我们想找主键值为6的记录，过程是这样的： 计算中间槽的位置：(0+4)/2=2，所以查看槽2对应记录的主键值为8，又因为8 &gt; 6，所以设置high=2，low保持不变。 重新计算中间槽的位置：(0+2)/2=1，所以查看槽1对应的主键值为4，又因为4 &lt; 6，所以设置low=1，high保持不变。 因为high - low的值为1，所以确定主键值为6的记录在槽2对应的组中。此刻我们需要找到槽2中主键值最小的那条记录，然后沿着单向链表遍历槽2中的记录。但是我们前边又说过，每个槽对应的记录都是该组中主键值最大的记录，这里槽2对应的记录是主键值为8的记录，怎么定位一个组中最小的记录呢？别忘了各个槽都是挨着的，我们可以很轻易的拿到槽1对应的记录（主键值为4），该条记录的下一条记录就是槽2中主键值最小的记录，该记录的主键值为5。所以我们可以从这条主键值为5的记录出发，遍历槽2中的各条记录，直到找到主键值为6的那条记录即可。由于一个组中包含的记录条数只能是1~8条，所以遍历一个组中的记录的代价是很小的。 所以在一个数据页中查找指定主键值的记录的过程分为两步： 通过二分法确定该记录所在的槽，并找到该槽所在分组中主键值最小的那条记录。 通过记录的next_record属性遍历该槽所在的组中的各个记录。 3.Page Header（页面头部）为了能得到一个数据页中存储的记录的状态信息，比如本页中已经存储了多少条记录，第一条记录的地址是什么，页目录中存储了多少个槽等等，特意在页中定义了一个叫Page Header的部分，它是页结构的第二部分，这个部分占用固定的56个字节，专门存储各种状态信息。 名称 占用空间大小（字节） 描述 PAGE_N_DIR_SLOTS 2 在页目录中的槽数量 PAGE_HEAP_TOP 2 还未使用的空间最小地址，也就是说从该地址之后就是Free Space PAGE_N_HEAP 2 本页中的记录的数量（包括最小和最大记录以及标记为删除的记录） PAGE_FREE 2 第一个已经标记为删除的记录地址（各个已删除的记录通过next_record也会组成一个单链表，这个单链表中的记录可以被重新利用） PAGE_GARBAGE 2 已删除记录占用的字节数 PAGE_LAST_INSERT 2 最后插入记录的位置 PAGE_DIRECTION 2 记录插入的方向 PAGE_N_DIRECTION 2 一个方向连续插入的记录数量 PAGE_N_RECS 2 该页中记录的数量（不包括最小和最大记录以及被标记为删除的记录） PAGE_MAX_TRX_ID 8 修改当前页的最大事务ID，该值仅在二级索引中定义 PAGE_LEVEL 2 当前页在B+树中所处的层级 PAGE_INDEX_ID 8 索引ID，表示当前页属于哪个索引 PAGE_BTR_SEG_LEAF 10 B+树叶子段的头部信息，仅在B+树的Root页定义 PAGE_BTR_SEG_TOP 10 B+树非叶子段的头部信息，仅在B+树的Root页定义 4.File Header（文件头部）File Header针对各种类型的页都通用，也就是说不同类型的页都会以File Header作为第一个组成部分，它描述了一些针对各种页都通用的一些信息，比方说这个页的编号是多少，它的上一个页、下一个页是谁，这个部分占用固定的38个字节。 名称 占用空间大小（字节） 描述 FIL_PAGE_SPACE_OR_CHKSUM 4 页的校验和（checksum值） FIL_PAGE_OFFSET 4 页号 FIL_PAGE_PREV 4 上一个页的页号 FIL_PAGE_NEXT 4 下一个页的页号 FIL_PAGE_LSN 8 页面被最后修改时对应的日志序列位置（英文名是：Log Sequence Number） FIL_PAGE_TYPE 2 该页的类型 FIL_PAGE_FILE_FLUSH_LSN 8 仅在系统表空间的一个页中定义，代表文件至少被刷新到了对应的LSN值 FIL_PAGE_ARCH_LOG_NO_OR_SPACE_ID 4 页属于哪个表空间 InnoDB为了不同的目的而把页分为不同的类型，我们上边介绍的其实都是存储记录的数据页，其实还有很多别的类型的页，具体如下表： 类型名称 十六进制 描述 FIL_PAGE_TYPE_ALLOCATED 0x0000 最新分配，还没使用 FIL_PAGE_UNDO_LOG 0x0002 Undo日志页 FIL_PAGE_INODE 0x0003 段信息节点 FIL_PAGE_IBUF_FREE_LIST 0x0004 Insert Buffer空闲列表 FIL_PAGE_IBUF_BITMAP 0x0005 Insert Buffer位图 FIL_PAGE_TYPE_SYS 0x0006 系统页 FIL_PAGE_TYPE_TRX_SYS 0x0007 事务系统数据 FIL_PAGE_TYPE_FSP_HDR 0x0008 表空间头部信息 FIL_PAGE_TYPE_XDES 0x0009 扩展描述页 FIL_PAGE_TYPE_BLOB 0x000A 溢出页 FIL_PAGE_INDEX 0x45BF 索引页，也就是我们所说的数据页 我们存放记录的数据页的类型其实是FIL_PAGE_INDEX，也就是所谓的索引页。 有时候我们存放某种类型的数据占用的空间非常大（比方说一张表中可以有成千上万条记录），InnoDB可能不可以一次性为这么多数据分配一个非常大的存储空间，如果分散到多个不连续的页中存储的话需要把这些页关联起来，FIL_PAGE_PREV和FIL_PAGE_NEXT就分别代表本页的上一个和下一个页的页号。这样通过建立一个双向链表把许许多多的页就都串联起来了，而无需这些页在物理上真正连着。需要注意的是，并不是所有类型的页都有上一个和下一个页的属性，不过我们现在分析的数据页（也就是类型为FIL_PAGE_INDEX的页）是有这两个属性的，所以所有的数据页其实是一个双链表。 5.File Trailer(文件尾部)如果页中的数据在内存中被修改了，那么在修改后的某个时间需要把数据同步到磁盘中。但是在同步了一半的时候中断电了咋办？ 为了检测一个页是否完整，在每个页的尾部都加了一个File Trailer部分，这个部分由8个字节组成，可以分成2个小部分： 前四个字节代表校验和 后四个字节代表页面被最后修改时对应的日志序列位置 这个File Trailer &amp; File Header 类似，都是所有类型的页通用的。 至此，整个数据页的结构我们也基本上分析完了，现在在回头看一下开头我们那张恐怖的图，是不是感觉清晰很多了呢？接下来，我们来分析索引的结构。 三，索引1.假如没有索引我们先来看看没有索引的情况下，我们进行数据的查找(毕竟没有对比就没有伤害)。 1.1 在一个页中查找假设表中的记录很少，所有的记录仅仅用一个页就存放下了，这个时候按照不同的搜索条件其实可以分为两种情况讨论： 【以主键为搜索的条件】：可以再页目录中根据二分查找快速定位到槽，在根据槽定位到该组的最小索引记录，然后进行遍历匹配查找。 【以其他列作为搜索条件】：在数据页中并没有为非主键列建立所谓的页目录，所以无法通过二分法快速定位相应的槽。在这种情况下，只能从最小记录开始依次往后遍历单链表中的每条记录，然后对比每条记录是否符合搜索条件，显然，效率很低。 1.2 在很多页中查找很多时候，表的记录一个页都是存储不下的，这个时候的查找其实分为两个步骤： 【定位到记录所在的页】 【从所在的页内查找相应的记录】 因为我们不能快速的定位到所在的页，所以只能从第一页开始沿着双链表往后遍历定位页，定位到页以后在根据在一个页中的查找方式进行匹配查找，显而易见，这个时候效率低的可怕。 有了痛点，就会有大牛去思考整个生命周期，完善逻辑和资源倾斜，形成一套自己的方法论，想办法为快速查找赋能。 2. 索引我们先创建一张表： 1234567mysql&gt; CREATE TABLE index_demo( -&gt; c1 INT, -&gt; c2 INT, -&gt; c3 CHAR(1), -&gt; PRIMARY KEY(c1) -&gt; ) ROW_FORMAT = Compact;Query OK, 0 rows affected (0.03 sec) 2.1 一个简单的索引方案我们在根据某个搜索条件查找一些记录时为什么要遍历所有的数据页呢？ 因为各个页中的记录并没有规律，我们并不知道我们的搜索条件匹配哪些页中的记录，所以 不得不 依次遍历所有的数据页。 如果我们想快速的定位到需要查找的记录在哪些数据页中该咋办？ 对比根据主键值快速定位一条记录从而在页中的位置建立页目录，我们也可以想办法为快速定位记录所在的数据页而建立一个别的目录。 【下一个数据页中用户记录的主键值必须大于上一个页中用户记录的主键值。】 假设我们现在每一页只能放三条记录，现在已经放了主键为1,3,5的三条记录。这个时候我们再添加一条主键为4的记录，我们不得不为他分配一个新的页。 注意：新分配的数据页编号可能和原来并不是连续的，也就是说我们使用的这些页在存储空间里可能并不挨着。他们只是通过维护着上一页和下一页的编号而建立了链表关系。 原来页中主键最大的值为5，现在我们新插入一条记录，如果直接放在新页里面，那就会有问题，这不符合下一个数据页中用户记录的主键值必须大于上一个页中用户记录的主键值得要求，所以在插入主键值为4 的记录的时候需要伴随一次记录的移动，也就是把主键值为5 的记录移动到新分配的页中，然后把主键值为4 的记录插入到原来的页中。 这个过程表明了在对页中的记录进行增删改操作的过程中，我们必须通过一些诸如记录移动的操作来始终保证这个状态一直成立：下一个数据页中用户记录的主键值必须大于上一个页中用户记录的主键值。这个过程我们也可以称为页分裂。 【给所有的页建立一个目录项。】 由于数据页的编号可能并不是连续的，所以在向index_demo表中插入许多条记录后，可能是这样的效果： 因为这些16KB的页在物理存储上可能并不挨着，所以如果想从这么多页中根据主键值快速定位某些记录所在的页，我们需要给它们做个目录，每个页对应一个目录项，每个目录项包括下边两个部分： 页的用户记录中最小的主键值，我们用key来表示。 页号，我们用page_no表示。 以页28为例，它对应目录项2，这个目录项中包含着该页的页号28以及该页中用户记录的最小主键值5。我们只需要把几个目录项在物理存储器上连续存储，比如把他们放到一个数组里，就可以实现根据主键值快速查找某条记录的功能了。比方说我们想找主键值为20的记录，具体查找过程分两步： 先从目录项中根据二分法快速确定出主键值为20的记录在目录项3中（因为 12 &lt; 20 &lt; 209），它对应的页是页9。 再根据前边说的在页中查找记录的方式去页9中定位具体的记录。 至此，针对数据页做的简易目录就搞定了。这个目录其实就是【索引】。 2.2 InnoDB中的索引方案上面的方案存在什么样的问题？ InnoDB是使用页来作为管理存储空间的基本单位，也就是最多能保证16KB的连续存储空间，而随着表中记录数量的增多，需要非常大的连续的存储空间才能把所有的目录项都放下，这对记录数量非常多的表是不现实的。 我们时常会对记录进行增删，假设我们把页28中的记录都删除了，页28也就没有存在的必要了，那意味着目录项2也就没有存在的必要了，这就需要把目录项2后的目录项都向前移动一下。 InnoDB复用了之前存储用户记录的数据页来存储目录项，为了和用户记录做一下区分，我们把这些用来表示目录项的记录称为目录项记录。 那InnoDB怎么区分一条记录是普通的用户记录还是目录项记录呢？通过记录头信息里的record_type属性，它的各个取值代表的意思如下： 0：普通的用户记录 **1**：目录项记录 2：最小记录 3：最大记录 把前边使用到的目录项放到数据页中的样子就是这样： 从图中可以看出来，我们新分配了一个编号为30的页来专门存储目录项记录。这里再次强调一遍目录项记录和普通的用户记录的不同点： 目录项记录的record_type值是1，而普通用户记录的record_type值是0。 目录项记录只有主键值和页的编号两个列，而普通的用户记录的列是用户自己定义的，可能包含很多列，另外还有InnoDB自己添加的隐藏列。 头信息里面有一个叫min_rec_mask的属性，只有在存储目录项记录的页中的主键值最小的目录项记录的min_rec_mask值为1，其他别的记录的min_rec_mask值都是0。 除此之外，两者就没有区别了，页的组成结构也是一样一样的（就是我们前边介绍过的7个部分），都会为主键值生成Page Directory（页目录），从而在按照主键值进行查找时可以使用二分法来加快查询速度。 现在以查找主键为20的记录为例，根据某个主键值去查找记录的步骤就可以大致拆分成下边两步： 先到存储目录项记录的页，也就是页30中通过二分法快速定位到对应目录项，因为12 &lt; 20 &lt; 209，所以定位到对应的记录所在的页就是页9。 再到存储用户记录的页9中根据二分法快速定位到主键值为20的用户记录。 虽然说目录项记录中只存储主键值和对应的页号，比用户记录需要的存储空间小多了，但是不论怎么说一个页只有16KB大小，能存放的目录项记录也是有限的，那如果表中的数据太多，以至于一个数据页不足以存放所有的目录项记录，该咋办呢？ 当然是再多整一个存储**目录项记录**的页。 从图中可以看出，我们插入了一条主键值为320的用户记录之后需要两个新的数据页： 为存储该用户记录而新生成了页31。 因为原先存储目录项记录的页30的容量已满（我们前边假设只能存储4条目录项记录），所以不得不需要一个新的页32来存放页31对应的目录项。 现在因为存储目录项记录的页不止一个，所以如果我们想根据主键值查找一条用户记录大致需要3个步骤，以查找主键值为20的记录为例： 确定目录项记录页 我们现在的存储目录项记录的页有两个，即页30和页32，又因为页30表示的目录项的主键值的范围是[1, 320)，页32表示的目录项的主键值不小于320，所以主键值为20的记录对应的目录项记录在页30中。 通过目录项记录页确定用户记录真实所在的页。 在真实存储用户记录的页中定位到具体的记录。 那么问题来了，在这个查询步骤的第1步中我们需要定位存储目录项记录的页，但是这些页在存储空间中也可能不挨着，如果我们表中的数据非常多则会产生很多存储目录项记录的页，那我们怎么根据主键值快速定位一个存储目录项记录的页呢？ 为这些存储目录项记录的页再生成一个更高级的目录，就像是一个多级目录一样，大目录里嵌套小目录，小目录里才是实际的数据，所以现在各个页的示意图就是这样子： 随着表中记录的增加，这个目录的层级会继续增加，如果简化一下，那么我们可以用下边这个图来描述它： 其实这是一种组织数据的形式，或者说是一种数据结构，它的名称是B+树。 不论是存放用户记录的数据页，还是存放目录项记录的数据页，我们都把它们存放到B+树这个数据结构中了，所以我们也称这些数据页为节点。从图中可以看出来，我们的实际用户记录其实都存放在B+树的最底层的节点上，这些节点也被称为叶子节点或叶节点，其余用来存放目录项的节点称为非叶子节点或者内节点，其中B+树最上边的那个节点也称为根节点。 从图中可以看出来，一个B+树的节点其实可以分成好多层，InnoDB规定最下边的那层，也就是存放我们用户记录的那层为第0层，之后依次往上加。之前的分析我们做了一个非常极端的假设：存放用户记录的页最多存放3条记录，存放目录项记录的页最多存放4条记录。其实真实环境中一个页存放的记录数量是非常大的，假设所有存放用户记录的叶子节点代表的数据页可以存放100条用户记录，所有存放目录项记录的内节点代表的数据页可以存放1000条目录项记录，那么： 如果B+树只有1层，也就是只有1个用于存放用户记录的节点，最多能存放100条记录。 如果B+树有2层，最多能存放1000×100=100000条记录。 如果B+树有3层，最多能存放1000×1000×100=100000000条记录。 如果B+树有4层，最多能存放1000×1000×1000×100=100000000000条记录。 一般情况下，我们用到的B+树都不会超过4层，那我们通过主键值去查找某条记录最多只需要做4个页面内的查找（查找3个目录项页和一个用户记录页），又因为在每个页面内有所谓的Page Directory（页目录），所以在页面内也可以通过二分法实现快速定位记录。 2.3 聚簇索引上边介绍的B+树本身就是一个目录，或者说本身就是一个索引。它有两个特点： 使用记录主键值的大小进行记录和页的排序，这包括三个方面的含义： 页内的记录是按照主键的大小顺序排成一个单向链表。 各个存放用户记录的页也是根据页中用户记录的主键大小顺序排成一个双向链表。 存放目录项记录的页分为不同的层次，在同一层次中的页也是根据页中目录项记录的主键大小顺序排成一个双向链表。 B+树的叶子节点存储的是完整的用户记录。所谓完整的用户记录，就是指这个记录中存储了所有列的值（包括隐藏列）。 我们把具有这两种特性的B+树称为聚簇索引，所有完整的用户记录都存放在这个聚簇索引的叶子节点处。这种聚簇索引并不需要我们在MySQL语句中显式的使用INDEX语句去创建，InnoDB存储引擎会自动的为我们创建聚簇索引。另外，在InnoDB存储引擎中，聚簇索引就是数据的存储方式（所有的用户记录都存储在了叶子节点），也就是所谓的索引即数据，数据即索引。 2.4 二级索引聚簇索引只能在搜索条件是主键值时才能发挥作用，因为B+树中的数据都是按照主键进行排序的。那如果我们想以别的列作为搜索条件怎么办？ 我们可以多建几棵B+树，不同的B+树中的数据采用不同的排序规则。比方说我们用c2列的大小作为数据页、页中记录的排序规则，再建一棵B+树，效果如下图所示： 这个B+树与上边介绍的聚簇索引有几处不同： 使用记录c2列的大小进行记录和页的排序，这包括三个方面的含义： 页内的记录是按照c2列的大小顺序排成一个单向链表。 各个存放用户记录的页也是根据页中记录的c2列大小顺序排成一个双向链表。 存放目录项记录的页分为不同的层次，在同一层次中的页也是根据页中目录项记录的c2列大小顺序排成一个双向链表。 B+树的叶子节点存储的并不是完整的用户记录，而只是c2列+主键这两个列的值。 目录项记录中不再是主键+页号的搭配，而变成了c2列+页号的搭配。 所以如果我们现在想通过c2列的值查找某些记录的话就可以使用我们刚刚建好的这个B+树了。以查找c2列的值为4的记录为例，查找过程如下： 确定目录项记录页 根据根页面，也就是页44，可以快速定位到目录项记录所在的页为页42（因为2 &lt; 4 &lt; 9）。 通过目录项记录页确定用户记录真实所在的页。 在页42中可以快速定位到实际存储用户记录的页，但是由于c2列并没有唯一性约束，所以c2列值为4的记录可能分布在多个数据页中，又因为2 &lt; 4 ≤ 4，所以确定实际存储用户记录的页在页34和页35中。 在真实存储用户记录的页中定位到具体的记录. 到页34和页35中定位到具体的记录。 但是这个B+树的叶子节点中的记录只存储了c2和c1（也就是主键）两个列，所以我们必须再根据主键值去聚簇索引中再查找一遍完整的用户记录。 我们根据这个以**c2**列大小排序的**B+**树只能确定我们要查找记录的主键值，所以如果我们想根据**c2**列的值查找到完整的用户记录的话，仍然需要到**聚簇索引**中再查一遍，这个过程也被称为**回表**。也就是根据c2列的值查询一条完整的用户记录需要使用到2棵B+树！！！ 为什么我们还需要一次回表操作呢？直接把完整的用户记录放到叶子节点不就好了么？ 如果把完整的用户记录放到叶子节点是可以不用回表，相当于每建立一棵B+树都需要把所有的用户记录再都拷贝一遍，这就有点太浪费存储空间了。因为这种按照非主键列建立的B+树需要一次回表操作才可以定位到完整的用户记录，所以这种B+树也被称为二级索引（英文名secondary index），或者辅助索引。由于我们使用的是c2列的大小作为B+树的排序规则，所以我们也称这个B+树为为c2列建立的索引。 假设我们的查询结果是十条，那就是要进行10次回表，那这样的话，效率不是又慢了？ 在MySQL5.6对这种情况进行了优化，如果发现查询结果会导致多次回表，那么就会进行IO合并，拿到所有的主键再去进行回表。 2.5 联合索引我们也可以同时以多个列的大小作为排序规则，也就是同时为多个列建立索引，比方说我们想让B+树按照c2和c3列的大小进行排序，这个包含两层含义： 先把各个记录和页按照c2列进行排序。 在记录的c2列相同的情况下，采用c3列进行排序 为c2和c3列建立的索引的示意图如下： 3. InnoDB的B+树索引的注意事项3.1 跟页面永远固定不动前边介绍B+树索引的时候，为了理解上的方便，先把存储用户记录的叶子节点都画出来，然后接着画存储目录项记录的内节点，实际上B+树的形成过程是这样的： 每当为某个表创建一个B+树索引（聚簇索引不是人为创建的，默认就有）的时候，都会为这个索引创建一个根节点页面。最开始表中没有数据的时候，每个B+树索引对应的根节点中既没有用户记录，也没有目录项记录。 随后向表中插入用户记录时，先把用户记录存储到这个根节点中。 当根节点中的可用空间用完时继续插入记录，此时会将根节点中的所有记录复制到一个新分配的页，比如页a中，然后对这个新页进行页分裂的操作，得到另一个新页，比如页b。这时新插入的记录根据键值（也就是聚簇索引中的主键值，二级索引中对应的索引列的值）的大小就会被分配到页a或者页b中，而根节点便升级为存储目录项记录的页。 这个过程需要特别注意的是：一个B+树索引的根节点自诞生之日起，便不会再移动。这样只要我们对某个表建立一个索引，那么它的根节点的页号便会被记录到某个地方，然后凡是InnoDB存储引擎需要用到这个索引的时候，都会从那个固定的地方取出根节点的页号，从而来访问这个索引。 3.2 内节点中目录项记录的唯一性我们知道B+树索引的内节点中目录项记录的内容是索引列 + 页号的搭配，但是这个搭配对于二级索引来说有点儿不严谨。假设表中的数据是这样的： c1 c2 c3 1 1 ‘u’ 3 1 ‘d’ 5 1 ‘y’ 7 1 ‘a’ 如果二级索引中目录项记录的内容只是索引列 + 页号的搭配的话，那么为c2列建立索引后的B+树应该长这样： 如果我们想新插入一行记录，其中c1、c2、c3的值分别是：9、1、&#39;c&#39;，那么在修改这个为c2列建立的二级索引对应的B+树时便碰到了个大问题：由于页3中存储的目录项记录是由c2列 + 页号的值构成的，页3中的两条目录项记录对应的c2列的值都是1，而我们新插入的这条记录的c2列的值也是1，那我们这条新插入的记录到底应该放到页4中，还是应该放到页5中? 为了让新插入记录能找到自己在那个页里，我们需要保证在B+树的同一层内节点的目录项记录除页号这个字段以外是唯一的。所以对于二级索引的内节点的目录项记录的内容实际上是由三个部分构成的： 索引列的值 主键值 页号 也就是我们把主键值也添加到二级索引内节点中的目录项记录了，这样就能保证B+树每一层节点中各条目录项记录除页号这个字段外是唯一的。 这样我们再插入记录(9, 1, &#39;c&#39;)时，由于页3中存储的目录项记录是由c2列 + 主键 + 页号的值构成的，可以先把新记录的c2列的值和页3中各目录项记录的c2列的值作比较，如果c2列的值相同的话，可以接着比较主键值，因为B+树同一层中不同目录项记录的c2列 + 主键的值肯定是不一样的，所以最后肯定能定位唯一的一条目录项记录，在本例中最后确定新记录应该被插入到页5中。 3.3 一个页面最少存储2条记录B+树只需要很少的层级就可以轻松存储数亿条记录，这是因为B+树本质上就是一个大的多层级目录，每经过一个目录时都会过滤掉许多无效的子目录，直到最后访问到存储真实数据的目录。那如果一个大的目录中只存放一个子目录会怎么样？那就是目录层级非常多，而且最后的那个存放真实数据的目录中只能存放一条记录，会导致效率很低。 其实让B+数的叶子结点值存储一条记录，让内节点存储多条记录，也还是可以发挥B+数的作用的。但是InnoDB为了避免数的层级过高，要求所有的数据页都至少可以容纳两条记录。 4. MyISAM中的索引方案简单介绍MyISAM的索引方案虽然也使用树形结构，但是却将索引和数据分开存储： 将表中的记录按照记录的插入顺序单独存储在一个文件中，称之为数据文件。这个文件并不划分为若干个数据页，有多少记录就往这个文件中塞多少记录就成了。我们可以通过行号而快速访问到一条记录。 使用MyISAM存储引擎的表会把索引信息另外存储到一个称为索引文件的另一个文件中。MyISAM会单独为表的主键创建一个索引，只不过在索引的叶子节点中存储的不是完整的用户记录，而是主键值 + 行号的组合。也就是先通过索引找到对应的行号，再通过行号去找对应的记录！这一点和InnoDB是完全不相同的，在InnoDB存储引擎中，我们只需要根据主键值对聚簇索引进行一次查找就能找到对应的记录，而在MyISAM中却需要进行一次回表操作，意味着MyISAM中建立的索引相当于全部都是二级索引！ 如果有需要的话，我们也可以对其它的列分别建立索引或者建立联合索引，原理和InnoDB中的索引差不多，不过在叶子节点处存储的是相应的列 + 行号。这些索引也全部都是二级索引。 由于在插入数据的时候并没有刻意按照主键大小排序，所以我们并不能在MyIsaM数据上使用二分法进行查找。 5. 创建和删除索引的语句InnoDB和MyISAM会自动为主键或者声明为UNIQUE的列去自动建立B+树索引，但是如果我们想为其他的列建立索引就需要我们显式的去指明。 我们可以在创建表的时候指定需要建立索引的单个列或者建立联合索引的多个列： 1234CREATE TALBE 表名 ( 各种列的信息 ··· , [KEY|INDEX] 索引名 (需要被索引的单个列或多个列)) 我们也可以在修改表结构的时候添加索引： 1ALTER TABLE 表名 ADD [INDEX|KEY] 索引名 (需要被索引的单个列或多个列); 也可以在修改表结构的时候删除索引： 1ALTER TABLE 表名 DROP [INDEX|KEY] 索引名; 至此，整个索引相关的结构我们就都分析完了。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/tags/MySQL/"}]},{"title":"MySQL[一]入门","slug":"MySQL/MySQL[一]入门","date":"2022-01-11T02:49:27.263Z","updated":"2022-01-11T03:07:39.587Z","comments":true,"path":"2022/01/11/MySQL/MySQL[一]入门/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/MySQL/MySQL[%E4%B8%80]%E5%85%A5%E9%97%A8/","excerpt":"","text":"一，MYSQL入门1.数据库相关概念123DB：数据库：存储数据的仓库，保存了一系列有组织的数据。DBMS：数据库管理系统：数据库是通过DBMS创建和操作的容器。SQL：结构化查询语言：专门用来与数据库通信的语言。 2.数据库的好处121.可以持久化数据到本地2.可以实现结构化查询，方便管理 3.数据库存储数据特点12345671.将数据放到表中，表放到库中。2.一个数据库有多张表，每个表都有一个名字，用来标识自己。表名具有唯一性。3.表具有一些特性，这些特性定义了数据在表中如何存储，类似Java中类的设计。4.表有列组成，我们也称为字段。所有表都是由一个列或多个列组成的，每一列类似Java中的属性。5.表中的数据按照行来存储，每一行类似于Java中的对象。 4.mysql的安装与使用参照mysql安装文档 5.Mysql常用命令12345678910111213141516171819显示数据库-----&gt;show Databases;使用数据库-----&gt;use 数据库名；显示表----&gt;show tables;显式指定数据库的表----&gt;show tables from 数据库名；查看位于那个数据库----&gt;select database();显示表结构---&gt;desc 表名；查看数据库版本：---&gt;select version();查看数据库版本2:-----&gt;Dos:mysql --version;查看数据库信息-----&gt;show CREATE DATABASE mydb1;查看服务器中的数据库，并把mydb1的字符集修改为utf-8-----&gt;ALTER DATABASE mydb1character set utf8;删除数据库-----&gt;drop database mydb1;表中增加一栏信息-----&gt;alter table student add image blob;删除表-----&gt;drop table student;修改地址-----&gt;alter table student modify address varchar(100);删除一个属性-----&gt; alter table student drop image;修改表名-----&gt;rename table student to students;查看表的创建细节-----&gt;show create table students;修改表的字符集为 gbk-----&gt;alter table students character set gbk;列名name修改为studentname-----&gt;alter table students change name studentname varchar(100); 6.mysql语法规范12345671.不区分大小写，建议关键字大写，表名列名小写。2.每条命令最好用分号结尾。3.每条语句可以缩进，换行。4.注释单行注释：#注释文字 -- 注释文字多行注释：/* */ 二，DQL查询语言1.基础查询12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758**语法： select 查询列表 from 表名****查询列表：表中的字段，常量，表达式，函数****查询的结果是张虚拟的表格**1.查询表中的单个字段select last_name from employee;2.查询表中的多个字段select last_name,salary,email from employee;3.查询表中的所有字段select * from employee;4.查询常量值select 100;select &#x27;john&#x27;;5.查询表达式select 100*98;6.查询函数select version();7.起别名select last_name as name from employee;select last_name name from employee;8.去重查询员工表中涉及到的所有的部门编号select distinct department_id from employee;9.+的作用#运算符：两个操作数都为数值型，则做加法运算；#其中一方为字符型，试图将字符型数值转换成数值型，#如果转换成功，继续做加法运算；否则，将字符型数值#转换为0；10.使用concat实现连接#案例：查询员工名和性连接成一个字段SELECT CONCAT(username,PASSWORD) FROM USER;#任何数与null做运算结果都为null 2.条件查询语法： select 查询列表 from 表名 where 筛选条件 分类： ①按照条件表达式筛选条件运算符：&gt;,&lt;,=,!=,&gt;=,&lt;= 123456查询员工工资&gt;1w2的员工信息select * from employee where salary &gt;12000;查询部门编号！=90号的员工名和部门编号select name, dep_id from employee where dep_id 1=90； ②按照逻辑表达式筛选逻辑运算符：&amp;&amp;,||,!,AND,OR,NOT 1234查询工资在一万到两万之见的员工名，工资以及奖金。select name,salary ,jiangjin where salary between 10000 and 20000;查询部门编号不在90-110之间，或者工资高于15000的员工信息。select * from employee where department&lt;90||department&gt;110 ||salary &gt;15000; ③模糊查询like：一般和通配符搭配使用通配符：%任意多个字符，包含0个字符_任意单个字符BETWEEN AND:包含临界值IN:判断某个字段的值是否属于in列表中的某一项IS NULL,IS NOT NULL:=或者！=不能用来判断null安全等于&lt;=&gt;可以判断null 123456789101112131415查询员工名中包含a的员工信息select * from emp where name like %a%;查询员工名中第三个字符为e第五个字符为a的员工名和工资select name ,salary from emp where name like %__e_a%;员工名中第二个字符为_的员工名select name from emp where name like %_\\_%;查询员工编号在100到120之间的所有员工信息select * from emp where id between 100 and 120;查询员工的工种编号是IT_PRIG,AD_PRES,AD_VP中的一个员工名和工种编号；select name , id from emp where id in(IT_PRIG,AD_PRES,AD_VP);查询没有奖金的员工名和奖金率select salary , jjl from emp where salary is Null;查询有奖金的员工名和奖金率select salary ,jjl from emp where salary is not null; ④IF null的使用：123查询员工号为176的员工的姓名和部门号和年薪SELECT last_name ,department_id , salary*(1+IFNULL(commission_pct,0))*12 &#x27;年薪&#x27;FROM employees WHERE employee_id =176; 3.排序查询语法： select 查询列表 from 表 where 筛选条件 order by 排序列表 asc 或desc （升序或者降序，默认为升序） 12345678910查询员工信息，要求工资从高到低排序select * from emp order by salary desc;查询部门编号大于等于90的员工信息，按照入职时间先后排序select * from emp where dep_id &gt;=90 order by createtime asc;按照员工年薪的高低显示员工的信息和年薪select * ,年薪 from emp order by salary*(1+if null(jjl,0))*12 as 年薪 desc;按姓名长度显示员工的姓名和工资select name ,salary from emp order by length(name) asc;查询员工信息，先按照工资排序，再按照员工编号排序select * from emp order by salary asc,id asc; 4.常见函数功能：类似Java中的方法分类：单行函数分组函数 1.单行函数1.字符函数12345678910111213141516171819202122232425262728293031323334353637381.length 获取参数值的字节个数select * from emp order by length(name);2.concat 拼接字符串select concat(last_name,first_name) as 姓名 from emp;3.upper，lower 大小写转换函数案例：将姓变大写，名字变小写，然后拼接SELECT CONCAT(UPPER(last_name),LOWER(first_name))FROM employees;4.substr,SUBSTRING 截取字符串SELECT SUBSTR(&#x27;李莫愁&#x27;,2);SELECT SUBSTR(&#x27;李莫愁&#x27;,2,3);案例：姓名中首字符大写，其他的小写然后用_拼接显示出来SELECT CONCAT( UPPER(SUBSTR(last_name, 1, 1)), &#x27;_&#x27;, LOWER(SUBSTR(last_name, 2)) ) output FROM employees ;5.instr:返回字串第一次出现的索引，如果找不到返回0SELECT INSTR(&#x27;风急天高猿啸哀&#x27;,&#x27;天&#x27;) AS out_put;6.trim :去掉前后空格或前后指定字符SELECT LENGTH(TRIM(&#x27; 张三丰 &#x27;)) AS out_put;SELECT TRIM(&#x27;a&#x27; FROM &#x27;aaaa1aa2aaa3aaa&#x27;) AS out_put;7.lpad :用指定字符填满指定长度（左填充）SELECT LPAD(&#x27;苍老师&#x27;,10,&#x27;*&#x27;);8.rpad:用指定字符填满指定长度（右填充）SELECT RPAD(&#x27;苍老师&#x27;,10,&#x27;*&#x27;);9.replace 替换SELECT REPLACE(&#x27;千锋培训机构&#x27;,&#x27;千锋&#x27;,&#x27;尚硅谷&#x27;); 2.数学函数12345678910111.round:四舍五入SELECT ROUND(1.666);SELECT ROUND(1.567,2);2.ceil 向上取整SELECT CEIL(1.52);3.floor 向下取整SELECT FLOOR(1.52);4.truncate:截断（小数点后保留几位）SELECT TRUNCATE(1.65,2);5.mod:取余SELECT MOD(10,3); 3.日期函数123456789101112131415161718192021222324252627281.now:返回当前系统日期时间SELECT NOW();2.curdate:返回当前系统日期SELECT CURDATE();3.curtime:返回当前时间SELECT CURTIME();4.获取指定部分的年月日时分秒SELECT YEAR(NOW());SELECT YEAR(hiredate) FROM employees;5.str_to_date将字符通过指定的格式转化成日期SELECT STR_TO_DATE(&#x27;1998-3-2&#x27;,&#x27;%Y-%c-%d&#x27;) AS out_put;案例：查询入职时间为1992-4-3的员工信息SELECT * FROM employeesWHERE hiredate=STR_TO_DATE(&#x27;2016-3-3&#x27;,&#x27;%Y-%c-%d&#x27;);6.date_format 将日期转换成字符SELECT DATE_FORMAT(NOW(),&#x27;%y年%m月%d日&#x27;) AS 日期;案例：查询有奖金的员工名和入职日期（xx月/xx日 xx年）SELECT last_name, DATE_FORMAT(hiredate, &#x27;%c月/%d日 %y&#x27;) FROM employees WHERE commission_pct IS NOT NULL ; 4.其他函数123SELECT VERSION();SELECT DATABASE();SELECT USER(); 5.流程控制函数1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465661.if:IF else效果SELECT IF(10&gt;5,&#x27;true&#x27;,&#x27;false&#x27;);案例：查询如果有奖金就备注有，没有就备注没有。SELECT last_name, commission_pct, IF( commission_pct IS NULL, &#x27;没奖金&#x27;, &#x27;有奖金&#x27; ) AS 备注 FROM employees ;2.case函数1)switch-CASE语法:CASE 要判断的字段或者表达式WHEN 常量1 THEN 要显示的值1或者语句1WHEN 常量2 THEN 要显示的值2或者语句2...ELSE 要显示的值n或者语句n；案例：查询员工的工资，要求部门号==30，显示的工资为1.1倍，部门号==40，显示的工资为1.2倍，部门号==50，显示的工资为1.3倍，其他部门，显示原有工资。SELECT salary AS 原始工资, department_id , CASE department_id WHEN 30 THEN salary * 1.1 WHEN 40 THEN salary * 1.2 WHEN 50 THEN salary * 1.3 ELSE salary END AS 新工资 FROM employees ;2)CASE 使用2：语法：CASE WHEN 条件1 THEN 要显示的值1或语句1WHEN 条件2 THEN 要显示的值2或语句2...ELSE 要显示的值n或语句nEND案例：查询员工的工资情况如果&gt;2w，显示A如果&gt;1.5w，显示B如果&gt;1w，显示C否则，显示DSELECT salary, CASE WHEN salary &gt; 20000 THEN &#x27;A&#x27; WHEN salary &gt; 15000 THEN &#x27;B&#x27; WHEN salary &gt; 10000 THEN &#x27;C&#x27; ELSE &#x27;D&#x27; END AS 工资等级 FROM employees ; 2.分组函数功能：用作统计使用 123456789101112131415161718192021222324251.sum :求和SELECT SUM(salary) FROM employees;2.avg：平均值SELECT AVG(salary) FROM employees;3.max：最大值SELECT MAX(salary) FROM employees;4.min：最小值SELECT MIN(salary) FROM employees;5.count：计算个数SELECT COUNT(salary) FROM employees;总结①.sum,avg一般用于处理数值类型②.max，min，count用来处理任何类型③.以上分组函数都忽略null值④.可以和distinct搭配SELECT SUM(DISTINCT salary) 纯净,SUM(salary) FROM employees;6.count的详细介绍①select COUNT(*) FROM employees;②select COUNT(1) FROM employees;③和分组函数一同查询的字段要求是group by后的字段。 5.分组查询GROUP BY 和分组函数对应分组查询中分组条件分为两类 数据源 位置 关键字 分组前筛选 原始表 GROUP BY 子句的前面 WHERE 分组后筛选 分组后的结果集 GROUP BY 子句的后面 HAVING 分组函数做条件肯定是放在having子句中。group BY 子句支持单个字段分组，多个字段分组（多个字段之间用逗号隔开没有顺序要求），表达式或函数。也可以添加排序，放在整个分组查询的最后。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869案例：查询每个工种的最高工资SELECT MAX(salary), job_id FROM employees GROUP BY job_id ORDER BY MAX(salary) ASC ;案例：查询邮箱中包含a字符的，每个部门的平均工资SELECT AVG(salary), department_id FROM employees WHERE email LIKE &#x27;%a%&#x27; GROUP BY department_id ;#select Avg(salary),dep_id from employee where email like %a% group by dep_id ;案例：查询有奖金的每个领导手下员工的最高工资SELECT MAX(salary), manager_id FROM employees WHERE commission_pct IS NOT NULL GROUP BY manager_id ;#select max(salary) ,manage_id from employees where commission_pct is not null group by manager_id;案例：哪个部门的员工个数大于二？SELECT COUNT(*), department_id FROM employees GROUP BY department_id HAVING COUNT(*) &gt; 2 ;#select dep_id from emp group by dep_id having count(*)&gt;2;案例：查询每个工种有奖金的员工的最高工资&gt;12000的工种编号和最高工资SELECT MAX(salary), job_id FROM employees WHERE commission_pct IS NOT NULL GROUP BY job_id HAVING MAX(salary) &gt; 12000 ;#select job_id ,max(salary) from emp where commission_pct IS NOT NULL group by job_id having max(salary)&gt;12000;案例：查询领导编号&gt;102的每个领导手下的最低工资&gt;5000的领导编号是哪个？SELECT manager_id ,MIN(salary)FROM employeesWHERE manager_id&gt;102GROUP BY manager_idHAVING MIN(salary)&gt;5000;#select manager_id from emp where manager_id&gt;102 group by manager_id having min(salary)&gt;5000;#按照员工姓名的长度分组，查询每一组的员工个数，筛选员工个数&gt;5的有哪些？SELECT COUNT(*) AS cFROM employeesGROUP BY LENGTH(last_name) HAVING c&gt;5;# select count(*) from emp group by length(name) having count(*)&gt;5;#查询每个部门每个工种的员工的平均工资SELECT AVG(salary),job_idFROM employeesGROUP BY department_id,job_id;#select avg(salary) from emp group by dep_id,job_id;#查询每个部门每个工种的员工的平均工资并且按照平均工资的高低显示SELECT AVG(salary),job_idFROM employeesGROUP BY department_id,job_idORDER BY AVG(salary) ASC;#select avg(salary) from emp group by dep_id,job_id order by avg(salary) asc; 6.连接查询又称为多表查询，当查询的字段来自多个表时，就会用到连接查询。**笛卡尔乘积现象：表1有m行，表2有n行，结果：m_n行_发生原因：没有有效的连接条件 分类 ①按年代分类sql92:仅仅支持内连接sql99：不支持全外连接 ②按功能分类 内连接 外连接 交叉连接 等值连接 左外连接 非等值连接 右外连接 自连接 全外连接 1.等值连接①多表等值连接的结果为多表的交集部分②n表连接，至少需要n-1个连接条件③多表的顺序没有要求④一般需要为表起别名⑤可以搭配前面介绍的所有子句使用，比如排序，分组，筛选 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192#案例一：查询女优名对应的男优名SELECT NAME, boyName FROM beauty, boys WHERE beauty.boyfriend_id = boys.`id` ;#select name, boyname from girl ,boy where girl.boyfriend_id=boy.id;#案例：查询员工名和对应的部门名SELECT last_name, department_name FROM employees, departments WHERE employees.`department_id` = departments.`department_id` ;#select name ,dep_name from emp e,dep d where e.dep.id= d.id;#案例：查询员工名，工种号，工种名。SELECT last_name, emp.`job_id`, job_title FROM employees emp, jobs job WHERE emp.`job_id` = job.`job_id` ;#select name , e.job_id,job_title from emp e,job j where e.job_id=j.id;#案例：查询有奖金的员工名和部门名SELECT last_name, department_name FROM employees emp, departments dep WHERE commission_pct IS NOT NULL &amp;&amp; emp.`department_id` = dep.`department_id` ;#select name ,dep_name from emp e ,dep d where e.dep_id =d.id &amp;&amp;e.salary_pct is not null;#案例：查询城市名第二个字符为o的部门SELECT department_name FROM locations l, departments d WHERE l.`location_id` = d.`location_id` AND l.`city` LIKE &#x27;_o%&#x27; ;#select dep_name from location l , dep d where l.city like %_o% &amp;&amp; l.id =d.location_id;#案例：查询每个城市的部门个数SELECT COUNT(*), city FROM locations l, departments d WHERE l.`location_id` = d.`location_id` GROUP BY l.`city` ;#select count(*),city from loca l,dep d where l.loc_id=d.loc_id group by count(*) asc;#案例：查询有奖金的每个部门的部门名和部门的领导编号和该部门的最低工资SELECT d.`department_name`, d.manager_id, MIN(salary) FROM employees e, departments d WHERE e.`department_id` = d.`department_id` AND e.`commission_pct` IS NOT NULL GROUP BY d.`department_id`, d.`department_name` ;#select dep_name ,d.manager_id ,min(salary) from emp e ,dep d where e.`department_id` = d.`department_id` AND e.`commission_pct` IS NOT NULL GROUP BY d.`department_id`,d.`department_name` ;#案例：查询每个工种的工种名和员工的个数，并且按照员工个数降序排序SELECT j.job_title, COUNT(*) FROM jobs j, employees e WHERE j.`job_id` = e.`job_id` GROUP BY e.`job_id`, j.`job_title` ORDER BY COUNT(*) DESC ;#案例：查询员工名，部门名和所在城市SELECT last_name, department_name, city FROM employees e, departments d, locations l WHERE e.`department_id` = d.`department_id` AND d.`location_id` = l.`location_id` ; 2.非等值连接123456789#案例：查询员工的工资和工资级别SELECT DISTINCT salary, grade_level FROM employees e, job_grades j WHERE e.salary &gt;= j.lowest_sal &amp;&amp; e.salary &lt;= j.highest_sal ORDER BY salary ASC ; 3.自连接12345678#案例：查询员工名和上级的名称SELECT e.last_name, m.last_name FROM employees e, employees m WHERE e.manager_id = m.employee_id ; 4.内连接INNER 可以省略 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475#查询员工名，部门名SELECT last_name, department_name FROM employees e INNER JOIN departments d ON e.department_id = d.department_id ;#查询名字中包含e的员工名和工种名SELECT last_name, job_title FROM employees e INNER JOIN jobs j ON e.job_id = j.job_id WHERE last_name LIKE &#x27;%e%&#x27; ;#查询部门个数&gt;3的城市名和部门个数SELECT city, COUNT(*) FROM departments d INNER JOIN locations l ON d.`location_id` = l.`location_id` GROUP BY cityHAVING COUNT(*) &gt; 3 ;#查询哪个部门的部门员工个数&gt;3的部门名和员工个数，并按照个数降序排序SELECT department_name, COUNT(*) FROM employees e INNER JOIN departments d ON e.`department_id` = d.`department_id` GROUP BY e.department_id HAVING COUNT(*) &gt; 3 ORDER BY COUNT(*) DESC ;#查询员工名，部门名，工种名，并按照部门名降序排序SELECT last_name, department_name, job_title FROM employees e INNER JOIN departments d ON e.`department_id` = d.`department_id` INNER JOIN jobs j ON e.`job_id` = j.`job_id` ORDER BY department_name DESC ;#查询员工工资级别SELECT grade_level, salaryFROM job_grades j INNER JOIN employees e ON e.`salary` BETWEEN j.`lowest_sal` AND j.`highest_sal` ;#查询每个工资级别的个数，并且降序排序SELECT grade_level,COUNT(*)FROM employees eINNER JOIN job_grades jON e.`salary` BETWEEN j.`lowest_sal` AND j.`highest_sal` GROUP BY grade_levelORDER BY COUNT(*) DESC;#查询员工的名字和上级的名字SELECT e1.last_name, e2.last_nameFROM employees e1INNER JOIN employees e2ON e1.`employee_id`=e2.`manager_id`; 5.左外连接语法：SELECT 查询列表FROM 表1 【连接类型】JOIN 表2ON 连接条件WHERE 筛选条件GROUP BY 分组HAVING 筛选条件ORDER BY 排序条件连接类型：内连接：inner左外连接：left右外连接：right全外连接：full交叉连接：cross外连接用于查询一个表中有，另一个表中没有的数据左外连接，left左边是主表右外连接，right右边是主表Mysql不支持全外连接 1234567#没有男朋友的女生SELECT g.`name`,b.`boyName`FROM beauty gLEFT JOIN boys bON g.`boyfriend_id`=b.`id`WHERE b.`boyName` IS NULL; 6.交叉连接笛卡尔乘积 7.子查询出现在其它语句中的select语句，称为子查询或内查询外部的查询语句，称为主查询或外查询分类： ①按照子查询出现的位置： select后面 from后面 where或having后面 exists后面 仅仅支持标量子查询 支持表子查询 标量子查询，列子查询 表子查询 ②按照结果集的行列数不同： 标量子查询 列子查询 行子查询 表子查询 结果只有一行一列 结果一列多行 一行多列 多行多列 1）where或having后面特点：子查询一般放在小括号内子查询一般放在条件的右边标量子查询，一般搭配着单行操作符列子查询：一般搭配多行操作符使用 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211.标量子查询#谁的工资比Abel高SELECT last_name FROM employees WHERE salary &gt; (SELECT salary FROM employees WHERE last_name = &#x27;Abel&#x27;) ;#返回job_id于141号员工相同，salary比143号员工多的员工 姓名，job_id和工资SELECT last_name, job_id, salary FROM employees WHERE job_id = (SELECT job_id FROM employees WHERE employee_id = 141) AND salary &gt; (SELECT salary FROM employees WHERE employee_id = 143)#返回公司工资工资最少的员工的姓名，job_id,salarySELECT last_name, job_id, salary FROM employees WHERE salary = (SELECT MIN(salary) FROM employees);#查询最低工资大于50号部门最低工资的部门id和其最低工资SELECT department_id, MIN(salary) FROM employees GROUP BY department_id HAVING MIN(salary) &gt; (SELECT MIN(salary) FROM employees WHERE department_id = 50) ;2.列子查询多行操作符：IN / NOT in：等于列表中的任意一个ANY / SOME ：和子查询返回的某一个值比较ALL ：和子查询返回的所有值比较#返回location_id是1400或者1700的部门中的所有员工姓名SELECT last_name FROM employees WHERE department_id IN (SELECT DISTINCT department_id FROM departments WHERE location_id IN (1400, 1700)) ;#返回其他工种中比job_id为IT_PROG部门任意工资低的员工#工号，姓名，job_id以及salarySELECT employee_id, last_name, job_id, salary FROM employees WHERE salary &lt; (SELECT MAX(salary) FROM employees WHERE job_id = &#x27;IT_PROG&#x27;) AND job_id !=&#x27;IT_PROG&#x27;;#返回其他工种中比job_id为IT_PROG部门所有工资低的员工#工号，姓名，job_id以及salarySELECT employee_id, last_name, job_id, salary FROM employees WHERE salary &lt; (SELECT MIN(salary) FROM employees WHERE job_id = &#x27;IT_PROG&#x27;) AND job_id !=&#x27;IT_PROG&#x27;;*********************************3.行子查询#查询员工编号最小并且工资最高的员工信息SELECT * FROM employees WHERE employee_id = (SELECT MIN(employee_id) FROM employees) AND salary = (SELECT MAX(salary) FROM employees) 2）SELECT 后面123456789101112131415161718192021#查询每个部门的员工个数SELECT d.*, (SELECT COUNT(*) FROM employees e WHERE e.department_id = d.department_id) FROM departments d ;#查询员工号等于102的部门名SELECT department_name FROM departments WHERE department_id = (SELECT department_id FROM employees WHERE employee_id = 102) ; 3）FROM 后面1234567891011121314#查询每个部门平均工资的工资等级SELECT grade_level ,aa.department_idFROM (SELECT AVG(salary) ag, department_id FROM employees GROUP BY department_id) aa INNER JOIN job_grades j ON aa.ag BETWEEN lowest_sal AND highest_sal ; 4）exists后面（相关子查询）12345678#查询有员工的部门名SELECT department_name FROM departments dWHERE EXISTS(SELECT * FROM employees e WHERE d.department_id=e.department_id);#查询没有女朋友的男生信息SELECT bo.* FROM boys bo WHEREbo.`id` NOT IN(SELECT boyfriend_id FROM beauty); 5）子查询经典案例祥讲1234567891011121314151617181920212223242526272829303132331.查询工资最低的员工信息：last_name,salarySELECT last_name,salary FROM employeesWHERE salary=(SELECT MIN(salary) FROM employees);2.查询平均工资最低的部门信息SELECT * FROM departments WHERE department_id=(SELECT department_id FROM employees GROUP BY department_id ORDER BY AVG(salary)LIMIT 1)3.查询平均工资最低的部门信息和该部门的平均工资SELECT d.*,a1.ag FROM departments d JOIN (SELECT AVG(salary) ag,department_id FROM employees GROUP BY department_id ORDER BY AVG(salary)LIMIT 1) a1ON d.department_id=a1.department_id4.查询平均工资最高的job信息SELECT j.* FROM jobs j WHERE j.job_id=(SELECT job_id FROM employeesGROUP BY job_id ORDER BY AVG(salary) DESC LIMIT 1)5.查询平均工资高于公司平均工资的部门有哪些SELECT department_id FROM (SELECT department_id ,AVG(salary) AS avg1 FROM employees GROUP BY department_id) e1WHERE e1.avg1&gt;(SELECT AVG(salary) AS avg2 FROM employees) 6.查询出公司中所有manager的详细信息SELECT * FROM employeesWHERE employee_id IN(SELECT DISTINCT manager_id FROM employees);7.各个部门中，最高工资中最低的那个部门的最低工资是多少SELECT MIN(salary) FROM employees GROUP BY department_idHAVING department_id=(SELECT department_id FROM employees GROUP BY department_id ORDER BY MAX(salary)LIMIT 1)8.查询平均工资最高的部门的manager的详细信息：last_name,department_id,email,salarySELECT last_name,department_id,email,salary FROM employees WHERE employee_id=(SELECT manager_id FROM departments WHERE department_id=(SELECT department_id FROM employees GROUP BY department_id ORDER BY AVG(salary)DESC LIMIT 1)) 8.分页查询**语法：limit(currentPage-1)size,size 123456789#查询前五条员工信息SELECT * FROM employees LIMIT 0,5;#查询第11-25条员工信息SELECT * FROM employees LIMIT 10,15;#查询有奖金的员工，并且工资最高的前十名显示出来SELECT * FROM employeesWHERE commission_pct IS NOT NULLORDER BY salary DESC LIMIT 0 ,10; 9.联合查询要查询的结果来自于多个表，且多个表没有直接的连接关系，单查询的信息一致时特点：1.要求多条查询语句的查询列数是一致的2.要求多条查询语句的查询的每一列的类型和顺序最好一致3.union关键字默认去重，如果使用union all 可以不去除重复项 1234案例：查询员工部门编号大于90或邮箱包含a的员工信息SELECT * FROM employees WHERE department_id&gt;90UNIONSELECT * FROM employees WHERE email LIKE &#x27;%a%&#x27;; 三，DML数据操作语言插入insert 1234567891011121314151617一：插入语句#插入beauty一行数据INSERT INTO beauty(NAME,sex,borndate,phone,photo,boyfriend_id)VALUES(&#x27;波多野吉依&#x27;,&#x27;女&#x27;,&#x27;1998-11-11&#x27;,&#x27;13342969497&#x27;,NULL,10)#可以为null的列如何不插入值直接写null，或列名少写一列INSERT INTO beauty(NAME,sex,borndate,phone,photo,boyfriend_id)VALUES(&#x27;小泽玛利亚&#x27;,&#x27;女&#x27;,&#x27;1999-11-11&#x27;,&#x27;13342456497&#x27;,NULL,11)INSERT INTO beauty VALUES(15,&#x27;马蓉&#x27;,&#x27;女&#x27;,&#x27;1989-11-11&#x27;,&#x27;13342456123&#x27;,NULL,12);INSERT INTO beauty SET id=16,NAME=&#x27;刘亦菲&#x27;, sex=&#x27;女&#x27;,borndate=&#x27;1989-10-01&#x27;,phone=&#x27;15945231056&#x27;,boyfriend_id=16;#insert 嵌套子查询，将一个表的数据插入另一张表INSERT INTO beauty (NAME,sex,borndate,phone,boyfriend_id)SELECT &#x27;妲己&#x27;,&#x27;女&#x27;,&#x27;1111-11-11&#x27;,&#x27;13146587954&#x27;,0; 修改update 1234567891011121314151617181920二，修改 UPDATE beauty SET phone=&#x27;110&#x27; WHERE id=16;多表修改：sql99UPDATE 表1 别名INNER|LEFT|RIGHT JOIN 表2 别名ON 连接条件SET 列=值WHERE 筛选条件#修改张无忌的女朋友手机号为114UPDATE beauty gINNER JOIN boys bON g.boyfriend_id=b.idSET g.phone=&#x27;114&#x27;WHERE b.boyName=&#x27;张无忌&#x27;;#修改没有男朋友的女生的男朋友编号都为4号UPDATE beauty gLEFT JOIN boys bON g.`boyfriend_id`=b.idSET g.`boyfriend_id`=4WHERE b.id=NULL; 删除delete 1234567891011121314151617181920212223三，删除DELETE 和 TRUNCATE 的区别：1.delete可以加where条件，truncate不行2.truncate删除效率高3.加入要删除的表中有自增列，用delete删除整个表后在插入数据，从断点处开始插入用truncate删除后在插入数据，从1开始。4.truncate删除没有返回值，delete有返回值5.truncate删除不能回滚，delete删除可以回滚DELETE FROM beauty WHERE id=17;语法：truncate TABLE 表名;#删除张无忌的女朋友的信息DELETE g FROM beauty gINNER JOIN boys bON g.boyfriend_id=b.idWHERE b.id=1;#删除黄晓明以及他女朋友的信息DELETE b,g FROM beauty gINNER JOIN boys bON b.`id`=g.`boyfriend_id`WHERE b.`boyName`=&#x27;黄晓明&#x27;;多表删除 :TRUNCATETRUNCATE TABLE boys 四，DDL数据定义语言1.库和表的管理12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849一，库的管理创建 CREATECREATE DATABASE IF NOT EXISTS mydb1 ;修改 ALTER1.更改字符集ALTER DATABASE mydb1 CHARACTER SET utf8;删除 DROPDROP DATABASE IF EXISTS school;二，表的管理创建 CREATECREATE TABLE book(id INT PRIMARY KEY,b_name VARCHAR(30),price DOUBLE,author_id INT ,publishDate DATE);DESC book ;CREATE TABLE author(id INT PRIMARY KEY ,au_name VARCHAR(20),nation VARCHAR(10));DESC author;修改 ALTER1.修改列名ALTER TABLE book CHANGE COLUMN publishDate pub_date DATETIME;2.修改列的类型或约束ALTER TABLE book MODIFY COLUMN pub_date DATE;3.添加新列ALTER TABLE author ADD COLUMN annual DOUBLE;4.删除新列ALTER TABLE author DROP COLUMN annual;5.修改表名ALTER TABLE author RENAME TO book_author;删除 DROPDROP TABLE IF EXISTS my_employee;SHOW TABLES;复制1.仅仅复制表的结构CREATE TABLE copy LIKE book_author;2.复制表的结构加数据CREATE TABLE copy2SELECT * FROM book_author;3.复制部分结构CREATE TABLE copy3 SELECT id,au_nameFROM book_authorWHERE id=0; 2.数据类型数值型1.整型 TINYINT SMALLINT MEDIUMINT INT/INTEGER BIGINT 1 2 3 4 8 12345如何设置无符号和有符号(默认有符号)DROP TABLE tab_int;CREATE TABLE tab_int(t1 INT,t2 INT UNSIGNED);INSERT INTO tab_int(t1,t2) VALUES(-1,1);DESC tab_int; 1）如果插入的数值超出了整形的范围，会报out of range异常，并且插入临界值。2）如果不设置长度，会有默认的长度。长度代表了显示的最大宽度，如果不够会用0在左边填充，但必须搭配zerofill使用。2.小数①定点数dec（M,D）②浮点数float（4） ，double（8）M，D的意思：M指定一共多少位，D指定小数几位，超出会四舍五入。MD都可以省略，如果是dec，则M默认为10，D默认为0如果是浮点数，则会根据插入数值的精度改变精度定点型精度相对较高。3.字符型①较短的文本CHAR(M)默认为1,VARCHAR(M)M:字符数char：固定长度字符，比较耗费空间，但是效率高。varchar：可变长度字符 123456789ENUM 枚举类CREATE TABLE tab_char( t1 ENUM(&#x27;a&#x27;,&#x27;c&#x27;,&#x27;b&#x27;));SET 集合CREATE TABLE tab_set(s1 SET(&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;,&#x27;d&#x27;));INSERT INTO tab_set(s1) VALUES(&#x27;a,b&#x27;); BINARY:保存较短的二进制。②较长的文本text（文本）,BLOB(较大的二进制)4.日期型DATE:日期DATETIME:日期加时间，8字节timestamp：跟时区有关系，建议使用，4字节time：时间year：年 123456789CREATE TABLE tab_date(t1 DATETIME,t2 TIMESTAMP);INSERT INTO tab_date(t1,t2)VALUES(NOW(),NOW());SELECT * FROM tab_date;SET time_zone=&#x27;+9:00&#x27;;#设置时区为东9区 3.常见约束含义：一种限制，用于限制表中的数据，保证数据的一致性。 NOT NULL DEFAULT PRIMARY KEY 唯一，且不为空 UNIQUE 唯一，可以为空 CHECK Mysql不支持 FOREIGN KEY 外键约束，用于限制两个表的关系，用于保证该字段的值必须来自于主表的关联列的值。约束的分类：列级约束：除外键约束表级约束：除了非空，默认。CREATE TABLE 表名(字段1 字段类型 列级约束,字段2 字段类型 列级约束,表级约束); 1234567891011121314151617181920212223242526#创建表时添加列级约束DROP TABLE tab_test;CREATE TABLE tab_test(id INT PRIMARY KEY,stu_name VARCHAR(20) NOT NULL,gender CHAR DEFAULT &#x27;男&#x27;,seat_id INT UNIQUE, major_id INT REFERENCES tab_major(id) );CREATE TABLE tab_major(id INT PRIMARY KEY ,major_name VARCHAR(20) NOT NULL);DESC tab_test;SHOW INDEX FROM tab_test;#查看索引信息#添加表级约束CREATE TABLE tab_test(id INT PRIMARY KEY AUTO_INCREMENT,stu_name VARCHAR(20) NOT NULL,gender CHAR DEFAULT &#x27;男&#x27;,seat_id INT UNIQUE, major_id INT ,CONSTRAINT m_id FOREIGN KEY(major_id) REFERENCES tab_major(id) );CONSTRAINT m_id 可以省略 面试题：主键约束和唯一约束的区别：都可以保证唯一性，主键不能为空 ，unique 能为空，但是只能有一个null。主键只能有1个，unique可以有多个。都允许两个列组合成一个约束。面试题：外键：要求在从表设置外键关系从表的外键列类型和主表的关联列类型一致，名称无要求要求主表的关联列必须是主键或者唯一键插入数据应该先插入主表再插入从表删除数据应该先删除从表，在删除主表二，修改表时添加约束 1234567891011CREATE TABLE tab_test2(id INT ,stu_name VARCHAR(20) ,gender CHAR ,seat_id INT , major_id INT );ALTER TABLE tab_test2 MODIFY COLUMN stu_name VARCHAR(20) NOT NULL ;ALTER TABLE tab_test2 MODIFY COLUMN id INT PRIMARY KEY AUTO_INCREMENT;#添加外键ALTER TABLE tab_test2 ADD FOREIGN KEY(major_id) REFERENCES tab_major(id); 4.标识列自增长列 AUTO_INCREMENT特点：1.表示必须和一个key搭配2.一个表最多一个标识列3.标识列类型只能是数值型4.标识列可以通过set auto_increment_increment=3;设置步长 1234CREATE tab_auto(id INT PRIMARY KEY AUTO_INCREMENT,NAME VARCHAR(20) NOT NULL); 五，TCL语言：事务控制语言事务：一个或一组sql语句组成的执行单元， 要么全部执行,要么都不执行。存储引擎:在MySQL中的数据用各种不同的技术存储在文件中。通过show ENGINES;来查看mysql支持的存储引擎。innodb引擎支持事务。事务的ACID属性：1.原子性:事务是一个不可分割的工作单位，要么都发生，要么都不发生。2.一致性：事务必须使数据库从一个一致性状态变为另一个一致性状态。3.隔离性：一个事务的执行不能被另一个事务干扰。4.持久性：事务一旦被提交，对数据库事务的改变就是永久性的。 DELETE 和 TRUNCATE 在事务中的区别： 1234567891011演示deleteSET autocommit=0;START TRANSACTION;DELETE FROM tab_teacher;ROLLBACK;演示 TRUNCATESET autocommit=0;START TRANSACTION;TRUNCATE TABLE tab_teacher;ROLLBACK;DELETE 是直接删除表中数据，truncate是江表删除，创建一张与原来一样的空表。 六，视图含义：虚拟表，和普通表格一样使用通过表动态生成的数据 1.创建视图语法：CREATE VIEW 视图名AS查询语句 ; 12345678910111213141516171819202122# 案例：查询姓名中包含a字符的员工名，部门名和工种信息create view view1 as select e.last_name,d.department_name ,j.job_title from employees einner join departments d on e.department_id = d.department_id inner join jobs j on e.job_id = j.job_idwhere e.last_name like &#x27;%a%&#x27;;select * from view1;# 案例：查询各个部门的平均工资级别create view view2 asselect j.grade_level ,aa.department_id from job_grades jinner join (select avg(salary) avg_s,department_id from employees group by department_id) aa on aa.avg_s between j.lowest_sal and j.highest_sal;select * from view2;# 案例：查询平均工资最低的部门信息create view view3 asselect avg(salary) avg_s ,department_idfrom employeesgroup by department_idorder by avg_s asclimit 1;select * from view3; 2.视图修改①create OR REPLACE VIEW 视图名 AS 查询语句;②alter VIEW 视图名 AS 查询语句; 3.删除视图DROP VIEW v1,v2; 4.查看视图DESC v1; 12345678910#创建视图emp_v1，要求查询电话号码以011开头的员工姓名和工资，邮箱CREATE VIEW emp_v1 ASSELECT last_name ,salary,email FROM employees WHEREphone_number LIKE &#x27;%011&#x27;;#创建视图emp_v2,要求查询部门的最高工资高于12000的部门信息CREATE VIEW v4 ASSELECT department_id FROM employees GROUP BY department_idHAVING MAX(salary)&gt; 12000;CREATE VIEW emp_v2 ASSELECT * FROM departments WHERE department_id IN(SELECT * FROM v4); 5.视图的更新视图的可更新性和视图中查询的定义有关，以下类型的视图是不能更新的。1.包含以下关键字的sql语句：分组函数，distinct，group by，having，union2.常量视图3.select中包含子查询的4.join5.from 一个不能更新的视图6.where子句的子查询引用了from子句的表 6.视图和表的对比： 创建语法的关键字 是否实际占用物理空间 使用 视图 CREATE VIEW 只是保存了sql逻辑 增删改查，一般不能增删改 表 CREATE TABLE 占用 增删改查 七，变量系统变量 ：变量由系统提供，不是用户自定义，属于服务器层面。查看系统所有变量：show GLOBAL VARIABLES;查看满足条件的部分系统变量： SHOW GLOBAL VARIABLES LIKE ‘%char%’;查看指定的某个系统变量的值： SELECT @@global.autocommit;为某个系统变量赋值：set @@global.系统变量名=值;全局变量:GLOBAL作用域：服务器每次启动将为所有的全局变量赋初始值，针对于所有的会话有效，但不能跨重启。会话变量:SESSION作用域：针对当前的会话有效。用户自定义变量用户变量声明： SET/SELECT @用户变量名 :=值;赋值：通过 SELECT 字段 INTO 变量名;或 SET/SELECT @用户变量名 :=值;使用：select @用户变量名;应用在任何地方。作用域：针对当前会话和连接有效。局部变量作用域：作用在定义它的begin END 块中。声明： DECLARE 变量名 类型 （default 值）;赋值：通过 SELECT 字段 INTO 变量名;或 SET/SELECT @变量名 :=值;使用：select @变量名;只能放在begin END 中的第一句话 八，存储过程和函数存储过程：一组预先定义好的sql语句集合，理解成批处理语句。1.提高代码的重用性2.简化操作3.减少了编译次数并且减少了和数据库服务器的连接次数，提高了效率。 1.创建语法：CREATE PROCEDURE 存储过程名（参数列表）BEGIN一组合法的sql语句;END参数列表：参数模式 参数名 参数类型 参数模式：in：该参数可以作为输入，也就是该参数需要调用方传入值OUT ：该参数可以作为输出，也就是该参数可以作为返回值inout：该参数既可以作为输入又可以作为输出 如果存储过程只有一句话，begin END 可以省略 存储过程体中的每条sql语句的结尾需要必须加分号，存储过程的结尾可以使用 DELIMITER 重新设置。 2.调用CALL 存储过程名（实参列表）; 3.案例1234567891011121314151617181920212223242526272829303132333435363738394041424344#插入到admin表中五条记录DELIMITER $CREATE PROCEDURE my_a()BEGININSERT INTO admin(username,PASSWORD) VALUES(&#x27;yin&#x27;,&#x27;666&#x27;);INSERT INTO admin(username,PASSWORD) VALUES(&#x27;aa&#x27;,&#x27;123&#x27;);INSERT INTO admin(username,PASSWORD) VALUES(&#x27;bb&#x27;,&#x27;666&#x27;);INSERT INTO admin(username,PASSWORD) VALUES(&#x27;cc&#x27;,&#x27;123&#x27;);INSERT INTO admin(username,PASSWORD) VALUES(&#x27;dd&#x27;,&#x27;666&#x27;);END $#创建存储过程实现 根据女生名查询对应的男生信息DELIMITER $CREATE PROCEDURE my_b(IN beauty_name VARCHAR(20))BEGIN SELECT bo.* FROM boys bo RIGHT JOIN beauty b ON bo.id=b.boyfriend_id WHERE b.name=beauty_name;END $CALL my_b(&#x27;热巴&#x27;);#根据女生名返回他的男朋友名DELIMITER $CREATE PROCEDURE my_d(IN beautyName VARCHAR(20),OUT boyName VARCHAR(20))BEGIN SELECT bo.boyName INTO boyName FROM boys bo INNER JOIN beauty b ON bo.id=b.boyfriend_id WHERE b.name=beautyName;END $CALL my_d(&#x27;小昭&#x27;,@b_name);SELECT @b_name;#传入两个值a，b，最终翻倍返回a和bDELIMITER $CREATE PROCEDURE my_e(INOUT a INT ,INOUT b INT )BEGIN SET a=a*2; SET b=b*2;END $SET @m=10;SET @n=20;CALL my_e(@m,@n);SELECT @m,@n; 4.删除存储过程12DROP PROCEDURE 存储过程名DROP PROCEDURE my_a; 5.查看存储过程的信息1SHOW CREATE PROCEDURE my_b; 函数存储过程可以有0/n个返回值：适合批量增删改函数有且仅有一个返回值：适合查询 1.创建12345DELIMITER $CREATE FUNCTION 函数名(参数列表) RETURNS 返回类型BEGINEND 注意：参数列表：参数名，参数类型一定会有return语句 2.使用SELECT 函数名(参数列表) 12345678910111213141516171819#返回公司员工个数DELIMITER $CREATE FUNCTION my_f1() RETURNS INTBEGINDECLARE c INT DEFAULT 0 ; SELECT COUNT(*) INTO c FROM employees; RETURN c;END $SELECT my_f1();#根据员工名返回他的工资DELIMITER $CREATE FUNCTION my_f2(NAME VARCHAR(20)) RETURNS DOUBLEBEGIN DECLARE c DOUBLE; SELECT salary INTO c FROM employees WHERE last_name=NAME; RETURN c;END $SET @a=&#x27;Hunold&#x27;;SELECT my_f2(@a); 3.查看1SHOW CREATE FUNCTION my_f2; 4.删除1DROP FUNCTION my_f2; 九，流程控制分支结构1.if （表达式1，表达式2，表达式3）如果表达式1成立，就返回表达式2的值，否则返回表达式3的值。应用在任何地方 2.case1)switch-CASE语法:CASE 要判断的字段或者表达式WHEN 常量1 THEN 要显示的值1或者语句1WHEN 常量2 THEN 要显示的值2或者语句2…ELSE 要显示的值n或者语句n； 1234567891011121314151617181920案例：查询员工的工资，要求部门号==30，显示的工资为1.1倍，部门号==40，显示的工资为1.2倍，部门号==50，显示的工资为1.3倍，其他部门，显示原有工资。SELECT salary AS 原始工资, department_id , CASE department_id WHEN 30 THEN salary * 1.1 WHEN 40 THEN salary * 1.2 WHEN 50 THEN salary * 1.3 ELSE salary END AS 新工资 FROM employees ; 2)CASE 使用2：语法：CASEWHEN 条件1 THEN 要显示的值1或语句1WHEN 条件2 THEN 要显示的值2或语句2…ELSE 要显示的值n或语句nEND 12345678910111213141516171819202122232425262728293031323334案例：查询员工的工资情况如果&gt;2w，显示A如果&gt;1.5w，显示B如果&gt;1w，显示C否则，显示DSELECT salary, CASE WHEN salary &gt; 20000 THEN &#x27;A&#x27; WHEN salary &gt; 15000 THEN &#x27;B&#x27; WHEN salary &gt; 10000 THEN &#x27;C&#x27; ELSE &#x27;D&#x27; END AS 工资等级 FROM employees 可以放在任何地方 #创建存储过程，根据传入的成绩，显示等级，90A,80B，70C，60D ，F DELIMITER $ CREATE PROCEDURE my_1(IN score INT) BEGIN CASE WHEN score BETWEEN 90 AND 100 THEN SELECT &#x27;A&#x27;; WHEN score BETWEEN 80 AND 90 THEN SELECT &#x27;B&#x27;; WHEN score BETWEEN 70 AND 80 THEN SELECT &#x27;C&#x27;; WHEN score BETWEEN 70 AND 60 THEN SELECT &#x27;D&#x27;; ELSE SELECT &#x27;E&#x27;; END CASE; END $CALL my_1(95); 3.if语法：IF 条件1 THEN 语句1;ELSEIF 条件2 THEN 语句2;…ELSE 语句n;END IF;只能用在begin end中 12345678910111213#创建存储过程，根据传入的成绩，返回等级，90A,80B，70C，60D ，FDELIMITER $ CREATE FUNCTION my_2( score INT) RETURNS CHAR BEGIN IF score &gt;=90 THEN RETURN&#x27;A&#x27;; ELSEIF score &gt;=80 THEN RETURN&#x27;B&#x27;; ELSEIF score &gt;=70 THEN RETURN&#x27;C&#x27;; ELSEIF score &gt;=60 THEN RETURN&#x27;D&#x27;; ELSE RETURN&#x27;E&#x27;; END IF; END $ SELECT my_2(85); 循环结构在存储过程或函数里面使用 1.while语法：标签:WHILE 循环条件 DO循环体;END WHILE 标签;循环控制和标签搭配使用 2.loop语法：标签： LOOP循环体;END LOOP 标签; 3.repeat语法：标签： REPEAT循环体;UNTIL 结束循环的条件END REPEAT 标签; 循环控制ITERATE 类似continueLEAVE 类似break left join==left outer join a left join b 就是取a和b的交集加a剩下的部分 inner join a inner join b就是取交集","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/tags/MySQL/"}]},{"title":"MySQL[二]概述","slug":"MySQL/MySQL[二]概述","date":"2022-01-10T13:58:17.078Z","updated":"2022-01-11T03:07:36.976Z","comments":true,"path":"2022/01/10/MySQL/MySQL[二]概述/","link":"","permalink":"https://yinhuidong.github.io/2022/01/10/MySQL/MySQL[%E4%BA%8C]%E6%A6%82%E8%BF%B0/","excerpt":"","text":"一，一条SQL的查询流程 去连接池获取连接 查询缓存，命中返回，否则继续向下 词法解析&amp;预处理 词法解析拆分SQL，语法分析检查SQL的正确性生成一颗解析树，预处理检查表名，列名，生成一颗解析树。 优化器优化，优化计划，查询计划 执行引擎生成执行计划 存储引擎查询SQL，加入缓存，返回结果。 1.获取连接MySQL支持多种通信协议，可以使用同步/异步的方式，支持长连接，短连接。​ 1.1 通信类型​ 一般来说，连接数据库都是同步连接。​ 同步连接：依赖于被调用方，受限制于被调用方的性能；一般只能一对一。 异步连接：避免阻塞，但不能节省SQL的执行时间，并发情况下，每个SQL的执行都要单独建立连接，占用大量CPU资源；异步连接必须使用连接池减少线程创建销毁的开销。 1.2 连接方式MySQL长短连接都支持，一般我们会在连接池中使用长连接。保持长连接会消耗内存，长时间不活动的连接，MySQL服务器会断开。​ 12show global variables like &#x27;wait_timeout&#x27;; -- 非交互式超时时间，如 JDBC 程序show global variables like &#x27;interactive_timeout&#x27;; -- 交互式超时时间，如数据库工具 默认长连接断开时间是8小时。 可以使用 show status;查看当前MySQL有多少个连接。 1show global status like &#x27;Thread%&#x27;; Threads_cached 缓存中的线程连接数 Threads_connected 当前打开的连接数 Threads_created 为处理连接创建的线程数 Threads_running 非睡眠状态的连接数，通常指并发连接数 每产生一个连接或者会话，服务端就会创建一个线程来处理。杀死会话本质就是kill 线程。​ 可以使用SHOW PROCESSLIST; （root 用户）查看 SQL 的执行状态。​ +—-+——+———–+——+———+——+———-+| Id | User | Host | db | Command | Time | State | Info |+—-+——+———–+——+———+——+———-+| 11 | root | localhost | NULL | Query | 0 | starting | show processlist |+—-+——+———–+——+———+——+———-+ 状态 含义 Sleep 线程正在等待客户端，以向它发送一个新语句 Query 线程正在执行查询或往客户端发送数据 Locked 该查询被其它查询锁定 Copying to tmp table on disk 临时结果集合大于 tmp_table_size。线程把临时表从存储器内部格式改变为磁盘模式，以节约存储器 Sending data 线程正在为 SELECT 语句处理行，同时正在向客户端发送数据 Sorting for group 线程正在进行分类，以满足 GROUP BY 要求 Sorting for order 线程正在进行分类，以满足 ORDER BY 要求 在5.7版本，MySQL的默认连接数是151个，我们最大可以修改为16384个 （214）。​ 123show variables like &#x27;max_connections&#x27;;set [global | session] max_connections =10000; 1.3 通信协议​ 编程语言的连接模块都是用 TCP 协议连接到 MySQL 服务器的，比如mysql-connector-java-x.x.xx.jar。 类unix系统上，支持 Socket套接字文件进行进程间通信。/tmp/mysql.sock windows系统上还支持命名管道和共享内存。 ​ 1.4 通信方式MySQL使用了半双工通信，所以客户端发送SQL语句给服务端的时候，不管SQL有多大，都是一次发过去的。​ 比如我们用MyBatis动态SQL生成了一个批量插入的语句，插入10万条数据，values后面跟了一长串的内容，或者 where 条件 in 里面的值太多，会出现问题。这个时候我们必须要调整 MySQL 服务器配置 max_allowed_packet 参数的值（默认是 4M），把它调大，否则就会报错。 对于服务端来说，也是一次性发送所有的数据，不能因为你已经取到了想要的数据就中断操作，这个时候会对网络和内存产生大量消耗。在程序里面避免不带 limit 的这种操作，比如一次把所有满足条件的数据全部查出来，一定要先 count 一下。如果数据量的话，可以分批查询。 2.查询缓存MySQL 的缓存默认是关闭的。​ 1show variables like &#x27;query_cache%&#x27;; MySQL不推荐使用自带的缓存，命中条件过于苛刻。且表里数据发生变化，整张表的缓存全部失效，MySQL8移除掉了缓存。 3.语法解析&amp;预处理3.1 词法解析词法分析就是把一个完整的 SQL 语句打碎成一个个的单词。​ 1select name from user where id =1; 它会打碎成 8 个符号，每个符号是什么类型，从哪里开始到哪里结束。​ 3.2 语法解析语法分析会对 SQL 做一些语法检查，比如单引号有没有闭合，然后根据 MySQL 定义的语法规则，根据 SQL 语句生成一个数据结构。这个数据结构我们把它叫做解析树（select_lex）。​ 任何数据库的中间件，比如 Mycat，Sharding-JDBC（用到了 Druid Parser），都必须要有词法和语法分析功能。 3.3 预处理​ 如果写了一个词法和语法都正确的 SQL，但是表名或者字段不存在，会在哪里报错？是在数据库的执行层还是解析器？​ 实际上还是在解析的时候报错，解析 SQL 的环节里面有个预处理器。它会检查生成的解析树，解决解析器无法解析的语义。比如，它会检查表和列名是否存在，检查名字和别名，保证没有歧义。预处理之后得到一个新的解析树。​ 4.查询优化&amp;查询执行计划一条SQL语句的执行方式有很多种，但是最终返回的结果都是相同的。查询优化器的目的就是根据解析树生成不同的执行计划（Execution Plan），然后选择一种最优的执行计划，MySQL 里面使用的是基于开销（cost）的优化器，那种执行计划开销最小，就用哪种。 12# 查看查询的开销show status like &#x27;Last_query_cost&#x27;; 4.1 优化器的作用 多表联查，以哪张表为基准表 用不用索引，用哪个索引 。。。。 ​ 4.2 优化器是怎么得到执行计划的 首先我们要启用优化器的追踪（默认是关闭的）。 开启这开关是会消耗性能的，因为它要把优化分析的结果写到表里面，所以不要轻易开启，或者查看完之后关闭它（改成 off）。 接着执行一个 SQL 语句，优化器会生成执行计划： 这个时候优化器分析的过程已经记录到系统表里面了，我们可以查询： 它是一个 JSON 类型的数据，主要分成三部分，准备阶段、优化阶段和执行阶段。 expanded_query 是优化后的 SQL 语句。 considered_execution_plans 里面列出了所有的执行计划。 分析完记得关掉它 通过追踪优化器，可以看到优化器对sql的初始优化，表的读取顺序，为什么采用了这种读取顺序。为什么采用了某个索引或者采用了全表查询。 4.3 优化器得到的结果​ 优化器最终会把解析树变成一个查询执行计划，查询执行计划是一个数据结构。 当然，这个执行计划是不是一定是最优的执行计划呢？不一定，因为 MySQL 也有可能覆盖不到所有的执行计划。​ MySQL 提供了一个执行计划的工具。我们在 SQL 语句前面加上 EXPLAIN，就可以看到执行计划的信息。​ Explain 的结果也不一定最终执行的方式。​ 4.4 选错索引这里错误决定分两类，第一，彻底错误。第二，基于成本最低，但执行速度不是最快。 由于InnoDB的 MVCC 功能和随机采样方式，默认随机采取几个数据页，当做总体数据。以部分代表整体，本来就有错误的风险。加上数据不断地添加过程中，索引树可能会分裂，结果更加不准确。 执行 ANALYZE TABLE ,可以重新构建索引，使索引树不过于分裂。 调整参数，加大InnoDB采样的页数，页数越大越精确，但性能消耗更高。一般不建议这么干。 在优化阶段，会对表中所有索引进行对比，优化器基于成本的原因，选择成本最低的索引，所以会错过最佳索引。带来的问题便是，执行速度很慢。 通过explain查看执行计划，结合sql条件查看可以利用哪些索引。 使用 force index(indexName)强制走指定索引。弊端就是后期若索引名发生改变，或索引被删除，该sql语句需要调整。 5. 存储引擎得到执行计划以后，SQL 语句是不是终于可以执行了？ 从逻辑的角度来说，我们的数据是放在哪里的，或者说放在一个什么结构里面？ 执行计划在哪里执行？是谁去执行？ 表在存储数据的同时，还要组织数据的存储结构，这个存储结构就是由存储引擎决定的，所以也可以把存储引擎叫做表类型。 在 MySQL 里面，支持多种存储引擎，他们是可以替换的，所以叫做插件式的存储引擎。​ 5.1 查看存储引擎我们数据库里面已经存在的表，我们怎么查看它们的存储引擎呢？​ 1show table status from `数据库名`; 或者通过 DDL 建表语句来查看。​ 在 MySQL 里面，我们创建的每一张表都可以指定它的存储引擎，而不是一个数据库只能使用一个存储引擎。存储引擎的使用是以表为单位的。而且，创建表之后还可以修改存储引擎。​ 一张表使用的存储引擎决定存储数据的结构，那在服务器上它们是怎么存储的呢？先要找到数据库存放数据的路径：默认情况下，每个数据库有一个自己文件夹，以 yhd数据库为例。任何一个存储引擎都有一个 frm 文件，这个是表结构定义文件。不同的存储引擎存放数据的方式不一样，产生的文件也不一样，innodb 是 1 个，memory 没有，myisam 是两个。 5.2 存储引擎比较①常见存储引擎MyISAM 和 InnoDB 是我们用得最多的两个存储引擎，在 MySQL 5.5 版本之前，默认的存储引擎是 MyISAM，它是 MySQL 自带的。 5.5 版本之后默认的存储引擎改成了 InnoDB，最主要的原因还是 InnoDB 支持事务，支持行级别的锁，对于业务一致性要求高的场景来说更适合。 ②数据库支持的存储引擎可以用这个命令查看数据库对存储引擎的支持情况： 1show engines ; 其中有存储引擎的描述和对事务、XA 协议和 Savepoints 的支持。 XA 协议用来实现分布式事务（分为本地资源管理器，事务管理器）。 Savepoints 用来实现子事务（嵌套事务）。创建了一个 Savepoints 之后，事务就可以回滚到这个点，不会影响到创建 Savepoints 之前的操作。 ③MyISAM（3 个文件）应用范围比较小。表级锁定限制了读/写的性能，因此在 Web 和数据仓库配置中，它通常用于只读或以读为主的工作。 特点 支持表级别的锁（插入和更新会锁表）。不支持事务。 拥有较高的插入（insert）和查询（select）速度。 存储了表的行数（count 速度更快）。 适合：只读之类的数据分析的项目。 ④InnoDB（2个文件）mysql 5.7 中的默认存储引擎。InnoDB 是一个事务安全（与 ACID 兼容）的 MySQL存储引擎，它具有提交、回滚和崩溃恢复功能来保护用户数据。InnoDB 行级锁（不升级为更粗粒度的锁）和 Oracle 风格的一致非锁读提高了多用户并发性和性能。InnoDB 将用户数据存储在聚集索引中，以减少基于主键的常见查询的 I/O。为了保持数据完整性，InnoDB 还支持外键引用完整性约束。 特点 支持事务，支持外键，因此数据的完整性、一致性更高。 支持行级别的锁和表级别的锁。 支持读写并发，写不阻塞读（MVCC）。 特殊的索引存放方式，可以减少 IO，提升查询效率。 适合：经常更新的表，存在并发读写或者有事务处理的业务系统。 ⑤Memory(1个文件)基于内存的存储引擎。 特征： 基于内存的表，服务器重启后，表结构会被保留，但表中的数据会被清空。 不需要进行磁盘IO，比 MYISAM 快了一个数量级。 表级锁，故并发插入性能较低。 每一行是固定的，VARCHAR 列在 memory 存储引擎中会变成 CHAR，可能导致内存浪费。 不支持 BLOB 或 TEXT 列，如果sql返回的结果列中包含 BLOB 或 TEXT，就直接采用 MYISAM 存储引擎，在磁盘上建临时表 支持哈希索引，B+树索引 MEMORY 存储引擎在很多地方可以发挥很好的作用： 用于查找或映射表，例如邮编和州名的映射表 用于缓存周期性聚合数据的结果 用于保存数据分析中产生的中间结果。即SQL执行过程中用到的临时表 监控MySQL内存中的执行情况，例如：information_schema 库下的表基本都是 memory 存储引擎，监控InnoDB缓冲池中page(INNODB_BUFFER_PAGE表)，InnoDB缓冲池状态(INNODB_BUFFER_POOL_STATS表)、InnoDB缓存页淘汰记录(INNODB_BUFFER_PAGE_LRU表)、InnoDB锁等待(INNODB_LOCK_WAITS表)、InnoDB锁信息(INNODB_LOCKS表)、InnoDB中正在执行的事务(INNODB_TRX表)等。 MEMORY 存储引擎默认 hash 索引，故等值查询特别快。同时也支持B+树索引。虽然查询速度特别快，但依旧无法取代传统的磁盘建表。 ⑥CSV(3个文件)它的表实际上是带有逗号分隔值的文本文件。csv表允许以csv格式导入或转储数据，以便与读写相同格式的脚本和应用程序交换数据。因为 csv 表没有索引，所以通常在正常操作期间将数据保存在 innodb 表中，并且只在导入或导出阶段使用 csv 表。 特点 不允许空行，不支持索引。格式通用，可以直接编辑，适合在不同数据库之间导入导出。​ 5.3 如何选择存储引擎 如果对数据一致性要求比较高，需要事务支持，可以选择 InnoDB。 如果数据查询多更新少，对查询性能要求比较高，可以选择 MyISAM。 如果需要一个用于查询的临时表，可以选择 Memory。 ​ 6.执行引擎执行引擎，它利用存储引擎提供的相应的 API 来完成操作。 为什么我们修改了表的存储引擎，操作方式不需要做任何改变？因为不同功能的存储引擎实现的 API 是相同的。 最后把数据返回给客户端，即使没有结果也要返回。​ 二，一条SQL的更新流程更新和查询很多地方并没有区别，仅仅在于拿到数据之后的操作。 1.内存结构InnnoDB 的数据都是放在磁盘上的，InnoDB 操作数据有一个最小的逻辑单位，叫做页（索引页和数据页）。我们对于数据的操作，不是每次都直接操作磁盘，因为磁盘的速度太慢了。InnoDB 使用了一种缓冲池的技术，也就是把磁盘读到的页放到一块内存区域里面。这个内存区域就叫 Buffer Pool。​ 下一次读取相同的页，先判断是不是在缓冲池里面，如果是，就直接读取，不用再次访问磁盘。 修改数据的时候，先修改缓冲池里面的页。内存的数据页和磁盘数据不一致的时候，我们把它叫做脏页。InnoDB 里面有专门的后台线程把 Buffer Pool 的数据写入到磁盘，每隔一段时间就一次性地把多个修改写入磁盘，这个动作就叫做刷脏。 Buffer Pool 是 InnoDB 里面非常重要的一个结构，主要分为 3 个部分： Buffer Pool、Change Buffer、Adaptive HashIndex，另外还有一个（redo）log buffer。​ 1.1 buffer poolBuffer Pool 缓存的是页信息，包括数据页、索引页，默认大小是 128M（134217728 字节），可以调整。 查看服务器状态，里面有很多跟 Buffer Pool 相关的信息： 1SHOW STATUS LIKE &#x27;%innodb_buffer_pool%&#x27;; 查看参数（系统变量）： 1SHOW VARIABLES like &#x27;%innodb_buffer_pool%&#x27;; 内存的缓冲池写满了怎么办？（Redis 设置的内存满了怎么办？）InnoDB 用 LRU算法来管理缓冲池（链表实现，不是传统的 LRU，分成了 young 和 old），经过淘汰的数据就是热点数据。 内存缓冲区对于提升读写性能有很大的作用。当需要更新一个数据页时，如果数据页在 Buffer Pool 中存在，那么就直接更新好了。否则的话就需要从磁盘加载到内存，再对内存的数据页进行操作。也就是说，如果没有命中缓冲池，至少要产生一次磁盘 IO。​ 1.2 ChangeBuffer写缓冲如果这个数据页不是唯一索引，不存在数据重复的情况，也就不需要从磁盘加载索引页判断数据是不是重复（唯一性检查）。这种情况下可以先把修改记录在内存的缓冲池中，从而提升更新语句（Insert、Delete、Update）的执行速度。 这一块区域就是 Change Buffer。5.5 之前叫 Insert Buffer 插入缓冲，现在也能支持 delete 和 update。 最后把 Change Buffer 记录到数据页的操作叫做 merge。什么时候发生 merge？有几种情况：在访问这个数据页的时候，或者通过后台线程、或者数据库 shut down、redo log 写满时触发。 如果数据库大部分索引都是非唯一索引，并且业务是写多读少，不会在写数据后立刻读取，就可以使用 Change Buffer（写缓冲）。写多读少的业务，调大这个值： 1SHOW VARIABLES LIKE &#x27;innodb_change_buffer_max_size&#x27;; 代表 Change Buffer 占 Buffer Pool 的比例，默认 25%。​ 1.3 Adaptive Hash Index当我们需要访问某个页中的数据时，就会把该页从磁盘加载到Buffer Pool中，如果该页已经在Buffer Pool中的话直接使用就可以了。那么问题也就来了，我们怎么知道该页在不在Buffer Pool中呢？ 我们其实是根据表空间号 + 页号来定位一个页的，也就相当于表空间号 + 页号是一个key，缓存页就是对应的value，怎么通过一个key来快速找着一个value呢？那肯定是哈希表。 所以我们可以用表空间号 + 页号作为key，缓存页作为value创建一个哈希表，在需要访问某个页的数据时，先从哈希表中根据表空间号 + 页号看看有没有对应的缓存页，如果有，直接使用该缓存页就好，如果没有，那就从free链表中选一个空闲的缓存页，然后把磁盘中对应的页加载到该缓存页的位置。​ 1.4 （redo）Log Buffer​ 如果 Buffer Pool 里面的脏页还没有刷入磁盘时，数据库宕机或者重启，这些数据丢失。如果写操作写到一半，甚至可能会破坏数据文件导致数据库不可用。为了避免这个问题，InnoDB 把所有对页面的修改操作专门写入一个日志文件，并且在数据库启动时从这个文件进行恢复操作（实现 crash-safe）——用它来实现事务的持久性。​ 这个文件就是磁盘的 redo log（叫做重做日志），对应于/var/lib/mysql/目录下的ib_logfile0 和 ib_logfile1，每个 48M。这 种 日 志 和 磁 盘 配 合 的 整 个 过 程 ， 其 实 就 是 MySQL 里 的 WAL 技 术（Write-Ahead Logging），它的关键点就是先写日志，再写磁盘。 1show variables like &#x27;innodb_log%&#x27;; 值 含义 innodb_log_file_size 指定每个文件的大小，默认 48M innodb_log_files_in_group 指定文件的数量，默认为 2 innodb_log_group_home_dir 指定文件所在路径，相对或绝对。如果不指定，则为datadir 路径。 同样是写磁盘，为什么不直接写到 db file 里面去？为什么先写日志再写磁盘？ 磁盘的最小组成单元是扇区，通常是 512 个字节。操作系统和内存打交道，最小的单位是页 Page。操作系统和磁盘打交道，读写磁盘，最小的单位是块 Block。​ 如果我们所需要的数据是随机分散在不同页的不同扇区中，那么找到相应的数据需要等到磁臂旋转到指定的页，然后盘片寻找到对应的扇区，才能找到我们所需要的一块数据，依次进行此过程直到找完所有数据，这个就是随机 IO，读取数据速度较慢。 假设我们已经找到了第一块数据，并且其他所需的数据就在这一块数据后边，那么就不需要重新寻址，可以依次拿到我们所需的数据，这个就叫顺序 IO。 刷盘是随机 I/O，而记录日志是顺序 I/O，顺序 I/O 效率更高。因此先把修改写入日志，可以延迟刷盘时机，进而提升系统吞吐。 当然 redo log 也不是每一次都直接写入磁盘，在 Buffer Pool 里面有一块内存区域（Log Buffer）专门用来保存即将要写入日志文件的数据，默认 16M，它一样可以节省磁盘 IO。​ 1SHOW VARIABLES LIKE &#x27;innodb_log_buffer_size&#x27;; redo log 的内容主要是用于崩溃恢复。磁盘的数据文件，数据来自 buffer pool。redo log 写入磁盘，不是写入数据文件。 那么，Log Buffer 什么时候写入 log file？ 在我们写入数据到磁盘的时候，操作系统本身是有缓存的。flush 就是把操作系统缓冲区写入到磁盘。 log buffer 写入磁盘的时机，由一个参数控制，默认是 1。 1SHOW VARIABLES LIKE &#x27;innodb_flush_log_at_trx_commit&#x27;; 值 含义 0（延迟写） log buffer 将每秒一次地写入 log file 中，并且 log file 的 flush 操作同时进行。该模式下，在事务提交的时候，不会主动触发写入磁盘的操作。 1（默认，实时写，实时刷） 每次事务提交时 MySQL 都会把 log buffer 的数据写入 log file，并且刷到磁盘中去。 2（实时写，延迟刷） 每次事务提交时 MySQL 都会把 log buffer 的数据写入 log file。但是 flush 操作并不会同时进行。该模式下，MySQL 会每秒执行一次 flush 操作。 redo log，它又分成内存和磁盘两部分。redo log 有什么特点？ redo log 是 InnoDB 存储引擎实现的，并不是所有存储引擎都有。 不是记录数据页更新之后的状态，而是记录这个页做了什么改动，属于物理日志。（redo log 记录的是执行的结果） redo log 的大小是固定的，前面的内容会被覆盖。 check point 是当前要覆盖的位置。如果 write pos 跟 check point 重叠，说明 redolog 已经写满，这时候需要同步 redo log 到磁盘中。 这是 MySQL 的内存结构，总结一下，分为：Buffer pool、change buffer、Adaptive Hash Index、 log buffer。 磁盘结构里面主要是各种各样的表空间，叫做 Table space。 1.5 缓存的疑问缓存（cache）是在读取硬盘中的数据时，把最常用的数据保存在内存的缓存区中，再次读取该数据时，就不去硬盘中读取了，而在缓存中读取。缓冲（buffer）是在向硬盘写入数据时，先把数据放入缓冲区,然后再一起向硬盘写入，把分散的写操作集中进行，减少磁盘碎片和硬盘的反复寻道，从而提高系统性能。 然后，InnoDB架构中，有非常重要的一个部分——缓冲池。该缓冲池需要占用服务器内存，且专用于MySQL的服务器，建议把80%的内存交给MySQL。 缓冲池有一个缓存的功能。这个缓存，是InnoDB自带的，而且经常会用到。该缓存功能并不是MySQL架构中的缓存组件。这是两者最大的区别。 MySQL组件中的缓存 所处位置：MySQL架构中的缓存组件 缓存内容：缓存的是SQL 和 该SQL的查询结果。如果SQL的大小写，格式，注释不一致，则被认为是不同的SQL，重新查询数据库，并缓存一份数据。 可否关闭：是可以手动关闭，并卸载该组件的。 InnoDB中的缓存 所处位置：InnoDB架构中的缓冲池 缓存内容：缓存的是所有需要查找的数据，所在的数据页。 可否关闭：是InnoDB缓冲池自带的功能，无法关闭，无法卸载。如果InnoDB的缓冲池被关闭或卸载，则InnoDB直接瘫痪。所以说缓冲池是InnoDB的最重要的一部分。 不建议使用MySQL的缓存是指，不建议使用MySQL架构中的缓存组件，并不是同时否定了InnoDB中的缓存功能。​ 2.磁盘结构表空间可以看做是 InnoDB 存储引擎逻辑结构的最高层，所有的数据都存放在表空间中。InnoDB 的表空间分为 5 大类。​ 2.1 系统表空间在默认情况下 InnoDB 存储引擎有一个共享表空间（对应文件/var/lib/mysql/ibdata1），也叫系统表空间。 InnoDB 系统表空间包含 InnoDB 数据字典和双写缓冲区，（Change Buffer 和 UndoLogs），如果没有指定 file-per-table，也包含用户创建的表和索引数据。 undo 在后面介绍，因为有独立的表空间。 数据字典：由内部系统表组成，存储表和索引的元数据（定义信息）。 双写缓冲（InnoDB 的一大特性） InnoDB 的页和操作系统的页大小不一致，InnoDB 页大小一般为 16K，操作系统页大小为 4K，InnoDB 的页写入到磁盘时，一个页需要分 4 次写。 如果存储引擎正在写入页的数据到磁盘时发生了宕机，可能出现页只写了一部分的情况，比如只写了 4K，就宕机了，这种情况叫做部分写失效（partial page write），可能会导致数据丢失。 1show variables like &#x27;innodb_doublewrite&#x27;; 如果这个页本身已经损坏了，用它来做崩溃恢复是没有意义的。所以在对于应用 redo log 之前，需要一个页的副本。如果出现了写入失效，就用页的副本来还原这个页，然后再应用 redo log。这个页的副本就是 double write，InnoDB 的双写技术。通过它实现了数据页的可靠性。 跟 redo log 一样，double write 由两部分组成，一部分是内存的 double write，一个部分是磁盘上的 double write。因为 double write 是顺序写入的，不会带来很大的开销。 在MySQL5.7之前，所有的表共享一个系统表空间，这个文件会越来越大，而且它的空间不会收缩。​ 2.2 独占表空间我们可以让每张表独占一个表空间。这个开关通过 innodb_file_per_table 设置，默认开启。 1SHOW VARIABLES LIKE &#x27;innodb_file_per_table&#x27;; 开启后，则每张表会开辟一个表空间，这个文件就是数据目录下的 ibd 文件，存放表的索引和数据。但是其他类的数据，如回滚（undo）信息，插入缓冲索引页、系统事务信息，二次写缓冲（Double write buffer）等还是存放在原来的共享表空间内。​ 2.3 通用表空间通用表空间也是一种共享的表空间，跟 ibdata1 类似。 可以创建一个通用的表空间，用来存储不同数据库的表，数据路径和文件可以自定义。语法： 1create tablespace ts2673 add datafile &#x27;/var/lib/mysql/ts2673.ibd&#x27; file_block_size=16K engine=innodb; 在创建表的时候可以指定表空间，用 ALTER 修改表空间可以转移表空间。 1create table t2673(id integer) tablespace ts2673; 不同表空间的数据是可以移动的。删除表空间需要先删除里面的所有表： 12drop table t2673;drop tablespace ts2673; 2.4 临时表空间存储临时表的数据，包括用户创建的临时表，和磁盘的内部临时表。对应数据目录下的 ibtmp1 文件。当数据服务器正常关闭时，该表空间被删除，下次重新产生。 memory向template的过渡，还有磁盘上简历临时表用的什么存储引擎？ 8.0之前，内存临时表用Memory引擎创建，但假如字段中有BLOB或TEXT,或结果太大，就会转用MYISM在磁盘上建表，8.0之后内存临时表由MEMORY引擎更改为TempTable引擎，相比于前者，后者支持以变长方式存储VARCHAR，VARBINARY等变长字段。从MySQL 8.0.13开始，TempTable引擎支持BLOB字段。如果超过内存表大小，则用InnoDB建表。 2.5 redo log2.6 undo log 表空间undo log（撤销日志或回滚日志）记录了事务发生之前的数据状态（不包括 select）。 如果修改数据时出现异常，可以用 undo log 来实现回滚操作（保持原子性）。 在执行 undo 的时候，仅仅是将数据从逻辑上恢复至事务之前的状态，而不是从物理页面上操作实现的，属于逻辑格式的日志(记录操作)。 redo Log 和 undo Log 与事务密切相关，统称为事务日志。 undo Log 的数据默认在系统表空间 ibdata1 文件中，因为共享表空间不会自动收缩，也可以单独创建一个 undo 表空间。 1show global variables like &#x27;%undo%&#x27;; 2.7 一条SQL的更新流程12# id =1 的记录原 name = &#x27;yhd&#x27;update user set name = &#x27;二十&#x27; where id=1; 事务开始，从内存或者磁盘取到这条数据，返回给server的执行器 执行器修改这一行数据的值为二十 记录name =yhd 到undo log 记录name = 二十 到redo log 调用存储引擎接口，在buffer pool 中修改 name =二十 事务提交 ​ 内存和磁盘之间，工作着很多后台线程。 3.后台线程后台线程的主要作用是负责刷新内存池中的数据和把修改的数据页刷新到磁盘。后台线程分为：master线程，IO 线程，purge 线程，page cleaner 线程。​ 3.1 Master 线程Master Thread是InnoDB存储引擎非常核心的一个后台线程，主要负责将缓冲池中的数据异步刷新到磁盘，保证数据的一致性，包括脏页的刷新、合并插入缓冲、UNDO页的回收等。​ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546void master_thread()&#123; loop: for(int i = 0; i &lt; 10; ++i)&#123; thread_sleep(1); // sleep 1秒 do log buffer flush to disk; if(last_one_second_ios &lt; 5) do merge at most 5 insert buffer; if(buf_get_modified_ratio_pct &gt; innodb_max_dirty_pages_pct) // 如果缓冲池中的脏页比例大于innodb_max_dirty_pages_pct(默认是75时) do buffer pool flush 100 dirty page; // 刷新100脏页到磁盘 if(no user activity) goto backgroud loop; &#125; if(last_ten_second_ios &lt; 200) // 如果过去10内磁盘IO次数小于设置的innodb_io_capacity的值（默认是200） do buffer pool flush 100 dirty page; do merge at most 5 insert buffer; // 合并插入缓冲是innodb_io_capacity的5%（10）（总是） do log buffer flush to disk; do full purge; if(buf_get_modified_ratio_pct &gt; 70%) do buffer pool flush 100 dirty page; else buffer pool flush 10 dirty page; backgroud loop： // 后台循环 do full purge // 删除无用的undo页 （总是） do merge 20 insert buffer; // 合并插入缓冲是innodb_io_capacity的5%（10）（总是） if not idle // 如果不空闲，就跳回主循环，如果空闲就跳入flush loop goto loop: // 跳到主循环 else goto flush loop flush loop: // 刷新循环 do buffer pool flush 100 dirty page; if(buf_get_modified_ratio_pct &gt; innodb_max_dirty_pages_pct) // 如果缓冲池中的脏页比例大于innodb_max_dirty_pages_pct的值（默认75%） goto flush loop; // 跳到刷新循环，不断刷新脏页，直到符合条件 goto suspend loop; // 完成刷新脏页的任务后，跳入suspend loop suspend loop: suspend_thread(); //master线程挂起，等待事件发生 waiting event; goto loop;&#125; Master Thread具有最高的线程优先级别。内部由多个循环组成：主循环（loop）、后台循环（backgroup loop）、刷新循环（flush loop）、暂停循环（suspend loop）。Master Thread会根据数据库运行的状态在loop、backgroup loop、flush loop和suspend loop中进行切换。loop是主循环，大多数的操作都在这个循环中，主要有两大部分的操作——每秒钟的操作和每10秒钟的操作。​ ①每秒钟的操作​ ​日志缓冲刷新到磁盘，即使这个事务还没有提交（总是）；即使某个事务还没有提交，InnoDB存储引擎仍然每秒会将重做日志缓冲中的内容刷新到重做日志文件。这也解释了为什么再大的事务提交的时间也是很短的。 合并插入缓冲（可能）；合并插入缓冲并不是每秒都会发生的。InnoDB存储引擎会判断当前一秒内发生的IO次数是否小于5次，如果小于5次，InnoDB存储引擎认为当前的IO压力很小，可以执行合并插入缓冲的操作； 至多刷新100个InnoDB的缓冲池中的脏页到磁盘（可能）； 刷新100个脏页也不是每秒都会发生的，InnoDB存储引擎通过判断当前缓冲池中脏页的比例(buf_get_modified_ratio_pct)是否超过了配置文件中 innodb_max_dirty_pages_pct这个参数（默认是75，代表75%），如果超过了这个值，InnoDB存储引擎则认为需要做磁盘同步的操作，将100个脏页写入磁盘中。 如果当前没有用户活动，则切换到background loop(可能)。 ​ ②每十秒的操作 刷新100个脏页到磁盘（可能） InnoDB存储引擎会先判断过去10秒之内磁盘的IO操作是否小于200次，如果是，InnoDB存储引擎认为当前有足够的磁盘IO能力，因此将100个脏页刷新到磁盘。 合并至多5个插入缓冲（总是） 将日志缓冲刷新到磁盘（总是） 删除无用的Undo页（总是） 刷新100个或者10个脏页到磁盘（总是） InnoDB存储引擎会执行full purge操作，即删除无用的Undo页。对表进行update，delete这类的操作时，原先的行被标记为删除，但是因为一致性读的关系，需要保留这些行版本的信息。但是在full purge过程中，InnoDB存储引擎会判断当前事务系统中已被删除的行是否可以删除，比如有时候可能还有查询操作需要读取之前版本的undo信息，如果可以删除，InnoDB存储引擎会立即将其删除。从源代码中可以看出，InnoDB存储引擎在执行full purge 操作时，每次最多尝试回收20个undo页。然后，InnoDB存储引擎会判断缓冲池中脏页的比例（buf_get_modified_ratio_pct）,如果有超过70%的脏页，则刷新100个脏页到磁盘，如果脏页的比例小于70%,则只需刷新10%的脏页到磁盘。 ​ 如果当前没有用户活动（数据库空闲）或者数据库关系，就会切换到backgroud loop这个循环。 backgroud loop会执行以下操作： 删除无用的Undo页（总是） 合并20个插入缓冲（总是） 跳回到主循环（总是） 不断刷新100个页直到符合条件（可能，需要跳转到flush loop中完成） 如果flush loop中也没有什么事情可以做了，InnoDB存储引擎会切换到suspend_loop，将Master Thread挂起，等待事件的发生。若用户启用了InnoDB存储引擎，却没有使用任何InnoDB存储引擎的表，那么Master Thread总是处于挂起的状态。​ 1.0.x版本中，InnoDB存储引擎最多只会刷新100个脏页到磁盘，合并20个插入缓冲。如果是在写入密集的应用程序中，每秒可能会产生大于100个的脏页，如果是产生大于20个插入缓冲的情况，那么可能会来不及刷新所有的脏页以及合并插入缓冲。后来，InnoDB存储引擎提供了参数innodb_io_capacity，用来表示磁盘IO的吞吐量，默认值为200。​ 对于刷新到磁盘的页的数量，会按照innodb_io_capacity的百分比来进行控制。规则如下： 在合并插入缓冲时，合并插入缓冲的数量为innodb_io_capacity值的5%; 在从缓冲区刷新脏页时，刷新脏页的数量为innodb_io_capacity; 如果用户使用的是SSD类的磁盘，可以将innodb_io_capacity的值调高，直到符合磁盘IO的吞吐量为止； 另一个问题是参数innodb_max_dirty_pages_pct的默认值，在1.0.x版本之前，该值的默认值是90，意味着脏页占缓冲池的90%。InnoDB存储引擎在每秒刷新缓冲池和flush loop时会判断这个值，如果该值大于innodb_max_dirty_pages_pct,才会刷新100个脏页，如果有很大的内存，或者数据库服务器的压力很大，这时刷新脏页的速度反而会降低。 后来将innodb_max_dirty_pages_pct的默认值改为了75。这样既可以加快刷新脏页的频率，又能够保证磁盘IO的负载。​ 还有一个新的参数是innodb_adaptive_flushing(自适应地刷新)，该值影响每秒刷新脏页的数量。原来的刷新规则是：脏页在缓冲池所占的比例小于innodb_max_dirty_pages_pct时，不刷新脏页；大于innodb_max_dirty_pages_pct时，刷新100个脏页。随着innodb_adaptive_flushing参数的引入，InnoDB通过一个名为buf_flush_get_desired_flush_rate的函数来判断需要刷新脏页最合适的数量。buf_flush_get_desired_flush_rate函数通过判断产生重做日志的速率来决定最合适的刷新脏页数量。 之前每次进行full purge 操作时，最多回收20个Undo页，从InnoDB 1.0.x版本开始引入了参数innodb_purge_batch_size,该参数可以控制每次full purge回收的Undo页的数量。该参数的默认值为20，并可以动态地对其进行修改。​ 1.2.x版本中再次对Master Thread进行了优化，对于刷新脏页的操作，从Master Thread线程分离到一个单独的Page Cleaner Thread，从而减轻了Master Thread的工作，同时进一步提高了系统的并发性。​ 3.2 IO 线程InnoDB中大量使用AIO (Async IO) 来处理IO请求。IO Thread的作用，是负责这些 IO 请求的回调（call back）。​ 3.3 Purge 线程事务被提交后，其所使用的undo log可能不在需要。因此，需要purge thread来回收已经使用并分配的undo页。以前Master Thread来完成释放undo log，InnoDB1.1独立出来，分担主线程压力。​ 3.4 Page Cleaner 线程​ 负责将脏页刷新到磁盘。以前Master Thread来刷新脏页，InnoDB1.2独立出来，分担主线程压力。​ 除了 InnoDB 架构中的日志文件，MySQL 的 Server 层也有一个日志文件，叫做binlog，它可以被所有的存储引擎使用。 4.binlogbinlog 以事件的形式记录了所有的DDL 和DML 语句（因为它记录的是操作而不是数据值，属于逻辑日志），可以用来做主从复制和数据恢复。跟redo log不一样，它的文件内容是可以追加的，没有固定大小限制。在开启了 binlog 功能的情况下，我们可以把 binlog 导出成 SQL 语句，把所有的操作重放一遍，来实现数据的恢复。binlog 的另一个功能就是用来实现主从复制，它的原理就是从服务器读取主服务器的 binlog，然后执行一遍。​ 有了这两个日志之后，来看一下一条更新语句是怎么执行的：​ 12# id =1 的记录原 name = &#x27;yhd&#x27;update user set name = &#x27;二十&#x27; where id=1; ​ 事务开始，从内存或者磁盘取到这条数据所在的数据页，返回给server的执行器 执行器修改这一行数据的值为二十 记录name =yhd 到undo log 在buffer pool 中修改 name =二十，此时该页变成脏页 记录name = 二十 到redo log buffer，redo log buffer每秒刷盘。 redo log 进入prepare状态，然后告诉执行器，执行完成了，可以随时提交 写入binlog 事务提交，并回写最终状态到redo log里，代表该事务已经提交 ​ 事务开始之后就产生redo log，redo log的落盘并不是随着事务的提交才写入的，而是在事务的执行过程中，便不断写入redo log文件中。一般情况下，每次事务commit时，必须调用 fsync 操作，将redo日志缓冲同步写到磁盘。另外，每次事务提交时同步写到磁盘bin log中。 那么就有了一个谁先谁后的问题：redo log 先，bin log 后。 两阶段提交的内容：**事务提交时，redo log处于 pre状态 -&gt; 写入bin log -&gt; 事务真正提交。 ** 当发生崩溃恢复时，查看的是bin log是否完整，如果bin log完整，则代表事务已经提交。 如果在两阶段提交过程中，bin log写入失败，则事务无法终止提交，崩溃恢复时就不需要重做。如果bin log写完的一瞬间，服务器宕机了，事务都来不及提交，此时bin log并不是完整的，缺少了最终的commit标记。因此也是提交失败。 简单说，redo log和bin log都可以用于表示事务的提交状态，而两阶段提交就是让这两个状态保持逻辑上的一致。 三，MySQL中支持的字符集和排序规则1.MySQL中的utf8和utf8mb4utf8字符集表示一个字符需要使用1～4个字节，但是我们常用的一些字符使用1～3个字节就可以表示了。而在MySQL中字符集表示一个字符所用最大字节长度在某些方面会影响系统的存储和性能，所以设计MySQL的大叔偷偷的定义了两个概念： utf8mb3：阉割过的utf8字符集，只使用1～3个字节表示字符。 utf8mb4：正宗的utf8字符集，使用1～4个字节表示字符。 在MySQL中utf8是utf8mb3的别名，所以之后在MySQL中提到utf8就意味着使用1~3个字节来表示一个字符，如果有使用4字节编码一个字符的情况，比如存储一些emoji表情啥的，那请使用utf8mb4。 查看字符集：SHOW (CHARACTER SET|CHARSET)。 2.字符集&amp;比较规则的应用2.1 各级别的字符集和比较规则MySQL有4个级别的字符集和比较规则，分别是： 服务器级别 数据库级别 表级别 列级别 接下来仔细看一下怎么设置和查看这几个级别的字符集和比较规则。 服务器级别123456789101112131415mysql&gt; SHOW VARIABLES LIKE &#x27;character_set_server&#x27;;+----------------------+-------+| Variable_name | Value |+----------------------+-------+| character_set_server | utf8 |+----------------------+-------+1 row in set (0.00 sec)mysql&gt; SHOW VARIABLES LIKE &#x27;collation_server&#x27;;+------------------+-----------------+| Variable_name | Value |+------------------+-----------------+| collation_server | utf8_general_ci |+------------------+-----------------+1 row in set (0.00 sec) 可以在启动服务器程序时通过启动选项或者在服务器程序运行过程中使用SET语句修改这两个变量的值。比如我们可以在配置文件中这样写： 123[server]character_set_server=gbkcollation_server=gbk_chinese_ci 当服务器启动的时候读取这个配置文件后这两个系统变量的值便修改了。 数据库级别我们在创建和修改数据库的时候可以指定该数据库的字符集和比较规则，具体语法如下： 1234567CREATE DATABASE 数据库名 [[DEFAULT] CHARACTER SET 字符集名称] [[DEFAULT] COLLATE 比较规则名称];ALTER DATABASE 数据库名 [[DEFAULT] CHARACTER SET 字符集名称] [[DEFAULT] COLLATE 比较规则名称]; 其中的DEFAULT可以省略，并不影响语句的语义。比方说我们新创建一个名叫charset_demo_db的数据库，在创建的时候指定它使用的字符集为gb2312，比较规则为gb2312_chinese_ci： 1234mysql&gt; CREATE DATABASE charset_demo_db -&gt; CHARACTER SET gb2312 -&gt; COLLATE gb2312_chinese_ci;Query OK, 1 row affected (0.01 sec) 查看 1234567891011121314151617181920mysql&gt; USE charset_demo_db;Database changedmysql&gt; SHOW VARIABLES LIKE &#x27;character_set_database&#x27;;+------------------------+--------+| Variable_name | Value |+------------------------+--------+| character_set_database | gb2312 |+------------------------+--------+1 row in set (0.00 sec)mysql&gt; SHOW VARIABLES LIKE &#x27;collation_database&#x27;;+--------------------+-------------------+| Variable_name | Value |+--------------------+-------------------+| collation_database | gb2312_chinese_ci |+--------------------+-------------------+1 row in set (0.00 sec)mysql&gt; 可以看到这个charset_demo_db数据库的字符集和比较规则就是我们在创建语句中指定的。需要注意的一点是： character_set_database 和 _collation_database_ 这两个系统变量是只读的，我们不能通过修改这两个变量的值而改变当前数据库的字符集和比较规则。 表级别我们也可以在创建和修改表的时候指定表的字符集和比较规则，语法如下： 1234567CREATE TABLE 表名 (列的信息) [[DEFAULT] CHARACTER SET 字符集名称] [COLLATE 比较规则名称]]ALTER TABLE 表名 [[DEFAULT] CHARACTER SET 字符集名称] [COLLATE 比较规则名称] 比方说我们在刚刚创建的charset_demo_db数据库中创建一个名为t的表，并指定这个表的字符集和比较规则： 1234mysql&gt; CREATE TABLE t( -&gt; col VARCHAR(10) -&gt; ) CHARACTER SET utf8 COLLATE utf8_general_ci;Query OK, 0 rows affected (0.03 sec) 如果创建和修改表的语句中没有指明字符集和比较规则，将使用该表所在数据库的字符集和比较规则作为该表的字符集和比较规则。 列级别需要注意的是，对于存储字符串的列，同一个表中的不同的列也可以有不同的字符集和比较规则。我们在创建和修改列定义的时候可以指定该列的字符集和比较规则，语法如下： 123456CREATE TABLE 表名( 列名 字符串类型 [CHARACTER SET 字符集名称] [COLLATE 比较规则名称], 其他列...);ALTER TABLE 表名 MODIFY 列名 字符串类型 [CHARACTER SET 字符集名称] [COLLATE 比较规则名称]; 比如我们修改一下表t中列col的字符集和比较规则可以这么写： 12345mysql&gt; ALTER TABLE t MODIFY col VARCHAR(10) CHARACTER SET gbk COLLATE gbk_chinese_ci;Query OK, 0 rows affected (0.04 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; 对于某个列来说，如果在创建和修改的语句中没有指明字符集和比较规则，将使用该列所在表的字符集和比较规则作为该列的字符集和比较规则。 在转换列的字符集时需要注意，如果转换前列中存储的数据不能用转换后的字符集进行表示会发生错误。比方说原先列使用的字符集是utf8，列中存储了一些汉字，现在把列的字符集转换为ascii的话就会出错，因为ascii字符集并不能表示汉字字符。 2.2 客户端和服务器通信中的字符集编码和解码使用的字符集不一致的后果如果对于同一个字符串编码和解码使用的字符集不一样，会产生意想不到的结果，作为人类的我们看上去就像是产生了乱码一样。 从发送请求到接收结果过程中发生的字符集转换 客户端使用操作系统的字符集编码请求字符串，向服务器发送的是经过编码的一个字节串。 服务器将客户端发送来的字节串采用character_set_client代表的字符集进行解码，将解码后的字符串再按照character_set_connection代表的字符集进行编码。 如果character_set_connection代表的字符集和具体操作的列使用的字符集一致，则直接进行相应操作，否则的话需要将请求中的字符串从character_set_connection代表的字符集转换为具体操作的列使用的字符集之后再进行操作。 将从某个列获取到的字节串从该列使用的字符集转换为character_set_results代表的字符集后发送到客户端。 客户端使用操作系统的字符集解析收到的结果集字节串。 在这个过程中各个系统变量的含义如下： 系统变量 描述 character_set_client 服务器解码请求时使用的字符集 character_set_connection 服务器处理请求时会把请求字符串从character_set_client转为character_set_connection character_set_results 服务器向客户端返回数据时使用的字符集 一般情况下要使用保持这三个变量的值和客户端使用的字符集相同。 比较规则的作用通常体现比较字符串大小的表达式以及对某个字符串列进行排序中。​","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/tags/MySQL/"}]},{"title":"ConcurrentHashMap源码解读","slug":"1.基础知识/ConcurrentHashMap","date":"2022-01-05T12:23:06.000Z","updated":"2022-01-05T12:23:06.000Z","comments":true,"path":"2022/01/05/1.基础知识/ConcurrentHashMap/","link":"","permalink":"https://yinhuidong.github.io/2022/01/05/1.%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/ConcurrentHashMap/","excerpt":"","text":"#1.成员变量 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147//散列表数组的最大限制private static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;//散列表默认值private static final int DEFAULT_CAPACITY = 16;static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8;//并发级别：jdk7历史遗留问题，仅仅在初始化的时候使用到，并不是真正的代表并发级别private static final int DEFAULT_CONCURRENCY_LEVEL = 16;//负载因子，JDK1.8中 ConcurrentHashMap 是固定值private static final float LOAD_FACTOR = 0.75f;//树化阈值，指定桶位 链表长度达到8的话，有可能发生树化操作。static final int TREEIFY_THRESHOLD = 8;//红黑树转化为链表的阈值static final int UNTREEIFY_THRESHOLD = 6;//联合TREEIFY_THRESHOLD控制桶位是否树化，只有当table数组长度达到64且 某个桶位 中的链表长度达到8，才会真正树化static final int MIN_TREEIFY_CAPACITY = 64;//线程迁移数据最小步长，控制线程迁移任务最小区间一个值private static final int MIN_TRANSFER_STRIDE = 16;//计算扩容时候生成的一个 标识戳private static int RESIZE_STAMP_BITS = 16;//结果是65535 表示并发扩容最多线程数private static final int MAX_RESIZERS = (1 &lt;&lt; (32 - RESIZE_STAMP_BITS)) - 1;//扩容相关private static final int RESIZE_STAMP_SHIFT = 32 - RESIZE_STAMP_BITS;//当node节点hash=-1 表示当前节点已经被迁移了 ，fwd节点static final int MOVED = -1; // hash for forwarding nodes//node hash=-2 表示当前节点已经树化 且 当前节点为treebin对象 ，代理操作红黑树static final int TREEBIN = -2; // hash for roots of treesstatic final int RESERVED = -3; // hash for transient reservations//转化成二进制实际上是 31个 1 可以将一个负数通过位移运算得到一个正数static final int HASH_BITS = 0x7fffffff; // usable bits of normal node hash//当前系统的cpu数量static final int NCPU = Runtime.getRuntime().availableProcessors();//为了兼容7版本的chp保存的，核心代码并没有使用到private static final ObjectStreamField[] serialPersistentFields = &#123; new ObjectStreamField(&quot;segments&quot;, Segment[].class), new ObjectStreamField(&quot;segmentMask&quot;, Integer.TYPE), new ObjectStreamField(&quot;segmentShift&quot;, Integer.TYPE) &#125;;//散列表，长度一定是2次方数transient volatile Node&lt;K,V&gt;[] table;//扩容过程中，会将扩容中的新table 赋值给nextTable 保持引用，扩容结束之后，这里会被设置为Nullprivate transient volatile Node&lt;K,V&gt;[] nextTable;//LongAdder 中的 baseCount 未发生竞争时 或者 当前LongAdder处于加锁状态时，增量累到到baseCount中private transient volatile long baseCount;/** * sizeCtl &lt; 0 * 1. -1 表示当前table正在初始化（有线程在创建table数组），当前线程需要自旋等待.. * 2.表示当前table数组正在进行扩容 ,高16位表示：扩容的标识戳 低16位表示：（1 + nThread） 当前参与并发扩容的线程数量 * * sizeCtl = 0，表示创建table数组时 使用DEFAULT_CAPACITY为大小 * * sizeCtl &gt; 0 * * 1. 如果table未初始化，表示初始化大小 * 2. 如果table已经初始化，表示下次扩容时的 触发条件（阈值） */private transient volatile int sizeCtl;/** * * 扩容过程中，记录当前进度。所有线程都需要从transferIndex中分配区间任务，去执行自己的任务。 */private transient volatile int transferIndex;/** * LongAdder中的cellsBuzy 0表示当前LongAdder对象无锁状态，1表示当前LongAdder对象加锁状态 */private transient volatile int cellsBusy;/** * LongAdder中的cells数组，当baseCount发生竞争后，会创建cells数组， * 线程会通过计算hash值 取到 自己的cell ，将增量累加到指定cell中 * 总数 = sum(cells) + baseCount */private transient volatile CounterCell[] counterCells;// Unsafe mechanicsprivate static final sun.misc.Unsafe U;/**表示sizeCtl属性在ConcurrentHashMap中内存偏移地址*/private static final long SIZECTL;/**表示transferIndex属性在ConcurrentHashMap中内存偏移地址*/private static final long TRANSFERINDEX;/**表示baseCount属性在ConcurrentHashMap中内存偏移地址*/private static final long BASECOUNT;/**表示cellsBusy属性在ConcurrentHashMap中内存偏移地址*/private static final long CELLSBUSY;/**表示cellValue属性在CounterCell中内存偏移地址*/private static final long CELLVALUE;/**表示数组第一个元素的偏移地址*/private static final long ABASE;private static final int ASHIFT;static &#123; try &#123; U = sun.misc.Unsafe.getUnsafe(); Class&lt;?&gt; k = ConcurrentHashMap.class; SIZECTL = U.objectFieldOffset (k.getDeclaredField(&quot;sizeCtl&quot;)); TRANSFERINDEX = U.objectFieldOffset (k.getDeclaredField(&quot;transferIndex&quot;)); BASECOUNT = U.objectFieldOffset (k.getDeclaredField(&quot;baseCount&quot;)); CELLSBUSY = U.objectFieldOffset (k.getDeclaredField(&quot;cellsBusy&quot;)); Class&lt;?&gt; ck = CounterCell.class; CELLVALUE = U.objectFieldOffset (ck.getDeclaredField(&quot;value&quot;)); Class&lt;?&gt; ak = Node[].class; ABASE = U.arrayBaseOffset(ak); //表示数组单元所占用空间大小,scale 表示Node[]数组中每一个单元所占用空间大小 int scale = U.arrayIndexScale(ak); //1 0000 &amp; 0 1111 = 0 if ((scale &amp; (scale - 1)) != 0) throw new Error(&quot;data type scale not a power of two&quot;); //numberOfLeadingZeros() 这个方法是返回当前数值转换为二进制后，从高位到低位开始统计，看有多少个0连续在一块。 //8 =&gt; 1000 numberOfLeadingZeros(8) = 28 //4 =&gt; 100 numberOfLeadingZeros(4) = 29 //ASHIFT = 31 - 29 = 2 ？？ //ABASE + （5 &lt;&lt; ASHIFT） ASHIFT = 31 - Integer.numberOfLeadingZeros(scale); &#125; catch (Exception e) &#123; throw new Error(e); &#125; &#125; #2.基础方法##2.1 spread高位运算 123static final int spread(int h) &#123; return (h ^ (h &gt;&gt;&gt; 16)) &amp; HASH_BITS;&#125; ##2.2 tabAt该方法获取对象中offset偏移地址对应的对象field的值。实际上这段代码的含义等价于tab[i],但是为什么不直接使用 tab[i]来计算呢？ getObjectVolatile，一旦看到 volatile 关键字，就表示可见性。因为对 volatile 写操作 happen-before 于 volatile 读操作，因此其他线程对 table 的修改均对 get 读取可见； 虽然 table 数组本身是增加了 volatile 属性，但是“volatile 的数组只针对数组的引用具有volatile 的语义，而不是它的元素”。 所以如果有其他线程对这个数组的元素进行写操作，那么当前线程来读的时候不一定能读到最新的值。出于性能考虑，Doug Lea 直接通过 Unsafe 类来对 table 进行操作。 123static final &lt;K,V&gt; Node&lt;K,V&gt; tabAt(Node&lt;K,V&gt;[] tab, int i) &#123; return (Node&lt;K,V&gt;)U.getObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE);&#125; ##2.3 casTabAtcas设置当前节点为桶位的头节点 1234static final &lt;K,V&gt; boolean casTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; c, Node&lt;K,V&gt; v) &#123; return U.compareAndSwapObject(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, c, v);&#125; ##2.4 setTabAt 123static final &lt;K,V&gt; void setTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; v) &#123; U.putObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, v);&#125; ##2.5 resizeStampresizeStamp 用来生成一个和扩容有关的扩容戳，具体有什么作用呢？ 123static final int resizeStamp(int n) &#123; return Integer.numberOfLeadingZeros(n) | (1 &lt;&lt; (RESIZE_STAMP_BITS - 1));&#125; Integer.numberOfLeadingZeros 这个方法是返回无符号整数 n 最高位非 0 位前面的 0 的个数。 比如 10 的二进制是 0000 0000 0000 0000 0000 0000 0000 1010，那么这个方法返回的值就是 28。 根据 resizeStamp 的运算逻辑，我们来推演一下，假如 n=16，那么 resizeStamp(16)=32796转化为二进制是[0000 0000 0000 0000 1000 0000 0001 1100] 接着再来看,当第一个线程尝试进行扩容的时候，会执行下面这段代码： U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)rs 左移 16 位，相当于原本的二进制低位变成了高位 1000 0000 0001 1100 0000 0000 00000000 然后再+2 =1000 0000 0001 1100 0000 0000 0000 0000+10=1000 0000 0001 1100 0000 00000000 0010 高 16 位代表扩容的标记、低 16 位代表并行扩容的线程数 这样来存储有什么好处呢？ 1，首先在 CHM 中是支持并发扩容的，也就是说如果当前的数组需要进行扩容操作，可以由多个线程来共同负责 2，可以保证每次扩容都生成唯一的生成戳，每次新的扩容，都有一个不同的 n，这个生成戳就是根据 n 来计算出来的一个数字，n 不同，这个数字也不同 第一个线程尝试扩容的时候，为什么是+2 因为 1 表示初始化，2 表示一个线程在执行扩容，而且对 sizeCtl 的操作都是基于位运算的，所以不会关心它本身的数值是多少，只关心它在二进制上的数值，而 sc + 1 会在低 16 位上加 1。##2.6 tableSizeFor经过多次位移返回大于等于c的最小的二次方数 1234567891011121314151617181920 /** * Returns a power of two table size for the given desired capacity. * See Hackers Delight, sec 3.2 * 返回&gt;=c的最小的2的次方数 * c=28 * n=27 =&gt; 0b 11011 * 11011 | 01101 =&gt; 11111 * 11111 | 00111 =&gt; 11111 * .... * =&gt; 11111 + 1 =100000 = 32 */private static final int tableSizeFor(int c) &#123; int n = c - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125; #3. 构造方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public ConcurrentHashMap() &#123;&#125;public ConcurrentHashMap(int initialCapacity) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException(); //如果指定的容量超过允许的最大值，设置为最大值 int cap = ((initialCapacity &gt;= (MAXIMUM_CAPACITY &gt;&gt;&gt; 1)) ? MAXIMUM_CAPACITY : tableSizeFor(initialCapacity + (initialCapacity &gt;&gt;&gt; 1) + 1)); /** * sizeCtl &gt; 0 * 当目前table未初始化时，sizeCtl表示初始化容量 */ this.sizeCtl = cap;&#125;public ConcurrentHashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; this.sizeCtl = DEFAULT_CAPACITY; putAll(m);&#125;public ConcurrentHashMap(int initialCapacity, float loadFactor) &#123; this(initialCapacity, loadFactor, 1);&#125;public ConcurrentHashMap(int initialCapacity, float loadFactor, int concurrencyLevel) &#123; //参数校验 if (!(loadFactor &gt; 0.0f) || initialCapacity &lt; 0 || concurrencyLevel &lt;= 0) throw new IllegalArgumentException(); //如果初始容量小于并发级别，那就设置初始容量为并发级别 if (initialCapacity &lt; concurrencyLevel) initialCapacity = concurrencyLevel; //16/0.75 +1 = 22 long size = (long)(1.0 + (long)initialCapacity / loadFactor); // 22 - &gt; 32 int cap = (size &gt;= (long)MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : tableSizeFor((int)size); /** * sizeCtl &gt; 0 * 当目前table未初始化时，sizeCtl表示初始化容量 */ this.sizeCtl = cap;&#125; #4.put 1234public V put(K key, V value) &#123; //如果key已经存在，是否覆盖，默认是false return putVal(key, value, false);&#125; #5 putVal 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129final V putVal(K key, V value, boolean onlyIfAbsent) &#123; //控制k 和 v 不能为null if (key == null || value == null) throw new NullPointerException(); //通过spread方法，可以让高位也能参与进寻址运算。 int hash = spread(key.hashCode()); //binCount表示当前k-v 封装成node后插入到指定桶位后，在桶位中的所属链表的下标位置 //0 表示当前桶位为null，node可以直接放着 //2 表示当前桶位已经可能是红黑树 int binCount = 0; //tab 引用map对象的table //自旋 for (Node&lt;K,V&gt;[] tab = table;;) &#123; //f 表示桶位的头结点 //n 表示散列表数组的长度 //i 表示key通过寻址计算后，得到的桶位下标 //fh 表示桶位头结点的hash值 Node&lt;K,V&gt; f; int n, i, fh; //CASE1：成立，表示当前map中的table尚未初始化.. if (tab == null || (n = tab.length) == 0) //最终当前线程都会获取到最新的map.table引用。 tab = initTable(); //CASE2：i 表示key使用路由寻址算法得到 key对应 table数组的下标位置，tabAt 获取指定桶位的头结点 f else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123; //进入到CASE2代码块 前置条件 当前table数组i桶位是Null时。 //使用CAS方式 设置 指定数组i桶位 为 new Node&lt;K,V&gt;(hash, key, value, null),并且期望值是null //cas操作成功 表示ok，直接break for循环即可 //cas操作失败，表示在当前线程之前，有其它线程先你一步向指定i桶位设置值了。 //当前线程只能再次自旋，去走其它逻辑。 if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin &#125; //CASE3：前置条件，桶位的头结点一定不是null。 //条件成立表示当前桶位的头结点 为 FWD结点，表示目前map正处于扩容过程中.. else if ((fh = f.hash) == MOVED) //看到fwd节点后，当前节点有义务帮助当前map对象完成迁移数据的工作 //帮助扩容 tab = helpTransfer(tab, f); //CASE4：当前桶位 可能是 链表 也可能是 红黑树代理结点TreeBin else &#123; //当插入key存在时，会将旧值赋值给oldVal，返回给put方法调用处.. V oldVal = null; //使用sync 加锁“头节点”，理论上是“头结点” synchronized (f) &#123; //为什么又要对比一下，看看当前桶位的头节点 是否为 之前获取的头结点？ //为了避免其它线程将该桶位的头结点修改掉，导致当前线程从sync 加锁 就有问题了。之后所有操作都不用在做了。 if (tabAt(tab, i) == f) &#123;//条件成立，说明咱们 加锁 的对象没有问题，可以进来造了！ //条件成立，说明当前桶位就是普通链表桶位。 if (fh &gt;= 0) &#123; //1.当前插入key与链表当中所有元素的key都不一致时，当前的插入操作是追加到链表的末尾，binCount表示链表长度 //2.当前插入key与链表当中的某个元素的key一致时，当前插入操作可能就是替换了。binCount表示冲突位置（binCount - 1） binCount = 1; //迭代循环当前桶位的链表，e是每次循环处理节点。 for (Node&lt;K,V&gt; e = f;; ++binCount) &#123; //当前循环节点 key K ek; //条件一：e.hash == hash 成立 表示循环的当前元素的hash值与插入节点的hash值一致，需要进一步判断 //条件二：((ek = e.key) == key ||(ek != null &amp;&amp; key.equals(ek))) // 成立：说明循环的当前节点与插入节点的key一致，发生冲突了 if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; //将当前循环的元素的 值 赋值给oldVal oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; &#125; //当前元素 与 插入元素的key不一致 时，会走下面程序。 //1.更新循环处理节点为 当前节点的下一个节点 //2.判断下一个节点是否为null，如果是null，说明当前节点已经是队尾了，插入数据需要追加到队尾节点的后面。 Node&lt;K,V&gt; pred = e; if ((e = e.next) == null) &#123; pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; &#125; &#125; &#125; //前置条件，该桶位一定不是链表 //条件成立，表示当前桶位是 红黑树代理结点TreeBin else if (f instanceof TreeBin) &#123; //p 表示红黑树中如果与你插入节点的key 有冲突节点的话 ，则putTreeVal 方法 会返回冲突节点的引用。 Node&lt;K,V&gt; p; //强制设置binCount为2，因为binCount &lt;= 1 时有其它含义，所以这里设置为了2 binCount = 2; //条件一：成立，说明当前插入节点的key与红黑树中的某个节点的key一致，冲突了 if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) &#123; //将冲突节点的值 赋值给 oldVal oldVal = p.val; if (!onlyIfAbsent) p.val = value; &#125; &#125; &#125; &#125; //说明当前桶位不为null，可能是红黑树 也可能是链表 if (binCount != 0) &#123; //如果binCount&gt;=8 表示处理的桶位一定是链表 if (binCount &gt;= TREEIFY_THRESHOLD) //调用转化链表为红黑树的方法 treeifyBin(tab, i); //说明当前线程插入的数据key，与原有k-v发生冲突，需要将原数据v返回给调用者。 if (oldVal != null) return oldVal; break; &#125; &#125; &#125; //1.统计当前table一共有多少数据 //2.判断是否达到扩容阈值标准，触发扩容。 addCount(1L, binCount); return null;&#125; #6 initTable数组初始化方法，这个方法比较简单，就是初始化一个合适大小的数组。 sizeCtl ：这个标志是在 Node 数组初始化或者扩容的时候的一个控制位标识，负数代表正在进行初始化或者扩容操作。 -1 代表正在初始化 -N 代表有 N-1 个线程正在进行扩容操作，这里不是简单的理解成 n 个线程，sizeCtl 就是-N 0 标识 Node 数组还没有被初始化，正数代表初始化或者下一次扩容的大小 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/** * Initializes table, using the size recorded in sizeCtl. * * sizeCtl &lt; 0 * * 1. -1 表示当前table正在初始化（有线程在创建table数组），当前线程需要自旋等待.. * * 2.表示当前table数组正在进行扩容 ,高16位表示：扩容的标识戳 低16位表示：（1 + nThread） 当前参与并发扩容的线程数量 * * * * sizeCtl = 0，表示创建table数组时 使用DEFAULT_CAPACITY为大小 * * * * sizeCtl &gt; 0 * * * * 1. 如果table未初始化，表示初始化大小 * * 2. 如果table已经初始化，表示下次扩容时的 触发条件（阈值） */private final Node&lt;K,V&gt;[] initTable() &#123; //tab 引用map.table //sc sizeCtl的临时值 Node&lt;K,V&gt;[] tab; int sc; //自旋 条件：map.table 尚未初始化 while ((tab = table) == null || tab.length == 0) &#123; if ((sc = sizeCtl) &lt; 0) //大概率就是-1，表示其它线程正在进行创建table的过程，当前线程没有竞争到初始化table的锁。 Thread.yield(); // lost initialization race; just spin //1.sizeCtl = 0，表示创建table数组时 使用DEFAULT_CAPACITY为大小 //2.如果table未初始化，表示初始化大小 //3.如果table已经初始化，表示下次扩容时的 触发条件（阈值） else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) &#123; try &#123; //这里为什么又要判断呢？ 防止其它线程已经初始化完毕了，然后当前线程再次初始化..导致丢失数据。 //条件成立，说明其它线程都没有进入过这个if块，当前线程就是具备初始化table权利了。 if ((tab = table) == null || tab.length == 0) &#123; //sc大于0 创建table时 使用 sc为指定大小，否则使用 16 默认值. int n = (sc &gt; 0) ? sc : DEFAULT_CAPACITY; @SuppressWarnings(&quot;unchecked&quot;) Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; //最终赋值给 map.table table = tab = nt; //n &gt;&gt;&gt; 2 =&gt; 等于 1/4 n n - (1/4)n = 3/4 n =&gt; 0.75 * n //sc 0.75 n 表示下一次扩容时的触发条件。 sc = n - (n &gt;&gt;&gt; 2); &#125; &#125; finally &#123; //1.如果当前线程是第一次创建map.table的线程话，sc表示的是 下一次扩容的阈值 //2.表示当前线程 并不是第一次创建map.table的线程，当前线程进入到else if 块 时，将 //sizeCtl 设置为了-1 ，那么这时需要将其修改为 进入时的值。 sizeCtl = sc; &#125; break; &#125; &#125; return tab;&#125; #7 addCount 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124private final void addCount(long x, int check) &#123; //as 表示 LongAdder.cells //b 表示LongAdder.base //s 表示当前map.table中元素的数量 CounterCell[] as; long b, s; //条件一：true-&gt;表示cells已经初始化了，当前线程应该去使用hash寻址找到合适的cell 去累加数据 // false-&gt;表示当前线程应该将数据累加到 base //条件二：false-&gt;表示写base成功，数据累加到base中了，当前竞争不激烈，不需要创建cells // true-&gt;表示写base失败，与其他线程在base上发生了竞争，当前线程应该去尝试创建cells。 if ((as = counterCells) != null || !U.compareAndSwapLong(this, BASECOUNT, b = baseCount, s = b + x)) &#123; //有几种情况进入到if块中？ //1.true-&gt;表示cells已经初始化了，当前线程应该去使用hash寻址找到合适的cell 去累加数据 //2.true-&gt;表示写base失败，与其他线程在base上发生了竞争，当前线程应该去尝试创建cells。 //a 表示当前线程hash寻址命中的cell CounterCell a; //v 表示当前线程写cell时的期望值 long v; //m 表示当前cells数组的长度 int m; //true -&gt; 未竞争 false-&gt;发生竞争 boolean uncontended = true; //条件一：as == null || (m = as.length - 1) &lt; 0 //true-&gt; 表示当前线程是通过 写base竞争失败 然后进入的if块，就需要调用fullAddCount方法去扩容 或者 重试.. LongAdder.longAccumulate //条件二：a = as[ThreadLocalRandom.getProbe() &amp; m]) == null 前置条件：cells已经初始化了 //true-&gt;表示当前线程命中的cell表格是个空，需要当前线程进入fullAddCount方法去初始化 cell，放入当前位置. //条件三：!(uncontended = U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x) // false-&gt;取反得到false，表示当前线程使用cas方式更新当前命中的cell成功 // true-&gt;取反得到true,表示当前线程使用cas方式更新当前命中的cell失败，需要进入fullAddCount进行重试 或者 扩容 cells。 if (as == null || (m = as.length - 1) &lt; 0 || (a = as[ThreadLocalRandom.getProbe() &amp; m]) == null || !(uncontended = U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x)) ) &#123; fullAddCount(x, uncontended); //考虑到fullAddCount里面的事情比较累，就让当前线程 不参与到 扩容相关的逻辑了，直接返回到调用点。 return; &#125; if (check &lt;= 1) return; //获取当前散列表元素个数，这是一个期望值 s = sumCount(); &#125; //表示一定是一个put操作调用的addCount if (check &gt;= 0) &#123; //tab 表示map.table //nt 表示map.nextTable //n 表示map.table数组的长度 //sc 表示sizeCtl的临时值 Node&lt;K,V&gt;[] tab, nt; int n, sc; /** * sizeCtl &lt; 0 * 1. -1 表示当前table正在初始化（有线程在创建table数组），当前线程需要自旋等待.. * 2.表示当前table数组正在进行扩容 ,高16位表示：扩容的标识戳 低16位表示：（1 + nThread） 当前参与并发扩容的线程数量 * * sizeCtl = 0，表示创建table数组时 使用DEFAULT_CAPACITY为大小 * * sizeCtl &gt; 0 * * 1. 如果table未初始化，表示初始化大小 * 2. 如果table已经初始化，表示下次扩容时的 触发条件（阈值） */ //自旋 //条件一：s &gt;= (long)(sc = sizeCtl) // true-&gt; 1.当前sizeCtl为一个负数 表示正在扩容中.. // 2.当前sizeCtl是一个正数，表示扩容阈值 // false-&gt; 表示当前table尚未达到扩容条件 //条件二：(tab = table) != null // 恒成立 true //条件三：(n = tab.length) &lt; MAXIMUM_CAPACITY // true-&gt;当前table长度小于最大值限制，则可以进行扩容。 while (s &gt;= (long)(sc = sizeCtl) &amp;&amp; (tab = table) != null &amp;&amp; (n = tab.length) &lt; MAXIMUM_CAPACITY) &#123; //扩容批次唯一标识戳 //16 -&gt; 32 扩容 标识为：1000 0000 0001 1011 int rs = resizeStamp(n); //条件成立：表示当前table正在扩容 // 当前线程理论上应该协助table完成扩容 if (sc &lt; 0) &#123; //条件一：(sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs // true-&gt;说明当前线程获取到的扩容唯一标识戳 非 本批次扩容 // false-&gt;说明当前线程获取到的扩容唯一标识戳 是 本批次扩容 //条件二： JDK1.8 中有bug jira已经提出来了 其实想表达的是 = sc == (rs &lt;&lt; 16 ) + 1 // true-&gt; 表示扩容完毕，当前线程不需要再参与进来了 // false-&gt;扩容还在进行中，当前线程可以参与 //条件三：JDK1.8 中有bug jira已经提出来了 其实想表达的是 = sc == (rs&lt;&lt;16) + MAX_RESIZERS // true-&gt; 表示当前参与并发扩容的线程达到了最大值 65535 - 1 // false-&gt;表示当前线程可以参与进来 //条件四：(nt = nextTable) == null // true-&gt;表示本次扩容结束 // false-&gt;扩容正在进行中 if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex &lt;= 0) break; //前置条件：当前table正在执行扩容中.. 当前线程有机会参与进扩容。 //条件成立：说明当前线程成功参与到扩容任务中，并且将sc低16位值加1，表示多了一个线程参与工作 //条件失败：1.当前有很多线程都在此处尝试修改sizeCtl，有其它一个线程修改成功了，导致你的sc期望值与内存中的值不一致 修改失败 // 2.transfer 任务内部的线程也修改了sizeCtl。 if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) //协助扩容线程，持有nextTable参数 transfer(tab, nt); &#125; //1000 0000 0001 1011 0000 0000 0000 0000 +2 =&gt; 1000 0000 0001 1011 0000 0000 0000 0010 //条件成立，说明当前线程是触发扩容的第一个线程，在transfer方法需要做一些扩容准备工作 else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) //触发扩容条件的线程 不持有nextTable transfer(tab, null); s = sumCount(); &#125; &#125;&#125; #8. transferConcurrentHashMap 支持并发扩容，实现方式是，把 Node 数组进行拆分，让每个线程处理自己的区域，假设 table 数组总长度是 64，默认情况下，那么每个线程可以分到 16 个 bucket。然后每个线程处理的范围，按照倒序来做迁移。 通过 for 自循环处理每个槽位中的链表元素，默认 advace 为真，通过 CAS 设置 transferIndex属性值，并初始化 i 和 bound 值，i 指当前处理的槽位序号，bound 指需要处理的槽位边界，先处理槽位 31 的节点； （bound,i） =(16,31) 从 31 的位置往前推动。 每存在一个线程执行完扩容操作，就通过 cas 执行 sc-1。 接着判断(sc-2) !=resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT ; 如果相等，表示当前为整个扩容操作的 最后一个线程，那么意味着整个扩容操作就结束了；如果不相等，说明还得继续。 这么做的目的，一方面是防止不同扩容之间出现相同的 sizeCtl，另外一方面，还可以避免sizeCtl 的 ABA 问题导致的扩容重叠的情况。 扩容图解判断是否需要扩容，也就是当更新后的键值对总数 baseCount &gt;= 阈值 sizeCtl 时，进行rehash，这里面会有两个逻辑。 如果当前正在处于扩容阶段，则当前线程会加入并且协助扩容。 如果当前没有在扩容，则直接触发扩容操作。 扩容操作的核心在于数据的转移，在单线程环境下数据的转移很简单，无非就是把旧数组中的数据迁移到新的数组。但是这在多线程环境下，在扩容的时候其他线程也可能正在添加元素，这时又触发了扩容怎么办？可能大家想到的第一个解决方案是加互斥锁，把转移过程锁住，虽然是可行的解决方案，但是会带来较大的性能开销。因为互斥锁会导致所有访问临界区的线程陷入到阻塞状态，持有锁的线程耗时越长，其他竞争线程就会一直被阻塞，导致吞吐量较低。而且还可能导致死锁。 而 ConcurrentHashMap 并没有直接加锁，而是采用 CAS 实现无锁的并发同步策略，最精华的部分是它可以利用多线程来进行协同扩容。 它把 Node 数组当作多个线程之间共享的任务队列，然后通过维护一个指针来划分每个线程锁负责的区间，每个线程通过区间逆向遍历来实现扩容，一个已经迁移完的bucket会被替换为一个ForwardingNode节点，标记当前bucket已经被其他线程迁移完了。接下来分析一下它的源码实现。 fwd:这个类是个标识类，用于指向新表用的，其他线程遇到这个类会主动跳过这个类，因为这个类要么就是扩容迁移正在进行，要么就是已经完成扩容迁移，也就是这个类要保证线程安全，再进行操作。 advance:这个变量是用于提示代码是否进行推进处理，也就是当前桶处理完，处理下一个桶的标识。 finishing:这个变量用于提示扩容是否结束用的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237private final void transfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab) &#123; //n 表示扩容之前table数组的长度 //stride 表示分配给线程任务的步长 int n = tab.length, stride; // stride 固定为 16 if ((stride = (NCPU &gt; 1) ? (n &gt;&gt;&gt; 3) / NCPU : n) &lt; MIN_TRANSFER_STRIDE) stride = MIN_TRANSFER_STRIDE; // subdivide range //条件成立：表示当前线程为触发本次扩容的线程，需要做一些扩容准备工作 //条件不成立：表示当前线程是协助扩容的线程.. if (nextTab == null) &#123; // initiating try &#123; //创建了一个比扩容之前大一倍的table @SuppressWarnings(&quot;unchecked&quot;) Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n &lt;&lt; 1]; nextTab = nt; &#125; catch (Throwable ex) &#123; // try to cope with OOME sizeCtl = Integer.MAX_VALUE; return; &#125; //赋值给对象属性 nextTable ，方便协助扩容线程 拿到新表 nextTable = nextTab; //记录迁移数据整体位置的一个标记。index计数是从1开始计算的。 transferIndex = n; &#125; //表示新数组的长度 int nextn = nextTab.length; //fwd 节点，当某个桶位数据处理完毕后，将此桶位设置为fwd节点，其它写线程 或读线程看到后，会有不同逻辑。 ForwardingNode&lt;K,V&gt; fwd = new ForwardingNode&lt;K,V&gt;(nextTab); //推进标记 boolean advance = true; //完成标记 boolean finishing = false; // to ensure sweep before committing nextTab //i 表示分配给当前线程任务，执行到的桶位 //bound 表示分配给当前线程任务的下界限制 int i = 0, bound = 0; //自旋 for (;;) &#123; //f 桶位的头结点 //fh 头结点的hash Node&lt;K,V&gt; f; int fh; /** * 1.给当前线程分配任务区间 * 2.维护当前线程任务进度（i 表示当前处理的桶位） * 3.维护map对象全局范围内的进度 */ while (advance) &#123; //分配任务的开始下标 //分配任务的结束下标 int nextIndex, nextBound; //CASE1: //条件一：--i &gt;= bound //成立：表示当前线程的任务尚未完成，还有相应的区间的桶位要处理，--i 就让当前线程处理下一个 桶位. //不成立：表示当前线程任务已完成 或 者未分配 if (--i &gt;= bound || finishing) advance = false; //CASE2: //前置条件：当前线程任务已完成 或 者未分配 //条件成立：表示对象全局范围内的桶位都分配完毕了，没有区间可分配了，设置当前线程的i变量为-1 跳出循环后，执行退出迁移任务相关的程序 //条件不成立：表示对象全局范围内的桶位尚未分配完毕，还有区间可分配 else if ((nextIndex = transferIndex) &lt;= 0) &#123; i = -1; advance = false; &#125; //CASE3: //前置条件：1、当前线程需要分配任务区间 2.全局范围内还有桶位尚未迁移 //条件成立：说明给当前线程分配任务成功 //条件失败：说明分配给当前线程失败，应该是和其它线程发生了竞争吧 else if (U.compareAndSwapInt (this, TRANSFERINDEX, nextIndex, nextBound = (nextIndex &gt; stride ? nextIndex - stride : 0))) &#123; bound = nextBound; i = nextIndex - 1; advance = false; &#125; &#125; //CASE1： //条件一：i &lt; 0 //成立：表示当前线程未分配到任务 if (i &lt; 0 || i &gt;= n || i + n &gt;= nextn) &#123; //保存sizeCtl 的变量 int sc; if (finishing) &#123; nextTable = null; table = nextTab; sizeCtl = (n &lt;&lt; 1) - (n &gt;&gt;&gt; 1); return; &#125; //条件成立：说明设置sizeCtl 低16位 -1 成功，当前线程可以正常退出 if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) &#123; //1000 0000 0001 1011 0000 0000 0000 0000 //条件成立：说明当前线程不是最后一个退出transfer任务的线程 if ((sc - 2) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) //正常退出 return; finishing = advance = true; i = n; // recheck before commit &#125; &#125; //前置条件：【CASE2~CASE4】 当前线程任务尚未处理完，正在进行中 //CASE2: //条件成立：说明当前桶位未存放数据，只需要将此处设置为fwd节点即可。 else if ((f = tabAt(tab, i)) == null) advance = casTabAt(tab, i, null, fwd); //CASE3: //条件成立：说明当前桶位已经迁移过了，当前线程不用再处理了，直接再次更新当前线程任务索引，再次处理下一个桶位 或者 其它操作 else if ((fh = f.hash) == MOVED) advance = true; // already processed //CASE4: //前置条件：当前桶位有数据，而且node节点 不是 fwd节点，说明这些数据需要迁移。 else &#123; //sync 加锁当前桶位的头结点 synchronized (f) &#123; //防止在你加锁头对象之前，当前桶位的头对象被其它写线程修改过，导致你目前加锁对象错误... if (tabAt(tab, i) == f) &#123; //ln 表示低位链表引用 //hn 表示高位链表引用 Node&lt;K,V&gt; ln, hn; //条件成立：表示当前桶位是链表桶位 if (fh &gt;= 0) &#123; //lastRun //可以获取出 当前链表 末尾连续高位不变的 node int runBit = fh &amp; n; Node&lt;K,V&gt; lastRun = f; for (Node&lt;K,V&gt; p = f.next; p != null; p = p.next) &#123; int b = p.hash &amp; n; if (b != runBit) &#123; runBit = b; lastRun = p; &#125; &#125; //条件成立：说明lastRun引用的链表为 低位链表，那么就让 ln 指向 低位链表 if (runBit == 0) &#123; ln = lastRun; hn = null; &#125; //否则，说明lastRun引用的链表为 高位链表，就让 hn 指向 高位链表 else &#123; hn = lastRun; ln = null; &#125; for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) &#123; int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); else hn = new Node&lt;K,V&gt;(ph, pk, pv, hn); &#125; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; &#125; //条件成立：表示当前桶位是 红黑树 代理结点TreeBin else if (f instanceof TreeBin) &#123; //转换头结点为 treeBin引用 t TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; //低位双向链表 lo 指向低位链表的头 loTail 指向低位链表的尾巴 TreeNode&lt;K,V&gt; lo = null, loTail = null; //高位双向链表 lo 指向高位链表的头 loTail 指向高位链表的尾巴 TreeNode&lt;K,V&gt; hi = null, hiTail = null; //lc 表示低位链表元素数量 //hc 表示高位链表元素数量 int lc = 0, hc = 0; //迭代TreeBin中的双向链表，从头结点 至 尾节点 for (Node&lt;K,V&gt; e = t.first; e != null; e = e.next) &#123; // h 表示循环处理当前元素的 hash int h = e.hash; //使用当前节点 构建出来的 新的 TreeNode TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt; (h, e.key, e.val, null, null); //条件成立：表示当前循环节点 属于低位链 节点 if ((h &amp; n) == 0) &#123; //条件成立：说明当前低位链表 还没有数据 if ((p.prev = loTail) == null) lo = p; //说明 低位链表已经有数据了，此时当前元素 追加到 低位链表的末尾就行了 else loTail.next = p; //将低位链表尾指针指向 p 节点 loTail = p; ++lc; &#125; //当前节点 属于 高位链 节点 else &#123; if ((p.prev = hiTail) == null) hi = p; else hiTail.next = p; hiTail = p; ++hc; &#125; &#125; ln = (lc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(lo) : (hc != 0) ? new TreeBin&lt;K,V&gt;(lo) : t; hn = (hc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(hi) : (lc != 0) ? new TreeBin&lt;K,V&gt;(hi) : t; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; &#125; &#125; &#125; &#125; &#125;&#125; 链表迁移原理 1）高低位原理分析 ConcurrentHashMap 在做链表迁移时，会用高低位来实现，这里有两个问题要分析一下 1，如何实现高低位链表的区分 假如有这样一个队列第 14 个槽位插入新节点之后，链表元素个数已经达到了 8，且数组长度为 16，优先通过扩容来缓解链表过长的问题 假如当前线程正在处理槽位为 14 的节点，它是一个链表结构，在代码中，首先定义两个变量节点 ln 和 hn，实际就是 lowNode 和 HighNode，分别保存 hash 值的第 x 位为 0 和不等于0 的节点 通过 fn&amp;n 可以把这个链表中的元素分为两类，A 类是 hash 值的第 X 位为 0，B 类是 hash 值的第 x 位为不等于 0（至于为什么要这么区分，稍后分析），并且通过 lastRun 记录最后要处理的节点。最终要达到的目的是，A 类的链表保持位置不动，B 类的链表为 14+16(扩容增加的长度)=30 把 14 槽位的链表单独伶出来，用蓝色表示 fn&amp;n=0 的节点，假如链表的分类是这样 1234567for (Node&lt;K,V&gt; p = f.next; p != null; p = p.next) &#123; int b = p.hash &amp; n; if (b != runBit) &#123; runBit = b; lastRun = p; &#125;&#125; 通过上面这段代码遍历，会记录 runBit 以及 lastRun，按照上面这个结构，那么 runBit 应该是蓝色节点，lastRun 应该是第 6 个节点接着，再通过这段代码进行遍历，生成 ln 链以及 hn 链 1234567for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) &#123; int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); else hn = new Node&lt;K,V&gt;(ph, pk, pv, hn);&#125; 接着，通过 CAS 操作，把 hn 链放在 i+n 也就是 14+16 的位置，ln 链保持原来的位置不动。并且设置当前节点为 fwd，表示已经被当前线程迁移完了。 123setTabAt(nextTab, i, ln);setTabAt(nextTab, i + n, hn);setTabAt(tab, i, fwd); 迁移完成以后的数据分布如下 2）为什么要做高低位的划分 要想了解这么设计的目的，我们需要从 ConcurrentHashMap 的根据下标获取对象的算法来看，在 putVal 方法中 1018 行： (f = tabAt(tab, i = (n - 1) &amp; hash)) == null 通过(n-1) &amp; hash 来获得在 table 中的数组下标来获取节点数据，【&amp;运算是二进制运算符，1&amp; 1=1，其他都为 0】。 #9.helpTransfer如果对应的节点存在，判断这个节点的 hash 是不是等于 MOVED(-1)，说明当前节点是ForwardingNode 节点，意味着有其他线程正在进行扩容，那么当前现在直接帮助它进行扩容，因此调用 helpTransfer方法。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455final Node&lt;K,V&gt;[] helpTransfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt; f) &#123; //nextTab 引用的是 fwd.nextTable == map.nextTable 理论上是这样。 //sc 保存map.sizeCtl Node&lt;K,V&gt;[] nextTab; int sc; //条件一：tab != null 恒成立 true //条件二：(f instanceof ForwardingNode) 恒成立 true //条件三：((ForwardingNode&lt;K,V&gt;)f).nextTable) != null 恒成立 true if (tab != null &amp;&amp; (f instanceof ForwardingNode) &amp;&amp; (nextTab = ((ForwardingNode&lt;K,V&gt;)f).nextTable) != null) &#123; //拿当前标的长度 获取 扩容标识戳 假设 16 -&gt; 32 扩容：1000 0000 0001 1011 int rs = resizeStamp(tab.length); //条件一：nextTab == nextTable //成立：表示当前扩容正在进行中 //不成立：1.nextTable被设置为Null 了，扩容完毕后，会被设为Null // 2.再次出发扩容了...咱们拿到的nextTab 也已经过期了... //条件二：table == tab //成立：说明 扩容正在进行中，还未完成 //不成立：说明扩容已经结束了，扩容结束之后，最后退出的线程 会设置 nextTable 为 table //条件三：(sc = sizeCtl) &lt; 0 //成立：说明扩容正在进行中 //不成立：说明sizeCtl当前是一个大于0的数，此时代表下次扩容的阈值，当前扩容已经结束。 while (nextTab == nextTable &amp;&amp; table == tab &amp;&amp; (sc = sizeCtl) &lt; 0) &#123; //条件一：(sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs // true-&gt;说明当前线程获取到的扩容唯一标识戳 非 本批次扩容 // false-&gt;说明当前线程获取到的扩容唯一标识戳 是 本批次扩容 //条件二： JDK1.8 中有bug jira已经提出来了 其实想表达的是 = sc == (rs &lt;&lt; 16 ) + 1 // true-&gt; 表示扩容完毕，当前线程不需要再参与进来了 // false-&gt;扩容还在进行中，当前线程可以参与 //条件三：JDK1.8 中有bug jira已经提出来了 其实想表达的是 = sc == (rs&lt;&lt;16) + MAX_RESIZERS // true-&gt; 表示当前参与并发扩容的线程达到了最大值 65535 - 1 // false-&gt;表示当前线程可以参与进来 //条件四：transferIndex &lt;= 0 // true-&gt;说明map对象全局范围内的任务已经分配完了，当前线程进去也没活干.. // false-&gt;还有任务可以分配。 if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || transferIndex &lt;= 0) break; if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) &#123; transfer(tab, nextTab); break; &#125; &#125; return nextTab; &#125; return table;&#125; #10.get 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public V get(Object key) &#123; //tab 引用map.table //e 当前元素 //p 目标节点 //n table数组长度 //eh 当前元素hash //ek 当前元素key Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; e, p; int n, eh; K ek; //扰动运算后得到 更散列的hash值 int h = spread(key.hashCode()); //条件一：(tab = table) != null //true-&gt;表示已经put过数据，并且map内部的table也已经初始化完毕 //false-&gt;表示创建完map后，并没有put过数据，map内部的table是延迟初始化的，只有第一次写数据时会触发创建逻辑。 //条件二：(n = tab.length) &gt; 0 true-&gt;表示table已经初始化 //条件三：(e = tabAt(tab, (n - 1) &amp; h)) != null //true-&gt;当前key寻址的桶位 有值 //false-&gt;当前key寻址的桶位中是null，是null直接返回null if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (e = tabAt(tab, (n - 1) &amp; h)) != null) &#123; //前置条件：当前桶位有数据 //对比头结点hash与查询key的hash是否一致 //条件成立：说明头结点与查询Key的hash值 完全一致 if ((eh = e.hash) == h) &#123; //完全比对 查询key 和 头结点的key //条件成立：说明头结点就是查询数据 if ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) return e.val; &#125; //条件成立： //1.-1 fwd 说明当前table正在扩容，且当前查询的这个桶位的数据 已经被迁移走了 //2.-2 TreeBin节点，需要使用TreeBin 提供的find 方法查询。 else if (eh &lt; 0) return (p = e.find(h, key)) != null ? p.val : null; //当前桶位已经形成链表的这种情况 while ((e = e.next) != null) &#123; if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) return e.val; &#125; &#125; return null;&#125; #11.remove 123public V remove(Object key) &#123; return replaceNode(key, null, null);&#125; #12.replaceNode 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143final V replaceNode(Object key, V value, Object cv) &#123; //计算key经过扰动运算后的hash int hash = spread(key.hashCode()); //自旋 for (Node&lt;K,V&gt;[] tab = table;;) &#123; //f表示桶位头结点 //n表示当前table数组长度 //i表示hash命中桶位下标 //fh表示桶位头结点 hash Node&lt;K,V&gt; f; int n, i, fh; //CASE1： //条件一：tab == null true-&gt;表示当前map.table尚未初始化.. false-&gt;已经初始化 //条件二：(n = tab.length) == 0 true-&gt;表示当前map.table尚未初始化.. false-&gt;已经初始化 //条件三：(f = tabAt(tab, i = (n - 1) &amp; hash)) == null true -&gt; 表示命中桶位中为null，直接break， 会返回 if (tab == null || (n = tab.length) == 0 || (f = tabAt(tab, i = (n - 1) &amp; hash)) == null) break; //CASE2： //前置条件CASE2 ~ CASE3：当前桶位不是null //条件成立：说明当前table正在扩容中，当前是个写操作，所以当前线程需要协助table完成扩容。 else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); //CASE3: //前置条件CASE2 ~ CASE3：当前桶位不是null //当前桶位 可能是 &quot;链表&quot; 也可能 是 &quot;红黑树&quot; TreeBin else &#123; //保留替换之前的数据引用 V oldVal = null; //校验标记 boolean validated = false; //加锁当前桶位 头结点，加锁成功之后会进入 代码块。 synchronized (f) &#123; //判断sync加锁是否为当前桶位 头节点，防止其它线程，在当前线程加锁成功之前，修改过 桶位 的头结点。 //条件成立：当前桶位头结点 仍然为f，其它线程没修改过。 if (tabAt(tab, i) == f) &#123; //条件成立：说明桶位 为 链表 或者 单个 node if (fh &gt;= 0) &#123; validated = true; //e 表示当前循环处理元素 //pred 表示当前循环节点的上一个节点 Node&lt;K,V&gt; e = f, pred = null; for (;;) &#123; //当前节点key K ek; //条件一：e.hash == hash true-&gt;说明当前节点的hash与查找节点hash一致 //条件二：((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) //if 条件成立，说明key 与查询的key完全一致。 if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; //当前节点的value V ev = e.val; //条件一：cv == null true-&gt;替换的值为null 那么就是一个删除操作 //条件二：cv == ev || (ev != null &amp;&amp; cv.equals(ev)) 那么是一个替换操作 if (cv == null || cv == ev || (ev != null &amp;&amp; cv.equals(ev))) &#123; //删除 或者 替换 //将当前节点的值 赋值给 oldVal 后续返回会用到 oldVal = ev; //条件成立：说明当前是一个替换操作 if (value != null) //直接替换 e.val = value; //条件成立：说明当前节点非头结点 else if (pred != null) //当前节点的上一个节点，指向当前节点的下一个节点。 pred.next = e.next; else //说明当前节点即为 头结点，只需要将 桶位设置为头结点的下一个节点。 setTabAt(tab, i, e.next); &#125; break; &#125; pred = e; if ((e = e.next) == null) break; &#125; &#125; //条件成立：TreeBin节点。 else if (f instanceof TreeBin) &#123; validated = true; //转换为实际类型 TreeBin t TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; //r 表示 红黑树 根节点 //p 表示 红黑树中查找到对应key 一致的node TreeNode&lt;K,V&gt; r, p; //条件一：(r = t.root) != null 理论上是成立 //条件二：TreeNode.findTreeNode 以当前节点为入口，向下查找key（包括本身节点） // true-&gt;说明查找到相应key 对应的node节点。会赋值给p if ((r = t.root) != null &amp;&amp; (p = r.findTreeNode(hash, key, null)) != null) &#123; //保存p.val 到pv V pv = p.val; //条件一：cv == null 成立：不必对value，就做替换或者删除操作 //条件二：cv == pv ||(pv != null &amp;&amp; cv.equals(pv)) 成立：说明“对比值”与当前p节点的值 一致 if (cv == null || cv == pv || (pv != null &amp;&amp; cv.equals(pv))) &#123; //替换或者删除操作 oldVal = pv; //条件成立：替换操作 if (value != null) p.val = value; //删除操作 else if (t.removeTreeNode(p)) //这里没做判断，直接搞了...很疑惑 setTabAt(tab, i, untreeify(t.first)); &#125; &#125; &#125; &#125; &#125; //当其他线程修改过桶位 头结点时，当前线程 sync 头结点 锁错对象时，validated 为false，会进入下次for 自旋 if (validated) &#123; if (oldVal != null) &#123; //替换的值 为null，说明当前是一次删除操作，oldVal ！=null 成立，说明删除成功，更新当前元素个数计数器。 if (value == null) addCount(-1L, -1); return oldVal; &#125; break; &#125; &#125; &#125; return null;&#125; #13.TreeBin##13.1 属性 1234567891011121314151617//红黑树 根节点 TreeNode&lt;K,V&gt; root;//链表的头节点volatile TreeNode&lt;K,V&gt; first;//等待者线程（当前lockState是读锁状态）volatile Thread waiter;/** * 1.写锁状态 写是独占状态，以散列表来看，真正进入到TreeBin中的写线程 同一时刻 只有一个线程。 1 * 2.读锁状态 读锁是共享，同一时刻可以有多个线程 同时进入到 TreeBin对象中获取数据。 每一个线程 都会给 lockStat + 4 * 3.等待者状态（写线程在等待），当TreeBin中有读线程目前正在读取数据时，写线程无法修改数据，那么就将lockState的最低2位 设置为 0b 10 */volatile int lockState;// values for lockStatestatic final int WRITER = 1; // set while holding write lockstatic final int WAITER = 2; // set when waiting for write lockstatic final int READER = 4; // increment value for setting read lock ##13.2 构造器 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586TreeBin(TreeNode&lt;K,V&gt; b) &#123; //设置节点hash为-2 表示此节点是TreeBin节点 super(TREEBIN, null, null, null); //使用first 引用 treeNode链表 this.first = b; //r 红黑树的根节点引用 TreeNode&lt;K,V&gt; r = null; //x表示遍历的当前节点 for (TreeNode&lt;K,V&gt; x = b, next; x != null; x = next) &#123; next = (TreeNode&lt;K,V&gt;)x.next; //强制设置当前插入节点的左右子树为null x.left = x.right = null; //条件成立：说明当前红黑树 是一个空树，那么设置插入元素 为根节点 if (r == null) &#123; //根节点的父节点 一定为 null x.parent = null; //颜色改为黑色 x.red = false; //让r引用x所指向的对象。 r = x; &#125; else &#123; //非第一次循环，都会来带else分支，此时红黑树已经有数据了 //k 表示 插入节点的key K k = x.key; //h 表示 插入节点的hash int h = x.hash; //kc 表示 插入节点key的class类型 Class&lt;?&gt; kc = null; //p 表示 为查找插入节点的父节点的一个临时节点 TreeNode&lt;K,V&gt; p = r; for (;;) &#123; //dir (-1, 1) //-1 表示插入节点的hash值大于 当前p节点的hash //1 表示插入节点的hash值 小于 当前p节点的hash //ph p表示 为查找插入节点的父节点的一个临时节点的hash int dir, ph; //临时节点 key K pk = p.key; //插入节点的hash值 小于 当前节点 if ((ph = p.hash) &gt; h) //插入节点可能需要插入到当前节点的左子节点 或者 继续在左子树上查找 dir = -1; //插入节点的hash值 大于 当前节点 else if (ph &lt; h) //插入节点可能需要插入到当前节点的右子节点 或者 继续在右子树上查找 dir = 1; //如果执行到 CASE3，说明当前插入节点的hash 与 当前节点的hash一致，会在case3 做出最终排序。最终 //拿到的dir 一定不是0，（-1， 1） else if ((kc == null &amp;&amp; (kc = comparableClassFor(k)) == null) || (dir = compareComparables(kc, k, pk)) == 0) dir = tieBreakOrder(k, pk); //xp 想要表示的是 插入节点的 父节点 TreeNode&lt;K,V&gt; xp = p; //条件成立：说明当前p节点 即为插入节点的父节点 //条件不成立：说明p节点 底下还有层次，需要将p指向 p的左子节点 或者 右子节点，表示继续向下搜索。 if ((p = (dir &lt;= 0) ? p.left : p.right) == null) &#123; //设置插入节点的父节点 为 当前节点 x.parent = xp; //小于P节点，需要插入到P节点的左子节点 if (dir &lt;= 0) xp.left = x; //大于P节点，需要插入到P节点的右子节点 else xp.right = x; //插入节点后，红黑树性质 可能会被破坏，所以需要调用 平衡方法 r = balanceInsertion(r, x); break; &#125; &#125; &#125; &#125; //将r 赋值给 TreeBin对象的 root引用。 this.root = r; assert checkInvariants(root);&#125; ##13.3 putTreeVal 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970final TreeNode&lt;K,V&gt; putTreeVal(int h, K k, V v) &#123; Class&lt;?&gt; kc = null; boolean searched = false; for (TreeNode&lt;K,V&gt; p = root;;) &#123; int dir, ph; K pk; if (p == null) &#123; first = root = new TreeNode&lt;K,V&gt;(h, k, v, null, null); break; &#125; else if ((ph = p.hash) &gt; h) dir = -1; else if (ph &lt; h) dir = 1; else if ((pk = p.key) == k || (pk != null &amp;&amp; k.equals(pk))) return p; else if ((kc == null &amp;&amp; (kc = comparableClassFor(k)) == null) || (dir = compareComparables(kc, k, pk)) == 0) &#123; if (!searched) &#123; TreeNode&lt;K,V&gt; q, ch; searched = true; if (((ch = p.left) != null &amp;&amp; (q = ch.findTreeNode(h, k, kc)) != null) || ((ch = p.right) != null &amp;&amp; (q = ch.findTreeNode(h, k, kc)) != null)) return q; &#125; dir = tieBreakOrder(k, pk); &#125; TreeNode&lt;K,V&gt; xp = p; if ((p = (dir &lt;= 0) ? p.left : p.right) == null) &#123; //当前循环节点xp 即为 x 节点的爸爸 //x 表示插入节点 //f 老的头结点 TreeNode&lt;K,V&gt; x, f = first; first = x = new TreeNode&lt;K,V&gt;(h, k, v, f, xp); //条件成立：说明链表有数据 if (f != null) //设置老的头结点的前置引用为 当前的头结点。 f.prev = x; if (dir &lt;= 0) xp.left = x; else xp.right = x; if (!xp.red) x.red = true; else &#123; //表示 当前新插入节点后，新插入节点 与 父节点 形成 “红红相连” lockRoot(); try &#123; //平衡红黑树，使其再次符合规范。 root = balanceInsertion(root, x); &#125; finally &#123; unlockRoot(); &#125; &#125; break; &#125; &#125; assert checkInvariants(root); return null;&#125; ##13.4 find 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748final Node&lt;K,V&gt; find(int h, Object k) &#123; if (k != null) &#123; //e 表示循环迭代的当前节点 迭代的是first引用的链表 for (Node&lt;K,V&gt; e = first; e != null; ) &#123; //s 保存的是lock临时状态 //ek 链表当前节点 的key int s; K ek; //(WAITER|WRITER) =&gt; 0010 | 0001 =&gt; 0011 //lockState &amp; 0011 != 0 条件成立：说明当前TreeBin 有等待者线程 或者 目前有写操作线程正在加锁 if (((s = lockState) &amp; (WAITER|WRITER)) != 0) &#123; if (e.hash == h &amp;&amp; ((ek = e.key) == k || (ek != null &amp;&amp; k.equals(ek)))) return e; e = e.next; &#125; //前置条件：当前TreeBin中 等待者线程 或者 写线程 都没有 //条件成立：说明添加读锁成功 else if (U.compareAndSwapInt(this, LOCKSTATE, s, s + READER)) &#123; TreeNode&lt;K,V&gt; r, p; try &#123; //查询操作 p = ((r = root) == null ? null : r.findTreeNode(h, k, null)); &#125; finally &#123; //w 表示等待者线程 Thread w; //U.getAndAddInt(this, LOCKSTATE, -READER) == (READER|WAITER) //1.当前线程查询红黑树结束，释放当前线程的读锁 就是让 lockstate 值 - 4 //(READER|WAITER) = 0110 =&gt; 表示当前只有一个线程在读，且“有一个线程在等待” //当前读线程为 TreeBin中的最后一个读线程。 //2.(w = waiter) != null 说明有一个写线程在等待读操作全部结束。 if (U.getAndAddInt(this, LOCKSTATE, -READER) == (READER|WAITER) &amp;&amp; (w = waiter) != null) //使用unpark 让 写线程 恢复运行状态。 LockSupport.unpark(w); &#125; return p; &#125; &#125; &#125; return null;&#125; #总结在java8中，ConcurrentHashMap使用数组+链表+红黑树的组合方式，利用cas和synchronized保证并发写的安全。 引入红黑树的原因：链表查询的时间复杂度为On，但是红黑树的查询时间复杂度为O(log(n)),所以在节点比较多的情况下，使用红黑树可以大大提升性能。 链式桶是一个由node节点组成的链表。树状桶是一颗由TreeNode节点组成的红黑树。输的根节点为TreeBin类型。 当链表长度大于8整个hash表长度大于64的时候，就会转化为TreeBin。TreeBin作为根节点，其实就是红黑树对象。在ConcurrentHashMap的table数组中，存放的就是TreeBin对象，而不是TreeNoe对象。 数组table是懒加载的，只有第一次添加元素的时候才会初始化，所以initTable()存在线程安全问题。 重要的属性就是sizeCtl，用来控制table的初始化和扩容操作的过程： ● -1代表table正在初始化，其他线程直接join等待。 ● -N代表有N-1个线程正在进行扩容操作，严格来说，当其为负数的时候，只用到了低16位，如果低16位为M，此时有M-1个线程进行扩容。 ● 大于0有两种情况：如果table没有初始化，她就表示table初始化的大小，如果table初始化完了，就表示table的容量，默认是table大小的四分之三。 Transfer()扩容 table数据转移到nextTable。扩容操作的核心在于数据的转移，把旧数组中的数据迁移到新的数组。ConcurrentHashMap精华的部分是它可以利用多线程来进行协同扩容，简单来说，它把table数组当作多个线程之间共享的任务队列，然后通过维护一个指针来划分每个线程所负责的区间，每个线程通过区间逆向遍历来实现扩容，一个已经迁移完的 Bucket会被替换为一个Forwarding节点，标记当前Bucket已经被其他线程迁移完了。 helpTransfer()帮助扩容 ConcurrentHashMap并发添加元素时，如果正在扩容，其他线程会帮助扩容，也就是多线程扩容。 第一次添加元素时，默认初始长度为16，当往table中继续添加元素时，通过Hash值跟数组长度取余来决定放在数组的哪个Bucket位置，如果出现放在同一个位置，就优先以链表的形式存放，在同一个位置的个数达到了8个以上，如果数组的长度还小于64，就会扩容数组。如果数组的长度大于等于64，就会将该节点的链表转换成树。 通过扩容数组的方式来把这些节点分散开。然后将这些元素复制到扩容后的新数组中，同一个Bucket中的元素通过Hash值的数组长度位来重新确定位置，可能还是放在原来的位置，也可能放到新的位置。而且，在扩容完成之后，如果之前某个节点是树，但是现在该节点的“Key-Value对”数又小于等于6个，就会将该树转为链表。 put() JDK1.8在使用CAS自旋完成桶的设置时，使用synchronized内置锁保证桶内并发操作的线程安全。尽管对同一个Map操作的线程争用会非常激烈，但是在同一个桶内的线程争用通常不会很激烈，所以使用CAS自旋、synchronized不会降低ConcurrentHashMap的性能。为什么不用ReentrantLock显式锁呢?如果为每 个桶都创建一个ReentrantLock实 例，就会带来大量的内存消耗，反过来，使用CAS自旋、synchronized，内存消耗的增加更小。 get() get()通过UnSafe的getObjectVolatile()来读取数组中的元素。为什么要这样做?虽然HashEntry数组的引用是volatile类型，但是数组内元素的 用不是volatile类型，因此多线程对 数组元素的修改是不安全的，可能会在数组中读取到尚未构造完成的元素对象。get()方法通过UnSafe的getObjectVolatile方法来保证元素的读取安全，调用getObjectVolatile()去读取数组元素需要先获得元素在数组中的偏移量，在这里，get()方法根据哈希码计算出偏移量为u，然后通过偏移量u来尝试读取数值。","categories":[{"name":"1.基础知识","slug":"1-基础知识","permalink":"https://yinhuidong.github.io/categories/1-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"集合源码分析","slug":"集合源码分析","permalink":"https://yinhuidong.github.io/tags/%E9%9B%86%E5%90%88%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"}]}],"categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/categories/MySQL/"},{"name":"1.基础知识","slug":"1-基础知识","permalink":"https://yinhuidong.github.io/categories/1-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/tags/MySQL/"},{"name":"集合源码分析","slug":"集合源码分析","permalink":"https://yinhuidong.github.io/tags/%E9%9B%86%E5%90%88%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"}]}