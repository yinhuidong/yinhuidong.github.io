{"meta":{"title":"二十","subtitle":"卷就完了","description":"欢迎来卷","author":"二十","url":"https://yinhuidong.github.io","root":"/"},"pages":[{"title":"留言板","date":"2022-01-10T12:46:21.418Z","updated":"2021-12-27T12:21:35.000Z","comments":true,"path":"message/index.html","permalink":"https://yinhuidong.github.io/message/index.html","excerpt":"","text":"本页面还在开发中……"},{"title":"","date":"2022-01-10T12:46:21.415Z","updated":"2021-12-27T12:21:34.000Z","comments":true,"path":"css/custom.css","permalink":"https://yinhuidong.github.io/css/custom.css","excerpt":"","text":"/* 文章页H1-H6图标样式效果 */ h1::before, h2::before, h3::before, h4::before, h5::before, h6::before { -webkit-animation: ccc 1.6s linear infinite ; animation: ccc 1.6s linear infinite ; } @-webkit-keyframes ccc { 0% { -webkit-transform: rotate(0deg); transform: rotate(0deg) } to { -webkit-transform: rotate(-1turn); transform: rotate(-1turn) } } @keyframes ccc { 0% { -webkit-transform: rotate(0deg); transform: rotate(0deg) } to { -webkit-transform: rotate(-1turn); transform: rotate(-1turn) } } #content-inner.layout h1::before { color: #ef50a8 ; margin-left: -1.55rem; font-size: 1.3rem; margin-top: -0.23rem; } #content-inner.layout h2::before { color: #fb7061 ; margin-left: -1.35rem; font-size: 1.1rem; margin-top: -0.12rem; } #content-inner.layout h3::before { color: #ffbf00 ; margin-left: -1.22rem; font-size: 0.95rem; margin-top: -0.09rem; } #content-inner.layout h4::before { color: #a9e000 ; margin-left: -1.05rem; font-size: 0.8rem; margin-top: -0.09rem; } #content-inner.layout h5::before { color: #57c850 ; margin-left: -0.9rem; font-size: 0.7rem; margin-top: 0.0rem; } #content-inner.layout h6::before { color: #5ec1e0 ; margin-left: -0.9rem; font-size: 0.66rem; margin-top: 0.0rem; } #content-inner.layout h1:hover, #content-inner.layout h2:hover, #content-inner.layout h3:hover, #content-inner.layout h4:hover, #content-inner.layout h5:hover, #content-inner.layout h6:hover { color: #49b1f5 ; } #content-inner.layout h1:hover::before, #content-inner.layout h2:hover::before, #content-inner.layout h3:hover::before, #content-inner.layout h4:hover::before, #content-inner.layout h5:hover::before, #content-inner.layout h6:hover::before { color: #49b1f5 ; -webkit-animation: ccc 3.2s linear infinite ; animation: ccc 3.2s linear infinite ; } /* 页面设置icon转动速度调整 */ #rightside_config i.fas.fa-cog.fa-spin { animation: fa-spin 5s linear infinite ; } /*--------更换字体------------*/ @font-face { font-family: 'tzy'; /* 字体名自定义即可 */ src: url('https://cdn.jsdelivr.net/gh/tzy13755126023/BLOG_SOURCE/font/ZhuZiAWan.woff2'); /* 字体文件路径 */ font-display: swap; } body, .gitcalendar { font-family: tzy !important; } .categoryBar-list { max-height: 400px; } .clock-row { overflow: hidden; text-overflow: ellipsis; } /*3s为加载动画的时间，1为加载动画的次数，ease-in-out为动画效果*/ #page-header, #web_bg { -webkit-animation: imgblur 2s 1 ease-in-out; animation: imgblur 2s 1 ease-in-out; } @keyframes imgblur { 0% { filter: blur(5px); } 100% { filter: blur(0px); } } /*适配使用-webkit内核的浏览器 */ @-webkit-keyframes imgblur { 0% { -webkit-filter: blur(5px); } 100% { -webkit-filter: blur(0px); } } .table-wrap img { margin: .6rem auto .1rem !important; } /* 标签外挂 网站卡片 start */ .site-card-group img { margin: 0 auto .1rem !important; } .site-card-group .info a img { margin-right: 10px !important; } [data-theme='dark'] .site-card-group .site-card .info .title { color: #f0f0f0 !important; } [data-theme='dark'] .site-card-group .site-card .info .desc { color: rgba(255, 255, 255, .7) !important; } .site-card-group .info .desc { margin-top: 4px !important; } /* 代码块颜色 */ figure.highlight pre .addition { color: #00bf03 !important; }"},{"title":"分类","date":"2022-01-10T12:46:21.418Z","updated":"2021-12-27T12:21:34.000Z","comments":true,"path":"categories/index.html","permalink":"https://yinhuidong.github.io/categories/index.html","excerpt":"","text":""},{"title":"","date":"2022-01-10T12:46:21.418Z","updated":"2021-12-27T12:21:35.000Z","comments":true,"path":"js/chocolate.js","permalink":"https://yinhuidong.github.io/js/chocolate.js","excerpt":"","text":"/* * @Author: tzy1997 * @Date: 2020-12-15 20:55:25 * @LastEditors: tzy1997 * @LastEditTime: 2021-01-12 19:02:25 */ // 友情链接页面 头像找不到时 替换图片 if (location.href.indexOf(\"link\") !== -1) { var imgObj = document.getElementsByTagName(\"img\"); for (i = 0; i < imgObj.length; i++) { imgObj[i].onerror = function() { this.src = \"https://cdn.jsdelivr.net/gh/tzy13755126023/BLOG_SOURCE/theme_f/friend_404.gif\" } } } $(function() { // 气泡 function bubble() { $('#page-header').circleMagic({ radius: 10, density: .2, color: 'rgba(255,255,255,.4)', clearOffset: 0.99 }); }! function(p) { p.fn.circleMagic = function(t) { var o, a, n, r, e = !0, i = [], d = p.extend({ color: \"rgba(255,0,0,.5)\", radius: 10, density: .3, clearOffset: .2 }, t), l = this[0]; function c() { e = !(document.body.scrollTop > a) } function s() { o = l.clientWidth, a = l.clientHeight, l.height = a + \"px\", n.width = o, n.height = a } function h() { if (e) for (var t in r.clearRect(0, 0, o, a), i) i[t].draw(); requestAnimationFrame(h) } function f() { var t = this; function e() { t.pos.x = Math.random() * o, t.pos.y = a + 100 * Math.random(), t.alpha = .1 + Math.random() * d.clearOffset, t.scale = .1 + .3 * Math.random(), t.speed = Math.random(), \"random\" === d.color ? t.color = \"rgba(\" + Math.floor(255 * Math.random()) + \", \" + Math.floor(0 * Math.random()) + \", \" + Math.floor(0 * Math.random()) + \", \" + Math.random().toPrecision(2) + \")\" : t.color = d.color } t.pos = {}, e(), this.draw = function() { t.alpha"},{"title":"关于我","date":"2022-01-11T02:40:01.624Z","updated":"2022-01-11T02:40:01.624Z","comments":true,"path":"关于我/index.html","permalink":"https://yinhuidong.github.io/%E5%85%B3%E4%BA%8E%E6%88%91/index.html","excerpt":"","text":"关于我十年生死两茫茫,写程序，到天亮。千行代码，Bug何处藏。纵使上线又怎样，朝令改，夕断肠。领导每天新想法，天天改，日日忙。 相顾无言，惟有泪千行。每晚灯火阑珊处，程序员，又加班，工作狂~ 🤣🤣🤣 基本信息 类别 信息 出生年月 1998年11月 现居地 北京市 籍贯 大庆市 邮箱 &#49;&#57;&#55;&#50;&#x30;&#x33;&#x39;&#55;&#x37;&#x33;&#64;&#113;&#113;&#x2e;&#99;&#111;&#x6d; 教育经历 时间 学校 专业 备注 2017.09~2021.07 齐齐哈尔大学 计算机科学与技术 统招本科 工作经历 时间 公司 职位 2021.06~至今 mi Java 开发工程师 专业技能 Java 基础扎实、掌握 JVM 原理、多线程、网络原理、设计模式、常用的数据结构和算法 熟悉 Windows、Mac、Linux 操作系统，熟练使用 linux 常用操作指令 熟练使用 IntelliJ IDEA 开发工具(及各种插件)、熟练使用 Git 版本同步工具 阅读过 Spring、SpringMVC、等开源框架源码，理解其设计原理及底层架构，具备框架定制开发能力 理解 Redis 线程模型，Netty 线程模型，掌握基于响应式的异步非阻塞模型的基本原理，了解 Webflux 熟练掌握分布式缓存 Redis、Elaticsearch，对分布式锁，幂等等常见问题有深入研究及多年实战经验 熟悉常见消息中间件的使用，有多年 RabbitMQ 的实战开发经验，对高级消息队列有深入理解 熟练掌握 Mysql 事务，索引，锁，SQL 优化相关知识，可根据业务场景给出详细及高性能设计方案 熟练使用数据库操作框架 Mybatis、Mybatsi-plus 进行高效业务功能开发 掌握 springCloud 相关框架，对 SpringBoot、SpringCloud 原理有一定了解，有成熟项目经验 熟悉定时任务及延迟任务等业务相关设计，如 xxl-job，延迟消息等相关技术有多年开发经验 熟悉微服务思想，MVC 分层，DDD 理论，服务拆分，治理，监控，服务熔断，降级等相关能力 熟悉 jvm 原理，熟悉垃圾回收以 jvm 性能调优技术，有过线上服务器性能监测及调优经验 熟悉多线程及线程池使用，有多年多线程业务处理经验，封装过多线程批处理工具类等公用组建 了解 Mysql 分库分表相关原理，如 Sardingsphere、Mycat 等框架有相关使用经验 了解操作系统底层原理以及 C、C++程序开发，对计算机底层原理有初步了解 了解前端开发，了解 html，css，js，vue 等前端技术，对前端开发有一定的了解 研究过单片机等硬件开发，喜欢科技产品，喜欢软件，喜欢折腾各种电子产品以及软件 项目经验​ 自我评价NB。"},{"title":"技术笔记","date":"2022-01-10T12:46:21.418Z","updated":"2021-12-27T12:21:35.000Z","comments":true,"path":"技术笔记/index.html","permalink":"https://yinhuidong.github.io/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/index.html","excerpt":"","text":"我是技术笔记"},{"title":"标签","date":"2022-01-10T12:46:21.418Z","updated":"2021-12-27T12:21:35.000Z","comments":true,"path":"tags/index.html","permalink":"https://yinhuidong.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"软件设计概述","slug":"设计模式/软件设计概述","date":"2022-01-11T12:02:25.638Z","updated":"2022-01-11T12:28:53.706Z","comments":true,"path":"2022/01/11/设计模式/软件设计概述/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%BD%AF%E4%BB%B6%E8%AE%BE%E8%AE%A1%E6%A6%82%E8%BF%B0/","excerpt":"","text":"一，软件设计概述1.软件设计的概念软件设计模式（或者说是设计模式），是一套被反复使用，多数人知晓的，经过系统整理的代码设计经验的总结。 它描述了在软件设计过程中的一些不断重复发生的问题，以及该问题的解决方案。 它是解决特定问题的一系列套路，是代码设计经验的总结，具有一定的普遍性，可以反复使用。 其目的是为了提高代码的可重用性、代码的可读性和代码的可靠性。 2.学习设计模式的意义设计模式的本质是面向对象设计原则的实际运用，是对类的封装性、继承性和多态性以及类的关联关系和组合关系的充分理解。 可以提高程序员的思维能力、编程能力和设计能力。 使程序设计更加标准化、代码编制更加工程化，使软件开发效率大大提高，从而缩短软件的开发周期。 使设计的代码可重用性高、可读性强、可靠性高、灵活性好、可维护性强。 软件设计模式只是一个引导。在具体的软件幵发中，必须根据设计的应用系统的特点和要求来恰当选择。对于简单的程序开发，苛能写一个简单的算法要比引入某种设计模式更加容易。但对大项目的开发或者框架设计，用设计模式来组织代码显然更好。 3.软件设计模式的基本要素 名称：可以根据模式的问题、特点、解决方案、功能和效果来命名。 问题：描述了该模式的应用环境，即何时使用该模式。它解释了设计问题和问题存在的前因后果，以及必须满足的一系列先决条件。 方案：包括设计的组成成分、它们之间的相互关系及各自的职责和协作方式。因为模式就像一个模板，可应用于多种不同场合，所以解决方案并不描述一个特定而具体的设计或实现，而是提供设计问题的抽象描述和怎样用一个具有一般意义的元素组合（类或对象的 组合）来解决这个问题。 效果：模式的优缺点。主要是对时间和空间的衡量，以及该模式对系统的灵活性、扩充性、可移植性的影响，也考虑其实现问题。 二，设计模式的分类与功能1.根据目的来分根据模式是用来完成什么工作来划分，这种方式可分为创建型模式、结构型模式和行为型模式 3 种。 创建型模式：用于描述“怎样创建对象”，它的主要特点是“将对象的创建与使用分离”。GoF 中提供了单例、原型、工厂方法、抽象工厂、建造者等 5 种创建型模式。 结构型模式：用于描述如何将类或对象按某种布局组成更大的结构，GoF 中提供了代理、适配器、桥接、装饰、外观、享元、组合等 7 种结构型模式。 行为型模式：用于描述类或对象之间怎样相互协作共同完成单个对象都无法单独完成的任务，以及怎样分配职责。GoF 中提供了模板方法、策略、命令、职责链、状态、观察者、中介者、迭代器、访问者、备忘录、解释器等 11 种行为型模式。 2.根据作用范围来分根据模式是主要用于类上还是主要用于对象上来分，这种方式可分为类模式和对象模式两种。 类模式：用于处理类与子类之间的关系，这些关系通过继承来建立，是静态的，在编译时刻便确定下来了。GoF中的工厂方法、（类）适配器、模板方法、解释器属于该模式。 对象模式：用于处理对象之间的关系，这些关系可以通过组合或聚合来实现，在运行时刻是可以变化的，更具动态性。GoF 中除了以上 4 种，其他的都是对象模式。 范围/目的 创建型模式 结构型模式 行为型模式 类模式 工厂方法 (类）适配器 模板方法、解释器 对象模式 单例 原型 抽象工厂 建造者 代理 (对象）适配器 桥接 装饰 外观 享元 组合 策略 命令 职责链 状态 观察者 中介者 迭代器 访问者 备忘录 3.设计模式的功能 单例（Singleton）模式：某个类只能生成一个实例，该类提供了一个全局访问点供外部获取该实例，其拓展是有限多例模式。 原型（Prototype）模式：将一个对象作为原型，通过对其进行复制而克隆出多个和原型类似的新实例。 工厂方法（Factory Method）模式：定义一个用于创建产品的接口，由子类决定生产什么产品。 抽象工厂（AbstractFactory）模式：提供一个创建产品族的接口，其每个子类可以生产一系列相关的产品。 建造者（Builder）模式：将一个复杂对象分解成多个相对简单的部分，然后根据不同需要分别创建它们，最后构建成该复杂对象。 代理（Proxy）模式：为某对象提供一种代理以控制对该对象的访问。即客户端通过代理间接地访问该对象，从而限制、增强或修改该对象的一些特性。 适配器（Adapter）模式：将一个类的接口转换成客户希望的另外一个接口，使得原本由于接口不兼容而不能一起工作的那些类能一起工作。 桥接（Bridge）模式：将抽象与实现分离，使它们可以独立变化。它是用组合关系代替继承关系来实现，从而降低了抽象和实现这两个可变维度的耦合度。 装饰（Decorator）模式：动态的给对象增加一些职责，即增加其额外的功能。 外观（Facade）模式：为多个复杂的子系统提供一个一致的接口，使这些子系统更加容易被访问。 享元（Flyweight）模式：运用共享技术来有效地支持大量细粒度对象的复用。 组合（Composite）模式：将对象组合成树状层次结构，使用户对单个对象和组合对象具有一致的访问性。 模板方法（TemplateMethod）模式：定义一个操作中的算法骨架，而将算法的一些步骤延迟到子类中，使得子类可以不改变该算法结构的情况下重定义该算法的某些特定步骤。 策略（Strategy）模式：定义了一系列算法，并将每个算法封装起来，使它们可以相互替换，且算法的改变不会影响使用算法的客户。 命令（Command）模式：将一个请求封装为一个对象，使发出请求的责任和执行请求的责任分割开。 职责链（Chain of Responsibility）模式：把请求从链中的一个对象传到下一个对象，直到请求被响应为止。通过这种方式去除对象之间的耦合。 状态（State）模式：允许一个对象在其内部状态发生改变时改变其行为能力。 观察者（Observer）模式：多个对象间存在一对多关系，当一个对象发生改变时，把这种改变通知给其他多个对象，从而影响其他对象的行为。 中介者（Mediator）模式：定义一个中介对象来简化原有对象之间的交互关系，降低系统中对象间的耦合度，使原有对象之间不必相互了解。 迭代器（Iterator）模式：提供一种方法来顺序访问聚合对象中的一系列数据，而不暴露聚合对象的内部表示。 访问者（Visitor）模式：在不改变集合元素的前提下，为一个集合中的每个元素提供多种访问方式，即每个元素有多个访问者对象访问。 备忘录（Memento）模式：在不破坏封装性的前提下，获取并保存一个对象的内部状态，以便以后恢复它。 解释器（Interpreter）模式：提供如何定义语言的文法，以及对语言句子的解释方法，即解释器。 三，UML统一建模语言UML（Unified Modeling Language，统一建模语言）是用来设计软件蓝图的可视化建模语言，是一种为面向对象系统的产品进行说明、可视化和编制文档的标准语言，独立于任何一种具体的程序设计语言。 它的特点是简单、统一、图形化、能表达软件设计中的动态与静态信息。 1.应用场景UML 能为软件开发的所有阶段提供模型化和可视化支持。而且融入了软件工程领域的新思想、新方法和新技术，使软件设计人员沟通更简明，进一步缩短了设计时间，减少开发成本。 UML 具有很宽的应用领域。其中最常用的是建立软件系统的模型，但它同样可以用于描述非软件领域的系统，如机械系统、企业机构或业务过程，以及处理复杂数据的信息系统、具有实时要求的工业系统或工业过程等。总之，UML 可以对任何具有静态结构和动态行为的系统进行建模，而且使用于从需求规格描述直至系统完成后的测试和维护等系统开发的各个阶段。 UML 模型大多以图表的方式表现出来，一份典型的建模图表通常包含几个块或框、连接线和作为模型附加信息的文本。这些虽简单却非常重要，在 UML 规则中相互联系和扩展。 2.基本构件UML 建模的核心是模型，模型是现实的简化、真实系统的抽象。UML 提供了系统的设计蓝图。当给软件系统建模时，需要采用通用的符号语言，这种描述模型所使用的语言被称为建模语言。在 UML 中，所有的描述由事物、关系和图这些构件组成。下图完整地描述了所有构件的关系。 下面对具体构件进行说明。 3.事物事物是抽象化的最终结果，分为结构事物、行为事物、分组事物和注释事物。 1）结构事物结构事物是模型中的静态部分，用以呈现概念或实体的表现元素。 事物 解释 图例 类（Class） 具有相同属性、方法、关系和语义的对象集合 接口（Interface） 指一个类或构件的一个服务的操作集合，它仅仅定义了一组操作的规范，并没有给出这组操作的具体实现 用例（User Case） 指对一组动作序列的描述，系统执行这些动作将产生一个对特定的参与者（Actor）有价值且可观察的结果 协作（Collaboration） 定义元素之间的相互作用 组件（Component） 描述物理系统的一部分 活动类（Active Class） 指对象有一个或多个进程或线程。活动类和类很相象，只是它的对象代表的元素的行为和其他元素是同时存在的 节点（Node） 定义为运行时存在的物理元素 2）行为事物行为事物指 UML 模型中的动态部分。 事物 解释 用例 交互（Interaction） 包括一组元素之间的消息交换 状态机（State Machine） 由一系列对象的状态组成 3）分组事物目前只有一种分组事物，即包。包纯碎是概念上的，只存在于开发阶段，结构事物、行为事物甚至分组事物都有可能放在一个包中。 事物 解释 用例 包（Package） UML中唯一的组织机制 4）注释事物注释事物是解释 UML 模型元素的部分。 事物 解释 用例 注释（Note） 用于解析说明 UML 元素 4.图UML2.0 一共有 13 种图（UML1.5 定义了 9 种，UML2.0 增加了 4 种），分别是类图、对象图、构件图、部署图、活动图、状态图、用例图、时序图、协作图 9 种，以及包图、组合结构图、时间图、交互概览图 4 种。 图名称 解释 类图（Class Diagrams） 用于定义系统中的类 对象图（Object Diagrams） 类图的一个实例，描述了系统在具体时间点上所包含的对象及各个对象之间的关系 构件图（Component Diagrams） 一种特殊的 UML 图，描述系统的静态实现视图 部署图（Deployment Diagrams） 定义系统中软硬件的物理体系结构 活动图（Activity Diagrams） 用来描述满足用例要求所要进行的活动及活动间的约束关系 状态图（State Chart Diagrams） 用来描述类的对象的所有可能的状态和时间发生时，状态的转移条件 用例图（Usecase Diagrams） 用来描述用户的需求，从用户的角度描述系统的功能，并指出各功能的执行者，强调谁在使用系统、系统为执行者完成哪些功能 时序图（Sequence Diagrams） 描述对象之间的交互顺序，着重体现对象间消息传递的时间顺序，强调对象之间消息的发送顺序，同时显示对象之间的交互过程 协作图（Collaboration Diagrams） 描述对象之间的合作关系，更侧重向用户对象说明哪些对象有消息的传递 包图（Package Diagrams） 对构成系统的模型元素进行分组整理的图 组合结构图（Composite Structure Diagrams） 表示类或者构建内部结构的图 时间图（Timing Diagrams） 用来显示随时间变化，一个或多个元素的值或状态的更改，也显示时间控制事件之间的交互及管理它们的时间和期限约束 交互概览图（Interaction Overview Diagrams） 用活动图来表示多个交互之间的控制关系的图 四，UML类图及类图之间的关系类图是一种模型类型，确切地说，是一种静态模型类型。类图表示类、接口和它们之间的协作关系，用于系统设计阶段。 1.类，接口，类图1）类类（Class）是指具有相同属性、方法和关系的对象的抽象，它封装了数据和行为，是面向对象程序设计（OOP）的基础，具有封装性、继承性和多态性等三大特性。在 UML 中，类使用包含类名、属性和操作且带有分隔线的矩形来表示。 (1) 类名（Name）是一个字符串，例如，Student。 (2) 属性（Attribute）是指类的特性，即类的成员变量。UML 按以下格式表示： 1[可见性]属性名:类型[=默认值] 例如：-name:String 注意：“可见性”表示该属性对类外的元素是否可见，包括公有（Public）、私有（Private）、受保护（Protected）和朋友（Friendly）4 种，在类图中分别用符号+、-、#、~表示。 (3) 操作（Operations）是类的任意一个实例对象都可以使用的行为，是类的成员方法。UML 按以下格式表示： 1[可见性]名称(参数列表)[:返回类型] 例如：+display():void。 如下所示是学生类的 UML 表示。 类图中，需注意以下几点： 抽象类或抽象方法用斜体表示 如果是接口，则在类名上方加 &lt;&gt; 字段和方法返回值的数据类型非必需 静态类或静态方法加下划线 2）接口接口（Interface）是一种特殊的类，它具有类的结构但不可被实例化，只可以被子类实现。它包含抽象操作，但不包含属性。它描述了类或组件对外可见的动作。在 UML 中，接口使用一个带有名称的小圆圈来进行表示。 如下所示是图形类接口的 UML 表示。 3)类图类图（ClassDiagram）是用来显示系统中的类、接口、协作以及它们之间的静态结构和关系的一种静态模型。它主要用于描述软件系统的结构化设计，帮助人们简化对软件系统的理解，它是系统分析与设计阶段的重要产物，也是系统编码与测试的重要模型依据。 类图中的类可以通过某种编程语言直接实现。类图在软件系统开发的整个生命周期都是有效的，它是面向对象系统的建模中最常见的图。如下所示是“计算长方形和圆形的周长与面积”的类图，图形接口有计算面积和周长的抽象方法，长方形和圆形实现这两个方法供访问类调用。 计算长方形与圆形的周长与面积 2.类之间的关系UML 将事物之间的联系归纳为 6 种，并用对应的图形类表示。下面根据类与类之间的耦合度从弱到强排列。UML 中的类图有以下几种关系：依赖关系、关联关系、聚合关系、组合关系、泛化关系和实现关系。其中泛化和实现的耦合度相等，它们是最强的。 1）依赖关系依赖（Dependency）关系是一种使用关系，它是对象之间耦合度最弱的一种关联方式，是临时性的关联。在代码中，某个类的方法通过局部变量、方法的参数或者对静态方法的调用来访问另一个类（被依赖类）中的某些方法来完成一些职责。 在 UML 类图中，依赖关系使用带箭头的虚线来表示，箭头从使用类指向被依赖的类。如下是人与手机的关系图，人通过手机的语音传送方法打电话。 依赖关系的实例 2）关联关系关联（Association）关系是对象之间的一种引用关系，用于表示一类对象与另一类对象之间的联系，如老师和学生、师傅和徒弟、丈夫和妻子等。关联关系是类与类之间最常用的一种关系，分为一般关联关系、聚合关系和组合关系。我们先介绍一般关联。 关联可以是双向的，也可以是单向的。在 UML 类图中，双向的关联可以用带两个箭头或者没有箭头的实线来表示，单向的关联用带一个箭头的实线来表示，箭头从使用类指向被关联的类。也可以在关联线的两端标注角色名，代表两种不同的角色。 在代码中通常将一个类的对象作为另一个类的成员变量来实现关联关系。如下是老师和学生的关系图，每个老师可以教多个学生，每个学生也可向多个老师学，他们是双向关联。 3）聚合关系聚合（Aggregation）关系是关联关系的一种，是强关联关系，是整体和部分之间的关系，是 has-a 的关系。 聚合关系也是通过成员对象来实现的，其中成员对象是整体对象的一部分，但是成员对象可以脱离整体对象而独立存在。例如，学校与老师的关系，学校包含老师，但如果学校停办了，老师依然存在。 在 UML 类图中，聚合关系可以用带空心菱形的实线来表示，菱形指向整体。如下是大学和教师的关系图。 4）组合关系组合（Composition）关系也是关联关系的一种，也表示类之间的整体与部分的关系，但它是一种更强烈的聚合关系，是 cxmtains-a 关系。 在组合关系中，整体对象可以控制部分对象的生命周期，一旦整体对象不存在，部分对象也将不存在，部分对象不能脱离整体对象而存在。例如，头和嘴的关系，没有了头，嘴也就不存在了。 在 UML 类图中，组合关系用带实心菱形的实线来表示，菱形指向整体。如下是头和嘴的关系图。 5）泛化关系泛化（Generalization）关系是对象之间耦合度最大的一种关系，表示一般与特殊的关系，是父类与子类之间的关系，是一种继承关系，是 is-a 的关系。 在 UML 类图中，泛化关系用带空心三角箭头的实线来表示，箭头从子类指向父类。在代码实现时，使用面向对象的继承机制来实现泛化关系。例如，Student 类和 Teacher 类都是 Person 类的子类，其类图如下所示。 6）实现关系实现（Realization）关系是接口与实现类之间的关系。在这种关系中，类实现了接口，类中的操作实现了接口中所声明的所有的抽象操作。 在 UML 类图中，实现关系使用带空心三角箭头的虚线来表示，箭头从实现类指向接口。例如，汽车和船实现了交通工具，其类图如下所示。 3.类关系记忆技巧 分类 箭头特征 记忆技巧 箭头方向 从子类指向父类 定义子类需要通过 extends 关键字指定父类 子类一定是知道父类定义的，但父类并不知道子类的定义 只有知道对方信息时才能指向对方 箭头的方向是从子类指向父类 继承/实现 用线条连接两个类； 空心三角箭头表示继承或实现 实现表示继承，是is-a的关系，表示扩展，不虚，很结实 虚线表示实现，虚线代表“虚”无实体 关联/依赖 用线条连接两个类； 普通箭头表示关联或依赖 虚线表示依赖关系：临时用一下，若即若离，虚无缥缈，若有若无 表示一种使用关系，一个类需要借助另一个类来实现功能 一般一个类将另一个类作为参数使用，或作为返回值 实线表示关联关系：关系稳定，实打实的关系 表示一个类对象和另一个类对象有关联 通常一个类中有另一个类对象作为属性 组合/聚合 用菱形表示：像一个盛东西的器皿（如盘子） 聚合：空心菱形，代表空器皿里可以放很多相同的东西，聚集在一起（箭头方向所指的类） 整体和局部的关系，两者有独立的生命周期，是 has-a 的关系 弱关系，消极的词：弱-空 组合：实心菱形，代表器皿里已经有实体结构的存在，生死与共 整体与局部的关系，和聚合关系对比，关系更加强烈，两者具有相同的生命周期，contains-a 的关系 强关系，积极的词；强-满 | UML 的标准类关系图中，没有实心箭头。有些 Java 编程的 IDE 自带类生成工具可能出现实心箭头，主要目的是降低理解难度。 下图是对动物衍生关系描述的类图。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"软件架构","slug":"软件架构","permalink":"https://yinhuidong.github.io/tags/%E8%BD%AF%E4%BB%B6%E6%9E%B6%E6%9E%84/"}]},{"title":"软件架构的基本原则","slug":"设计模式/软件架构的基本原则","date":"2022-01-11T12:02:16.665Z","updated":"2022-01-11T12:28:39.120Z","comments":true,"path":"2022/01/11/设计模式/软件架构的基本原则/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%BD%AF%E4%BB%B6%E6%9E%B6%E6%9E%84%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%8E%9F%E5%88%99/","excerpt":"","text":"一，软件架构的两个要点1.代码复用无论是开发哪种软件产品，成本和时间都是最重要的。较少的开发时间意味着可以比竞争对手更早进入市场。较低的开发成本意味着能够留出更多的营销资金，覆盖更广泛的潜在客户。 代码复用是减少开发成本最常用的方式之一，其目的非常明显，即：与其反复从头开发，不如在新对象中重用已有的代码。 使用设计模式是增加软件组件灵活性并使其易于复用的方式之一。但是，这可能也会让组件变得更加复杂。 一般情况下，复用可以分为三个层次。在最底层，可以复用类、类库、容器，也许还有一些类的“团体（例如容器和迭代器）”。 框架位于最高层。它们能帮助你精简自己的设计，可以明确解决问题所需的抽象概念，然后用类来表示这些概念并定义其关系。例如，JUnit 是一个小型框架，也是框架的“Hello, world”，其中定义了 Test、TestCase 和 TestSuite 这几个类及其关系。框架通常比单个类的颗粒度要大。你可以通过在某处构建子类来与框架建立联系。这些子类信奉“别给我们打电话，我们会给你打电话的。” 还有一个中间层次。这是我觉得设计模式所处的位置。设计模式比框架更小且更抽象。它们实际上是对一组类的关系及其互动方式的描述。当你从类转向模式，并最终到达框架的过程中，复用程度会不断增加。 中间层次的优点在于模式提供的复用方式要比框架的风险小。创建框架是一项投入重大且风险很高的工作，模式则能让你独立于具体代码来复用设计思想和理念。 2.扩展性需求变化是程序员生命中唯一不变的事情。比如以下几种场景： 你在 Windows 平台上发布了一款游戏，现在人们想要 Mac OS 的版本。 你创建了一个使用方形按钮的 GUI 框架，但几个月后开始流行原型按钮。 你设计了一款优秀的电子商务网站，但仅仅几个月后，客户就要求新增电话订单的功能。 首先，在完成了第一版的程序后，我们就应该做好了从头开始优化重写代码的准备，因为现在你已经能在很多方面更好的理解问题了，同时在专业水平上也有所提高，所以之前的代码现在看上去可能会显得很糟糕。 其次，可能是在你掌控之外的某些事情发生了变化，这也是导致许多开发团队转变最初想法的原因。比如，每位在网络应用中使用 Flash 的开发者都必须重新开发或移植代码，因为不断地有浏览器停止对 Flash 格式地支持。 最后，可能是需求的改变，之前你的客户对当前版本的程序感到满意，但是现在希望对程序进行 11 个“小小”的改动，使其可完成原始计划阶段中完全没有提到的功能，新增或改变功能。 当然这也有好的一面，如果有人要求你对程序进行修改，至少说明还有人关心它。因此在设计程序架构时，有经验的开发者都会尽量选择支持未来任何可能变更的方式。 二，正确使用设计模式设计模式不是为每个人准备的，而是基于业务来选择设计模式，需要时就能想到它。要明白一点，技术永远为业务服务，技术只是满足业务需要的一个工具。我们需要掌握每种设计模式的应用场景、特征、优缺点，以及每种设计模式的关联关系，这样就能够很好地满足日常业务的需要。 设计模式不是为了特定场景而生的，而是为了让人可以更好和更快地开发。 设计模式只是实现了七大设计原则的具体方式，套用太多设计模式只会陷入模式套路陷阱，最后代码写的凌乱不堪。 不能为了使用设计模式而去做架构，而是有了做架构的需求后，发现它符合某一类设计模式的结构，在将两者结合。 想要游刃有余地使用设计模式，需要打下牢固的程序设计语言基础、夯实自己的编程思想、积累大量的时间经验、提高开发能力。目的都是让程序低耦合，高复用，高内聚，易扩展，易维护。 1.需求驱动不仅仅是功能性需求，需求驱动还包括性能和运行时的需求，如软件的可维护性和可复用性等方面。设计模式是针对软件设计的，而软件设计是针对需求的，一定不要为了使用设计模式而使用设计模式，否则可能会使设计变得复杂，使软件难以调试和维护。 2.分析已经存在的项目对现有的应用实例进行分析是一个很好的学习途径，应当注意学习已有的项目，而不仅是学习设计模式如何实现，更重要的是注意在什么场合使用设计模式。 3.掌握当前开发平台设计模式大部分都是针对面向对象的软件设计，因此在理论上适合任何面向对象的语言，但随着技术的发展和编程环境的改善，设计模式的实现方式会有很大的差别。在一些平台下，某些设计模式是自然实现的。 4.开发中领悟软件开发是一项实践工作，最直接的方法就是编程。没有从来不下棋却熟悉定式的围棋高手，也没有不会编程就能成为架构设计师的先例。掌握设计模式是水到渠成的事情，除了理论只是和实践积累，可能会“渐悟”或者“顿悟”。 5.避免过度设计设计模式解决的是设计不足的问题，但同时也要避免设计过度。一定要牢记简洁原则，要知道设计模式是为了使设计简单，而不是更复杂。如果引入设计模式使得设计变得复杂，只能说我们把简单问题复杂化了，问题本身不需要设计模式。 这里需要把握的是需求变化的程度，一定要区分需求的稳定部分和可变部分。一个软件必然有稳定部分，这个部分就是核心业务逻辑。如果核心业务逻辑发生变化，软件就没有存在的必要，核心业务逻辑是我们需要固化的。对于可变的部分，需要判断可能发生变化的程度来确定设计策略和设计风险。要知道，设计过度与设计不足同样对项目有害。 设计模式从来都不是单个设计模式独立使用的。在实际应用中，通常多个设计模式混合使用，你中有我，我中有你。 三，开闭原则1.定义软件实体应当对扩展开放，对修改关闭。 何为软件实体？ 项目中划分出的模块 类与接口 方法 开闭原则的含义是：当应用的需求改变时，在不修改软件实体的源代码或者二进制代码的前提下，可以扩展模块的功能，使其满足新的需求。 2.作用开闭原则是面向对象程序设计的终极目标，它使软件实体拥有一定的适应性和灵活性的同时具备稳定性和延续性。 软件测试：软件遵守开闭原则的话，软件测试时只需要对扩展的代码进行测试就可以了，因为原有的测试代码仍然能够正常运行。 提高代码复用性：粒度越小，被复用的可能性就越大；在面向对象的程序设计中，根据原子和抽象编程可以提高代码的可复用性。 提高软件可维护性：遵守开闭原则的软件，其稳定性高和延续性强，从而易于扩展和维护。 3.实现方法可以通过“抽象约束、封装变化”来实现开闭原则，即通过接口或者抽象类为软件实体定义一个相对稳定的抽象层，而将相同的可变因素封装在相同的具体实现类中。 因为抽象灵活性好，适应性广，只要抽象的合理，可以基本保持软件架构的稳定。而软件中易变的细节可以从抽象派生来的实现类来进行扩展，当软件需要发生变化时，只需要根据需求重新派生一个实现类来扩展就可以了。 Windows 的桌面主题设计 Windows 的主题是桌面背景图片、窗口颜色和声音等元素的组合。用户可以根据自己的喜爱更换自己的桌面主题，也可以从网上下载新的主题。这些主题有共同的特点，可以为其定义一个抽象类（Abstract Subject），而每个具体的主题（Specific Subject）是其子类。用户窗体可以根据需要选择或者增加新的主题，而不需要修改原代码，所以它是满足开闭原则的。 四，里氏替换原则1.定义继承必须确保超类所拥有的性质在子类中仍然成立 里氏替换原则主要阐述了有关继承的一些原则，也就是什么时候应该使用继承，什么时候不应该使用继承，以及其中蕴含的原理。里氏替换原是继承复用的基础，它反映了基类与子类之间的关系，是对开闭原则的补充，是对实现抽象化的具体步骤的规范。 2.作用 里氏替换原则是实现开闭原则的重要方式之一 它克服了继承中重写父类造成的可复用性变差的缺点 它是动作正确性的保证。即类的扩展不会给已有的系统引入新的错误，降低了代码出错的可能性 加强程序的健壮性，同时变更时可以做到非常好的兼容性，提高程序的维护性、可扩展性，降低需求变更时引入的风险 3.实现方法里氏替换原则通俗来讲就是：子类可以扩展父类的功能，但不能改变父类原有的功能。也就是说：子类继承父类时，除添加新的方法完成新增功能外，尽量不要重写父类的方法。 子类可以实现父类的抽象方法，但不能覆盖父类的非抽象方法 子类中可以增加自己特有的方法 当子类的方法重载父类的方法时，方法的前置条件（即方法的输入参数）要比父类的方法更宽松 当子类的方法实现父类的方法时（重写/重载或实现抽象方法），方法的后置条件（即方法的的输出/返回值）要比父类的方法更严格或相等 通过重写父类的方法来完成新的功能写起来虽然简单，但是整个继承体系的可复用性会比较差，特别是运用多态比较频繁时，程序运行出错的概率会非常大。 如果程序违背了里氏替换原则，则继承类的对象在基类出现的地方会出现运行错误。这时其修正方法是：取消原来的继承关系，重新设计它们之间的关系。 里氏替换原则在“几维鸟不是鸟”实例中的应用。 分析：鸟一般都会飞行，如燕子的飞行速度大概是每小时 120 千米。但是新西兰的几维鸟由于翅膀退化无法飞行。假如要设计一个实例，计算这两种鸟飞行 300 千米要花费的时间。显然，拿燕子来测试这段代码，结果正确，能计算出所需要的时间；但拿几维鸟来测试，结果会发生“除零异常”或是“无穷大”，明显不符合预期。 正确的做法是：取消几维鸟原来的继承关系，定义鸟和几维鸟的更一般的父类，如动物类，它们都有奔跑的能力。几维鸟的飞行速度虽然为 0，但奔跑速度不为 0，可以计算出其奔跑 300 千米所要花费的时间。 五，依赖倒置原则1.定义高层模块不应该依赖低层模块，两者都应该依赖其抽象；抽象不应该依赖细节，细节应该依赖抽象 核心思想是：要面向接口编程，不要面向实现编程。 依赖倒置原则是实现开闭原则的重要途径之一，它降低了客户与实现模块之间的耦合。 由于在软件设计中，细节具有多变性，而抽象层则相对稳定，因此以抽象为基础搭建起来的架构要比以细节为基础搭建起来的架构要稳定得多。这里的抽象指的是接口或者抽象类，而细节是指具体的实现类。 使用接口或者抽象类的目的是制定好规范和契约，而不去涉及任何具体的操作，把展现细节的任务交给它们的实现类去完成。 2.作用 依赖倒置原则可以降低类间的耦合性。 依赖倒置原则可以提高系统的稳定性。 依赖倒置原则可以减少并行开发引起的风险。 依赖倒置原则可以提高代码的可读性和可维护性。 3.实现方法依赖倒置原则的目的是通过要面向接口的编程来降低类间的耦合性，所以我们在实际编程中只要遵循以下4点，就能在项目中满足这个规则。 每个类尽量提供接口或抽象类，或者两者都具备。 变量的声明类型尽量是接口或者是抽象类。 任何类都不应该从具体类派生 使用继承时尽量遵循里氏替换原则 依赖倒置原则在“顾客购物程序”中的应用。 六，单一职责原则1.定义一个类应该有且仅有一个引起它变化的原因，否则类应该被拆分。 该原则提出对象不应该承担太多职责，如果一个对象承担了太多的职责，至少存在以下两个缺点： 一个职责的变化可能会削弱或者抑制这个类实现其他职责的能力； 当客户端需要该对象的某一个职责时，不得不将其他不需要的职责全都包含进来，从而造成冗余代码或代码的浪费。 2.优点单一职责原则的核心就是控制类的粒度大小、将对象解耦、提高其内聚性。如果遵循单一职责原则将有以下优点。 降低类的复杂度。一个类只负责一项职责，其逻辑肯定要比负责多项职责简单得多。 提高类的可读性。复杂性降低，自然其可读性会提高。 提高系统的可维护性。可读性提高，那自然更容易维护了。 变更引起的风险降低。变更是必然的，如果单一职责原则遵守得好，当修改一个功能时，可以显著降低对其他功能的影响。 3.实现方法单一职责原则是最简单但又最难运用的原则，需要设计人员发现类的不同职责并将其分离，再封装到不同的类或模块中。而发现类的多重职责需要设计人员具有较强的分析设计能力和相关重构经验。 大学学生工作管理程序 分析：大学学生工作主要包括学生生活辅导和学生学业指导两个方面的工作，其中生活辅导主要包括班委建设、出勤统计、心理辅导、费用催缴、班级管理等工作，学业指导主要包括专业引导、学习辅导、科研指导、学习总结等工作。如果将这些工作交给一位老师负责显然不合理，正确的做 法是生活辅导由辅导员负责，学业指导由学业导师负责。 单一职责同样也适用于方法。一个方法应该尽可能做好一件事情。如果一个方法处理的事情太多，其颗粒度会变得很粗，不利于重用。 七，接口隔离原则1.定义单一职责同样也适用于方法。一个方法应该尽可能做好一件事情。如果一个方法处理的事情太多，其颗粒度会变得很粗，不利于重用。 要为各个类建立它们需要的专用接口，而不要试图去建立一个很庞大的接口供所有依赖它的类去调用。 接口隔离原则和单一职责都是为了提高类的内聚性、降低它们之间的耦合性，体现了封装的思想，但两者是不同的： 单一职责原则注重的是职责，而接口隔离原则注重的是对接口依赖的隔离。 单一职责原则主要是约束类，它针对的是程序中的实现和细节；接口隔离原则主要约束接口，主要针对抽象和程序整体框架的构建。 2.优点接口隔离原则是为了约束接口、降低类对接口的依赖性，遵循接口隔离原则有以下 5 个优点。 将臃肿庞大的接口分解为多个粒度小的接口，可以预防外来变更的扩散，提高系统的灵活性和可维护性。 接口隔离提高了系统的内聚性，减少了对外交互，降低了系统的耦合性。 如果接口的粒度大小定义合理，能够保证系统的稳定性；但是，如果定义过小，则会造成接口数量过多，使设计复杂化；如果定义太大，灵活性降低，无法提供定制服务，给整体项目带来无法预料的风险。 使用多个专门的接口还能够体现对象的层次，因为可以通过接口的继承，实现对总接口的定义。 能减少项目工程中的代码冗余。过大的大接口里面通常放置许多不用的方法，当实现这个接口的时候，被迫设计冗余的代码。 3.实现方法在具体应用接口隔离原则时，应该根据以下几个规则来衡量。 接口尽量小，但是要有限度。一个接口只服务于一个子模块或业务逻辑。 为依赖接口的类定制服务。只提供调用者需要的方法，屏蔽不需要的方法。 了解环境，拒绝盲从。每个项目或产品都有选定的环境因素，环境不同，接口拆分的标准就不同深入了解业务逻辑。 提高内聚，减少对外交互。使接口用最少的方法去完成最多的事情。 学生成绩管理程序 分析：学生成绩管理程序一般包含插入成绩、删除成绩、修改成绩、计算总分、计算均分、打印成绩信息、査询成绩信息等功能，如果将这些功能全部放到一个接口中显然不太合理，正确的做法是将它们分别放在输入模块、统计模块和打印模块等 3 个模块中。 八，迪米特法则1.定义如果两个软件实体无须直接通信，那么就不应当发生直接的相互调用，可以通过第三方转发该调用。其目的是降低类之间的耦合度，提高模块的相对独立性。 迪米特法则中的“朋友”是指：当前对象本身、当前对象的成员对象、当前对象所创建的对象、当前对象的方法参数等，这些对象同当前对象存在关联、聚合或组合关系，可以直接访问这些对象的方法。 2.优点迪米特法则要求限制软件实体之间通信的宽度和深度，正确使用迪米特法则将有以下两个优点。 降低了类之间的耦合度，提高了模块的相对独立性。 由于亲合度降低，从而提高了类的可复用率和系统的扩展性。 但是，过度使用迪米特法则会使系统产生大量的中介类，从而增加系统的复杂性，使模块之间的通信效率降低。所以，在釆用迪米特法则时需要反复权衡，确保高内聚和低耦合的同时，保证系统的结构清晰。 3.实现方法从迪米特法则的定义和特点可知，它强调以下两点： 从依赖者的角度来说，只依赖应该依赖的对象。 从被依赖者的角度说，只暴露应该暴露的方法。 所以，在运用迪米特法则时要注意以下 6 点。 在类的划分上，应该创建弱耦合的类。类与类之间的耦合越弱，就越有利于实现可复用的目标。 在类的结构设计上，尽量降低类成员的访问权限。 在类的设计上，优先考虑将一个类设置成不变类。 在对其他类的引用上，将引用其他对象的次数降到最低。 不暴露类的属性成员，而应该提供相应的访问器（set 和 get 方法）。 谨慎使用序列化（Serializable）功能。 明星与经纪人的关系实例 分析：明星由于全身心投入艺术，所以许多日常事务由经纪人负责处理，如与粉丝的见面会，与媒体公司的业务洽淡等。这里的经纪人是明星的朋友，而粉丝和媒体公司是陌生人，所以适合使用迪米特法则。 九，合成复用原则1.定义它要求在软件复用时，要尽量先使用组合或者聚合等关联关系来实现，其次才考虑使用继承关系来实现。 如果要使用继承关系，则必须严格遵循里氏替换原则。合成复用原则同里氏替换原则相辅相成的，两者都是开闭原则的具体实现规范。 2.优点通常类的复用分为继承复用和合成复用两种，继承复用虽然有简单和易实现的优点，但它也存在以下缺点。 继承复用破坏了类的封装性。因为继承会将父类的实现细节暴露给子类，父类对子类是透明的，所以这种复用又称为“白箱”复用。 子类与父类的耦合度高。父类的实现的任何改变都会导致子类的实现发生变化，这不利于类的扩展与维护。 它限制了复用的灵活性。从父类继承而来的实现是静态的，在编译时已经定义，所以在运行时不可能发生变化。 采用组合或聚合复用时，可以将已有对象纳入新对象中，使之成为新对象的一部分，新对象可以调用已有对象的功能，它有以下优点。 它维持了类的封装性。因为成分对象的内部细节是新对象看不见的，所以这种复用又称为“黑箱”复用。 新旧类之间的耦合度低。这种复用所需的依赖较少，新对象存取成分对象的唯一方法是通过成分对象的接口。 复用的灵活性高。这种复用可以在运行时动态进行，新对象可以动态地引用与成分对象类型相同的对象。 3.实现方法合成复用原则是通过将已有的对象纳入新对象中，作为新对象的成员对象来实现的，新对象可以调用已有对象的功能，从而达到复用。 汽车分类管理程序 分析：汽车按“动力源”划分可分为汽油汽车、电动汽车等；按“颜色”划分可分为白色汽车、黑色汽车和红色汽车等。如果同时考虑这两种分类，其组合就很多。图 1 所示是用继承关系实现的汽车分类的类图。 十，总结7 种设计原则，它们分别为开闭原则、里氏替换原则、依赖倒置原则、单一职责原则、接口隔离原则、迪米特法则和合成复用原则。 这 7 种设计原则是软件设计模式必须尽量遵循的原则，是设计模式的基础。在实际开发过程中，并不是一定要求所有代码都遵循设计原则，而是要综合考虑人力、时间、成本、质量，不刻意追求完美，要在适当的场景遵循设计原则。这体现的是一种平衡取舍，可以帮助我们设计出更加优雅的代码结构。 设计原则 一句话归纳 目的 开闭原则 对扩展开放，对修改关闭 降低维护带来的新风险 依赖倒置原则 高层不应该依赖低层，要面向接口编程 更利于代码结构的升级扩展 单一职责原则 一个类只干一件事，实现类要单一 便于理解，提高代码的可读性 接口隔离原则 一个接口只干一件事，接口要精简单一 功能解耦，高聚合、低耦合 迪米特法则 不该知道的不要知道，一个类应该保持对其它对象最少的了解，降低耦合度 只和朋友交流，不和陌生人说话，减少代码臃肿 里氏替换原则 不要破坏继承体系，子类重写方法功能发生改变，不应该影响父类方法的含义 防止继承泛滥 合成复用原则 尽量使用组合或者聚合关系实现代码复用，少使用继承 降低代码耦合 实际上，这些原则的目的只有一个：降低对象之间的耦合，增加程序的可复用性、可扩展性和可维护性。 在程序设计时，我们应该将程序功能最小化，每个类只干一件事。若有类似功能基础之上添加新功能，则要合理使用继承。对于多方法的调用，要会运用接口，同时合理设置接口功能与数量。最后类与类之间做到低耦合高内聚。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"软件架构","slug":"软件架构","permalink":"https://yinhuidong.github.io/tags/%E8%BD%AF%E4%BB%B6%E6%9E%B6%E6%9E%84/"}]},{"title":"设计模式概览","slug":"设计模式/三种类型设计模式的特点","date":"2022-01-11T12:02:08.137Z","updated":"2022-01-11T12:13:24.895Z","comments":true,"path":"2022/01/11/设计模式/三种类型设计模式的特点/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E4%B8%89%E7%A7%8D%E7%B1%BB%E5%9E%8B%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E7%9A%84%E7%89%B9%E7%82%B9/","excerpt":"","text":"一，创建型模式的特点和分类创建型模式的主要关注点是如何创建对象？，它的主要特点是将对象的创建与使用分离。这样可以降低系统的耦合度，使用者不需要关注对象的创建细节，对象的创建由相关的工厂来完成。就像去商场购物，不需要知道商品是怎么生产出来的，因为他们由专门的厂商生产。 创建型模式分为以下几种 单例模式：某各类只能生成一个实例，该类提供了一个全局访问点供外部获取该实例，其拓展是有限多例模式。 原型模式：将一个对象作为原型，通过对其进行复制而克隆出多个和原型类似的新实例。 工厂方法：定义一个用于创建产品的接口，由子类决定生产什么产品。 抽象工厂：提供一个创建产品族的接口，其每个子类可以生产一系列相关的产品。 建造者模式：将一个复杂对象分解成多个相对简单部分，然后根据不同需要分别创建他们，最后构建成该复杂对象。 以上5种创建型模式，除了工厂方法模式属于类创建型模式，其他全部属于对象创建型模式，我们将在之后的教程中详细的介绍他们的特点木结构与应用。 二，结构型模式概述结构型模式描述如何将类或者对象按某种布局组成更大的结构。它分为类结构型模式和对象结构型模式，前者采用继承机制来组织接口和类，后者采用组合或聚合来组合对象。 由于组合关系或聚合关系比继承关系耦合度低，满足合成复用原则，所以对象结构型模式比类结构型模式具有更大的灵活性。 结构型模式分为以下7种 代理模式：为某对象提供一种代理以控制该对象的访问，即客户端通过代理间接的访问该对象，从而限制，增强或者修改该对象的一些特性。 适配器模式：将一个类的接口转换成客户希望的另一个接口，使得原本由于接口不兼容而不能一起工作的那些类能一起工作。 桥接模式：将抽象与实现分离，使他们可以独立变化。他是用组合关系来代替继承关系来实现的，从而降低了抽象和实现这两个可变维度的耦合性。 装饰着模式：动态的给对象增加一些职责，即增加额外的功能。 外观模式：将多个复杂的子系统提供一个一致的接口，使这些子系统更加容易被访问。 享元模式：运用共享技术来有效的支持大量细粒度对象的复用 组合模式：将对象组合成树状层次结构，使用户对单个对象和组合对象具有一致访问性 以上7种结构型模式，除了适配器模式分为类结构模式和对象结构型模式两种，其他的全部属于对象结构型模式。 三，行为型模式概述行为型模式用于描述程序在运行时复杂的流程控制，即描述多个类或对象之间怎样相互协作共同完成单个对象都无法单独完成的任务，它涉及算法与对象间职责的分配。 行为型模式分为类行为模式和对象行为模式，前者采用继承机制来在类间分派行为，后者采用组合或聚合在对象间分配行为。由于组合关系或聚合关系比继承关系耦合度低，满足“合成复用原则”，所以对象行为模式比类行为模式具有更大的灵活性。 行为型模式包含11种设计模式 模板方法（Template Method）模式：定义一个操作中的算法骨架，将算法的一些步骤延迟到子类中，使得子类在可以不改变该算法结构的情况下重定义该算法的某些特定步骤。 策略（Strategy）模式：定义了一系列算法，并将每个算法封装起来，使它们可以相互替换，且算法的改变不会影响使用算法的客户。 命令（Command）模式：将一个请求封装为一个对象，使发出请求的责任和执行请求的责任分割开。 职责链（Chain of Responsibility）模式：把请求从链中的一个对象传到下一个对象，直到请求被响应为止。通过这种方式去除对象之间的耦合。 状态（State）模式：允许一个对象在其内部状态发生改变时改变其行为能力。 观察者（Observer）模式：多个对象间存在一对多关系，当一个对象发生改变时，把这种改变通知给其他多个对象，从而影响其他对象的行为。 中介者（Mediator）模式：定义一个中介对象来简化原有对象之间的交互关系，降低系统中对象间的耦合度，使原有对象之间不必相互了解。 迭代器（Iterator）模式：提供一种方法来顺序访问聚合对象中的一系列数据，而不暴露聚合对象的内部表示。 访问者（Visitor）模式：在不改变集合元素的前提下，为一个集合中的每个元素提供多种访问方式，即每个元素有多个访问者对象访问。 备忘录（Memento）模式：在不破坏封装性的前提下，获取并保存一个对象的内部状态，以便以后恢复它。 解释器（Interpreter）模式：提供如何定义语言的文法，以及对语言句子的解释方法，即解释器。 以上11种行为型模式，除了模板方法模式和解释器模式是类行为模式，其他的全部属于对象行为型模式。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"单例模式","slug":"设计模式/创建型模式之单例模式","date":"2022-01-11T12:02:00.546Z","updated":"2022-01-11T12:06:59.633Z","comments":true,"path":"2022/01/11/设计模式/创建型模式之单例模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E5%88%9B%E5%BB%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"在一些系统中，为了节省内存资源，保证数据一致性，对某些类要求只能创建一个实例，这就是所谓的单例模式。 一，单例模式的定义与特点单例模式的定义：指一个类只有一个实例，且该类能自行创建这个实例的一种模式 单例模式的特点： 单例类只有一个实例对象 该单例对象必须由单例类自行创建 单例类对外提供一个访问该单例的全局访问点 二，单例模式的优点和缺点优点 单例模式可以保证内存中只有一个实例，减少了内存的开销 可以避免对资源的多重占用 单例模式设置全局访问点，可以优化和共享资源的访问 缺点 单例模式一般没有接口，扩展困难，除非修改源代码，违反开闭原则。 在并发测试中，单例模式不利于代码调试，在调试过程中，如果单例中的代码没有执行完，也不能模拟生成一个新的对象。 单例模式的功能代码通常写在一个类，如果功能设计不合理，则很容易违背单一职责原则。 三，单例模式的应用场景对于Java来说，单例模式可以保证在一个JVM中只存在单一实例。单例模式的应用场景主要有以下几个方面。 需要频繁创建的一些类，使用单例可以降低系统的内存压力，减少 GC。 某类只要求生成一个对象的时候，如一个班中的班长、每个人的身份证号等。 某些类创建实例时占用资源较多，或实例化耗时较长，且经常使用。 某类需要频繁实例化，而创建的对象又频繁被销毁的时候，如多线程的线程池、网络连接池等。 频繁访问数据库或文件的对象。 对于一些控制硬件级别的操作，或者从系统上来讲应当是单一控制逻辑的操作，如果有多个实例，则系统会完全乱套。 当对象需要被共享的场合。由于单例模式只允许创建一个对象，共享该对象可以节省内存，并加快对象访问速度。如 Web 中的配置对象、数据库的连接池等。 四，单例模式的结构与实现单例模式是设计模式中最简单的模式之一。通常，普通类的构造函数是公有的，外部类可以通过“new 构造函数()”来生成多个实例。但是，如果将类的构造函数设为私有的，外部类就无法调用该构造函数，也就无法生成多个实例。这时该类自身必须定义一个静态私有实例，并向外提供一个静态的公有函数用于创建或获取该静态私有实例。 1.单例模式的结构单例模式的主要结构包括： 单例类：包含一个实例且能自行创建这个实例的类 访问类：使用单例的类 2.单例模式的实现1）懒汉式该模式的特点是类加载时没有生成单例，只有当第一次调用 getlnstance 方法时才去创建这个单例。 12345678910111213141516171819public class LazySinglenton &#123; //内存可见性 private static volatile LazySinglenton instance ; private LazySinglenton()&#123; &#125; public static LazySinglenton getInstance()&#123; if (instance == null)&#123; synchronized (LazySinglenton.class)&#123; if (instance == null)&#123; instance = new LazySinglenton(); &#125; &#125; &#125; return instance; &#125;&#125; 注意：如果编写的是多线程程序，则不要删除上例代码中的关键字 volatile 和 synchronized，否则将存在线程非安全的问题。如果不删除这两个关键字就能保证线程安全，但是每次访问时都要同步，会影响性能，且消耗更多的资源，这是懒汉式单例的缺点。 2)饿汉式该模式的特点是类一旦加载就创建一个单例，保证在调用 getInstance 方法之前单例已经存在了。 123456789101112public class HungrySingleton &#123; private static final HungrySingleton instance = new HungrySingleton(); private HungrySingleton()&#123; &#125; public static HungrySingleton getInstance()&#123; return instance; &#125;&#125; 饿汉式单例在类创建的同时就已经创建好一个静态的对象供系统使用，以后不再改变，所以是线程安全的，可以直接用于多线程而不会出现问题。 五，单例模式的应用实例1.用懒汉式单例模式模拟产生美国当今总统对象分析：在每一届任期内，美国的总统只有一人，所以本实例适合用单例模式实现，图 2 所示是用懒汉式单例实现的结构图。 1234567891011121314151617181920212223242526272829303132333435363738public class SingletonLazy &#123; public static void main(String[] args) &#123; President zt1 = President.getInstance(); zt1.getName(); //输出总统的名字 President zt2 = President.getInstance(); zt2.getName(); //输出总统的名字 if (zt1 == zt2) &#123; System.out.println(&quot;他们是同一人！&quot;); &#125; else &#123; System.out.println(&quot;他们不是同一人！&quot;); &#125; &#125;&#125;class President &#123; private static volatile President instance = null; //保证instance在所有线程中同步 //private避免类在外部被实例化 private President() &#123; System.out.println(&quot;产生一个总统！&quot;); &#125; public static synchronized President getInstance() &#123; //在getInstance方法上加同步 if (instance == null) &#123; synchronized (President.class) &#123; if (instance == null) &#123; instance = new President(); &#125; &#125; &#125; return instance; &#125; public void getName() &#123; System.out.println(&quot;我是美国总统：特朗普。&quot;); &#125;&#125; 程序运行结果如下： 1234产生一个总统！我是美国总统：特朗普。我是美国总统：特朗普。他们是同一人！ 2.用饿汉式单例模式模拟产生猪八戒对象分析：同上例类似，猪八戒也只有一个，所以本实例同样适合用单例模式实现。这里的猪八戒类是单例类，可以将其定义成面板 JPanel 的子类，里面包含了标签，用于保存猪八戒的图像，客户窗体可以获得猪八戒对象，并显示它。图 3 所示是用饿汉式单例实现的结构图。 1234567891011121314151617181920212223242526272829303132public class SingletonEager &#123; public static void main(String[] args) &#123; JFrame jf = new JFrame(&quot;饿汉单例模式测试&quot;); jf.setLayout(new GridLayout(1, 2)); Container contentPane = jf.getContentPane(); Bajie obj1 = Bajie.getInstance(); contentPane.add(obj1); Bajie obj2 = Bajie.getInstance(); contentPane.add(obj2); if (obj1 == obj2) &#123; System.out.println(&quot;他们是同一人！&quot;); &#125; else &#123; System.out.println(&quot;他们不是同一人！&quot;); &#125; jf.pack(); jf.setVisible(true); jf.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE); &#125;&#125;class Bajie extends JPanel &#123; private static Bajie instance = new Bajie(); private Bajie() &#123; JLabel l1 = new JLabel(new ImageIcon(&quot;/home/yhd/图片/4.jpg&quot;)); this.add(l1); &#125; public static Bajie getInstance() &#123; return instance; &#125;&#125; 六，单例模式的扩展单例模式可扩展为有限的多例（Multitcm）模式，这种模式可生成有限个实例并保存在 ArrayList 中，客户需要时可随机获取，其结构图如图 5 所示。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"原型模式","slug":"设计模式/创建型模式之原型模式","date":"2022-01-11T12:01:51.844Z","updated":"2022-01-11T12:07:20.406Z","comments":true,"path":"2022/01/11/设计模式/创建型模式之原型模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E5%88%9B%E5%BB%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%8E%9F%E5%9E%8B%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"在有些系统中，存在大量相同或相似对象的创建问题，如果用传统的构造函数来创建对象，会比较复杂且耗时耗资源，用原型模式生成对象就很高效，就像孙悟空拔下猴毛轻轻一吹就变出很多孙悟空一样简单。 一，原型模式的定义与特点用一个已经创建的实例作为原型，通过复制该原型对象来创建一个和原型相同或相似的新对象。在这里，原型实例指定了要创建的对象的种类。用这种方式创建对象非常高效，根本无须知道对象创建的细节。 原型模式的优点 Java 自带的原型模式基于内存二进制流的复制，在性能上比直接 new 一个对象更加优良。 可以使用深克隆方式保存对象的状态，使用原型模式将对象复制一份，并将其状态保存起来，简化了创建对象的过程，以便在需要的时候使用（例如恢复到历史某一状态），可辅助实现撤销操作。 原型模式的缺点 需要为每一个类都配置一个 clone 方法 clone 方法位于类的内部，当对已有类进行改造的时候，需要修改代码，违背了开闭原则。 当实现深克隆时，需要编写较为复杂的代码，而且当对象之间存在多重嵌套引用时，为了实现深克隆，每一层对象对应的类都必须支持深克隆，实现起来会比较麻烦。因此，深克隆、浅克隆需要运用得当。 二，原型模式的结构与实现由于 Java 提供了对象的 clone() 方法，所以用 Java 实现原型模式很简单。 1.模式的结构原型模式包含以下主要角色。 抽象原型类：规定了具体原型对象必须实现的接口。 具体原型类：实现抽象原型类的 clone() 方法，它是可被复制的对象。 访问类：使用具体原型类中的 clone() 方法来复制新的对象。 2.模式的实现原型模式的克隆分为浅克隆和深克隆。 浅克隆：创建一个新对象，新对象的属性和原来对象完全相同，对于非基本类型属性，仍指向原有属性所指向的对象的内存地址。 深克隆：创建一个新对象，属性中引用的其他对象也会被克隆，不再指向原有对象地址。 Java 中的 Object 类提供了浅克隆的 clone() 方法，具体原型类只要实现 Cloneable 接口就可实现对象的浅克隆，这里的 Cloneable 接口就是抽象原型类。其代码如下： 12345678910111213141516171819public class Realizetype implements Cloneable&#123; public Realizetype()&#123; System.out.println(&quot;具体原型创建成功！&quot;); &#125; @Override protected Object clone() throws CloneNotSupportedException &#123; System.out.println(&quot;具体原型克隆成功！&quot;); return super.clone(); &#125;&#125;class RealizetypeTest&#123; public static void main(String[]args)throws Exception&#123; Realizetype obj1 = new Realizetype(); Realizetype obj2 = (Realizetype) obj1.clone(); System.out.println(&quot;obj1==obj2? &quot;+ (obj1==obj2)); &#125;&#125; 程序运行结果 123具体原型创建成功！具体原型克隆成功！obj1==obj2? false 三，原型模式的应用实例1.用原型模式生成“三好学生”奖状分析：同一学校的“三好学生”奖状除了获奖人姓名不同，其他都相同，属于相似对象的复制，同样可以用原型模式创建，然后再做简单修改就可以了。 12345678910111213141516171819202122232425262728293031323334353637383940public class Citation implements Cloneable&#123; protected String name; protected String info; protected String college; public Citation(String name,String info,String college)&#123; this.name = name; this.info = info; this.college = college; &#125; @Override protected Object clone() throws CloneNotSupportedException &#123; return super.clone(); &#125; public void setName(String name) &#123; this.name = name; &#125; public String getName() &#123; return name; &#125; public void display()&#123; System.out.println(&quot;name:&quot;+name+&quot; info:&quot;+info+&quot; college:&quot;+college); &#125;&#125;class ProtoTypeCitation&#123; public static void main(String[] args) throws Exception&#123; Citation obj1 = new Citation(&quot;张三&quot;,&quot;男&quot;,&quot;pc&quot;); obj1.display(); Citation obj2 = (Citation) obj1.clone(); obj2.setName(&quot;李四&quot;); obj2.display(); &#125;&#125; 12name:张三 info:男 college:pcname:李四 info:男 college:pc 四，原型模式的应用场景 对象之间相同或相似，即只是个别的几个属性不同的时候。 创建对象成本较大，例如初始化时间长，占用CPU太多，或者占用网络资源太多等，需要优化资源。 创建一个对象需要繁琐的数据准备或访问权限等，需要提高性能或者提高安全性。 系统中大量使用该类对象，且各个调用者都需要给它的属性重新赋值。 在 Spring 中，原型模式应用的非常广泛，例如 scope=&#39;prototype&#39;、JSON.parseObject() 等都是原型模式的具体应用。 五，原型模式的扩展原型模式可扩展为带原型管理器的原型模式，它在原型模式的基础上增加了一个原型管理器 PrototypeManager 类。该类用 HashMap 保存多个复制的原型，Client 类可以通过管理器的 get(String id) 方法从中获取复制的原型。 用带原型管理器的原型模式来生成包含“圆”和“正方形”等图形的原型，并计算其面积。分析：本实例中由于存在不同的图形类，例如，“圆”和“正方形”，它们计算面积的方法不一样，所以需要用一个原型管理器来管理它们。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475import java.util.*;interface Shape extends Cloneable &#123; public Object clone(); //拷贝 public void countArea(); //计算面积&#125;class Circle implements Shape &#123; public Object clone() &#123; Circle w = null; try &#123; w = (Circle) super.clone(); &#125; catch (CloneNotSupportedException e) &#123; System.out.println(&quot;拷贝圆失败!&quot;); &#125; return w; &#125; public void countArea() &#123; int r = 0; System.out.print(&quot;这是一个圆，请输入圆的半径：&quot;); Scanner input = new Scanner(System.in); r = input.nextInt(); System.out.println(&quot;该圆的面积=&quot; + 3.1415 * r * r + &quot;\\n&quot;); &#125;&#125;class Square implements Shape &#123; public Object clone() &#123; Square b = null; try &#123; b = (Square) super.clone(); &#125; catch (CloneNotSupportedException e) &#123; System.out.println(&quot;拷贝正方形失败!&quot;); &#125; return b; &#125; public void countArea() &#123; int a = 0; System.out.print(&quot;这是一个正方形，请输入它的边长：&quot;); Scanner input = new Scanner(System.in); a = input.nextInt(); System.out.println(&quot;该正方形的面积=&quot; + a * a + &quot;\\n&quot;); &#125;&#125;class ProtoTypeManager &#123; private HashMap&lt;String, Shape&gt; ht = new HashMap&lt;String, Shape&gt;(); public ProtoTypeManager() &#123; ht.put(&quot;Circle&quot;, new Circle()); ht.put(&quot;Square&quot;, new Square()); &#125; public void addshape(String key, Shape obj) &#123; ht.put(key, obj); &#125; public Shape getShape(String key) &#123; Shape temp = ht.get(key); return (Shape) temp.clone(); &#125;&#125;public class ProtoTypeShape &#123; public static void main(String[] args) &#123; ProtoTypeManager pm = new ProtoTypeManager(); Shape obj1 = (Circle) pm.getShape(&quot;Circle&quot;); obj1.countArea(); Shape obj2 = (Shape) pm.getShape(&quot;Square&quot;); obj2.countArea(); &#125;&#125; 运行结果 12345这是一个圆，请输入圆的半径：3该圆的面积=28.2735这是一个正方形，请输入它的边长：3该正方形的面积=9","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"简单工厂模式","slug":"设计模式/创建型模式之简单工厂模式","date":"2022-01-11T12:01:41.405Z","updated":"2022-01-11T12:07:09.399Z","comments":true,"path":"2022/01/11/设计模式/创建型模式之简单工厂模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E5%88%9B%E5%BB%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E7%AE%80%E5%8D%95%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"在日常开发中，凡是需要生成复杂对象的地方，都可以尝试考虑使用工厂模式来代替。 复杂对象指的是类的构造函数参数过多等对类的构造有影响的情况，因为类的构造过于复杂，如果直接在其他业务类内使用，则两者的耦合过重，后续业务更改，就需要在任何引用该类的源代码内进行更改，光是查找所有依赖就很消耗时间了，更别说要一个一个修改了。 一，工厂模式的定义定义一个创建产品对象的工厂接口，将产品对象的实际创建工作推迟到具体子工厂类当中。这满足创建型模式中所要求的“创建与使用相分离”的特点。 按实际业务场景划分，工厂模式有 3 种不同的实现方式，分别是简单工厂模式、工厂方法模式和抽象工厂模式。 我们把被创建的对象称为“产品”，把创建产品的对象称为“工厂”。如果要创建的产品不多，只要一个工厂类就可以完成，这种模式叫“简单工厂模式”。 在简单工厂模式中创建实例的方法通常为静态（static）方法，因此简单工厂模式（Simple Factory Pattern）又叫作静态工厂方法模式（Static Factory Method Pattern）。 简单来说，简单工厂模式有一个具体的工厂类，可以生成多个不同的产品，属于创建型设计模式。简单工厂模式不在 GoF 23 种设计模式之列。 简单工厂模式每增加一个产品就要增加一个具体产品类和一个对应的具体工厂类，这增加了系统的复杂度，违背了“开闭原则”。 “工厂方法模式”是对简单工厂模式的进一步抽象化，其好处是可以使系统在不修改原来代码的情况下引进新的产品，即满足开闭原则。 二，优点和缺点优点 工厂类包含必要的逻辑判断，可以决定在什么时候创建哪一个产品的实例。客户端可以免除直接创建产品对象的职责，很方便的创建出相应的产品。工厂和产品的职责区分明确。 客户端无需知道所创建具体产品的类名，只需知道参数即可。 也可以引入配置文件，在不修改客户端代码的情况下更换和添加新的具体产品类。 缺点 简单工厂模式的工厂类单一，负责所有产品的创建，职责过重，一旦异常，整个系统将受影响。且工厂类代码会非常臃肿，违背高聚合原则。 使用简单工厂模式会增加系统中类的个数（引入新的工厂类），增加系统的复杂度和理解难度 系统扩展困难，一旦增加新产品不得不修改工厂逻辑，在产品类型较多时，可能造成逻辑过于复杂 简单工厂模式使用了 static 工厂方法，造成工厂角色无法形成基于继承的等级结构。 应用场景 对于产品种类相对较少的情况，考虑使用简单工厂模式。使用简单工厂模式的客户端只需要传入工厂类的参数，不需要关心如何创建对象的逻辑，可以很方便地创建所需产品。 三，模式的结构与实现简单工厂模式的主要角色如下： 简单工厂（SimpleFactory）：是简单工厂模式的核心，负责实现创建所有实例的内部逻辑。工厂类的创建产品类的方法可以被外界直接调用，创建所需的产品对象。 抽象产品（Product）：是简单工厂创建的所有对象的父类，负责描述所有实例共有的公共接口。 具体产品（ConcreteProduct）：是简单工厂模式的创建目标。 12345678910111213141516171819202122232425262728293031323334353637383940414243public class Client &#123; public static void main(String[] args) &#123; &#125; //抽象产品 public interface Product &#123; void show(); &#125; //具体产品：ProductA static class ConcreteProduct1 implements Product &#123; @Override public void show() &#123; System.out.println(&quot;具体产品1显示...&quot;); &#125; &#125; //具体产品：ProductB static class ConcreteProduct2 implements Product &#123; @Override public void show() &#123; System.out.println(&quot;具体产品2显示...&quot;); &#125; &#125; final class Const &#123; static final int PRODUCT_A = 0; static final int PRODUCT_B = 1; static final int PRODUCT_C = 2; &#125; static class SimpleFactory &#123; public static Product makeProduct(int kind) &#123; switch (kind) &#123; case Const.PRODUCT_A: return new ConcreteProduct1(); case Const.PRODUCT_B: return new ConcreteProduct2(); &#125; return null; &#125; &#125;&#125;","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"工厂方法模式","slug":"设计模式/创建型模式之工厂方法模式","date":"2022-01-11T12:01:34.895Z","updated":"2022-01-11T12:07:04.629Z","comments":true,"path":"2022/01/11/设计模式/创建型模式之工厂方法模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E5%88%9B%E5%BB%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%B7%A5%E5%8E%82%E6%96%B9%E6%B3%95%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"一，定义与优缺点简单工厂模式违背了开闭原则，而“工厂方法模式”是对简单工厂模式的进一步抽象化，其好处是可以使系统在不修改原来代码的情况下引进新的产品，即满足开闭原则。 优点 用户只需要知道具体工厂的名称就可得到所要的产品，无须知道产品的具体创建过程。 灵活性增强，对于新产品的创建，只需多写一个相应的工厂类。 典型的解耦框架。高层模块只需要知道产品的抽象类，无须关心其他实现类，满足迪米特法则、依赖倒置原则和里氏替换原则。 缺点 类的个数容易过多，增加复杂度 增加了系统的抽象性和理解难度 抽象产品只能生产一种产品，此弊端可使用抽象工厂模式解决。 应用场景 客户只知道创建产品的工厂名，而不知道具体的产品名。如 TCL 电视工厂、海信电视工厂等。 创建对象的任务由多个具体子工厂中的某一个完成，而抽象工厂只提供创建产品的接口。 客户不关心创建产品的细节，只关心产品的品牌 二，模式的结构与实现工厂方法模式由抽象工厂、具体工厂、抽象产品和具体产品等4个要素构成。本节来分析其基本结构和实现方法。 1.模式的结构工厂方法模式的主要角色如下。 抽象工厂（Abstract Factory）：提供了创建产品的接口，调用者通过它访问具体工厂的工厂方法 newProduct() 来创建产品。 具体工厂（ConcreteFactory）：主要是实现抽象工厂中的抽象方法，完成具体产品的创建。 抽象产品（Product）：定义了产品的规范，描述了产品的主要特性和功能。 具体产品（ConcreteProduct）：实现了抽象产品角色所定义的接口，由具体工厂来创建，它同具体工厂之间一一对应。 2.模式的实现不同的工厂生产不同的妹子，想要获取某种妹子，就要通过对应的工厂获取。 123456789101112131415161718192021222324252627282930313233343536373839404142434445public abstract class AbstractFactory &#123; private AbstractMeiZi meiZi; public abstract AbstractMeiZi getMeiZi();&#125;public class PiaoLiangFactory extends AbstractFactory &#123; @Override public AbstractMeiZi getMeiZi()&#123; return new PiaoLiangMeiZi(); &#125;&#125;public class XingGanFactory extends AbstractFactory &#123; @Override public AbstractMeiZi getMeiZi() &#123; return new XingGanMeiZi(); &#125;&#125;public abstract class AbstractMeiZi &#123; private Integer id; private String name;&#125;public class PiaoLiangMeiZi extends AbstractMeiZi &#123;&#125;public class XingGanMeiZi extends AbstractMeiZi &#123;&#125;public class MainTest &#123; @Test public void test()&#123; AbstractFactory factory = new PiaoLiangFactory(); AbstractMeiZi meiZi = factory.getMeiZi(); System.out.println(meiZi); &#125;&#125; 当需要生成的产品不多且不会增加，一个具体工厂类就可以完成任务时，可删除抽象工厂类。这时工厂方法模式将退化到简单工厂模式。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"抽象工厂模式","slug":"设计模式/创建型模式之抽象工厂模式","date":"2022-01-11T12:01:26.778Z","updated":"2022-01-11T12:06:49.176Z","comments":true,"path":"2022/01/11/设计模式/创建型模式之抽象工厂模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E5%88%9B%E5%BB%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E6%8A%BD%E8%B1%A1%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"工厂方法模式中考虑的是一类产品的生产，如畜牧场只养动物、电视机厂只生产电视机、计算机软件学院只培养计算机软件专业的学生等。 同种类称为同等级，也就是说：[工厂方法模式]只考虑生产同等级的产品，但是在现实生活中许多工厂是综合型的工厂，能生产多等级（种类） 的产品，如农场里既养动物又种植物，电器厂既生产电视机又生产洗衣机或空调，大学既有软件专业又有生物专业等。 一，模式的定义与特点抽象工厂（AbstractFactory）模式的定义：是一种为访问类提供一个创建一组相关或相互依赖对象的接口，且访问类无须指定所要产品的具体类就能得到同族的不同等级的产品的模式结构。 抽象工厂模式是工厂方法模式的升级版本，工厂方法模式只生产一个等级的产品，而抽象工厂模式可生产多个等级的产品。 使用抽象工厂模式一般要满足以下条件。 系统中有多个产品族，每个具体工厂创建同一族但属于不同等级结构的产品。 系统一次只可能消费其中某一族产品，即同族的产品一起使用。 抽象工厂模式除了具有工厂方法模式的优点外，其他主要优点如下。 可以在类的内部对产品族中相关联的多等级产品共同管理，而不必专门引入多个新的类来进行管理。 当需要产品族时，抽象工厂可以保证客户端始终只使用同一个产品的产品组。 抽象工厂增强了程序的可扩展性，当增加一个新的产品族时，不需要修改原代码，满足开闭原则。 其缺点是：当产品族中需要增加一个新的产品时，所有的工厂类都需要进行修改。增加了系统的抽象性和理解难度。 二，模式的结构与实现抽象工厂模式同工厂方法模式一样，也是由抽象工厂、具体工厂、抽象产品和具体产品等 4 个要素构成，但抽象工厂中方法个数不同，抽象产品的个数也不同。现在我们来分析其基本结构和实现方法。 1. 模式的结构抽象工厂模式的主要角色如下。 抽象工厂（Abstract Factory）：提供了创建产品的接口，它包含多个创建产品的方法 newProduct()，可以创建多个不同等级的产品。 具体工厂（Concrete Factory）：主要是实现抽象工厂中的多个抽象方法，完成具体产品的创建。 抽象产品（Product）：定义了产品的规范，描述了产品的主要特性和功能，抽象工厂模式有多个抽象产品。 具体产品（ConcreteProduct）：实现了抽象产品角色所定义的接口，由具体工厂来创建，它同具体工厂之间是多对一的关系。 2.模式的实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374public abstract class AbstractFactory &#123; public abstract AbstractMeiZi getMeiZi(); public abstract AbstractCar getCar();&#125;//==================妹子工厂====================//public abstract class AbstractMeiZiFactory &#123; public abstract AbstractMeiZi getMeiZi();&#125;public class PiaoLiangFactory extends AbstractMeiZiFactory &#123; @Override public AbstractMeiZi getMeiZi()&#123; return new PiaoLiangMeiZi(); &#125;&#125;public class XingGanFactory extends AbstractMeiZiFactory &#123; @Override public AbstractMeiZi getMeiZi() &#123; return new XingGanMeiZi(); &#125;&#125;public abstract class AbstractMeiZi &#123; private Integer id; private String name;&#125;public class XingGanMeiZi extends AbstractMeiZi&#123;&#125;public class PiaoLiangMeiZi extends AbstractMeiZi&#123;&#125;//==================汽车工厂====================//public abstract class AbstractCarFactory &#123; public abstract AbstractCar getCar();&#125;public class BsjFactory extends AbstractCarFactory &#123; @Override public AbstractCar getCar() &#123; return new Bsj(); &#125;&#125;public class LbjnFactory extends AbstractCarFactory&#123; @Override public AbstractCar getCar() &#123; return new Lbjn(); &#125;&#125;public abstract class AbstractCar &#123; private Integer id; private String name;&#125;public class Bsj extends AbstractCar&#123;&#125;public class Lbjn extends AbstractCar&#123;&#125; 三，模式的应用场景抽象工厂模式最早的应用是用于创建属于不同操作系统的视窗构件。如 [Java]的 AWT 中的 Button 和 Text 等构件在 Windows 和 UNIX 中的本地实现是不同的。 抽象工厂模式通常适用于以下场景： 当需要创建的对象是一系列相互关联或相互依赖的产品族时，如电器工厂中的电视机、洗衣机、空调等。 系统中有多个产品族，但每次只使用其中的某一族产品。如有人只喜欢穿某一个品牌的衣服和鞋。 系统中提供了产品的类库，且所有产品的接口相同，客户端不依赖产品实例的创建细节和内部结构。 四，模式的扩展抽象工厂模式的扩展有一定的“开闭原则”倾斜性： 当增加一个新的产品族时只需增加一个新的具体工厂，不需要修改原代码，满足开闭原则。 当产品族中需要增加一个新种类的产品时，则所有的工厂类都需要进行修改，不满足开闭原则。 另一方面，当系统中只存在一个等级结构的产品时，抽象工厂模式将退化到工厂方法模式。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"构建者模式","slug":"设计模式/创建型模式之建造者模式","date":"2022-01-11T12:01:18.420Z","updated":"2022-01-11T12:07:14.560Z","comments":true,"path":"2022/01/11/设计模式/创建型模式之建造者模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E5%88%9B%E5%BB%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%BB%BA%E9%80%A0%E8%80%85%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"在软件开发过程中有时需要创建一个复杂的对象，这个复杂对象通常由多个子部件按一定的步骤组合而成。例如，计算机是由 CPU、主板、内存、硬盘、显卡、机箱、显示器、键盘、鼠标等部件组装而成的，采购员不可能自己去组装计算机，而是将计算机的配置要求告诉计算机销售公司，计算机销售公司安排技术人员去组装计算机，然后再交给要买计算机的采购员。 以上所有这些产品都是由多个部件构成的，各个部件可以灵活选择，但其创建步骤都大同小异。这类产品的创建无法用工厂模式描述，只有建造者模式可以很好地描述该类产品的创建。 一，模式的定义与特点建造者（Builder）模式的定义：指将一个复杂对象的构造与它的表示分离，使同样的构建过程可以创建不同的表示，这样的设计模式被称为建造者模式。它是将一个复杂的对象分解为多个简单的对象，然后一步一步构建而成。它将变与不变相分离，即产品的组成部分是不变的，但每一部分是可以灵活选择的。 该模式的主要优点如下： 封装性好，构建和表示分离。 扩展性好，各个具体的建造者相互独立，有利于系统的解耦。 客户端不必知道产品内部组成的细节，建造者可以对创建过程逐步细化，而不对其它模块产生任何影响，便于控制细节风险。 其缺点如下： 产品的组成部分必须相同，这限制了其使用范围。 如果产品的内部变化复杂，如果产品内部发生变化，则建造者也要同步修改，后期维护成本较大。 建造者（Builder）模式和工厂模式的关注点不同：建造者模式注重零部件的组装过程，而[工厂方法模式]更注重零部件的创建过程，但两者可以结合使用。 二，模式的结构与实现建造者（Builder）模式由产品、抽象建造者、具体建造者、指挥者等 4 个要素构成，现在我们来分析其基本结构和实现方法。 1.模式的结构建造者（Builder）模式的主要角色如下。 产品角色（Product）：它是包含多个组成部件的复杂对象，由具体建造者来创建其各个零部件。 抽象建造者（Builder）：它是一个包含创建产品各个子部件的抽象方法的接口，通常还包含一个返回复杂产品的方法 getResult()。 具体建造者(Concrete Builder）：实现 Builder 接口，完成复杂产品的各个部件的具体创建方法。 指挥者（Director）：它调用建造者对象中的部件构造与装配方法完成复杂对象的创建，在指挥者中不涉及具体产品的信息。 2.模式的实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public abstract class PhoneBuilder &#123; protected Phone phone; public abstract void doCpu(); public abstract void doMemory(); public abstract void doSsd(); public Phone getPhone()&#123; doCpu(); doMemory(); doSsd(); return new Phone(); &#125;&#125;public class XiaoMiPhoneBuilder extends PhoneBuilder&#123; @Override public void doCpu() &#123; System.out.println(&quot;小米工厂生产CPU&quot;); &#125; @Override public void doMemory() &#123; System.out.println(&quot;小米工厂生产Memory&quot;); &#125; @Override public void doSsd() &#123; System.out.println(&quot;小米工厂生产SSD&quot;); &#125;&#125;public class Phone &#123; private Integer id; private String name; private String logo;&#125;public class MainTest &#123; @Test public void test() &#123; PhoneBuilder builder = new XiaoMiPhoneBuilder(); Phone phone = builder.getPhone(); &#125;&#125; 三，模式的应用场景建造者模式唯一区别于工厂模式的是针对复杂对象的创建。也就是说，如果创建简单对象，通常都是使用工厂模式进行创建，而如果创建复杂对象，就可以考虑使用建造者模式。 当需要创建的产品具备复杂创建过程时，可以抽取出共性创建过程，然后交由具体实现类自定义创建流程，使得同样的创建行为可以生产出不同的产品，分离了创建与表示，使创建产品的灵活性大大增加。 建造者模式主要适用于以下应用场景： 相同的方法，不同的执行顺序，产生不同的结果。 多个部件或零件，都可以装配到一个对象中，但是产生的结果又不相同。 产品类非常复杂，或者产品类中不同的调用顺序产生不同的作用。 初始化一个对象特别复杂，参数多，而且很多参数都具有默认值。 四，建造者和工厂模式的区别 建造者模式更加注重方法的调用顺序，工厂模式注重创建对象。 创建对象的力度不同，建造者模式创建复杂的对象，由各种复杂的部件组成，工厂模式创建出来的对象都一样 关注重点不一样，工厂模式只需要把对象创建出来就可以了，而建造者模式不仅要创建出对象，还要知道对象由哪些部件组成。 建造者模式根据建造过程中的顺序不一样，最终对象部件组成也不一样。 五，模式的扩展建造者（Builder）模式在应用过程中可以根据需要改变，如果创建的产品种类只有一种，只需要一个具体建造者，这时可以省略掉抽象建造者，甚至可以省略掉指挥者角色。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"代理模式","slug":"设计模式/结构型模式之代理模式","date":"2022-01-11T12:01:09.773Z","updated":"2022-01-11T12:07:27.650Z","comments":true,"path":"2022/01/11/设计模式/结构型模式之代理模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E7%BB%93%E6%9E%84%E5%9E%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"在有些情况下，一个客户不能或者不想直接访问另一个对象，这时需要找一个中介帮忙完成某项任务，这个中介就是代理对象。例如，购买火车票不一定要去火车站买，可以通过 12306 网站或者去火车票代售点买。又如找女朋友、找保姆、找工作等都可以通过找中介完成。 在软件设计中，使用代理模式的例子也很多，例如，要访问的远程对象比较大（如视频或大图像等），其下载要花很多时间。还有因为安全原因需要屏蔽客户端直接访问真实对象，如某单位的内部数据库等。 一，代理模式的定义与特点代理模式的定义：由于某些原因需要给某对象提供一个代理以控制对该对象的访问。这时，访问对象不适合或者不能直接引用目标对象，代理对象作为访问对象和目标对象之间的中介。 代理模式的主要优点有： 代理模式在客户端与目标对象之间起到一个中介作用和保护目标对象的作用； 代理对象可以扩展目标对象的功能； 代理模式能将客户端与目标对象分离，在一定程度上降低了系统的耦合度，增加了程序的可扩展性 其主要缺点是： 代理模式会造成系统设计中类的数量增加 在客户端和目标对象之间增加一个代理对象，会造成请求处理速度变慢； 增加了系统的复杂度； 如何解决以上提到的缺点呢？答案是可以使用动态代理方式 二，代理模式的结构与实现代理模式的结构比较简单，主要是通过定义一个继承抽象主题的代理来包含真实主题，从而实现对真实主题的访问。 1.模式的结构代理模式的主要角色如下。 抽象主题（Subject）类：通过接口或抽象类声明真实主题和代理对象实现的业务方法。 真实主题（Real Subject）类：实现了抽象主题中的具体业务，是代理对象所代表的真实对象，是最终要引用的对象。 代理（Proxy）类：提供了与真实主题相同的接口，其内部含有对真实主题的引用，它可以访问、控制或扩展真实主题的功能。 在代码中，一般代理会被理解为代码增强，实际上就是在原代码逻辑前后增加一些代码逻辑，而使调用者无感知。 根据代理的创建时期，代理模式分为静态代理和动态代理。 静态：由程序员创建代理类或特定工具自动生成源代码再对其编译，在程序运行前代理类的 .class 文件就已经存在了。 动态：在程序运行时，运用反射机制动态创建而成 2.模式的实现123456789101112131415161718192021222324252627282930313233343536373839404142package proxy;public class ProxyTest &#123; public static void main(String[] args) &#123; Proxy proxy = new Proxy(); proxy.Request(); &#125;&#125;//抽象主题interface Subject &#123; void Request();&#125;//真实主题class RealSubject implements Subject &#123; public void Request() &#123; System.out.println(&quot;访问真实主题方法...&quot;); &#125;&#125;//代理class Proxy implements Subject &#123; private RealSubject realSubject; public void Request() &#123; if (realSubject == null) &#123; realSubject = new RealSubject(); &#125; preRequest(); realSubject.Request(); postRequest(); &#125; public void preRequest() &#123; System.out.println(&quot;访问真实主题之前的预处理。&quot;); &#125; public void postRequest() &#123; System.out.println(&quot;访问真实主题之后的后续处理。&quot;); &#125;&#125; 程序运行的结果如下： 123访问真实主题之前的预处理。访问真实主题方法...访问真实主题之后的后续处理。 三，代理模式的应用实例1.jdk 123456789101112131415161718192021222324252627public interface Game &#123; void playGame(String name);&#125;public class UziProxy&lt;T&gt; &#123; public static&lt;T&gt; T getProxy(T t)&#123; return (T) Proxy.newProxyInstance(t.getClass().getClassLoader(), t.getClass().getInterfaces(), new InvocationHandler() &#123; @Override public Object invoke(Object o, Method method, Object[] objects) throws Throwable &#123; System.out.println(&quot;增强&quot;); return method.invoke(t,objects); &#125; &#125;); &#125;&#125;public class MainTest &#123; @Test public void test()&#123; Game game = UziProxy.getProxy(name -&gt; System.out.println(&quot;选vn，一打五。&quot;+name)); game.playGame(&quot;LOL&quot;); &#125;&#125; 2.cglib123456789101112131415161718192021222324252627282930313233public class ProxyFactory&lt;T&gt; &#123; public static&lt;T&gt; T getProxy(T t)&#123; return (T) Enhancer.create(t.getClass(), (MethodInterceptor) (o, method, objects, methodProxy) -&gt; &#123; System.out.println(&quot;增强&quot;); return method.invoke(t,objects); &#125;); &#125;&#125;public class Phone &#123; void call()&#123; System.out.println(&quot;打电话&quot;); &#125;&#125;public class MiPhone extends Phone&#123; @Override void call() &#123; System.out.println(&quot;打微信电话&quot;); &#125;&#125;public class MainTest &#123; public static void main(String[] args) &#123; Phone proxy = ProxyFactory.getProxy(new MiPhone()); proxy.call(); &#125;&#125; 四，代理模式的应用场景当无法或不想直接引用某个对象或访问某个对象存在困难时，可以通过代理对象来间接访问。使用代理模式主要有两个目的：一是保护目标对象，二是增强目标对象。 前面分析了代理模式的结构与特点，现在来分析以下的应用场景。 远程代理，这种方式通常是为了隐藏目标对象存在于不同地址空间的事实，方便客户端访问。例如，用户申请某些网盘空间时，会在用户的文件系统中建立一个虚拟的硬盘，用户访问虚拟硬盘时实际访问的是网盘空间。 虚拟代理，这种方式通常用于要创建的目标对象开销很大时。例如，下载一幅很大的图像需要很长时间，因某种计算比较复杂而短时间无法完成，这时可以先用小比例的虚拟代理替换真实的对象，消除用户对服务器慢的感觉。 安全代理，这种方式通常用于控制不同种类客户对真实对象的访问权限。 智能指引，主要用于调用目标对象时，代理附加一些额外的处理功能。例如，增加计算真实对象的引用次数的功能，这样当该对象没有被引用时，就可以自动释放它。 延迟加载，指为了提高系统的性能，延迟对目标的加载。例如，Hibernate 中就存在属性的延迟加载和关联表的延时加载。 五，代理模式的扩展在前面介绍的代理模式中，代理类中包含了对真实主题的引用，这种方式存在两个缺点。 真实主题与代理主题一一对应，增加真实主题也要增加代理。 设计代理以前真实主题必须事先存在，不太灵活。采用动态代理模式可以解决以上问题，如Spring AOP。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"适配器模式","slug":"设计模式/结构型模式之适配器模式","date":"2022-01-11T12:00:58.995Z","updated":"2022-01-11T12:08:00.723Z","comments":true,"path":"2022/01/11/设计模式/结构型模式之适配器模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E7%BB%93%E6%9E%84%E5%9E%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E9%80%82%E9%85%8D%E5%99%A8%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"在软件设计中也可能出现：需要开发的具有某种业务功能的组件在现有的组件库中已经存在，但它们与当前系统的接口规范不兼容，如果重新开发这些组件成本又很高，这时用适配器模式能很好地解决这些问题。 一，模式的定义与特点适配器模式（Adapter）的定义如下：将一个类的接口转换成客户希望的另外一个接口，使得原本由于接口不兼容而不能一起工作的那些类能一起工作。适配器模式分为类结构型模式和对象结构型模式两种，前者类之间的耦合度比后者高，且要求程序员了解现有组件库中的相关组件的内部结构，所以应用相对较少些。 该模式的主要优点如下。 客户端通过适配器可以透明地调用目标接口。 复用了现存的类，程序员不需要修改原有代码而重用现有的适配者类。 将目标类和适配者类解耦，解决了目标类和适配者类接口不一致的问题。 在很多业务场景中符合开闭原则。 其缺点是： 适配器编写过程需要结合业务场景全面考虑，可能会增加系统的复杂性。 增加代码阅读难度，降低代码可读性，过多使用适配器会使系统代码变得凌乱。 二，模式的结构与实现类适配器模式可采用多重继承方式实现，如 C++ 可定义一个适配器类来同时继承当前系统的业务接口和现有组件库中已经存在的组件接口；Java 不支持多继承，但可以定义一个适配器类来实现当前系统的业务接口，同时又继承现有组件库中已经存在的组件。 对象适配器模式可釆用将现有组件库中已经实现的组件引入适配器类中，该类同时实现当前系统的业务接口。现在来介绍它们的基本结构。 1.模式的结构适配器模式（Adapter）包含以下主要角色。 目标（Target）接口：当前系统业务所期待的接口，它可以是抽象类或接口。 适配者（Adaptee）类：它是被访问和适配的现存组件库中的组件接口。 适配器（Adapter）类：它是一个转换器，通过继承或引用适配者的对象，把适配者接口转换成目标接口，让客户按目标接口的格式访问适配者。 类适配器 对象适配器 2.模式的实现1）类适配器1234567891011121314151617181920212223242526272829303132package adapter;//目标接口interface Target&#123; public void request();&#125;//适配者接口class Adaptee&#123; public void specificRequest() &#123; System.out.println(&quot;适配者中的业务代码被调用！&quot;); &#125;&#125;//类适配器类class ClassAdapter extends Adaptee implements Target&#123; public void request() &#123; specificRequest(); &#125;&#125;//客户端代码public class ClassAdapterTest&#123; public static void main(String[] args) &#123; System.out.println(&quot;类适配器模式测试：&quot;); Target target = new ClassAdapter(); target.request(); &#125;&#125; 2）对象适配器12345678910111213141516171819202122232425package adapter;//对象适配器类class ObjectAdapter implements Target&#123; private Adaptee adaptee; public ObjectAdapter(Adaptee adaptee) &#123; this.adaptee=adaptee; &#125; public void request() &#123; adaptee.specificRequest(); &#125;&#125;//客户端代码public class ObjectAdapterTest&#123; public static void main(String[] args) &#123; System.out.println(&quot;对象适配器模式测试：&quot;); Adaptee adaptee = new Adaptee(); Target target = new ObjectAdapter(adaptee); target.request(); &#125;&#125; 对象适配器模式中的“目标接口”和“适配者类”的代码同类适配器模式一样，只要修改适配器类和客户端的代码即可。 三，模式的应用实例用适配器模式（Adapter）模拟新能源汽车的发动机。 分析：新能源汽车的发动机有电能发动机（Electric Motor）和光能发动机（Optical Motor）等，各种发动机的驱动方法不同，例如，电能发动机的驱动方法 electricDrive() 是用电能驱动，而光能发动机的驱动方法 opticalDrive() 是用光能驱动，它们是适配器模式中被访问的适配者。 客户端希望用统一的发动机驱动方法 drive() 访问这两种发动机，所以必须定义一个统一的目标接口 Motor，然后再定义电能适配器（Electric Adapter）和光能适配器（Optical Adapter）去适配这两种发动机。 我们把客户端想访问的新能源发动机的适配器的名称放在 XML 配置文件中（点此下载 XML 文件），客户端可以通过对象生成器类 ReadXML 去读取。这样，客户端就可以通过 Motor 接口随便使用任意一种新能源发动机去驱动汽车。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687package adapter;//目标：发动机interface Motor&#123; public void drive();&#125;//适配者1：电能发动机class ElectricMotor&#123; public void electricDrive() &#123; System.out.println(&quot;电能发动机驱动汽车！&quot;); &#125;&#125;//适配者2：光能发动机class OpticalMotor&#123; public void opticalDrive() &#123; System.out.println(&quot;光能发动机驱动汽车！&quot;); &#125;&#125;//电能适配器class ElectricAdapter implements Motor&#123; private ElectricMotor emotor; public ElectricAdapter() &#123; emotor=new ElectricMotor(); &#125; public void drive() &#123; emotor.electricDrive(); &#125;&#125;//光能适配器class OpticalAdapter implements Motor&#123; private OpticalMotor omotor; public OpticalAdapter() &#123; omotor=new OpticalMotor(); &#125; public void drive() &#123; omotor.opticalDrive(); &#125;&#125;//客户端代码public class MotorAdapterTest&#123; public static void main(String[] args) &#123; System.out.println(&quot;适配器模式测试：&quot;); Motor motor=(Motor)ReadXML.getObject(); motor.drive(); &#125;&#125;===========================================================================================package adapter;import javax.xml.parsers.*;import org.w3c.dom.*;import java.io.*;class ReadXML&#123; public static Object getObject() &#123; try &#123; DocumentBuilderFactory dFactory=DocumentBuilderFactory.newInstance(); DocumentBuilder builder=dFactory.newDocumentBuilder(); Document doc; doc=builder.parse(new File(&quot;src/adapter/config.xml&quot;)); NodeList nl=doc.getElementsByTagName(&quot;className&quot;); Node classNode=nl.item(0).getFirstChild(); String cName=&quot;adapter.&quot;+classNode.getNodeValue(); Class&lt;?&gt; c=Class.forName(cName); Object obj=c.newInstance(); return obj; &#125; catch(Exception e) &#123; e.printStackTrace(); return null; &#125; &#125;&#125; 四，模式的应用场景适配器模式（Adapter）通常适用于以下场景。 以前开发的系统存在满足新系统功能需求的类，但其接口同新系统的接口不一致。 使用第三方提供的组件，但组件接口定义和自己要求的接口定义不同。 五，模式的扩展适配器模式（Adapter）可扩展为双向适配器模式，双向适配器类既可以把适配者接口转换成目标接口，也可以把目标接口转换成适配者接口。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package adapter;//目标接口interface TwoWayTarget&#123; public void request();&#125;//适配者接口interface TwoWayAdaptee&#123; public void specificRequest();&#125;//目标实现class TargetRealize implements TwoWayTarget&#123; public void request() &#123; System.out.println(&quot;目标代码被调用！&quot;); &#125;&#125;//适配者实现class AdapteeRealize implements TwoWayAdaptee&#123; public void specificRequest() &#123; System.out.println(&quot;适配者代码被调用！&quot;); &#125;&#125;//双向适配器class TwoWayAdapter implements TwoWayTarget,TwoWayAdaptee&#123; private TwoWayTarget target; private TwoWayAdaptee adaptee; public TwoWayAdapter(TwoWayTarget target) &#123; this.target=target; &#125; public TwoWayAdapter(TwoWayAdaptee adaptee) &#123; this.adaptee=adaptee; &#125; public void request() &#123; adaptee.specificRequest(); &#125; public void specificRequest() &#123; target.request(); &#125;&#125;//客户端代码public class TwoWayAdapterTest&#123; public static void main(String[] args) &#123; System.out.println(&quot;目标通过双向适配器访问适配者：&quot;); TwoWayAdaptee adaptee=new AdapteeRealize(); TwoWayTarget target=new TwoWayAdapter(adaptee); target.request(); System.out.println(&quot;-------------------&quot;); System.out.println(&quot;适配者通过双向适配器访问目标：&quot;); target=new TargetRealize(); adaptee=new TwoWayAdapter(target); adaptee.specificRequest(); &#125;&#125; 程序的运行结果如下： 12345目标通过双向适配器访问适配者：适配者代码被调用！-------------------适配者通过双向适配器访问目标：目标代码被调用！","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"桥接模式","slug":"设计模式/结构型模式之桥接模式","date":"2022-01-11T12:00:46.212Z","updated":"2022-01-11T12:07:51.865Z","comments":true,"path":"2022/01/11/设计模式/结构型模式之桥接模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E7%BB%93%E6%9E%84%E5%9E%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E6%A1%A5%E6%8E%A5%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"在现实生活中，某些类具有两个或多个维度的变化，如图形既可按形状分，又可按颜色分。如何设计类似于 Photoshop 这样的软件，能画不同形状和不同颜色的图形呢？如果用继承方式，m 种形状和 n 种颜色的图形就有 m×n 种，不但对应的子类很多，而且扩展困难。 当然，这样的例子还有很多，如不同颜色和字体的文字、不同品牌和功率的汽车、不同性别和职业的男女、支持不同平台和不同文件格式的媒体播放器等。如果用桥接模式就能很好地解决这些问题。 一，桥接模式的定义与特点将抽象与实现分离，使它们可以独立变化。它是用组合关系代替继承关系来实现，从而降低了抽象和实现这两个可变维度的耦合度。 桥接模式遵循了里氏替换原则和依赖倒置原则，最终实现了开闭原则，对修改关闭，对扩展开放。 桥接（Bridge）模式的优点是： 抽象与实现分离，扩展能力强 符合开闭原则 符合合成复用原则 其实现细节对客户透明 缺点是：由于聚合关系建立在抽象层，要求开发者针对抽象化进行设计与编程，能正确地识别出系统中两个独立变化的维度，这增加了系统的理解与设计难度。 二，桥接模式的结构与实现可以将抽象化部分与实现化部分分开，取消二者的继承关系，改用组合关系。 1.模式的结构桥接（Bridge）模式包含以下主要角色。 抽象化（Abstraction）角色：定义抽象类，并包含一个对实现化对象的引用。 扩展抽象化（Refined Abstraction）角色：是抽象化角色的子类，实现父类中的业务方法，并通过组合关系调用实现化角色中的业务方法。 实现化（Implementor）角色：定义实现化角色的接口，供扩展抽象化角色调用。 具体实现化（Concrete Implementor）角色：给出实现化角色接口的具体实现。 2.模式的实现1234567891011121314151617181920212223242526272829303132333435363738394041424344package bridge;public class BridgeTest &#123; public static void main(String[] args) &#123; Implementor imple = new ConcreteImplementorA(); Abstraction abs = new RefinedAbstraction(imple); abs.Operation(); &#125;&#125;//实现化角色interface Implementor &#123; public void OperationImpl();&#125;//具体实现化角色class ConcreteImplementorA implements Implementor &#123; public void OperationImpl() &#123; System.out.println(&quot;具体实现化(Concrete Implementor)角色被访问&quot;); &#125;&#125;//抽象化角色abstract class Abstraction &#123; protected Implementor imple; protected Abstraction(Implementor imple) &#123; this.imple = imple; &#125; public abstract void Operation();&#125;//扩展抽象化角色class RefinedAbstraction extends Abstraction &#123; protected RefinedAbstraction(Implementor imple) &#123; super(imple); &#125; public void Operation() &#123; System.out.println(&quot;扩展抽象化(Refined Abstraction)角色被访问&quot;); imple.OperationImpl(); &#125;&#125; 三，桥接模式的应用实例模拟女士皮包的选购 分析：女士皮包有很多种，可以按用途分、按皮质分、按品牌分、按颜色分、按大小分等，存在多个维度的变化，所以采用桥接模式来实现女士皮包的选购比较合适。 本实例按用途分可选钱包（Wallet）和挎包（HandBag），按颜色分可选黄色（Yellow）和红色（Red）。可以按两个维度定义为颜色类和包类。 颜色类（Color）是一个维度，定义为实现化角色，它有两个具体实现化角色：黄色和红色，通过 getColor() 方法可以选择颜色；包类（Bag）是另一个维度，定义为抽象化角色，它有两个扩展抽象化角色：挎包和钱包，它包含了颜色类对象，通过 getName() 方法可以选择相关颜色的挎包和钱包。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667//============= Color ====================//public interface Color &#123; String getColor();&#125;public class Red implements Color&#123; @Override public String getColor() &#123; return &quot;红色&quot;; &#125;&#125;public class Yellow implements Color&#123; @Override public String getColor() &#123; return &quot;黄色&quot;; &#125;&#125;//========================== Bag ========================//public abstract class Bag &#123; Color color; public Bag(Color color) &#123; this.color = color; &#125; public void setColor(Color color) &#123; this.color = color; &#125; public abstract String getName();&#125;public class HandBag extends Bag&#123; public HandBag(Color color) &#123; super(color); &#125; @Override public String getName() &#123; return &quot;挎包&quot;; &#125;&#125;public class Wallet extends Bag&#123; public Wallet(Color color) &#123; super(color); &#125; @Override public String getName() &#123; return &quot;钱包&quot;; &#125;&#125;//=================== Test ==========================//public class MainTest &#123; public static void main(String[] args) &#123; Color color = new Red(); Bag bag = new Wallet(color); System.out.println(&quot;bag.getName() = &quot; + bag.getName()); System.out.println(&quot;color.getColor() = &quot; + color.getColor()); &#125;&#125; 四，桥接模式的应用场景当一个类内部具备两种或多种变化维度时，使用桥接模式可以解耦这些变化的维度，使高层代码架构稳定。 桥接模式通常适用于以下场景。 当一个类存在两个独立变化的维度，且这两个维度都需要进行扩展时。 当一个系统不希望使用继承或因为多层次继承导致系统类的个数急剧增加时。 当一个系统需要在构件的抽象化角色和具体化角色之间增加更多的灵活性时。 桥接模式的一个常见使用场景就是替换继承。我们知道，继承拥有很多优点，比如，抽象、封装、多态等，父类封装共性，子类实现特性。继承可以很好的实现代码复用（封装）的功能，但这也是继承的一大缺点。 因为父类拥有的方法，子类也会继承得到，无论子类需不需要，这说明继承具备强侵入性（父类代码侵入子类），同时会导致子类臃肿。因此，在设计模式中，有一个原则为优先使用组合/聚合，而不是继承。 很多时候，我们分不清该使用继承还是组合/聚合或其他方式等，其实可以从现实语义进行思考。因为软件最终还是提供给现实生活中的人使用的，是服务于人类社会的，软件是具备现实场景的。当我们从纯代码角度无法看清问题时，现实角度可能会提供更加开阔的思路。 五，桥接模式的扩展在软件开发中，有时桥接（Bridge）模式可与[适配器模式]联合使用。当桥接（Bridge）模式的实现化角色的接口与现有类的接口不一致时，可以在二者中间定义一个适配器将二者连接起来。 其实也可以和工厂模式组合。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"装饰者模式","slug":"设计模式/结构型模式之装饰者模式","date":"2022-01-11T12:00:37.880Z","updated":"2022-01-11T12:08:48.107Z","comments":true,"path":"2022/01/11/设计模式/结构型模式之装饰者模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E7%BB%93%E6%9E%84%E5%9E%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E8%A3%85%E9%A5%B0%E8%80%85%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"有些人早餐可能会吃煎饼，煎饼中可以加鸡蛋，也可以加香肠，但是不管怎么“加码”，都还是一个煎饼。在现实生活中，常常需要对现有产品增加新的功能或美化其外观，如房子装修、相片加相框等，都是装饰器模式。 在软件开发过程中，有时想用一些现存的组件。这些组件可能只是完成了一些核心功能。但在不改变其结构的情况下，可以动态地扩展其功能。所有这些都可以釆用装饰器模式来实现。 一，装饰者模式的定义与特点装饰器（Decorator）模式的定义：指在不改变现有对象结构的情况下，动态地给该对象增加一些职责（即增加其额外功能）的模式，它属于对象结构型模式。 装饰器模式的主要优点有： 装饰器是继承的有力补充，比继承灵活，在不改变原有对象的情况下，动态的给一个对象扩展功能，即插即用 通过使用不用装饰类及这些装饰类的排列组合，可以实现不同效果 装饰器模式完全遵守开闭原则 其主要缺点是：装饰器模式会增加许多子类，过度使用会增加程序得复杂性。 二，装饰者模式的结构与实现通常情况下，扩展一个类的功能会使用继承方式来实现。但继承具有静态特征，耦合度高，并且随着扩展功能的增多，子类会很膨胀。如果使用组合关系来创建一个包装对象（即装饰对象）来包裹真实对象，并在保持真实对象的类结构不变的前提下，为其提供额外的功能，这就是装饰器模式的目标。 1.模式的结构装饰器模式主要包含以下角色。 抽象构件（Component）角色：定义一个抽象接口以规范准备接收附加责任的对象。 具体构件（ConcreteComponent）角色：实现抽象构件，通过装饰角色为其添加一些职责。 抽象装饰（Decorator）角色：继承抽象构件，并包含具体构件的实例，可以通过其子类扩展具体构件的功能。 具体装饰（ConcreteDecorator）角色：实现抽象装饰的相关方法，并给具体构件对象添加附加的责任。 2.模式的实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public interface Component &#123; void operation();&#125;public class ConcreteComponent implements Component&#123; public ConcreteComponent() &#123; System.out.println(&quot;创建具体构件角色&quot;); &#125; @Override public void operation() &#123; System.out.println(&quot;调用具体构件角色的方法operation()&quot;); &#125;&#125;public abstract class Decorator implements Component&#123; private Component component; public Decorator(Component component) &#123; this.component = component; &#125; @Override public void operation() &#123; component.operation(); &#125;&#125;public class ConcreteDecorator extends Decorator&#123; public ConcreteDecorator(Component component) &#123; super(component); &#125; @Override public void operation() &#123; super.operation(); addedFunction(); &#125; public void addedFunction() &#123; System.out.println(&quot;为具体构件角色增加额外的功能addedFunction()&quot;); &#125;&#125;public class MainTest &#123; public static void main(String[] args) &#123; Component component = new ConcreteComponent(); Decorator decorator = new ConcreteDecorator(component); decorator.operation(); &#125;&#125; 三，装饰器模式的应用实例用装饰器模式实现游戏角色“莫莉卡·安斯兰”的变身。 分析：在《恶魔战士》中，游戏角色“莫莉卡·安斯兰”的原身是一个可爱少女，但当她变身时，会变成头顶及背部延伸出蝙蝠状飞翼的女妖，当然她还可以变为穿着漂亮外衣的少女。这些都可用装饰器模式来实现，在本实例中的“莫莉卡”原身有 setImage(String t) 方法决定其显示方式，而其 变身“蝙蝠状女妖”和“着装少女”可以用 setChanger() 方法来改变其外观，原身与变身后的效果用 display() 方法来显示。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475import java.awt.*;import javax.swing.*;public class MorriganAensland &#123; public static void main(String[] args) &#123; Morrigan m0 = new original(); m0.display(); Morrigan m1 = new Succubus(m0); m1.display(); Morrigan m2 = new Girl(m0); m2.display(); &#125;&#125;//抽象构件角色：莫莉卡interface Morrigan &#123; public void display();&#125;//具体构件角色：原身class original extends JFrame implements Morrigan &#123; private static final long serialVersionUID = 1L; private String t = &quot;2.jpeg&quot;; public original() &#123; super(&quot;《恶魔战士》中的莫莉卡·安斯兰&quot;); &#125; public void setImage(String t) &#123; this.t = t; &#125; @Override public void display() &#123; this.setLayout(new FlowLayout()); JLabel l1 = new JLabel(new ImageIcon(&quot;/home/yhd/图片/&quot; + t)); this.add(l1); this.pack(); this.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE); this.setVisible(true); &#125;&#125;//抽象装饰角色：变形class Changer implements Morrigan &#123; Morrigan m; public Changer(Morrigan m) &#123; this.m = m; &#125; @Override public void display() &#123; m.display(); &#125;&#125;//具体装饰角色：女妖class Succubus extends Changer &#123; public Succubus(Morrigan m) &#123; super(m); &#125; @Override public void display() &#123; setChanger(); super.display(); &#125; public void setChanger() &#123; ((original) super.m).setImage(&quot;3.jpeg&quot;); &#125;&#125;//具体装饰角色：少女class Girl extends Changer &#123; public Girl(Morrigan m) &#123; super(m); &#125; @Override public void display() &#123; setChanger(); super.display(); &#125; public void setChanger() &#123; ((original) super.m).setImage(&quot;4.jpeg&quot;); &#125;&#125; 四，装饰器模式的应用场景 当需要给一个现有类添加附加职责，而又不能采用生成子类的方法进行扩充时。例如，该类被隐藏或者该类是终极类或者采用继承方式会产生大量的子类。 当需要通过对现有的一组基本功能进行排列组合而产生非常多的功能时，采用继承关系很难实现，而采用装饰器模式却很好实现。 当对象的功能要求可以动态地添加，也可以再动态地撤销时。 Java I/O 标准库的设计 例如，InputStream 的子类 FilterInputStream，OutputStream 的子类 FilterOutputStream，Reader 的子类 BufferedReader 以及 FilterReader，还有 Writer 的子类 BufferedWriter、FilterWriter 以及 PrintWriter 等，它们都是抽象装饰类。 五，装饰器模式的扩展装饰器模式所包含的 4 个角色不是任何时候都要存在的，在有些应用环境下模式是可以简化的，如以下两种情况。 1.如果只有一个具体构件而没有抽象构件时，可以让抽象装饰继承具体构件。 2.如果只有一个具体装饰时，可以将抽象装饰和具体装饰合并。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"外观模式","slug":"设计模式/结构型模式之外观模式","date":"2022-01-11T12:00:30.269Z","updated":"2022-01-11T12:08:09.351Z","comments":true,"path":"2022/01/11/设计模式/结构型模式之外观模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E7%BB%93%E6%9E%84%E5%9E%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%A4%96%E8%A7%82%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"在现实生活中，常常存在办事较复杂的例子，如办房产证或注册一家公司，有时要同多个部门联系，这时要是有一个综合部门能解决一切手续问题就好了。 软件设计也是这样，当一个系统的功能越来越强，子系统会越来越多，客户对系统的访问也变得越来越复杂。这时如果系统内部发生改变，客户端也要跟着改变，这违背了“开闭原则”，也违背了“迪米特法则”，所以有必要为多个子系统提供一个统一的接口，从而降低系统的耦合度，这就是外观模式的目标。 客户去当地房产局办理房产证过户要遇到的相关部门 一，外观模式的定义与特点外观（Facade）模式又叫作门面模式，是一种通过为多个复杂的子系统提供一个一致的接口，而使这些子系统更加容易被访问的模式。该模式对外有一个统一接口，外部应用程序不用关心内部子系统的具体细节，这样会大大降低应用程序的复杂度，提高了程序的可维护性。 在日常编码工作中，我们都在有意无意的大量使用外观模式。只要是高层模块需要调度多个子系统（2个以上的类对象），我们都会自觉地创建一个新的类封装这些子系统，提供精简的接口，让高层模块可以更加容易地间接调用这些子系统的功能。尤其是现阶段各种第三方SDK、开源类库，很大概率都会使用外观模式。 外观（Facade）模式是“迪米特法则”的典型应用，它有以下主要优点。 降低了子系统与客户端之间的耦合度，使得子系统的变化不会影响调用它的客户类。 对客户屏蔽了子系统组件，减少了客户处理的对象数目，并使得子系统使用起来更加容易。 降低了大型软件系统中的编译依赖性，简化了系统在不同平台之间的移植过程，因为编译一个子系统不会影响其他的子系统，也不会影响外观对象。 外观（Facade）模式的主要缺点如下。 不能很好地限制客户使用子系统类，很容易带来未知风险。 增加新的子系统可能需要修改外观类或客户端的源代码，违背了“开闭原则”。 二，外观模式的结构与实现外观（Facade）模式的结构比较简单，主要是定义了一个高层接口。它包含了对各个子系统的引用，客户端可以通过它访问各个子系统的功能。现在来分析其基本结构和实现方法。 1.模式的结构外观（Facade）模式包含以下主要角色。 外观（Facade）角色：为多个子系统对外提供一个共同的接口。 子系统（Sub System）角色：实现系统的部分功能，客户可以通过外观角色访问它。 客户（Client）角色：通过一个外观角色访问各个子系统的功能。 2.模式的实现1234567891011121314151617181920212223242526272829303132333435public class FacadePattern &#123; public static void main(String[] args) &#123; Facade f = new Facade(); f.method(); &#125;&#125;//外观角色class Facade &#123; private SubSystem01 obj1 = new SubSystem01(); private SubSystem02 obj2 = new SubSystem02(); private SubSystem03 obj3 = new SubSystem03(); public void method() &#123; obj1.method1(); obj2.method2(); obj3.method3(); &#125;&#125;//子系统角色class SubSystem01 &#123; public void method1() &#123; System.out.println(&quot;子系统01的method1()被调用！&quot;); &#125;&#125;//子系统角色class SubSystem02 &#123; public void method2() &#123; System.out.println(&quot;子系统02的method2()被调用！&quot;); &#125;&#125;//子系统角色class SubSystem03 &#123; public void method3() &#123; System.out.println(&quot;子系统03的method3()被调用！&quot;); &#125;&#125; 三，外观模式的应用实例用门面模式实现开保时捷一晚上约三个妹子 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public class BaoShiJie &#123; public void happy()&#123; GirlOne one = new GirlOne(); GirlTwo two = new GirlTwo(); GirlThree three = new GirlThree(); one.papapa(&quot;张三&quot;); two.papa(&quot;张三&quot;); three.kuaile(&quot;张三&quot;); &#125;&#125;// =============== 妹子类 ========================== //public class GirlOne &#123; public void papapa(String name )&#123; System.out.println(name+&quot;与美女一号一起鼓掌&quot;); &#125;&#125;public class GirlTwo &#123; public void papa(String name)&#123; System.out.println(name+&quot;与美女二号一起起床&quot;); &#125;&#125;public class GirlThree &#123; public void kuaile(String name)&#123; System.out.println(name+&quot;与美女三号一度春宵&quot;); &#125;&#125;public class MainTest &#123; /** * 设计模式之门面模式 * 需求：一晚上约三个妹子 * 一个一个约 * 门面模式：一窝端 */ @Test public void test()&#123; GirlOne one = new GirlOne(); GirlTwo two = new GirlTwo(); GirlThree three = new GirlThree(); one.papapa(&quot;张三&quot;); two.papa(&quot;张三&quot;); three.kuaile(&quot;张三&quot;); &#125; @Test public void testA()&#123; BaoShiJie panameila = new BaoShiJie(); panameila.happy(); &#125;&#125; 四，外观模式的应用场景通常在以下情况下可以考虑使用外观模式。 对分层结构系统构建时，使用外观模式定义子系统中每层的入口点可以简化子系统之间的依赖关系。 当一个复杂系统的子系统很多时，外观模式可以为系统设计一个简单的接口供外界访问。 当客户端与多个子系统之间存在很大的联系时，引入外观模式可将它们分离，从而提高子系统的独立性和可移植性。 五，外观模式的扩展在外观模式中，当增加或移除子系统时需要修改外观类，这违背了“开闭原则”。如果引入抽象外观类，则在一定程度上解决了该问题。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"享元模式","slug":"设计模式/结构型模式之享元模式","date":"2022-01-11T12:00:19.054Z","updated":"2022-01-11T12:08:36.283Z","comments":true,"path":"2022/01/11/设计模式/结构型模式之享元模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E7%BB%93%E6%9E%84%E5%9E%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E4%BA%AB%E5%85%83%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"在面向对象程序设计过程中，有时会面临要创建大量相同或相似对象实例的问题。创建那么多的对象将会耗费很多的系统资源，它是系统性能提高的一个瓶颈。 例如，围棋和五子棋中的黑白棋子，图像中的坐标点或颜色，局域网中的路由器、交换机和集线器，教室里的桌子和凳子等。这些对象有很多相似的地方，如果能把它们相同的部分提取出来共享，则能节省大量的系统资源，这就是享元模式的产生背景。 一，享元模式的定义与特点享元（Flyweight）模式的定义：运用共享技术来有效地支持大量细粒度对象的复用。它通过共享已经存在的对象来大幅度减少需要创建的对象数量、避免大量相似类的开销，从而提高系统资源的利用率。 享元模式的主要优点是：相同对象只要保存一份，这降低了系统中对象的数量，从而降低了系统中细粒度对象给内存带来的压力。 其主要缺点是： 为了使对象可以共享，需要将一些不能共享的状态外部化，这将增加程序的复杂性。 读取享元模式的外部状态会使得运行时间稍微变长。 时间换空间的思想 二，享元模式的结构与实现享元模式的定义提出了两个要求，细粒度和共享对象。因为要求细粒度，所以不可避免地会使对象数量多且性质相近，此时我们就将这些对象的信息分为两个部分：内部状态和外部状态。 内部状态指对象共享出来的信息，存储在享元信息内部，并且不回随环境的改变而改变； 外部状态指对象得以依赖的一个标记，随环境的改变而改变，不可共享。 比如，连接池中的连接对象，保存在连接对象中的用户名、密码、连接URL等信息，在创建对象的时候就设置好了，不会随环境的改变而改变，这些为内部状态。而当每个连接要被回收利用时，我们需要将它标记为可用状态，这些为外部状态。 享元模式的本质是缓存共享对象，降低内存消耗。 1.模式的结构享元模式的主要角色有如下。 抽象享元角色（Flyweight）：是所有的具体享元类的基类，为具体享元规范需要实现的公共接口，非享元的外部状态以参数的形式通过方法传入。 具体享元（Concrete Flyweight）角色：实现抽象享元角色中所规定的接口。 非享元（Unsharable Flyweight)角色：是不可以共享的外部状态，它以参数的形式注入具体享元的相关方法中。 享元工厂（Flyweight Factory）角色：负责创建和管理享元角色。当客户对象请求一个享元对象时，享元工厂检査系统中是否存在符合要求的享元对象，如果存在则提供给客户；如果不存在的话，则创建一个新的享元对象。 享元模式的结构图，其中： UnsharedConcreteFlyweight 是非享元角色，里面包含了非共享的外部状态信息 info； Flyweight 是抽象享元角色，里面包含了享元方法 operation(UnsharedConcreteFlyweight state)，非享元的外部状态以参数的形式通过该方法传入； ConcreteFlyweight 是具体享元角色，包含了关键字 key，它实现了抽象享元接口； FlyweightFactory 是享元工厂角色，它是关键字 key 来管理具体享元； 客户角色通过享元工厂获取具体享元，并访问具体享元的相关方法。 三，享元模式的应用实例享元模式在五子棋游戏中的应用 分析：五子棋同围棋一样，包含多个“黑”或“白”颜色的棋子，所以用享元模式比较好。 本实例中: 棋子（ChessPieces）类是抽象享元角色，它包含了一个落子的 DownPieces(Graphics g,Point pt) 方法； 白子（WhitePieces）和黑子（BlackPieces）类是具体享元角色，它实现了落子方法； Point 是非享元角色，它指定了落子的位置； WeiqiFactory 是享元工厂角色，它通过 ArrayList 来管理棋子，并且提供了获取白子或者黑子的 getChessPieces(String type) 方法； 客户类（Chessboard）利用 Graphics 组件在框架窗体中绘制一个棋盘，并实现 mouseClicked(MouseEvent e) 事件处理方法，该方法根据用户的选择从享元工厂中获取白子或者黑子并落在棋盘上。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113public class WzqGame &#123; public static void main(String[] args) &#123; new Chessboard(); &#125;&#125;//棋盘class Chessboard extends MouseAdapter &#123; WeiqiFactory wf; JFrame f; Graphics g; JRadioButton wz; JRadioButton bz; private final int x = 50; private final int y = 50; private final int w = 40; //小方格宽度和高度 private final int rw = 400; //棋盘宽度和高度 Chessboard() &#123; wf = new WeiqiFactory(); f = new JFrame(&quot;享元模式在五子棋游戏中的应用&quot;); f.setBounds(100, 100, 500, 550); f.setVisible(true); f.setResizable(false); f.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE); JPanel SouthJP = new JPanel(); f.add(&quot;South&quot;, SouthJP); wz = new JRadioButton(&quot;白子&quot;); bz = new JRadioButton(&quot;黑子&quot;, true); ButtonGroup group = new ButtonGroup(); group.add(wz); group.add(bz); SouthJP.add(wz); SouthJP.add(bz); JPanel CenterJP = new JPanel(); CenterJP.setLayout(null); CenterJP.setSize(500, 500); CenterJP.addMouseListener(this); f.add(&quot;Center&quot;, CenterJP); try &#123; Thread.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; g = CenterJP.getGraphics(); g.setColor(Color.BLUE); g.drawRect(x, y, rw, rw); for (int i = 1; i &lt; 10; i++) &#123; //绘制第i条竖直线 g.drawLine(x + (i * w), y, x + (i * w), y + rw); //绘制第i条水平线 g.drawLine(x, y + (i * w), x + rw, y + (i * w)); &#125; &#125; @Override public void mouseClicked(MouseEvent e) &#123; Point pt = new Point(e.getX() - 15, e.getY() - 15); if (wz.isSelected()) &#123; ChessPieces c1 = wf.getChessPieces(&quot;w&quot;); c1.DownPieces(g, pt); &#125; else if (bz.isSelected()) &#123; ChessPieces c2 = wf.getChessPieces(&quot;b&quot;); c2.DownPieces(g, pt); &#125; &#125;&#125;//抽象享元角色：棋子interface ChessPieces &#123; public void DownPieces(Graphics g, Point pt); //下子&#125;//具体享元角色：白子class WhitePieces implements ChessPieces &#123; @Override public void DownPieces(Graphics g, Point pt) &#123; g.setColor(Color.WHITE); g.fillOval(pt.x, pt.y, 30, 30); &#125;&#125;//具体享元角色：黑子class BlackPieces implements ChessPieces &#123; @Override public void DownPieces(Graphics g, Point pt) &#123; g.setColor(Color.BLACK); g.fillOval(pt.x, pt.y, 30, 30); &#125;&#125;//享元工厂角色class WeiqiFactory &#123; private ArrayList&lt;ChessPieces&gt; qz; public WeiqiFactory() &#123; qz = new ArrayList&lt;ChessPieces&gt;(); ChessPieces w = new WhitePieces(); qz.add(w); ChessPieces b = new BlackPieces(); qz.add(b); &#125; public ChessPieces getChessPieces(String type) &#123; if (type.equalsIgnoreCase(&quot;w&quot;)) &#123; return (ChessPieces) qz.get(0); &#125; else if (type.equalsIgnoreCase(&quot;b&quot;)) &#123; return (ChessPieces) qz.get(1); &#125; else &#123; return null; &#125; &#125;&#125; 四，享元模式的应用场景当系统中多处需要同一组信息时，可以把这些信息封装到一个对象中，然后对该对象进行缓存，这样，一个对象就可以提供给多出需要使用的地方，避免大量同一对象的多次创建，降低大量内存空间的消耗。 享元模式其实是[工厂方法模式]的一个改进机制，享元模式同样要求创建一个或一组对象，并且就是通过工厂方法模式生成对象的，只不过享元模式为工厂方法模式增加了缓存这一功能。 前面分析了享元模式的结构与特点，下面分析它适用的应用场景。享元模式是通过减少内存中对象的数量来节省内存空间的，所以以下几种情形适合采用享元模式。 系统中存在大量相同或相似的对象，这些对象耗费大量的内存资源。 大部分的对象可以按照内部状态进行分组，且可将不同部分外部化，这样每一个组只需保存一个内部状态。 由于享元模式需要额外维护一个保存享元的[数据结构]，所以应当在有足够多的享元实例时才值得使用享元模式。 五，享元模式的扩展在前面介绍的享元模式中，其结构图通常包含可以共享的部分和不可以共享的部分。在实际使用过程中，有时候会稍加改变，即存在两种特殊的享元模式：单纯享元模式和复合享元模式，下面分别对它们进行简单介绍。 单纯享元模式，这种享元模式中的所有的具体享元类都是可以共享的，不存在非共享的具体享元类。 2)复合享元模式，这种享元模式中的有些享元对象是由一些单纯享元对象组合而成的，它们就是复合享元对象。虽然复合享元对象本身不能共享，但它们可以分解成单纯享元对象再被共享。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"组合模式","slug":"设计模式/结构型模式之组合模式","date":"2022-01-11T11:59:59.908Z","updated":"2022-01-11T12:08:58.455Z","comments":true,"path":"2022/01/11/设计模式/结构型模式之组合模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E7%BB%93%E6%9E%84%E5%9E%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E7%BB%84%E5%90%88%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"在现实生活中，存在很多“部分-整体”的关系，例如，大学中的部门与学院、总公司中的部门与分公司、学习用品中的书与书包、生活用品中的衣服与衣柜、以及厨房中的锅碗瓢盆等。在软件开发中也是这样，例如，文件系统中的文件与文件夹、窗体程序中的简单控件与容器控件等。对这些简单对象与复合对象的处理，如果用组合模式来实现会很方便。 一，组合模式的定义与特点它是一种将对象组合成树状的层次结构的模式，用来表示“整体-部分”的关系，使用户对单个对象和组合对象具有一致的访问性，属于结构型[设计模式]。 组合模式一般用来描述整体与部分的关系，它将对象组织到树形结构中，顶层的节点被称为根节点，根节点下面可以包含树枝节点和叶子节点，树枝节点下面又可以包含树枝节点和叶子节点，树形结构图如下。 由上图可以看出，其实根节点和树枝节点本质上属于同一种数据类型，可以作为容器使用；而叶子节点与树枝节点在语义上不属于用一种类型。但是在组合模式中，会把树枝节点和叶子节点看作属于同一种数据类型（用统一接口定义），让它们具备一致行为。 这样，在组合模式中，整个树形结构中的对象都属于同一种类型，带来的好处就是用户不需要辨别是树枝节点还是叶子节点，可以直接进行操作，给用户的使用带来极大的便利。 组合模式的主要优点有： 组合模式使得客户端代码可以一致地处理单个对象和组合对象，无须关心自己处理的是单个对象，还是组合对象，这简化了客户端代码； 更容易在组合体内加入新的对象，客户端不会因为加入了新的对象而更改源代码，满足“开闭原则”； 其主要缺点是： 设计较复杂，客户端需要花更多时间理清类之间的层次关系； 不容易限制容器中的构件； 不容易用继承的方法来增加构件的新功能； 二，组合模式的结构与实现组合模式的结构不是很复杂，下面对它的结构和实现进行分析。 1.模式的结构组合模式包含以下主要角色。 抽象构件（Component）角色：它的主要作用是为树叶构件和树枝构件声明公共接口，并实现它们的默认行为。在透明式的组合模式中抽象构件还声明访问和管理子类的接口；在安全式的组合模式中不声明访问和管理子类的接口，管理工作由树枝构件完成。（总的抽象类或接口，定义一些通用的方法，比如新增、删除） 树叶构件（Leaf）角色：是组合中的叶节点对象，它没有子节点，用于继承或实现抽象构件。 树枝构件（Composite）角色 / 中间构件：是组合中的分支节点对象，它有子节点，用于继承和实现抽象构件。它的主要作用是存储和管理子部件，通常包含 Add()、Remove()、GetChild() 等方法。 组合模式分为透明式的组合模式和安全式的组合模式。 1）透明方式在该方式中，由于抽象构件声明了所有子类中的全部方法，所以客户端无须区别树叶对象和树枝对象，对客户端来说是透明的。但其缺点是：树叶构件本来没有 Add()、Remove() 及 GetChild() 方法，却要实现它们（空实现或抛异常），这样会带来一些安全性问题。 2）安全方式在该方式中，将管理子构件的方法移到树枝构件中，抽象构件和树叶构件没有对子对象的管理方法，这样就避免了上一种方式的安全性问题，但由于叶子和分支有不同的接口，客户端在调用时要知道树叶对象和树枝对象的存在，所以失去了透明性。 2.模式的实现假如要访问集合 c0=&#123;leaf1,&#123;leaf2,leaf3&#125;&#125; 中的元素，其对应的树状图如图所示。 1)透明组合模式1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071class Leaf implements Component &#123; String name; public Leaf(String name) &#123; this.name = name; &#125; @Override public void add(Component c) &#123; &#125; @Override public void remove(Component c) &#123; &#125; @Override public Component getChild(int i) &#123; return null; &#125; @Override public void operation() &#123; System.out.println(&quot;树叶&quot; + name + &quot;被访问！&quot;); &#125;&#125;class Composite implements Component &#123; List&lt;Component&gt; children = new ArrayList&lt;&gt;(); @Override public void add(Component c) &#123; children.add(c); &#125; @Override public void remove(Component c) &#123; children.remove(c); &#125; @Override public Component getChild(int i) &#123; return children.get(i); &#125; @Override public void operation() &#123; if(children!=null &amp;&amp; children.size()&gt;0)&#123; children.forEach(Component::operation); &#125; &#125;&#125;class MainTest&#123; public static void main(String[] args) &#123; Component c0 = new Composite(); Component c1 = new Composite(); Component l1 = new Leaf(&quot;leaf1&quot;); Component l2 = new Leaf(&quot;leaf2&quot;); Component l3 = new Leaf(&quot;leaf3&quot;); c0.add(l1); c0.add(c1); c1.add(l2); c1.add(l3); c0.operation(); &#125;&#125; 2)安全组合模式安全式的组合模式与透明式组合模式的实现代码类似，只要对其做简单修改就可以了，代码如下。 123456789101112131415161718192021public interface Component &#123; void operation();&#125;class MainTest&#123; public static void main(String[] args) &#123; Composite c0 = new Composite(); Composite c1 = new Composite(); Component l1 = new Leaf(&quot;leaf1&quot;); Component l2 = new Leaf(&quot;leaf2&quot;); Component l3 = new Leaf(&quot;leaf3&quot;); c0.add(l1); c0.add(c1); c1.add(l2); c1.add(l3); c0.operation(); &#125;&#125; 三，组合模式的应用实例用组合模式实现当用户在商店购物后，显示其所选商品信息，并计算所选商品总价的功能。 说明：假如李先生到韶关“天街e角”生活用品店购物，用 1 个红色小袋子装了 2 包婺源特产（单价 7.9 元）、1 张婺源地图（单价 9.9 元）；用 1 个白色小袋子装了 2 包韶关香藉（单价 68 元）和 3 包韶关红茶（单价 180 元）；用 1 个中袋子装了前面的红色小袋子和 1 个景德镇瓷器（单价 380 元）；用 1 个大袋子装了前面的中袋子、白色小袋子和 1 双李宁牌运动鞋（单价 198 元）。 最后“大袋子”中的内容有：{1 双李宁牌运动鞋（单价 198 元）、白色小袋子{2 包韶关香菇（单价 68 元）、3 包韶关红茶（单价 180 元）}、中袋子{1 个景德镇瓷器（单价 380 元）、红色小袋子{2 包婺源特产（单价 7.9 元）、1 张婺源地图（单价 9.9 元）}}}，现在要求编程显示李先生放在大袋子中的所有商品信息并计算要支付的总价。 本实例可按安全组合模式设计。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public interface Dad &#123; float calculation(); //计算 void show();&#125;public class Bag implements Dad &#123; private String name; List&lt;Dad&gt; list = new ArrayList&lt;&gt;(); public Bag(String name) &#123; this.name = name; &#125; @Override public float calculation() &#123; float total = 0; for (Dad dad : list) &#123; total += dad.calculation(); &#125; return total; &#125; @Override public void show() &#123; list.forEach(Dad::show); &#125; public void add(Dad c) &#123; list.add(c); &#125; public void remove(Dad c) &#123; list.remove(c); &#125; public Dad getChild(int i) &#123; return list.get(i); &#125;&#125;class Product implements Dad &#123; private String name; //名字 private int quantity; //数量 private float unitPrice; //单价 public Product(String name, int quantity, float unitPrice) &#123; this.name = name; this.quantity = quantity; this.unitPrice = unitPrice; &#125; @Override public float calculation() &#123; return quantity * unitPrice; &#125; @Override public void show() &#123; System.out.println(name + &quot;(数量：&quot; + quantity + &quot;，单价：&quot; + unitPrice + &quot;元)&quot;); &#125;&#125; 四，组合模式的应用场景 在需要表示一个对象整体与部分的层次结构的场合。 要求对用户隐藏组合对象与单个对象的不同，用户可以用统一的接口使用组合结构中的所有对象的场合。 五，组合模式的扩展如果对前面介绍的组合模式中的树叶节点和树枝节点进行抽象，也就是说树叶节点和树枝节点还有子节点，这时组合模式就扩展成复杂的组合模式了，如 [Java] AWT/[Swing] 中的简单组件 JTextComponent 有子类 JTextField、JTextArea，容器组件 Container 也有子类 Window、Panel。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"模板方法模式","slug":"设计模式/行为型模式之模板方法模式","date":"2022-01-11T11:59:38.496Z","updated":"2022-01-11T12:11:00.871Z","comments":true,"path":"2022/01/11/设计模式/行为型模式之模板方法模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%A1%8C%E4%B8%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E6%A8%A1%E6%9D%BF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"在面向对象程序设计过程中，程序员常常会遇到这种情况：设计一个系统时知道了算法所需的关键步骤，而且确定了这些步骤的执行顺序，但某些步骤的具体实现还未知，或者说某些步骤的实现与具体的环境相关。 例如，去银行办理业务一般要经过以下4个流程：取号、排队、办理具体业务、对银行工作人员进行评分等，其中取号、排队和对银行工作人员进行评分的业务对每个客户是一样的，可以在父类中实现，但是办理具体业务却因人而异，它可能是存款、取款或者转账等，可以延迟到子类中实现。 这样的例子在生活中还有很多，例如，一个人每天会起床、吃饭、做事、睡觉等，其中“做事”的内容每天可能不同。我们把这些规定了流程或格式的实例定义成模板，允许使用者根据自己的需求去更新它，例如，简历模板、论文模板、Word 中模板文件等。 一，模式的定义与特点定义一个操作中的算法骨架，而将算法的一些步骤延迟到子类中，使得子类可以不改变该算法结构的情况下重定义该算法的某些特定步骤。它是一种类行为型模式。 该模式的主要优点如下。 它封装了不变部分，扩展可变部分。它把认为是不变部分的算法封装到父类中实现，而把可变部分算法由子类继承实现，便于子类继续扩展。 它在父类中提取了公共的部分代码，便于代码复用。 部分方法是由子类实现的，因此子类可以通过扩展方式增加相应的功能，符合开闭原则。 该模式的主要缺点如下。 对每个不同的实现都需要定义一个子类，这会导致类的个数增加，系统更加庞大，设计也更加抽象，间接地增加了系统实现的复杂度。 父类中的抽象方法由子类实现，子类执行的结果会影响父类的结果，这导致一种反向的控制结构，它提高了代码阅读的难度。 由于继承关系自身的缺点，如果父类添加新的抽象方法，则所有子类都要改一遍。 二，模式的结构与实现模板方法模式需要注意抽象类与具体子类之间的协作。它用到了虚函数的多态性技术以及“不用调用我，让我来调用你”的反向控制技术。现在来介绍它们的基本结构。 1.模式的结构模板方法模式包含以下主要角色： 1）抽象类/抽象模板抽象模板类，负责给出一个算法的轮廓和骨架。它由一个模板方法和若干个基本方法构成。这些方法的定义如下。 ① 模板方法：定义了算法的骨架，按某种顺序调用其包含的基本方法。 ② 基本方法：是整个算法中的一个步骤，包含以下几种类型。 抽象方法：在抽象类中声明，由具体子类实现。 具体方法：在抽象类中已经实现，在具体子类中可以继承或重写它。 钩子方法：在抽象类中已经实现，包括用于判断的逻辑方法和需要子类重写的空方法两种。 2）具体子类/具体实现具体实现类，实现抽象类中所定义的抽象方法和钩子方法，它们是一个顶级逻辑的一个组成步骤。 1234567891011121314151617181920212223242526272829303132333435363738public class TemplateMethodPattern &#123; public static void main(String[] args) &#123; AbstractClass tm = new ConcreteClass(); tm.TemplateMethod(); &#125;&#125;//抽象类abstract class AbstractClass &#123; //模板方法 public void TemplateMethod() &#123; SpecificMethod(); abstractMethod1(); abstractMethod2(); &#125; //具体方法 public void SpecificMethod() &#123; System.out.println(&quot;抽象类中的具体方法被调用...&quot;); &#125; //抽象方法1 public abstract void abstractMethod1(); //抽象方法2 public abstract void abstractMethod2();&#125;//具体子类class ConcreteClass extends AbstractClass &#123; public void abstractMethod1() &#123; System.out.println(&quot;抽象方法1的实现被调用...&quot;); &#125; public void abstractMethod2() &#123; System.out.println(&quot;抽象方法2的实现被调用...&quot;); &#125;&#125; 三，模式的应用实例泡一个妹子分为多个步骤 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public abstract class PaoMei &#123; protected void success(String name)&#123; getWeChat(); chat(); yuehui(); kfc(); &#125; protected void getWeChat()&#123; System.out.println(&quot;得到微信&quot;); &#125; protected void chat()&#123; System.out.println(&quot;聊天，约出来&quot;); &#125; protected void yuehui()&#123; &#125; protected void kfc()&#123; System.out.println(&quot;一起起床&quot;); &#125;&#125;public class Shl extends PaoMei&#123; @Override protected void yuehui() &#123; System.out.println(&quot;开保时捷带她兜风&quot;); &#125;&#125;public class LaTiao extends PaoMei&#123; @Override protected void yuehui() &#123; System.out.println(&quot;舔狗&quot;); &#125;&#125;public class MainTest &#123; @Test public void test()&#123; PaoMei shl= new Shl(); shl.success(&quot;lxm&quot;); PaoMei lt =new LaTiao(); lt.success(&quot;lxm&quot;); &#125;&#125; 四，模式的应用场景模板方法模式通常适用于以下场景。 算法的整体步骤很固定，但其中个别部分易变时，这时候可以使用模板方法模式，将容易变的部分抽象出来，供子类实现。 当多个子类存在公共的行为时，可以将其提取出来并集中到一个公共父类中以避免代码重复。首先，要识别现有代码中的不同之处，并且将不同之处分离为新的操作。最后，用一个调用这些新的操作的模板方法来替换这些不同的代码。 当需要控制子类的扩展时，模板方法只在特定点调用钩子操作，这样就只允许在这些点进行扩展。 五，模式的扩展在模板方法模式中，基本方法包含：抽象方法、具体方法和钩子方法，正确使用“钩子方法”可以使得子类控制父类的行为。如下面例子中，可以通过在具体子类中重写钩子方法 HookMethod1() 和 HookMethod2() 来改变抽象父类中的运行结果。 如果钩子方法 HookMethod1() 和钩子方法 HookMethod2() 的代码改变，则程序的运行结果也会改变。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"策略模式","slug":"设计模式/行为型模式之策略模式","date":"2022-01-11T11:59:27.773Z","updated":"2022-01-11T12:09:53.174Z","comments":true,"path":"2022/01/11/设计模式/行为型模式之策略模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%A1%8C%E4%B8%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E7%AD%96%E7%95%A5%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"在现实生活中常常遇到实现某种目标存在多种策略可供选择的情况，例如，出行旅游可以乘坐飞机、乘坐火车、骑自行车或自己开私家车等，超市促销可以釆用打折、送商品、送积分等方法。 在软件开发中也常常遇到类似的情况，当实现某一个功能存在多种算法或者策略，我们可以根据环境或者条件的不同选择不同的算法或者策略来完成该功能，如数据排序策略有冒泡排序、选择排序、插入排序、二叉树排序等。 如果使用多重条件转移语句实现（即硬编码），不但使条件语句变得很复杂，而且增加、删除或更换算法要修改原代码，不易维护，违背开闭原则。如果采用策略模式就能很好解决该问题。 一，策略模式的定义与特点该模式定义了一系列算法，并将每个算法封装起来，使它们可以相互替换，且算法的变化不会影响使用算法的客户。策略模式属于对象行为模式，它通过对算法进行封装，把使用算法的责任和算法的实现分割开来，并委派给不同的对象对这些算法进行管理。 策略模式的主要优点如下。 多重条件语句不易维护，而使用策略模式可以避免使用多重条件语句，如 if…else 语句、switch…case 语句。 策略模式提供了一系列的可供重用的算法族，恰当使用继承可以把算法族的公共代码转移到父类里面，从而避免重复的代码。 策略模式可以提供相同行为的不同实现，客户可以根据不同时间或空间要求选择不同的。 策略模式提供了对开闭原则的完美支持，可以在不修改原代码的情况下，灵活增加新算法。 策略模式把算法的使用放到环境类中，而算法的实现移到具体策略类中，实现了二者的分离。 其主要缺点如下。 客户端必须理解所有策略算法的区别，以便适时选择恰当的算法类。 策略模式造成很多的策略类，增加维护难度。 二，策略模式的结构与实现策略模式是准备一组算法，并将这组算法封装到一系列的策略类里面，作为一个抽象策略类的子类。策略模式的重心不是如何实现算法，而是如何组织这些算法，从而让程序结构更加灵活，具有更好的维护性和扩展性。 1.模式的结构策略模式的主要角色如下。 抽象策略（Strategy）类：定义了一个公共接口，各种不同的算法以不同的方式实现这个接口，环境角色使用这个接口调用不同的算法，一般使用接口或抽象类实现。 具体策略（Concrete Strategy）类：实现了抽象策略定义的接口，提供具体的算法实现。 环境（Context）类：持有一个策略类的引用，最终给客户端调用。 2.模式的实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class StrategyPattern &#123; public static void main(String[] args) &#123; Context c = new Context(); Strategy s = new ConcreteStrategyA(); c.setStrategy(s); c.strategyMethod(); System.out.println(&quot;-----------------&quot;); s = new ConcreteStrategyB(); c.setStrategy(s); c.strategyMethod(); &#125;&#125;//抽象策略类interface Strategy &#123; public void strategyMethod(); //策略方法&#125;//具体策略类Aclass ConcreteStrategyA implements Strategy &#123; public void strategyMethod() &#123; System.out.println(&quot;具体策略A的策略方法被访问！&quot;); &#125;&#125;//具体策略类Bclass ConcreteStrategyB implements Strategy &#123; public void strategyMethod() &#123; System.out.println(&quot;具体策略B的策略方法被访问！&quot;); &#125;&#125;//环境类class Context &#123; private Strategy strategy; public Strategy getStrategy() &#123; return strategy; &#125; public void setStrategy(Strategy strategy) &#123; this.strategy = strategy; &#125; public void strategyMethod() &#123; strategy.strategyMethod(); &#125;&#125; 三，策略模式的应用实例1234567891011121314151617181920212223242526272829public abstract class GoToSchool &#123; public abstract void gotoSchool();&#125;@Data@AllArgsConstructor@NoArgsConstructorpublic class XiaoMing &#123; private GoToSchool goToSchool; public void gotoSchool()&#123; goToSchool.gotoSchool(); &#125;&#125;public class Car extends GoToSchool&#123; @Override public void gotoSchool() &#123; System.out.println(&quot;有钱，自己开车去&quot;); &#125;&#125;public class Walk extends GoToSchool&#123; @Override public void gotoSchool() &#123; System.out.println(&quot;穷逼，走着走去&quot;); &#125;&#125; 四，策略模式的应用场景策略模式在很多地方用到，如 [Java] SE 中的容器布局管理就是一个典型的实例，Java SE 中的每个容器都存在多种布局供用户选择。在程序设计中，通常在以下几种情况中使用策略模式较多。 一个系统需要动态地在几种算法中选择一种时，可将每个算法封装到策略类中。 一个类定义了多种行为，并且这些行为在这个类的操作中以多个条件语句的形式出现，可将每个条件分支移入它们各自的策略类中以代替这些条件语句。 系统中各算法彼此完全独立，且要求对客户隐藏具体算法的实现细节时。 系统要求使用算法的客户不应该知道其操作的数据时，可使用策略模式来隐藏与算法相关的[数据结构]。 多个类只区别在表现行为不同，可以使用策略模式，在运行时动态选择具体要执行的行为。 五，策略模式的扩展在一个使用策略模式的系统中，当存在的策略很多时，客户端管理所有策略算法将变得很复杂，如果在环境类中使用策略工厂模式来管理这些策略类将大大减少客户端的工作复杂度。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"命令模式","slug":"设计模式/行为型模式之命令模式","date":"2022-01-11T11:59:16.871Z","updated":"2022-01-11T12:10:50.755Z","comments":true,"path":"2022/01/11/设计模式/行为型模式之命令模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%A1%8C%E4%B8%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%91%BD%E4%BB%A4%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"在软件开发系统中，“方法的请求者”与“方法的实现者”之间经常存在紧密的耦合关系，这不利于软件功能的扩展与维护。例如，想对方法进行“撤销、重做、记录”等处理都很不方便，因此“如何将方法的请求者与实现者解耦？”变得很重要，命令模式就能很好地解决这个问题。 在现实生活中，命令模式的例子也很多。比如看电视时，我们只需要轻轻一按遥控器就能完成频道的切换，这就是命令模式，将换台请求和换台处理完全解耦了。电视机遥控器（命令发送者）通过按钮（具体命令）来遥控电视机（命令接收者）。 再比如，我们去餐厅吃饭，菜单不是等到客人来了之后才定制的，而是已经预先配置好的。这样，客人来了就只需要点菜，而不是任由客人临时定制。餐厅提供的菜单就相当于把请求和处理进行了解耦，这就是命令模式的体现。 一，命令/委派模式的定义与特点将一个请求封装为一个对象，使发出请求的责任和执行请求的责任分割开。这样两者之间通过命令对象进行沟通，这样方便将命令对象进行储存、传递、调用、增加与管理。 命令模式的主要优点如下。 通过引入中间件（抽象接口）降低系统的耦合度。 扩展性良好，增加或删除命令非常方便。采用命令模式增加与删除命令不会影响其他类，且满足“开闭原则”。 可以实现宏命令。命令模式可以与[组合模式]结合，将多个命令装配成一个组合命令，即宏命令。 方便实现 Undo 和 Redo 操作。命令模式可以和[备忘录模式](结合，实现命令的撤销与恢复。 可以在现有命令的基础上，增加额外功能。比如日志记录，结合装饰器模式会更加灵活。 其缺点是： 可能产生大量具体的命令类。因为每一个具体操作都需要设计一个具体命令类，这会增加系统的复杂性。 命令模式的结果其实就是接收方的执行结果，但是为了以命令的形式进行架构、解耦请求与实现，引入了额外类型结构（引入了请求方与抽象命令接口），增加了理解上的困难。不过这也是[设计模式]的通病，抽象必然会额外增加类的数量，代码抽离肯定比代码聚合更加难理解。 二，命令模式的结构与实现可以将系统中的相关操作抽象成命令，使调用者与实现者相关分离，其结构如下。 1.模式的结构命令模式包含以下主要角色。 抽象命令类（Command）角色：声明执行命令的接口，拥有执行命令的抽象方法 execute()。 具体命令类（Concrete Command）角色：是抽象命令类的具体实现类，它拥有接收者对象，并通过调用接收者的功能来完成命令要执行的操作。 实现者/接收者（Receiver）角色：执行命令功能的相关操作，是具体命令对象业务的真正实现者。 调用者/请求者（Invoker）角色：是请求的发送者，它通常拥有很多的命令对象，并通过访问命令对象来执行相关请求，它不直接访问接收者。 2.模式的实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package command;public class CommandPattern &#123; public static void main(String[] args) &#123; Command cmd = new ConcreteCommand(); Invoker ir = new Invoker(cmd); System.out.println(&quot;客户访问调用者的call()方法...&quot;); ir.call(); &#125;&#125;//调用者class Invoker &#123; private Command command; public Invoker(Command command) &#123; this.command = command; &#125; public void setCommand(Command command) &#123; this.command = command; &#125; public void call() &#123; System.out.println(&quot;调用者执行命令command...&quot;); command.execute(); &#125;&#125;//抽象命令interface Command &#123; public abstract void execute();&#125;//具体命令class ConcreteCommand implements Command &#123; private Receiver receiver; ConcreteCommand() &#123; receiver = new Receiver(); &#125; public void execute() &#123; receiver.action(); &#125;&#125;//接收者class Receiver &#123; public void action() &#123; System.out.println(&quot;接收者的action()方法被调用...&quot;); &#125;&#125; 三，命令模式的应用实例历总与妹子 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public interface Option &#123; void todoOption();&#125;public class CryOption implements Option&#123; @Override public void todoOption() &#123; System.out.println(&quot;papapa&quot;); &#125;&#125;public class SimleOption implements Option&#123; @Override public void todoOption() &#123; System.out.println(&quot;hahha&quot;); &#125;&#125;public class MeiZi &#123; Option option; public MeiZi(Option option) &#123; this.option = option; &#125; public void doOption()&#123; option.todoOption(); &#125;&#125;public class LiZong &#123; MeiZi meiZi; public LiZong(MeiZi meiZi) &#123; this.meiZi = meiZi; &#125; public void happy()&#123; meiZi.doOption(); &#125;&#125;class MainTest&#123; public static void main(String[] args) &#123; Option option = new CryOption(); new LiZong(new MeiZi(option)).happy(); &#125;&#125; 四，命令模式的应用场景当系统的某项操作具备命令语义，且命令实现不稳定（变化）时，可以通过命令模式解耦请求与实现。使用抽象命令接口使请求方的代码架构稳定，封装接收方具体命令的实现细节。接收方与抽象命令呈现弱耦合（内部方法无需一致），具备良好的扩展性。 命令模式通常适用于以下场景。 请求调用者需要与请求接收者解耦时，命令模式可以使调用者和接收者不直接交互。 系统随机请求命令或经常增加、删除命令时，命令模式可以方便地实现这些功能。 当系统需要执行一组操作时，命令模式可以定义宏命令来实现该功能。 当系统需要支持命令的撤销（Undo）操作和恢复（Redo）操作时，可以将命令对象存储起来，采用备忘录模式来实现。 五，命令模式的扩展在软件开发中，有时将命令模式与前面学的组合模式联合使用，这就构成了宏命令模式，也叫组合命令模式。宏命令包含了一组命令，它充当了具体命令与调用者的双重角色，执行它时将递归调用它所包含的所有命令。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"责任链模式","slug":"设计模式/行为型模式之责任链模式","date":"2022-01-11T11:59:06.959Z","updated":"2022-01-11T12:11:11.154Z","comments":true,"path":"2022/01/11/设计模式/行为型模式之责任链模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%A1%8C%E4%B8%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E8%B4%A3%E4%BB%BB%E9%93%BE%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"在现实生活中，一个事件需要经过多个对象处理是很常见的场景。例如，采购审批流程、请假流程等。公司员工请假，可批假的领导有部门负责人、副总经理、总经理等，但每个领导能批准的天数不同，员工必须根据需要请假的天数去找不同的领导签名，也就是说员工必须记住每个领导的姓名、电话和地址等信息，这无疑增加了难度。 在计算机软硬件中也有相关例子，如总线网中数据报传送，每台计算机根据目标地址是否同自己的地址相同来决定是否接收；还有异常处理中，处理程序根据异常的类型决定自己是否处理该异常。 一，模式的定义与特点责任链（Chain of Responsibility）模式的定义：为了避免请求发送者与多个请求处理者耦合在一起，于是将所有请求的处理者通过前一对象记住其下一个对象的引用而连成一条链；当有请求发生时，可将请求沿着这条链传递，直到有对象处理它为止。 在责任链模式中，客户只需要将请求发送到责任链上即可，无须关心请求的处理细节和请求的传递过程，请求会自动进行传递。所以责任链将请求的发送者和请求的处理者解耦了。 责任链模式是一种对象行为型模式，其主要优点如下。 降低了对象之间的耦合度。该模式使得一个对象无须知道到底是哪一个对象处理其请求以及链的结构，发送者和接收者也无须拥有对方的明确信息。 增强了系统的可扩展性。可以根据需要增加新的请求处理类，满足开闭原则。 增强了给对象指派职责的灵活性。当工作流程发生变化，可以动态地改变链内的成员或者调动它们的次序，也可动态地新增或者删除责任。 责任链简化了对象之间的连接。每个对象只需保持一个指向其后继者的引用，不需保持其他所有处理者的引用，这避免了使用众多的 if 或者 if···else 语句。 责任分担。每个类只需要处理自己该处理的工作，不该处理的传递给下一个对象完成，明确各类的责任范围，符合类的单一职责原则。 其主要缺点如下。 不能保证每个请求一定被处理。由于一个请求没有明确的接收者，所以不能保证它一定会被处理，该请求可能一直传到链的末端都得不到处理。 对比较长的职责链，请求的处理可能涉及多个处理对象，系统性能将受到一定影响。 职责链建立的合理性要靠客户端来保证，增加了客户端的复杂性，可能会由于职责链的错误设置而导致系统出错，如可能会造成循环调用。 二，模式的结构与实现1.模式的结构职责链模式主要包含以下角色。 抽象处理者（Handler）角色：定义一个处理请求的接口，包含抽象处理方法和一个后继连接。 具体处理者（Concrete Handler）角色：实现抽象处理者的处理方法，判断能否处理本次请求，如果可以处理请求则处理，否则将该请求转给它的后继者。 客户类（Client）角色：创建处理链，并向链头的具体处理者对象提交请求，它不关心处理细节和请求的传递过程。 责任链模式的本质是解耦请求与处理，让请求在处理链中能进行传递与被处理；理解责任链模式应当理解其模式，而不是其具体实现。责任链模式的独到之处是将其节点处理者组合成了链式结构，并允许节点自身决定是否进行请求处理或转发，相当于让请求流动起来。 2.模式的实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public abstract class Handler &#123; private Handler next; public void setNext(Handler next) &#123; this.next = next; &#125; public Handler getNext() &#123; return next; &#125; public abstract void handleRequest(String request);&#125;class Handler1 extends Handler&#123; @Override public void handleRequest(String request) &#123; if (request.equals(&quot;one&quot;))&#123; System.out.println(&quot;handler1&quot;); &#125; if (getNext()!=null)&#123; getNext().handleRequest(request); &#125; &#125;&#125;class Handler2 extends Handler&#123; @Override public void handleRequest(String request) &#123; if (request.equals(&quot;one&quot;))&#123; System.out.println(&quot;handler2&quot;); &#125; if (getNext()!=null)&#123; getNext().handleRequest(request); &#125; &#125;&#125;class MainTest&#123; public static void main(String[] args) &#123; Handler h1 = new Handler1(); Handler h2 = new Handler2(); h1.setNext(h2); h1.handleRequest(&quot;one&quot;); &#125;&#125; 在上面代码中，我们把消息硬编码为 String 类型，而在真实业务中，消息是具备多样性的，可以是 int、String 或者自定义类型。因此，在上面代码的基础上，可以对消息类型进行抽象 Request，增强了消息的兼容性。 三，模式的应用实例用责任链设计一个请假条审批模块 分析：假如规定学生请假小于或等于 2 天，班主任可以批准；小于或等于 7 天，系主任可以批准；小于或等于 10 天，院长可以批准；其他情况不予批准；这个实例适合使用职责链模式实现。 首先，定义一个领导类（Leader），它是抽象处理者，包含了一个指向下一位领导的指针 next 和一个处理假条的抽象处理方法 handleRequest(int LeaveDays)；然后，定义班主任类（ClassAdviser）、系主任类（DepartmentHead）和院长类（Dean），它们是抽象处理者的子类，是具体处理者，必须根据自己的权力去实现父类的 handleRequest(int LeaveDays) 方法，如果无权处理就将假条交给下一位具体处理者，直到最后；客户类负责创建处理链，并将假条交给链头的具体处理者（班主任）。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101public abstract class Leader &#123; private Leader next; //0 未处理 1 通过 2 驳回 public int status; public void setNext(Leader next) &#123; this.next = next; &#125; public Leader getNext() &#123; return next; &#125; public abstract Boolean doRequest(int day);&#125;class Teacher extends Leader &#123; @Override public Boolean doRequest(int day) &#123; if (day &lt;= 2) &#123; status = 1; System.out.println(&quot;老师处理，通过&quot;); return true; &#125; else &#123; if (getNext() != null) &#123; System.out.println(&quot;老师流转，通过&quot;); return getNext().doRequest(day); &#125; &#125; return false; &#125;&#125;class XiZhuRen extends Leader &#123; @Override public Boolean doRequest(int day) &#123; if (day &lt;= 7) &#123; status = 1; System.out.println(&quot;系主任处理，通过&quot;); return true; &#125; else &#123; if (getNext() != null) &#123; System.out.println(&quot;系主任流转，通过&quot;); return getNext().doRequest(day); &#125; &#125; return false; &#125;&#125;class YuanZhang extends Leader &#123; @Override public Boolean doRequest(int day) &#123; if (day &lt;= 10) &#123; status = 1; System.out.println(&quot;院长处理，通过&quot;); return true; &#125; else &#123; if (getNext() != null) &#123; System.out.println(&quot;院长流转，通过&quot;); return getNext().doRequest(day); &#125; &#125; status = 2; System.out.println(&quot;院长处理，NO通过&quot;); return false; &#125;&#125;class Student &#123; Leader leader; public Student(Leader leader) &#123; this.leader = leader; &#125; public Boolean doRequest(int day) &#123; return leader.doRequest(day); &#125;&#125;class MainTest &#123; public static void main(String[] args) &#123; Leader tea = new Teacher(); Leader xi = new XiZhuRen(); Leader yuan = new YuanZhang(); tea.setNext(xi); xi.setNext(yuan); Student stu = new Student(tea); stu.doRequest(11); &#125;&#125; 四，模式的应用场景 多个对象可以处理一个请求，但具体由哪个对象处理该请求在运行时自动确定。 可动态指定一组对象处理请求，或添加新的处理者。 需要在不明确指定请求处理者的情况下，向多个处理者中的一个提交请求。 五，模式的扩展职责链模式存在以下两种情况。 纯的职责链模式：一个请求必须被某一个处理者对象所接收，且一个具体处理者对某个请求的处理只能采用以下两种行为之一：自己处理（承担责任）；把责任推给下家处理。 不纯的职责链模式：允许出现某一个具体处理者对象在承担了请求的一部分责任后又将剩余的责任传给下家的情况，且一个请求可以最终不被任何接收端对象所接收。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"状态模式","slug":"设计模式/行为型模式之状态模式","date":"2022-01-11T11:58:57.230Z","updated":"2022-01-11T12:11:33.423Z","comments":true,"path":"2022/01/11/设计模式/行为型模式之状态模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%A1%8C%E4%B8%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E7%8A%B6%E6%80%81%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"在软件开发过程中，应用程序中的部分对象可能会根据不同的情况做出不同的行为，我们把这种对象称为有状态的对象，而把影响对象行为的一个或多个动态变化的属性称为状态。当有状态的对象与外部事件产生互动时，其内部状态就会发生改变，从而使其行为也发生改变。如人都有高兴和伤心的时候，不同的情绪有不同的行为，当然外界也会影响其情绪变化。 对这种有状态的对象编程，传统的解决方案是：将这些所有可能发生的情况全都考虑到，然后使用 if-else 或 switch-case 语句来做状态判断，再进行不同情况的处理。但是显然这种做法对复杂的状态判断存在天然弊端，条件判断语句会过于臃肿，可读性差，且不具备扩展性，维护难度也大。且增加新的状态时要添加新的 if-else 语句，这违背了“开闭原则”，不利于程序的扩展。 以上问题如果采用“状态模式”就能很好地得到解决。状态模式的解决思想是：当控制一个对象状态转换的条件表达式过于复杂时，把相关“判断逻辑”提取出来，用各个不同的类进行表示，系统处于哪种情况，直接使用相应的状态类对象进行处理，这样能把原来复杂的逻辑判断简单化，消除了 if-else、switch-case 等冗余语句，代码更有层次性，并且具备良好的扩展力。 一，状态模式的定义与特点状态（State）模式的定义：对有状态的对象，把复杂的“判断逻辑”提取到不同的状态对象中，允许状态对象在其内部状态发生改变时改变其行为。 状态模式是一种对象行为型模式，其主要优点如下。 结构清晰，状态模式将与特定状态相关的行为局部化到一个状态中，并且将不同状态的行为分割开来，满足“单一职责原则”。 将状态转换显示化，减少对象间的相互依赖。将不同的状态引入独立的对象中会使得状态转换变得更加明确，且减少对象间的相互依赖。 状态类职责明确，有利于程序的扩展。通过定义新的子类很容易地增加新的状态和转换。 状态模式的主要缺点如下。 状态模式的使用必然会增加系统的类与对象的个数。 状态模式的结构与实现都较为复杂，如果使用不当会导致程序结构和代码的混乱。 状态模式对开闭原则的支持并不太好，对于可以切换状态的状态模式，增加新的状态类需要修改那些负责状态转换的源码，否则无法切换到新增状态，而且修改某个状态类的行为也需要修改对应类的源码。 二，状态模式的结构与实现状态模式把受环境改变的对象行为包装在不同的状态对象里，其意图是让一个对象在其内部状态改变的时候，其行为也随之改变。 1.模式的结构状态模式包含以下主要角色。 环境类（Context）角色：也称为上下文，它定义了客户端需要的接口，内部维护一个当前状态，并负责具体状态的切换。 抽象状态（State）角色：定义一个接口，用以封装环境对象中的特定状态所对应的行为，可以有一个或多个行为。 具体状态（Concrete State）角色：实现抽象状态所对应的行为，并且在需要的情况下进行状态切换。 2.模式的实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public class StatePatternClient &#123; public static void main(String[] args) &#123; Context context = new Context(); //创建环境 context.Handle(); //处理请求 context.Handle(); context.Handle(); context.Handle(); &#125;&#125;//环境类class Context &#123; private State state; //定义环境类的初始状态 public Context() &#123; this.state = new ConcreteStateA(); &#125; //设置新状态 public void setState(State state) &#123; this.state = state; &#125; //读取状态 public State getState() &#123; return (state); &#125; //对请求做处理 public void Handle() &#123; state.Handle(this); &#125;&#125;//抽象状态类abstract class State &#123; public abstract void Handle(Context context);&#125;//具体状态A类class ConcreteStateA extends State &#123; public void Handle(Context context) &#123; System.out.println(&quot;当前状态是 A.&quot;); context.setState(new ConcreteStateB()); &#125;&#125;//具体状态B类class ConcreteStateB extends State &#123; public void Handle(Context context) &#123; System.out.println(&quot;当前状态是 B.&quot;); context.setState(new ConcreteStateA()); &#125;&#125; 三，状态模式的应用实例由状态决定yh结果 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273public abstract class State &#123; public void happy() &#123; &#125; public State next() &#123; throw new RuntimeException(); &#125;&#125;public class Bad extends State&#123; @Override public void happy() &#123; System.out.println(&quot;状态不好 回家睡觉&quot;); &#125; @Override public State next() &#123; return new Good(); &#125;&#125;public class Good extends State&#123; @Override public void happy() &#123; System.out.println(&quot;状态很好 一起起床&quot;); &#125; @Override public State next() &#123; return new Bad(); &#125;&#125;@Data@NoArgsConstructor@AllArgsConstructor@Accessors(chain = true)public class YueHui &#123; private State state; public void happy()&#123; state.happy(); &#125; public void next()&#123; this.state=state.next(); &#125;&#125;/** * @author yhd * @email yinhuidong1@xiaomi.com * @description 跟妹子出去约会 * 状态很好 一起起床 * 状态很差 回家睡觉 * @since 2021/5/22 上午12:52 */public class MainTest &#123; @Test public void test() &#123; YueHui event = new YueHui(new Good()); event.happy(); event.next(); event.happy(); &#125;&#125; 设计一个多线程的状态转换程序 分析：多线程存在 5 种状态，分别为新建状态、就绪状态、运行状态、阻塞状态和死亡状态，各个状态当遇到相关方法调用或事件触发时会转换到其他状态。 现在先定义一个抽象状态类（TheadState），然后为每个状态设计一个具体状态类，它们是新建状态（New）、就绪状态（Runnable ）、运行状态（Running）、阻塞状态（Blocked）和死亡状态（Dead），每个状态中有触发它们转变状态的方法，环境类（ThreadContext）中先生成一个初始状态（New），并提供相关触发方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138public class ScoreStateTest &#123; public static void main(String[] args) &#123; ThreadContext context = new ThreadContext(); context.start(); context.getCPU(); context.suspend(); context.resume(); context.getCPU(); context.stop(); &#125;&#125;//环境类class ThreadContext &#123; private ThreadState state; ThreadContext() &#123; state = new New(); &#125; public void setState(ThreadState state) &#123; this.state = state; &#125; public ThreadState getState() &#123; return state; &#125; public void start() &#123; ((New) state).start(this); &#125; public void getCPU() &#123; ((Runnable) state).getCPU(this); &#125; public void suspend() &#123; ((Running) state).suspend(this); &#125; public void stop() &#123; ((Running) state).stop(this); &#125; public void resume() &#123; ((Blocked) state).resume(this); &#125;&#125;//抽象状态类：线程状态abstract class ThreadState &#123; protected String stateName; //状态名&#125;//具体状态类：新建状态class New extends ThreadState &#123; public New() &#123; stateName = &quot;新建状态&quot;; System.out.println(&quot;当前线程处于：新建状态.&quot;); &#125; public void start(ThreadContext hj) &#123; System.out.print(&quot;调用start()方法--&gt;&quot;); if (stateName.equals(&quot;新建状态&quot;)) &#123; hj.setState(new Runnable()); &#125; else &#123; System.out.println(&quot;当前线程不是新建状态，不能调用start()方法.&quot;); &#125; &#125;&#125;//具体状态类：就绪状态class Runnable extends ThreadState &#123; public Runnable() &#123; stateName = &quot;就绪状态&quot;; System.out.println(&quot;当前线程处于：就绪状态.&quot;); &#125; public void getCPU(ThreadContext hj) &#123; System.out.print(&quot;获得CPU时间--&gt;&quot;); if (stateName.equals(&quot;就绪状态&quot;)) &#123; hj.setState(new Running()); &#125; else &#123; System.out.println(&quot;当前线程不是就绪状态，不能获取CPU.&quot;); &#125; &#125;&#125;//具体状态类：运行状态class Running extends ThreadState &#123; public Running() &#123; stateName = &quot;运行状态&quot;; System.out.println(&quot;当前线程处于：运行状态.&quot;); &#125; public void suspend(ThreadContext hj) &#123; System.out.print(&quot;调用suspend()方法--&gt;&quot;); if (stateName.equals(&quot;运行状态&quot;)) &#123; hj.setState(new Blocked()); &#125; else &#123; System.out.println(&quot;当前线程不是运行状态，不能调用suspend()方法.&quot;); &#125; &#125; public void stop(ThreadContext hj) &#123; System.out.print(&quot;调用stop()方法--&gt;&quot;); if (stateName.equals(&quot;运行状态&quot;)) &#123; hj.setState(new Dead()); &#125; else &#123; System.out.println(&quot;当前线程不是运行状态，不能调用stop()方法.&quot;); &#125; &#125;&#125;//具体状态类：阻塞状态class Blocked extends ThreadState &#123; public Blocked() &#123; stateName = &quot;阻塞状态&quot;; System.out.println(&quot;当前线程处于：阻塞状态.&quot;); &#125; public void resume(ThreadContext hj) &#123; System.out.print(&quot;调用resume()方法--&gt;&quot;); if (stateName.equals(&quot;阻塞状态&quot;)) &#123; hj.setState(new Runnable()); &#125; else &#123; System.out.println(&quot;当前线程不是阻塞状态，不能调用resume()方法.&quot;); &#125; &#125;&#125;//具体状态类：死亡状态class Dead extends ThreadState &#123; public Dead() &#123; stateName = &quot;死亡状态&quot;; System.out.println(&quot;当前线程处于：死亡状态.&quot;); &#125;&#125; 四，状态模式的应用场景通常在以下情况下可以考虑使用状态模式。 当一个对象的行为取决于它的状态，并且它必须在运行时根据状态改变它的行为时，就可以考虑使用状态模式。 一个操作中含有庞大的分支结构，并且这些分支决定于对象的状态时。 五，状态模式的扩展在有些情况下，可能有多个环境对象需要共享一组状态，这时需要引入享元模式，将这些具体状态对象放在集合中供程序共享。 分析：共享状态模式的不同之处是在环境类中增加了一个 HashMap 来保存相关状态，当需要某种状态时可以从中获取，其程序代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364package state;import java.util.HashMap;public class FlyweightStatePattern &#123; public static void main(String[] args) &#123; ShareContext context = new ShareContext(); //创建环境 context.Handle(); //处理请求 context.Handle(); context.Handle(); context.Handle(); &#125;&#125;//环境类class ShareContext &#123; private ShareState state; private HashMap&lt;String, ShareState&gt; stateSet = new HashMap&lt;String, ShareState&gt;(); public ShareContext() &#123; state = new ConcreteState1(); stateSet.put(&quot;1&quot;, state); state = new ConcreteState2(); stateSet.put(&quot;2&quot;, state); state = getState(&quot;1&quot;); &#125; //设置新状态 public void setState(ShareState state) &#123; this.state = state; &#125; //读取状态 public ShareState getState(String key) &#123; ShareState s = (ShareState) stateSet.get(key); return s; &#125; //对请求做处理 public void Handle() &#123; state.Handle(this); &#125;&#125;//抽象状态类abstract class ShareState &#123; public abstract void Handle(ShareContext context);&#125;//具体状态1类class ConcreteState1 extends ShareState &#123; public void Handle(ShareContext context) &#123; System.out.println(&quot;当前状态是： 状态1&quot;); context.setState(context.getState(&quot;2&quot;)); &#125;&#125;//具体状态2类class ConcreteState2 extends ShareState &#123; public void Handle(ShareContext context) &#123; System.out.println(&quot;当前状态是： 状态2&quot;); context.setState(context.getState(&quot;1&quot;)); &#125;&#125; 六，扩展状态模式与责任链模式的区别 状态模式和责任链模式都能消除 if-else 分支过多的问题。但在某些情况下，状态模式中的状态可以理解为责任，那么在这种情况下，两种模式都可以使用。 从定义来看，状态模式强调的是一个对象内在状态的改变，而责任链模式强调的是外部节点对象间的改变。 从代码实现上来看，两者最大的区别就是状态模式的各个状态对象知道自己要进入的下一个状态对象，而责任链模式并不清楚其下一个节点处理对象，因为链式组装由客户端负责。 状态模式与策略模式的区别 状态模式和策略模式的 UML 类图架构几乎完全一样，但两者的应用场景是不一样的。策略模式的多种算法行为择其一都能满足，彼此之间是独立的，用户可自行更换策略算法，而状态模式的各个状态间存在相互关系，彼此之间在一定条件下存在自动切换状态的效果，并且用户无法指定状态，只能设置初始状态。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"观察者模式","slug":"设计模式/行为型模式之观察者模式","date":"2022-01-11T11:58:43.800Z","updated":"2022-01-11T12:10:31.385Z","comments":true,"path":"2022/01/11/设计模式/行为型模式之观察者模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%A1%8C%E4%B8%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E8%A7%82%E5%AF%9F%E8%80%85%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"在现实世界中，许多对象并不是独立存在的，其中一个对象的行为发生改变可能会导致一个或者多个其他对象的行为也发生改变。例如，某种商品的物价上涨时会导致部分商家高兴，而消费者伤心；还有，当我们开车到交叉路口时，遇到红灯会停，遇到绿灯会行。这样的例子还有很多，例如，股票价格与股民、微信公众号与微信用户、气象局的天气预报与听众、小偷与警察等。 在软件世界也是这样，例如，Excel 中的数据与折线图、饼状图、柱状图之间的关系；MVC 模式中的模型与视图的关系；事件模型中的事件源与事件处理者。所有这些，如果用观察者模式来实现就非常方便。 一，模式的定义与特点指多个对象间存在一对多的依赖关系，当一个对象的状态发生改变时，所有依赖于它的对象都得到通知并被自动更新。这种模式有时又称作发布-订阅模式、模型-视图模式，它是对象行为型模式。 观察者模式是一种对象行为型模式，其主要优点如下。 降低了目标与观察者之间的耦合关系，两者之间是抽象耦合关系。符合依赖倒置原则。 目标与观察者之间建立了一套触发机制。 它的主要缺点如下。 目标与观察者之间的依赖关系并没有完全解除，而且有可能出现循环引用。 当观察者对象很多时，通知的发布会花费很多时间，影响程序的效率。 二，模式的结构与实现实现观察者模式时要注意具体目标对象和具体观察者对象之间不能直接调用，否则将使两者之间紧密耦合起来，这违反了面向对象的设计原则。 1.模式的结构观察者模式的主要角色如下。 抽象主题（Subject）角色：也叫抽象目标类，它提供了一个用于保存观察者对象的聚集类和增加、删除观察者对象的方法，以及通知所有观察者的抽象方法。 具体主题（Concrete Subject）角色：也叫具体目标类，它实现抽象目标中的通知方法，当具体主题的内部状态发生改变时，通知所有注册过的观察者对象。 抽象观察者（Observer）角色：它是一个抽象类或接口，它包含了一个更新自己的抽象方法，当接到具体主题的更改通知时被调用。 具体观察者（Concrete Observer）角色：实现抽象观察者中定义的抽象方法，以便在得到目标的更改通知时更新自身的状态。 2.模式的实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public abstract class Subject &#123; protected List&lt;Observer&gt; observers = new ArrayList(); public void add(Observer obs) &#123; observers.add(obs); &#125; public void remove(Observer obs) &#123; observers.remove(obs); &#125; public abstract void notifyObserver(); //通知观察者方法&#125;//抽象观察者interface Observer &#123; void response(); //反应&#125;//具体目标class ConcreteSubject extends Subject &#123; @Override public void notifyObserver() &#123; observers.forEach(Observer::response); &#125;&#125;//具体观察者1class ConcreteObserver1 implements Observer &#123; @Override public void response() &#123; System.out.println(&quot;具体观察者1作出反应！&quot;); &#125;&#125;//具体观察者1class ConcreteObserver2 implements Observer &#123; @Override public void response() &#123; System.out.println(&quot;具体观察者2作出反应！&quot;); &#125;&#125;class MainTest &#123; public static void main(String[] args) &#123; Subject subject = new ConcreteSubject(); subject.add(new ConcreteObserver1()); subject.add(new ConcreteObserver2()); subject.notifyObserver(); &#125;&#125; 三，模式的应用实例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455@Data@AllArgsConstructor@NoArgsConstructorpublic class Mm &#123; private List&lt;TianGou&gt; tianGous; public void sendEvent()&#123; for (TianGou gou : tianGous) &#123; gou.acceptEvent(&quot;今天休息，不出去了&quot;); &#125; &#125;&#125;public abstract class TianGou &#123; abstract void acceptEvent(String event);&#125;@Data@NoArgsConstructorpublic class TianGouOne extends TianGou&#123; @Override public void acceptEvent(String event) &#123; System.out.println(&quot;TianGouOne ====&quot;+event); &#125;&#125;@Data@NoArgsConstructorpublic class TianGouTwo extends TianGou&#123; @Override public void acceptEvent(String event) &#123; System.out.println(&quot;TianGouTwo ====&quot;+event); &#125;&#125;public class MainTest &#123; @Test public void test() &#123; TianGou one = new TianGouOne(); TianGou two = new TianGouTwo(); List&lt;TianGou&gt; tianGous = new ArrayList&lt;&gt;(); tianGous.add(one); tianGous.add(two); Mm mm = new Mm(tianGous); mm.sendEvent(); &#125;&#125; 四，模式的应用场景在软件系统中，当系统一方行为依赖另一方行为的变动时，可使用观察者模式松耦合联动双方，使得一方的变动可以通知到感兴趣的另一方对象，从而让另一方对象对此做出响应。 通过前面的分析与应用实例可知观察者模式适合以下几种情形。 对象间存在一对多关系，一个对象的状态发生改变会影响其他对象。 当一个抽象模型有两个方面，其中一个方面依赖于另一方面时，可将这二者封装在独立的对象中以使它们可以各自独立地改变和复用。 实现类似广播机制的功能，不需要知道具体收听者，只需分发广播，系统中感兴趣的对象会自动接收该广播。 多层级嵌套使用，形成一种链式触发机制，使得事件具备跨域（跨越两种观察者类型）通知。 五，模式的扩展在 [Java] 中，通过 java.util.Observable 类和 java.util.Observer 接口定义了观察者模式，只要实现它们的子类就可以编写观察者模式实例。 1. Observable类Observable 类是抽象目标类，它有一个 Vector 向量，用于保存所有要通知的观察者对象，下面来介绍它最重要的 3 个方法。 void addObserver(Observer o) 方法：用于将新的观察者对象添加到向量中。 void notifyObservers(Object arg) 方法：调用向量中的所有观察者对象的 update() 方法，通知它们数据发生改变。通常越晚加入向量的观察者越先得到通知。 void setChange() 方法：用来设置一个 boolean 类型的内部标志位，注明目标对象发生了变化。当它为真时，notifyObservers() 才会通知观察者。 2. Observer 接口Observer 接口是抽象观察者，它监视目标对象的变化，当目标对象发生变化时，观察者得到通知，并调用 void update(Observable o,Object arg) 方法，进行相应的工作。 12345678910111213141516171819202122232425262728293031public class PublishAdapter extends Observable &#123; public void publishMessage(String message) &#123; setChanged(); notifyObservers(message); &#125;&#125;@Datapublic class ListenerAdapter implements Observer &#123; @Override public void update(Observable o, Object arg) &#123; PublishAdapter publishAdapter= (PublishAdapter) o; String message = (String) arg; System.out.println(&quot;o = &quot; + o); System.out.println(&quot;arg = &quot; + arg); &#125;&#125;public static void main(String[] args) &#123; PublishAdapter publish = new PublishAdapter(); ListenerAdapter listenerAdapter = new ListenerAdapter(); publish.addObserver(listenerAdapter); publish.publishMessage(&quot;HAHAH&quot;);&#125; 3.基于 Guava API 轻松落地观察者模式12345&lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;29.0-jre&lt;/version&gt;&lt;/dependency&gt; 1234567891011121314151617public class GuavaEvent &#123; @Subscribe public void subscribe(String str) &#123; //业务逻辑 System.out.println(&quot;执行 subscribe 方法,传入的参数是:&quot; + str); &#125;&#125;public class GuavaEventTest &#123; public static void main(String[] args) &#123; EventBus eventbus = new EventBus(); GuavaEvent guavaEvent = new GuavaEvent(); eventbus.register(guavaEvent); eventbus.post(&quot;Tom&quot;); &#125;&#125;","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"中介者模式","slug":"设计模式/行为型模式之中介者模式","date":"2022-01-11T11:58:32.749Z","updated":"2022-01-11T12:11:22.263Z","comments":true,"path":"2022/01/11/设计模式/行为型模式之中介者模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%A1%8C%E4%B8%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E4%B8%AD%E4%BB%8B%E8%80%85%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"在现实生活中，常常会出现好多对象之间存在复杂的交互关系，这种交互关系常常是“网状结构”，它要求每个对象都必须知道它需要交互的对象。例如，每个人必须记住他（她）所有朋友的电话；而且，朋友中如果有人的电话修改了，他（她）必须让其他所有的朋友一起修改，这叫作“牵一发而动全身”，非常复杂。 如果把这种“网状结构”改为“星形结构”的话，将大大降低它们之间的“耦合性”，这时只要找一个“中介者”就可以了。如前面所说的“每个人必须记住所有朋友电话”的问题，只要在网上建立一个每个朋友都可以访问的“通信录”就解决了。这样的例子还有很多，例如，你刚刚参加工作想租房，可以找“房屋中介”；或者，自己刚刚到一个陌生城市找工作，可以找“人才交流中心”帮忙。 在软件的开发过程中，这样的例子也很多，例如，在 MVC 框架中，控制器（C）就是模型（M）和视图（V）的中介者；还有大家常用的 QQ 聊天程序的“中介者”是 QQ 服务器。所有这些，都可以采用“中介者模式”来实现，它将大大降低对象之间的耦合性，提高系统的灵活性。 一，模式的定义与特点中介者（Mediator）模式的定义：定义一个中介对象来封装一系列对象之间的交互，使原有对象之间的耦合松散，且可以独立地改变它们之间的交互。中介者模式又叫调停模式，它是迪米特法则的典型应用。 中介者模式是一种对象行为型模式，其主要优点如下。 类之间各司其职，符合迪米特法则。 降低了对象之间的耦合性，使得对象易于独立地被复用。 将对象间的一对多关联转变为一对一的关联，提高系统的灵活性，使得系统易于维护和扩展。 其主要缺点是：中介者模式将原本多个对象直接的相互依赖变成了中介者和多个同事类的依赖关系。当同事类越多时，中介者就会越臃肿，变得复杂且难以维护。 二，模式的结构与实现中介者模式实现的关键是找出“中介者” 1.模式的结构中介者模式包含以下主要角色。 抽象中介者（Mediator）角色：它是中介者的接口，提供了同事对象注册与转发同事对象信息的抽象方法。 具体中介者（Concrete Mediator）角色：实现中介者接口，定义一个 List 来管理同事对象，协调各个同事角色之间的交互关系，因此它依赖于同事角色。 抽象同事类（Colleague）角色：定义同事类的接口，保存中介者对象，提供同事对象交互的抽象方法，实现所有相互影响的同事类的公共功能。 具体同事类（Concrete Colleague）角色：是抽象同事类的实现者，当需要与其他同事对象交互时，由中介者对象负责后续的交互。 2.模式的实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677public class MediatorPattern &#123; public static void main(String[] args) &#123; Mediator md = new ConcreteMediator(); Colleague c1, c2; c1 = new ConcreteColleague1(); c2 = new ConcreteColleague2(); md.register(c1); md.register(c2); c1.send(); System.out.println(&quot;-------------&quot;); c2.send(); &#125;&#125;//抽象中介者abstract class Mediator &#123; public abstract void register(Colleague colleague); public abstract void relay(Colleague cl); //转发&#125;//具体中介者class ConcreteMediator extends Mediator &#123; private List&lt;Colleague&gt; colleagues = new ArrayList&lt;Colleague&gt;(); public void register(Colleague colleague) &#123; if (!colleagues.contains(colleague)) &#123; colleagues.add(colleague); colleague.setMedium(this); &#125; &#125; public void relay(Colleague cl) &#123; for (Colleague ob : colleagues) &#123; if (!ob.equals(cl)) &#123; ((Colleague) ob).receive(); &#125; &#125; &#125;&#125;//抽象同事类abstract class Colleague &#123; protected Mediator mediator; public void setMedium(Mediator mediator) &#123; this.mediator = mediator; &#125; public abstract void receive(); public abstract void send();&#125;//具体同事类class ConcreteColleague1 extends Colleague &#123; public void receive() &#123; System.out.println(&quot;具体同事类1收到请求。&quot;); &#125; public void send() &#123; System.out.println(&quot;具体同事类1发出请求。&quot;); mediator.relay(this); //请中介者转发 &#125;&#125;//具体同事类class ConcreteColleague2 extends Colleague &#123; public void receive() &#123; System.out.println(&quot;具体同事类2收到请求。&quot;); &#125; public void send() &#123; System.out.println(&quot;具体同事类2发出请求。&quot;); mediator.relay(this); //请中介者转发 &#125;&#125; 三，模式的应用实例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126import javax.swing.*;import java.awt.*;import java.awt.event.ActionEvent;import java.awt.event.ActionListener;import java.util.ArrayList;import java.util.List;public class DatingPlatform &#123; public static void main(String[] args) &#123; Medium md = new EstateMedium(); //房产中介 Customer member1, member2; member1 = new Seller(&quot;张三(卖方)&quot;); member2 = new Buyer(&quot;李四(买方)&quot;); md.register(member1); //客户注册 md.register(member2); &#125;&#125;//抽象中介者：中介公司interface Medium &#123; void register(Customer member); //客户注册 void relay(String from, String ad); //转发&#125;//具体中介者：房地产中介class EstateMedium implements Medium &#123; private List&lt;Customer&gt; members = new ArrayList&lt;Customer&gt;(); public void register(Customer member) &#123; if (!members.contains(member)) &#123; members.add(member); member.setMedium(this); &#125; &#125; public void relay(String from, String ad) &#123; for (Customer ob : members) &#123; String name = ob.getName(); if (!name.equals(from)) &#123; ((Customer) ob).receive(from, ad); &#125; &#125; &#125;&#125;//抽象同事类：客户abstract class Customer extends JFrame implements ActionListener &#123; private static final long serialVersionUID = -7219939540794786080L; protected Medium medium; protected String name; JTextField SentText; JTextArea ReceiveArea; public Customer(String name) &#123; super(name); this.name = name; &#125; void ClientWindow(int x, int y) &#123; Container cp; JScrollPane sp; JPanel p1, p2; cp = this.getContentPane(); SentText = new JTextField(18); ReceiveArea = new JTextArea(10, 18); ReceiveArea.setEditable(false); p1 = new JPanel(); p1.setBorder(BorderFactory.createTitledBorder(&quot;接收内容：&quot;)); p1.add(ReceiveArea); sp = new JScrollPane(p1); cp.add(sp, BorderLayout.NORTH); p2 = new JPanel(); p2.setBorder(BorderFactory.createTitledBorder(&quot;发送内容：&quot;)); p2.add(SentText); cp.add(p2, BorderLayout.SOUTH); SentText.addActionListener(this); this.setLocation(x, y); this.setSize(250, 330); this.setResizable(false); //窗口大小不可调整 this.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE); this.setVisible(true); &#125; public void actionPerformed(ActionEvent e) &#123; String tempInfo = SentText.getText().trim(); SentText.setText(&quot;&quot;); this.send(tempInfo); &#125; public String getName() &#123; return name; &#125; public void setMedium(Medium medium) &#123; this.medium = medium; &#125; public abstract void send(String ad); public abstract void receive(String from, String ad);&#125;//具体同事类：卖方class Seller extends Customer &#123; private static final long serialVersionUID = -1443076716629516027L; public Seller(String name) &#123; super(name); ClientWindow(50, 100); &#125; public void send(String ad) &#123; ReceiveArea.append(&quot;我(卖方)说: &quot; + ad + &quot;\\n&quot;); //使滚动条滚动到最底端 ReceiveArea.setCaretPosition(ReceiveArea.getText().length()); medium.relay(name, ad); &#125; public void receive(String from, String ad) &#123; ReceiveArea.append(from + &quot;说: &quot; + ad + &quot;\\n&quot;); //使滚动条滚动到最底端 ReceiveArea.setCaretPosition(ReceiveArea.getText().length()); &#125;&#125;//具体同事类：买方class Buyer extends Customer &#123; private static final long serialVersionUID = -474879276076308825L; public Buyer(String name) &#123; super(name); ClientWindow(350, 100); &#125; public void send(String ad) &#123; ReceiveArea.append(&quot;我(买方)说: &quot; + ad + &quot;\\n&quot;); //使滚动条滚动到最底端 ReceiveArea.setCaretPosition(ReceiveArea.getText().length()); medium.relay(name, ad); &#125; public void receive(String from, String ad) &#123; ReceiveArea.append(from + &quot;说: &quot; + ad + &quot;\\n&quot;); //使滚动条滚动到最底端 ReceiveArea.setCaretPosition(ReceiveArea.getText().length()); &#125;&#125; 四，模式的应用场景 当对象之间存在复杂的网状结构关系而导致依赖关系混乱且难以复用时。 当想创建一个运行于多个类之间的对象，又不想生成新的子类时。 五，模式的扩展在实际开发中，通常采用以下两种方法来简化中介者模式，使开发变得更简单。 不定义中介者接口，把具体中介者对象实现成为单例。 同时对象不持有中介者，而是在需要的时候直接获取中介者对象并调用。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081public class SimpleMediatorPattern &#123; public static void main(String[] args) &#123; SimpleColleague c1, c2; c1 = new SimpleConcreteColleague1(); c2 = new SimpleConcreteColleague2(); c1.send(); System.out.println(&quot;-----------------&quot;); c2.send(); &#125;&#125;//简单单例中介者class SimpleMediator &#123; private static SimpleMediator smd = new SimpleMediator(); private List&lt;SimpleColleague&gt; colleagues = new ArrayList&lt;SimpleColleague&gt;(); private SimpleMediator() &#123; &#125; public static SimpleMediator getMedium() &#123; return (smd); &#125; public void register(SimpleColleague colleague) &#123; if (!colleagues.contains(colleague)) &#123; colleagues.add(colleague); &#125; &#125; public void relay(SimpleColleague scl) &#123; for (SimpleColleague ob : colleagues) &#123; if (!ob.equals(scl)) &#123; ((SimpleColleague) ob).receive(); &#125; &#125; &#125;&#125;//抽象同事类interface SimpleColleague &#123; void receive(); void send();&#125;//具体同事类class SimpleConcreteColleague1 implements SimpleColleague &#123; SimpleConcreteColleague1() &#123; SimpleMediator smd = SimpleMediator.getMedium(); smd.register(this); &#125; public void receive() &#123; System.out.println(&quot;具体同事类1：收到请求。&quot;); &#125; public void send() &#123; SimpleMediator smd = SimpleMediator.getMedium(); System.out.println(&quot;具体同事类1：发出请求...&quot;); smd.relay(this); //请中介者转发 &#125;&#125;//具体同事类class SimpleConcreteColleague2 implements SimpleColleague &#123; SimpleConcreteColleague2() &#123; SimpleMediator smd = SimpleMediator.getMedium(); smd.register(this); &#125; public void receive() &#123; System.out.println(&quot;具体同事类2：收到请求。&quot;); &#125; public void send() &#123; SimpleMediator smd = SimpleMediator.getMedium(); System.out.println(&quot;具体同事类2：发出请求...&quot;); smd.relay(this); //请中介者转发 &#125;&#125;","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"迭代器模式","slug":"设计模式/行为型模式之迭代器模式","date":"2022-01-11T11:58:16.449Z","updated":"2022-01-11T12:10:11.981Z","comments":true,"path":"2022/01/11/设计模式/行为型模式之迭代器模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%A1%8C%E4%B8%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E8%BF%AD%E4%BB%A3%E5%99%A8%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"现实生活以及程序设计中，经常要访问一个聚合对象中的各个元素，如“数据结构”中的链表遍历，通常的做法是将链表的创建和遍历都放在同一个类中，但这种方式不利于程序的扩展，如果要更换遍历方法就必须修改程序源代码，这违背了 “开闭原则”。 既然将遍历方法封装在聚合类中不可取，那么聚合类中不提供遍历方法，将遍历方法由用户自己实现是否可行呢？答案是同样不可取，因为这种方式会存在两个缺点： 暴露了聚合类的内部表示，使其数据不安全； 增加了客户的负担。 “迭代器模式”能较好地克服以上缺点，它在客户访问类与聚合类之间插入一个迭代器，这分离了聚合对象与其遍历行为，对客户也隐藏了其内部细节，且满足“单一职责原则”和“开闭原则”，如 [Java] 中的 Collection、List、Set、Map 等都包含了迭代器。 迭代器模式在生活中应用的比较广泛，比如：物流系统中的传送带，不管传送的是什么物品，都会被打包成一个个箱子，并且有一个统一的二维码。这样我们不需要关心箱子里是什么，在分发时只需要一个个检查发送的目的地即可。再比如，我们平时乘坐交通工具，都是统一刷卡或者刷脸进站，而不需要关心是男性还是女性、是残疾人还是正常人等信息。 一，模式的定义与特点迭代器（Iterator）模式的定义：提供一个对象来顺序访问聚合对象中的一系列数据，而不暴露聚合对象的内部表示。迭代器模式是一种对象行为型模式，其主要优点如下。 访问一个聚合对象的内容而无须暴露它的内部表示。 遍历任务交由迭代器完成，这简化了聚合类。 它支持以不同方式遍历一个聚合，甚至可以自定义迭代器的子类以支持新的遍历。 增加新的聚合类和迭代器类都很方便，无须修改原有代码。 封装性良好，为遍历不同的聚合结构提供一个统一的接口。 其主要缺点是：增加了类的个数，这在一定程度上增加了系统的复杂性。 在日常开发中，我们几乎不会自己写迭代器。除非需要定制一个自己实现的数据结构对应的迭代器，否则，开源框架提供的 API 完全够用。 二，模式的结构与实现迭代器模式是通过将聚合对象的遍历行为分离出来，抽象成迭代器类来实现的，其目的是在不暴露聚合对象的内部结构的情况下，让外部代码透明地访问聚合的内部数据。 1.模式的结构迭代器模式主要包含以下角色。 抽象聚合（Aggregate）角色：定义存储、添加、删除聚合对象以及创建迭代器对象的接口。 具体聚合（ConcreteAggregate）角色：实现抽象聚合类，返回一个具体迭代器的实例。 抽象迭代器（Iterator）角色：定义访问和遍历聚合元素的接口，通常包含 hasNext()、first()、next() 等方法。 具体迭代器（Concretelterator）角色：实现抽象迭代器接口中所定义的方法，完成对聚合对象的遍历，记录遍历的当前位置。 2.模式的实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869public class IteratorPattern &#123; public static void main(String[] args) &#123; Aggregate ag = new ConcreteAggregate(); ag.add(&quot;中山大学&quot;); ag.add(&quot;华南理工&quot;); ag.add(&quot;韶关学院&quot;); System.out.print(&quot;聚合的内容有：&quot;); Iterator it = ag.getIterator(); while (it.hasNext()) &#123; Object ob = it.next(); System.out.print(ob.toString() + &quot;\\t&quot;); &#125; Object ob = it.first(); System.out.println(&quot;\\nFirst：&quot; + ob.toString()); &#125;&#125;//抽象聚合interface Aggregate &#123; public void add(Object obj); public void remove(Object obj); public Iterator getIterator();&#125;//具体聚合class ConcreteAggregate implements Aggregate &#123; private List&lt;Object&gt; list = new ArrayList&lt;Object&gt;(); public void add(Object obj) &#123; list.add(obj); &#125; public void remove(Object obj) &#123; list.remove(obj); &#125; public Iterator getIterator() &#123; return (new ConcreteIterator(list)); &#125;&#125;//抽象迭代器interface Iterator &#123; Object first(); Object next(); boolean hasNext();&#125;//具体迭代器class ConcreteIterator implements Iterator &#123; private List&lt;Object&gt; list = null; private int index = -1; public ConcreteIterator(List&lt;Object&gt; list) &#123; this.list = list; &#125; public boolean hasNext() &#123; if (index &lt; list.size() - 1) &#123; return true; &#125; else &#123; return false; &#125; &#125; public Object first() &#123; index = 0; Object obj = list.get(index); ; return obj; &#125; public Object next() &#123; Object obj = null; if (this.hasNext()) &#123; obj = list.get(++index); &#125; return obj; &#125;&#125; 三，模式的应用场景前面介绍了关于迭代器模式的结构与特点，下面介绍其应用场景，迭代器模式通常在以下几种情况使用。 当需要为聚合对象提供多种遍历方式时。 当需要为遍历不同的聚合结构提供一个统一的接口时。 当访问一个聚合对象的内容而无须暴露其内部细节的表示时。 由于聚合与迭代器的关系非常密切，所以大多数语言在实现聚合类时都提供了迭代器类，因此大数情况下使用语言中已有的聚合类的迭代器就已经够了。 四，模式的扩展迭代器模式常常与[组合模式]结合起来使用，在对组合模式中的容器构件进行访问时，经常将迭代器潜藏在组合模式的容器构成类中。当然，也可以构造一个外部迭代器来对容器构件进行访问。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"访问者模式","slug":"设计模式/行为型模式之访问者模式","date":"2022-01-11T11:58:06.865Z","updated":"2022-01-11T12:10:20.708Z","comments":true,"path":"2022/01/11/设计模式/行为型模式之访问者模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%A1%8C%E4%B8%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E8%AE%BF%E9%97%AE%E8%80%85%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"在现实生活中，有些集合对象存在多种不同的元素，且每种元素也存在多种不同的访问者和处理方式。例如，公园中存在多个景点，也存在多个游客，不同的游客对同一个景点的评价可能不同；医院医生开的处方单中包含多种药元素，査看它的划价员和药房工作人员对它的处理方式也不同，划价员根据处方单上面的药品名和数量进行划价，药房工作人员根据处方单的内容进行抓药。 这样的例子还有很多，例如，电影或电视剧中的人物角色，不同的观众对他们的评价也不同；还有顾客在商场购物时放在“购物车”中的商品，顾客主要关心所选商品的性价比，而收银员关心的是商品的价格和数量。 这些被处理的数据元素相对稳定而访问方式多种多样的[数据结构]，如果用“访问者模式”来处理比较方便。访问者模式能把处理方法从数据结构中分离出来，并可以根据需要增加新的处理方法，且不用修改原来的程序代码与数据结构，这提高了程序的扩展性和灵活性。 一，模式的定义与特点访问者（Visitor）模式的定义：将作用于某种数据结构中的各元素的操作分离出来封装成独立的类，使其在不改变数据结构的前提下可以添加作用于这些元素的新的操作，为数据结构中的每个元素提供多种访问方式。它将对数据的操作与数据结构进行分离，是行为类模式中最复杂的一种模式。 访问者（Visitor）模式是一种对象行为型模式，其主要优点如下。 扩展性好。能够在不修改对象结构中的元素的情况下，为对象结构中的元素添加新的功能。 复用性好。可以通过访问者来定义整个对象结构通用的功能，从而提高系统的复用程度。 灵活性好。访问者模式将数据结构与作用于结构上的操作解耦，使得操作集合可相对自由地演化而不影响系统的数据结构。 符合单一职责原则。访问者模式把相关的行为封装在一起，构成一个访问者，使每一个访问者的功能都比较单一。 访问者（Visitor）模式的主要缺点如下。 增加新的元素类很困难。在访问者模式中，每增加一个新的元素类，都要在每一个具体访问者类中增加相应的具体操作，这违背了“开闭原则”。 破坏封装。访问者模式中具体元素对访问者公布细节，这破坏了对象的封装性。 违反了依赖倒置原则。访问者模式依赖了具体类，而没有依赖抽象类。 二，模式的结构与实现访问者（Visitor）模式实现的关键是如何将作用于元素的操作分离出来封装成独立的类。 1.模式的结构 2.模式的实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273public class VisitorPattern &#123; public static void main(String[] args) &#123; ObjectStructure os = new ObjectStructure(); os.add(new ConcreteElementA()); os.add(new ConcreteElementB()); Visitor visitor = new ConcreteVisitorA(); os.accept(visitor); System.out.println(&quot;------------------------&quot;); visitor = new ConcreteVisitorB(); os.accept(visitor); &#125;&#125;//抽象访问者interface Visitor &#123; void visit(ConcreteElementA element); void visit(ConcreteElementB element);&#125;//具体访问者A类class ConcreteVisitorA implements Visitor &#123; public void visit(ConcreteElementA element) &#123; System.out.println(&quot;具体访问者A访问--&gt;&quot; + element.operationA()); &#125; public void visit(ConcreteElementB element) &#123; System.out.println(&quot;具体访问者A访问--&gt;&quot; + element.operationB()); &#125;&#125;//具体访问者B类class ConcreteVisitorB implements Visitor &#123; public void visit(ConcreteElementA element) &#123; System.out.println(&quot;具体访问者B访问--&gt;&quot; + element.operationA()); &#125; public void visit(ConcreteElementB element) &#123; System.out.println(&quot;具体访问者B访问--&gt;&quot; + element.operationB()); &#125;&#125;//抽象元素类interface Element &#123; void accept(Visitor visitor);&#125;//具体元素A类class ConcreteElementA implements Element &#123; public void accept(Visitor visitor) &#123; visitor.visit(this); &#125; public String operationA() &#123; return &quot;具体元素A的操作。&quot;; &#125;&#125;//具体元素B类class ConcreteElementB implements Element &#123; public void accept(Visitor visitor) &#123; visitor.visit(this); &#125; public String operationB() &#123; return &quot;具体元素B的操作。&quot;; &#125;&#125;//对象结构角色class ObjectStructure &#123; private List&lt;Element&gt; list = new ArrayList&lt;Element&gt;(); public void accept(Visitor visitor) &#123; Iterator&lt;Element&gt; i = list.iterator(); while (i.hasNext()) &#123; ((Element) i.next()).accept(visitor); &#125; &#125; public void add(Element element) &#123; list.add(element); &#125; public void remove(Element element) &#123; list.remove(element); &#125;&#125; 三，模式的应用场景当系统中存在类型数量稳定（固定）的一类数据结构时，可以使用访问者模式方便地实现对该类型所有数据结构的不同操作，而又不会对数据产生任何副作用（脏数据）。 简而言之，就是当对集合中的不同类型数据（类型数量稳定）进行多种操作时，使用访问者模式。 通常在以下情况可以考虑使用访问者（Visitor）模式。 对象结构相对稳定，但其操作算法经常变化的程序。 对象结构中的对象需要提供多种不同且不相关的操作，而且要避免让这些操作的变化影响对象的结构。 对象结构包含很多类型的对象，希望对这些对象实施一些依赖于其具体类型的操作。 四，模式的扩展访问者（Visitor）模式是使用频率较高的一种[设计模式]，它常常同以下两种设计模式联用。 (1)与“[迭代器模式]”联用。因为访问者模式中的“对象结构”是一个包含元素角色的容器，当访问者遍历容器中的所有元素时，常常要用迭代器。 (2)访问者（Visitor）模式同“[组合模式]”联用。因为访问者（Visitor）模式中的“元素对象”可能是叶子对象或者是容器对象，如果元素对象包含容器对象，就必须用到[组合模式]。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"备忘录模式","slug":"设计模式/行为型模式之备忘录模式","date":"2022-01-11T11:57:59.752Z","updated":"2022-01-11T12:09:22.827Z","comments":true,"path":"2022/01/11/设计模式/行为型模式之备忘录模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%A1%8C%E4%B8%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%A4%87%E5%BF%98%E5%BD%95%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"一，模式的定义与特点在不破坏封装性的前提下，捕获一个对象的内部状态，并在该对象之外保存这个状态，以便以后当需要时能将该对象恢复到原先保存的状态。该模式又叫快照模式。 备忘录模式是一种对象行为型模式，其主要优点如下。 提供了一种可以恢复状态的机制。当用户需要时能够比较方便地将数据恢复到某个历史的状态。 实现了内部状态的封装。除了创建它的发起人之外，其他对象都不能够访问这些状态信息。 简化了发起人类。发起人不需要管理和保存其内部状态的各个备份，所有状态信息都保存在备忘录中，并由管理者进行管理，这符合单一职责原则。 其主要缺点是：资源消耗大。如果要保存的内部状态信息过多或者特别频繁，将会占用比较大的内存资源。 二，模式的结构与实现备忘录模式的核心是设计备忘录类以及用于管理备忘录的管理者类。 1.模式的结构备忘录模式的主要角色如下。 发起人（Originator）角色：记录当前时刻的内部状态信息，提供创建备忘录和恢复备忘录数据的功能，实现其他业务功能，它可以访问备忘录里的所有信息。 备忘录（Memento）角色：负责存储发起人的内部状态，在需要的时候提供这些内部状态给发起人。 管理者（Caretaker）角色：对备忘录进行管理，提供保存与获取备忘录的功能，但其不能对备忘录的内容进行访问与修改。 2.模式的实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class MementoPattern &#123; public static void main(String[] args) &#123; Originator or = new Originator(); Caretaker cr = new Caretaker(); or.setState(&quot;S0&quot;); System.out.println(&quot;初始状态:&quot; + or.getState()); cr.setMemento(or.createMemento()); //保存状态 or.setState(&quot;S1&quot;); System.out.println(&quot;新的状态:&quot; + or.getState()); or.restoreMemento(cr.getMemento()); //恢复状态 System.out.println(&quot;恢复状态:&quot; + or.getState()); &#125;&#125;//备忘录class Memento &#123; private String state; public Memento(String state) &#123; this.state = state; &#125; public void setState(String state) &#123; this.state = state; &#125; public String getState() &#123; return state; &#125;&#125;//发起人class Originator &#123; private String state; public void setState(String state) &#123; this.state = state; &#125; public String getState() &#123; return state; &#125; public Memento createMemento() &#123; return new Memento(state); &#125; public void restoreMemento(Memento m) &#123; this.setState(m.getState()); &#125;&#125;//管理者class Caretaker &#123; private Memento memento; public void setMemento(Memento m) &#123; memento = m; &#125; public Memento getMemento() &#123; return memento; &#125;&#125; 三，模式的应用场景 需要保存与恢复数据的场景，如玩游戏时的中间结果的存档功能。 需要提供一个可回滚操作的场景，如 Word、记事本、Photoshop，Eclipse 等软件在编辑时按 Ctrl+Z 组合键，还有数据库中事务操作。 四，模式的扩展在备忘录模式中，通过定义“备忘录”来备份“发起人”的信息，而原型模式的 clone() 方法具有自备份功能，所以，如果让发起人实现 Cloneable 接口就有备份自己的功能，这时可以删除备忘录类。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class PrototypeMemento &#123; public static void main(String[] args) &#123; OriginatorPrototype or = new OriginatorPrototype(); PrototypeCaretaker cr = new PrototypeCaretaker(); or.setState(&quot;S0&quot;); System.out.println(&quot;初始状态:&quot; + or.getState()); cr.setMemento(or.createMemento()); //保存状态 or.setState(&quot;S1&quot;); System.out.println(&quot;新的状态:&quot; + or.getState()); or.restoreMemento(cr.getMemento()); //恢复状态 System.out.println(&quot;恢复状态:&quot; + or.getState()); &#125;&#125;//发起人原型class OriginatorPrototype implements Cloneable &#123; private String state; public void setState(String state) &#123; this.state = state; &#125; public String getState() &#123; return state; &#125; public OriginatorPrototype createMemento() &#123; return this.clone(); &#125; public void restoreMemento(OriginatorPrototype opt) &#123; this.setState(opt.getState()); &#125; public OriginatorPrototype clone() &#123; try &#123; return (OriginatorPrototype) super.clone(); &#125; catch (CloneNotSupportedException e) &#123; e.printStackTrace(); &#125; return null; &#125;&#125;//原型管理者class PrototypeCaretaker &#123; private OriginatorPrototype opt; public void setMemento(OriginatorPrototype opt) &#123; this.opt = opt; &#125; public OriginatorPrototype getMemento() &#123; return opt; &#125;&#125;","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"解释器模式","slug":"设计模式/行为型模式之解释器模式","date":"2022-01-11T11:57:49.640Z","updated":"2022-01-11T12:10:40.428Z","comments":true,"path":"2022/01/11/设计模式/行为型模式之解释器模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%A1%8C%E4%B8%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E8%A7%A3%E9%87%8A%E5%99%A8%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"在软件开发中，会遇到有些问题多次重复出现，而且有一定的相似性和规律性。如果将它们归纳成一种简单的语言，那么这些问题实例将是该语言的一些句子，这样就可以用“编译原理”中的解释器模式来实现了。 一，模式的定义与特点解释器（Interpreter）模式的定义：给分析对象定义一个语言，并定义该语言的文法表示，再设计一个解析器来解释语言中的句子。也就是说，用编译语言的方式来分析应用中的实例。这种模式实现了文法表达式处理的接口，该接口解释一个特定的上下文。 这里提到的文法和句子的概念同编译原理中的描述相同，“文法”指语言的语法规则，而“句子”是语言集中的元素。例如，汉语中的句子有很多，“我是中国人”是其中的一个句子，可以用一棵语法树来直观地描述语言中的句子。 解释器模式是一种类行为型模式，其主要优点如下。 扩展性好。由于在解释器模式中使用类来表示语言的文法规则，因此可以通过继承等机制来改变或扩展文法。 容易实现。在语法树中的每个表达式节点类都是相似的，所以实现其文法较为容易。 解释器模式的主要缺点如下。 执行效率较低。解释器模式中通常使用大量的循环和递归调用，当要解释的句子较复杂时，其运行速度很慢，且代码的调试过程也比较麻烦。 会引起类膨胀。解释器模式中的每条规则至少需要定义一个类，当包含的文法规则很多时，类的个数将急剧增加，导致系统难以管理与维护。 可应用的场景比较少。在软件开发中，需要定义语言文法的应用实例非常少，所以这种模式很少被使用到。 二，模式的结构与实现解释器模式常用于对简单语言的编译或分析实例中，为了掌握好它的结构与实现，必须先了解编译原理中的“文法、句子、语法树”等相关概念。 文法 文法是用于描述语言的语法结构的形式规则。没有规矩不成方圆，例如，有些人认为完美爱情的准则是“相互吸引、感情专一、任何一方都没有恋爱经历”，虽然最后一条准则较苛刻，但任何事情都要有规则，语言也一样，不管它是机器语言还是自然语言，都有它自己的文法规则。例如，中文中的“句子”的文法如下。 1234567〈句子〉::=〈主语〉〈谓语〉〈宾语〉〈主语〉::=〈代词〉|〈名词〉〈谓语〉::=〈动词〉〈宾语〉::=〈代词〉|〈名词〉〈代词〉你|我|他〈名词〉7大学生I筱霞I英语〈动词〉::=是|学习 注：这里的符号“::=”表示“定义为”的意思，用“〈”和“〉”括住的是非终结符，没有括住的是终结符。 句子 句子是语言的基本单位，是语言集中的一个元素，它由终结符构成，能由“文法”推导出。例如，上述文法可以推出“我是大学生”，所以它是句子。 语法树 语法树是句子结构的一种树型表示，它代表了句子的推导结果，它有利于理解句子语法结构的层次。图 1 所示是“我是大学生”的语法树。 有了以上基础知识，现在来介绍解释器模式的结构就简单了。解释器模式的结构与组合模式相似，不过其包含的组成元素比组合模式多，而且组合模式是对象结构型模式，而解释器模式是类行为型模式。 1.模式的结构解释器模式包含以下主要角色。 抽象表达式（Abstract Expression）角色：定义解释器的接口，约定解释器的解释操作，主要包含解释方法 interpret()。 终结符表达式（Terminal Expression）角色：是抽象表达式的子类，用来实现文法中与终结符相关的操作，文法中的每一个终结符都有一个具体终结表达式与之相对应。 非终结符表达式（Nonterminal Expression）角色：也是抽象表达式的子类，用来实现文法中与非终结符相关的操作，文法中的每条规则都对应于一个非终结符表达式。 环境（Context）角色：通常包含各个解释器需要的数据或是公共的功能，一般用来传递被所有解释器共享的数据，后面的解释器可以从这里获取这些值。 客户端（Client）：主要任务是将需要分析的句子或表达式转换成使用解释器对象描述的抽象语法树，然后调用解释器的解释方法，当然也可以通过环境角色间接访问解释器的解释方法。 2.模式的实现解释器模式实现的关键是定义文法规则、设计终结符类与非终结符类、画出结构图，必要时构建语法树。 12345678910111213141516171819202122232425262728//抽象表达式类interface AbstractExpression &#123; public void interpret(String info); //解释方法&#125;//终结符表达式类class TerminalExpression implements AbstractExpression &#123; public void interpret(String info) &#123; //对终结符表达式的处理 &#125;&#125;//非终结符表达式类class NonterminalExpression implements AbstractExpression &#123; private AbstractExpression exp1; private AbstractExpression exp2; public void interpret(String info) &#123; //非对终结符表达式的处理 &#125;&#125;//环境类class Context &#123; private AbstractExpression exp; public Context() &#123; //数据初始化 &#125; public void operation(String info) &#123; //调用相关表达式类的解释方法 &#125;&#125; 三，模式的应用场景前面介绍了解释器模式的结构与特点，下面分析它的应用场景。 当语言的文法较为简单，且执行效率不是关键问题时。 当问题重复出现，且可以用一种简单的语言来进行表达时。 当一个语言需要解释执行，并且语言中的句子可以表示为一个抽象语法树的时候，如 XML 文档解释。 注意：解释器模式在实际的软件开发中使用比较少，因为它会引起效率、性能以及维护等问题。如果碰到对表达式的解释，在 Java 中可以用 Expression4J 或 Jep 等来设计。 四，模式的扩展在项目开发中，如果要对数据表达式进行分析与计算，无须再用解释器模式进行设计了，Java 提供了以下强大的数学公式解析器：Expression4J、MESP(Math Expression String Parser) 和 Jep 等，它们可以解释一些复杂的文法，功能强大，使用简单。 现在以 Jep 为例来介绍该工具包的使用方法。Jep 是 Java expression parser 的简称，即 Java 表达式分析器，它是一个用来转换和计算数学表达式的 Java 库。通过这个程序库，用户可以以字符串的形式输入一个任意的公式，然后快速地计算出其结果。而且 Jep 支持用户自定义变量、常量和函数，它包括许多常用的数学函数和常量。 使用前先下载 Jep 压缩包，解压后，将 jep-x.x.x.jar 文件移到选择的目录中，在 Eclipse 的“Java 构建路径”对话框的“库”选项卡中选择“添加外部 JAR(X)…”，将该 Jep 包添加项目中后即可使用其中的类库。 下面以计算存款利息为例来介绍。存款利息的计算公式是：本金x利率x时间=利息，其相关代码如下： 123456789101112131415import com.singularsys.jep.*;public class JepDemo &#123; public static void main(String[] args) throws JepException &#123; Jep jep = new Jep(); //定义要计算的数据表达式 String 存款利息 = &quot;本金*利率*时间&quot;; //给相关变量赋值 jep.addVariable(&quot;本金&quot;, 10000); jep.addVariable(&quot;利率&quot;, 0.038); jep.addVariable(&quot;时间&quot;, 2); jep.parse(存款利息); //解析表达式 Object accrual = jep.evaluate(); //计算 System.out.println(&quot;存款利息：&quot; + accrual); &#125;&#125;","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"ClickHouse","slug":"clickhouse/clickhouse","date":"2022-01-11T11:50:20.738Z","updated":"2022-01-11T11:55:06.481Z","comments":true,"path":"2022/01/11/clickhouse/clickhouse/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/clickhouse/clickhouse/","excerpt":"","text":"一，clickhouse特点1.列式存储行存储 好处是想查某个人所有的属性时，可以通过一次磁盘查找加顺序读取就可以。但是当想查所有人的年龄时，需要不停的查找，或者全表扫描才行，遍历的很多数据都是不需要的。 id 姓名 年龄 1 张三 18 2 李四 19 3 王五 20 列存储 列存储的好处 1 对于列的聚合，计数，求和等统计操作要优于行式存储。 2 由于某一列的数据类型都是相同的，针对于数据存储更容易进行数据压缩，每一列选择更优的数据压缩算法，大大提高了数据的压缩比重。 3 由于数据压缩比更好，一方面节省了磁盘空间，另一方面对于cache也有了更大的发挥空间。 id 1 2 3 姓名 张三 李四 王五 年龄 18 19 20 2.DBMS的功能 几乎覆盖了标准SQL的大部分语法，包括 DDL和 DML ,以及配套的各种函数。 用户管理及权限管理 数据的备份与恢复 3.多样化引擎clickhouse和mysql类似，把表级的存储引擎插件化，根据表的不同需求可以设定不同的存储引擎。目前包括合并树、日志、接口和其他四大类20多种引擎。 4.高吞吐写入能力ClickHouse采用类LSM Tree的结构，数据写入后定期在后台Compaction。通过类LSM tree的结构，ClickHouse在数据导入时全部是顺序append写，写入后数据段不可更改，在后台compaction时也是多个段merge sort后顺序写回磁盘。顺序写的特性，充分利用了磁盘的吞吐能力，即便在HDD上也有着优异的写入性能。 官方公开benchmark测试显示能够达到50MB-200MB/s的写入吞吐能力，按照每行100Byte估算，大约相当于50W-200W条/s的写入速度。 5.数据分区与线程级并行ClickHouse将数据划分为多个partition，每个partition再进一步划分为多个index granularity，然后通过多个CPU核心分别处理其中的一部分来实现并行数据处理。 在这种设计下，单条Query就能利用整机所有CPU。极致的并行处理能力，极大的降低了查询延时。 所以，clickhouse即使对于大量数据的查询也能够化整为零平行处理。但是有一个弊端就是对于单条查询使用多cpu，就不利于同时并发多条查询。所以对于高qps的查询业务，clickhouse并不是强项。 6.关联查询clickhouse像很多OLAP数据库一样，单表查询速度由于关联查询，而且clickhouse的两者差距更为明显。 二，clickhouse安装1.取消打开文件数限制在/etc/security/limits.conf、/etc/security/limits.d/90-nproc.conf这2个文件的末尾加入以下内容： 1234* soft nofile 65536 * hard nofile 65536 * soft nproc 131072 * hard nproc 131072 2.取消SELINUX修改/etc/selinux/config中的SELINUX=disabled后重启。 3.开放端口HTTP：8123 TCP：9000 4.安装123456yum install -y libtoolyum install -y *unixODBC*yum install yum-utilsrpm --import https://repo.clickhouse.tech/CLICKHOUSE-KEY.GPGyum-config-manager --add-repo https://repo.clickhouse.tech/rpm/clickhouse.repoyum install clickhouse-server clickhouse-client 5.修改配置文件1vim /etc/clickhouse-server/config.xml 把 &lt;listen_host&gt;::&lt;/listen_host&gt;的注解打开，这样的话才能让clickhouse被除本机以外的服务器访问。 6.启动ClickServer1systemctl start clickhouse-server 7.使用client连接server1clickhouse-client -m 三，数据类型1.整型固定长度的整型，包括有符号整型或无符号整型。 12345678910整型范围（-2n-1~2n-1-1）：Int8 - [-128 : 127]Int16 - [-32768 : 32767]Int32 - [-2147483648 : 2147483647]Int64 - [-9223372036854775808 : 9223372036854775807]无符号整型范围（0~2n-1）：UInt8 - [0 : 255]UInt16 - [0 : 65535]UInt32 - [0 : 4294967295]UInt64 - [0 : 18446744073709551615] 适用场景：个数，数量，存储id等。 2.浮点型12Float32 - floatFloat64 – double 建议尽可能以整数形式存储数据。例如，将固定精度的数字转换为整数值，如时间用毫秒为单位表示，因为浮点型进行计算时可能引起四舍五入的误差。 适用场景：一般数据值比较小，不涉及大量的统计计算，精度要求不高的时候。比如保存商品的重量。 3.布尔型没有单独的类型来存储布尔值。可以使用 UInt8 类型，取值限制为 0 或 1。 4.Decimal型有符号的浮点数，可在加、减和乘法运算过程中保持精度。对于除法，最低有效数字会被丢弃（不舍入）。 有三种声明： 123Decimal32(s)，相当于Decimal(9-s,s)Decimal64(s)，相当于Decimal(18-s,s)Decimal128(s)，相当于Decimal(38-s,s) 适用场景：一般金额字段，汇率，利率等字段为了保证小数点精度，都是用Decimal进行存储。 5.字符串1）String字符串可以任意长度的。它可以包含任意的字节集，包含空字节。 2）FixedString(N)固定长度 N 的字符串，N 必须是严格的正自然数。当服务端读取长度小于 N 的字符串时候，通过在字符串末尾添加空字节来达到 N 字节长度。 当服务端读取长度大于 N 的字符串时候，将返回错误消息。 与String相比，极少会使用FixedString，因为使用起来不是很方便。 适用场景：名称，文字描述，字符型编码。固定长度的可以保存一些定长的内容，比如一些编码，性别等，但是考虑到一定的变化风险，带来收益不够明显，所以定长字符串使用意义有限。 6.枚举类型包括 Enum8 和 Enum16 类型。Enum 保存&#39;string&#39;= integer 的对应关系。 Enum8 用 &#39;String&#39;= Int8 对描述。 Enum16 用&#39;String&#39;= Int16对描述。 用法演示： 创建一个带有一个枚举 Enum8(&#39;hello&#39; = 1, &#39;world&#39; = 2) 类型的列： 12345CREATE TABLE t_enum( x Enum8(&#x27;hello&#x27; = 1, &#x27;world&#x27; = 2))ENGINE = TinyLog 这个 x 列只能存储类型定义中列出的值：&#39;hello&#39;或&#39;world&#39;。如果尝试保存任何其他值，ClickHouse 抛出异常。 从表中查询数据时，ClickHouse 从 Enum 中输出字符串值。 1234567SELECT * FROM t_enum┌─x─────┐│ hello ││ world ││ hello │└───────┘ 如果需要看到对应行的数值，则必须将 Enum 值转换为整数类型。 1234567SELECT CAST(x, &#x27;Int8&#x27;) FROM t_enum┌─CAST(x, &#x27;Int8&#x27;)────┐│ 1 ││ 2 ││ 1 │└────────────────────┘ 适用场景：对于一些状态，类型的字段算是一种空间优化，也算是一种数据约束。但是实际使用中往往因为一些数据内容的变化增加一定的维护成本，甚至是数据丢失的问题。所以谨慎使用。 7.时间类型目前clickhouse 有三种时间类型 Date 接受 年-月-日 的字符串比如 ‘2019-12-16’ Datetime 接受 年-月-日 时:分:秒 的字符串比如 ‘2019-12-16 20:50:10’ Datetime64 接受 年-月-日 时:分:秒.亚秒 的字符串比如 ‘2019-12-16 20:50:10.66’ 日期类型，用两个字节存储，表示从 1970-01-01 (无符号) 到当前的日期值。 还有很多数据结构，可以参考官方文档：https://clickhouse.yandex/docs/zh/data_types/ 8.数组**Array(T)**：由 T 类型元素组成的数组。 T 可以是任意类型，包含数组类型。 但不推荐使用多维数组，ClickHouse 对多维数组的支持有限。例如，不能在 MergeTree 表中存储多维数组。 可以使用array函数来创建数组，也可以使用方括号：[]。 创建数组案例： 1234567891011121314151617181920SELECT array(1, 2) AS x, toTypeName(x)SELECT [1, 2] AS x, toTypeName(x)┌─x─────┬─toTypeName(array(1, 2))─┐│ [1,2] │ Array(UInt8) │└───────┴─────────────────────────┘1 rows in set. Elapsed: 0.002 sec.:) SELECT [1, 2] AS x, toTypeName(x)┌─x─────┬─toTypeName([1, 2])─┐│ [1,2] │ Array(UInt8) │└───────┴────────────────────┘1 rows in set. Elapsed: 0.002 sec. 四，表引擎1.表引擎的使用表引擎是clickhouse的一大特色。可以说， 表引擎决定了如何存储表的数据。包括： 1）数据的存储方式和位置，写到哪里以及从哪里读取数据。 2）支持哪些查询以及如何支持。 3）并发数据访问。 4）索引的使用（如果存在）。 5）是否可以执行多线程请求。 6）数据复制参数。 表引擎的使用方式就是必须显形在创建表时定义该表使用的引擎，以及引擎使用的相关参数。如： 1create table t_tinylog ( id String, name String) engine=TinyLog; 引擎的名称大小写敏感。 2.TinyLog以列文件的形式保存在磁盘上，不支持索引，没有并发控制。一般保存少量数据的小表，生产环境上作用有限。可以用于平时练习测试用。 3.Memory内存引擎，数据以未压缩的原始形式直接保存在内存当中，服务器重启数据就会消失。读写操作不会相互阻塞，不支持索引。简单查询下有非常非常高的性能表现（超过10G/s）。 一般用到它的地方不多，除了用来测试，就是在需要非常高的性能，同时数据量又不太大（上限大概 1 亿行）的场景。 4.MergeTreeClickhouse 中最强大的表引擎当属 MergeTree （合并树）引擎及该系列（MergeTree）中的其他引擎。地位可以相当于innodb之于Mysql。 而且基于MergeTree，还衍生出了很多小弟，也是非常有特色的引擎。 建表语句 1234567891011121314151617create table t_order_mt( id UInt32, sku_id String, total_amount Decimal(16,2), create_time Datetime ) engine =MergeTree partition by toYYYYMMDD(create_time) primary key (id) order by (id,sku_id) insert into t_order_mtvalues(101,&#x27;sku_001&#x27;,1000.00,&#x27;2020-06-01 12:00:00&#x27;) ,(102,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 11:00:00&#x27;),(102,&#x27;sku_004&#x27;,2500.00,&#x27;2020-06-01 12:00:00&#x27;),(102,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 13:00:00&#x27;)(102,&#x27;sku_002&#x27;,12000.00,&#x27;2020-06-01 13:00:00&#x27;)(102,&#x27;sku_002&#x27;,600.00,&#x27;2020-06-02 12:00:00&#x27;) MergeTree其实还有很多参数(绝大多数用默认值即可)，但是三个参数是更加重要的，也涉及了关于MergeTree的很多概念。 1）partition by 分区（可选项）作用： 学过hive的应该都不陌生，分区的目的主要是降低扫描的范围，优化查询速度。 如果不填： 只会使用一个分区。 分区目录： MergeTree 是以列文件+索引文件+表定义文件组成的，但是如果设定了分区那么这些文件就会保存到不同的分区目录中。 并行：分区后，面对涉及跨分区的查询统计，clickhouse会以分区为单位并行处理。 数据写入与分区合并： 任何一个批次的数据写入都会产生一个临时分区，不会纳入任何一个已有的分区。写入后的某个时刻（大概10-15分钟后），clickhouse会自动执行合并操作（等不及也可以手动通过optimize执行），把临时分区的数据，合并到已有分区中。 1optimize table xxxx [final] 2) primary key主键(可选)clickhouse中的主键，和其他数据库不太一样，它只提供了数据的一级索引，但是却不是唯一约束。这就意味着是可以存在相同primary key的数据的。 主键的设定主要依据是查询语句中的 where 条件。 根据条件通过对主键进行某种形式的二分查找，能够定位到对应的index granularity,避免了全表扫描。 index granularity： 直接翻译的话就是索引粒度，指在稀疏索引中两个相邻索引对应数据的间隔。clickhouse中的MergeTree默认是8192。官方不建议修改这个值，除非该列存在大量重复值，比如在一个分区中几万行才有一个不同数据。 稀疏索引： 稀疏索引的好处就是可以用很少的索引数据，定位更多的数据，代价就是只能定位到索引粒度的第一行，然后再进行进行一点扫描。 3）order by(必选)order by 设定了分区内的数据按照哪些字段顺序进行有序保存。 order by是MergeTree中唯一一个必填项，甚至比primary key 还重要，因为当用户不设置主键的情况，很多处理会依照order by的字段进行处理（比如去重和汇总）。 要求：主键必须是order by字段的前缀字段。 比如order by 字段是 (id,sku_id) 那么主键必须是id 或者(id,sku_id) 4)二级索引目前在clickhouse的官网上二级索引的功能是被标注为实验性的。 所以使用二级索引前需要增加设置。 1set allow_experimental_data_skipping_indices=1; 12345678910create table t_order_mt2( id UInt32, sku_id String, total_amount Decimal(16,2), create_time Datetime, INDEX a total_amount TYPE minmax GRANULARITY 5 ) engine =MergeTree partition by toYYYYMMDD(create_time) primary key (id) order by (id, sku_id) 其中GRANULARITY N 是设定二级索引对于一级索引粒度的粒度。 那么在使用下面语句进行测试，可以看出二级索引能够为非主键字段的查询发挥作用。 1&#x27;select * from test1.t_order_mt where total_amount &gt; toDecimal32(900., 2)&#x27; 5)数据TTLTTL即Time To Live，MergeTree提供了可以管理数据或者列的生命周期的功能。 ①列级别TTL123456789 create table t_order_mt3( id UInt32, sku_id String, total_amount Decimal(16,2) TTL create_time+interval 10 SECOND, create_time Datetime ) engine =MergeTreepartition by toYYYYMMDD(create_time) primary key (id) order by (id, sku_id) 插入数据 1234insert into t_order_mt3values(106,&#x27;sku_001&#x27;,1000.00,&#x27;2020-06-12 22:52:30&#x27;) ,(107,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-12 22:52:30&#x27;),(110,&#x27;sku_003&#x27;,600.00,&#x27;2020-06-13 12:00:00&#x27;) ②表级TTL针对整张表，下面的这条语句是数据会在create_time之后10秒丢失。 1alter table t_order_mt3 MODIFY TTL create_time + INTERVAL 10 SECOND; 涉及判断的字段必须是Date或者Datetime类型，推荐使用分区的日期字段。 能够使用的时间周期： 12345678- SECOND- MINUTE- HOUR- DAY- WEEK- MONTH- QUARTER- YEAR 5.ReplacingMergeTreeReplacingMergeTree是MergeTree的一个变种，它存储特性完全继承MergeTree，只是多了一个去重的功能。 尽管MergeTree可以设置主键，但是primary key其实没有唯一约束的功能。如果你想处理掉重复的数据，可以借助这个ReplacingMergeTree。 去重时机：数据的去重只会在合并的过程中出现。合并会在未知的时间在后台进行，所以你无法预先作出计划。有一些数据可能仍未被处理。 去重范围：如果表经过了分区，去重只会在分区内部进行去重，不能执行跨分区的去重。 所以ReplacingMergeTree能力有限， ReplacingMergeTree 适用于在后台清除重复的数据以节省空间，但是它不保证没有重复的数据出现。 123456789 create table t_order_rmt( id UInt32, sku_id String, total_amount Decimal(16,2) , create_time Datetime ) engine =ReplacingMergeTree(create_time)partition by toYYYYMMDD(create_time) primary key (id) order by (id, sku_id) ReplacingMergeTree()填入的参数为版本字段，重复数据保留版本字段值最大的。 如果不填版本字段，默认保留最后一条。 1234567insert into t_order_rmtvalues(101,&#x27;sku_001&#x27;,1000.00,&#x27;2020-06-01 12:00:00&#x27;) ,(102,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 11:00:00&#x27;),(102,&#x27;sku_004&#x27;,2500.00,&#x27;2020-06-01 12:00:00&#x27;),(102,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 13:00:00&#x27;)(102,&#x27;sku_002&#x27;,12000.00,&#x27;2020-06-01 13:00:00&#x27;)(102,&#x27;sku_002&#x27;,600.00,&#x27;2020-06-02 12:00:00&#x27;) 1SELECT * FROM t_order_rmt 1OPTIMIZE TABLE t_order_rmt FINAL 1SELECT * FROM t_order_rmt 通过测试得到结论： 实际上是使用order by 字段作为唯一键。 去重不能跨分区。 只有合并分区才会进行去重。 认定重复的数据保留，版本字段值最大的。 如果版本字段相同则保留最后一条。 6.SummingMergeTree对于不查询明细，只关心以维度进行汇总聚合结果的场景。如果只使用普通的MergeTree的话，无论是存储空间的开销，还是查询时临时聚合的开销都比较大。 Clickhouse 为了这种场景，提供了一种能够“预聚合”的引擎，SummingMergeTree. 表定义 123456789create table t_order_smt( id UInt32, sku_id String, total_amount Decimal(16,2) , create_time Datetime ) engine =SummingMergeTree(total_amount) partition by toYYYYMMDD(create_time) primary key (id) order by (id,sku_id ) 插入数据 1234567insert into t_order_smtvalues(101,&#x27;sku_001&#x27;,1000.00,&#x27;2020-06-01 12:00:00&#x27;) ,(102,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 11:00:00&#x27;),(102,&#x27;sku_004&#x27;,2500.00,&#x27;2020-06-01 12:00:00&#x27;),(102,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 13:00:00&#x27;)(102,&#x27;sku_002&#x27;,12000.00,&#x27;2020-06-01 13:00:00&#x27;)(102,&#x27;sku_002&#x27;,600.00,&#x27;2020-06-02 12:00:00&#x27;) 1optimize table t_order_smt final; 通过结果可以得到以下结论： 以SummingMergeTree（）中指定的列作为汇总数据列。可以填写多列必须数字列，如果不填，以所有非维度列且为数字列的字段为汇总数据列。 以order by 的列为准，作为维度列。 其他的列保留第一行。 不在一个分区的数据不会被聚合。 设计聚合表的话，唯一键值、流水号可以去掉，所有字段全部是维度、度量或者时间戳。 能不能直接 select total_amount from province_name=’’ and create_date=’xxx’ 来得到汇总值？ 不行，可能会包含一些还没来得及聚合的临时明细 select sum(total_amount) from province_name=’’ and create_date=’xxx’ 五，SQL操作基本上来说传统关系型数据库（以MySQL为例）的SQL语句，基本支持但是也有不一样的地方。这里不会从头讲解SQL语法只介绍Clickhouse与标准SQL（MySQL）不一致的地方。 1.insert基本与标准SQL（MySQL）基本一致 包括标准 insert into [table_name] values(…),(….) 以及从表到表的插入 1insert into [table_name] select a,b,c from [table_name_2] 2.update和deleteClickHouse提供了Delete 和Update的能力，这类操作被称为Mutation查询，它可以看做Alter 的一种。 虽然可以实现修改和删除，但是和一般的OLTP数据库不一样，Mutation语句是一种很“重”的操作，而且不支持事务。 “重”的原因主要是每次修改或者删除都会导致放弃目标数据的原有分区，重建新分区。所以尽量做批量的变更，不要进行频繁小数据的操作。 删除操作 1alter table t_order_smt delete where sku_id =&#x27;sku_001&#x27;; 修改操作 12alter table t_order_smt update total_amount=toDecimal32(2000.00,2) where id =102; 由于操作比较“重”，所以 Mutation语句分两步执行，同步执行的部分其实只是进行新增数据新增分区和并把旧分区打上逻辑上的失效标记。直到触发分区合并的时候，才会删除旧数据释放磁盘空间。 3.查询操作clickhouse基本上与标准SQL 差别不大。 支持子查询 支持CTE(with 子句) 支持各种JOIN， 但是JOIN操作无法使用缓存，所以即使是两次相同的JOIN语句，Clickhouse也会视为两条新SQL。 不支持窗口函数。 不支持自定义函数。 GROUP BY 操作增加了 with rollup\\with cube\\with total 用来计算小计和总计。 模拟数据 12345678910111213insert into t_order_mtvalues(101,&#x27;sku_001&#x27;,1000.00,&#x27;2020-06-01 12:00:00&#x27;) ,(102,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 12:00:00&#x27;),(103,&#x27;sku_004&#x27;,2500.00,&#x27;2020-06-01 12:00:00&#x27;),(104,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 12:00:00&#x27;)(105,&#x27;sku_003&#x27;,600.00,&#x27;2020-06-02 12:00:00&#x27;),(106,&#x27;sku_001&#x27;,1000.00,&#x27;2020-06-04 12:00:00&#x27;),(107,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-04 12:00:00&#x27;),(108,&#x27;sku_004&#x27;,2500.00,&#x27;2020-06-04 12:00:00&#x27;),(109,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-04 12:00:00&#x27;),(110,&#x27;sku_003&#x27;,600.00,&#x27;2020-06-01 12:00:00&#x27;)select id , sku_id,sum(total_amount) from t_order_mt group by id,sku_id with rollup; with rollup : 从右至左去掉维度进行小计。 1select id , sku_id,sum(total_amount) from t_order_mt group by id,sku_id with cube; with cube : 从右至左去掉维度进行小计，再从左至右去掉维度进行小计。 1select id , sku_id,sum(total_amount) from t_order_mt group by id,sku_id with totals; with totals: 只计算合计。 4.alter操作同mysql的修改字段基本一致。 新增字段 1alter table tableName add column newcolname String after col1 修改字段类型 1alter table tableName modify column newcolname String ； 删除字段 1alter table tableName drop column newcolname ; 5.导出数据1&quot;select toHour(create_time) hr ,count(*) from test1.order_wide where dt=&#x27;2020-06-23&#x27; group by hr&quot; --format CSVWithNames&gt; ~/rs1.csv 支持格式的地址 六，副本副本的目的主要是保障数据的高可用性，即使一台clickhouse节点宕机，那么也可以从其他服务器获得相同的数据。 1.副本写入流程 2.配置这时需要启动zookeeper集群 和另外一台clickhouse 服务器。 另外一台clickhouse服务器的安装完全和第一台一直即可。 在两台服务器的/etc/clickhouse-server/config.d目录下创建一个名为metrika.xml的配置文件： 123456789101112131415161718&lt;?xml version=&quot;1.0&quot;?&gt;&lt;yandex&gt; &lt;zookeeper-servers&gt; &lt;node index=&quot;1&quot;&gt; &lt;host&gt;hdp1&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;node index=&quot;2&quot;&gt; &lt;host&gt;hdp2&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt;&lt;node index=&quot;3&quot;&gt; &lt;host&gt;hdp3&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;/zookeeper-servers&gt;&lt;/yandex&gt; 在 /etc/clickhouse-server/config.xml中增加 1&lt;include_from&gt;/etc/clickhouse-server/config.d/metrika.xml&lt;/include_from&gt; 在两台电脑上分别建表 A机器 123456789create table rep_t_order_mt_0105 ( id UInt32, sku_id String, total_amount Decimal(16,2), create_time Datetime ) engine =ReplicatedMergeTree(&#x27;/clickhouse/tables/01/rep_t_order_mt_0105&#x27;,&#x27;rep_hdp1&#x27;) partition by toYYYYMMDD(create_time) primary key (id) order by (id,sku_id); B机器 123456789create table rep_t_order_mt_0105 ( id UInt32, sku_id String, total_amount Decimal(16,2), create_time Datetime) engine =ReplicatedMergeTree(&#x27;/clickhouse/tables/01/rep_t_order_mt_0105&#x27;,&#x27;rep_hdp2&#x27;)partition by toYYYYMMDD(create_time) primary key (id) order by (id,sku_id); 3.参数解释ReplicatedMergeTree 中， 第一参数是分片的zk_path，一般按照： /clickhouse/table/&#123;shard&#125;/&#123;table_name&#125; 的格式写，如果只有一个分片就写01即可。 第二个参数是副本名称，相同的分片副本名称不能相同。 insert语句 123456 insert into rep_t_order_mt_0105 values(101,&#x27;sku_001&#x27;,1000.00,&#x27;2020-06-01 12:00:00&#x27;) ,(102,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 12:00:00&#x27;),(103,&#x27;sku_004&#x27;,2500.00,&#x27;2020-06-01 12:00:00&#x27;),(104,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 12:00:00&#x27;)(105,&#x27;sku_003&#x27;,600.00,&#x27;2020-06-02 12:00:00&#x27;) 七，分片集群副本虽然能够提高数据的可用性，降低丢失风险，但是对数据的横向扩容没有解决。每台机子实际上必须容纳全量数据。 要解决数据水平切分的问题，需要引入分片的概念。通过分片把一份完整的数据进行切分，不同的分片分布到不同的节点上。在通过Distributed表引擎把数据拼接起来一同使用。 Distributed表引擎本身不存储数据，有点类似于MyCat之于MySql，成为一种中间件，通过分布式逻辑表来写入、分发、路由来操作多台节点不同分片的分布式数据。 1.配置配置的位置还是在之前的metrika.xml，配置分片如下的结构 1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;yandex&gt;&lt;clickhouse_remote_servers&gt;&lt;gmall_cluster&gt; &lt;!-- 集群名称--&gt; &lt;shard&gt; &lt;!--集群的第一个分片--&gt;&lt;internal_replication&gt;true&lt;/internal_replication&gt; &lt;replica&gt; &lt;!—该分片的第一个副本--&gt; &lt;host&gt;hdp1&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;replica&gt; &lt;!—该分片的第二个副本--&gt; &lt;host&gt;hdp2&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;shard&gt; &lt;!--集群的第二个分片--&gt; &lt;internal_replication&gt;true&lt;/internal_replication&gt; &lt;replica&gt; &lt;!—该分片的第一个副本--&gt; &lt;host&gt;hdp3&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;replica&gt; &lt;!—该分片的第二个副本--&gt; &lt;host&gt;hdp4&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;shard&gt; &lt;!--集群的第三个分片--&gt; &lt;internal_replication&gt;true&lt;/internal_replication&gt; &lt;replica&gt; &lt;!—该分片的第一个副本--&gt; &lt;host&gt;hdp5&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;replica&gt; &lt;!—该分片的第二个副本--&gt; &lt;host&gt;hdp6&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;/shard&gt;&lt;/gmall_cluster&gt;&lt;/clickhouse_remote_servers&gt;&lt;/yandex&gt; 2.读写原理 3.三节点版本配置metrika.xml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&lt;yandex&gt;&lt;clickhouse_remote_servers&gt;&lt;gmall_cluster&gt; &lt;!-- 集群名称--&gt; &lt;shard&gt; &lt;!--集群的第一个分片--&gt;&lt;internal_replication&gt;true&lt;/internal_replication&gt; &lt;replica&gt; &lt;!—该分片的第一个副本--&gt; &lt;host&gt;hdp1&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;replica&gt; &lt;!—该分片的第二个副本--&gt; &lt;host&gt;hdp2&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;shard&gt; &lt;!--集群的第二个分片--&gt; &lt;internal_replication&gt;true&lt;/internal_replication&gt; &lt;replica&gt; &lt;!—该分片的第一个副本--&gt; &lt;host&gt;hdp3&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt;&lt;/shard&gt;&lt;/gmall_cluster&gt;&lt;/clickhouse_remote_servers&gt;&lt;zookeeper-servers&gt; &lt;node index=&quot;1&quot;&gt; &lt;host&gt;hadoop102&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;node index=&quot;2&quot;&gt; &lt;host&gt;hadoop103&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;node index=&quot;3&quot;&gt; &lt;host&gt;hadoop104&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt;&lt;/zookeeper-servers&gt;&lt;macros&gt;&lt;shard&gt;01&lt;/shard&gt; &lt;!—不同机器放的分片数不一样--&gt;&lt;replica&gt;rep_1_1&lt;/replica&gt; &lt;!—不同机器放的副本数不一样--&gt;&lt;/macros&gt;&lt;/yandex&gt; hdp1 hdp2 hdp3 01 rep_1_1 01 rep_1_2 02 rep_2_1 123456789 create table st_order_mt_0105 on cluster gmall_cluster ( id UInt32, sku_id String, total_amount Decimal(16,2), create_time Datetime) engine =ReplicatedMergeTree(&#x27;/clickhouse/tables/&#123;shard&#125;/st_order_mt_0105&#x27;,&#x27;&#123;replica&#125;&#x27;)partition by toYYYYMMDD(create_time) primary key (id) order by (id,sku_id); 4.Distribute 分布式表1234567create table st_order_mt_0105_all on cluster gmall_cluster( id UInt32, sku_id String, total_amount Decimal(16,2), create_time Datetime)engine = Distributed(gmall_cluster,test0105, st_order_mt_0105,hiveHash(sku_id)) 其中参数： Distributed( 集群名称，库名，本地表名，分片键) 分片键必须是整型数字 也可以rand() 插入数据 123456 insert into st_order_mt_0105_all values(201,&#x27;sku_001&#x27;,1000.00,&#x27;2020-06-01 12:00:00&#x27;) ,(202,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 12:00:00&#x27;),(203,&#x27;sku_004&#x27;,2500.00,&#x27;2020-06-01 12:00:00&#x27;),(204,&#x27;sku_002&#x27;,2000.00,&#x27;2020-06-01 12:00:00&#x27;)(205,&#x27;sku_003&#x27;,600.00,&#x27;2020-06-02 12:00:00&#x27;) 通过查询分布式表语句 1SELECT * FROM st_order_mt_all 和 本地表 1select * from st_order_mt; 来观察数据的分布是否正确。 八，java操作clickHouse1.依赖12345678910111213&lt;!-- 官方驱动,默认连接为HTTP协议，8123端口 --&gt;&lt;!-- &lt;dependency&gt;--&gt;&lt;!-- &lt;groupId&gt;ru.yandex.clickhouse&lt;/groupId&gt;--&gt;&lt;!-- &lt;artifactId&gt;clickhouse-jdbc&lt;/artifactId&gt;--&gt;&lt;!-- &lt;version&gt;0.1.52&lt;/version&gt;--&gt;&lt;!-- &lt;/dependency&gt;--&gt;&lt;!--两者不可共用--&gt;&lt;!-- 三方提供的驱动，默认连接协议为TCP，端口为9000 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.housepower&lt;/groupId&gt; &lt;artifactId&gt;clickhouse-native-jdbc&lt;/artifactId&gt; &lt;version&gt;1.6-stable&lt;/version&gt; &lt;/dependency&gt; 2.基本操作123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899/** * @author yhd * @since 2021/3/31 15:25 * @email yinhuidong1@xiaomi.com * @description 测试 java 连接 clickhouse 的基础操作 * @params * @return */@SpringBootTestclass ClickhouseApplicationTests &#123; /** * @return * @author yhd * @email yinhuidong1@xiaomi.com * @description 尝试获取连接并在default数据库创建一张表 * @params * @since 2021/3/31 15:20 */ @Test void contextLoads() throws Exception &#123; Class.forName(&quot;com.github.housepower.jdbc.ClickHouseDriver&quot;); Connection connection = DriverManager.getConnection(&quot;jdbc:clickhouse://121.199.31.160:9000&quot;); Statement statement = connection.createStatement(); statement.executeQuery(&quot;create table default.jdbc_example(day Date, name String, age UInt8) Engine=Log&quot;); &#125; /** * @return * @author yhd * @email yinhuidong1@xiaomi.com * @description 批量插入10条数据 * @params * @since 2021/3/31 15:20 */ @Test public void test() throws Exception &#123; Class.forName(&quot;com.github.housepower.jdbc.ClickHouseDriver&quot;); Connection connection = DriverManager.getConnection(&quot;jdbc:clickhouse://121.199.31.160:9000&quot;); PreparedStatement pstmt = connection.prepareStatement(&quot;insert into default.jdbc_example values(?, ?, ?)&quot;); // insert 10 records for (int i = 0; i &lt; 10; i++) &#123; pstmt.setDate(1, new Date(System.currentTimeMillis())); pstmt.setString(2, &quot;panda_&quot; + (i + 1)); pstmt.setInt(3, 18); pstmt.addBatch(); &#125; pstmt.executeBatch(); &#125; /** * @return * @author yhd * @email yinhuidong1@xiaomi.com * @description 查询 * @params * @since 2021/3/31 15:22 */ @Test public void test2() throws Exception &#123; Class.forName(&quot;com.github.housepower.jdbc.ClickHouseDriver&quot;); Connection connection = DriverManager.getConnection(&quot;jdbc:clickhouse://121.199.31.160:9000&quot;); Statement statement = connection.createStatement(); String sql = &quot;select * from default.jdbc_example&quot;; ResultSet rs = statement.executeQuery(sql); while (rs.next()) &#123; // ResultSet 的下标值从 1 开始，不可使用 0，否则越界，报 ArrayIndexOutOfBoundsException 异常 System.out.println(rs.getDate(1) + &quot;, &quot; + rs.getString(2) + &quot;, &quot; + rs.getInt(3)); &#125; &#125; /** * @author yhd * @since 2021/3/31 15:24 * @email yinhuidong1@xiaomi.com * @description 删除表操作 * @params * @return */ @Test public void test3()throws Exception &#123; Class.forName(&quot;com.github.housepower.jdbc.ClickHouseDriver&quot;); Connection connection = DriverManager.getConnection(&quot;jdbc:clickhouse://121.199.31.160:9000&quot;); Statement statement = connection.createStatement(); statement.executeQuery(&quot;drop table default.jdbc_example&quot;); &#125;&#125; 九，SpringBoot整合ClickHouse案例基于：Druid连接池和mybatis进行整合。Druid 1.1.10 版本 SQL Parser对clickhouse的开始提供支持。 1.依赖1234567891011121314151617181920 &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.2.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.1.4&lt;/version&gt; &lt;/dependency&gt;&lt;!-- 官方驱动,默认连接为HTTP协议，8123端口 --&gt; &lt;dependency&gt; &lt;groupId&gt;ru.yandex.clickhouse&lt;/groupId&gt; &lt;artifactId&gt;clickhouse-jdbc&lt;/artifactId&gt; &lt;version&gt;0.1.52&lt;/version&gt; &lt;/dependency&gt; 2.配置数据源1234567spring.datasource.type=com.alibaba.druid.pool.DruidDataSourcespring.datasource.click.driverClassName=ru.yandex.clickhouse.ClickHouseDriverspring.datasource.click.url=jdbc:clickhouse://121.199.31.160:8123/defaultspring.datasource.click.initialSize=10spring.datasource.click.maxActive=100spring.datasource.click.minIdle=10spring.datasource.click.maxWait=6000 3.代码配置123456789101112@Data@Component@ConfigurationProperties(prefix = &quot;spring.datasource.click&quot;)public class ClickHouseProperties &#123; private String driverClassName ; private String url ; private Integer initialSize ; private Integer maxActive ; private Integer minIdle ; private Integer maxWait ;&#125; 12345678910111213141516@SpringBootConfigurationpublic class DruidConfig &#123; @Resource private ClickHouseProperties clickHouseProperties ; @Bean public DataSource dataSource() &#123; DruidDataSource datasource = new DruidDataSource(); datasource.setUrl(clickHouseProperties.getUrl()); datasource.setDriverClassName(clickHouseProperties.getDriverClassName()); datasource.setInitialSize(clickHouseProperties.getInitialSize()); datasource.setMinIdle(clickHouseProperties.getMinIdle()); datasource.setMaxActive(clickHouseProperties.getMaxActive()); datasource.setMaxWait(clickHouseProperties.getMaxWait()); return datasource; &#125;&#125;","categories":[{"name":"ClickHouse","slug":"ClickHouse","permalink":"https://yinhuidong.github.io/categories/ClickHouse/"}],"tags":[{"name":"ClickHouse","slug":"ClickHouse","permalink":"https://yinhuidong.github.io/tags/ClickHouse/"}]},{"title":"ThreadLocal","slug":"JUC/ThreadLocal","date":"2022-01-11T11:08:04.200Z","updated":"2022-01-11T11:31:15.889Z","comments":true,"path":"2022/01/11/JUC/ThreadLocal/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/JUC/ThreadLocal/","excerpt":"","text":"一，TL的基本使用与原理为线程创建独一份的副本数据。 1.基本使用12345678910111213141516171819202122232425262728293031323334/** * @author 二十 * @since 2021/8/28 11:19 下午 */public class TlTest &#123; private static AtomicInteger id = new AtomicInteger(0); private static ThreadLocal&lt;Integer&gt; tl = ThreadLocal.withInitial(()-&gt;id.getAndIncrement()); private static CountDownLatch count = new CountDownLatch(3); public static void main(String[] args)throws Exception &#123; new Thread(()-&gt;&#123; System.out.println(tl.get()+&quot; &quot;+Thread.currentThread().getName()); tl.remove(); count.countDown(); &#125;,&quot;A&quot;).start(); new Thread(()-&gt;&#123; System.out.println(tl.get()+&quot; &quot;+Thread.currentThread().getName()); tl.remove(); count.countDown(); &#125;,&quot;B&quot;).start(); new Thread(()-&gt;&#123; System.out.println(tl.get()+&quot; &quot;+Thread.currentThread().getName()); tl.remove(); count.countDown(); &#125;,&quot;C&quot;).start(); count.await(); &#125;&#125; 2.原理分析​ 里面维护一个ThreadLocalMap结构，每一个元素对应一个桶位。 ​ 使用ThreadLocal定义的变量，将指向当前线程本地的一个LocalMap空间。 ​ ThreadLocal变量作为key，其内容作为value，保存在本地。 ​ 多线程对ThreadLocal对象进行操作，实际上是对各自的本地变量进行操作，不存在线程安全问题。 ​ 假设一个类里面定义了三个threadlocal，三个线程来访问这个类，每个线程本地会维护一个threadlocalmap，每一个map里面会有三个entry，key是threadlocal对象，value是threadlocal里面set的值。 二，TL源码1.属性1234567891011121314151617181920212223242526 /** * 线程获取Threadlocal.get()时，如果是第一次在某个threadlocal对象上get，会给当前线程分配一个value， * 这个value和当前的threadlocal对象被包装成一个entry，其中key=threadlocal对象， * value=threadlocal对象给当前线程生成的value。这个entry存放到哪个位置与这个value有关。 */private final int threadLocalHashCode = nextHashCode();//创建threadlocal对象时会使用到，每创建一个threadlocal对象就会使用它分配一个hash值给对象。private static AtomicInteger nextHashCode = new AtomicInteger();//每创建一个threadlocal对象，这个nextHashCode就会增长0x61c88647。private static final int HASH_INCREMENT = 0x61c88647;//创建新的threadlocal对象的时候，给当前对象分配hash的时候用到。private static int nextHashCode() &#123; return nextHashCode.getAndAdd(HASH_INCREMENT);&#125;//留给子类重写扩展的protected T initialValue() &#123; return null;&#125;//带初始化值得threadlocalpublic static &lt;S&gt; ThreadLocal&lt;S&gt; withInitial(Supplier&lt;? extends S&gt; supplier) &#123; return new SuppliedThreadLocal&lt;&gt;(supplier);&#125;public ThreadLocal() &#123;&#125; 2.get()1234567891011121314151617public T get() &#123; //获取当前线程 Thread t = Thread.currentThread(); //根据当前线程获取对应的map ThreadLocalMap map = getMap(t); if (map != null) &#123; //已经初始化 //根据当前threadlocal对象获取entry节点 ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) &#123; //节点初始化过 //获取entry的value并返回 T result = (T)e.value; return result; &#125; &#125; //走到这里说明map尚未初始化获取entry尚未初始化 return setInitialValue();&#125; 2.1 setInitialValue()1234567891011121314151617private T setInitialValue() &#123; //获取初始值，留给子类重写 T value = initialValue(); //获取当前线程 Thread t = Thread.currentThread(); //获取当前线程对应的map ThreadLocalMap map = getMap(t); //如果map初始化过 if (map != null) //map里面放入当前对象和value map.set(this, value); else //map尚未初始化过 //初始化map--直接new一个并放入当前对象和value createMap(t, value); //返回value return value;&#125; 2.2 getMap（）1234//返回当前线程的threadLocalsThreadLocalMap getMap(Thread t) &#123; return t.threadLocals;&#125; 2.3 createMap（）1234//利用构造器初始化threadLocals并将当前线程和线程对应的value设置进去void createMap(Thread t, T firstValue) &#123; t.threadLocals = new ThreadLocalMap(this, firstValue);&#125; 3.set()12345678public void set(T value) &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) //当前线程对应的map已经初始化 map.set(this, value); //map放入值 else //map未初始化 createMap(t, value); //初始化map&#125; 4.remove()12345public void remove() &#123; ThreadLocalMap m = getMap(Thread.currentThread()); if (m != null) //map已经初始化 m.remove(this); //调用map的remove移除掉当前对象对应的entry&#125; 5.内部类ThreadLocalMap1234567891011121314151617181920212223242526272829303132333435363738394041424344454647//threadlocalmap里面的key是弱引用 ，key=threadlocal对象//value是强引用，value保存的是threadlocal对象与当前线程关联的value//这样设计的好处是为了防止内存泄漏static class Entry extends WeakReference&lt;ThreadLocal&lt;?&gt;&gt; &#123; Object value; Entry(ThreadLocal&lt;?&gt; k, Object v) &#123; super(k); value = v; &#125;&#125;//map的初始化容量为16private static final int INITIAL_CAPACITY = 16;//map里面的entry桶位列表private Entry[] table;//列表容量private int size = 0;/** * 扩容阈值 当前数组长度的三分之二 */private int threshold; // Default to 0//将扩容阈值设置为当前数组长度的三分之二private void setThreshold(int len) &#123; threshold = len * 2 / 3;&#125;//获取下一个位置private static int nextIndex(int i, int len) &#123; return ((i + 1 &lt; len) ? i + 1 : 0);&#125;//获取下一个位置private static int prevIndex(int i, int len) &#123; return ((i - 1 &gt;= 0) ? i - 1 : len - 1);&#125;//其实从上层的api可以发现这里其实是延迟初始化，只有线程第一次调用threadlocal的//get或者set的时候才会初始化。ThreadLocalMap(ThreadLocal&lt;?&gt; firstKey, Object firstValue) &#123; //初始化散列表，长度为16 table = new Entry[INITIAL_CAPACITY]; //计算entry的存储位置 int i = firstKey.threadLocalHashCode &amp; (INITIAL_CAPACITY - 1); //创建新的entry table[i] = new Entry(firstKey, firstValue); //占用量设置为1 size = 1; //修改扩容阈值为初始化长度 setThreshold(INITIAL_CAPACITY);&#125; 5.1 getEntry()123456789private Entry getEntry(ThreadLocal&lt;?&gt; key) &#123; //根据当前线程的threadlocal对象获取entry的存储位置 int i = key.threadLocalHashCode &amp; (table.length - 1); Entry e = table[i]; if (e != null &amp;&amp; e.get() == key) //校验entry是不是已经丢了，或者已经被覆盖 return e; else //执行打这里说明entry已经丢了或者被发生了hash冲突，继续向后寻找 return getEntryAfterMiss(key, i, e);&#125; 5.2 getEntryAfterMiss()123456789101112131415161718192021222324private Entry getEntryAfterMiss(ThreadLocal&lt;?&gt; key, int i, Entry e) &#123; //获取散列表 Entry[] tab = table; //获取散列表的长度 int len = tab.length; //如果entry不为空，那就说明entryhash冲突了 while (e != null) &#123; //获取entry对应的threadlocal对象 ThreadLocal&lt;?&gt; k = e.get(); //说明key对应的threadlocal对象已经被回收了，当前entry属于脏数据 if (k == key) //直接返回 return e; //如果key==null，说明key对应的threadlocal对象已经被回收了，当前entry属于脏数据 if (k == null) //做一次探测式过期清理 expungeStaleEntry(i); else //执行到这里说明发生了hash冲突，继续从当前位置往后寻找 i = nextIndex(i, len); e = tab[i]; &#125; //说明entry过期了，直接返回null return null;&#125; 5.3 expungeStaleEntry() 探测式过期清理123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051private int expungeStaleEntry(int staleSlot) &#123; //获取散列表 Entry[] tab = table; //获取散列表的长度 int len = tab.length; //因为此处threadlocal对象已经被回收，所以直接将value设置为null，help GC tab[staleSlot].value = null; //再讲当前桶位设置为空 tab[staleSlot] = null; /** * 为什么这里要分两次设置为null？ * 因为key本身是弱引用，但是value是强引用，如果直接回收桶位，value无法直接被回收 */ //散列表的占用长度-1 size--; Entry e; int i; //从当前节点所在位置的下一个位置直到最后循环， for (i = nextIndex(staleSlot, len); //停止条件是当前索引对应桶位=null (e = tab[i]) != null; //循环条件是每次索引+1 i = nextIndex(i, len)) &#123; //获取entry的threadlocal对象 ThreadLocal&lt;?&gt; k = e.get(); if (k == null) &#123;//如果对象为空，说明已经过期了，entry是脏数据 //回收 e.value = null; tab[i] = null; size--; &#125; else &#123;//此时说明entry不是脏数据 //计算threadlocal对象在散列表的新索引，为啥重新计算？ //因为当前get到了脏数据，刚刚从散列表移除，所以散列表的占用量已经发生了变化 int h = k.threadLocalHashCode &amp; (len - 1); //如果没有发生hash冲突 if (h != i) &#123; //将原来的桶位释放 tab[i] = null; //寻找存放位置，直到所在桶位为空，因为可能计算出的位置发生了hash冲突， //这个时候，就要索引下推到下一桶位 while (tab[h] != null) h = nextIndex(h, len); //将entry放到新的桶位 tab[h] = e; &#125; &#125; &#125; //返回最后处理的索引处 return i;&#125; 5.4 set()1234567891011121314151617181920212223242526272829private void set(ThreadLocal&lt;?&gt; key, Object value) &#123; Entry[] tab = table; int len = tab.length; int i = key.threadLocalHashCode &amp; (len-1); for (Entry e = tab[i];//当前threadlocal对象所对应的节点 e != null; //终止条件是entry为空，说明这个桶位能存放entry e = tab[i = nextIndex(i, len)]) &#123; //桶位下推 ThreadLocal&lt;?&gt; k = e.get(); //如果当前对象所对应的桶位有值，且当前桶位的key是当前对象， //说明这是一次值重置，直接覆盖旧的值即可 if (k == key) &#123; e.value = value; return; &#125; //如果k==null，说明当前位置对应的entry是过期的， if (k == null) &#123; replaceStaleEntry(key, value, i); return; &#125; &#125; //来到这里的条件：这次操作不是一次对已经有的值得覆盖，或者已经找到了应该存放当前entry的桶位 tab[i] = new Entry(key, value); int sz = ++size; //如果达到了扩容的条件，进行扩容操作 if (!cleanSomeSlots(i, sz) &amp;&amp; sz &gt;= threshold) rehash();&#125; 5.5 replaceStaleEntry()替换过期entry123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051//替换过期entryprivate void replaceStaleEntry(ThreadLocal&lt;?&gt; key, Object value, int staleSlot) &#123; Entry[] tab = table; int len = tab.length; Entry e; //进入这个方法的条件说明：当前位置的节点其实是过期的，但是还没来得及回收 int slotToExpunge = staleSlot; //当前桶位的索引 //从当前位置向前清理 for (int i = prevIndex(staleSlot, len); //i=当前索引的前一个索引 (e = tab[i]) != null; //终止条件是索引所在的桶位有数据 i = prevIndex(i, len)) //循环条件是每次往前一个桶位 //说明是过期的，那就继续往前清理 if (e.get() == null) slotToExpunge = i; //从当前位置向后清理 for (int i = nextIndex(staleSlot, len); (e = tab[i]) != null; i = nextIndex(i, len)) &#123; ThreadLocal&lt;?&gt; k = e.get(); //如果当前位置的key和当前threadlocal对象一致 if (k == key) &#123; //进行值覆盖操作 e.value = value; //将过期数据放到当前循环到的table[i] tab[i] = tab[staleSlot]; //这里的逻辑其实就是进行一下位置优化 tab[staleSlot] = e; //说明上面的循环并没有找到过期数据 if (slotToExpunge == staleSlot) //吧探测的开始位置改成当前位置 slotToExpunge = i; //进行探测式过期清理 cleanSomeSlots(expungeStaleEntry(slotToExpunge), len); return; &#125; //当前遍历entry是一个过期数据 &amp;&amp; 往前找过期数据没找到 if (k == null &amp;&amp; slotToExpunge == staleSlot) //更新探测位置为当前位置 slotToExpunge = i; &#125; //将新的值放入当前节点 tab[staleSlot].value = null; tab[staleSlot] = new Entry(key, value); //如果两个索引不相等，就继续清理 if (slotToExpunge != staleSlot) cleanSomeSlots(expungeStaleEntry(slotToExpunge), len);&#125; 5.6 cleanSomeSlots()启发式清理工作123456789101112131415161718192021//启发式清理工作 i 开始清理位置 n 结束条件，数组长度private boolean cleanSomeSlots(int i, int n) &#123; boolean removed = false; Entry[] tab = table; int len = tab.length; do &#123; //获取当前i的下一个下标 i = nextIndex(i, len); //获取当前下标为I的元素 Entry e = tab[i]; //断定为过期元素 if (e != null &amp;&amp; e.get() == null) &#123; n = len;//更新数组长度 removed = true; //从当前过期位置开始一次谈测试清理工作 i = expungeStaleEntry(i); &#125; &#125; while ( (n &gt;&gt;&gt;= 1) != 0);//假设table.length=16 return removed;&#125; 5.7 rehash()12345678private void rehash() &#123; //遍历，探测式清理，干掉所有过期数据 expungeStaleEntries(); //仍然达到扩容条件 if (size &gt;= threshold - threshold / 4) //扩容 resize();&#125; 5.8 resize()123456789101112131415161718192021222324252627private void resize() &#123; Entry[] oldTab = table; int oldLen = oldTab.length; int newLen = oldLen * 2; //扩容为原来的2倍 Entry[] newTab = new Entry[newLen]; int count = 0; for (int j = 0; j &lt; oldLen; ++j) &#123; Entry e = oldTab[j]; //访问old表指定位置的data if (e != null) &#123; //data存在 ThreadLocal&lt;?&gt; k = e.get(); if (k == null) &#123; //过期数据 e.value = null; // Help the GC &#125; else &#123; int h = k.threadLocalHashCode &amp; (newLen - 1);//重新计算hash值 while (newTab[h] != null) //获取到一个最近的，可以使用的位置 h = nextIndex(h, newLen); newTab[h] = e; //数据迁移 count++; &#125; &#125; &#125; setThreshold(newLen);//设置下一次扩容的指标 size = count; table = newTab;&#125; 5.9 remove()12345678910111213141516private void remove(ThreadLocal&lt;?&gt; key) &#123; Entry[] tab = table; int len = tab.length; int i = key.threadLocalHashCode &amp; (len-1); for (Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) &#123; //从当前位置开始，如果桶位是空，就去下一个 //如果不为空的桶位与当前线程的threadlocal对象一致 if (e.get() == key) &#123; e.clear(); //干掉key的引用 expungeStaleEntry(i); //探测式过期清理 return; &#125; &#125;&#125;","categories":[{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"阻塞队列","slug":"JUC/阻塞队列","date":"2022-01-11T11:07:55.302Z","updated":"2022-01-11T11:25:05.613Z","comments":true,"path":"2022/01/11/JUC/阻塞队列/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/JUC/%E9%98%BB%E5%A1%9E%E9%98%9F%E5%88%97/","excerpt":"","text":"把多线程环境比作是分布式的话，那么线程与线程之间是不是也可以使用这种消息队列的方式进行数据通信和解耦呢？​ 一，阻塞队列使用案例1.注册成功后增加积分假如模拟一个场景，就是用户注册的时候，在注册成功以后发放积分。这个场景在一般来说，会这么去实现：但是实际上，我们需要考虑两个问题： 性能，在注册这个环节里面，假如添加用户需要花费 1 秒钟，增加积分需要花费 1 秒钟，那么整个注册结果的返回就可能需要大于 2 秒，虽然影响不是很大，但是在量比较大的时候，我们也需要做一些优化。 耦合，添加用户和增加积分，可以认为是两个领域，也就是说，增加积分并不是注册必须要具备的功能，但是一旦增加积分这个逻辑出现异常，就会导致注册失败。这种耦合在程序设计的时候是一定要规避的。 因此我们可以通过异步的方式来实现。​ 2.改造之前的代码逻辑12345678910111213141516171819202122232425262728293031323334353637public class UserService &#123; public boolean register() &#123; User user = new User(); user.setName(&quot;Mic&quot;); addUser(user); sendPoints(user); return true; &#125; public static void main(String[] args) &#123; new UserService().register(); &#125; private void addUser(User user) &#123; System.out.println(&quot; 添加用户：&quot; + user); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; private void sendPoints(User user) &#123; System.out.println(&quot; 发 送 积 分 给 指 定 用 户:&quot; + user); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125;@Dataclass User &#123; private String name;&#125; 3.改造之后的逻辑123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public class UserService &#123; private final ExecutorService single = Executors.newSingleThreadExecutor(); private volatile boolean isRunning = true; ArrayBlockingQueue arrayBlockingQueue = new ArrayBlockingQueue(10); &#123; init(); &#125; public void init() &#123; single.execute(() -&gt; &#123; while (isRunning) &#123; try &#123; User user = (User) arrayBlockingQueue.take();// 阻塞的方式获取队列中的数据 sendPoints(user); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; public boolean register() &#123; User user = new User(); user.setName(&quot;Mic&quot;); addUser(user); arrayBlockingQueue.add(user);// 添加到异步队列 return true; &#125; public static void main(String[] args) &#123; new UserService().register(); &#125; private void addUser(User user) &#123; System.out.println(&quot; 添加用户：&quot; + user); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; private void sendPoints(User user) &#123; System.out.println(&quot; 发 送 积 分 给 指 定 用 户:&quot; + user); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125;@Dataclass User &#123; private String name;&#125; 优化以后，整个流程就变成了这样我们使用了 ArrayBlockingQueue 基于数组的阻塞队列，来优化代码的执行逻辑。 4.阻塞队列的应用场景阻塞队列这块的应用场景，比较多的仍然是对于生产者消费者场景的应用，但是由于分布式架构的普及，更多的关注在分布式消息队列上。所以其实如果把阻塞队列比作成分布式消息队列的话，那么所谓的生产者和消费者其实就是基于阻塞队列的解耦。 另外，阻塞队列是一个 fifo 的队列，所以对于希望在线程级别需要实现对目标服务的顺序访问的场景中，也可以使用。 二,J.U.C 中的阻塞队列1.JUC中提供的阻塞队列在 Java8 中，提供了 7 个阻塞队列。 ArrayBlockingQueue 数组实现的有界阻塞队列, 此队列按照先进先出（FIFO）的原则对元素进行排序。 LinkedBlockingQueue 链表实现的有界阻塞队列, 此队列的默认和最大长度为Integer.MAX_VALUE。此队列按照先进先出的原则对元素进行排序 PriorityBlockingQueue 支持优先级排序的无界阻塞队列, 默认情况下元素采取自然顺序升序排列。也可以自定义类实现 compareTo()方法来指定元素排序规则，或者初始化 PriorityBlockingQueue 时，指定构造参数 Comparator 来对元素进行排序。 DelayQueue 优先级队列实现的无界阻塞队列 SynchronousQueue 不存储元素的阻塞队列, 每一个 put 操作必须等待一个 take 操作，否则不能继续添加元素。 LinkedTransferQueue 链表实现的无界阻塞队列 LinkedBlockingDeque 链表实现的双向阻塞队列 2.阻塞队列的操作方法在阻塞队列中，提供了四种处理方式 2.1插入操作add(e) ：添加元素到队列中，如果队列满了，继续插入元素会报错，IllegalStateException。 offer(e) : 添加元素到队列，同时会返回元素是否插入成功的状态，如果成功则返回 true。 put(e) ：当阻塞队列满了以后，生产者继续通过 put添加元素，队列会一直阻塞生产者线程，直到队列可用。 offer(e,time,unit) ：当阻塞队列满了以后继续添加元素，生产者线程会被阻塞指定时间，如果超时，则线程直接退出。 2.2移除操作remove()：当队列为空时，调用 remove 会返回 false，如果元素移除成功，则返回 true。 poll(): 当队列中存在元素，则从队列中取出一个元素，如果队列为空，则直接返回 null。 take()：基于阻塞的方式获取队列中的元素，如果队列为空，则 take 方法会一直阻塞，直到队列中有新的数据可以消费。 poll(time,unit)：带超时机制的获取数据，如果队列为空，则会等待指定的时间再去获取元素返回。​ 三，ArrayBlockingQueue源码1.构造方法 ArrayBlockingQueue 提供了三个构造方法，分别如下。 capacity： 表示数组的长度，也就是队列的长度。 fair：表示是否为公平的阻塞队列，默认情况下构造的是非公平的阻塞队列。 ​ 1234567891011121314151617181920212223242526272829303132333435public ArrayBlockingQueue(int capacity) &#123; this(capacity, false);&#125;public ArrayBlockingQueue(int capacity, boolean fair) &#123; if (capacity &lt;= 0) throw new IllegalArgumentException(); this.items = new Object[capacity]; lock = new ReentrantLock(fair);//重入锁，出队和入队持有这一把锁 notEmpty = lock.newCondition();//初始化非空等待队列 notFull = lock.newCondition();//初始化非满等待队列&#125;public ArrayBlockingQueue(int capacity, boolean fair, Collection&lt;? extends E&gt; c) &#123; this(capacity, fair); final ReentrantLock lock = this.lock; lock.lock(); // Lock only for visibility, not mutual exclusion try &#123; int i = 0; try &#123; for (E e : c) &#123; checkNotNull(e); items[i++] = e; &#125; &#125; catch (ArrayIndexOutOfBoundsException ex) &#123; throw new IllegalArgumentException(); &#125; count = i; putIndex = (i == capacity) ? 0 : i; &#125; finally &#123; lock.unlock(); &#125;&#125; items 构造以后，大概是一个这样的数组结构：​ 2.add以 add 方法作为入口，在 add 方法中会调用父类的 add 方法，也就是 AbstractQueue.​ 12345678910public boolean add(E e) &#123; return super.add(e);&#125;======================================================public boolean add(E e) &#123; if (offer(e)) return true; else throw new IllegalStateException(&quot;Queue full&quot;);&#125; 从父类的 add 方法可以看到，这里做了一个队列是否满了的判断，如果队列满了直接抛出一个异常。​ 3.offer​ 123456789101112131415161718public boolean offer(E e) &#123; //校验放入队列的元素如果为null，抛出空指针异常 checkNotNull(e); final ReentrantLock lock = this.lock; lock.lock(); try &#123; //如果队列已经满了，返回false if (count == items.length) return false; else &#123; //否则，执行入队逻辑 enqueue(e); return true; &#125; &#125; finally &#123; lock.unlock(); &#125;&#125; 4.checkNotNull1234private static void checkNotNull(Object v) &#123; if (v == null) throw new NullPointerException();&#125; 5.enqueue1234567891011121314private void enqueue(E x) &#123; //当前队列的引用 final Object[] items = this.items; //元素入队 items[putIndex] = x; //如果下一个元素存放位置大于队列长度，把队列长度置为0 if (++putIndex == items.length) putIndex = 0; //元素个数+1 count++; //唤醒处于等待状态下的线程，表示当前队列中的元素不为空,如果存在消费者线程阻塞，就可以开始取出元素 notEmpty.signal();&#125; putIndex 为什么会在等于数组长度的时候重新设置为 0？​ 因为 ArrayBlockingQueue 是一个 FIFO 的队列，队列添加元素时，是从队尾获取 putIndex 来存储元素，当 putIndex等于数组长度时，下次就需要从数组头部开始添加了。 下面这个图模拟了添加到不同长度的元素时，putIndex 的变化，当 putIndex 等于数组长度时，不可能让 putIndex 继续累加，否则会超出数组初始化的容量大小。 当元素满了以后是无法继续添加的，因为会报错。 队列中的元素肯定会有一个消费者线程通过 take或者其他方法来获取数据，而获取数据的同时元素也会从队列中移除。 ​ 6.putput 方法和 add 方法功能一样，差异是 put 方法如果队列满了，会阻塞。 1234567891011121314public void put(E e) throws InterruptedException &#123; checkNotNull(e); final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try &#123; //元素个数如果等于数组的长度，阻塞当前线程 while (count == items.length) notFull.await(); //否则，入队逻辑 enqueue(e); &#125; finally &#123; lock.unlock(); &#125;&#125; 7.taketake 方法是一种阻塞获取队列中元素的方法。​ 它的实现原理很简单，有就删除没有就阻塞，注意这个阻塞是可以中断的，如果队列没有数据那么就加入 notEmpty条件队列等待(有数据就直接取走，方法结束)，如果有新的put 线程添加了数据，那么 put 操作将会唤醒 take 线程，执行 take 操作。​ 12345678910111213public E take() throws InterruptedException &#123; final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try &#123; //队列元素个数==0 阻塞 while (count == 0) notEmpty.await(); //否则执行出队逻辑 return dequeue(); &#125; finally &#123; lock.unlock(); &#125;&#125; 如果队列中添加了元素，那么这个时候，会在 enqueue 中调用 notempty.signal 唤醒 take 线程来获得元素 8.dequeue这个是出队列的方法，主要是删除队列头部的元素并发返回给客户端。 123456789101112131415161718private E dequeue() &#123; //获取队列引用 final Object[] items = this.items; //拿出来一个元素 E x = (E) items[takeIndex]; //将拿出的元素的索引位置设置为null items[takeIndex] = null; //如果到了数组长度 从零开始 if (++takeIndex == items.length) takeIndex = 0; //队列元素个数-1 count--; if (itrs != null) itrs.elementDequeued();//更新迭代器中的元素个数 //触发 因为队列满了以后导致的被阻塞的线程 notFull.signal(); return x;&#125; 9.elementDequeued​ ArrayBlockingQueue 中，实现了迭代器的功能，也就是可以通过迭代器来遍历阻塞队列中的元素。 所以 itrs.elementDequeued() 是用来更新迭代器中的元素数据的。 takeIndex 的索引变化图如下，同时随着数据的移除，会唤醒处于 put 阻塞状态下的线程来继续添加数据。 10.removeremove 方法是移除一个指定元素。看看它的实现代码： 1234567891011121314151617181920212223242526272829303132public boolean remove(Object o) &#123; //判空 if (o == null) return false; //获取队列的引用 final Object[] items = this.items; final ReentrantLock lock = this.lock; lock.lock(); try &#123; //如果队列中元素个数大于0 if (count &gt; 0) &#123; //获取上一次放入的元素的索引位置 final int putIndex = this.putIndex; //获取上一次取出的元素的索引的位置 int i = takeIndex; do &#123; //遍历每一个元素，如果找到了，就移除元素 if (o.equals(items[i])) &#123; removeAt(i); return true; &#125; //这个的逻辑有啥用？ //可能存在这样一种情况： // 1 null 2 3 4 if (++i == items.length) i = 0; &#125; while (i != putIndex); &#125; return false; &#125; finally &#123; lock.unlock(); &#125;&#125; 11.removeAt123456789101112131415161718192021222324252627282930313233343536373839404142void removeAt(final int removeIndex) &#123; //获取队列的引用 final Object[] items = this.items; //如果要删除的节点恰好是下一个获取元素要拿的索引位置，直接干掉就中 if (removeIndex == takeIndex) &#123; // removing front item; just advance items[takeIndex] = null; if (++takeIndex == items.length) takeIndex = 0; count--; if (itrs != null) itrs.elementDequeued(); &#125; else &#123; //这个时候就需要轮训删除 final int putIndex = this.putIndex; //自旋 for (int i = removeIndex;;) &#123; //删除索引的下一个索引 int next = i + 1; //如果到头了 ，意思就是 ， 那就得 从头再来 if (next == items.length) next = 0; // 断流了 得跳过去 if (next != putIndex) &#123; items[i] = items[next]; i = next; &#125; else &#123; //找到了 删除 退出 items[i] = null; this.putIndex = i; break; &#125; &#125; //元素个数-- count--; //处理迭代器的逻辑 if (itrs != null) itrs.removedAt(removeIndex); &#125; //将往队列放元素，但是因为队列满了阻塞的线程唤醒一个，当然了，也可能队列没有线程 notFull.signal();&#125;","categories":[{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"ThreadPoolExecutor","slug":"JUC/ThreadPoolExecutor","date":"2022-01-11T11:07:47.466Z","updated":"2022-01-11T11:31:41.419Z","comments":true,"path":"2022/01/11/JUC/ThreadPoolExecutor/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/JUC/ThreadPoolExecutor/","excerpt":"","text":"一，线程池的基本使用1.线程池的介绍线程复用，控制最大并发数，管理线程。​ 优点：提高响应速度，避免每次都去创建线程。便于管理，降低资源消耗。​ 2.常用的线程池 Executors.newFixedThreadPool();执行长期任务性能好，创建一个线程池，一池有N个固定的线程，有固定的线程数的线程 Executors.newSingleThreadExecutor();一个任务一个任务的执行，一池一线程 Executors.newCachedThreadPool();执行很多短期异步任务，线程池根据需要创建新线程，但在先前构建的线程可用时将重用它们。可扩容，遇强则强。 ​ 线程池的核心其实都是同一个类ThreadPoolExecutor。​ 注意​ 线程池不允许使用Executors去创建，而是通过ThreadPoolExecutor的方式，规避资源消耗。 说明：Executors返回的线程池对象的弊端如下： FixedThreadPool和SingleThreadPool；允许的请求队列长度为Integer的最大值，可能会堆积大量的请求，从而导致OOM。 CachedThreadPool和ScheduledThreadPool允许的创建线程数量为Integer最大值，可能会创建大量的线程，从而导致OOM。 123456789101112131415161718public static void main(String[] args) &#123; ExecutorService pool1 = Executors.newFixedThreadPool(5);//一个银行网点5个受理业务窗口 ExecutorService pool2 = Executors.newSingleThreadExecutor();//一个银行网点1个受理业务窗口 ExecutorService pool3 = Executors.newCachedThreadPool();//一个银行网点n个受理业务窗口 //3个顾客 try &#123; for (int i = 0; i &lt; 30; i++) &#123; pool3.execute(()-&gt;&#123; System.out.println(Thread.currentThread().getName()+&quot;\\t线程办理业务！&quot;); &#125;); //pool1.submit(()-&gt;&#123;&#125;); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; pool3.shutdown(); &#125;&#125; 3.线程池的7大参数 int corePoolSize, 线程池中的常驻核心线程数 ​ int maximumPoolSize, 能够容纳同时执行的最大线程数，必须大于1 ​ long keepAliveTime, 多余空闲线程存活时间 ​ TimeUnit unit, 上个参数的单位 ​ BlockingQueue workQueue, 任务队列，被提交但是尚未执行的任务 ​ ThreadFactory threadFactory, 表示生成线程池中工作线程的线程工厂，用于创建线程，一般默认 ​ RejectedExecutionHandler handler，拒绝策略，表示当队列满了，并且工作线程大于等于线程池的最大连接数时，如何来拒绝请求执行的runnable的策略。3.1 生产中线程池参数如何设置​ 为什么IO 密集型的设置为 2n, 计算密集型设置为 n+1不对？ 因为核心线程数设置多少要具体情况具体分析,使用线程池的业务场景不同,解决方案自然是不一样的。​ 场景假设​ 假设现在要给 100w 用户发放优惠券,通过线程池异步发送 ​ 假设某线程池执行发优惠券的任务共耗时 50ms,其中 45ms 在io, 5ms 在进行计算(真正的 io 耗时 计算耗时可以通过 记录log 判断时间差值计算出来 取平均值即可 ) ​ 如何设置线程池的参数快速的将这 100w 张券发完？ ​ 核心线程数 = CPU核数 * ((Io耗时 / 计算耗时) + 1) 核心线程数 = 8C * ((45ms / 5ms) +1 ) = 80个 45ms / 5ms 是什么意思？​ CPU 在等待 IO 返回时完全可以将 CPU 时间片拿出来去做其他的计算,45ms 可以多处理 9 个计算任务,再加上原本就有一个 5ms 在计算,也就是说: 一个CPU 核在执行这个 50ms 发券任务时,可以并发的起10个线程去处理任务！那8C CPU 最多同时可以有 8个核心并行的处理任务, 8 _ 10 = 80，一秒钟一个线程可以处理 1000ms / 50ms = 20个任务可以算出线程池执行任务的峰值 qps = 20 _ 80 = 1600，发完100w 张券所需时间: 100w / 1600 = 625S,也就是说大概 10分钟左右就能发完 100w 张券。​ 不太正确的结论: 核心线程数在处理这个任务的情况下可以设置为 80 用来极限的压榨机器CPU 的性能。​ 核心线程数设置为 80,这几乎吃完了所有的 CPU 时间片, CPU 的负载将会达到 100% ; 试想一下生产环境如果你的机器 CPU 负载是 100% , 慌不慌？(CPU 负载打满机器不会宕机, 但没有 CPU 资源来处理用户的请求,表现为服务假死/机器请求半天无反应)​ 设置线程池核心线程数要考虑 CPU 的使用要素​ 每台机器操作系统需要消耗一些 CPU 资源; 假设用了 2% 的CPU 资源; ​ 如果是面向用户的服务,处理用户的请求也是要消耗CPU 资源的,可以通过一些监控系统,看看平时 CPU 在繁忙时间段的负载是多少; 假设用了 10% 的资源; ​ 如果除了发券任务的线程池还有其他线程池在运行,就得把其他线程池消耗的CPU资源也算上,假设用了 13% 的资源; ​ 实际情况一些中间件框架也会用线程池,也会吃一些CPU 资源。 ​ 为什么用线程池没考虑上下文的切换？​ 1ms = 1000us, 一次上下文的切换大概是 1us, 上下文切换的时间跟执行任务的时间比起来可以忽略不计。​ 结论 : CPU核数 * ((Io耗时 / 计算耗时) + 1)​ 这是机器 CPU 负载 100% 时极限的值, 乘以期望的 CPU 负载百分比即可算出实际情况最佳的线程数。​ 3.2 8C16G 的机器需要几台可以抗起 3W 的qps？假设一个 用户领券系统的 qps 在3w左右大部分服务通常的部署在 Tomcat 上, Tomcat 内部也是通过线程来处理用户的请求,Tomcat 也是通过线程池来管理线程, 实际上算出 Tomcat 实际的并发和理想状态能支持的的并发就好了。​ 上个问题分析出来发券接口 50ms 耗时, 8C 的CPU 占用 100%, 不考虑内存 磁盘 网络等其他开销, 线程池极限的QPS 是1600, 这里也不考虑有没有其他线程池或者七七八八的东西消耗 CPU 资源了。假设 CPU 只能维持在 70% 左右的负载；单台机器的 qps 就只能有 1600 * 70% = 1120,就算 1100，3w / 1100 = 27.27 向上取整 大概需要 28 台机器。作为一个有经验的开发人员实际部署的时候绝对要多扩容几台服务器来兜底, 推荐部署 32 - 36 台机器分两个集群部署。​ 3.3 线程池可以先启动最大线程数再将任务放到阻塞队列里么？​ 启动最大线程数再将任务放到阻塞队列的诀窍就在 workQueue 的 offer 方法;我们可以用自己实现的阻塞队列在重写 offer 方法; 在 offer 方法中判断 当前线程数是否大于等于最大线程数，如果不大于就返回 false, 这样就跳过了 execute 方法的第二步, 来到了第三步的创建最大线程数的逻辑。dubbo就是这么干的。​ 4.线程池的工作原理 在创建了线程池后，线程池中的线程数为零。 ​ 当调用execute ()方法添加一个请求任务时，线程池会做出如下判断: ​ 如果正在运行的线程数量小于corePoolSize,那么马上创建线程运行这个任务; 如果正在运行的线程数量大于或等于corePoolSize,那么将这个任务放入队列; 如果这个时候队列满了且正在运行的线程数量还小于maximumPoolSize，那么还是要创建非核心线程立刻运行这个任务; 如果队列满了且正在运行的线程数量大于或等于max imumPoolSize,那么线程池会启动饱和拒绝策略来执行。 ​ 当一个线程完成任务时，它会从队列中取下一个任务来执行。 ​ 当一个线程无事可做超过一定的时间(keepAliveTime) 时，线程会判断: ​ 如果当前运行的线程数大于corePoolSize.那么这个线程就被停掉。所以线程池的所有任务完成后，它最终会收缩到corePoolSize的大小。 ​ 4.1 线程池的状态 4.2 线程池的拒绝策略 AbortPolicy 抛出异常 CallerRunsPolicy 调用者线程执行 DiscardOldestPolicy 丢弃队列中最老的任务，再次尝试提交 DiscardPolicy 直接丢弃 ​ 5. 自定义线程池1234567891011121314151617181920212223242526public static void main(String[] args) &#123; ExecutorService pool = new ThreadPoolExecutor( 2, 5, 3l, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;&gt;(3), Executors.defaultThreadFactory(), //new ThreadPoolExecutor.AbortPolicy() //抛异常 //new ThreadPoolExecutor.CallerRunsPolicy() //main 线程办理业务！ //new ThreadPoolExecutor.DiscardOldestPolicy() //就处理能处理的，剩下的老的直接丢了。 new ThreadPoolExecutor.DiscardPolicy() //如果新来的处理不了，直接就扔了。 ); try &#123; for (int i = 0; i &lt; 30; i++) &#123; pool.execute(() -&gt; &#123; System.out.println(Thread.currentThread().getName() + &quot;\\t线程办理业务！&quot;); &#125;); //pool1.submit(()-&gt;&#123;&#125;); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; pool.shutdown(); &#125;&#125; 二，线程池源码 1. 成员属性/静态属性/构造方法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108//高三位：表示当前线程池运行状态 除去高三位之后的低位：表示当前线程池中所拥有的的线程数量 private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0));//表示在ctl中，低COUNT_BITS位适用于存放当前线程数量的位。 private static final int COUNT_BITS = Integer.SIZE - 3;//线程池所能存放的最大容量 private static final int CAPACITY = (1 &lt;&lt; COUNT_BITS) - 1; //-1左移29位 负数 111 private static final int RUNNING = -1 &lt;&lt; COUNT_BITS;// 0 000 private static final int SHUTDOWN = 0 &lt;&lt; COUNT_BITS;//001 private static final int STOP = 1 &lt;&lt; COUNT_BITS;//010 private static final int TIDYING = 2 &lt;&lt; COUNT_BITS;//011 private static final int TERMINATED = 3 &lt;&lt; COUNT_BITS; // 获取当前线程池运行状态 private static int runStateOf(int c) &#123; return c &amp; ~COUNT_MASK; &#125;//获取当前线程池线程数量 private static int workerCountOf(int c) &#123; return c &amp; COUNT_MASK; &#125;//用在重置当前线程ctl值时，会用到 rs表示线程池状态 wc表示线程池worker数量 private static int ctlOf(int rs, int wc) &#123; return rs | wc; &#125;//cas的方式让ctl值+1 private boolean compareAndIncrementWorkerCount(int expect) &#123; return ctl.compareAndSet(expect, expect + 1); &#125;//cas的方式让ctl-1 private boolean compareAndDecrementWorkerCount(int expect) &#123; return ctl.compareAndSet(expect, expect - 1); &#125;//将ctl值-1 private void decrementWorkerCount() &#123; ctl.addAndGet(-1); &#125;//当线程池中的线程达到核心线程数时，在提交任务，就会提交到任务队列 private final BlockingQueue&lt;Runnable&gt; workQueue; //线程池中的全局锁，增加worker，减少worker，修改线程池运行状态 private final ReentrantLock mainLock = new ReentrantLock(); //真正存放worker-&gt;thread的地方 private final HashSet&lt;Worker&gt; workers = new HashSet&lt;&gt;(); /* 线程池中提供了一个对外方法，awaitTermination(long time,TimeUnit),该方法调用会被阻塞， 并且在以下几种情况任意一种发生时都会导致该方法的执行: 即shutdown方法被调用之后， 或者参数中定义的timeout时间到达或者当前线程被打断，这几种情况任意一个发生了都会导致该方法在所有任务完成之后才执行。 第一个参数是long类型的超时时间，第二个参数可以为该时间指定单位。*/ private final Condition termination = mainLock.newCondition(); //记录线程池生命周期内，线程最大值 private int largestPoolSize; //记录线程池所完成的任务总数，当worker退出时，会将worker完成的任务累积到这里 private long completedTaskCount; //创建线程池会使用到线程工厂，当我们使用Executor.newFix ... /newCache ... 使用的DefaultThreadFactory，//生成的线程名不容易分析执行的是哪里的业务//一般不建议使用使用自带的，推荐自己实现这个接口 private volatile ThreadFactory threadFactory; //拒绝策略，默认是采用抛出异常 private volatile RejectedExecutionHandler handler; //空闲线程存活时间 ：allowCoreThreadTimeOut=false时，会维护核心线程数量内的线程存活，超出部分会超时。//allowCoreThreadTimeOut=true时，核心数量内的线程也会被回收。 private volatile long keepAliveTime; //控制核心线程是否可以被回收，true可以，false不可以。 private volatile boolean allowCoreThreadTimeOut; //核心线程数限制 private volatile int corePoolSize; //最大线程数限制 private volatile int maximumPoolSize; //默认的拒绝策略，抛异常的方式 private static final RejectedExecutionHandler defaultHandler = new AbortPolicy(); public ThreadPoolExecutor(int corePoolSize, //核心线程数 int maximumPoolSize, //最大线程数 long keepAliveTime, //空闲等待时间 TimeUnit unit, //时间单位 BlockingQueue&lt;Runnable&gt; workQueue, //任务队列 ThreadFactory threadFactory, //线程工厂 RejectedExecutionHandler handler //拒绝策略 ) &#123; if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler; &#125; 2.内部类worker线程池中的线程实际上都封装成了一个个worker来执行。​ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667 private final class Worker extends AbstractQueuedSynchronizer implements Runnable &#123; private static final long serialVersionUID = 6138294804551838833L; //worker内部封装的工作线程 @SuppressWarnings(&quot;serial&quot;) final Thread thread; //假设firstTask不为空，当worker启动后（worker内部的线程启动后）会优先执行firstTask，//当执行完firstTask后，会去queue中去获取下一个任务 @SuppressWarnings(&quot;serial&quot;) Runnable firstTask; //记录当前worker所完成的任务数量 volatile long completedTasks; //firstTask可以为空，启动后会到queue中获取 Worker(Runnable firstTask) &#123; setState(-1); // 设置aqs独占模式为 初始化中状态，不能被抢占锁 this.firstTask = firstTask; this.thread = getThreadFactory().newThread(this); //使用线程工厂创建了一个线程，并且将当前worker指定为 //runnable，并且说当前thread启动的时候，会以worker.run()为入口。 &#125; /** */ public void run() &#123; //调用了threadPoolExecutor的方法 runWorker(this); &#125; //当前worker的独占锁是否被独占 protected boolean isHeldExclusively() &#123; return getState() != 0; &#125;//尝试去占用worker的独占锁 protected boolean tryAcquire(int unused) &#123; if (compareAndSetState(0, 1)) &#123; setExclusiveOwnerThread(Thread.currentThread()); return true; &#125; return false; &#125;//尝试去释放worker的独占锁 protected boolean tryRelease(int unused) &#123; setExclusiveOwnerThread(null); setState(0); return true; &#125; public void lock() &#123; acquire(1); &#125; public boolean tryLock() &#123; return tryAcquire(1); &#125; public void unlock() &#123; release(1); &#125; public boolean isLocked() &#123; return isHeldExclusively(); &#125; void interruptIfStarted() &#123; Thread t; if (getState() &gt;= 0 &amp;&amp; (t = thread) != null &amp;&amp; !t.isInterrupted()) &#123; try &#123; t.interrupt(); &#125; catch (SecurityException ignore) &#123; &#125; &#125; &#125; &#125; 3.execute当调用execute()来提交一个任务的时候，首先会判断如果当前线程数量小于核心线程数，此次提交任务会创建一个核心线程执行任务。如果提交失败，会继续判断：如果当前线程池处于RUNNING状态，尝试将task放入到workQueue中。如果还是失败，会继续判断：达到了最大线程数，所以执行拒绝策略。 123456789101112131415161718192021222324252627282930313233343536373839404142434445public void execute(Runnable command) &#123; //如果任务为空，抛出空指针异常 if (command == null) throw new NullPointerException(); //获取ctl的最新值，高三位表示此线程池的状态，低位表示当前线程池的线程数量 int c = ctl.get(); //如果当前线程数小于核心线程数，此任务会交给一个核心线程去执行 if (workerCountOf(c) &lt; corePoolSize) &#123; //创建worker成功以后，会把这个任务交给worker的firstTask，由worker包装的核心线程执行 if (addWorker(command, true)) return; //获取ctl的最新值，高三位表示此线程池的状态，低位表示当前线程池的线程数量 c = ctl.get(); //执行到这里，说明addworker失败 /** * 1.存在并发 * 2.线程池的状态非running，shutdown状态下也会创建成功，前提是firsttask==null,队列！=null */ &#125; /** * 执行到这里，说明，此时线程池已经达到核心线程数 addworker失败 */ //如果线程池没有被shutdown，也就是正常运行，说明此时存在并发，那就把这个任务放到队列 if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; //执行到这里说明任务提交到队列成功 //再次获取ctl 线程池的状态 和 工作线程数 int recheck = ctl.get(); //如果此时的线程池状态不在是running 需要删除刚刚提交的任务 //删除成功说明此时任务尚未被执行，删除失败代表此时已经有核心线程正在执行任务 if (! isRunning(recheck) &amp;&amp; remove(command)) //进入这里说明线程池的状态发生改变，并且任务删除成功，启动拒绝策略 reject(command); //当前是running状态 或者当前是非running状态，并且任务已经在被核心线程执行 else if (workerCountOf(recheck) == 0) //线程池是running状态但是线程池的工作线程数==0 addWorker(null, false); &#125; /** * 执行到这里有几种情况： * 1.任务入队失败，说明队列满了 * 2.当前线程池是非running状态，这个时候commond！=null，addworker一定返回false */ else if (!addWorker(command, false)) //达到了最大线程数，执行拒绝策略 reject(command);&#125; 4.addWorker自旋去申请一个工作线程，申请成功以后，将线程包装成一个worker，加入到worker队列中，并启动任务。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465//firstTask可以为空，自动取queue拿任务， core：采用的线程数限制，true，核心线程数，false，非核心线程 private boolean addWorker(Runnable firstTask, boolean core) &#123; retry: for (int c = ctl.get();;) &#123;//获取当前ctl保存到c if (runStateAtLeast(c, SHUTDOWN)//当前线程池不是RUNNING &amp;&amp; (runStateAtLeast(c, STOP) || firstTask != null || workQueue.isEmpty())) //当前线程池状态为SHUTDOWN || 任务不为空 || 队列为空 return false; //自旋：来到这里说明线程池的状态允许执行任务 for (;;) &#123; //如果当前线程池的数量大于等于最大线程数 &amp;&amp; 工作线程数达到5亿 if (workerCountOf(c) &gt;= ((core ? corePoolSize : maximumPoolSize) &amp; COUNT_MASK)) return false; //如果cas使工作线程数+1成功，相当于申请到了一个工作线程 if (compareAndIncrementWorkerCount(c)) break retry; c = ctl.get(); //来到这里说明： //1.并发导致申请工作线程失败 //2.线程池状态改变 if (runStateAtLeast(c, SHUTDOWN))//如果线程池状态没问题，再次尝试获取工作线程 continue retry; &#125; &#125; boolean workerStarted = false;//任务是否开始执行 boolean workerAdded = false;//任务是否添加到池子中 Worker w = null; try &#123; w = new Worker(firstTask); //包装成一个worker final Thread t = w.thread; if (t != null) &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; int c = ctl.get(); //如果当前线程池状态是RUNNING，任务不为空 if (isRunning(c) || (runStateLessThan(c, STOP) &amp;&amp; firstTask == null)) &#123; //如果线程状态不是就绪 if (t.getState() != Thread.State.NEW) throw new IllegalThreadStateException(); workers.add(w);//加入到队列 workerAdded = true; int s = workers.size(); if (s &gt; largestPoolSize) largestPoolSize = s; &#125; &#125; finally &#123; mainLock.unlock(); &#125; if (workerAdded) &#123; t.start();//启动任务 workerStarted = true; &#125; &#125; &#125; finally &#123; if (! workerStarted) //如果启动失败，释放掉刚才自旋获取的工作线程 addWorkerFailed(w); &#125; return workerStarted; &#125; 5.runworker执行任务的线程去获取独占锁执行任务，执行完成后将线程完成的任务书+1并释放锁，然后执行退出逻辑。 1234567891011121314151617181920212223242526272829303132333435363738394041 final void runWorker(Worker w) &#123; Thread wt = Thread.currentThread(); Runnable task = w.firstTask; w.firstTask = null; w.unlock(); //为什么先调用unlock()?为了强制刷新当前持有锁的线程为空。 boolean completedAbruptly = true;//是否突然退出？true表示发生异常，当前线程突然退出的，false表示正常退出。 try &#123;//任务不为空 || 在queue中获取任务成功 while (task != null || (task = getTask()) != null) &#123; w.lock();//设置独占锁为当前线程 //为什么要设置独占锁？防止shutdown时会判断当前work状态 //如果线程池状态为STOP || //（获取当前线程中断状态并给当前线程设置中断信号未false &amp;&amp; 线程池状态为STOP）&amp;&amp; 线程未设置中断状态 if ((runStateAtLeast(ctl.get(), STOP) || (Thread.interrupted() &amp;&amp; runStateAtLeast(ctl.get(), STOP))) &amp;&amp; !wt.isInterrupted()) wt.interrupt(); try &#123; //钩子方法 beforeExecute(wt, task); try &#123; //执行任务 task.run(); afterExecute(task, null);//钩子方法 &#125; catch (Throwable ex) &#123; afterExecute(task, ex);//钩子方法 throw ex; &#125; &#125; finally &#123; task = null; w.completedTasks++;//将线程完成的任务数+1 w.unlock();//释放锁 &#125; &#125; completedAbruptly = false;//异常打断标记设置为false &#125; finally &#123;//执行退出逻辑 processWorkerExit(w, completedAbruptly); &#125; &#125; 6.getTask自旋的方式去队列获取任务 123456789101112131415161718192021222324252627282930313233343536373839404142private Runnable getTask() &#123; boolean timedOut = false; for (;;) &#123; int c = ctl.get(); //如果当前线程池是非RUNNING &amp;&amp; 当前状态大于等于 STOP || queue为空 if (runStateAtLeast(c, SHUTDOWN) &amp;&amp; (runStateAtLeast(c, STOP) || workQueue.isEmpty())) &#123; decrementWorkerCount(); //将获取到的工作线程还回去 return null; &#125; /* 能够来到这里的情况： 1.RUNNING 2.SHUTDOWN但是队列不为空 */ int wc = workerCountOf(c); //获取到worker数量 //如果是核心线程就不需要超时时间 boolean timed = allowCoreThreadTimeOut || wc &gt; corePoolSize; //条件1说明 当前工作线程数已经大于最大线程数 //条件2说明 当前线程可以被回收 ||wc==1且任务队列已经空了，当前线程可以放心退出。 if ((wc &gt; maximumPoolSize || (timed &amp;&amp; timedOut)) &amp;&amp; (wc &gt; 1 || workQueue.isEmpty())) &#123; if (compareAndDecrementWorkerCount(c))//如果成功减少worker数量 return null; continue; &#125; try &#123; Runnable r = timed ? workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) : workQueue.take();//去队列获取任务 if (r != null)//任务不为空，返回任务 return r; timedOut = true;//否则标记为超时 &#125; catch (InterruptedException retry) &#123; timedOut = false; &#125; &#125;&#125; 7.shutdown他会先尝试去停止所有空闲的线程，然后执行tryTerminate()，tryTerminate()会通过自旋的方式判断，当队列里面的任务都执行完了，销毁掉执行完任务的线程，当最后一个线程退出的时候，将线程池状态修改为TIDYING，最终修改为TERMINATED。 12345678910111213public void shutdown() &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock();//获取全局锁 try &#123; checkShutdownAccess();//权限判断 advanceRunState(SHUTDOWN);//自旋设置线程池状态为SHUTDOWN interruptIdleWorkers();//遍历所有的线程，中断空闲线程 onShutdown(); // 空方法，子类可以扩展 &#125; finally &#123; mainLock.unlock(); &#125; tryTerminate();&#125; 8.shutdownNow自旋设置线程池状态为STOP，遍历中断（interrupt的方式）线程池中所有线程，导出所有未处理的任务，然后执行tryTerminate()。 123456789101112131415public List&lt;Runnable&gt; shutdownNow() &#123; List&lt;Runnable&gt; tasks; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; checkShutdownAccess(); advanceRunState(STOP);//自旋设置线程池状态为STOP interruptWorkers();//遍历中断线程池中所有线程 tasks = drainQueue();//导出所有未处理的任务 &#125; finally &#123; mainLock.unlock(); &#125; tryTerminate(); return tasks;&#125; 9.tryTerminate通过自旋的方式，当非空闲线程执行完任务，就会来到这里被中断，最后一个退出的线程，设置线程池状态为TIDYING，最终设置为设置为TERMINATED，并调用termination.signalAll();唤醒调用awaitTermination()的线程继续执行程序。 123456789101112131415161718192021222324252627282930313233343536373839404142434445final void tryTerminate() &#123; for (;;) &#123;//自旋 int c = ctl.get(); /* 线程池RUNNING ||线程池大于等于TIDYING(说明已经有线程在执行这个方法) || 线程池大于STOP状态 且 队列不为空 */ if (isRunning(c) || runStateAtLeast(c, TIDYING) || (runStateLessThan(c, STOP) &amp;&amp; ! workQueue.isEmpty())) return; /* 什么时候执行到这里？ 1.线程池状态大于等于STOP 2.线程池状态大于等于STOP并且队列为空 当前线程池中的线程数量&gt;0 */ if (workerCountOf(c) != 0) &#123; //非空闲线程当执行完任务，最终也会执行到这里 interruptIdleWorkers(ONLY_ONE);//中断一个空闲线程 return; &#125; //最后一个退出的线程会来到这里 final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; //强制设置线程池状态为TIDYING if (ctl.compareAndSet(c, ctlOf(TIDYING, 0))) &#123; try &#123; terminated(); //钩子方法 &#125; finally &#123; //设置为TERMINATED ctl.set(ctlOf(TERMINATED, 0)); //唤醒调用awaitTermination()的线程 termination.signalAll(); &#125; return; &#125; &#125; finally &#123; mainLock.unlock(); &#125; // else retry on failed CAS &#125;&#125;","categories":[{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"CyclicBarrier","slug":"JUC/CyclicBarrier","date":"2022-01-11T11:07:39.796Z","updated":"2022-01-11T11:28:00.513Z","comments":true,"path":"2022/01/11/JUC/CyclicBarrier/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/JUC/CyclicBarrier/","excerpt":"","text":"一，使用CyclicBarrier 的字面意思是可循环使用（Cyclic）的屏障（Barrier）。它要做的事情是，让一组线程到达一个屏障（也可以叫同步点）时被阻塞，直到最后一个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程才会继续工作。CyclicBarrier 默认的构造方法是 CyclicBarrier(int parties)，其参数表示屏障拦截的线程数量，每个线程调用 await 方法告诉 CyclicBarrier 当前线程已经到达了屏障，然后当前线程被阻塞。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class CyclicBarrierTest01 &#123; /** * 案例： * 模拟过气游戏 “王者荣耀” 游戏开始逻辑 */ public static void main(String[] args) &#123; //第一步：定义玩家，定义5个 String[] heros = &#123;&quot;安琪拉&quot;,&quot;亚瑟&quot;,&quot;马超&quot;,&quot;张飞&quot;, &quot;刘备&quot;&#125;; //第二步：创建固定线程数量的线程池，线程数量为5 ExecutorService service = Executors.newFixedThreadPool(5); //第三步：创建barrier，parties 设置为5 CyclicBarrier barrier = new CyclicBarrier(5); //第四步：通过for循环开启5任务，模拟开始游戏，传递给每个任务 英雄名称和barrier for(int i = 0; i &lt; 5; i++) &#123; service.execute(new Player(heros[i], barrier)); &#125; service.shutdown(); &#125; static class Player implements Runnable &#123; private String hero; private CyclicBarrier barrier; public Player(String hero, CyclicBarrier barrier) &#123; this.hero = hero; this.barrier = barrier; &#125; @Override public void run() &#123; try &#123; //每个玩家加载进度不一样，这里使用随机数来模拟！ TimeUnit.SECONDS.sleep(new Random().nextInt(10)); System.out.println(hero + &quot;：加载进度100%，等待其他玩家加载完成中...&quot;); barrier.await(); System.out.println(hero + &quot;：发现所有英雄加载完成，开始战斗吧！&quot;); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (BrokenBarrierException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 对于指定计数值 parties，若由于某种原因，没有足够的线程调用 CyclicBarrier 的 await，则所有调用 await 的线程都会被阻塞； 同样的 CyclicBarrier 也可以调用 await(timeout, unit)，设置超时时间，在设定时间内，如果没有足够线程到达，则解除阻塞状态，继续工作； 通过 reset 重置计数，会使得进入 await 的线程出现BrokenBarrierException； 如 果 采 用 是 CyclicBarrier(int parties, RunnablebarrierAction) 构造方法，执行 barrierAction 操作的是最后一个到达的线程。 二，源码 2.1 成员变量 属性 内部类1234567891011121314151617181920212223/** * 每个barrier都表示为一个generation实例。当barrier触发trip条件或重置时generation随之改变。 * 使用barrier时有很多generation与线程关联-由于不确定性的方式，锁可能分配给等待的线程。 * 但是在同一时间只有一个是活跃的generation(通过count变量确定)，并且其余的要么被销毁，要么被trip条件等待。 * 如果有一个中断，但没有随后的重置，就不需要有活跃的generation。 */private static class Generation &#123; boolean broken = false;&#125;//因为barrier实现是依赖于Condition条件队列的，condition条件队列必须依赖lock才能使用。private final ReentrantLock lock = new ReentrantLock();//线程挂起实现使用的 condition 队列。条件：当前代所有线程到位，这个条件队列内的线程 才会被唤醒。private final Condition trip = lock.newCondition();//Barrier需要参与进来的线程数量private final int parties;//当前代 最后一个到位的线程 需要执行的事件private final Runnable barrierCommand;//表示barrier对象 当前 “代”private Generation generation = new Generation();//表示当前“代”还有多少个线程 未到位。//初始值为partiesprivate int count; 2.2 构造器123456789101112//构造函数，指定参与线程数，并在所有线程到达barrier之后执行给定的barrierAction逻辑public CyclicBarrier(int parties, Runnable barrierAction) &#123; if (parties &lt;= 0) throw new IllegalArgumentException(); this.parties = parties; this.count = parties; this.barrierCommand = barrierAction;&#125;//构造函数，指定参与线程数public CyclicBarrier(int parties) &#123; this(parties, null);&#125; 2.3 await12345678//等待所有的参与者到达barrierpublic int await() throws InterruptedException, BrokenBarrierException &#123; try &#123; return dowait(false, 0L); &#125; catch (TimeoutException toe) &#123; throw new Error(toe); // cannot happen &#125;&#125; 2.4 dowait123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133/** * Main barrier code, covering the various policies. * timed：表示当前调用await方法的线程是否指定了 超时时长，如果true 表示 线程是响应超时的 * nanos：线程等待超时时长 纳秒，如果timed == false ,nanos == 0 */private int dowait(boolean timed, long nanos) throws InterruptedException, BrokenBarrierException,TimeoutException &#123; //获取barrier全局锁对象 final ReentrantLock lock = this.lock; //加锁 //为什么要加锁呢？ //因为 barrier的挂起 和 唤醒 依赖的组件是 condition。 lock.lock(); try &#123; //获取barrier当前的 “代” final Generation g = generation; //如果当前代是已经被打破状态，则当前调用await方法的线程，直接抛出Broken异常 if (g.broken) throw new BrokenBarrierException(); //如果当前线程的中断标记位 为 true，则打破当前代，然后当前线程抛出 中断异常 if (Thread.interrupted()) &#123; //1.设置当前代的状态为broken状态 2.唤醒在trip 条件队列内的线程 breakBarrier(); throw new InterruptedException(); &#125; //执行到这里，说明 当前线程中断状态是正常的 false， 当前代的broken为 false（未打破状态） //正常逻辑... //假设 parties 给的是 5，那么index对应的值为 4,3,2,1,0 int index = --count; //条件成立：说明当前线程是最后一个到达barrier的线程，此时需要做什么呢？ if (index == 0) &#123; // tripped //标记：true表示 最后一个线程 执行cmd时未抛异常。 false，表示最后一个线程执行cmd时抛出异常了. //cmd就是创建 barrier对象时 指定的第二个 Runnable接口实现，这个可以为null boolean ranAction = false; try &#123; final Runnable command = barrierCommand; //条件成立：说明创建barrier对象时 指定 Runnable接口了，这个时候最后一个到达的线程 就需要执行这个接口 if (command != null) command.run(); //command.run()未抛出异常的话，那么线程会执行到这里。 ranAction = true; //开启新的一代 //1.唤醒trip条件队列内挂起的线程，被唤醒的线程 会依次 获取到lock，然后依次退出await方法。 //2.重置count 为 parties //3.创建一个新的generation对象，表示新的一代 nextGeneration(); //返回0，因为当前线程是此 代 最后一个到达的线程，所以Index == 0 return 0; &#125; finally &#123; if (!ranAction) //如果command.run()执行抛出异常的话，会进入到这里。 breakBarrier(); &#125; &#125; //执行到这里，说明当前线程 并不是最后一个到达Barrier的线程..此时需要进入一个自旋中. // loop until tripped, broken, interrupted, or timed out //自旋，一直到 条件满足、当前代被打破、线程被中断，等待超时 for (;;) &#123; try &#123; //条件成立：说明当前线程是不指定超时时间的 if (!timed) //当前线程 会 释放掉lock，然后进入到trip条件队列的尾部，然后挂起自己，等待被唤醒。 trip.await(); else if (nanos &gt; 0L) //说明当前线程调用await方法时 是指定了 超时时间的！ nanos = trip.awaitNanos(nanos); &#125; catch (InterruptedException ie) &#123; //抛出中断异常，会进来这里。 //什么时候会抛出InterruptedException异常呢？ //Node节点在 条件队列内 时 收到中断信号时 会抛出中断异常！ //条件一：g == generation 成立，说明当前代并没有变化。 //条件二：! g.broken 当前代如果没有被打破，那么当前线程就去打破，并且抛出异常.. if (g == generation &amp;&amp; ! g.broken) &#123; breakBarrier(); throw ie; &#125; else &#123; //执行到else有几种情况？ //1.代发生了变化，这个时候就不需要抛出中断异常了，因为 代已经更新了，这里唤醒后就走正常逻辑了..只不过设置下 中断标记。 //2.代没有发生变化，但是代被打破了，此时也不用返回中断异常，执行到下面的时候会抛出 brokenBarrier异常。也记录下中断标记位。 // We&#x27;re about to finish waiting even if we had not // been interrupted, so this interrupt is deemed to // &quot;belong&quot; to subsequent execution. Thread.currentThread().interrupt(); &#125; &#125; //唤醒后，执行到这里，有几种情况？ //1.正常情况，当前barrier开启了新的一代（trip.signalAll()） //2.当前Generation被打破，此时也会唤醒所有在trip上挂起的线程 //3.当前线程trip中等待超时，然后主动转移到 阻塞队列 然后获取到锁 唤醒。 //条件成立：当前代已经被打破 if (g.broken) //线程唤醒后依次抛出BrokenBarrier异常。 throw new BrokenBarrierException(); //唤醒后，执行到这里，有几种情况？ //1.正常情况，当前barrier开启了新的一代（trip.signalAll()） //3.当前线程trip中等待超时，然后主动转移到 阻塞队列 然后获取到锁 唤醒。 //条件成立：说明当前线程挂起期间，最后一个线程到位了，然后触发了开启新的一代的逻辑，此时唤醒trip条件队列内的线程。 if (g != generation) //返回当前线程的index。 return index; //唤醒后，执行到这里，有几种情况？ //3.当前线程trip中等待超时，然后主动转移到 阻塞队列 然后获取到锁 唤醒。 if (timed &amp;&amp; nanos &lt;= 0L) &#123; //打破barrier breakBarrier(); //抛出超时异常. throw new TimeoutException(); &#125; &#125; &#125; finally &#123; lock.unlock(); &#125;&#125; 2.5 breakBarrier打破barrier屏障，再屏障内的线程 都会抛出异常。 123456789private void breakBarrier() &#123; //将代中的broken设置为true，表示这一代是被打破了的，再来到这一代的线程，直接抛出异常. generation.broken = true; //重置count为parties count = parties; //将在trip条件队列内挂起的线程 全部唤醒，唤醒后的线程 会检查当前代 是否是打破的， //如果是打破的话，接下来的逻辑和 开启下一代 唤醒的逻辑不一样. trip.signalAll();&#125; 2.6 nextGeneration开启下一代，当这一代所有的线程到位后（假设barrierCommand不为空，还需要最后一个线程执行完事件），会调用nextGeneration开启新的一代。 123456789101112private void nextGeneration() &#123; //将在trip条件队列内挂起的线程 全部唤醒 // signal completion of last generation trip.signalAll(); //重置count为parties // set up next generation count = parties; //开启新的一代..使用一个新的 generation对象，表示新的一代，新的一代和上一代 没有任何关系。 generation = new Generation();&#125;","categories":[{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"Semaphore","slug":"JUC/Semaphore","date":"2022-01-11T11:07:31.422Z","updated":"2022-01-11T11:32:49.088Z","comments":true,"path":"2022/01/11/JUC/Semaphore/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/JUC/Semaphore/","excerpt":"","text":"一，使用12345678910111213141516171819202122232425262728293031323334353637383940414243@Testpublic void test3()throws InterruptedException&#123; final Semaphore semaphore = new Semaphore(2,true); new Thread(()-&gt;&#123; try &#123; semaphore.acquire(); System.out.println(&quot;线程A获取通行证成功！&quot;); TimeUnit.SECONDS.sleep(10); &#125;catch (Exception e)&#123; &#125;finally &#123; semaphore.release(); &#125; &#125;).start(); TimeUnit.MICROSECONDS.sleep(200); new Thread(()-&gt;&#123; try &#123; semaphore.acquire(); System.out.println(&quot;线程B获取通行证成功！&quot;); TimeUnit.SECONDS.sleep(10); &#125;catch (Exception e)&#123; &#125;finally &#123; semaphore.release(); &#125; &#125;).start(); TimeUnit.MILLISECONDS.sleep(200); new Thread(() -&gt;&#123; try &#123; semaphore.acquire(); System.out.println(&quot;线程C获取通行证成功!&quot;); &#125; catch (InterruptedException e) &#123; &#125;finally &#123; semaphore.release(); &#125; &#125;).start(); while (true)&#123;&#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public class Pool &#123; /** 可同时访问资源的最大线程数*/ private static final int MAX_AVAILABLE = 100; /** 信号量 表示：可获取的对象通行证*/ private final Semaphore available = new Semaphore(MAX_AVAILABLE, true); /** 共享资源，可以想象成 items 数组内存储的都是Connection对象 模拟是链接池*/ protected Object[] items = new Object[MAX_AVAILABLE]; /** 共享资源占用情况，与items数组一一对应，比如：items[0]对象被外部线程占用，那么 used[0] == true，否则used[0] == false*/ protected boolean[] used = new boolean[MAX_AVAILABLE]; /** * 获取一个空闲对象 * 如果当前池中无空闲对象，则等待..直到有空闲对象为止 */ public Object getItem() throws InterruptedException &#123; available.acquire(); return getNextAvailableItem(); &#125; /** * 归还对象到池中 */ public void putItem(Object x) &#123; if (markAsUnused(x)) available.release(); &#125; /** * 获取池内一个空闲对象，获取成功则返回Object，失败返回Null * 成功后将对应的 used[i] = true */ private synchronized Object getNextAvailableItem() &#123; for (int i = 0; i &lt; MAX_AVAILABLE; ++i) &#123; if (!used[i]) &#123; used[i] = true; return items[i]; &#125; &#125; return null; &#125; /** * 归还对象到池中，归还成功返回true * 归还失败： * 1.池中不存在该对象引用，返回false * 2.池中存在该对象引用，但该对象目前状态为空闲状态，也返回false */ private synchronized boolean markAsUnused(Object item) &#123; for (int i = 0; i &lt; MAX_AVAILABLE; ++i) &#123; if (item == items[i]) &#123; if (used[i]) &#123; used[i] = false; return true; &#125; else return false; &#125; &#125; return false; &#125;&#125; 二，源码 1.构造器1234567891011121314//默认是非公平锁public Semaphore(int permits) &#123; sync = new NonfairSync(permits);&#125;//可以指定为公平锁的构造器public Semaphore(int permits, boolean fair) &#123; sync = fair ? new FairSync(permits) : new NonfairSync(permits);&#125;//=======================//Sync(int permits) &#123; setState(permits);&#125; 2.acquire1234public void acquire() throws InterruptedException &#123; //去抢占锁，默认传1，如果想要传其他参数，可以手动指定 sync.acquireSharedInterruptibly(1);&#125; 3.AQS.acquireSharedInterruptibly1234567891011public final void acquireSharedInterruptibly(int arg) throws InterruptedException &#123; //如果当前线程已经被中断 抛出中断异常 if (Thread.interrupted()) throw new InterruptedException(); //小于0 的情况有两种： //锁没有被持有 或者 持有锁的线程不是当前线程 //当前剩下的证书不足以支持当前线程本次获取 if (tryAcquireShared(arg) &lt; 0) doAcquireSharedInterruptibly(arg);&#125; 4.Semaphore.FairSync.tryAcquireShared123456789101112131415161718protected int tryAcquireShared(int acquires) &#123; for (;;) &#123; // 当前队列还有元素，头节点的下一个节点不是空节点&amp;&amp;当前线程的节点不是头节点的下一个节点 if (hasQueuedPredecessors()) //返回-1 return -1; //获取当前最新的state值 int available = getState(); //用state-传入的值 int remaining = available - acquires; //条件1成立 ：当前剩下的证书不足以支持当前线程获取 //条件2成立：前置条件：当前剩下的证书足够支持当前线程持有。 //如果cas去设置新的state成功，返回 if (remaining &lt; 0 || compareAndSetState(available, remaining)) return remaining; &#125;&#125; 5.AQS.hasQueuedPredecessors判断当前AQS阻塞队列里面是否有等待的线程 123456789101112131415public final boolean hasQueuedPredecessors() &#123; Node t = tail; // Read fields in reverse initialization order Node h = head; Node s; /* 1.h!=t:头节点不等于尾结点，说明当前队列还有元素 2.头节点的下一个节点是空 || 头节点的下一个节点不是空节点&amp;&amp;当前线程的节点不是头节点的下一个节点 总结一下：如果返回true： 当前队列还有元素，头节点的下一个节点不是空节点&amp;&amp;当前线程的节点不是头节点的下一个节点 */ return h != t &amp;&amp; ((s = h.next) == null || s.thread != Thread.currentThread());&#125; 6.AQS.doAcquireSharedInterruptibly123456789101112131415161718192021222324252627282930313233private void doAcquireSharedInterruptibly(int arg) throws InterruptedException &#123; //将当前线程构建成一个共享节点入队 final Node node = addWaiter(Node.SHARED); boolean failed = true; try &#123; for (;;) &#123; //获取当前节点的前置节点 final Node p = node.predecessor(); //如果前置节点是头节点 if (p == head) &#123; //尝试去获取锁 int r = tryAcquireShared(arg); //大于0说明成功拿到了锁 if (r &gt;= 0) &#123; //设置头节点并向后传播唤醒 setHeadAndPropagate(node, r); //原头节点出队 p.next = null; // help GC failed = false; return; &#125; &#125; //如果当前节点的前驱节点不是头节点，给当前线程找一个好爸爸 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) throw new InterruptedException(); &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; 7.AQS.setHeadAndPropagate1234567891011121314private void setHeadAndPropagate(Node node, int propagate) &#123; Node h = head; // Record old head for check below //设置node为头节点 setHead(node); if (propagate &gt; 0 || h == null || h.waitStatus &lt; 0 || (h = head) == null || h.waitStatus &lt; 0) &#123; Node s = node.next; //如果当前节点的下一个节点为空 || 当前节点的下一个节点不为空且当前节点的下一个节点是共享节点 if (s == null || s.isShared()) //执行向后唤醒逻辑 doReleaseShared(); &#125;&#125; 8.release123public void release() &#123; sync.releaseShared(1);&#125; 9.releaseShared123456789public final boolean releaseShared(int arg) &#123; //如果尝试释放锁成功 if (tryReleaseShared(arg)) &#123; //执行向后唤醒逻辑 doReleaseShared(); return true; &#125; return false;&#125; 10.tryReleaseShared12345678910111213protected final boolean tryReleaseShared(int releases) &#123; for (;;) &#123; //获取当前最新的state int current = getState(); //将当前的state 和释放的许可证个数相加 int next = current + releases; if (next &lt; current) // overflow throw new Error(&quot;Maximum permit count exceeded&quot;); //cas成功 返回true if (compareAndSetState(current, next)) return true; &#125;&#125; 11.doReleaseShared1234567891011121314151617181920212223242526272829private void doReleaseShared() &#123; for (;;) &#123; //获取头节点 Node h = head; //头节点不为空 &amp;&amp; 头节点不是尾结点 if (h != null &amp;&amp; h != tail) &#123; int ws = h.waitStatus; //如果头结点的等待状态 是 -1 if (ws == Node.SIGNAL) &#123; //如果cas 设置头节点 -1 ， 0 失败 //为啥会失败？其他线程获取到锁以后执行向后唤醒逻辑了 if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) continue; // 唤醒节点的线程 unparkSuccessor(h); &#125; //如果等待状态 ==0 且cas 设置 头节点的等待状态为向后传播失败 else if (ws == 0 &amp;&amp; !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) continue; // loop on failed CAS &#125; //如果头节点没变 也就是说 ，唤醒的后面的节点 还没来得及将自己设置为头节点 ，跳出循环 。 //为什么可以直接跳出？不怕向后唤醒中断么？ 不怕 ，首先 ，极端情况已经都判断完了 if (h == head) // loop if head changed break; &#125;&#125;","categories":[{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"CountDownLatch","slug":"JUC/CountDownLatch","date":"2022-01-11T11:07:22.214Z","updated":"2022-01-11T11:27:37.985Z","comments":true,"path":"2022/01/11/JUC/CountDownLatch/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/JUC/CountDownLatch/","excerpt":"","text":"一，源码​ 对于 CountDownLatch，我们仅仅需要关心两个方法，一个是 countDown() 方法，另一个是 await() 方法。countDown() 方法每次调用都会将 state 减 1，直到state 的值为 0；而 await 是一个阻塞方法，当 state 减为 0 的时候，await 方法才会返回。await 可以被多个线程调用，所有调用了await 方法的线程阻塞在 AQS 的阻塞队列中，等待条件满足（state == 0），将线程从队列中一个个唤醒过来。 1.内部类12345678910111213141516171819202122232425262728293031private static final class Sync extends AbstractQueuedSynchronizer &#123; private static final long serialVersionUID = 4982264981922014374L; //CountDownLatch中的计数其实就是AQS的state Sync(int count) &#123; setState(count); &#125; int getCount() &#123; return getState(); &#125; protected int tryAcquireShared(int acquires) &#123; //如果state =0 返回1 否则返回 0 return (getState() == 0) ? 1 : -1; &#125; protected boolean tryReleaseShared(int releases) &#123; for (;;) &#123; //获取最新的state int c = getState(); //如果state==0 返回false if (c == 0) return false; int nextc = c-1; //如果cas成功，且c=1，返回true if (compareAndSetState(c, nextc)) return nextc == 0; &#125; &#125;&#125; 2.构造函数123456789private final Sync sync;//构造函数public CountDownLatch(int count) &#123; //边界值判断 if (count &lt; 0) throw new IllegalArgumentException(&quot;count &lt; 0&quot;); //初始化Sync this.sync = new Sync(count);&#125; 3.await使当前线程挂起，直到计数器减为0或者当前线程被中断。 1234public void await() throws InterruptedException &#123; //执行aqs.acquireSharedInterruptibly() sync.acquireSharedInterruptibly(1);&#125; 4.AQS.acquireSharedInterruptiblycountdownlatch 也用到了 AQS，在 CountDownLatch 内部写了一个 Sync 并且继承了AQS 这个抽象类重写了 AQS中的共享锁方法。首先看到下面这个代码，这块代码主要是 判 断 当 前 线 程 是 否 获 取 到 了 共 享 锁 ; （ 在CountDownLatch 中 ， 使 用 的是 共 享 锁 机 制 ， 因 为CountDownLatch 并不需要实现互斥的特性）。 12345678910public final void acquireSharedInterruptibly(long arg) throws InterruptedException &#123; //如果当前线程被中断，抛出中断异常 if (Thread.interrupted()) throw new InterruptedException(); //条件成立：说明此时state&gt;0将线程入队，然后等待唤醒 //条件不成立：说明此时state=0，说明此时阻塞已经放开，当前线程不会被阻塞 if (tryAcquireShared(arg) &lt; 0) //将当前线程加入到共享锁队列 doAcquireSharedInterruptibly(arg);&#125; 5.tryAcquireShared判断state状态. 1234protected int tryAcquireShared(int acquires) &#123; //如果state =0 返回1 否则返回 0 return (getState() == 0) ? 1 : -1;&#125; 6.AQS.doAcquireSharedInterruptibly addWaiter 设置为 shared 模式。 ​ tryAcquire 和 tryAcquireShared 的返回值不同，因此会多出一个判断过程。 ​ 在 判 断 前 驱 节 点 是 头 节 点 后 ， 调 用 了setHeadAndPropagate 方法，而不是简单的更新一下头节点。 ​ 123456789101112131415161718192021222324252627282930313233343536private void doAcquireSharedInterruptibly(long arg) throws InterruptedException &#123; //将当前线程封装成节点入队，共享节点，使用的是state的高16位运算 final Node node = addWaiter(Node.SHARED); boolean failed = true; try &#123; //自旋 for (;;) &#123; //获取当前节点的前驱 final Node p = node.predecessor(); //如果前驱节点是头节点 if (p == head) &#123; //当前节点就可以尝试去抢锁 long r = tryAcquireShared(arg); //此时说明抢到锁了 if (r &gt;= 0) &#123; //修改头节点的值 setHeadAndPropagate(node, r); //头节点出队 p.next = null; // help GC //代表抢锁成功 failed = false; return; &#125; &#125; //否则的话，线程在这里park，如果线程中断信号=true，就会抛出中断异常 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) throw new InterruptedException(); &#125; &#125; finally &#123; //如果抢锁失败了，就走取消竞争锁的逻辑 if (failed) cancelAcquire(node); &#125;&#125; 假如这个时候有 3 个线程调用了 await 方法，由于这个时候 state 的值还不为 0，所以这三个线程都会加入到 AQS队列中。并且三个线程都处于阻塞状态。 7.countDown递减锁计数，如果锁计数为0，释放所有阻塞线程。 123public void countDown() &#123; sync.releaseShared(1);&#125; 8.AQS.releaseShared由于线程被 await 方法阻塞了，所以只有等到countdown 方法使得 state=0 的时候才会被唤醒。 只有当 state 减为 0 的时候，tryReleaseShared 才返回 true, 否则只是简单的 state = state - 1。 ​ 如果 state=0, 则调用 doReleaseShared唤醒处于 await 状态下的线程。 ​ 12345678910public final boolean releaseShared(int arg) &#123; //执行子类重写的方法，state=0的时候，执行doReleaseShared //条件成立：说明当前调用latch.countDown()方法的线程，正好是state-1 == 0 的这个线程，需要做触发唤醒await状态的线程。 if (tryReleaseShared(arg)) &#123; //调用countDown()方法的线程，只有一个线程会进入到这个if块里面，执行下面的方法 doReleaseShared(); return true; &#125; return false;&#125; 9.tryReleaseShared自旋释放锁，释放完了返回true，否则返回false。 12345678910111213protected boolean tryReleaseShared(int releases) &#123; for (;;) &#123; //获取最新的state int c = getState(); //如果state==0 返回false if (c == 0) return false; int nextc = c-1; //如果cas成功，且c=1，返回true if (compareAndSetState(c, nextc)) return nextc == 0; &#125;&#125; 10.AQS.doReleaseShared共享锁的释放和独占锁的释放有一定的差别​ 前面唤醒锁的逻辑和独占锁是一样，先判断头结点是不是SIGNAL 状态，如果是，则修改为 0，并且唤醒头结点的下一个节点。​ PROPAGATE ： 标识为 PROPAGATE 状态的节点，是共享锁模式下的节点状态，处于这个状态下的节点，会对线程的唤醒进行传播 123456789101112131415161718192021222324252627282930313233343536373839404142434445private void doReleaseShared() &#123; //自旋 for (;;) &#123; //获取头节点的引用 Node h = head; //如果头节点不为空 &amp;&amp; 头节点不等于尾结点 //条件一成立：说明阻塞队列不为空 //什么时候不成立？latch创建出来以后，没有任何线程调用过await方法之前，就有线程调用countDown操作，并且触发了唤醒阻塞节点的逻辑 //条件二成立：说明当前队列除了头节点还有其他节点 //什么时候不成立？ //1.正常唤醒情况：依次获取共享锁，当前线程执行到这里的时候是tail节点 //2.第一个调用await的线程与调用countDown的线程并发了 if (h != null &amp;&amp; h != tail) &#123; int ws = h.waitStatus; //如果头结点的转态=-1 if (ws == Node.SIGNAL) &#123; //cas设置头节点的状态失败 if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) continue; //cas成功，唤醒头节点的下一个节点 unparkSuccessor(h); &#125; //cas失败走到这里， //执行到这里的时候，刚好有一个节点入队，入队会将这个 ws 设置为 -1 else if (ws == 0 &amp;&amp; !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) continue; &#125; /* 条件成立： 1.说明刚刚唤醒的后继节点，还没将自己设置为头节点，没执行到呢.... 这个时候，当前线程直接跳出去结束了 此时并不需要担心 唤醒逻辑 在这里断开 ，因为被唤醒的线程，早晚会执行到doReleaseShared方法 2.head==null latch创建出来以后，没有任何线程调用过await方法之前，就有线程调用countDown操作，并且触发了唤醒阻塞节点的逻辑 3.h==tail break 条件不成立： 条件成立1的相反情况，此时唤醒他的节点 执行 h == head 不成立，此时 原头节点不会跳出，会继续唤醒新的头节点的后继节点。 */ if (h == head) break; &#125;&#125; 11.AQS.doAcquireSharedInterruptibly一旦 ThreadA 被唤醒，代码又会继续回到doAcquireSharedInterruptibly 中来执行。如果当前 state满足=0 的条件，则会执行 setHeadAndPropagate 方法。 123456789101112131415161718192021222324252627private void doAcquireSharedInterruptibly(int arg) throws InterruptedException &#123; final Node node = addWaiter(Node.SHARED); //创建一个共享模式的节点添加到队列中 boolean failed = true; try &#123; for (;;) &#123;//被唤醒的线程进入下一次循环继续判断 final Node p = node.predecessor(); if (p == head) &#123; int r = tryAcquireShared(arg);//就判断尝试获取锁 if (r &gt;= 0) &#123;//r&gt;=0 表示获取到了执行权限，这个时候因为 state!=0，所以不会执行这段代码 setHeadAndPropagate(node, r); p.next = null; // help GC 把当前节点移除 aqs 队列 failed = false; return; &#125; &#125; //阻塞线程 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) throw new InterruptedException(); &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; 12.setHeadAndPropagate这个方法的主要作用是把被唤醒的节点，设置成 head 节点。 然后继续唤醒队列中的其他线程。由于现在队列中有 3 个线程处于阻塞状态，一旦 ThreadA被唤醒，并且设置为 head 之后，会继续唤醒后续的ThreadB。​ 123456789101112131415private void setHeadAndPropagate(Node node, int propagate) &#123; Node h = head; // Record old head for check below //将当前节点设置为头节点 setHead(node); //1&gt;0 if (propagate &gt; 0 || h == null || h.waitStatus &lt; 0 || (h = head) == null || h.waitStatus &lt; 0) &#123; Node s = node.next; //条件一：s==null 什么时候成立呢？ 当前node节点已经是tail节点了， //条件二的前置条件：s!=null 要求s的模式是共享模式 if (s == null || s.isShared()) //继续向后唤醒 doReleaseShared(); &#125;&#125; 13.流程图 二，使用countdownlatch 是一个同步工具类，它允许一个或多个线程一直等待，直到其他线程的操作执行完毕再执行。从命名可以解读到 countdown 是倒数的意思，类似于倒计时的概念。​ countdownlatch 提供了两个方法，一个是 countDown，一个是 await， countdownlatch 初始化的时候需要传入一个整数，在这个整数倒数到 0 之前，调用了 await 方法的程序都必须要等待，然后通过 countDown 来倒数。​ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687/** * @author 二十 * @since 2021/9/6 2:00 下午 */public class DemoA &#123; private static CountDownLatch c = new CountDownLatch(6); private static ThreadPoolExecutor executor = new ThreadPoolExecutor( 6, 6, 1, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;&gt;(1), new MyDefaultFactory(), new ThreadPoolExecutor.AbortPolicy() ); public static void main(String[] args)throws Exception &#123; for (int i = 0; i &lt; 6; i++) executor.submit(()-&gt;&#123; System.out.println(Thread.currentThread().getName() + &quot;国被灭！&quot;); c.countDown(); &#125;); c.await(); if (Thread.currentThread().getName().equals(&quot;main&quot;)) System.out.println(&quot;main线程执行结束:&quot; + Thread.currentThread().getName() ); &#125; private static class MyDefaultFactory implements ThreadFactory&#123; private static Queue&lt;String&gt; queue = new LinkedList(); static &#123; for (int i = 1; i &lt;= 6; i++) queue.add(Objects.requireNonNull(Message.foreach_CountryEnum(i)).message); &#125; @Override public Thread newThread(Runnable r) &#123; return new Thread(r,&quot;thread-&quot;+queue.poll() +&quot;-er_shi&quot;); &#125; &#125; enum Message &#123; ONE(1, &quot;齐&quot;), TWO(2, &quot;楚&quot;), THREE(3, &quot;燕&quot;), FOUR(4, &quot;赵&quot;), FIVE(5, &quot;魏&quot;), SIX(6, &quot;韩&quot;); private int code; private String message; Message(int code, String message) &#123; this.code = code; this.message = message; &#125; public int getCode() &#123; return code; &#125; public void setCode(int code) &#123; this.code = code; &#125; public String getMessage() &#123; return message; &#125; public void setMessage(String message) &#123; this.message = message; &#125; public static Message foreach_CountryEnum(int index) &#123; Message[] countryEnums = Message.values(); for (Message countryEnum : countryEnums) &#123; if (countryEnum.getCode() == index) &#123; return countryEnum; &#125; &#125; return null; &#125; &#125;&#125;","categories":[{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"ReentrantReadWriteLock","slug":"JUC/ReentrantReadWriteLock","date":"2022-01-11T11:06:57.592Z","updated":"2022-01-11T11:29:11.743Z","comments":true,"path":"2022/01/11/JUC/ReentrantReadWriteLock/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/JUC/ReentrantReadWriteLock/","excerpt":"","text":"读写锁：读锁和写锁都会发生死锁。读和读之间允许共享 ，读和写之间独占，写和写之间独占。​ 一，读写锁的使用123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public class Demo7 &#123; public static void main(String[] args)throws Exception &#123; MyCache myCache=new MyCache(); for (int i = 1; i &lt;=5; i++) &#123; int num = i ; new Thread(()-&gt;&#123; myCache.put(String.valueOf(num),String.valueOf(num)); &#125;,String.valueOf(i)).start(); &#125; TimeUnit.SECONDS.sleep(3); for (int i = 1; i &lt;=5; i++) &#123; int num = i ; new Thread(()-&gt;&#123; myCache.get(String.valueOf(num)); &#125;,String.valueOf(i)).start(); &#125; &#125;&#125;class MyCache&#123; //volatile:表示经常变化的 private volatile Map&lt;String,Object&gt; map=new HashMap&lt;&gt;(); private ReadWriteLock lock=new ReentrantReadWriteLock(); public void put(String key,Object val)&#123; try &#123; lock.writeLock().lock(); System.out.println(Thread.currentThread().getName()+&quot;\\t 开始写入数据&quot;+key+&quot;!!!!!!!!!&quot;); TimeUnit.SECONDS.sleep(1); map.put(key,val); System.out.println(Thread.currentThread().getName()+&quot;\\t 完成写入数据&quot;+key+&quot;----------&quot;); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;finally &#123; lock.writeLock().unlock(); &#125; &#125; public void get(String key)&#123; try &#123; lock.readLock().lock(); System.out.println(Thread.currentThread().getName()+&quot;\\t 开始读取数据&quot;+key+&quot;!!!!!!!!!&quot;); TimeUnit.SECONDS.sleep(1); Object result = map.get(key); System.out.println(Thread.currentThread().getName()+&quot;\\t 完成读取数据&quot;+result+&quot;----------&quot;); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;finally &#123; lock.readLock().unlock(); &#125; &#125;&#125; 二，读写锁的原理读写锁用的是同一个 Sycn 同步器，因此等待队列、state 等也是同一个。​ 1.线程1写锁加锁线程1成功上锁，流程与ReentrantLock加锁相比没有特殊之处，不同的是写锁状态占了state的低16位，而读锁使用的是state的高16位。​ 2.线程2读锁加锁t2 执行 r.lock，这时进入读锁的 sync.acquireShared(1) 流程，首先会进入 tryAcquireShared 流程。如果有写锁占据，那么 tryAcquireShared 返回 -1 表示失败。​ tryAcquireShared 返回值表示: -1 表示失败 0 表示成功，但后继节点不会继续唤醒 正数表示成功，而且数值是还有几个后继节点需要唤醒，读写锁返回 1。 ​ 这时会进入 sync.doAcquireShared(1) 流程，首先也是调用 addWaiter 添加节点。​ t2 会看看自己的节点是不是老二，如果是，还会再次调用 tryAcquireShared(1) 来尝试获取锁。​ 如果没有成功，在 doAcquireShared 内 for (; ; ) 循环一次，把前驱节点的 waitStatus 改为 -1，再 for (;; ) 循环一次尝试 tryAcquireShared(1) 如果还不成功，那么在 parkAndCheckInterrupt() 处 park。​ 3.线程3读锁加锁，线程4写锁加锁这种状态下，假设又有 t3 加读锁和 t4 加写锁，这期间 t1 仍然持有锁，就变成了下面的样子。​ 4.线程1写锁释放锁这时会走到写锁的 sync.release(1) 流程，调用 sync.tryRelease(1) 成功，变成下面的样子​ 接下来执行唤醒流程 sync.unparkSuccessor，即让老二恢复运行，这时 t2 在 doAcquireShared 内parkAndCheckInterrupt() 处恢复运行。​ 这回再来一次 for (;;) 执行 tryAcquireShared 成功则让读锁计数加一。​ 这时 t2 已经恢复运行，接下来 t2 调用 setHeadAndPropagate(node, 1)，它原本所在节点被置为头节点。​ 事情还没完，在 setHeadAndPropagate 方法内还会检查下一个节点是否是 shared，如果是则调用doReleaseShared() 将 head 的状态从 -1 改为 0 并唤醒老二，这时 t3 在 doAcquireShared 内parkAndCheckInterrupt() 处恢复运行。​ 这回再来一次 for (;;) 执行 tryAcquireShared 成功则让读锁计数加一。​ 这时 t3 已经恢复运行，接下来 t3 调用 setHeadAndPropagate(node, 1)，它原本所在节点被置为头节点。​ 下一个节点不是 shared 了，因此不会继续唤醒 t4 所在节点。​ 5.线程2读锁释放锁，线程3读锁释放锁t2 进入 sync.releaseShared(1) 中，调用 tryReleaseShared(1) 让计数减一，但由于计数还不为零。​ t3 进入 sync.releaseShared(1) 中，调用 tryReleaseShared(1) 让计数减一，这回计数为零了，进入doReleaseShared() 将头节点从 -1 改为 0 并唤醒老二，即：​ 之后 t4 在 acquireQueued 中 parkAndCheckInterrupt 处恢复运行，再次 for (;;) 这次自己是老二，并且没有其他竞争，tryAcquire(1) 成功，修改头结点，流程结束。​ 三，源码分析1.写锁上锁流程1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162static final class NonfairSync extends Sync &#123; // ... 省略无关代码 // 外部类 WriteLock 方法, 方便阅读, 放在此处 public void lock() &#123; sync.acquire(1); &#125; // AQS 继承过来的方法, 方便阅读, 放在此处 public final void acquire(int arg) &#123; if ( // 尝试获得写锁失败 !tryAcquire(arg) &amp;&amp; // 将当前线程关联到一个 Node 对象上, 模式为独占模式 // 进入 AQS 队列阻塞 acquireQueued(addWaiter(Node.EXCLUSIVE), arg) ) &#123; selfInterrupt(); &#125; &#125; // Sync 继承过来的方法, 方便阅读, 放在此处 protected final boolean tryAcquire(int acquires) &#123; // 获得低 16 位, 代表写锁的 state 计数 Thread current = Thread.currentThread(); int c = getState(); int w = exclusiveCount(c); if (c != 0) &#123; if ( // c != 0 and w == 0 表示有读锁, 或者 w == 0 || // 如果 exclusiveOwnerThread 不是自己 current != getExclusiveOwnerThread() ) &#123; // 获得锁失败 return false; &#125; // 写锁计数超过低 16 位, 报异常 if (w + exclusiveCount(acquires) &gt; MAX_COUNT) throw new Error(&quot;Maximum lock count exceeded&quot;); // 写锁重入, 获得锁成功 setState(c + acquires); return true; &#125; if ( // 判断写锁是否该阻塞, 或者 writerShouldBlock() || // 尝试更改计数失败 !compareAndSetState(c, c + acquires) ) &#123; // 获得锁失败 return false; &#125; // 获得锁成功 setExclusiveOwnerThread(current); return true; &#125; // 非公平锁 writerShouldBlock 总是返回 false, 无需阻塞 final boolean writerShouldBlock() &#123; return false; &#125;&#125; 2.写锁释放锁的流程12345678910111213141516171819202122232425262728293031323334static final class NonfairSync extends Sync &#123; // ... 省略无关代码 // WriteLock 方法, 方便阅读, 放在此处 public void unlock() &#123; sync.release(1); &#125; // AQS 继承过来的方法, 方便阅读, 放在此处 public final boolean release(int arg) &#123; // 尝试释放写锁成功 if (tryRelease(arg)) &#123; // unpark AQS 中等待的线程 Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; &#125; return false; &#125; // Sync 继承过来的方法, 方便阅读, 放在此处 protected final boolean tryRelease(int releases) &#123; if (!isHeldExclusively()) throw new IllegalMonitorStateException(); int nextc = getState() - releases; // 因为可重入的原因, 写锁计数为 0, 才算释放成功 boolean free = exclusiveCount(nextc) == 0; if (free) &#123; setExclusiveOwnerThread(null); &#125; setState(nextc); return free; &#125;&#125; 3.读锁上锁流程123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151static final class NonfairSync extends Sync &#123; // ReadLock 方法, 方便阅读, 放在此处 public void lock() &#123; sync.acquireShared(1); &#125; // AQS 继承过来的方法, 方便阅读, 放在此处 public final void acquireShared(int arg) &#123; // tryAcquireShared 返回负数, 表示获取读锁失败 if (tryAcquireShared(arg) &lt; 0) &#123; doAcquireShared(arg); &#125; &#125; // Sync 继承过来的方法, 方便阅读, 放在此处 protected final int tryAcquireShared(int unused) &#123; Thread current = Thread.currentThread(); int c = getState(); // 如果是其它线程持有写锁, 获取读锁失败 if ( exclusiveCount(c) != 0 &amp;&amp; getExclusiveOwnerThread() != current ) &#123; return -1; &#125; int r = sharedCount(c); if ( // 读锁不该阻塞(如果老二是写锁，读锁该阻塞), 并且 !readerShouldBlock() &amp;&amp; // 小于读锁计数, 并且 r &lt; MAX_COUNT &amp;&amp; // 尝试增加计数成功 compareAndSetState(c, c + SHARED_UNIT) ) &#123; // ... 省略不重要的代码 return 1; &#125; return fullTryAcquireShared(current); &#125; // 非公平锁 readerShouldBlock 看 AQS 队列中第一个节点是否是写锁 // true 则该阻塞, false 则不阻塞 final boolean readerShouldBlock() &#123; return apparentlyFirstQueuedIsExclusive(); &#125; // AQS 继承过来的方法, 方便阅读, 放在此处 // 与 tryAcquireShared 功能类似, 但会不断尝试 for (;;) 获取读锁, 执行过程中无阻塞 final int fullTryAcquireShared(Thread current) &#123; HoldCounter rh = null; for (; ; ) &#123; int c = getState(); if (exclusiveCount(c) != 0) &#123; if (getExclusiveOwnerThread() != current) return -1; &#125; else if (readerShouldBlock()) &#123; // ... 省略不重要的代码 &#125; if (sharedCount(c) == MAX_COUNT) throw new Error(&quot;Maximum lock count exceeded&quot;); if (compareAndSetState(c, c + SHARED_UNIT)) &#123; // ... 省略不重要的代码 return 1; &#125; &#125; &#125; // AQS 继承过来的方法, 方便阅读, 放在此处 private void doAcquireShared(int arg) &#123; // 将当前线程关联到一个 Node 对象上, 模式为共享模式 final Node node = addWaiter(Node.SHARED); boolean failed = true; try &#123; boolean interrupted = false; for (; ; ) &#123; final Node p = node.predecessor(); if (p == head) &#123; // 再一次尝试获取读锁 int r = tryAcquireShared(arg); // 成功 if (r &gt;= 0) &#123; // ㈠ // r 表示可用资源数, 在这里总是 1 允许传播 //（唤醒 AQS 中下一个 Share 节点） setHeadAndPropagate(node, r); p.next = null; // help GC if (interrupted) selfInterrupt(); failed = false; return; &#125; &#125; if ( // 是否在获取读锁失败时阻塞（前一个阶段 waitStatus == Node.SIGNAL） shouldParkAfterFailedAcquire(p, node) &amp;&amp; // park 当前线程 parkAndCheckInterrupt() ) &#123; interrupted = true; &#125; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125; &#125; // ㈠ AQS 继承过来的方法, 方便阅读, 放在此处 private void setHeadAndPropagate(Node node, int propagate) &#123; Node h = head; // Record old head for check below // 设置自己为 head setHead(node); // propagate 表示有共享资源（例如共享读锁或信号量） // 原 head waitStatus == Node.SIGNAL 或 Node.PROPAGATE // 现在 head waitStatus == Node.SIGNAL 或 Node.PROPAGATE if (propagate &gt; 0 || h == null || h.waitStatus &lt; 0 || (h = head) == null || h.waitStatus &lt; 0) &#123; Node s = node.next; // 如果是最后一个节点或者是等待共享读锁的节点 if (s == null || s.isShared()) &#123; // 进入 ㈡ doReleaseShared(); &#125; &#125; &#125; // ㈡ AQS 继承过来的方法, 方便阅读, 放在此处 private void doReleaseShared() &#123; // 如果 head.waitStatus == Node.SIGNAL ==&gt; 0 成功, 下一个节点 unpark // 如果 head.waitStatus == 0 ==&gt; Node.PROPAGATE, 为了解决 bug, 见后面分析 for (; ; ) &#123; Node h = head; // 队列还有节点 if (h != null &amp;&amp; h != tail) &#123; int ws = h.waitStatus; if (ws == Node.SIGNAL) &#123; if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) continue; // loop to recheck cases // 下一个节点 unpark 如果成功获取读锁 // 并且下下个节点还是 shared, 继续 doReleaseShared unparkSuccessor(h); &#125; else if (ws == 0 &amp;&amp; !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) continue; // loop on failed CAS &#125; if (h == head) // loop if head changed break; &#125; &#125;&#125; 4.读锁释放锁的流程123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354static final class NonfairSync extends Sync &#123; // ReadLock 方法, 方便阅读, 放在此处 public void unlock() &#123; sync.releaseShared(1); &#125; // AQS 继承过来的方法, 方便阅读, 放在此处 public final boolean releaseShared(int arg) &#123; if (tryReleaseShared(arg)) &#123; doReleaseShared(); return true; &#125; return false; &#125; // Sync 继承过来的方法, 方便阅读, 放在此处 protected final boolean tryReleaseShared(int unused) &#123; // ... 省略不重要的代码 for (; ; ) &#123; int c = getState(); int nextc = c - SHARED_UNIT; if (compareAndSetState(c, nextc)) &#123; // 读锁的计数不会影响其它获取读锁线程, 但会影响其它获取写锁线程 // 计数为 0 才是真正释放 return nextc == 0; &#125; &#125; &#125; // AQS 继承过来的方法, 方便阅读, 放在此处 private void doReleaseShared() &#123; // 如果 head.waitStatus == Node.SIGNAL ==&gt; 0 成功, 下一个节点 unpark // 如果 head.waitStatus == 0 ==&gt; Node.PROPAGATE for (; ; ) &#123; Node h = head; if (h != null &amp;&amp; h != tail) &#123; int ws = h.waitStatus; // 如果有其它线程也在释放读锁，那么需要将 waitStatus 先改为 0 // 防止 unparkSuccessor 被多次执行 if (ws == Node.SIGNAL) &#123; if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) continue; // loop to recheck cases unparkSuccessor(h); &#125; // 如果已经是 0 了，改为 -3，用来解决传播性，见后文信号量 bug 分析 else if (ws == 0 &amp;&amp; !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) continue; // loop on failed CAS &#125; if (h == head) // loop if head changed break; &#125; &#125;&#125;","categories":[{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"AQS","slug":"JUC/AQS","date":"2022-01-11T11:06:49.064Z","updated":"2022-01-11T11:24:53.300Z","comments":true,"path":"2022/01/11/JUC/AQS/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/JUC/AQS/","excerpt":"","text":"一，LockLock在juc中是最核心的组件​ 1.Lock的实现Lock 本质上是一个接口，它定义了释放锁和获得锁的抽象方法，定义成接口就意味着它定义了锁的一个标准规范，也同时意味着锁的不同实现。实现 Lock 接口的类有很多，以下为几个常见的锁实现。​ ReentrantLock：表示重入锁，它是唯一一个实现了 Lock 接口的类。重入锁指的是线程在获得锁之后，再次获取该锁不需要阻塞，而是直接关联一次计数器增加重入次数。​ ReentrantReadWriteLock：重入读写锁，它实现了 ReadWriteLock 接口，在这个类中维护了两个锁，一个是 ReadLock，一个是 WriteLock，他们都分别实现了 Lock接口。读写锁是一种适合读多写少的场景下解决线程安全问题的工具，基本原则是： 读和读不互斥、读和写互斥、写和写互斥。也就是说涉及到影响数据变化的操作都会存在互斥。​ StampedLock： stampedLock 是 JDK8 引入的新的锁机制，可以简单认为是读写锁的一个改进版本，读写锁虽然通过分离读和写的功能使得读和读之间可以完全并发，但是读和写是有冲突的，如果大量的读线程存在，可能会引起写线程的饥饿。stampedLock 是一种乐观的读策略，使得乐观锁完全不会阻塞写线程。​ 2.Lock的类关系图Lock有很多的实现，但是直观的实现是ReentrantLock重入锁。​ 12345void lock() // 如果锁可用就获得锁，如果锁不可用就阻塞直到锁释放void lockInterruptibly() // 和lock()方法相似, 但阻塞的线程 可 中 断 ， 抛 出java.lang.InterruptedException 异常boolean tryLock() // 非阻塞获取锁;尝试获取锁，如果成功返回 trueboolean tryLock(long timeout, TimeUnit timeUnit) //带有超时时间的获取锁方法void unlock() // 释放锁 二，ReentrantLock重入锁，表示支持重新进入的锁，也就是说，如果当前线程 t1 通过调用 lock 方法获取了锁之后，再次调用 lock，是不会再阻塞去获取锁的，直接增加重试次数就行了。synchronized 和 ReentrantLock 都是可重入锁。​ 1.重入锁的设计目的比如调用 demo 方法获得了当前的对象锁，然后在这个方法中再去调用demo2，demo2 中的存在同一个实例锁，这个时候当前线程会因为无法获得demo2 的对象锁而阻塞，就会产生死锁。重入锁的设计目的是避免线程的死锁。​ 1234567891011121314151617public class ReentrantDemo &#123; public synchronized void demo() &#123; System.out.println(&quot;begin:demo&quot;); demo2(); &#125; public void demo2() &#123; System.out.println(&quot;begin:demo1&quot;); synchronized (this) &#123; &#125; &#125; public static void main(String[] args) &#123; ReentrantDemo rd = new ReentrantDemo(); new Thread(rd::demo).start(); &#125;&#125; 2.ReentrantLock使用123456789101112131415161718192021222324public class AtomicDemo &#123; private static int count = 0; static Lock lock = new ReentrantLock(); public static void inc() &#123; lock.lock(); try &#123; Thread.sleep(1); count++; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;finally &#123; lock.unlock(); &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; for (int i = 0; i &lt; 1000; i++) &#123; new Thread(() -&gt; AtomicDemo.inc()).start(); &#125; Thread.sleep(3000); System.out.println(&quot;result:&quot; + count); &#125;&#125; 3.ReentrantReadWriteLock​ 以前理解的锁，基本都是排他锁，也就是这些锁在同一时刻只允许一个线程进行访问，而读写所在同一时刻可以允许多个线程访问，但是在写线程访问时，所有的读线程和其他写线程都会被阻塞。读写锁维护了一对锁，一个读锁、一个写锁;一般情况下，读写锁的性能都会比排它锁好，因为大多数场景读是多于写的。在读多于写的情况下，读写锁能够提供比排它锁更好的并发性和吞吐量。​ 12345678910111213141516171819202122232425262728public class LockDemo &#123; static Map&lt;String, Object&gt; cacheMap = new HashMap&lt;&gt;(); static ReentrantReadWriteLock rwl = new ReentrantReadWriteLock(); static Lock read = rwl.readLock(); static Lock write = rwl.writeLock(); public static final Object get(String key) &#123; System.out.println(&quot; 开始读取数据&quot;); read.lock(); // 读锁 try &#123; return cacheMap.get(key); &#125; finally &#123; read.unlock(); &#125; &#125; public static final Object put(String key, Object value) &#123; System.out.println(&quot; 开始写数据&quot;); write.lock(); try &#123; return cacheMap.put(key, value); &#125; finally &#123; write.unlock(); &#125; &#125;&#125; 在这个案例中，通过 hashmap 来模拟了一个内存缓存，然后使用读写锁来保证这个内存缓存的线程安全性。当执行读操作的时候，需要获取读锁，在并发访问的时候，读锁不会被阻塞，因为读操作不会影响执行结果。​ 在执行写操作时，线程必须要获取写锁，当已经有线程持有写锁的情况下，当前线程会被阻塞，只有当写锁释放以后，其他读写操作才能继续执行。使用读写锁提升读操作的并发性，也保证每次写操作对所有的读写操作的可见性。​ 读锁与读锁可以共享​ 读锁与写锁不可以共享（排他） 写锁与写锁不可以共享（排他）​ 三，ReentrantLock 的实现原理​ 锁的基本原理是，基于将多线程并行任务通过某一种机制实现线程的串行执行，从而达到线程安全性的目的。在 ReentrantLock 中，在多线程竞争重入锁时，竞争失败的线程是如何实现阻塞以及被唤醒的呢？​ 1.AQS是什么？在 Lock 中，用到了一个同步队列 AQS，全称 AbstractQueuedSynchronizer，它是一个同步工具也是 Lock 用来实现线程同步的核心组件。​ 2.AQS的两种功能使用层面上来说，AQS的功能分为两种:独占和共享。​ 独占锁：每次只能有一个线程持有锁。​ 共享锁：允许多线程同时获取锁，并发访问共享资源。​ 3.AQS的内部实现AQS队列内部维护的是一个FIFO的双向链表，这种结构的特点是每个数据结构都有两个指针，分别指向直接的后继节点和直接的前驱结点。所以双向链表可以从任意一个结点开始很方便的访问前驱和后继。每个Node其实是由线程封装，当线程争抢锁失败后会封装成Node加入到AQS队列中去，当获取锁的线程释放锁之后，会从队列中唤醒一个阻塞的节点（线程）。​ 4.Node的组成1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950static final class Node &#123; /** Marker to indicate a node is waiting in shared mode */ static final Node SHARED = new Node(); /** Marker to indicate a node is waiting in exclusive mode */ static final Node EXCLUSIVE = null; static final int CANCELLED = 1; static final int SIGNAL = -1; static final int CONDITION = -2; static final int PROPAGATE = -3; volatile int waitStatus; volatile Node prev; //前驱结点 volatile Node next; //后继节点 volatile Thread thread; //当前线程 Node nextWaiter; //存储在condition队列中的后继节点 //是否为共享锁 final boolean isShared() &#123; return nextWaiter == SHARED; &#125; final Node predecessor() throws NullPointerException &#123; Node p = prev; if (p == null) throw new NullPointerException(); else return p; &#125; Node() &#123; // Used to establish initial head or SHARED marker &#125; Node(Thread thread, Node mode) &#123; // Used by addWaiter this.nextWaiter = mode; this.thread = thread; &#125; Node(Thread thread, int waitStatus) &#123; // Used by Condition this.waitStatus = waitStatus; this.thread = thread; &#125;&#125; 5.释放锁以及添加线程对于队列的变化当出现锁竞争以及释放锁的时候，AQS 同步队列中的节点会发生变化，首先看一下添加节点的场景。​ 这里会涉及到两个变化：​ 新的线程封装成Node节点追加到同步队列中，设置prev节点以及修改当前节点的前驱节点的next节点指向自己。 通过CAS的方式将tail重新指向新的尾部节点。 ​ head节点表示获取锁成功的节点，当头结点在释放同步状态的时候，会唤醒后继节点，如果后继节点获得锁成功，会把自己设置为头节点，节点的变化过程如下：​ ​ 这个过程也是涉及到两个变化：​ 修改head节点指向下一个获得锁的节点 新的获得锁的节点，将prev的指针指向null ​ 设置head节点不需要用CAS，原因是设置head节点是由获得锁的线程来完成的，而同步锁只能由一个线程来获得，所以不需要CAS保证，只需要把head节点设置为原首节点的后继节点，并且断开原head节点的next引用即可。 四，ReentrantLock 的源码分析1.加锁逻辑ReentrantLock.lock123public void lock() &#123; sync.lock();&#125; ​ sync 实际上是一个抽象的静态内部类，它继承了 AQS 来实现重入锁的逻辑，前面说过 AQS 是一个同步队列，它能够实现线程的阻塞以及唤醒，但它并不具备业务功能，所以在不同的同步场景中，会继承 AQS 来实现对应场景的功能。​ Sync 有两个具体的实现类，分别是： NofairSync：表示可以存在抢占锁的功能，也就是说不管当前队列上是否存在其他线程等待，新线程都有机会抢占锁。 FailSync: 表示所有线程严格按照 FIFO 来获取锁。​ NonfairSync.lock 非公平锁和公平锁最大的区别在于：非公平锁中抢占锁的逻辑是，不管有没有线程排队，先上来cas去抢占一下 cas成功，就表示成功获得了锁 cas失败，调用acquire(1)走锁竞争逻辑 ​ state 是 AQS 中的一个属性，它在不同的实现中所表达的含义不一样，对于重入锁的实现来说，表示一个同步状态。它有两个含义的表示：​ 当 state=0 时，表示无锁状态。 ​ 当 state&gt;0 时，表示已经有线程获得了锁，也就是 state=1，但是因为ReentrantLock 允许重入，所以同一个线程多次获得同步锁的时候，state 会递增，比如重入 5 次，那么 state=5。 而在释放锁的时候，同样需要释放 5 次直到 state=0其他线程才有资格获得锁。 ​ 1234567final void lock() &#123; //如果cas的方式修改state状态成功，说明获取到了锁，直接设置当前持有锁的线程是当前线程 if (compareAndSetState(0, 1)) setExclusiveOwnerThread(Thread.currentThread()); else //cas失败，走抢占锁的逻辑 acquire(1);&#125; \u0000 AQS.acquireacquire是AQS中的方法，如果cas操作未能成功，说明state已经不为0，此时继续acquire(1)操作。​ 这个方法的主要逻辑：​ 通过 tryAcquire 尝试获取独占锁，如果成功返回 true，失败返回 false. 如果 tryAcquire 失败，则会通过 addWaiter 方法将当前线程封装成 Node 添加到 AQS 队列尾部. acquireQueued，将 Node 作为参数，通过自旋去尝试获取锁。12345678public final void acquire(int arg) &#123; //如果尝试去抢占锁失败 就讲当前线尾插进入阻塞队列，让当前现成的node自旋的方式去尝试获取锁 if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) //响应中断逻辑，表示如果当前线程在 acquireQueued 中被中断过，则需要产生一个中断请求， //原因是线程在调用 acquireQueued 方法的时候是不会响应中断请求的。 selfInterrupt();&#125; AQS.selfInterrupt 表示如果当前线程在 acquireQueued 中被中断过，则需要产生一个中断请求，原因是线程在调用 acquireQueued 方法的时候是不会响应中断请求的。 1234//执行中断逻辑static void selfInterrupt() &#123; Thread.currentThread().interrupt();&#125; AQS.tryAcquire这里采用模板模式，具体的逻辑交给继承该类的子类实现，直接看非公平锁的逻辑。 123protected boolean tryAcquire(int arg) &#123; throw new UnsupportedOperationException();&#125; NonfairSync.tryAcquire这个方法的作用是尝试获取锁，如果成功返回true，不成功返回false。​ 它是重写AQS类中的tryAcquire方法，AQS中的tryAcquire()的定义，并没有实现，而是抛出异常。 123protected final boolean tryAcquire(int acquires) &#123; return nonfairTryAcquire(acquires);&#125; ReentrantLock.nofairTryAcquire 获取当前线程，判断当前的锁的状态 如果state=0 表示当前是无锁状态，通过cas更新state状态的值 当前线程是属于重入，则增加重入次数12345678910111213141516171819202122232425262728final boolean nonfairTryAcquire(int acquires) &#123; //获取当前线程 final Thread current = Thread.currentThread(); //获取当前的state值 int c = getState(); //如果此时没有线程持有锁 if (c == 0) &#123; //cas去获取一次锁 if (compareAndSetState(0, acquires)) &#123; //走到这里说明cas的方式获取到了锁，直接设置当前锁的线程是当前线程，并返回true setExclusiveOwnerThread(current); return true; &#125; &#125; //走到这里说明 当前锁被其他线程持有 或者 当前cas失败，存在竞争 else if (current == getExclusiveOwnerThread()) &#123; //如果持有锁的线程是当前线程自己 //执行到这里说明是一次重入锁的逻辑，直接在原有状态上+就可以 int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; &#125; //什么情况下会走到这里？ //cas失败，锁被其他线程持有， //state》0 return false;&#125; AQS.addWaiter 当 tryAcquire 方法获取锁失败以后，则会先调用 addWaiter 将当前线程封装成Node. 入参 mode 表示当前节点的状态，传递的参数是 Node.EXCLUSIVE，表示独占状态。意味着重入锁用到了 AQS 的独占锁功能。​ 将当前线程封装成 Node ​ 当前链表中的 tail 节点是否为空，如果不为空，则通过 cas 操作把当前线程的node 添加到 AQS 队列 ​ 如果为空或者 cas 失败，调用 enq 将节点添加到 AQS 队列1234567891011121314151617181920private Node addWaiter(Node mode) &#123; //构建一个新的节点 Node node = new Node(Thread.currentThread(), mode); // Try the fast path of enq; backup to full enq on failure Node pred = tail; //尾结点不为空，说明队列已经初始化过了 if (pred != null) &#123; //将当前节点的前驱节点指向为尾结点 node.prev = pred; //cas的方式设置尾结点 if (compareAndSetTail(pred, node)) &#123; //cas成功，让尾结点的后继指针指向node pred.next = node; return node; &#125; &#125; //执行到这里 说明队列尚未初始化， 或者cas的方式设置尾结点失败，说明有其他线程先一步入队了。 enq(node); return node;&#125; AQS.enq enq 就是通过自旋操作把当前节点加入到队列中。 123456789101112131415161718192021private Node enq(final Node node) &#123; //自旋： for (;;) &#123; Node t = tail; if (t == null) &#123; // 尾结点为空，说明需要初始化 if (compareAndSetHead(new Node())) tail = head; //注意，这里初始化完之后并没有直接退出，而是继续往下走，为啥呢？ //因为当前执行的是初始化操作，那就说明，当前这个抢占锁失败的线程需要给当前持有锁的线程补充一个头节点 &#125; else &#123;//尾结点不为空 //让当前节点的前驱指针指向尾结点 node.prev = t; //cas的方式设置尾结点 if (compareAndSetTail(t, node)) &#123; //让尾结点的后继指针指向当前节点 t.next = node; return t; &#125; &#125; &#125;&#125; AQS .acquireQueued通过 addWaiter 方法把线程添加到链表后，会接着把 Node 作为参数传递给acquireQueued 方法，去竞争锁​ 获取当前节点的 prev 节点 如果 prev 节点为 head 节点，那么它就有资格去争抢锁，调用 tryAcquire 抢占锁 ​ 抢占锁成功以后，把获得锁的节点设置为 head，并且移除原来的初始化 head节点 ​ 如果获得锁失败，则根据 waitStatus 决定是否需要挂起线程 最后，通过 cancelAcquire 取消获得锁的操作 123456789101112131415161718192021222324252627282930final boolean acquireQueued(final Node node, int arg) &#123; boolean failed = true; try &#123; boolean interrupted = false; for (;;) &#123; //获取当前节点的前驱节点 final Node p = node.predecessor(); //如果当前节点的前驱节点是头节点，说明当前节点拥有了竞争锁的权利，尝试cas的方式去获取一次锁 if (p == head &amp;&amp; tryAcquire(arg)) &#123; //执行到这里说名cas获取到了锁 //设置头节点为当前节点，这里为啥不用加锁，因为设置头节点的线程是持有锁的线程，独占模式下，只有一个线程持有锁，所以设置头节点不存在线程安全问题 setHead(node); //让p的后继指针指向null 帮助垃圾回收 p.next = null; // help GC //声明获取锁成功 failed = false; //当前线程不需要被中断 return interrupted; &#125; //当前持有锁的线程可能还没有释放锁，所以当前线程尝试获取锁会失败，这个时候当前线程应该park if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) //这里比较友好，仅仅是把中断标识位改成true，如果是响应中断逻辑，直接这里抛异常了，然后走cancelAcquire(node)的逻辑。 interrupted = true; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; AQS.shouldParkAfterFailedAcquire 如果 ThreadA 的锁还没有释放的情况下，ThreadB 和 ThreadC 来争抢锁肯定是会失败，那么失败以后会调用 shouldParkAfterFailedAcquire 方法​ Node 有 5 种状态，分别是：CANCELLED（1），SIGNAL（-1）、CONDITION（-2）、PROPAGATE(-3)、默认状态(0) CANCELLED: 在同步队列中等待的线程等待超时或被中断，需要从同步队列中取消该 Node 的结点, 其结点的 waitStatus 为 CANCELLED，即结束状态，进入该状态后的结点将不会再变化 ​ SIGNAL: 只要前置节点释放锁，就会通知标识为 SIGNAL 状态的后续节点的线程 ​ CONDITION： 和 Condition 有关系 ​ PROPAGATE：共享模式下，PROPAGATE 状态的线程处于可运行状态 ​ 0:初始状态 ​ 这个方法的主要作用是，通过 Node 的状态来判断，ThreadA 竞争锁失败以后是否应该被挂起。​ 如果 ThreadA 的 pred 节点状态为 SIGNAL，那就表示可以放心挂起当前线程 ​ 通过循环扫描链表把 CANCELLED 状态的节点移除 ​ 修改 pred 节点的状态为 SIGNAL,返回 false. ​ 返回 false 时，也就是不需要挂起，返回 true，则需要调用 parkAndCheckInterrupt挂起当前线程 1234567891011121314151617181920private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) &#123; //前驱节点的等待状态 int ws = pred.waitStatus; //如果前置节点为 SIGNAL，意味着只需要等待其他前置节点的线程被释放 if (ws == Node.SIGNAL) return true; //ws 大于 0，意味着 prev 节点取消了排队，直接移除这个节点就行 if (ws &gt; 0) &#123; do &#123;//相当于: pred=pred.prev; node.prev=pred; node.prev = pred = pred.prev; &#125; while (pred.waitStatus &gt; 0);//这里采用循环，从双向列表中移除 CANCELLED 的节点 pred.next = node; &#125; else &#123; //利用 cas 设置 prev 节点的状态为 SIGNAL(-1) compareAndSetWaitStatus(pred, ws, Node.SIGNAL); &#125; return false;&#125; AQS.parkAndCheckInterrupt​ 使用 LockSupport.park 挂起当前线程变成 WATING 状态​ Thread.interrupted，返回当前线程是否被其他线程触发过中断请求，也就是thread.interrupt(); 如果有触发过中断请求，那么这个方法会返回当前的中断标识true，并且对中断标识进行复位标识已经响应过了中断请求。如果返回 true，意味着在 acquire 方法中会执行 selfInterrupt()。 1234private final boolean parkAndCheckInterrupt() &#123; LockSupport.park(this); return Thread.interrupted();&#125; AQS.cancelAcquire123456789101112131415161718192021222324252627282930313233343536373839404142434445464748private void cancelAcquire(Node node) &#123; //如果节点不存在，直接返回 if (node == null) return; //将当前节点的线程设置为空 node.thread = null; //获取当前节点的前驱节点 Node pred = node.prev; //循环判断：如果前驱节点是一个取消状态，那就继续往前找 while (pred.waitStatus &gt; 0) node.prev = pred = pred.prev; //可能是当前节点，可能是ws&gt;0的节点 Node predNext = pred.next; //设置当前节点的waitStatus是取消状态 node.waitStatus = Node.CANCELLED; /** * 当前取消排队的node所在 队列的位置不同，执行的出队策略是不一样的，一共分为三种情况： * 1.当前node是队尾 tail -&gt; node * 2.当前node 不是 head.next 节点，也不是 tail * 3.当前node 是 head.next节点。 */ //尾结点且cas tail成功 if (node == tail &amp;&amp; compareAndSetTail(node, pred)) &#123; //pred.next = null node出队 compareAndSetNext(pred, predNext, null); &#125; else &#123; int ws; /* node不是head.next node 不是tail node的前驱节点是取消状态 或 假设前驱状态是 &lt;= 0 则设置前驱状态为 Signal状态..表示要唤醒后继节点。 if里面要做的就是让node 的prev 跨过node 直接指向 node.next */ if (pred != head &amp;&amp; ((ws = pred.waitStatus) == Node.SIGNAL || (ws &lt;= 0 &amp;&amp; compareAndSetWaitStatus(pred, ws, Node.SIGNAL))) &amp;&amp; pred.thread != null) &#123; Node next = node.next; if (next != null &amp;&amp; next.waitStatus &lt;= 0) compareAndSetNext(pred, predNext, next); &#125; else &#123; //node是head.next &amp;&amp; node !=tail //那就跨过node head.next = node.next node.next.prev=head unparkSuccessor(node); &#125; node.next = node; // help GC &#125;&#125; 2.释放锁的逻辑如果这个时候 ThreadA 释放锁了，那么来看锁被释放后会产生什么效果？​ ReentrantLock.unlock1234public void unlock() &#123; //执行sync的release方法，默认是非公平锁，所以看非公平锁的逻辑 sync.release(1);&#125; AQS.release123456789101112public final boolean release(int arg) &#123; //如果尝试释放锁成功 if (tryRelease(arg)) &#123; //如果头结点不为空，且头节点状态不等于0 Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) //唤醒后继线程 unparkSuccessor(h); return true; &#125; return false;&#125; ReentrantLock.tryRelease这个方法可以认为是一个设置锁状态的操作，通过将 state 状态减掉传入的参数值（参数是 1），如果结果状态为 0，就将排它锁的 Owner 设置为 null，以使得其它的线程有机会进行执行。​ 在排它锁中，加锁的时候状态会增加 1（当然可以自己修改这个值），在解锁的时候减掉 1，同一个锁，在可以重入后，可能会被叠加为 2、3、4 这些值，只有 unlock()的次数与 lock()的次数对应才会将 Owner 线程设置为空，而且也只有这种情况下才会返回 true。 1234567891011121314protected final boolean tryRelease(int releases) &#123; //将当前值与传入的值进行相减 int c = getState() - releases; //如果当前线程不是线程拥有者，抛异常 if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; if (c == 0) &#123;//重入锁已经完全释放 free = true; setExclusiveOwnerThread(null);//设置当前线程的拥有者为null &#125; setState(c);//修改state状态 return free;&#125; AQS.unparkSuccessor1234567891011121314151617181920private void unparkSuccessor(Node node) &#123; //获取头节点的状态 int ws = node.waitStatus; if (ws &lt; 0)//如果头结点的状态小于0 //修改头节点的状态为0 compareAndSetWaitStatus(node, ws, 0); //获取头节点的下一个节点 Node s = node.next; /如果下一个节点为 null 或者 status&gt;0 表示 cancelled 状态 if (s == null || s.waitStatus &gt; 0) &#123; s = null; //通过从尾部节点开始扫描，找到距离 head 最近的一个waitStatus&lt;=0 的节点 for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) if (t.waitStatus &lt;= 0) s = t; &#125; if (s != null)//唤醒这个节点 LockSupport.unpark(s.thread);&#125; 通过锁的释放，原本的结构就发生了一些变化。head 节点的 waitStatus 变成了 0，ThreadB 被唤醒。​ 为什么释放锁的时候是从尾结点开始扫描？​ 再回到 enq那个方法。看一个新的节点是如何加入到链表中的​ 将新的节点的 prev 指向 tail。 通过 cas 将 tail 设置为新的节点，因为 cas 是原子操作所以能够保证线程安全性。 ​ t.next=node；设置原 tail 的 next 节点指向新的节点。 ​ 在 cas 操作之后，t.next=node 操作之前。 存在其他线程调用 unlock 方法从 head开始往后遍历，由于 t.next=node 还没执行意味着链表的关系还没有建立完整。就会导致遍历到 t 节点的时候被中断。所以从后往前遍历，一定不会存在这个问题。​ 原本被挂起的线程在被唤醒后继续执行​ 通过 ReentrantLock.unlock，原本挂起的线程被唤醒以后继续执行，原来被挂起的线程是在 acquireQueued 方法中，所以被唤醒以后继续从这个方法开始执行。​ 由于 ThreadB 的 prev 节点指向的是 head，并且 ThreadA 已经释放了锁。所以这个时候调用 tryAcquire 方法时，可以顺利获取到锁。​ 把 ThreadB 节点当成 head。 ​ 把原 head 节点的 next 节点指向为 null。 ​ 3.公平锁和非公平锁的区别锁的公平性是相对于获取锁的顺序而言的，如果是一个公平锁，那么锁的获取顺序就应该符合请求的绝对时间顺序，也就是 FIFO。 在上面分析的例子来说，只要CAS 设置同步状态成功，则表示当前线程获取了锁，而公平锁则不一样，差异点有两个： FairSync.tryAcquire123final void lock() &#123; acquire(1); &#125; 非公平锁在获取锁的时候，会先通过 CAS 进行抢占，而公平锁则不会。​ FairSync .tryAcquire123456789101112131415161718192021222324252627282930protected final boolean tryAcquire(int acquires) &#123; //获取当前线程 final Thread current = Thread.currentThread(); //获取当前state状态 int c = getState(); if (c == 0) &#123;//状态为0，说明此时没有现成持有锁 //如果队列里面没有节点 且cas的方式获取锁成功 if (!hasQueuedPredecessors() &amp;&amp; compareAndSetState(0, acquires)) &#123; //设置持有锁的线程我当前线程 setExclusiveOwnerThread(current); return true; &#125; &#125; //如果当前线程是获取锁的线程 else if (current == getExclusiveOwnerThread()) &#123; //锁重入逻辑 int nextc = c + acquires; if (nextc &lt; 0) throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; &#125; /* 返回false的情况： 1. 当前锁被其他线程持有 2. 队列里面有节点或cas争抢锁失败 */ return false;&#125; 这个方法与 nonfairTryAcquire(int acquires)比较，不同的地方在于判断条件多了hasQueuedPredecessors()方法，也就是加入了[同步队列中当前节点是否有前驱节点]的判断，如果该方法返回 true，则表示有线程比当前线程更早地请求获取锁，因此需要等待前驱线程获取并释放锁之后才能继续获取锁。 hasQueuedPredecessors1234567891011121314//首先什么时候返回false？返回false代表当前队列没有线程或者当前队列头节点的下一个线程就是当前线程//1.队列为空//2.头节点的下一个节点的线程是当前线程public final boolean hasQueuedPredecessors() &#123; Node t = tail; Node h = head; Node s; //h!=t:头节点不是尾结点，说明队列里面还有其他的节点 //(头节点的下一个节点为空||头节点的后继节点对应的线程不是当前线程) //头节点的下一个节点的线程正在拿到锁 或者 头节点的下一个节点的线程不是当前线程 return h != t &amp;&amp; ((s = h.next) == null || s.thread != Thread.currentThread()); &#125; 五，Condition 条件队列对比synchronized管程模型：Condition原理图：​ 1.AQS.ConditionObject12345678public class ConditionObject implements Condition, java.io.Serializable &#123; private static final long serialVersionUID = 1173984872572414699L; //第一个等待的节点 private transient Node firstWaiter; //最后一个等待的节点 private transient Node lastWaiter; public ConditionObject() &#123; &#125; 2.Condition接口12345678910111213141516public interface Condition &#123; void await() throws InterruptedException; void awaitUninterruptibly(); long awaitNanos(long nanosTimeout) throws InterruptedException; boolean await(long time, TimeUnit unit) throws InterruptedException; boolean awaitUntil(Date deadline) throws InterruptedException; void signal(); void signalAll();&#125; 3.newCondition()创建一个等待条件 123public Condition newCondition() &#123; return sync.newCondition();&#125; 123final ConditionObject newCondition() &#123; return new ConditionObject();&#125; 实际上这里new了一个AQS的ConditionObject。​ 4.AQS.await这个方法的逻辑就是先进行边界判断，如果当前线程已经被中断，就抛出中断异常。否则将当前线程封装成节点加入到条件队列，并释放全部锁资源。接下来判断是否当前持有锁的线程执行了当前线程的signal方法： 如果没有执行：当前线程就还是在条件队列，此时应当将当前线程挂起 如果执行了：当前线程就迁移到了阻塞队列，就走竞争锁的逻辑…. 1234567891011121314151617181920212223242526272829303132333435363738public final void await() throws InterruptedException &#123; //如果当前线程已经被挂起，直接抛出中断异常 if (Thread.interrupted()) throw new InterruptedException(); //将当前线程封装成节点加入到条件队列 Node node = addConditionWaiter(); //释放线程的全部锁资源 //为什么这里要释放掉全部的锁？ //如果此时线程不释放锁资源，其他线程没办法拿到锁触发当前线程的唤醒机制 int savedState = fullyRelease(node); //中断状态： //-1:当前线程在条件队列接收到了中断信号 //0:当前线程在条件队列没有接收到中断信号 //1:当前线程在条件队列没有接受到中断信号，但是在阻塞队列接收到了中断信号 int interruptMode = 0; //如果当前线程不在阻塞队列 while (!isOnSyncQueue(node)) &#123; //挂起当前线程 LockSupport.park(this); //判断当前线程在等待期间是否被中断过，无论是否发生中断，最终都会进入到阻塞队列 //什么时候会被唤醒？都有几种情况？ //1.常规：外部线程获取到lock之后，调用了signal方法，转移条件队列的头节点到阻塞队列，当这个节点获取到锁之后，会唤醒 //2.转移至阻塞队列后，发现阻塞队列的前驱节点状态是取消状态，此时会唤醒当前节点 //3.当前节点挂起期间，被外部线程使用中断唤醒... if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) break; &#125; //来到这里说明当前线程已经被其他线程调用了signal方法，加入到了阻塞队列 //如果当前线程在竞争锁的过程中被中断过 &amp;&amp; 中断标识位不是 throw_ie if (acquireQueued(node, savedState) &amp;&amp; interruptMode != THROW_IE) //将当前线程的中断标识位设置为重新中断 interruptMode = REINTERRUPT; if (node.nextWaiter != null) unlinkCancelledWaiters(); if (interruptMode != 0) reportInterruptAfterWait(interruptMode);&#125; 5.Condition.addConditionWaiter线程进入条件队列挂起逻辑： 如果队列有元素，但是元素的状态不对，做一次全队的清理，将所有取消状态的节点干掉，并重新获取尾结点 将线程封装成node节点，判断当前条件队列是否有元素 如果没有元素，就将当前node设置为头节点 如果有元素，就将当前尾结点的next指针指向node 最终设置当前节点为尾节点，并返回当前线程的node。 1234567891011121314151617181920212223private Node addConditionWaiter() &#123; //获取尾结点的引用 Node t = lastWaiter; //条件一：尾结点！=空 //条件二：尾结点的状态！=-2，说明被中断了 if (t != null &amp;&amp; t.waitStatus != Node.CONDITION) &#123; //做一次全队列的垃圾节点清理 unlinkCancelledWaiters(); //重新获取尾结点的引用，因为在刚才的清理中，尾结点可能换成新的了 t = lastWaiter; &#125; //封装成node Node node = new Node(Thread.currentThread(), Node.CONDITION); //如果尾结点为空，说明队列为空，当前节点是进入队列的第一个元素 if (t == null) firstWaiter = node; //队列不为空 else t.nextWaiter = node; lastWaiter = node; //返回当前线程封装成的节点 return node;&#125; 6.AQS.fullyRelease释放当前节点线程的全部锁资源 正常情况下是会释放成功的，但是可能存在未持有锁的线程调用await方法，错误写法 这个时候，就会将当前线程对应的node状态修改为取消状态，后继线程在假如队列的时候，就会把取消状态的线程清理出去 如果成功释放了锁，会返回当前线程释放的state值，为什么要返回？ 因为当前节点迁移到阻塞队列以后，再次被唤醒，并且当前节点在队列中是头节点的下一个节点而且state状态=0表示无锁，这个时候说明当前节点要去竞争锁，此时要将node的state设置为savedStated 12345678910111213141516171819202122232425262728293031final int fullyRelease(Node node) &#123; /** * 完全释放锁是否成功 * 当failed失败的时候，说明当前线程是未持有锁调用 await 方法的线程... 错误写法 * 假设失败，在finally代码块中 会将刚刚加入到条件队列的 当前线程对应的node状态 修改为 取消状态 * 后继线程就会将取消状态的节点给清理出去... */ boolean failed = true; try &#123; //获取当前线程持有的state值总数 int savedState = getState(); //绝大部分情况下：release这里会返回true if (release(savedState)) &#123; //失败标记设置为false failed = false; /** * 返回当前线程释放的state值 * 为什么要返回？ * 因为当节点被迁移到阻塞队列以后，再次被唤醒，且当前node在阻塞队列中是head.next 而且 * 当前lock状态是state=0 的情况下 当前node 可以获取到锁 此时需要将state 设置为 savedState */ return savedState; &#125; else &#123; throw new IllegalMonitorStateException(); &#125; &#125; finally &#123; //如果失败了，就把当前节点状态改成取消 if (failed) node.waitStatus = Node.CANCELLED; &#125;&#125; 此时，同步队列会触发锁的释放和重新竞争。ThreadB 获得了锁。 7.isOnSyncQueue判断节点是否在阻塞队列 第一个if判断： 节点状态==-2说明当前节点一定在条件队列 当前节点的前驱节点为空：首先分析前置条件 node 的状态肯定不是 -2 ，可能=0或者1，等于1就说明当前节点是未持有锁的await，最终状态会被设置为取消，等于0就说明当前节点已经被持有锁的线程唤醒，但是唤醒之后是先修改状态在入队，所以前驱节点为空，说明此时是当前节点已经被唤醒，但是还未进入到阻塞队列。 执行到第二个if，当前节点的下一个节点不为空，说明一定在同步队列，为啥那？ 执行到这里，可以排除掉取消状态的节点，也就是说node的状态一定不等于1，为啥？取消状态的节点不会被加入到阻塞队列 入队的两个条件：当前节点的前驱节点设置为tail，然后cas设置当前节点为tail节点，两个条件都成功了，才是真正进入到了阻塞队列 所以说，就算前驱节点不等于空，也不是一定就是进入了阻塞队列。可能在过程中。 剩下最后一种情况就是当前这个node节点可能进入到阻塞队列了，但是还不是尾结点，这个时候就需要从后往前便利找到他才行，找到了就返回true，找不到说明当前这个节点正在迁移中，返回false。 12345678910111213141516171819202122232425262728//判断节点是否在阻塞队列final boolean isOnSyncQueue(Node node) &#123; /** * 条件1：node.waitStatus==node.condition * 条件成立说明当前node一定是在条件队列，因为signal方法迁移节点到阻塞队列前，会将node的状态设置为0 * 条件2：前置条件：node.waitStatus ！= Node.CONDITION ====&gt; * node.waitStatus ==0 : 表示当前节点已经被signal * node.waitStatus ==1 : 当前线程是未持有锁调用await 最终这个节点将被取消 * node.waitStatus ==0 为什么还要判断 node.prev == null ？ * 因为signal方法是先修改状态在迁移 */ if (node.waitStatus == Node.CONDITION || node.prev == null) return false; /** * 执行到这里， * node.waitStatus !=Condition 且 node.prev!=null ====&gt; 可以排除掉 node.waitStatus ==1 取消状态 * 为什么可以排除掉取消状态？因为signal方法是不会吧取消状态的节点迁移走的 * 设置prev引用的逻辑是迁移阻塞队列 逻辑的设置的 eng方法 * 入队的逻辑：1.设置node.prev =tail * 2.cas当前node为 阻塞队列的tail节点成功才算是真正的进入到阻塞队列 * 可以推算出，就算是prev不是null，也不能说明当前node已经成功入队到阻塞队列了。 */ if (node.next != null) return true; //执行到这里从阻塞队列的尾巴开始向前遍历查找node，找到了就返回true，找不到返回false， //当前node有可能正在迁移中，还未完成。 return findNodeFromTail(node);&#125; 8.signal 首先当前线程持有的必须是独占锁，否则没有必要进入到阻塞队列，所以如果不是独占锁，会抛出异常。 获取到条件队列的头节点，如果头节点不为空，就说明条件队列里面有节点，去唤醒条件队列的第一个节点。 12345678910public final void signal() &#123; //判断当前线程是不是持有的独占锁，如果不是，直接抛出异常 if (!isHeldExclusively()) throw new IllegalMonitorStateException(); //获取条件队列的首节点 Node first = firstWaiter; //如果首节点不为空 if (first != null) doSignal(first);//唤醒条件队列的首节点线程&#125; 9.doSignal 如果头节点的下一个节点为空，说明头节点出队之后，头节点，尾结点都会是空，也就是条件队列没有元素了。 断开当前头节点的next 调用transferForSignal方法去迁移头节点到阻塞队列，如果成功返回true，失败返回false。 循环的条件是：如果当前头节点迁移失败了，那就将头节点设置为当前头节点的下一个节点继续尝试，直到迁移成功或者头节点为空。 12345678910111213141516private void doSignal(Node first) &#123; do &#123; //当前first马上要出队了，所以更新firstWaiter为当前节点的下一个节点 //如果当前节点的下一个节点是null，说明条件队列只有当前一个节点了...当前出队后，整个队列就空了 //所以需要更新lastWaiter==null if((firstWaiter=first.nextWaiter)==null) lastWaiter=null; //当前节点即将出队列，断开和下一个节点的关系 first.nextWaiter = null; /** * transferForSignal(first) 返回true 当前first节点迁移到阻塞队列成功 false 迁移失败 * while循环：(first =firstWaiter)!=null 当前first 迁移失败 ，则将first更新为first.next 继续尝试迁移 * 直至迁移某个节点成功，或者条件队列为 null 为止。 */ &#125; while (!transferForSignal(first)&amp;&amp;(first =firstWaiter)!=null);&#125; 10.transferForSignal接收到signal信号以后，把节点转入等待队列 首先上来先cas将node的状态从-2设置为0，设置成功才继续往下，那么什么时候会设置失败呢？ 当前节点是取消状态，或者当前节点的线程在挂起期间被其他线程使用中断信号唤醒过，这个时候节点会进入到阻塞队列，这个时候节点的状态也会修改成0 使用enq方法添加当前节点到阻塞队列，并返回当前节点的前驱节点 如果前驱节点的状态大于0就说明前驱结点的状态是取消状态， 第二个条件的前置条件就是前驱节点状态小于0，cas设置前驱节点为signal， 如果条件一或者条件二成立一个，那就唤醒当前节点的线程 12345678910111213141516171819202122232425262728293031//接收到signal信号后，把节点转入等待队列final boolean transferForSignal(Node node) &#123; /** * cas修改当前节点的状态，修改为0，因为当前节点马上要迁移到阻塞队列了 * 成功：当前节点在条件队列中状态正常 * 失败： * 1.取消状态 （线程await时，未持有锁，最终线程对应的node将会被设置为取消状态） * 2.node对应的线程 挂起期间，被其他线程使用中断信号唤醒过...（就会主动进入到阻塞队列，这时 * 也会修改状态为0） */ if (!compareAndSetWaitStatus(node, Node.CONDITION, 0)) return false; //添加节点到等待队列，并返回节点的前继节点(prev) Node p = enq(node); //ws:前驱节点的状态 int ws = p.waitStatus; /** * 条件1：ws&gt;0成立：说明前驱节点的状态在阻塞队列中是取消状态，唤醒当前节点 * 条件2：前置条件 ws&lt;=0 * cas 返回true表示设置前驱节点状态为signal状态成功 * cas返回false，什么时候返回false？ * 当前驱node对应的线程是lockInterrupt入队的node时，是会响应中断的，外部线程给前驱线程中断信号之后 * 前驱node会将状态修改为 取消状态，并且执行出队逻辑... * 前驱节点状态只要不是0 或者 -1 ，那么就唤醒当前线程 */ if (ws &gt; 0 || !compareAndSetWaitStatus(p, ws, Node.SIGNAL)) //如果前节点被取消，说明当前为最后一个等待线程，unpark唤醒当前线程 LockSupport.unpark(node.thread); //如果 node 的 prev 节点已经是signal 状态，那么被阻塞的 ThreadA 的唤醒工作由 AQS 队列来完成 return true;&#125; 执行完 doSignal 以后，会把 condition 队列中的节点转移到 aqs 队列上，逻辑结构图如下，这个时候会判断 ThreadA 的 prev 节点也就是 head 节点的 waitStatus，如果大于 0 或者设置 SIGNAL 失败，表示节点被设置成了 CANCELLED 状态。这个时候会唤醒ThreadA 这个线程。否则就基于 AQS 队列的机制来唤醒，也就是等到 ThreadB 释放锁之后来唤醒 ThreadA。 再往下就是被阻塞的线程唤醒之后的逻辑。​ 11.AQS.await​ 前面在分析 await 方法时，线程会被阻塞。而通过 signal被唤醒之后又继续回到上次执行的逻辑中。​ checkInterruptWhileWaiting 这个方法是干嘛呢？其实从名字就可以看出来，就是 ThreadA 在 condition 队列被阻塞的过程中，有没有被其他线程触发过中断请求。​ 123456789101112131415161718192021222324252627public final void await() throws InterruptedException &#123; if (Thread.interrupted())//表示 await 允许被中断 throw new InterruptedException(); Node node = addConditionWaiter();//创建一个新的节点，节点状态为 condition，采用的数据结构仍然是链表 int savedState = fullyRelease(node); //释放当前的锁，得到锁的状态，并唤醒 AQS 队列中的一个线程 int interruptMode = 0; //如果当前节点没有在同步队列上，即还没有被 signal，则将当前线程阻塞 while (!isOnSyncQueue(node)) &#123;//判断这个节点是否在 AQS 队列上，第一次判断的是 false，因为前面已经释放锁了 LockSupport.park(this);//通过 park 挂起当前线程 if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) break; &#125; // 当这个线程醒来,会尝试拿锁, 当 acquireQueued返回 false 就是拿到锁了. // interruptMode != THROW_IE -&gt; 表示这个线程没有成功将 node 入队,但 signal 执行了 enq 方法让其入队了. // 将这个变量设置成 REINTERRUPT if (acquireQueued(node, savedState) &amp;&amp; interruptMode != THROW_IE) interruptMode = REINTERRUPT; // 如果 node 的下一个等待者不是 null, 则进行清理,清理 Condition 队列上的节点. // 如果是 null ,就没有什么好清理的了. if (node.nextWaiter != null) // clean up if cancelled unlinkCancelledWaiters(); // 如果线程被中断了,需要抛出异常.或者什么都不做 if (interruptMode != 0) reportInterruptAfterWait(interruptMode);&#125; 12.checkInterruptWhileWaiting​ 如果当前线程被中断，则调用transferAfterCancelledWait 方法判断后续的处理应该是抛出 InterruptedException 还是重新中断。​ 这里需要注意的地方是，如果第一次 CAS 失败了，则不能判断当前线程是先进行了中断还是先进行了 signal 方法的调用，可能是先执行了 signal 然后中断，也可能是先执行了中断，后执行了 signal，当然，这两个操作肯定是发生在 CAS 之前。这时需要做的就是等待当前线程的 node被添加到 AQS 队列后，也就是 enq 方法返回后，返回false 告诉 checkInterruptWhileWaiting 方法返回REINTERRUPT(1)，后续进行重新中断。​ 简单来说，该方法的返回值代表当前线程是否在 park 的时候被中断唤醒，如果为 true 表示中断在 signal 调用之前，signal 还未执行，那么这个时候会根据 await 的语义，在 await 时遇到中断需要抛出interruptedException，返回 true 就是告诉checkInterruptWhileWaiting 返回 THROW_IE(-1)。如果返回 false，否则表示 signal 已经执行过了，只需要重新响应中断即可。​ 12345private int checkInterruptWhileWaiting(Node node) &#123; return Thread.interrupted() ? (transferAfterCancelledWait(node) ? THROW_IE : REINTERRUPT) : 0;&#125; 12345678910111213final boolean transferAfterCancelledWait(Node node) &#123; //使用 cas 修改节点状态，如果还能修改成功，说明线程被中断时，signal 还没有被调用。// 这里有一个知识点，就是线程被唤醒，并不一定是在 java 层面执行了locksupport.unpark，也可能是调用了线程的 interrupt()方法，这个方法会更新一个中断标识，并且会唤醒处于阻塞状态下的线程。 if (compareAndSetWaitStatus(node, Node.CONDITION, 0)) &#123; enq(node);//如果 cas 成功，则把node 添加到 AQS 队列 return true; &#125;// 如果 cas 失败，则判断当前 node 是否已经在 AQS 队列上，如果不在，则让给其他线程执行// 当 node 被触发了 signal 方法时， node 就会被加到 aqs 队列上 while (!isOnSyncQueue(node))//循环检测 node 是否已经成功添加到 AQS 队列中。如果没有，则通过 yield Thread.yield();//使当前线程由执行状态，变成为就绪状态，让出cpu时间，在下一个线程执行时候，此线程有可能被执行，也有可能没有被执行。 return false;&#125; 12.AQS.acquireQueued​ 这个方法是当前被唤醒的节点ThreadA 去抢占同步锁。并且要恢复到原本的重入次数状态。调用完这个方法之后，AQS 队列的状态如下：​ 将 head 节点的 waitStatus 设置为-1，Signal 状态。​ 12.reportInterruptAfterWait根据 checkInterruptWhileWaiting 方法返回的中断标识来进行中断上报。 如果是 THROW_IE，则抛出中断异常。​ 如果是 REINTERRUPT，则重新响应中断。​ 1234567private void reportInterruptAfterWait(int interruptMode) throws InterruptedException &#123; if (interruptMode == THROW_IE) throw new InterruptedException(); else if (interruptMode == REINTERRUPT) selfInterrupt();&#125; 13.总结与梳理线程 awaitThread 先通过 lock.lock()方法获取锁成功后调用了 condition.await 方法进入等待队列，而另一个线程 signalThread 通过 lock.lock()方法获取锁成功后调用了 condition.signal 或者 signalAll 方法，使得线程awaitThread 能够有机会移入到同步队列中，当其他线程释放 lock 后使得线程 awaitThread 能够有机会获取lock，从而使得线程 awaitThread 能够从 await 方法中退出执行后续操作。如果 awaitThread 获取 lock 失败会直接进入到同步队列。​ ​ 阻塞：await()方法中，在线程释放锁资源之后，如果节点不在 AQS 等待队列，则阻塞当前线程，如果在等待队列，则自旋等待尝试获取锁。​ 释放：signal()后，节点会从 condition 队列移动到 AQS等待队列，则进入正常锁的获取流程。​","categories":[{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"CompletableFuture","slug":"JUC/CompletableFuture","date":"2022-01-11T11:06:39.043Z","updated":"2022-01-11T11:26:51.313Z","comments":true,"path":"2022/01/11/JUC/CompletableFuture/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/JUC/CompletableFuture/","excerpt":"","text":"一，前置知识1，用户线程和守护线程1.所有用户线程执行完毕，程序就会退出，不再等待守护线程。123public static void main(String[] args) &#123; new Thread(()-&gt; System.out.println(&quot;当前线程&quot;+Thread.currentThread().getName()+&quot;是&quot;+(Thread.currentThread().isDaemon()?&quot;守护线程&quot;:&quot;用户线程&quot;))).start();&#125; 2.用户线程不结束，程序就不会退出123456789101112public class DemoA &#123; public static void main(String[] args) &#123; Thread thread = new Thread(DemoA::run,&quot;线程1&quot;); thread.start(); &#125; private static void run() &#123; System.out.println(&quot;当前线程&quot; + Thread.currentThread().getName() + &quot;是&quot; + (Thread.currentThread().isDaemon() ? &quot;守护线程&quot; : &quot;用户线程&quot;)); while (true) ; &#125;&#125; 3.不管守护线程结束没有，只要用户线程结束，程序就会退出1234567891011121314public class DemoA &#123; public static void main(String[] args) &#123; Thread thread = new Thread(DemoA::run,&quot;线程1&quot;); //将线程1设置为守护线程 thread.setDaemon(true); thread.start(); &#125; private static void run() &#123; System.out.println(&quot;当前线程&quot; + Thread.currentThread().getName() + &quot;是&quot; + (Thread.currentThread().isDaemon() ? &quot;守护线程&quot; : &quot;用户线程&quot;)); while (true) ; &#125;&#125; 2.回首FutureTask1.FutureTask存在的问题123456789101112131415161718192021222324252627public class Demob &#123; public static void main(String[] args)throws Exception &#123; FutureTask&lt;String&gt; futureTask = new FutureTask&lt;&gt;(Demob::call); new Thread(futureTask,&quot;t1&quot;).start(); //异步结果集的展现 ---- 会阻塞 //System.out.println(futureTask.get()); //轮询的方式去问 --- 消耗cpu资源 while (true) &#123; if (futureTask.isDone())&#123; System.out.println(futureTask.get()); break; &#125; &#125; System.out.println(&quot;main结束&quot;); &#125; private static String call() &#123; try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return &quot;yhd&quot;; &#125;&#125; 从上面的代码可以看出FutureTask存在的问题： get()异步结果集的展现 —- 会阻塞 futureTask.isDone()轮询的方式去问 — 消耗cpu资源 2.想完成一些复杂的任务 应对Future的完成时间，完成了可以告诉我，也就是我们的回调通知。 将两个异步计算合成一个异步计算，这两个异步计算互相独立，同时第二个又依赖第一个的结果。 当Future集合中某个任务最快结束时，返回结果。 等待Future集合中的所有任务都完成。 二，CompletableFuture1.介绍 异步函数式编程，Future接口的扩展与增强版。 可以通过回调的方式处理计算结果，并且提供了转换和组合CompletableFuture的方法。 完成通知回调 类似Linux管道符的形式，可以异步的计算，上一个计算结果可以给下一个，结合lambda表达式和函数式编程串起来转换和组合获得结果。 2.实例化如果不指定线程池，默认所有的线程都是守护线程。 123456789101112public class DemoC &#123; private static ThreadPoolExecutor pool=new ThreadPoolExecutor(1,5,1, TimeUnit.SECONDS,new ArrayBlockingQueue&lt;&gt;(1)); public static void main(String[] args) throws Exception&#123; //创建一个有返回结果的实例 CompletableFuture&lt;String&gt; supplyAsync = CompletableFuture.supplyAsync(() -&gt; &quot;yhd&quot;, pool); //创建一个没有返回结果的实例 CompletableFuture&lt;Void&gt; runAsync = CompletableFuture.runAsync(System.out::println); pool.shutdown(); &#125;&#125; 3.异步+回调whenComplete 和 join 的区别： 一个是执行完返回结果，一个是主动获取结果。 whenComplete：是执行当前任务的线程执行继续执行 whenComplete 的任务。whenCompleteAsync：是执行把 whenCompleteAsync 这个任务继续提交给线程池来进行执行。方法不以Async结尾，意味着Action使用相同的线程执行，而Async可能会使用其他线程执行（如果是使用相同的线程池，也可能会被同一个线程选中执行） 1234567891011121314public class DemoC &#123; private static ThreadPoolExecutor pool=new ThreadPoolExecutor(1,5,1, TimeUnit.SECONDS,new ArrayBlockingQueue&lt;&gt;(1)); public static void main(String[] args) throws Exception&#123; //创建一个有返回结果的实例 计算完成时回调 发生异常时执行 CompletableFuture.supplyAsync(() -&gt; &quot;yhd&quot;, pool).whenComplete(DemoC::accept).exceptionally(Throwable::toString); pool.shutdown(); &#125; private static void accept(String result, Throwable e) &#123; System.out.println(result); &#125;&#125; 4.结果处理handle 是执行任务完成时对结果的处理。handle 是在任务完成后再执行，还可以处理异常的任务。 12345678910111213141516/** * @author yhd * @createtime 2020/11/15 23:10 */public class DemoB &#123; public static void main(String[] args) &#123; CompletableFuture.supplyAsync(()-&gt;&quot;尹会东&quot;).handle(DemoB::apply).whenCompleteAsync(((s, throwable) -&gt; System.out.println(s))); &#125; private static String apply(String s, Throwable throwable) &#123; if (null != throwable) return throwable.getMessage(); return s + &quot; 牛逼！&quot;; &#125;&#125; 5.线程串行化thenApply 方法：当一个线程依赖另一个线程时，获取上一个任务返回的结果，并返回当前任务的返回值。thenAccept方法：消费处理结果。接收任务的处理结果，并消费处理，无返回结果。thenRun方法：只要上面的任务执行完成，就开始执行thenRun，只是处理完任务后，执行 thenRun的后续操作带有Async默认是异步执行的。这里所谓的异步指的是不在当前线程内执行。 123456789public class DemoE &#123; public static void main(String[] args) &#123; CompletableFuture&lt;Integer&gt; thenApply = CompletableFuture.supplyAsync(() -&gt; 1).thenApply(integer -&gt; 1); CompletableFuture&lt;Void&gt; thenAccept = CompletableFuture.supplyAsync(() -&gt; 1).thenAccept(System.out::println); CompletableFuture&lt;Void&gt; thenRun = CompletableFuture.supplyAsync(() -&gt; 1).thenRun(System.out::println); System.out.println(thenApply.join()); &#125;&#125; 6.任务组合1.两任务组合 - 都要完成两个任务必须都完成，触发该任务。 thenCombine：组合两个future，获取两个future任务的返回结果，并返回当前任务的返回值thenAcceptBoth：组合两个future，获取两个future任务的返回结果，然后处理任务，没有返回值。runAfterBoth：组合两个future，不需要获取future的结果，只需两个future处理完任务后，处理该任务。 12345678910111213141516public class DemoF &#123; public static void main(String[] args) &#123; CompletableFuture .supplyAsync(() -&gt; &quot;hello&quot;) .thenApplyAsync(t -&gt; t + &quot; world!&quot;) .thenCombineAsync( CompletableFuture .completedFuture(&quot; CompletableFuture&quot;), (t, u) -&gt; t + u) .whenComplete(DemoF::accept); &#125; private static void accept(String t, Throwable u) &#123; System.out.println(t); &#125;&#125; 2.两任务组合 - 一个完成当两个任务中，任意一个future任务完成的时候，执行任务。applyToEither：两个任务有一个执行完成，获取它的返回值，处理任务并有新的返回值。acceptEither：两个任务有一个执行完成，获取它的返回值，处理任务，没有新的返回值。runAfterEither：两个任务有一个执行完成，不需要获取future的结果，处理任务，也没有返回值 3.多任务组合allOf：等待所有任务完成anyOf：只要有一个任务完成 7.计算接口性能1.一淘123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566/** * @author yhd * @createtime 2020/11/15 15:11 * 查询多个电商网站同一个商品的价格 * 同步和异步两种方式 */public class DemoD &#123; static List&lt;Gmall&gt; gmalls= Arrays.asList( new Gmall(&quot;京东&quot;), new Gmall(&quot;拼多多&quot;), new Gmall(&quot;淘宝&quot;), new Gmall(&quot;唯品会&quot;), new Gmall(&quot;分期乐&quot;)); /** * 同步 * @param gmalls * @param productName * @return */ public static List&lt;String&gt; getPriceSync(List&lt;Gmall&gt; gmalls,String productName)&#123; return gmalls.stream().map(gmall-&gt;String.format(productName+&quot;in %s price is %d &quot;,gmall.getGmallName(),gmall.getPriceByProductName(productName))).collect(Collectors.toList()); &#125; /** * 异步 * @param gmalls * @param productName * @return */ public static List&lt;String&gt; getPriceAsync(List&lt;Gmall&gt; gmalls,String productName)&#123; return gmalls.stream().map(gmall-&gt;CompletableFuture.supplyAsync(()-&gt;String.format(productName+&quot;in %s price is %d &quot;,gmall.getGmallName(),gmall.getPriceByProductName(productName)))).collect(Collectors.toList()).stream().map(CompletableFuture::join).collect(Collectors.toList()); &#125; public static void main(String[] args) &#123; long startTime = System.currentTimeMillis(); getPriceSync(gmalls,&quot;金鳞岂是池中物&quot;).forEach(System.out::println); long endTime = System.currentTimeMillis(); System.out.println(&quot;同步&quot;+ (endTime-startTime)); System.out.println(&quot;------------------------&quot;); long s = System.currentTimeMillis(); getPriceAsync(gmalls,&quot;金鳞岂是池中物&quot;).forEach(System.out::println); long e = System.currentTimeMillis(); System.out.println(&quot;异步&quot;+ (e-s)); &#125;&#125;@Data@NoArgsConstructor@AllArgsConstructorclass Gmall&#123; private String gmallName; public Integer getPriceByProductName(String productName)&#123; try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return (int)(Math.random()*100)+1; &#125;&#125; 结果： 123456789101112131415=================================金鳞岂是池中物in 京东 price is 100 金鳞岂是池中物in 拼多多 price is 12 金鳞岂是池中物in 淘宝 price is 25 金鳞岂是池中物in 唯品会 price is 35 金鳞岂是池中物in 分期乐 price is 60 同步5081------------------------金鳞岂是池中物in 京东 price is 57 金鳞岂是池中物in 拼多多 price is 68 金鳞岂是池中物in 淘宝 price is 84 金鳞岂是池中物in 唯品会 price is 16 金鳞岂是池中物in 分期乐 price is 20 异步1387================================= 2.检索123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566/** * @author yhd * @createtime 2020/11/15 23:40 */public class DemoC &#123; //模拟数据库 private static Map&lt;String,Company&gt; db=new HashMap&lt;&gt;(); //模拟查询条件 private static List&lt;String&gt; persons; //初始化数据 static &#123; persons=Arrays.asList(&quot;马云&quot;,&quot;马化腾&quot;,&quot;李彦宏&quot;,&quot;张朝阳&quot;,&quot;刘强东&quot;,&quot;王兴&quot;); db.put(&quot;马云&quot;,new Company(&quot;马云&quot;,&quot;阿里巴巴&quot;)); db.put(&quot;马化腾&quot;,new Company(&quot;马化腾&quot;,&quot;腾讯&quot;)); db.put(&quot;李彦宏&quot;,new Company(&quot;李彦宏&quot;,&quot;百度&quot;)); db.put(&quot;张朝阳&quot;,new Company(&quot;张朝阳&quot;,&quot;搜狐&quot;)); db.put(&quot;刘强东&quot;,new Company(&quot;刘强东&quot;,&quot;京东&quot;)); db.put(&quot;王兴&quot;,new Company(&quot;王兴&quot;,&quot;美团&quot;)); &#125; //模拟去数据库查询 public static Company selectDB(String key)&#123; try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return db.get(key); &#125; public static void main(String[] args) &#123; Sync(); System.out.println(); Async(); System.out.println(); System.out.println(result()); &#125; //同步查询 public static void Sync()&#123; Long startTime=System.currentTimeMillis(); persons.stream().map(key-&gt;selectDB(key).getWorks()).collect(Collectors.toList()).forEach(System.out::println); Long endTime=System.currentTimeMillis(); System.out.println(endTime-startTime); &#125; //异步查询 public static void Async()&#123; Long startTime=System.currentTimeMillis(); persons.stream().map(key-&gt;CompletableFuture.supplyAsync(()-&gt;selectDB(key).getWorks())).collect(Collectors.toList()).stream().map(CompletableFuture::join).collect(Collectors.toList()).forEach(System.out::println); Long endTime=System.currentTimeMillis(); System.out.println(endTime-startTime); &#125; //转换成map返回 public static Map&lt;String,String&gt; result()&#123; return persons.stream().map(key -&gt; CompletableFuture.supplyAsync(() -&gt; selectDB(key))).collect(Collectors.toList()).stream().map(CompletableFuture::join).collect(Collectors.toMap(Company::getPerson,Company::getWorks)); &#125;&#125;@Data@NoArgsConstructor@AllArgsConstructorclass Company&#123; private String person; private String works;&#125; 结果 1234567891011121314151617阿里巴巴腾讯百度搜狐京东美团6053阿里巴巴腾讯百度搜狐京东美团1406&#123;张朝阳=搜狐, 马云=阿里巴巴, 王兴=美团, 李彦宏=百度, 刘强东=京东, 马化腾=腾讯&#125; 3.商品详情1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889/** * @author 二十 * @since 2021/8/28 7:54 下午 */public class CompletableFutureTest &#123; private static ThreadPoolExecutor pool = new ThreadPoolExecutor(1, 5, 1, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;&gt;(1)); public static void main(String[] args) &#123; Map&lt;String, Result&gt; result = new HashMap&lt;&gt;(4); Long s = System.currentTimeMillis(); CompletableFuture&lt;Result&gt; skuInfoFuture = CompletableFuture.supplyAsync(() -&gt; &#123; Result skuInfo = getSkuInfo(1); result.put(skuInfo.getName(), skuInfo); return skuInfo; &#125;, pool); CompletableFuture&lt;Void&gt; skuPriceFuture = CompletableFuture.runAsync(() -&gt; &#123; Result skuPrice = getSkuPrice(1); result.put(skuPrice.getName(), skuPrice); &#125;, pool); CompletableFuture&lt;Void&gt; skuImgsFuture = skuInfoFuture.thenAcceptAsync(r -&gt; &#123; Result skuImgs = getSkuImgs(r.getId()); result.put(skuImgs.getName(), skuImgs); &#125;, pool); CompletableFuture&lt;Void&gt; spuInfoFuture = skuInfoFuture.thenAcceptAsync(r -&gt; &#123; Result spuInfo = getSpuInfo(r.getId()); result.put(spuInfo.getName(), spuInfo); &#125;, pool); CompletableFuture.allOf(skuInfoFuture, skuPriceFuture, skuImgsFuture, spuInfoFuture).join(); System.out.println(result); Long e = System.currentTimeMillis(); System.out.println(&quot;查詢總計耗時：&quot; + (e - s)); pool.shutdown(); &#125; /** * 查询sku基本信息 */ public static Result getSkuInfo(Integer key) &#123; sleep(); return new Result(1, &quot;skuInfo&quot;); &#125; /** * 查询sku图片信息 */ public static Result getSkuImgs(Integer key) &#123; sleep(); return new Result(1, &quot;skuImgs&quot;); &#125; /** * 查询spu销售属性 */ public static Result getSpuInfo(Integer key) &#123; sleep(); return new Result(1, &quot;spuInfo&quot;); &#125; /** * 查询sku价格 */ public static Result getSkuPrice(Integer key) &#123; sleep(); return new Result(1, &quot;skuPrice&quot;); &#125; public static void sleep() &#123; try &#123; TimeUnit.SECONDS.sleep(2); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; @Data @NoArgsConstructor @AllArgsConstructor private static class Result &#123; private Integer id; private String name; &#125;&#125; 哪有什么岁月静好，不过是在你看不见的地方负重前行。","categories":[{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"FutureTask","slug":"JUC/FutureTask","date":"2022-01-11T11:06:27.504Z","updated":"2022-01-11T11:28:34.940Z","comments":true,"path":"2022/01/11/JUC/FutureTask/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/JUC/FutureTask/","excerpt":"","text":"一，Future和FutureTask的关系1.Future12345678910111213/** * @author 二十 * @since 2021/8/28 3:50 下午 */public class FutureTaskTest &#123; public static void main(String[] args) &#123; ThreadPoolExecutor executor = new ThreadPoolExecutor(5, 5, 5, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;&gt;(1)); Future&lt;String&gt; future = executor.submit(() -&gt; &quot;callable&quot;); Future&lt;?&gt; future1 = executor.submit(() -&gt; System.out.println(&quot;runnable&quot;)); FutureTask&lt;String&gt; futureTask = new FutureTask&lt;&gt;(() -&gt; &quot;hahah&quot;); &#125;&#125; 当我们调用线程池的submit()方法的时候，会给我们返回一个Future类型的对象。那么这个Future又是什么？ 12345678910111213141516public interface Future&lt;V&gt; &#123; //取消任务 boolean cancel(boolean mayInterruptIfRunning); //是否取消了任务 boolean isCancelled(); //是否执行完了任务 boolean isDone(); //获取线程的执行结果 V get() throws InterruptedException, ExecutionException; //获取结果超时 V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException;&#125; 这个接口主要的功能就是可以获取到线程执行完成后的结果。​ 2.两者的关系 FutureTask可以看做是Future 和 Runnable 两个接口的实现类。​ 二，FutureTask源码1.属性12345678910111213141516171819202122232425//表示当前任务的状态private volatile int state;//任务尚未执行private static final int NEW = 0;//任务还未结束private static final int COMPLETING = 1;//任务正常执行结束private static final int NORMAL = 2;//任务发生了异常，由callable.call()向上抛出private static final int EXCEPTIONAL = 3;//任务被取消private static final int CANCELLED = 4;//任务正在中断private static final int INTERRUPTING = 5;//任务已经被中断private static final int INTERRUPTED = 6;//submit(runnable/callable) 使用装饰者模式将runnable装饰成callableprivate Callable&lt;V&gt; callable;//正常情况下：用来保存call的执行结果，异常情况下：用来保存call抛出的异常private Object outcome; // non-volatile, protected by state reads/writes//执行当前任务的线程private volatile Thread runner;//因为会有很多线程去get当前任务的结果，所以 这里使用了一种数据结构 stack 头插 头取 的一个队列。private volatile WaitNode waiters; 2.WaitNode内部类这个类里面封装着执行任务的线程和指向下一个线程的指针。​ 12345static final class WaitNode &#123; volatile Thread thread; volatile WaitNode next; WaitNode() &#123; thread = Thread.currentThread(); &#125;&#125; 3.构造器当我们传入一个runnable接口的时候： 1234public FutureTask(Runnable runnable, V result) &#123; this.callable = Executors.callable(runnable, result); this.state = NEW; // ensure visibility of callable&#125; 调用了Executors.callable(runnable, result); 12345public static &lt;T&gt; Callable&lt;T&gt; callable(Runnable task, T result) &#123; if (task == null) throw new NullPointerException(); return new RunnableAdapter&lt;T&gt;(task, result);&#125; 当任务不为空，实际上返回的是一个RunnableAdapter对象。 123456789101112static final class RunnableAdapter&lt;T&gt; implements Callable&lt;T&gt; &#123; final Runnable task; final T result; RunnableAdapter(Runnable task, T result) &#123; this.task = task; this.result = result; &#125; public T call() &#123; task.run(); return result; &#125;&#125; 在这里，将runnable装饰成了callable。​ 4.run()1234567891011121314151617181920212223242526272829303132public void run() &#123; if(任务不是刚创建||任务没有获取到执行权)&#123; return &#125; try&#123; if(如果任务不为空&amp;&amp;任务没有被其他线程执行)&#123; 声明 result 声明 ran = true try&#123; 调用call方法执行任务，并将结果赋值给result ran = true &#125;catch(Exception e)&#123; result=null ran =false 调用setException(ex) &#125; if(ran)&#123; 执行set(result) &#125; &#125; &#125;catch(Exception e)&#123; &#125;finally&#123; 释放任务执行者 if(任务状态为被打断中或者已经被打断)&#123; 执行 handlePossibleCancellationInterrupt(s) &#125; &#125;&#125; 5.set()12345678protected void set(V v)&#123; if(cas的方式设置当前任务的状态为完成中成功)&#123; 将任务结果赋值给outcome 设置当前任务状态为正常结束 调用finishCompletion() &#125;&#125; 6.setException()1234567protected void setException(Throwable t) &#123; if (cas的方式设置当前任务的状态为完成中成功) &#123; 将异常赋值给outcome 设置当前任务状态为异常 finishCompletion(); &#125;&#125; 7.handlePossibleCancellationInterrupt()12345678private void handlePossibleCancellationInterrupt(int s) &#123; if (如果当前任务状态为被打断) while (当前任务状态为被打断) Thread.yield(); // 释放当前线程cpu的执行权&#125; 8.finishCompletion()123456789101112131415161718192021222324private void finishCompletion() &#123; for循环让q指向当前链表的头节点 //这里的操作是为了可能任务还没开始执行就被其他线程取消了，这个时候将等待结果的线程全部唤醒，小概率事件 if(使用cas设置waiters为null成功)&#123; for(;;)&#123; 获取q包装的线程 if(当前节点包装的线程不为空)&#123; 不让q在继续包装这个线程 唤醒q之前包装的线程 &#125; q指向链表的下一个节点 if(下一个节点为空)&#123; break &#125; &#125; break &#125; done() 将callable设置为null帮助gc&#125; 9.done()1protected void done() &#123; &#125; 10.get()123456public V get() throws InterruptedException, ExecutionException &#123; 获取当前任务执行状态 if (当前任务尚未执行完) s = awaitDone(false, 0L); return report(s);&#125; 11.awaitDone()123456789101112131415161718192021222324252627282930//参数说明：是否设置了超时时间 超时时间private int awaitDone(boolean timed, long nanos) throws InterruptedException &#123; final long deadline = timed ? System.nanoTime() + nanos : 0L; 声明 当前线程是否进入获取结果的等待队列 =false for (;;) &#123; if (如果当前线程被其他线程用中断的方式唤醒) &#123; 当前节点出队列 并抛出中断异常 &#125; //假设当前线程是被其他线程以unpark的方式正常唤醒，那么就会走下面的逻辑 获取当前任务状态 //大于完成中有可能正常结束，也有可能异常结束 if (如果当前任务状态大于完成中) &#123; if (如果当前线程创建过node) 释放node return 当前任务状态; &#125; else if (当前任务状态是完成中) 释放线程的cpu执行权，接着等 else if (等待节点为空，说明是第一次自旋，还没有创建节点) 创建一个新的节点 else if (创建好了对象但是还没有入队) cas的方式加入队列 else if (有超时时间) &#123; 走超时逻辑 &#125; else 阻塞当前线程 &#125;&#125; 12.report(int s)123456789//参数说明：当前任务状态private V report(int s) throws ExecutionException &#123; if (任务正常结束) return 任务结果; if (任务被取消或中断) 抛异常 否则就是任务发生异常，将异常向上抛出&#125; 13.cancel()1234567891011121314151617181920public boolean cancel(boolean mayInterruptIfRunning) &#123; //说明此时任务不能取消了，直接返回false if (!(任务状态==刚创建 &amp;&amp; cas修改任务状态为取消或中断成功))) return false; try &#123; if (尝试打断成功) &#123; try &#123; 获取执行任务的线程 if (执行任务的线程！=null) 给执行任务的线程设置中断标识 &#125; finally &#123; cas的方式设置任务的状态为被打断 &#125; &#125; &#125; finally &#123; finishCompletion(); &#125; return true;&#125; 三，流程梳理​ 首先明确一点，不管我们传入的是Runnable还是Callable接口，他最终用的都是Callable，Runnable接口会被他通过装饰者模式封装为Callable。 ​ 一个任务执行的入口其实就是run方法。 ​ 进入run方法，首先他会判断当前任务是否已经被执行过或者当前线程通过cas的方式并没有抢到执行当前任务的机会。如果是的话，说明已经有线程正在执行或者执行完了当前的任务，直接返回即可。 接下来他会判断当前任务是否为空或者当前任务的状态，其实判断状态就是为了判断当前任务是不是被取消了。如果任务不为空并且没有被取消，他会执行call方法（实际上就是我们自己的业务代码）并将结果设置到result将任务的是否被执行状态改成true表示顺利执行完。 ​ call方法执行过程中如果发生异常了，会将结果设置为null，是否被顺利执行的状态为设置为false，表示执行过程发生了异常。 ​ 接下来，它会将异常信息封装到outcome，然后设置但该你任务的状态为异常结束，并自旋的方式唤醒所有等待队列中等待获取结果的线程。 ​ 如果call正常执行完了，他会用cas的方式设置当前任务的完成状态为完成中，如果设置成功，outcome来接收任务的结果，设置当前任务的状态为正常完成状态，并且唤醒所有等待结果的线程。 ​ 最终它会将执行当前任务的线程设置为null，并判断，如果当前任务的状态是被打断中或者已经被打断，他就会自旋判断如果当前线程的状态为被打断，让执行当前任务的线程释放cpu。 ​ get方法是获取当前任务的结果。首先他会获取当前任务的状态，如果状态小于等于未完成首先他会通过自旋的方式，第一次自旋尚未给当前线程创建waitNode对象，此时就需要位当前线程创建waitNode对象。第二次自旋，创建好了对象还没有入队，cas的方式入队。第三次自旋，判断是否设置超时时间，如果没设置超时时间，当前get操作的线程就会被park了。 线程状态会变为 WAITING状态，相当于休眠了..除非有其它线程将你唤醒 或者 将当前线程 中断。他会获取当前线程的任务状态，如果任务还没有完成，释放cpu接着等。如果任务已经完成，此时需要将node设置位null help GC，直接返回当前的状态。除此之外还要判断，如果当前线程被其他线程用中断的方式唤醒，这种唤醒方式会将Thread的中断标记位设置为false，当前线程出队，get方法抛出中断异常。 ​ 如果状态表示已经有结果，会执行report方法。 ​ report方法，如果任务正常执行结束，返回结果，如果任务被取消或者中断了，抛出异常，如果任务执行过程中发生异常结束了，返回异常。 ​ 最后就是任务的取消方法 cancel。他会先判断state == NEW 成立 表示当前任务处于运行中 或者 处于线程池 任务队列中..并且cas修改状态成功，他就会尝试取打断。 ​ 如果尝试打断成功，给runner线程一个中断信号..如果你的程序是响应中断 会走中断逻辑..假设你程序不是响应中断的..啥也不会发生。最后，设置线程状态为中断，唤醒获取结果的线程。","categories":[{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"ConcurrentHashMap","slug":"JUC/ConcurrentHashMap","date":"2022-01-11T11:06:18.507Z","updated":"2022-01-11T11:27:21.431Z","comments":true,"path":"2022/01/11/JUC/ConcurrentHashMap/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/JUC/ConcurrentHashMap/","excerpt":"","text":"一，使用ConcurrentHashMap 是 J.U.C 包里面提供的一个线程安全并且高效的 HashMap，所以ConcurrentHashMap 在并发编程的场景中使用的频率比较高。 ConcurrentHashMap 是 Map 的派生类，所以 api 基本和 Hashmap 是类似，主要就是 put、get 这些方法，接下来基于 ConcurrentHashMap 的 put 和 get 这两个方法作为切入点来分析 ConcurrentHashMap 的源码实现。 二，源码 1.成员变量123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147//散列表数组的最大限制private static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;//散列表默认值private static final int DEFAULT_CAPACITY = 16;static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8;//并发级别：jdk7历史遗留问题，仅仅在初始化的时候使用到，并不是真正的代表并发级别private static final int DEFAULT_CONCURRENCY_LEVEL = 16;//负载因子，JDK1.8中 ConcurrentHashMap 是固定值private static final float LOAD_FACTOR = 0.75f;//树化阈值，指定桶位 链表长度达到8的话，有可能发生树化操作。static final int TREEIFY_THRESHOLD = 8;//红黑树转化为链表的阈值static final int UNTREEIFY_THRESHOLD = 6;//联合TREEIFY_THRESHOLD控制桶位是否树化，只有当table数组长度达到64且 某个桶位 中的链表长度达到8，才会真正树化static final int MIN_TREEIFY_CAPACITY = 64;//线程迁移数据最小步长，控制线程迁移任务最小区间一个值private static final int MIN_TRANSFER_STRIDE = 16;//计算扩容时候生成的一个 标识戳private static int RESIZE_STAMP_BITS = 16;//结果是65535 表示并发扩容最多线程数private static final int MAX_RESIZERS = (1 &lt;&lt; (32 - RESIZE_STAMP_BITS)) - 1;//扩容相关private static final int RESIZE_STAMP_SHIFT = 32 - RESIZE_STAMP_BITS;//当node节点hash=-1 表示当前节点已经被迁移了 ，fwd节点static final int MOVED = -1; // hash for forwarding nodes//node hash=-2 表示当前节点已经树化 且 当前节点为treebin对象 ，代理操作红黑树static final int TREEBIN = -2; // hash for roots of treesstatic final int RESERVED = -3; // hash for transient reservations//转化成二进制实际上是 31个 1 可以将一个负数通过位移运算得到一个正数static final int HASH_BITS = 0x7fffffff; // usable bits of normal node hash//当前系统的cpu数量static final int NCPU = Runtime.getRuntime().availableProcessors();//为了兼容7版本的chp保存的，核心代码并没有使用到private static final ObjectStreamField[] serialPersistentFields = &#123; new ObjectStreamField(&quot;segments&quot;, Segment[].class), new ObjectStreamField(&quot;segmentMask&quot;, Integer.TYPE), new ObjectStreamField(&quot;segmentShift&quot;, Integer.TYPE)&#125;;//散列表，长度一定是2次方数transient volatile Node&lt;K,V&gt;[] table;//扩容过程中，会将扩容中的新table 赋值给nextTable 保持引用，扩容结束之后，这里会被设置为Nullprivate transient volatile Node&lt;K,V&gt;[] nextTable;//LongAdder 中的 baseCount 未发生竞争时 或者 当前LongAdder处于加锁状态时，增量累到到baseCount中private transient volatile long baseCount;/** * sizeCtl &lt; 0 * 1. -1 表示当前table正在初始化（有线程在创建table数组），当前线程需要自旋等待.. * 2.表示当前table数组正在进行扩容 ,高16位表示：扩容的标识戳 低16位表示：（1 + nThread） 当前参与并发扩容的线程数量 * * sizeCtl = 0，表示创建table数组时 使用DEFAULT_CAPACITY为大小 * * sizeCtl &gt; 0 * * 1. 如果table未初始化，表示初始化大小 * 2. 如果table已经初始化，表示下次扩容时的 触发条件（阈值） */private transient volatile int sizeCtl;/** * * 扩容过程中，记录当前进度。所有线程都需要从transferIndex中分配区间任务，去执行自己的任务。 */private transient volatile int transferIndex;/** * LongAdder中的cellsBuzy 0表示当前LongAdder对象无锁状态，1表示当前LongAdder对象加锁状态 */private transient volatile int cellsBusy;/** * LongAdder中的cells数组，当baseCount发生竞争后，会创建cells数组， * 线程会通过计算hash值 取到 自己的cell ，将增量累加到指定cell中 * 总数 = sum(cells) + baseCount */private transient volatile CounterCell[] counterCells;// Unsafe mechanicsprivate static final sun.misc.Unsafe U;/**表示sizeCtl属性在ConcurrentHashMap中内存偏移地址*/private static final long SIZECTL;/**表示transferIndex属性在ConcurrentHashMap中内存偏移地址*/private static final long TRANSFERINDEX;/**表示baseCount属性在ConcurrentHashMap中内存偏移地址*/private static final long BASECOUNT;/**表示cellsBusy属性在ConcurrentHashMap中内存偏移地址*/private static final long CELLSBUSY;/**表示cellValue属性在CounterCell中内存偏移地址*/private static final long CELLVALUE;/**表示数组第一个元素的偏移地址*/private static final long ABASE;private static final int ASHIFT;static &#123; try &#123; U = sun.misc.Unsafe.getUnsafe(); Class&lt;?&gt; k = ConcurrentHashMap.class; SIZECTL = U.objectFieldOffset (k.getDeclaredField(&quot;sizeCtl&quot;)); TRANSFERINDEX = U.objectFieldOffset (k.getDeclaredField(&quot;transferIndex&quot;)); BASECOUNT = U.objectFieldOffset (k.getDeclaredField(&quot;baseCount&quot;)); CELLSBUSY = U.objectFieldOffset (k.getDeclaredField(&quot;cellsBusy&quot;)); Class&lt;?&gt; ck = CounterCell.class; CELLVALUE = U.objectFieldOffset (ck.getDeclaredField(&quot;value&quot;)); Class&lt;?&gt; ak = Node[].class; ABASE = U.arrayBaseOffset(ak); //表示数组单元所占用空间大小,scale 表示Node[]数组中每一个单元所占用空间大小 int scale = U.arrayIndexScale(ak); //1 0000 &amp; 0 1111 = 0 if ((scale &amp; (scale - 1)) != 0) throw new Error(&quot;data type scale not a power of two&quot;); //numberOfLeadingZeros() 这个方法是返回当前数值转换为二进制后，从高位到低位开始统计，看有多少个0连续在一块。 //8 =&gt; 1000 numberOfLeadingZeros(8) = 28 //4 =&gt; 100 numberOfLeadingZeros(4) = 29 //ASHIFT = 31 - 29 = 2 ？？ //ABASE + （5 &lt;&lt; ASHIFT） ASHIFT = 31 - Integer.numberOfLeadingZeros(scale); &#125; catch (Exception e) &#123; throw new Error(e); &#125;&#125; 2.基础方法2.1 spread高位运算 123static final int spread(int h) &#123; return (h ^ (h &gt;&gt;&gt; 16)) &amp; HASH_BITS;&#125; 2.2 tabAt该方法获取对象中offset偏移地址对应的对象field的值。实际上这段代码的含义等价于tab[i],但是为什么不直接使用 tab[i]来计算呢？ getObjectVolatile，一旦看到 volatile 关键字，就表示可见性。因为对 volatile 写操作 happen-before 于 volatile 读操作，因此其他线程对 table 的修改均对 get 读取可见； 虽然 table 数组本身是增加了 volatile 属性，但是“volatile 的数组只针对数组的引用具有volatile 的语义，而不是它的元素”。 所以如果有其他线程对这个数组的元素进行写操作，那么当前线程来读的时候不一定能读到最新的值。出于性能考虑，Doug Lea 直接通过 Unsafe 类来对 table 进行操作。 123static final &lt;K,V&gt; Node&lt;K,V&gt; tabAt(Node&lt;K,V&gt;[] tab, int i) &#123; return (Node&lt;K,V&gt;)U.getObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE);&#125; 2.3 casTabAtcas设置当前节点为桶位的头节点 1234static final &lt;K,V&gt; boolean casTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; c, Node&lt;K,V&gt; v) &#123; return U.compareAndSwapObject(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, c, v);&#125; 2.4 setTabAt123static final &lt;K,V&gt; void setTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; v) &#123; U.putObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, v);&#125; 2.5 resizeStampresizeStamp 用来生成一个和扩容有关的扩容戳，具体有什么作用呢？ 123static final int resizeStamp(int n) &#123; return Integer.numberOfLeadingZeros(n) | (1 &lt;&lt; (RESIZE_STAMP_BITS - 1));&#125; Integer.numberOfLeadingZeros 这个方法是返回无符号整数 n 最高位非 0 位前面的 0 的个数。 比如 10 的二进制是 0000 0000 0000 0000 0000 0000 0000 1010，那么这个方法返回的值就是 28。 根据 resizeStamp 的运算逻辑，我们来推演一下，假如 n=16，那么 resizeStamp(16)=32796转化为二进制是[0000 0000 0000 0000 1000 0000 0001 1100]​ 接着再来看,当第一个线程尝试进行扩容的时候，会执行下面这段代码： 1U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2) rs 左移 16 位，相当于原本的二进制低位变成了高位 1000 0000 0001 1100 0000 0000 00000000 然后再+2 =1000 0000 0001 1100 0000 0000 0000 0000+10=1000 0000 0001 1100 0000 00000000 0010 高 16 位代表扩容的标记、低 16 位代表并行扩容的线程数 这样来存储有什么好处呢？ 1，首先在 CHM 中是支持并发扩容的，也就是说如果当前的数组需要进行扩容操作，可以由多个线程来共同负责 2，可以保证每次扩容都生成唯一的生成戳，每次新的扩容，都有一个不同的 n，这个生成戳就是根据 n 来计算出来的一个数字，n 不同，这个数字也不同 第一个线程尝试扩容的时候，为什么是+2 因为 1 表示初始化，2 表示一个线程在执行扩容，而且对 sizeCtl 的操作都是基于位运算的，所以不会关心它本身的数值是多少，只关心它在二进制上的数值，而 sc + 1 会在低 16 位上加 1。 2.6 tableSizeFor经过多次位移返回大于等于c的最小的二次方数 1234567891011121314151617181920/** * Returns a power of two table size for the given desired capacity. * See Hackers Delight, sec 3.2 * 返回&gt;=c的最小的2的次方数 * c=28 * n=27 =&gt; 0b 11011 * 11011 | 01101 =&gt; 11111 * 11111 | 00111 =&gt; 11111 * .... * =&gt; 11111 + 1 =100000 = 32 */private static final int tableSizeFor(int c) &#123; int n = c - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125; 3. 构造方法1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public ConcurrentHashMap() &#123;&#125;public ConcurrentHashMap(int initialCapacity) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException(); //如果指定的容量超过允许的最大值，设置为最大值 int cap = ((initialCapacity &gt;= (MAXIMUM_CAPACITY &gt;&gt;&gt; 1)) ? MAXIMUM_CAPACITY : tableSizeFor(initialCapacity + (initialCapacity &gt;&gt;&gt; 1) + 1)); /** * sizeCtl &gt; 0 * 当目前table未初始化时，sizeCtl表示初始化容量 */ this.sizeCtl = cap;&#125;public ConcurrentHashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; this.sizeCtl = DEFAULT_CAPACITY; putAll(m);&#125;public ConcurrentHashMap(int initialCapacity, float loadFactor) &#123; this(initialCapacity, loadFactor, 1);&#125;public ConcurrentHashMap(int initialCapacity, float loadFactor, int concurrencyLevel) &#123; //参数校验 if (!(loadFactor &gt; 0.0f) || initialCapacity &lt; 0 || concurrencyLevel &lt;= 0) throw new IllegalArgumentException(); //如果初始容量小于并发级别，那就设置初始容量为并发级别 if (initialCapacity &lt; concurrencyLevel) initialCapacity = concurrencyLevel; //16/0.75 +1 = 22 long size = (long)(1.0 + (long)initialCapacity / loadFactor); // 22 - &gt; 32 int cap = (size &gt;= (long)MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : tableSizeFor((int)size); /** * sizeCtl &gt; 0 * 当目前table未初始化时，sizeCtl表示初始化容量 */ this.sizeCtl = cap;&#125; 4.put1234public V put(K key, V value) &#123; //如果key已经存在，是否覆盖，默认是false return putVal(key, value, false);&#125; 5 putVal123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129final V putVal(K key, V value, boolean onlyIfAbsent) &#123; //控制k 和 v 不能为null if (key == null || value == null) throw new NullPointerException(); //通过spread方法，可以让高位也能参与进寻址运算。 int hash = spread(key.hashCode()); //binCount表示当前k-v 封装成node后插入到指定桶位后，在桶位中的所属链表的下标位置 //0 表示当前桶位为null，node可以直接放着 //2 表示当前桶位已经可能是红黑树 int binCount = 0; //tab 引用map对象的table //自旋 for (Node&lt;K,V&gt;[] tab = table;;) &#123; //f 表示桶位的头结点 //n 表示散列表数组的长度 //i 表示key通过寻址计算后，得到的桶位下标 //fh 表示桶位头结点的hash值 Node&lt;K,V&gt; f; int n, i, fh; //CASE1：成立，表示当前map中的table尚未初始化.. if (tab == null || (n = tab.length) == 0) //最终当前线程都会获取到最新的map.table引用。 tab = initTable(); //CASE2：i 表示key使用路由寻址算法得到 key对应 table数组的下标位置，tabAt 获取指定桶位的头结点 f else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123; //进入到CASE2代码块 前置条件 当前table数组i桶位是Null时。 //使用CAS方式 设置 指定数组i桶位 为 new Node&lt;K,V&gt;(hash, key, value, null),并且期望值是null //cas操作成功 表示ok，直接break for循环即可 //cas操作失败，表示在当前线程之前，有其它线程先你一步向指定i桶位设置值了。 //当前线程只能再次自旋，去走其它逻辑。 if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin &#125; //CASE3：前置条件，桶位的头结点一定不是null。 //条件成立表示当前桶位的头结点 为 FWD结点，表示目前map正处于扩容过程中.. else if ((fh = f.hash) == MOVED) //看到fwd节点后，当前节点有义务帮助当前map对象完成迁移数据的工作 //帮助扩容 tab = helpTransfer(tab, f); //CASE4：当前桶位 可能是 链表 也可能是 红黑树代理结点TreeBin else &#123; //当插入key存在时，会将旧值赋值给oldVal，返回给put方法调用处.. V oldVal = null; //使用sync 加锁“头节点”，理论上是“头结点” synchronized (f) &#123; //为什么又要对比一下，看看当前桶位的头节点 是否为 之前获取的头结点？ //为了避免其它线程将该桶位的头结点修改掉，导致当前线程从sync 加锁 就有问题了。之后所有操作都不用在做了。 if (tabAt(tab, i) == f) &#123;//条件成立，说明咱们 加锁 的对象没有问题，可以进来造了！ //条件成立，说明当前桶位就是普通链表桶位。 if (fh &gt;= 0) &#123; //1.当前插入key与链表当中所有元素的key都不一致时，当前的插入操作是追加到链表的末尾，binCount表示链表长度 //2.当前插入key与链表当中的某个元素的key一致时，当前插入操作可能就是替换了。binCount表示冲突位置（binCount - 1） binCount = 1; //迭代循环当前桶位的链表，e是每次循环处理节点。 for (Node&lt;K,V&gt; e = f;; ++binCount) &#123; //当前循环节点 key K ek; //条件一：e.hash == hash 成立 表示循环的当前元素的hash值与插入节点的hash值一致，需要进一步判断 //条件二：((ek = e.key) == key ||(ek != null &amp;&amp; key.equals(ek))) // 成立：说明循环的当前节点与插入节点的key一致，发生冲突了 if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; //将当前循环的元素的 值 赋值给oldVal oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; &#125; //当前元素 与 插入元素的key不一致 时，会走下面程序。 //1.更新循环处理节点为 当前节点的下一个节点 //2.判断下一个节点是否为null，如果是null，说明当前节点已经是队尾了，插入数据需要追加到队尾节点的后面。 Node&lt;K,V&gt; pred = e; if ((e = e.next) == null) &#123; pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; &#125; &#125; &#125; //前置条件，该桶位一定不是链表 //条件成立，表示当前桶位是 红黑树代理结点TreeBin else if (f instanceof TreeBin) &#123; //p 表示红黑树中如果与你插入节点的key 有冲突节点的话 ，则putTreeVal 方法 会返回冲突节点的引用。 Node&lt;K,V&gt; p; //强制设置binCount为2，因为binCount &lt;= 1 时有其它含义，所以这里设置为了2 binCount = 2; //条件一：成立，说明当前插入节点的key与红黑树中的某个节点的key一致，冲突了 if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) &#123; //将冲突节点的值 赋值给 oldVal oldVal = p.val; if (!onlyIfAbsent) p.val = value; &#125; &#125; &#125; &#125; //说明当前桶位不为null，可能是红黑树 也可能是链表 if (binCount != 0) &#123; //如果binCount&gt;=8 表示处理的桶位一定是链表 if (binCount &gt;= TREEIFY_THRESHOLD) //调用转化链表为红黑树的方法 treeifyBin(tab, i); //说明当前线程插入的数据key，与原有k-v发生冲突，需要将原数据v返回给调用者。 if (oldVal != null) return oldVal; break; &#125; &#125; &#125; //1.统计当前table一共有多少数据 //2.判断是否达到扩容阈值标准，触发扩容。 addCount(1L, binCount); return null;&#125; 6 initTable数组初始化方法，这个方法比较简单，就是初始化一个合适大小的数组。 sizeCtl ：这个标志是在 Node 数组初始化或者扩容的时候的一个控制位标识，负数代表正在进行初始化或者扩容操作。 -1 代表正在初始化 -N 代表有 N-1 个线程正在进行扩容操作，这里不是简单的理解成 n 个线程，sizeCtl 就是-N 0 标识 Node 数组还没有被初始化，正数代表初始化或者下一次扩容的大小 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/** * Initializes table, using the size recorded in sizeCtl. * * sizeCtl &lt; 0 * * 1. -1 表示当前table正在初始化（有线程在创建table数组），当前线程需要自旋等待.. * * 2.表示当前table数组正在进行扩容 ,高16位表示：扩容的标识戳 低16位表示：（1 + nThread） 当前参与并发扩容的线程数量 * * * * sizeCtl = 0，表示创建table数组时 使用DEFAULT_CAPACITY为大小 * * * * sizeCtl &gt; 0 * * * * 1. 如果table未初始化，表示初始化大小 * * 2. 如果table已经初始化，表示下次扩容时的 触发条件（阈值） */private final Node&lt;K,V&gt;[] initTable() &#123; //tab 引用map.table //sc sizeCtl的临时值 Node&lt;K,V&gt;[] tab; int sc; //自旋 条件：map.table 尚未初始化 while ((tab = table) == null || tab.length == 0) &#123; if ((sc = sizeCtl) &lt; 0) //大概率就是-1，表示其它线程正在进行创建table的过程，当前线程没有竞争到初始化table的锁。 Thread.yield(); // lost initialization race; just spin //1.sizeCtl = 0，表示创建table数组时 使用DEFAULT_CAPACITY为大小 //2.如果table未初始化，表示初始化大小 //3.如果table已经初始化，表示下次扩容时的 触发条件（阈值） else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) &#123; try &#123; //这里为什么又要判断呢？ 防止其它线程已经初始化完毕了，然后当前线程再次初始化..导致丢失数据。 //条件成立，说明其它线程都没有进入过这个if块，当前线程就是具备初始化table权利了。 if ((tab = table) == null || tab.length == 0) &#123; //sc大于0 创建table时 使用 sc为指定大小，否则使用 16 默认值. int n = (sc &gt; 0) ? sc : DEFAULT_CAPACITY; @SuppressWarnings(&quot;unchecked&quot;) Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; //最终赋值给 map.table table = tab = nt; //n &gt;&gt;&gt; 2 =&gt; 等于 1/4 n n - (1/4)n = 3/4 n =&gt; 0.75 * n //sc 0.75 n 表示下一次扩容时的触发条件。 sc = n - (n &gt;&gt;&gt; 2); &#125; &#125; finally &#123; //1.如果当前线程是第一次创建map.table的线程话，sc表示的是 下一次扩容的阈值 //2.表示当前线程 并不是第一次创建map.table的线程，当前线程进入到else if 块 时，将 //sizeCtl 设置为了-1 ，那么这时需要将其修改为 进入时的值。 sizeCtl = sc; &#125; break; &#125; &#125; return tab;&#125; 7 addCount123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124private final void addCount(long x, int check) &#123; //as 表示 LongAdder.cells //b 表示LongAdder.base //s 表示当前map.table中元素的数量 CounterCell[] as; long b, s; //条件一：true-&gt;表示cells已经初始化了，当前线程应该去使用hash寻址找到合适的cell 去累加数据 // false-&gt;表示当前线程应该将数据累加到 base //条件二：false-&gt;表示写base成功，数据累加到base中了，当前竞争不激烈，不需要创建cells // true-&gt;表示写base失败，与其他线程在base上发生了竞争，当前线程应该去尝试创建cells。 if ((as = counterCells) != null || !U.compareAndSwapLong(this, BASECOUNT, b = baseCount, s = b + x)) &#123; //有几种情况进入到if块中？ //1.true-&gt;表示cells已经初始化了，当前线程应该去使用hash寻址找到合适的cell 去累加数据 //2.true-&gt;表示写base失败，与其他线程在base上发生了竞争，当前线程应该去尝试创建cells。 //a 表示当前线程hash寻址命中的cell CounterCell a; //v 表示当前线程写cell时的期望值 long v; //m 表示当前cells数组的长度 int m; //true -&gt; 未竞争 false-&gt;发生竞争 boolean uncontended = true; //条件一：as == null || (m = as.length - 1) &lt; 0 //true-&gt; 表示当前线程是通过 写base竞争失败 然后进入的if块，就需要调用fullAddCount方法去扩容 或者 重试.. LongAdder.longAccumulate //条件二：a = as[ThreadLocalRandom.getProbe() &amp; m]) == null 前置条件：cells已经初始化了 //true-&gt;表示当前线程命中的cell表格是个空，需要当前线程进入fullAddCount方法去初始化 cell，放入当前位置. //条件三：!(uncontended = U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x) // false-&gt;取反得到false，表示当前线程使用cas方式更新当前命中的cell成功 // true-&gt;取反得到true,表示当前线程使用cas方式更新当前命中的cell失败，需要进入fullAddCount进行重试 或者 扩容 cells。 if (as == null || (m = as.length - 1) &lt; 0 || (a = as[ThreadLocalRandom.getProbe() &amp; m]) == null || !(uncontended = U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x)) ) &#123; fullAddCount(x, uncontended); //考虑到fullAddCount里面的事情比较累，就让当前线程 不参与到 扩容相关的逻辑了，直接返回到调用点。 return; &#125; if (check &lt;= 1) return; //获取当前散列表元素个数，这是一个期望值 s = sumCount(); &#125; //表示一定是一个put操作调用的addCount if (check &gt;= 0) &#123; //tab 表示map.table //nt 表示map.nextTable //n 表示map.table数组的长度 //sc 表示sizeCtl的临时值 Node&lt;K,V&gt;[] tab, nt; int n, sc; /** * sizeCtl &lt; 0 * 1. -1 表示当前table正在初始化（有线程在创建table数组），当前线程需要自旋等待.. * 2.表示当前table数组正在进行扩容 ,高16位表示：扩容的标识戳 低16位表示：（1 + nThread） 当前参与并发扩容的线程数量 * * sizeCtl = 0，表示创建table数组时 使用DEFAULT_CAPACITY为大小 * * sizeCtl &gt; 0 * * 1. 如果table未初始化，表示初始化大小 * 2. 如果table已经初始化，表示下次扩容时的 触发条件（阈值） */ //自旋 //条件一：s &gt;= (long)(sc = sizeCtl) // true-&gt; 1.当前sizeCtl为一个负数 表示正在扩容中.. // 2.当前sizeCtl是一个正数，表示扩容阈值 // false-&gt; 表示当前table尚未达到扩容条件 //条件二：(tab = table) != null // 恒成立 true //条件三：(n = tab.length) &lt; MAXIMUM_CAPACITY // true-&gt;当前table长度小于最大值限制，则可以进行扩容。 while (s &gt;= (long)(sc = sizeCtl) &amp;&amp; (tab = table) != null &amp;&amp; (n = tab.length) &lt; MAXIMUM_CAPACITY) &#123; //扩容批次唯一标识戳 //16 -&gt; 32 扩容 标识为：1000 0000 0001 1011 int rs = resizeStamp(n); //条件成立：表示当前table正在扩容 // 当前线程理论上应该协助table完成扩容 if (sc &lt; 0) &#123; //条件一：(sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs // true-&gt;说明当前线程获取到的扩容唯一标识戳 非 本批次扩容 // false-&gt;说明当前线程获取到的扩容唯一标识戳 是 本批次扩容 //条件二： JDK1.8 中有bug jira已经提出来了 其实想表达的是 = sc == (rs &lt;&lt; 16 ) + 1 // true-&gt; 表示扩容完毕，当前线程不需要再参与进来了 // false-&gt;扩容还在进行中，当前线程可以参与 //条件三：JDK1.8 中有bug jira已经提出来了 其实想表达的是 = sc == (rs&lt;&lt;16) + MAX_RESIZERS // true-&gt; 表示当前参与并发扩容的线程达到了最大值 65535 - 1 // false-&gt;表示当前线程可以参与进来 //条件四：(nt = nextTable) == null // true-&gt;表示本次扩容结束 // false-&gt;扩容正在进行中 if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex &lt;= 0) break; //前置条件：当前table正在执行扩容中.. 当前线程有机会参与进扩容。 //条件成立：说明当前线程成功参与到扩容任务中，并且将sc低16位值加1，表示多了一个线程参与工作 //条件失败：1.当前有很多线程都在此处尝试修改sizeCtl，有其它一个线程修改成功了，导致你的sc期望值与内存中的值不一致 修改失败 // 2.transfer 任务内部的线程也修改了sizeCtl。 if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) //协助扩容线程，持有nextTable参数 transfer(tab, nt); &#125; //1000 0000 0001 1011 0000 0000 0000 0000 +2 =&gt; 1000 0000 0001 1011 0000 0000 0000 0010 //条件成立，说明当前线程是触发扩容的第一个线程，在transfer方法需要做一些扩容准备工作 else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) //触发扩容条件的线程 不持有nextTable transfer(tab, null); s = sumCount(); &#125; &#125;&#125; 8. transferConcurrentHashMap 支持并发扩容，实现方式是，把 Node 数组进行拆分，让每个线程处理自己的区域，假设 table 数组总长度是 64，默认情况下，那么每个线程可以分到 16 个 bucket。然后每个线程处理的范围，按照倒序来做迁移。 通过 for 自循环处理每个槽位中的链表元素，默认 advace 为真，通过 CAS 设置 transferIndex属性值，并初始化 i 和 bound 值，i 指当前处理的槽位序号，bound 指需要处理的槽位边界，先处理槽位 31 的节点； （bound,i） =(16,31) 从 31 的位置往前推动。​ 每存在一个线程执行完扩容操作，就通过 cas 执行 sc-1。 接着判断(sc-2) !=resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT ; 如果相等，表示当前为整个扩容操作的 最后一个线程，那么意味着整个扩容操作就结束了；如果不相等，说明还得继续。 这么做的目的，一方面是防止不同扩容之间出现相同的 sizeCtl，另外一方面，还可以避免sizeCtl 的 ABA 问题导致的扩容重叠的情况。​ 扩容图解lastRun机制判断是否需要扩容，也就是当更新后的键值对总数 baseCount &gt;= 阈值 sizeCtl 时，进行rehash，这里面会有两个逻辑。 如果当前正在处于扩容阶段，则当前线程会加入并且协助扩容。 如果当前没有在扩容，则直接触发扩容操作。 ​ 扩容操作的核心在于数据的转移，在单线程环境下数据的转移很简单，无非就是把旧数组中的数据迁移到新的数组。但是这在多线程环境下，在扩容的时候其他线程也可能正在添加元素，这时又触发了扩容怎么办？可能大家想到的第一个解决方案是加互斥锁，把转移过程锁住，虽然是可行的解决方案，但是会带来较大的性能开销。因为互斥锁会导致所有访问临界区的线程陷入到阻塞状态，持有锁的线程耗时越长，其他竞争线程就会一直被阻塞，导致吞吐量较低。而且还可能导致死锁。 而 ConcurrentHashMap 并没有直接加锁，而是采用 CAS 实现无锁的并发同步策略，最精华的部分是它可以利用多线程来进行协同扩容。 它把 Node 数组当作多个线程之间共享的任务队列，然后通过维护一个指针来划分每个线程锁负责的区间，每个线程通过区间逆向遍历来实现扩容，一个已经迁移完的bucket会被替换为一个ForwardingNode节点，标记当前bucket已经被其他线程迁移完了。接下来分析一下它的源码实现。 fwd:这个类是个标识类，用于指向新表用的，其他线程遇到这个类会主动跳过这个类，因为这个类要么就是扩容迁移正在进行，要么就是已经完成扩容迁移，也就是这个类要保证线程安全，再进行操作。 advance:这个变量是用于提示代码是否进行推进处理，也就是当前桶处理完，处理下一个桶的标识。 finishing:这个变量用于提示扩容是否结束用的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237private final void transfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab) &#123; //n 表示扩容之前table数组的长度 //stride 表示分配给线程任务的步长 int n = tab.length, stride; // stride 固定为 16 if ((stride = (NCPU &gt; 1) ? (n &gt;&gt;&gt; 3) / NCPU : n) &lt; MIN_TRANSFER_STRIDE) stride = MIN_TRANSFER_STRIDE; // subdivide range //条件成立：表示当前线程为触发本次扩容的线程，需要做一些扩容准备工作 //条件不成立：表示当前线程是协助扩容的线程.. if (nextTab == null) &#123; // initiating try &#123; //创建了一个比扩容之前大一倍的table @SuppressWarnings(&quot;unchecked&quot;) Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n &lt;&lt; 1]; nextTab = nt; &#125; catch (Throwable ex) &#123; // try to cope with OOME sizeCtl = Integer.MAX_VALUE; return; &#125; //赋值给对象属性 nextTable ，方便协助扩容线程 拿到新表 nextTable = nextTab; //记录迁移数据整体位置的一个标记。index计数是从1开始计算的。 transferIndex = n; &#125; //表示新数组的长度 int nextn = nextTab.length; //fwd 节点，当某个桶位数据处理完毕后，将此桶位设置为fwd节点，其它写线程 或读线程看到后，会有不同逻辑。 ForwardingNode&lt;K,V&gt; fwd = new ForwardingNode&lt;K,V&gt;(nextTab); //推进标记 boolean advance = true; //完成标记 boolean finishing = false; // to ensure sweep before committing nextTab //i 表示分配给当前线程任务，执行到的桶位 //bound 表示分配给当前线程任务的下界限制 int i = 0, bound = 0; //自旋 for (;;) &#123; //f 桶位的头结点 //fh 头结点的hash Node&lt;K,V&gt; f; int fh; /** * 1.给当前线程分配任务区间 * 2.维护当前线程任务进度（i 表示当前处理的桶位） * 3.维护map对象全局范围内的进度 */ while (advance) &#123; //分配任务的开始下标 //分配任务的结束下标 int nextIndex, nextBound; //CASE1: //条件一：--i &gt;= bound //成立：表示当前线程的任务尚未完成，还有相应的区间的桶位要处理，--i 就让当前线程处理下一个 桶位. //不成立：表示当前线程任务已完成 或 者未分配 if (--i &gt;= bound || finishing) advance = false; //CASE2: //前置条件：当前线程任务已完成 或 者未分配 //条件成立：表示对象全局范围内的桶位都分配完毕了，没有区间可分配了，设置当前线程的i变量为-1 跳出循环后，执行退出迁移任务相关的程序 //条件不成立：表示对象全局范围内的桶位尚未分配完毕，还有区间可分配 else if ((nextIndex = transferIndex) &lt;= 0) &#123; i = -1; advance = false; &#125; //CASE3: //前置条件：1、当前线程需要分配任务区间 2.全局范围内还有桶位尚未迁移 //条件成立：说明给当前线程分配任务成功 //条件失败：说明分配给当前线程失败，应该是和其它线程发生了竞争吧 else if (U.compareAndSwapInt (this, TRANSFERINDEX, nextIndex, nextBound = (nextIndex &gt; stride ? nextIndex - stride : 0))) &#123; bound = nextBound; i = nextIndex - 1; advance = false; &#125; &#125; //CASE1： //条件一：i &lt; 0 //成立：表示当前线程未分配到任务 if (i &lt; 0 || i &gt;= n || i + n &gt;= nextn) &#123; //保存sizeCtl 的变量 int sc; if (finishing) &#123; nextTable = null; table = nextTab; sizeCtl = (n &lt;&lt; 1) - (n &gt;&gt;&gt; 1); return; &#125; //条件成立：说明设置sizeCtl 低16位 -1 成功，当前线程可以正常退出 if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) &#123; //1000 0000 0001 1011 0000 0000 0000 0000 //条件成立：说明当前线程不是最后一个退出transfer任务的线程 if ((sc - 2) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) //正常退出 return; finishing = advance = true; i = n; // recheck before commit &#125; &#125; //前置条件：【CASE2~CASE4】 当前线程任务尚未处理完，正在进行中 //CASE2: //条件成立：说明当前桶位未存放数据，只需要将此处设置为fwd节点即可。 else if ((f = tabAt(tab, i)) == null) advance = casTabAt(tab, i, null, fwd); //CASE3: //条件成立：说明当前桶位已经迁移过了，当前线程不用再处理了，直接再次更新当前线程任务索引，再次处理下一个桶位 或者 其它操作 else if ((fh = f.hash) == MOVED) advance = true; // already processed //CASE4: //前置条件：当前桶位有数据，而且node节点 不是 fwd节点，说明这些数据需要迁移。 else &#123; //sync 加锁当前桶位的头结点 synchronized (f) &#123; //防止在你加锁头对象之前，当前桶位的头对象被其它写线程修改过，导致你目前加锁对象错误... if (tabAt(tab, i) == f) &#123; //ln 表示低位链表引用 //hn 表示高位链表引用 Node&lt;K,V&gt; ln, hn; //条件成立：表示当前桶位是链表桶位 if (fh &gt;= 0) &#123; //lastRun //可以获取出 当前链表 末尾连续高位不变的 node int runBit = fh &amp; n; Node&lt;K,V&gt; lastRun = f; for (Node&lt;K,V&gt; p = f.next; p != null; p = p.next) &#123; int b = p.hash &amp; n; if (b != runBit) &#123; runBit = b; lastRun = p; &#125; &#125; //条件成立：说明lastRun引用的链表为 低位链表，那么就让 ln 指向 低位链表 if (runBit == 0) &#123; ln = lastRun; hn = null; &#125; //否则，说明lastRun引用的链表为 高位链表，就让 hn 指向 高位链表 else &#123; hn = lastRun; ln = null; &#125; for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) &#123; int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); else hn = new Node&lt;K,V&gt;(ph, pk, pv, hn); &#125; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; &#125; //条件成立：表示当前桶位是 红黑树 代理结点TreeBin else if (f instanceof TreeBin) &#123; //转换头结点为 treeBin引用 t TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; //低位双向链表 lo 指向低位链表的头 loTail 指向低位链表的尾巴 TreeNode&lt;K,V&gt; lo = null, loTail = null; //高位双向链表 lo 指向高位链表的头 loTail 指向高位链表的尾巴 TreeNode&lt;K,V&gt; hi = null, hiTail = null; //lc 表示低位链表元素数量 //hc 表示高位链表元素数量 int lc = 0, hc = 0; //迭代TreeBin中的双向链表，从头结点 至 尾节点 for (Node&lt;K,V&gt; e = t.first; e != null; e = e.next) &#123; // h 表示循环处理当前元素的 hash int h = e.hash; //使用当前节点 构建出来的 新的 TreeNode TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt; (h, e.key, e.val, null, null); //条件成立：表示当前循环节点 属于低位链 节点 if ((h &amp; n) == 0) &#123; //条件成立：说明当前低位链表 还没有数据 if ((p.prev = loTail) == null) lo = p; //说明 低位链表已经有数据了，此时当前元素 追加到 低位链表的末尾就行了 else loTail.next = p; //将低位链表尾指针指向 p 节点 loTail = p; ++lc; &#125; //当前节点 属于 高位链 节点 else &#123; if ((p.prev = hiTail) == null) hi = p; else hiTail.next = p; hiTail = p; ++hc; &#125; &#125; ln = (lc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(lo) : (hc != 0) ? new TreeBin&lt;K,V&gt;(lo) : t; hn = (hc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(hi) : (lc != 0) ? new TreeBin&lt;K,V&gt;(hi) : t; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; &#125; &#125; &#125; &#125; &#125;&#125; 链表迁移原理 1）高低位原理分析ConcurrentHashMap 在做链表迁移时，会用高低位来实现，这里有两个问题要分析一下 1，如何实现高低位链表的区分 假如有这样一个队列 第 14 个槽位插入新节点之后，链表元素个数已经达到了 8，且数组长度为 16，优先通过扩容来缓解链表过长的问题 假如当前线程正在处理槽位为 14 的节点，它是一个链表结构，在代码中，首先定义两个变量节点 ln 和 hn，实际就是 lowNode 和 HighNode，分别保存 hash 值的第 x 位为 0 和不等于0 的节点 通过 fn&amp;n 可以把这个链表中的元素分为两类，A 类是 hash 值的第 X 位为 0，B 类是 hash 值的第 x 位为不等于 0（至于为什么要这么区分，稍后分析），并且通过 lastRun 记录最后要处理的节点。最终要达到的目的是，A 类的链表保持位置不动，B 类的链表为 14+16(扩容增加的长度)=30 把 14 槽位的链表单独伶出来，用蓝色表示 fn&amp;n=0 的节点，假如链表的分类是这样 1234567for (Node&lt;K,V&gt; p = f.next; p != null; p = p.next) &#123; int b = p.hash &amp; n; if (b != runBit) &#123; runBit = b; lastRun = p; &#125;&#125; 通过上面这段代码遍历，会记录 runBit 以及 lastRun，按照上面这个结构，那么 runBit 应该是蓝色节点，lastRun 应该是第 6 个节点接着，再通过这段代码进行遍历，生成 ln 链以及 hn 链 1234567for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) &#123; int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); else hn = new Node&lt;K,V&gt;(ph, pk, pv, hn);&#125; 接着，通过 CAS 操作，把 hn 链放在 i+n 也就是 14+16 的位置，ln 链保持原来的位置不动。并且设置当前节点为 fwd，表示已经被当前线程迁移完了。 123setTabAt(nextTab, i, ln);setTabAt(nextTab, i + n, hn);setTabAt(tab, i, fwd); 迁移完成以后的数据分布如下 2）为什么要做高低位的划分要想了解这么设计的目的，我们需要从 ConcurrentHashMap 的根据下标获取对象的算法来看，在 putVal 方法中 1018 行： 1(f = tabAt(tab, i = (n - 1) &amp; hash)) == null 通过(n-1) &amp; hash 来获得在 table 中的数组下标来获取节点数据，【&amp;运算是二进制运算符，1&amp; 1=1，其他都为 0】。 9.helpTransfer如果对应的节点存在，判断这个节点的 hash 是不是等于 MOVED(-1)，说明当前节点是ForwardingNode 节点，意味着有其他线程正在进行扩容，那么当前现在直接帮助它进行扩容，因此调用 helpTransfer方法。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455final Node&lt;K,V&gt;[] helpTransfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt; f) &#123; //nextTab 引用的是 fwd.nextTable == map.nextTable 理论上是这样。 //sc 保存map.sizeCtl Node&lt;K,V&gt;[] nextTab; int sc; //条件一：tab != null 恒成立 true //条件二：(f instanceof ForwardingNode) 恒成立 true //条件三：((ForwardingNode&lt;K,V&gt;)f).nextTable) != null 恒成立 true if (tab != null &amp;&amp; (f instanceof ForwardingNode) &amp;&amp; (nextTab = ((ForwardingNode&lt;K,V&gt;)f).nextTable) != null) &#123; //拿当前标的长度 获取 扩容标识戳 假设 16 -&gt; 32 扩容：1000 0000 0001 1011 int rs = resizeStamp(tab.length); //条件一：nextTab == nextTable //成立：表示当前扩容正在进行中 //不成立：1.nextTable被设置为Null 了，扩容完毕后，会被设为Null // 2.再次出发扩容了...咱们拿到的nextTab 也已经过期了... //条件二：table == tab //成立：说明 扩容正在进行中，还未完成 //不成立：说明扩容已经结束了，扩容结束之后，最后退出的线程 会设置 nextTable 为 table //条件三：(sc = sizeCtl) &lt; 0 //成立：说明扩容正在进行中 //不成立：说明sizeCtl当前是一个大于0的数，此时代表下次扩容的阈值，当前扩容已经结束。 while (nextTab == nextTable &amp;&amp; table == tab &amp;&amp; (sc = sizeCtl) &lt; 0) &#123; //条件一：(sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs // true-&gt;说明当前线程获取到的扩容唯一标识戳 非 本批次扩容 // false-&gt;说明当前线程获取到的扩容唯一标识戳 是 本批次扩容 //条件二： JDK1.8 中有bug jira已经提出来了 其实想表达的是 = sc == (rs &lt;&lt; 16 ) + 1 // true-&gt; 表示扩容完毕，当前线程不需要再参与进来了 // false-&gt;扩容还在进行中，当前线程可以参与 //条件三：JDK1.8 中有bug jira已经提出来了 其实想表达的是 = sc == (rs&lt;&lt;16) + MAX_RESIZERS // true-&gt; 表示当前参与并发扩容的线程达到了最大值 65535 - 1 // false-&gt;表示当前线程可以参与进来 //条件四：transferIndex &lt;= 0 // true-&gt;说明map对象全局范围内的任务已经分配完了，当前线程进去也没活干.. // false-&gt;还有任务可以分配。 if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || transferIndex &lt;= 0) break; if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) &#123; transfer(tab, nextTab); break; &#125; &#125; return nextTab; &#125; return table;&#125; 10.get1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public V get(Object key) &#123; //tab 引用map.table //e 当前元素 //p 目标节点 //n table数组长度 //eh 当前元素hash //ek 当前元素key Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; e, p; int n, eh; K ek; //扰动运算后得到 更散列的hash值 int h = spread(key.hashCode()); //条件一：(tab = table) != null //true-&gt;表示已经put过数据，并且map内部的table也已经初始化完毕 //false-&gt;表示创建完map后，并没有put过数据，map内部的table是延迟初始化的，只有第一次写数据时会触发创建逻辑。 //条件二：(n = tab.length) &gt; 0 true-&gt;表示table已经初始化 //条件三：(e = tabAt(tab, (n - 1) &amp; h)) != null //true-&gt;当前key寻址的桶位 有值 //false-&gt;当前key寻址的桶位中是null，是null直接返回null if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (e = tabAt(tab, (n - 1) &amp; h)) != null) &#123; //前置条件：当前桶位有数据 //对比头结点hash与查询key的hash是否一致 //条件成立：说明头结点与查询Key的hash值 完全一致 if ((eh = e.hash) == h) &#123; //完全比对 查询key 和 头结点的key //条件成立：说明头结点就是查询数据 if ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) return e.val; &#125; //条件成立： //1.-1 fwd 说明当前table正在扩容，且当前查询的这个桶位的数据 已经被迁移走了 //2.-2 TreeBin节点，需要使用TreeBin 提供的find 方法查询。 else if (eh &lt; 0) return (p = e.find(h, key)) != null ? p.val : null; //当前桶位已经形成链表的这种情况 while ((e = e.next) != null) &#123; if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) return e.val; &#125; &#125; return null;&#125; 11.remove123public V remove(Object key) &#123; return replaceNode(key, null, null);&#125; 12.replaceNode123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143final V replaceNode(Object key, V value, Object cv) &#123; //计算key经过扰动运算后的hash int hash = spread(key.hashCode()); //自旋 for (Node&lt;K,V&gt;[] tab = table;;) &#123; //f表示桶位头结点 //n表示当前table数组长度 //i表示hash命中桶位下标 //fh表示桶位头结点 hash Node&lt;K,V&gt; f; int n, i, fh; //CASE1： //条件一：tab == null true-&gt;表示当前map.table尚未初始化.. false-&gt;已经初始化 //条件二：(n = tab.length) == 0 true-&gt;表示当前map.table尚未初始化.. false-&gt;已经初始化 //条件三：(f = tabAt(tab, i = (n - 1) &amp; hash)) == null true -&gt; 表示命中桶位中为null，直接break， 会返回 if (tab == null || (n = tab.length) == 0 || (f = tabAt(tab, i = (n - 1) &amp; hash)) == null) break; //CASE2： //前置条件CASE2 ~ CASE3：当前桶位不是null //条件成立：说明当前table正在扩容中，当前是个写操作，所以当前线程需要协助table完成扩容。 else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); //CASE3: //前置条件CASE2 ~ CASE3：当前桶位不是null //当前桶位 可能是 &quot;链表&quot; 也可能 是 &quot;红黑树&quot; TreeBin else &#123; //保留替换之前的数据引用 V oldVal = null; //校验标记 boolean validated = false; //加锁当前桶位 头结点，加锁成功之后会进入 代码块。 synchronized (f) &#123; //判断sync加锁是否为当前桶位 头节点，防止其它线程，在当前线程加锁成功之前，修改过 桶位 的头结点。 //条件成立：当前桶位头结点 仍然为f，其它线程没修改过。 if (tabAt(tab, i) == f) &#123; //条件成立：说明桶位 为 链表 或者 单个 node if (fh &gt;= 0) &#123; validated = true; //e 表示当前循环处理元素 //pred 表示当前循环节点的上一个节点 Node&lt;K,V&gt; e = f, pred = null; for (;;) &#123; //当前节点key K ek; //条件一：e.hash == hash true-&gt;说明当前节点的hash与查找节点hash一致 //条件二：((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) //if 条件成立，说明key 与查询的key完全一致。 if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; //当前节点的value V ev = e.val; //条件一：cv == null true-&gt;替换的值为null 那么就是一个删除操作 //条件二：cv == ev || (ev != null &amp;&amp; cv.equals(ev)) 那么是一个替换操作 if (cv == null || cv == ev || (ev != null &amp;&amp; cv.equals(ev))) &#123; //删除 或者 替换 //将当前节点的值 赋值给 oldVal 后续返回会用到 oldVal = ev; //条件成立：说明当前是一个替换操作 if (value != null) //直接替换 e.val = value; //条件成立：说明当前节点非头结点 else if (pred != null) //当前节点的上一个节点，指向当前节点的下一个节点。 pred.next = e.next; else //说明当前节点即为 头结点，只需要将 桶位设置为头结点的下一个节点。 setTabAt(tab, i, e.next); &#125; break; &#125; pred = e; if ((e = e.next) == null) break; &#125; &#125; //条件成立：TreeBin节点。 else if (f instanceof TreeBin) &#123; validated = true; //转换为实际类型 TreeBin t TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; //r 表示 红黑树 根节点 //p 表示 红黑树中查找到对应key 一致的node TreeNode&lt;K,V&gt; r, p; //条件一：(r = t.root) != null 理论上是成立 //条件二：TreeNode.findTreeNode 以当前节点为入口，向下查找key（包括本身节点） // true-&gt;说明查找到相应key 对应的node节点。会赋值给p if ((r = t.root) != null &amp;&amp; (p = r.findTreeNode(hash, key, null)) != null) &#123; //保存p.val 到pv V pv = p.val; //条件一：cv == null 成立：不必对value，就做替换或者删除操作 //条件二：cv == pv ||(pv != null &amp;&amp; cv.equals(pv)) 成立：说明“对比值”与当前p节点的值 一致 if (cv == null || cv == pv || (pv != null &amp;&amp; cv.equals(pv))) &#123; //替换或者删除操作 oldVal = pv; //条件成立：替换操作 if (value != null) p.val = value; //删除操作 else if (t.removeTreeNode(p)) //这里没做判断，直接搞了...很疑惑 setTabAt(tab, i, untreeify(t.first)); &#125; &#125; &#125; &#125; &#125; //当其他线程修改过桶位 头结点时，当前线程 sync 头结点 锁错对象时，validated 为false，会进入下次for 自旋 if (validated) &#123; if (oldVal != null) &#123; //替换的值 为null，说明当前是一次删除操作，oldVal ！=null 成立，说明删除成功，更新当前元素个数计数器。 if (value == null) addCount(-1L, -1); return oldVal; &#125; break; &#125; &#125; &#125; return null;&#125; 13.TreeBin13.1 属性1234567891011121314151617//红黑树 根节点 TreeNode&lt;K,V&gt; root;//链表的头节点volatile TreeNode&lt;K,V&gt; first;//等待者线程（当前lockState是读锁状态）volatile Thread waiter;/** * 1.写锁状态 写是独占状态，以散列表来看，真正进入到TreeBin中的写线程 同一时刻 只有一个线程。 1 * 2.读锁状态 读锁是共享，同一时刻可以有多个线程 同时进入到 TreeBin对象中获取数据。 每一个线程 都会给 lockStat + 4 * 3.等待者状态（写线程在等待），当TreeBin中有读线程目前正在读取数据时，写线程无法修改数据，那么就将lockState的最低2位 设置为 0b 10 */volatile int lockState;// values for lockStatestatic final int WRITER = 1; // set while holding write lockstatic final int WAITER = 2; // set when waiting for write lockstatic final int READER = 4; // increment value for setting read lock 13.2 构造器1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586TreeBin(TreeNode&lt;K,V&gt; b) &#123; //设置节点hash为-2 表示此节点是TreeBin节点 super(TREEBIN, null, null, null); //使用first 引用 treeNode链表 this.first = b; //r 红黑树的根节点引用 TreeNode&lt;K,V&gt; r = null; //x表示遍历的当前节点 for (TreeNode&lt;K,V&gt; x = b, next; x != null; x = next) &#123; next = (TreeNode&lt;K,V&gt;)x.next; //强制设置当前插入节点的左右子树为null x.left = x.right = null; //条件成立：说明当前红黑树 是一个空树，那么设置插入元素 为根节点 if (r == null) &#123; //根节点的父节点 一定为 null x.parent = null; //颜色改为黑色 x.red = false; //让r引用x所指向的对象。 r = x; &#125; else &#123; //非第一次循环，都会来带else分支，此时红黑树已经有数据了 //k 表示 插入节点的key K k = x.key; //h 表示 插入节点的hash int h = x.hash; //kc 表示 插入节点key的class类型 Class&lt;?&gt; kc = null; //p 表示 为查找插入节点的父节点的一个临时节点 TreeNode&lt;K,V&gt; p = r; for (;;) &#123; //dir (-1, 1) //-1 表示插入节点的hash值大于 当前p节点的hash //1 表示插入节点的hash值 小于 当前p节点的hash //ph p表示 为查找插入节点的父节点的一个临时节点的hash int dir, ph; //临时节点 key K pk = p.key; //插入节点的hash值 小于 当前节点 if ((ph = p.hash) &gt; h) //插入节点可能需要插入到当前节点的左子节点 或者 继续在左子树上查找 dir = -1; //插入节点的hash值 大于 当前节点 else if (ph &lt; h) //插入节点可能需要插入到当前节点的右子节点 或者 继续在右子树上查找 dir = 1; //如果执行到 CASE3，说明当前插入节点的hash 与 当前节点的hash一致，会在case3 做出最终排序。最终 //拿到的dir 一定不是0，（-1， 1） else if ((kc == null &amp;&amp; (kc = comparableClassFor(k)) == null) || (dir = compareComparables(kc, k, pk)) == 0) dir = tieBreakOrder(k, pk); //xp 想要表示的是 插入节点的 父节点 TreeNode&lt;K,V&gt; xp = p; //条件成立：说明当前p节点 即为插入节点的父节点 //条件不成立：说明p节点 底下还有层次，需要将p指向 p的左子节点 或者 右子节点，表示继续向下搜索。 if ((p = (dir &lt;= 0) ? p.left : p.right) == null) &#123; //设置插入节点的父节点 为 当前节点 x.parent = xp; //小于P节点，需要插入到P节点的左子节点 if (dir &lt;= 0) xp.left = x; //大于P节点，需要插入到P节点的右子节点 else xp.right = x; //插入节点后，红黑树性质 可能会被破坏，所以需要调用 平衡方法 r = balanceInsertion(r, x); break; &#125; &#125; &#125; &#125; //将r 赋值给 TreeBin对象的 root引用。 this.root = r; assert checkInvariants(root);&#125; 13.3 putTreeVal12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970final TreeNode&lt;K,V&gt; putTreeVal(int h, K k, V v) &#123; Class&lt;?&gt; kc = null; boolean searched = false; for (TreeNode&lt;K,V&gt; p = root;;) &#123; int dir, ph; K pk; if (p == null) &#123; first = root = new TreeNode&lt;K,V&gt;(h, k, v, null, null); break; &#125; else if ((ph = p.hash) &gt; h) dir = -1; else if (ph &lt; h) dir = 1; else if ((pk = p.key) == k || (pk != null &amp;&amp; k.equals(pk))) return p; else if ((kc == null &amp;&amp; (kc = comparableClassFor(k)) == null) || (dir = compareComparables(kc, k, pk)) == 0) &#123; if (!searched) &#123; TreeNode&lt;K,V&gt; q, ch; searched = true; if (((ch = p.left) != null &amp;&amp; (q = ch.findTreeNode(h, k, kc)) != null) || ((ch = p.right) != null &amp;&amp; (q = ch.findTreeNode(h, k, kc)) != null)) return q; &#125; dir = tieBreakOrder(k, pk); &#125; TreeNode&lt;K,V&gt; xp = p; if ((p = (dir &lt;= 0) ? p.left : p.right) == null) &#123; //当前循环节点xp 即为 x 节点的爸爸 //x 表示插入节点 //f 老的头结点 TreeNode&lt;K,V&gt; x, f = first; first = x = new TreeNode&lt;K,V&gt;(h, k, v, f, xp); //条件成立：说明链表有数据 if (f != null) //设置老的头结点的前置引用为 当前的头结点。 f.prev = x; if (dir &lt;= 0) xp.left = x; else xp.right = x; if (!xp.red) x.red = true; else &#123; //表示 当前新插入节点后，新插入节点 与 父节点 形成 “红红相连” lockRoot(); try &#123; //平衡红黑树，使其再次符合规范。 root = balanceInsertion(root, x); &#125; finally &#123; unlockRoot(); &#125; &#125; break; &#125; &#125; assert checkInvariants(root); return null;&#125; 13.4 find123456789101112131415161718192021222324252627282930313233343536373839404142434445464748final Node&lt;K,V&gt; find(int h, Object k) &#123; if (k != null) &#123; //e 表示循环迭代的当前节点 迭代的是first引用的链表 for (Node&lt;K,V&gt; e = first; e != null; ) &#123; //s 保存的是lock临时状态 //ek 链表当前节点 的key int s; K ek; //(WAITER|WRITER) =&gt; 0010 | 0001 =&gt; 0011 //lockState &amp; 0011 != 0 条件成立：说明当前TreeBin 有等待者线程 或者 目前有写操作线程正在加锁 if (((s = lockState) &amp; (WAITER|WRITER)) != 0) &#123; if (e.hash == h &amp;&amp; ((ek = e.key) == k || (ek != null &amp;&amp; k.equals(ek)))) return e; e = e.next; &#125; //前置条件：当前TreeBin中 等待者线程 或者 写线程 都没有 //条件成立：说明添加读锁成功 else if (U.compareAndSwapInt(this, LOCKSTATE, s, s + READER)) &#123; TreeNode&lt;K,V&gt; r, p; try &#123; //查询操作 p = ((r = root) == null ? null : r.findTreeNode(h, k, null)); &#125; finally &#123; //w 表示等待者线程 Thread w; //U.getAndAddInt(this, LOCKSTATE, -READER) == (READER|WAITER) //1.当前线程查询红黑树结束，释放当前线程的读锁 就是让 lockstate 值 - 4 //(READER|WAITER) = 0110 =&gt; 表示当前只有一个线程在读，且“有一个线程在等待” //当前读线程为 TreeBin中的最后一个读线程。 //2.(w = waiter) != null 说明有一个写线程在等待读操作全部结束。 if (U.getAndAddInt(this, LOCKSTATE, -READER) == (READER|WAITER) &amp;&amp; (w = waiter) != null) //使用unpark 让 写线程 恢复运行状态。 LockSupport.unpark(w); &#125; return p; &#125; &#125; &#125; return null;&#125; 三，总结在java8中，ConcurrentHashMap使用数组+链表+红黑树的组合方式，利用cas和synchronized保证并发写的安全。​ 引入红黑树的原因：链表查询的时间复杂度为On，但是红黑树的查询时间复杂度为O(log(n)),所以在节点比较多的情况下，使用红黑树可以大大提升性能。 ​ 链式桶是一个由node节点组成的链表。树状桶是一颗由TreeNode节点组成的红黑树。输的根节点为TreeBin类型。​ 当链表长度大于8整个hash表长度大于64的时候，就会转化为TreeBin。TreeBin作为根节点，其实就是红黑树对象。在ConcurrentHashMap的table数组中，存放的就是TreeBin对象，而不是TreeNoe对象。​ 数组table是懒加载的，只有第一次添加元素的时候才会初始化，所以initTable()存在线程安全问题。​ 重要的属性就是sizeCtl，用来控制table的初始化和扩容操作的过程：​ -1代表table正在初始化，其他线程直接join等待。 -N代表有N-1个线程正在进行扩容操作，严格来说，当其为负数的时候，只用到了低16位，如果低16位为M，此时有M-1个线程进行扩容。 大于0有两种情况：如果table没有初始化，她就表示table初始化的大小，如果table初始化完了，就表示table的容量，默认是table大小的四分之三。 ​ Transfer()扩容​ table数据转移到nextTable。扩容操作的核心在于数据的转移，把旧数组中的数据迁移到新的数组。ConcurrentHashMap精华的部分是它可以利用多线程来进行协同扩容，简单来说，它把table数组当作多个线程之间共享的任务队列，然后通过维护一个指针来划分每个线程所负责的区间，每个线程通过区间逆向遍历来实现扩容，一个已经迁移完的 Bucket会被替换为一个Forwarding节点，标记当前Bucket已经被其他线程迁移完了。​ helpTransfer()帮助扩容​ ConcurrentHashMap并发添加元素时，如果正在扩容，其他线程会帮助扩容，也就是多线程扩容。​ 第一次添加元素时，默认初始长度为16，当往table中继续添加元素时，通过Hash值跟数组长度取余来决定放在数组的哪个Bucket位置，如果出现放在同一个位置，就优先以链表的形式存放，在同一个位置的个数达到了8个以上，如果数组的长度还小于64，就会扩容数组。如果数组的长度大于等于64，就会将该节点的链表转换成树。​ 通过扩容数组的方式来把这些节点分散开。然后将这些元素复制到扩容后的新数组中，同一个Bucket中的元素通过Hash值的数组长度位来重新确定位置，可能还是放在原来的位置，也可能放到新的位置。而且，在扩容完成之后，如果之前某个节点是树，但是现在该节点的“Key-Value对”数又小于等于6个，就会将该树转为链表。​ put()​ JDK1.8在使用CAS自旋完成桶的设置时，使用synchronized内置锁保证桶内并发操作的线程安全。尽管对同一个Map操作的线程争用会非常激烈，但是在同一个桶内的线程争用通常不会很激烈，所以使用CAS自旋、synchronized不会降低ConcurrentHashMap的性能。为什么不用ReentrantLock显式锁呢?如果为每 个桶都创建一个ReentrantLock实 例，就会带来大量的内存消耗，反过来，使用CAS自旋、synchronized，内存消耗的增加更小。​ get()​ get()通过UnSafe的getObjectVolatile()来读取数组中的元素。为什么要这样做?虽然HashEntry数组的引用是volatile类型，但是数组内元素的 用不是volatile类型，因此多线程对 数组元素的修改是不安全的，可能会在数组中读取到尚未构造完成的元素对象。get()方法通过UnSafe的getObjectVolatile方法来保证元素的读取安全，调用getObjectVolatile()去读取数组元素需要先获得元素在数组中的偏移量，在这里，get()方法根据哈希码计算出偏移量为u，然后通过偏移量u来尝试读取数值。​ ​ ​ ​ ​ ​","categories":[{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"LongAdder","slug":"JUC/LongAdder","date":"2022-01-11T11:06:11.172Z","updated":"2022-01-11T11:28:53.952Z","comments":true,"path":"2022/01/11/JUC/LongAdder/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/JUC/LongAdder/","excerpt":"","text":"一，为什么要用LongAdder 【参考】volatile 解决多线程内存不可见问题。对于一写多读，是可以解决变量同步问题，但是如果多写，同样无法解决线程安全问题。说明：如果是 count++ 操作，使用如下类实现：AtomicInteger count = new AtomicInteger(); count.addAndGet(1); 如果是 JDK8，推荐使用 LongAdder 对象，比 AtomicLong 性能更好（减少乐观 锁的重试次数）。 以上内容来自阿里《Java开发手册》。​ 里面提到了多线程读写环境下，LongAdder相对于Atomic原子类拥有更好的效率。​ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283public class LongAdderAndAtomicTest &#123; private static AtomicInteger a = new AtomicInteger(0); private static LongAdder b = new LongAdder(); public static void main(String[] args) throws Exception &#123; test(1, 10000000); test(10, 10000000); test(20, 10000000); test(50, 10000000); test(100, 10000000); &#125; /** * 测试LongAdder和Atomic的效率 * * @param threadNum 线程数 * @param times 执行时间 */ public static void test(Integer threadNum, Integer times) throws Exception &#123; System.out.println(&quot;线程数为：&quot; + threadNum); testAtomic(threadNum, times); testLongAdder(threadNum, times); &#125; /** * 测试Atomic的效率 * * @param threadNum * @param times */ public static void testAtomic(Integer threadNum, Integer times) throws InterruptedException &#123; //开始时间 long start = System.currentTimeMillis(); CountDownLatch countDownLatch = new CountDownLatch(threadNum); for (int i = 0; i &lt; threadNum; i++) &#123; new Thread(() -&gt; &#123; for (int j = 0; j &lt; times; j++) &#123; a.incrementAndGet(); &#125; countDownLatch.countDown(); &#125;).start(); &#125; countDownLatch.await(); //结束时间 long end = System.currentTimeMillis(); System.out.println(&quot;Atomic 消耗时间：&quot; + (end - start)); &#125; /** * 测试LongAdder的效率 * * @param threadNum * @param times */ public static void testLongAdder(Integer threadNum, Integer times) throws InterruptedException &#123; //开始时间 long start = System.currentTimeMillis(); CountDownLatch countDownLatch = new CountDownLatch(threadNum); for (int i = 0; i &lt; threadNum; i++) &#123; new Thread(() -&gt; &#123; for (int j = 0; j &lt; times; j++) &#123; b.increment(); &#125; countDownLatch.countDown(); &#125;).start(); &#125; countDownLatch.await(); //结束时间 long end = System.currentTimeMillis(); System.out.println(&quot;LongAdder 消耗时间：&quot; + (end - start)); &#125;&#125; 由图可以看到，线程数越多，LongAdder的效率型对于Atomic越高，由此可以看出，LongAdder更适合于高并发情况下。​ 二，LongAdder源码阅读看LongAdder类的继承关系： 1public class LongAdder extends Striped64 implements Serializable LongAdder这个类继承自Striped64，Striped64里面声明了一个内部类Cell。 1234567891011121314151617181920212223242526@sun.misc.Contended static final class Cell &#123; //拥有内存可见性的value volatile long value; //带参数的构造器 Cell(long x) &#123; value = x; &#125; //调用unsafe类的cas对cmp和bal进行比较交换，返回是否成功 final boolean cas(long cmp, long val) &#123; return UNSAFE.compareAndSwapLong(this, valueOffset, cmp, val); &#125; //声明unsafe类 private static final sun.misc.Unsafe UNSAFE; //声明cell成员属性的内存偏移量 private static final long valueOffset; //初始化 static &#123; try &#123; UNSAFE = sun.misc.Unsafe.getUnsafe(); Class&lt;?&gt; ak = Cell.class; valueOffset = UNSAFE.objectFieldOffset (ak.getDeclaredField(&quot;value&quot;)); &#125; catch (Exception e) &#123; throw new Error(e); &#125; &#125;&#125; 1234567891011//获取当前系统的cpu数 控制cells数组长度的一个关键条件static final int NCPU = Runtime.getRuntime().availableProcessors();//数组的长度，只要数组不为空，一定是2的倍数，这样-1转化为二进制的时候一定是一大堆1transient volatile Cell[] cells;//没有发生过竞争时，数据会累加到 base上 | 当cells扩容时，需要将数据写到base中transient volatile long base;//初始化cells或者扩容cells都需要获取锁，0 表示无锁状态，1 表示其他线程已经持有锁了transient volatile int cellsBusy; LongAdder里面使用的这两个属性实际上是继承自他的父类的。​ 12345678910111213141516171819202122232425262728293031323334353637383940public void add(long x) &#123; /** * as: 表示cells数组的引用 * b： 表示获取的base值 * v: 表示期望值 * m: 表示cells数组的长度 * a: 表示当前线程命中的cell单元格 */ Cell[] as; long b, v; int m; Cell a; /** * 条件一：true-&gt;表示cells已经初始化过，当前线程应该将数据写入到对应的cell中 * false-&gt;表示cells未初始化，当前所有线程应该将数据写入到base中 * 条件二：false-&gt;表示当前线程cas替换数据成功 * true-&gt;表示发生竞争了，可能需要重试或者扩容 * 进入if的条件：数组已经初始化 或者 cas交换数据失败，表示有竞争 * */ if ((as = cells) != null || !casBase(b = base, b + x)) &#123; /** * uncontended: true -&gt; 未竞争 false-&gt;发生竞争 * 条件一：true-&gt;数组没有初始化 * false-&gt;数组已经初始化 * 条件二：true-&gt;数组没有初始化 * false-&gt;数组已经初始化 * * getProbe()：获取当前线程的hash值 * 条件三：true-&gt; 当前线程对应的cell并没有初始化 * false-&gt;当前线程对应的cell已经初始化 * 条件四：true-&gt;cas交换失败，表示有竞争 * false-&gt;cas交换成功 * 进入if的条件： * cells未初始化， 或者 当前线程对应的cell未初始化， 或者 cas交换失败 */ boolean uncontended = true; if (as == null || (m = as.length - 1) &lt; 0 || (a = as[getProbe() &amp; m]) == null || !(uncontended = a.cas(v = a.value, v + x))) longAccumulate(x, null, uncontended); &#125;&#125; 接下来看longAccumulate（）。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178/** * 首先：哪些情况会进入当前方法？ * cells未初始化， 或者 当前线程对应的cell未初始化， 或者 cas交换失败 * @param x 新值 * @param fn 没用上。一个扩展接口 * @param wasUncontended 只有cells初始化之后，并且当前线程 竞争修改失败，才会是false */final void longAccumulate(long x, LongBinaryOperator fn, boolean wasUncontended) &#123; //h：代表当前线程hash值 int h; /** * 如果当前线程hash值等于0，条件成立 * 给当前线程分配hash值 * 将当前线程的hash值重新赋值给h * 设置为未竞争或者竞争修改成功状态。 * 为什么？ * 因为默认所有线程进来操做的都是cells[0]的位置，所以不把它当作一次真正的竞争。 */ if ((h = getProbe()) == 0) &#123; //给当前线程分配hash值 ThreadLocalRandom.current(); // force initialization //将当前线程的hash值重新赋值给h h = getProbe(); wasUncontended = true; &#125; //表示扩容意向 false 一定不会扩容，true 可能会扩容。 boolean collide = false; //自旋 for (;;) &#123; /** * as 代表cells的引用 * a 当前线程对应的cell * n cells的长度 * v 期望值 */ Cell[] as; Cell a; int n; long v; /** * case1： * cells已经初始化 * * case1.1： * if(当前线程对应的cell还没有初始化 &amp;&amp; 当前处于无锁状态)&#123; * 创建一个新的cell对象 r * * if（当前锁状态未0并且获取到了锁）&#123; * created : 标记是否创建成功 * if（cells已经被初始化 &amp;&amp; 当前线程对应的cell为空）&#123; * 将当前线程对应位置的cell初始化为新创建的cell r * create=true 表示创建成功，最终在释放锁。 * &#125; * &#125; * 将扩容意向改成false * &#125; * * case1.2: * if(如果当前线程竞争修改失败)&#123; * 状态改为true； * //默认所有线程一开始都在cell[0]的位置，所以一定会发生竞争， * //这次竞争就不当作一次真正的竞争。 * &#125; * * case1.3： * if(当前线程rehash过hash值 &amp;&amp; 新命中的cell不为空)&#123; * 尝试cas一次 * &#125; * * case1.4： * if(如果cells的长度&gt;cpu数 || cells和as不一致)&#123; * //cells和as不一致 说明其他线程已经扩容过了，当前线程只需要rehash重试即可 * 扩容意向强制改为false。 * &#125; * * case1.5： * //!collide = true 设置扩容意向 为true 但是不一定真的发生扩容 * * case1.6： * if(锁状态为0 &amp;&amp; 获取到了锁)&#123; * //第二层判断为了防止当前线程在对第一层id的条件判断一半的时候，又进来一个线程，将所有业务已经执行一遍了。 * //只有当cells==as才能说明，当前线程在第一层if执行条件的过程中，没有其他线程进来破坏。 * if(cells==as)&#123; * 扩容为原来的二倍 * 重置当前线程Hash值 * &#125; * &#125; */ if ((as = cells) != null &amp;&amp; (n = as.length) &gt; 0) &#123; if ((a = as[(n - 1) &amp; h]) == null) &#123; if (cellsBusy == 0) &#123; // Try to attach new Cell Cell r = new Cell(x); // Optimistically create if (cellsBusy == 0 &amp;&amp; casCellsBusy()) &#123; //标记是否创建成功 boolean created = false; try &#123; /** * rs：cells的引用 * m：cells的长度 * j：当前线程对应的cells下标 */ Cell[] rs; int m, j; if ((rs = cells) != null &amp;&amp; (m = rs.length) &gt; 0 &amp;&amp; rs[j = (m - 1) &amp; h] == null) &#123; rs[j] = r; created = true; &#125; &#125; finally &#123; cellsBusy = 0; &#125; if (created) break; continue; // Slot is now non-empty &#125; &#125; collide = false; &#125; else if (!wasUncontended) // CAS already known to fail wasUncontended = true; // Continue after rehash else if (a.cas(v = a.value, ((fn == null) ? v + x : fn.applyAsLong(v, x)))) break; else if (n &gt;= NCPU || cells != as) collide = false; // At max size or stale else if (!collide) collide = true; else if (cellsBusy == 0 &amp;&amp; casCellsBusy()) &#123; try &#123; if (cells == as) &#123; // Expand table unless stale Cell[] rs = new Cell[n &lt;&lt; 1]; for (int i = 0; i &lt; n; ++i) rs[i] = as[i]; cells = rs; &#125; &#125; finally &#123; cellsBusy = 0; &#125; collide = false; continue; // Retry with expanded table &#125; //重置当前线程Hash值 h = advanceProbe(h); &#125; /** * case2： * cells并未初始化 * 锁状态为0 * cells==as ？ 因为其它线程可能会在你给as赋值之后修改了 cells * 获取锁成功 * 里面再次判断cells==as是因为防止其他线程在判断第一层if的中间被其他线程先进来修改了一次 * 初始化cells */ else if (cellsBusy == 0 &amp;&amp; cells == as &amp;&amp; casCellsBusy()) &#123; boolean init = false; try &#123; // Initialize table if (cells == as) &#123; Cell[] rs = new Cell[2]; rs[h &amp; 1] = new Cell(x); cells = rs; init = true; &#125; &#125; finally &#123; cellsBusy = 0; &#125; if (init) break; &#125; /** * case3： * cellsBusy处于加锁状态，表示其他线程正在初始化cells， * 那么当前线程就应该将数据累加到base */ else if (casBase(v = base, ((fn == null) ? v + x : fn.applyAsLong(v, x)))) break; // Fall back on using base &#125;&#125; getProbe()获取当前线程hash值。 123static final int getProbe() &#123; return UNSAFE.getInt(Thread.currentThread(), PROBE);&#125; casCellsBusy()cas的方式获取锁。 123final boolean casCellsBusy() &#123; return UNSAFE.compareAndSwapInt(this, CELLSBUSY, 0, 1);&#125; advanceProbe(h)重置当前线程hash值。 1234567static final int advanceProbe(int probe) &#123; probe ^= probe &lt;&lt; 13; // xorshift probe ^= probe &gt;&gt;&gt; 17; probe ^= probe &lt;&lt; 5; UNSAFE.putInt(Thread.currentThread(), PROBE, probe); return probe;&#125; casBase()cas的方式改变base值。 123final boolean casBase(long cmp, long val) &#123; return UNSAFE.compareAndSwapLong(this, BASE, cmp, val);&#125; 三，LongAdder执行流程​ LongAdder的执行流程实际上就是：​ 当没有线程竞争的时候，线程会直接操做base里面的值。 当有线程竞争的时候，会将base的值拷贝成一个cells数组，每个线程都来操作一个cell数组中的桶位，最终将cells各个桶位和base求和，就可以得到LongAdder的最终值。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192public void add(long x) &#123; if(数组未初始化||cas的方式把值写入base失败)&#123; if(数组未初始化||当前线程对应的单元格未初始化||或cas的方式在当前线程对应的单元格交换数据失败，说明有竞争)&#123; longAccumulate(x, null, uncontended); &#125; &#125;&#125;final void longAccumulate(long x, LongBinaryOperator fn,boolean wasUncontended) &#123; if(当前线程hash值==0)&#123; 重置当前线程hash值 是否发生竞争改为true &#125; 声明扩容意向=false for(;;)&#123; if(数组已经初始化过)&#123; if(当前线程命中的单元格未初始化)&#123; if(如果当前数组不是正在扩容)&#123; 创建一个新的Cell(x) if(当前并未有其他线程对数组进行扩容&amp;&amp;且当前线程竞争扩容数组的权利成功)&#123; 声明 创建完成 =false； if(数组不为空&amp;&amp;当前没有线程正在操作数组&amp;&amp;且当前线程命中的单元格为空)&#123; 将当前线程对应的单元格初始化为刚才创建的cell 创建完成 =true； 释放 数组扩容的权利 &#125; if(数组已经创建完成)&#123; break &#125; continue; &#125; &#125; 扩容意向 =false &#125; else if(如果当前线程竞争修改失败)&#123; 状态设置为true //默认所有线程1开始都在cell[0]的位置，所以一定会发生竞争，这次竞争就不当做一次真正的竞争 &#125; else if(cas的方式修改当前线程命中的单元格成功)&#123; break &#125; else if(数组长度大于cpu数||cells长度 和当前数组长度不一致)&#123; 扩容意向 =false //cells长度 和当前数组长度不一致：说明已经有其他线程扩容了，当前线程只要rehash重试即可。 &#125; else if(扩容意向==false)&#123; 扩容意向 = true //但是不一定真的发生扩容 &#125; else if(当前没有线程正在在扩容数组 &amp;&amp; 并且当前线程竞争到了扩容数组的资格)&#123; if(在这个过程中没有线程把数组扩容了,也就是啥也没发生)&#123; 数组扩容一倍，将原数组的值进行一个拷贝，将新数组的地址指向原数组 &#125; 释放数组扩容的权利 扩容意向改成false &#125; 把当前线程的hash值rehash &#125; //当前数组没有被初始化 else if(当前没有线程正在初始化数组，也没有线程已经初始化完了，并且当前线程竞争到了初始化的权利)&#123; 声明 初始化 == false if(当前数组还未初始化，说明没有线程已经初始化完了)&#123; 创建一个长度为2 的数组 并把当前线程命中的单元格初始化 初始化 == true &#125; 释放数组扩容权利 if(初始化==true)&#123; break &#125; &#125; //如果有线程正在初始化数组，那么当前线程就应该将数据累加到base else if(cas的方式修改base值成功)&#123; break &#125; &#125;&#125; 1.为什么比cas的效率高？​ cas是多线程竞争，只有拿到锁的线程才能去操做资源，其他线程不断的自旋重试，相当于线程排队。 LongAdder是多线程竞争的时候，他会将共享资源拷贝多份，采用分支合并的思想，提升效率。","categories":[{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"Atomic原子类","slug":"JUC/Atomic原子类","date":"2022-01-11T11:06:04.400Z","updated":"2022-01-11T11:24:37.487Z","comments":true,"path":"2022/01/11/JUC/Atomic原子类/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/JUC/Atomic%E5%8E%9F%E5%AD%90%E7%B1%BB/","excerpt":"","text":"所谓的原子性表示一个或者多个操作，要么全部执行完，要么一个也不执行。不能出现成功一部分失败一部分的情况。​ 在多线程中，如果多个线程同时更新一个共享变量，可能会得到一个意料之外的值。比如 i=1 。A 线程更新 i+1 、B 线程也更新 i+1。​ 通过两个线程并行操作之后可能 i 的值不等于 3。而可能等于 2。因为 A 和 B 在更新变量 i 的时候拿到的 i 可能都是 1这就是一个典型的原子性问题。​ 从 JDK1.5 开始，在 J.U.C 包中提供了 Atomic 包，提供了对于常用数据结构的原子操作。它提供了简单、高效、以及线程安全的更新一个变量的方式。​ 1.juc中的原子操作类由于变量类型的关系，在 J.U.C 中提供了 12 个原子操作的类。这 12 个类可以分为四大类：​ 原子更新基本类型：AtomicBoolean、AtomicInteger、AtomicLong ​ 原子更新数组：AtomicIntegerArray 、 AtomicLongArray 、AtomicReferenceArray ​ 原子更新引用：AtomicReference 、 AtomicReferenceFieldUpdater 、AtomicMarkableReference（更新带有标记位的引用类型） ​ 原子更新字段：AtomicIntegerFieldUpdater、AtomicLongFieldUpdater、AtomicStampedReference ​ 2.AtomicInterger源码分析2.1属性&amp;构造器12345678910111213141516171819202122232425private static final long serialVersionUID = 6214790243416807050L;//获取到Unsafe类private static final Unsafe unsafe = Unsafe.getUnsafe();//value的内存偏移量private static final long valueOffset;//初始化的时候获取value的偏移量static &#123; try &#123; valueOffset = unsafe.objectFieldOffset (AtomicInteger.class.getDeclaredField(&quot;value&quot;)); &#125; catch (Exception ex) &#123; throw new Error(ex); &#125;&#125;//具体存放的value值，通过volatile保证内存可见性private volatile int value;//创建一个带有初始值的原子类对象public AtomicInteger(int initialValue) &#123; value = initialValue;&#125;//创建一个默认0值的原子类对象public AtomicInteger() &#123;&#125; 2.2 get()&amp;set()&amp;lazySet()1234567891011121314151617181920212223//获取当前最新值//get 方法只需要直接返回 value 的值就行，这里的 value 是通过 Volatile 修饰的，用来保证可见性。 public final int get() &#123; return value; &#125; //将当前值设置为指定的值 public final void set(int newValue) &#123; value = newValue; &#125; /** * lazyset调用了unsafe的putOrderedInt方法 * 对比一下set和lazySet： * 对比汇编指令差异，可以看到set操作加了lock锁，lazySet没有。 * lock锁的含义是什么呢？ * lazySet提供一个store store屏障（在当代系统中是很低成本的操作，或者说没有操作）， * 但是没有store load屏障（这通常是volatile写入动作最昂贵的部分）。 * @param newValue */ public final void lazySet(int newValue) &#123; unsafe.putOrderedInt(this, valueOffset, newValue); &#125; 2.3 getAndIncrement()​ getAndIncrement 实际上是调用 unsafe 这个类里面提供的方法，这个类相当于是一个后门，使得 Java 可以像 C 语言的指针一样直接操作内存空间。当然也会带来一些弊端，就是指针的问题。​ 实际上这个类在很多方面都有使用，除了 J.U.C 这个包以外，还有 Netty、kafka 等等，这个类提供了很多功能，包括多线程同步(monitorEnter)、CAS 操 作 (compareAndSwap) 、 线 程 的 挂 起 和 恢 复(park/unpark)、内存屏(loadFence/storeFence)内存管理（内存分配、释放内存、获取内存地址等.）​ 123public final int getAndIncrement() &#123; return unsafe.getAndAddInt(this, valueOffset, 1);&#125; 2.4 getAndAddInt()通过 do/while 循环，基于 CAS 乐观锁来做原子递增。实际上前面的 valueOffset 的作用就是从主内存中获得当前value 的值和预期值做一个比较，如果相等，对 value 做递增并结束循环。 123public final int getAndIncrement() &#123; return unsafe.getAndAddInt(this, valueOffset, 1);&#125; 2.5 compareAndSet()它 提 供 了 compareAndSet ， 允 许 客 户 端 基 于AtomicInteger 来实现乐观锁的操作。 123public final boolean compareAndSet(int expect, int update) &#123; return unsafe.compareAndSwapInt(this, valueOffset, expect, update);&#125; 3.AtomicStampReference​ \u0000解决cas的ABA问题锁提供的一个类，解决方法是对数据加版本号。​ 3.1 属性&amp;构造器123456//用volatile来修饰Pair对象保证其内存可见性private volatile Pair&lt;V&gt; pair;//通过传入的初始值和版本号构建pair对象public AtomicStampedReference(V initialRef, int initialStamp) &#123; pair = Pair.of(initialRef, initialStamp);&#125; 3.2 内部类123456789101112private static class Pair&lt;T&gt; &#123; final T reference; //引用数据 final int stamp; //版本号 private Pair(T reference, int stamp) &#123; this.reference = reference; this.stamp = stamp; &#125; //通过泛型方法构造对象 static &lt;T&gt; Pair&lt;T&gt; of(T reference, int stamp) &#123; return new Pair&lt;T&gt;(reference, stamp); &#125;&#125; 3.3 compareAndSet()比较并交换 123456789101112131415161718192021//参数：期望值，新值，期望版本，新版本public boolean compareAndSet(V expectedReference, V newReference, int expectedStamp, int newStamp) &#123; Pair&lt;V&gt; current = pair; return //期望值和当前值相等 expectedReference == current.reference &amp;&amp; //期望版本和新版本相等 expectedStamp == current.stamp &amp;&amp; ( ( //表示版本号对应上的同时，值也对应上，就没有必要创建新的pair对象了 newReference == current.reference &amp;&amp; newStamp == current.stamp ) || //创建新的pair对象 casPair(current, Pair.of(newReference, newStamp)) );&#125; 3.4 casPair()实际上调用的还是unsafe类里面的compareAndSwapObject（）。 12345private boolean casPair(Pair&lt;V&gt; cmp, Pair&lt;V&gt; val) &#123; return UNSAFE.compareAndSwapObject(this, pairOffset, cmp, val);&#125;","categories":[{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"CAS原理","slug":"JUC/CAS原理","date":"2022-01-11T11:05:54.708Z","updated":"2022-01-11T11:24:20.232Z","comments":true,"path":"2022/01/11/JUC/CAS原理/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/JUC/CAS%E5%8E%9F%E7%90%86/","excerpt":"","text":"1.保证i++在多线程下的安全并发对一个数进行++操作会导致结果不准确，因为++操作本身并不是原子性的，所以在多线程下会出现问题。 1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * @author 二十 * @since 2021/8/26 8:53 上午 */public class Demo &#123; private static volatile int count = 0; private static int threadSize = 100; private static CountDownLatch countDownLatch = new CountDownLatch(threadSize); private static void request() &#123; try &#123; TimeUnit.MICROSECONDS.sleep(5); /** * 出现问题的原因：count++分为三步操作， * count-&gt;栈 A * B=A+1 * count=B */ count++; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; for (int i = 0; i &lt; 100; i++) &#123; new Thread(() -&gt; &#123; for (int j = 0; j &lt; 10; j++) &#123; request(); &#125; countDownLatch.countDown(); &#125;).start(); &#125; countDownLatch.await(); System.out.println(&quot;count:&quot; + count); &#125;&#125; 通过加锁的方式可以保证数据的准确性，但是如果直接在request()加锁，效率会很低，因为这样锁的粒度比较大。​ 分析一下count++操作：​ count的值赋值给操作数栈内的a 将操作数栈内的a+1并赋值给b 将操作数栈内b的值赋值给count 并发条件下出现错误的原因就是因为假如a线程在操作一半的过程中（1-3之间），线程b来获取count的值进行++操作，就会获取到不准确的count值。可以控制在第三步加锁，在线程将自己操作数栈内的结果赋值给count之前，先比较当前的最新count和当前线程操作数栈内拷贝的count副本值是否一致，如果一致，则当前线程可以将自己操作数栈的结果赋值给count，否则重新获取count值进行操作，直到成功。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/** * @author 二十 * @since 2021/8/26 8:53 上午 */public class Demo &#123; private static volatile int count = 0; private static int threadSize = 100; private static CountDownLatch countDownLatch = new CountDownLatch(threadSize); private static void request() &#123; try &#123; TimeUnit.MICROSECONDS.sleep(5); /** * 出现问题的原因：count++分为三步操作， * count-&gt;栈 A * B=A+1 * count=B */// count++; int e; while (!compareAndSwap(e = getCount(), e + 1)) &#123; &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; private static int getCount() &#123; return count; &#125; private static synchronized boolean compareAndSwap(int e, int n) &#123; if (getCount() == e) &#123; count = n; return true; &#125; return false; &#125; public static void main(String[] args) throws InterruptedException &#123; for (int i = 0; i &lt; 100; i++) &#123; new Thread(() -&gt; &#123; for (int j = 0; j &lt; 10; j++) &#123; request(); &#125; countDownLatch.countDown(); &#125;).start(); &#125; countDownLatch.await(); System.out.println(&quot;count:&quot; + count); &#125;&#125; 2.Java对cas的支持2.1 cas概念CAS 全称“CompareAndSwap”，中文翻译过来为“比较并替换”​ CAS操作包含三个操作数————内存位置（V）、期望值（A）和新值（B）。 如果内存位置的值与期望值匹配，那么处理器会自动将该位置值更新为新值。 否则，处理器不作任何操作。 无论哪种情况，它都会在CAS指令之前返回该位置的值。（CAS在一些特殊情况下仅返回CAS是否成功，而不提取当前值） CAS有效的说明了“我认为位置V应该包含值A；如果包含该值，则将B放到这个位置；否则，不要更改。 ​ 2.2 jdk中对cas提供的支持java中提供了对CAS操作的支持，具体在sun.misc.unsafe类中，声明如下：​ 123public final native boolean compareAndSwapObject(Object var1, long var2, Object var4, Object var5);public final native boolean compareAndSwapInt(Object var1, long var2, int var4, int var5);public final native boolean compareAndSwapLong(Object var1, long var2, long var4, long var6); 参数var1 表示要操作的对象 参数var2 表示要操作对象中属性地址的偏移量 参数var4 表示需要修改数据的期望的值 参数var5 表示需要修改为的新值 2.3 cas实现原理CAS通过调用JNI的代码实现，JNI：java Native Interface，允许java调用其它语言。而compareAndSwapxxx系列的方法就是借助“C语言”来调用cpu底层指令实现的。以常用的Intel x86平台来说，最终映射到的cpu的指令为“cmpxchg”，这是一个原子指令，cpu执行此命令时，实现比较并替换的操作！​ 2.4 cmpxchg怎么保证多核心下的线程安全？系统底层进行CAS操作的时候，会判断当前系统是否为多核心系统，如果是就给“总线”加锁，只有一个线程会对总线加锁成功，加锁成功之后会执行CAS操作，也就是说CAS的原子性是平台级别的！​ 2.5 cas存在的问题1)ABA问题​ 线程1准备用CAS将变量的值由A替换为B，在此之前，线程2将变量的值由A替换为C，又由C替换为A，然后线程1执行CAS时发现变量的值仍然为A，所以CAS成功。但实际上这时的现场已经和最初不同了，尽管CAS成功，但可能存在潜藏的问题。举例：一个小偷，把别人家的钱偷了之后又还了回来，还是原来的钱吗，你老婆出轨之后又回来，还是原来的老婆吗？ABA问题也一样，如果不好好解决就会带来大量的问题。最常见的就是资金问题，也就是别人如果挪用了你的钱，在你发现之前又还了回来。但是别人却已经触犯了法律。​ 2)循环时间长开销大​ 3)只能保证一个共享变量的原子操作2.6 ABA问题演示123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * @author 二十 * @since 2021/8/26 2:15 下午 */public class Aba &#123; static CountDownLatch countDownLatch = new CountDownLatch(2); static AtomicInteger i = new AtomicInteger(1); public static void main(String[] args) &#123; new Thread(() -&gt; &#123; int e = Aba.i.get(); int n = e + 1; try &#123; TimeUnit.SECONDS.sleep(2); &#125; catch (InterruptedException ex) &#123; ex.printStackTrace(); &#125; boolean flag = i.compareAndSet(e, n); System.out.println(&quot;flag = &quot; + flag); countDownLatch.countDown(); &#125;).start(); try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; new Thread(() -&gt; &#123; i.incrementAndGet(); i.decrementAndGet(); countDownLatch.countDown(); &#125;).start(); try &#123; countDownLatch.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 2.7 jdk如何解决ABA问题解决ABA问题其实很简单，只需要加一个版本号，每次操作，版本号都会加1，每次比较交换的时候，把版本号也比较一下，这样就能确保，这个数据是正确的。​ AtomicStampReference主要包含一个对象引用及一个可以自动更新的整数“stamp”的pair对象来解决ABA问题。​ 123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * @author 二十 * @since 2021/8/26 2:29 下午 */public class AtomicStampReferenceTest &#123; static CountDownLatch countDownLatch = new CountDownLatch(2); static AtomicStampedReference&lt;Integer&gt; i = new AtomicStampedReference(1, 1); public static void main(String[] args) &#123; new Thread(() -&gt; &#123; int e = Aba.i.get(); int n = e + 1; int ev = i.getStamp(); try &#123; TimeUnit.SECONDS.sleep(2); &#125; catch (InterruptedException ex) &#123; ex.printStackTrace(); &#125; boolean flag = i.compareAndSet(e, n, ev, ev + 1); System.out.println(&quot;flag = &quot; + flag); countDownLatch.countDown(); &#125;).start(); try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; new Thread(() -&gt; &#123; i.set(i.getReference() + 1, i.getStamp() + 1); i.set(i.getReference() - 1, i.getStamp() + 1); countDownLatch.countDown(); &#125;).start(); try &#123; countDownLatch.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; ​","categories":[{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"Volatile","slug":"JUC/Volatile","date":"2022-01-11T11:05:46.558Z","updated":"2022-01-11T11:30:33.567Z","comments":true,"path":"2022/01/11/JUC/Volatile/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/JUC/Volatile/","excerpt":"","text":"一，内存模型的相关概念计算机在执行程序时，每条指令都是在CPU中执行的，而执行指令过程中，势必涉及到数据的读取和写入。由于程序运行过程中的临时数据是存放在主存（物理内存）当中的，这时就存在一个问题，由于CPU执行速度很快，而从内存读取数据和向内存写入数据的过程跟CPU执行指令的速度比起来要慢的多，因此如果任何时候对数据的操作都要通过和内存的交互来进行，会大大降低指令执行的速度。因此在CPU里面就有了高速缓存。​ 也就是，当程序在运行过程中，会将运算需要的数据从主存复制一份到CPU的高速缓存当中，那么CPU进行计算时就可以直接从它的高速缓存读取数据和向其中写入数据，当运算结束之后，再将高速缓存中的数据刷新到主存当中。举个简单的例子，比如下面的这段代码：​ 1i=i+1 当线程执行这个语句时，会先从主存当中读取i的值，然后复制一份到高速缓存当中，然后CPU执行指令对i进行加1操作，然后将数据写入高速缓存，最后将高速缓存中i最新的值刷新到主存当中。​ 这个代码在单线程中运行是没有任何问题的，但是在多线程中运行就会有问题了。在多核CPU中，每条线程可能运行于不同的CPU中，因此每个线程运行时有自己的高速缓存（对单核CPU来说，其实也会出现这种问题，只不过是以线程调度的形式来分别执行的）。本文我们以多核CPU为例。​ 比如同时有2个线程执行这段代码，假如初始时i的值为0，那么我们希望两个线程执行完之后i的值变为2。但是事实会是这样吗？​ 可能存在下面一种情况：初始时，两个线程分别读取i的值存入各自所在的CPU的高速缓存当中，然后线程1进行加1操作，然后把i的最新值1写入到内存。此时线程2的高速缓存当中i的值还是0，进行加1操作之后，i的值为1，然后线程2把i的值写入内存。​ 最终结果i的值是1，而不是2。这就是著名的缓存一致性问题。通常称这种被多个线程访问的变量为共享变量。​ 也就是说，如果一个变量在多个CPU中都存在缓存（一般在多线程编程时才会出现），那么就可能存在缓存不一致的问题。​ 为了解决缓存不一致性问题，通常来说有以下2种解决方法：​ 通过总线加Lock锁的方式 通过缓存一致性协议 ​ 这两种方式都是在硬件层面上提供的方式。​ 在早期的CPU当中，是通过在总线上加LOCK锁的形式来解决缓存不一致的问题（相当于让CPU串行，效率低）。因为CPU和其他部件进行通信都是通过总线来进行的，如果对总线加LOCK锁的话，也就是说阻塞了其他CPU对其他部件访问（如内存），从而使得只能有一个CPU能使用这个变量的内存。比如上面例子中 如果一个线程在执行 i = i +1，如果在执行这段代码的过程中，在总线上发出了LCOK锁的信号，那么只有等待这段代码完全执行完毕之后，其他CPU才能从变量i所在的内存读取变量，然后进行相应的操作。这样就解决了缓存不一致的问题。​ 但是上面的方式会有一个问题，由于在锁住总线期间，其他CPU无法访问内存，导致效率低下。​ 所以就出现了缓存一致性协议。最出名的就是Intel 的MESI协议，MESI协议保证了每个缓存中使用的共享变量的副本是一致的（改动则通知的方式）。它核心的思想是：当CPU写数据时，如果发现操作的变量是共享变量，即在其他CPU中也存在该变量的副本，会发出信号通知其他CPU将该变量的缓存行置为无效状态，因此当其他CPU需要读取这个变量时，发现自己缓存中缓存该变量的缓存行是无效的，那么它就会从内存重新读取。 嗅探是实现缓存一致性的常见机制。​ 注意，缓存的一致性问题，不是多处理器导致，而是多缓存导致的。 ​ 嗅探机制工作原理：每个处理器通过监听在总线上传播的数据来检查自己的缓存值是不是过期了，如果处理器发现自己缓存行对应的内存地址修改，就会将当前处理器的缓存行设置无效状态，当处理器对这个数据进行修改操作的时候，会重新从主内存中把数据读到处理器缓存中。​ 注意：基于 CPU 缓存一致性协议，JVM 实现了 volatile 的可见性，但由于总线嗅探机制，会不断的监听总线，如果大量使用 volatile 会引起总线风暴。所以，volatile 的使用要适合具体场景。 ​ 二，并发编程中的三个概念在并发编程中，通常会关心以下三个问题：原子性问题，可见性问题，有序性问题。​ 1.原子性原子性：即一个操作或者多个操作 要么全部执行并且执行的过程不会被任何因素打断，要么就都不执行。​ 一个很经典的例子就是银行账户转账问题：​ 比如从账户A向账户B转1000元，那么必然包括2个操作：从账户A减去1000元，往账户B加上1000元。​ 如果这2个操作不具备原子性，会造成什么样的后果。假如从账户A减去1000元之后，操作突然中止。然后又从B取出了500元，取出500元之后，再执行 往账户B加上1000元 的操作。这样就会导致账户A虽然减去了1000元，但是账户B没有收到这个转过来的1000元。​ 所以这2个操作必须要具备原子性才能保证不出现一些意外的问题。​ 同样地反映到并发编程中会出现什么结果呢？​ 假如为一个32位的变量赋值过程不具备原子性的话，会发生什么后果？​ 1i=9； ​ 假若一个线程执行到这个语句时，暂且假设为一个32位的变量赋值包括两个过程：为低16位赋值，为高16位赋值。​ 那么就可能发生一种情况：当将低16位数值写入之后，突然被中断，而此时又有一个线程去读取i的值，那么读取到的就是错误的数据。​ 2.可见性可见性是指当多个线程访问同一个变量时，一个线程修改了这个变量的值，其他线程能够立即看得到修改的值。​ 123456//线程1执行的代码int i = 0;i = 10; //线程2执行的代码j = i; ​ 假若执行线程1的是CPU1，执行线程2的是CPU2。由上面的分析可知，当线程1执行 i =10这句时，会先把i的初始值加载到CPU1的高速缓存中，然后赋值为10，那么在CPU1的高速缓存当中i的值变为10了，却没有立即写入到主存当中。​ 此时线程2执行 j = i，它会先去主存读取i的值并加载到CPU2的缓存当中，注意此时内存当中i的值还是0，那么就会使得j的值为0，而不是10.​ 这就是可见性问题，线程1对变量i修改了之后，线程2没有立即看到线程1修改的值。​ 3.有序性有序性：即程序执行的顺序按照代码的先后顺序执行。​ 1234int i = 0; boolean flag = false;i = 1; //语句1 flag = true; //语句2 ​ 上面代码定义了一个int型变量，定义了一个boolean类型变量，然后分别对两个变量进行赋值操作。从代码顺序上看，语句1是在语句2前面的，那么JVM在真正执行这段代码的时候会保证语句1一定会在语句2前面执行吗？不一定，为什么呢？这里可能会发生指令重排序（Instruction Reorder）。​ 下面解释一下什么是指令重排序，一般来说，处理器为了提高程序运行效率，可能会对输入代码进行优化，它不保证程序中各个语句的执行先后顺序同代码中的顺序一致，但是它会保证程序最终执行结果和代码顺序执行的结果是一致的。​ 比如上面的代码中，语句1和语句2谁先执行对最终的程序结果并没有影响，那么就有可能在执行过程中，语句2先执行而语句1后执行。​ 但是要注意，虽然处理器会对指令进行重排序，但是它会保证程序最终结果会和代码顺序执行结果相同，那么它靠什么保证的呢？​ 1234int a = 10; //语句1int r = 2; //语句2a = a + 3; //语句3r = a*a; //语句4 这段代码有4个语句，那么可能的一个执行顺序是：​ 那么可不可能是这个执行顺序呢： 语句2 语句1 语句4 语句3​ 不可能，因为处理器在进行重排序时是会考虑指令之间的数据依赖性，如果一个指令Instruction 2必须用到Instruction 1的结果，那么处理器会保证Instruction 1会在Instruction 2之前执行。​ 虽然重排序不会影响单个线程内程序执行的结果，但是多线程呢？​ 123456789//线程1:context = loadContext(); //语句1inited = true; //语句2 //线程2:while(!inited )&#123; sleep()&#125;doSomethingwithconfig(context); 上面代码中，由于语句1和语句2没有数据依赖性，因此可能会被重排序。假如发生了重排序，在线程1执行过程中先执行语句2，而此是线程2会以为初始化工作已经完成，那么就会跳出while循环，去执行doSomethingwithconfig(context)方法，而此时context并没有被初始化，就会导致程序出错。​ 从上面可以看出，指令重排序不会影响单个线程的执行，但是会影响到线程并发执行的正确性。​ 也就是说，要想并发程序正确地执行，必须要保证原子性、可见性以及有序性。只要有一个没有被保证，就有可能会导致程序运行不正确。​ 三，Java内存模型在前面谈到了一些关于内存模型以及并发编程中可能会出现的一些问题。下面来看一下Java内存模型，研究一下Java内存模型提供了哪些保证以及在java中提供了哪些方法和机制来让我们在进行多线程编程时能够保证程序执行的正确性。​ 在Java虚拟机规范中试图定义一种Java内存模型（Java Memory Model，JMM）来屏蔽各个硬件平台和操作系统的内存访问差异，以实现让Java程序在各种平台下都能达到一致的内存访问效果。那么Java内存模型规定了哪些东西呢，它定义了程序中变量的访问规则，往大一点说是定义了程序执行的次序。注意，为了获得较好的执行性能，Java内存模型并没有限制执行引擎使用处理器的寄存器或者高速缓存来提升指令执行速度，也没有限制编译器对指令进行重排序。也就是说，在java内存模型中，也会存在缓存一致性问题和指令重排序的问题。​ JMM 定义了线程和主内存之间的抽象关系：线程之间的共享变量存储在主内存中，每个线程都有一个私有的本地内存，本地内存中存储了该线程以读/写共享变量的副本。​ JMM 的规定： 所有的共享变量都存储于主内存。这里所说的变量指的是实例变量和类变量，不包含局部变量，因为局部变量是线程私有的，因此不存在竞争问题。 每一个线程还存在自己的工作内存，线程的工作内存，保留了被线程使用的变量的工作副本。 线程对变量的所有的操作（读，取）都必须在工作内存中完成，而不能直接读写主内存中的变量。 不同线程之间也不能直接访问对方工作内存中的变量，线程间变量的值的传递需要通过主内存中转来完成。 ​ 1i = 10; ​ 执行线程必须先在自己的工作线程中对变量i所在的缓存行进行赋值操作，然后再写入主存当中。而不是直接将数值10写入主存当中。​ 那么Java语言 本身对 原子性、可见性以及有序性提供了哪些保证呢？​ 1.原子性在Java中，对基本数据类型的变量的读取和赋值操作是原子性操作，即这些操作是不可被中断的，要么执行，要么不执行。​ 1234x = 10; //语句1y = x; //语句2x++; //语句3x = x + 1; //语句4 ​ 只有语句1是原子性操作，其他三个语句都不是原子性操作。​ 语句1是直接将数值10赋值给x，也就是说线程执行这个语句的会直接将数值10写入到工作内存中。​ 语句2实际上包含2个操作，它先要去读取x的值，再将x的值写入工作内存，虽然读取x的值以及 将x的值写入工作内存 这2个操作都是原子性操作，但是合起来就不是原子性操作了。​ 同样的，x++和 x = x+1包括3个操作：读取x的值，进行加1操作，写入新的值。​ 所以上面4个语句只有语句1的操作具备原子性。​ 也就是说，只有简单的读取、赋值（而且必须是将数字赋值给某个变量，变量之间的相互赋值不是原子操作）才是原子操作。​ 不过这里有一点需要注意：在32位平台下，对64位数据的读取和赋值是需要通过两个操作来完成的，不能保证其原子性。但是好像在最新的JDK中，JVM已经保证对64位数据的读取和赋值也是原子性操作了。​ 从上面可以看出，Java内存模型只保证了基本读取和赋值是原子性操作，如果要实现更大范围操作的原子性，可以通过synchronized和Lock来实现。由于synchronized和Lock能够保证任一时刻只有一个线程执行该代码块，那么自然就不存在原子性问题了，从而保证了原子性。​ 2.可见性对于可见性，Java提供了volatile关键字来保证可见性。​ 当一个共享变量被volatile修饰时，它会保证修改的值会立即被更新到主存，当有其他线程需要读取时，它会去内存中读取新值。​ 而普通的共享变量不能保证可见性，因为普通共享变量被修改之后，什么时候被写入主存是不确定的，当其他线程去读取时，此时内存中可能还是原来的旧值，因此无法保证可见性。​ 另外，通过synchronized和Lock也能够保证可见性，synchronized和Lock能保证同一时刻只有一个线程获取锁然后执行同步代码，并且在释放锁之前会将对变量的修改刷新到主存当中。因此可以保证可见性。​ 3.有序性在Java内存模型中，允许编译器和处理器对指令进行重排序，但是重排序过程不会影响到单线程程序的执行，却会影响到多线程并发执行的正确性。 在Java里面，可以通过volatile关键字来保证一定的“有序性”（具体原理在下一节讲述）。另外可以通过synchronized和Lock来保证有序性，很显然，synchronized和Lock保证每个时刻是有一个线程执行同步代码，相当于是让线程顺序执行同步代码，自然就保证了有序性。 另外，Java内存模型具备一些先天的“有序性”，即不需要通过任何手段就能够得到保证的有序性，这个通常也称为 happens-before 原则。如果两个操作的执行次序无法从happens-before原则推导出来，那么它们就不能保证它们的有序性，虚拟机可以随意地对它们进行重排序。 下面就来具体介绍下happens-before原则（先行发生原则）： 程序次序规则：一个线程内，按照代码顺序，书写在前面的操作先行发生于书写在后面的操作 锁定规则：一个unLock操作先行发生于后面对同一个锁额lock操作 volatile变量规则：对一个变量的写操作先行发生于后面对这个变量的读操作 传递规则：如果操作A先行发生于操作B，而操作B又先行发生于操作C，则可以得出操作A先行发生于操作C 线程启动规则：Thread对象的start()方法先行发生于此线程的每个一个动作 线程中断规则：对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生 线程终结规则：线程中所有的操作都先行发生于线程的终止检测，我们可以通过Thread.join()方法结束、Thread.isAlive()的返回值手段检测到线程已经终止执行 对象终结规则：一个对象的初始化完成先行发生于他的finalize()方法的开始 这8条原则摘自《深入理解Java虚拟机》。 对于程序次序规则来说，我的理解就是一段程序代码的执行在单个线程中看起来是有序的。注意，虽然这条规则中提到“书写在前面的操作先行发生于书写在后面的操作”，这个应该是程序看起来执行的顺序是按照代码顺序执行的，因为虚拟机可能会对程序代码进行指令重排序。虽然进行重排序，但是最终执行的结果是与程序顺序执行的结果一致的，它只会对不存在数据依赖性的指令进行重排序。因此，在单个线程中，程序执行看起来是有序执行的，这一点要注意理解。事实上，这个规则是用来保证程序在单线程中执行结果的正确性，但无法保证程序在多线程中执行的正确性。​ 第二条规则也比较容易理解，也就是说无论在单线程中还是多线程中，同一个锁如果出于被锁定的状态，那么必须先对锁进行了释放操作，后面才能继续进行lock操作。​ 第三条规则是一条比较重要的规则，也是后文将要重点讲述的内容。直观地解释就是，如果一个线程先去写一个变量，然后一个线程去进行读取，那么写入操作肯定会先行发生于读操作。​ 第四条规则实际上就是体现happens-before原则具备传递性。​ 后4条规则都是显而易见的。​ 四，深入理解volatile关键字1.volatile关键的两层语义一旦一个共享变量（类的成员变量、类的静态成员变量）被volatile修饰之后，那么就具备了两层语义： 保证了不同线程对这个变量进行操作时的可见性，即一个线程修改了某个变量的值，这新值对其他线程来说是立即可见的。 禁止进行指令重排序。 看一段代码，假如线程1先执行，线程2后执行：​ 12345678//线程1boolean stop = false;while(!stop)&#123; doSomething();&#125; //线程2stop = true; ​ 这段代码是很典型的一段代码，很多人在中断线程时可能都会采用这种标记办法。但是事实上，这段代码会完全运行正确么？即一定会将线程中断么？不一定，也许在大多数时候，这个代码能够把线程中断，但是也有可能会导致无法中断线程（虽然这个可能性很小，但是只要一旦发生这种情况就会造成死循环了）。 下面解释一下这段代码为何有可能导致无法中断线程。在前面已经解释过，每个线程在运行过程中都有自己的工作内存，那么线程1在运行的时候，会将stop变量的值拷贝一份放在自己的工作内存当中。 那么当线程2更改了stop变量的值之后，但是还没来得及写入主存当中，线程2转去做其他事情了，那么线程1由于不知道线程2对stop变量的更改，因此还会一直循环下去。 但是用volatile修饰之后就变得不一样了： 使用volatile关键字会强制将修改的值立即写入主存； 使用volatile关键字的话，当线程2进行修改时，会导致线程1的工作内存中缓存变量stop的缓存行无效（反映到硬件层的话，就是CPU的L1或者L2缓存中对应的缓存行无效）； 由于线程1的工作内存中缓存变量stop的缓存行无效，所以线程1再次读取变量stop的值时会去主存读取。 那么在线程2修改stop值时（当然这里包括2个操作，修改线程2工作内存中的值，然后将修改后的值写入内存），会使得线程1的工作内存中缓存变量stop的缓存行无效，然后线程1读取时，发现自己的缓存行无效，它会等待缓存行对应的主存地址被更新之后，然后去对应的主存读取最新的值。 那么线程1读取到的就是最新的正确的值。​ 2.volatile不保证原子性从上面知道volatile关键字保证了操作的可见性，但是volatile能保证对变量的操作是原子性吗？​ 12345678910111213141516171819202122232425public class Test &#123; public volatile int inc = 0; public void increase() &#123; inc++; &#125; public static void main(String[] args) &#123; final Test test = new Test(); for (int i = 0; i &lt; 10; i++) &#123; new Thread() &#123; public void run() &#123; for (int j = 0; j &lt; 1000; j++) test.increase(); &#125; ; &#125;.start(); &#125; while (Thread.activeCount() &gt; 1) //保证前面的线程都执行完 Thread.yield(); System.out.println(test.inc); &#125;&#125; 这段程序的输出结果是多少？事实上运行它会发现每次运行结果都不一致，都是一个小于10000的数字。 可能有疑问，不对啊，上面是对变量inc进行自增操作，由于volatile保证了可见性，那么在每个线程中对inc自增完之后，在其他线程中都能看到修改后的值啊，所以有10个线程分别进行了1000次操作，那么最终inc的值应该是1000*10=10000。 这里面就有一个误区了，volatile关键字能保证可见性没有错，但是上面的程序错在没能保证原子性。可见性只能保证每次读取的是最新的值，但是volatile没办法保证对变量的操作的原子性。 在前面已经提到过，自增操作是不具备原子性的，它包括读取变量的原始值、进行加1操作、写入工作内存。那么就是说自增操作的三个子操作可能会分割开执行，就有可能导致下面这种情况出现： 假如某个时刻变量inc的值为10， 线程1对变量进行自增操作，线程1先读取了变量inc的原始值，然后线程1被阻塞了； 然后线程2对变量进行自增操作，线程2也去读取变量inc的原始值，由于线程1只是对变量inc进行读取操作，而没有对变量进行修改操作，所以不会导致线程2的工作内存中缓存变量inc的缓存行无效，所以线程2会直接去主存读取inc的值，发现inc的值时10，然后进行加1操作，并把11写入工作内存，最后写入主存。 然后线程1接着进行加1操作，由于已经读取了inc的值，注意此时在线程1的工作内存中inc的值仍然为10，所以线程1对inc进行加1操作后inc的值为11，然后将11写入工作内存，最后写入主存。 那么两个线程分别进行了一次自增操作后，inc只增加了1。 虽然一个变量在修改volatile变量时，会让缓存行无效，然后其他线程去读就会读到新的值。这个就是上面的happens-before规则中的volatile变量规则，但是要注意，线程1对变量进行读取操作之后，被阻塞了的话，并没有对inc值进行修改。然后虽然volatile能保证线程2对变量inc的值读取是从内存中读取的，但是线程1没有进行修改，所以线程2根本就不会看到修改的值。 根源就在这里，自增操作不是原子性操作，而且volatile也无法保证对变量的任何操作都是原子性的。 把上面的代码改改就可以。​ 采用synchronized：​ 1234567891011121314151617181920212223public class Test &#123; public int inc = 0; public synchronized void increase() &#123; inc++; &#125; public static void main(String[] args) &#123; final Test test = new Test(); for(int i=0;i&lt;10;i++)&#123; new Thread()&#123; public void run() &#123; for(int j=0;j&lt;1000;j++) test.increase(); &#125;; &#125;.start(); &#125; while(Thread.activeCount()&gt;1) //保证前面的线程都执行完 Thread.yield(); System.out.println(test.inc); &#125;&#125; 采用Lock：​ 1234567891011121314151617181920212223242526272829public class Test &#123; public int inc = 0; Lock lock = new ReentrantLock(); public void increase() &#123; lock.lock(); try &#123; inc++; &#125; finally&#123; lock.unlock(); &#125; &#125; public static void main(String[] args) &#123; final Test test = new Test(); for(int i=0;i&lt;10;i++)&#123; new Thread()&#123; public void run() &#123; for(int j=0;j&lt;1000;j++) test.increase(); &#125;; &#125;.start(); &#125; while(Thread.activeCount()&gt;1) //保证前面的线程都执行完 Thread.yield(); System.out.println(test.inc); &#125;&#125; 采用AtomicInteger：​ 1234567891011121314151617181920212223public class Test &#123; public AtomicInteger inc = new AtomicInteger(); public void increase() &#123; inc.getAndIncrement(); &#125; public static void main(String[] args) &#123; final Test test = new Test(); for(int i=0;i&lt;10;i++)&#123; new Thread()&#123; public void run() &#123; for(int j=0;j&lt;1000;j++) test.increase(); &#125;; &#125;.start(); &#125; while(Thread.activeCount()&gt;1) //保证前面的线程都执行完 Thread.yield(); System.out.println(test.inc); &#125;&#125; ​ 在java 1.5的java.util.concurrent.atomic包下提供了一些原子操作类，即对基本数据类型的 自增（加1操作），自减（减1操作）、以及加法操作（加一个数），减法操作（减一个数）进行了封装，保证这些操作是原子性操作。atomic是利用CAS来实现原子性操作的（Compare And Swap），CAS实际上是利用处理器提供的CMPXCHG指令实现的，而处理器执行CMPXCHG指令是一个原子性操作。​ 3.volatile保证有序性在前面提到volatile关键字能禁止指令重排序，所以volatile能在一定程度上保证有序性。 volatile关键字禁止指令重排序有两层意思： 当程序执行到volatile变量的读操作或者写操作时，在其前面的操作的更改肯定全部已经进行，且结果已经对后面的操作可见；在其后面的操作肯定还没有进行； 在进行指令优化时，不能将在对volatile变量访问的语句放在其后面执行，也不能把volatile变量后面的语句放到其前面执行。 12345678//x、y为非volatile变量//flag为volatile变量 x = 2; //语句1y = 0; //语句2flag = true; //语句3x = 4; //语句4y = -1; //语句5 由于flag变量为volatile变量，那么在进行指令重排序的过程的时候，不会将语句3放到语句1、语句2前面，也不会讲语句3放到语句4、语句5后面。但是要注意语句1和语句2的顺序、语句4和语句5的顺序是不作任何保证的。​ 并且volatile关键字能保证，执行到语句3时，语句1和语句2必定是执行完毕了的，且语句1和语句2的执行结果对语句3、语句4、语句5是可见的。​ 回到前面举的一个例子：​ 123456789//线程1:context = loadContext(); //语句1inited = true; //语句2 //线程2:while(!inited )&#123; sleep()&#125;doSomethingwithconfig(context); ​ 前面举这个例子的时候，提到有可能语句2会在语句1之前执行，那么久可能导致context还没被初始化，而线程2中就使用未初始化的context去进行操作，导致程序出错。​ 这里如果用volatile关键字对inited变量进行修饰，就不会出现这种问题了，因为当执行到语句2时，必定能保证context已经初始化完毕。​ 4.volatile的原理和实现机制volatile到底如何保证可见性和禁止指令重排序的？​ 下面这段话摘自《深入理解Java虚拟机》：​ 观察加入volatile关键字和没有加入volatile关键字时所生成的汇编代码发现，加入volatile关键字时，会多出一个lock前缀指令​ lock前缀指令实际上相当于一个内存屏障（也成内存栅栏），内存屏障会提供3个功能：​ 它确保指令重排序时不会把其后面的指令排到内存屏障之前的位置，也不会把前面的指令排到内存屏障的后面；即在执行到内存屏障这句指令时，在它前面的操作已经全部完成； 它会强制将对缓存的修改操作立即写入主存； 如果是写操作，它会导致其他CPU中对应的缓存行无效。 ​ 5.禁止指令重排序什么是重排序？​ 为了提高性能，在遵守 as-if-serial 语义（即不管怎么重排序，单线程下程序的执行结果不能被改变。编译器，runtime 和处理器都必须遵守。）的情况下，编译器和处理器常常会对指令做重排序。​ 一般重排序可以分为如下三种类型：​ 编译器优化重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。 指令级并行重排序。现代处理器采用了指令级并行技术来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。 内存系统重排序。由于处理器使用缓存和读 / 写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。 数据依赖性：如果两个操作访问同一个变量，且这两个操作中有一个为写操作，此时这两个操作之间就存在数据依赖性。这里所说的数据依赖性仅针对单个处理器中执行的指令序列和单个线程中执行的操作，不同处理器之间和不同线程之间的数据依赖性不被编译器和处理器考虑。 从 Java 源代码到最终执行的指令序列，会分别经历下面三种重排序： 为了更好地理解重排序，请看下面的部分示例代码： 123456789int a = 0;// 线程 Aa = 1; // 1flag = true; // 2// 线程 Bif (flag) &#123; // 3 int i = a; // 4&#125; ​ 单看上面的程序好像没有问题，最后 i 的值是 1。但是为了提高性能，编译器和处理器常常会在不改变数据依赖的情况下对指令做重排序。假设线程 A 在执行时被重排序成先执行代码 2，再执行代码 1；而线程 B 在线程 A 执行完代码 2 后，读取了 flag 变量。由于条件判断为真，线程 B 将读取变量 a。此时，变量 a 还根本没有被线程 A 写入，那么 i 最后的值是 0，导致执行结果不正确。那么如何程序执行结果正确呢？这里仍然可以使用 volatile 关键字。​ 这个例子中， 使用 volatile 不仅保证了变量的内存可见性，还禁止了指令的重排序，即保证了 volatile 修饰的变量编译后的顺序与程序的执行顺序一样。那么使用 volatile 修饰 flag 变量后，在线程 A 中，保证了代码 1 的执行顺序一定在代码 2 之前。​ 那么， volatile 是如何禁止指令重排序的呢？这里将引出一个概念：内存屏障指令​ 内存屏障指令​ 为了实现 volatile 内存语义（即内存可见性），JMM 会限制特定类型的编译器和处理器重排序。为此，JMM 针对编译器制定了 volatile 重排序规则表，如下所示：​ ​ 使用 volatile 修饰变量时，根据 volatile 重排序规则表，Java 编译器在生成字节码时，会在指令序列中插入内存屏障指令来禁止特定类型的处理器重排序。​ 内存屏障是一组处理器指令，它的作用是禁止指令重排序和解决内存可见性的问题。​ JMM 把内存屏障指令分为下列四类：​ ​ StoreLoad 屏障是一个全能型的屏障，它同时具有其他三个屏障的效果。所以执行该屏障开销会很大，因为它使处理器要把缓存中的数据全部刷新到内存中。 volatile 读 / 写时是如何插入内存屏障的，见下图：​ 从上图，可以知道 volatile 读 / 写插入内存屏障规则： 在每个 volatile 读操作的后面插入 LoadLoad 屏障和 LoadStore 屏障。 在每个 volatile 写操作的前后分别插入一个 StoreStore 屏障和一个 StoreLoad 屏障。 ​ 也就是说，编译器不会对 volatile 读与 volatile 读后面的任意内存操作重排序；编译器不会对 volatile 写与 volatile 写前面的任意内存操作重排序。​ 五，使用volatile的场景synchronized关键字是防止多个线程同时执行一段代码，那么就会很影响程序执行效率，而volatile关键字在某些情况下性能要优于synchronized，但是要注意volatile关键字是无法替代synchronized关键字的，因为volatile关键字无法保证操作的原子性。通常来说，使用volatile必须具备以下2个条件：​ 对变量的写操作不依赖于当前值 该变量没有包含在具有其他变量的不变式中 ​ 实际上，这些条件表明，可以被写入 volatile 变量的这些有效值独立于任何程序的状态，包括变量的当前状态。​ 事实上，我的理解就是上面的2个条件需要保证操作是原子性操作，才能保证使用volatile关键字的程序在并发时能够正确执行。​ 1.状态标记位​ 123456789volatile boolean flag = false; while(!flag)&#123; doSomething();&#125; public void setFlag() &#123; flag = true;&#125; 12345678910volatile boolean inited = false;//线程1:context = loadContext(); inited = true; //线程2:while(!inited )&#123;sleep()&#125;doSomethingwithconfig(context); 2.double check1234567891011121314151617class Singleton&#123; private volatile static Singleton instance = null; private Singleton() &#123; &#125; public static Singleton getInstance() &#123; if(instance==null) &#123; synchronized (Singleton.class) &#123; if(instance==null) instance = new Singleton(); &#125; &#125; return instance; &#125;&#125;","categories":[{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"Synchronized","slug":"JUC/Synchronized","date":"2022-01-11T11:05:39.534Z","updated":"2022-01-11T11:32:10.744Z","comments":true,"path":"2022/01/11/JUC/Synchronized/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/JUC/Synchronized/","excerpt":"","text":"一，线程安全问题线程的合理使用能够提升程序的处理性能，主要有两个方面，第一个是能够利用多核CPU以及超线程技术来实现线程的并行执行；第二个是线程的异步化执行相比于同步来说，异步执行能够很好的优化程序的处理性能提升并发吞吐量。 1.线程安全一个变量i，假如一个线程去访问这个变量进行修改，这个时候对于数据的修改和访问没有任何问题。但是如果更多的线程对于着同一个变量进行修改，就会存在一个数据安全性问题。一个对象是否是线程安全的，取决于他是否会被多个线程访问，以及程序中是如何去使用这个对象的。所以，如果多个线程访问同一个共享对象，在不需要额外的同步以及调用端代码不用做其他协调的情况下，这个共享对象的状态依然是正确的（正确性意味着这个对象的结果与我们预期规定的结果保持一致），那说明这个对象是线程安全的。 2.如何保证线程并行的数据安全能够有一种方法使得线程的并行变成串行。加锁什么是锁？他是处理并发的一种同步手段，如果达到前面的目的，那么这个锁一定需要实现互斥的特性。Java提供加锁的方法就是synchronized关键字。 二，synchronized实现原理多线程并发编程中synchronized一直是元老级的，很多人都会称呼为重量级锁。但是随着jdk6对synchronized进行各种优化之后，有些情况下他就并不是那么重量级了。java6中为了减少获得锁和释放锁带来的性能消耗而引入的偏向锁和轻量级锁。​ synchronized实现同步的基础：Java中每一个对象都可以作为锁。具体表现为三种情况： 普通同步方法，锁是当前实例对象 静态同步方法，锁是当前类的Class对象 同步方法块，锁是sync括号里面配置的对象 ​ 当一个线程试图访问同步代码块时，他首先必须得到锁，退出或者抛出异常时必须释放锁。那么锁到底存储在哪里呢？锁里面会存储什么信息？​ 从JVM规范中可以看到synchronized在jvm里面的实现原理，jvm基于进入和退出monitor对象来实现方法同步和代码块的同步，但是两者的实现细节不一样。代码块同步使用monitorenter和monitorexit指令实现的，而方法的同步是使用另外一种方式实现的，细节在JVM规范里并没有详细说明。但是方法的同步同样可以使用这两个指令来实现。​ monitorenter指令是在编译后插入到同步代码块的开始位置，而monitorexit是插入到方法结束处和异常处，JVM要保证每个monitorenter必须有对应的monitorexit与之配对。任何对象都有一个monitor与之关联，当且一个monitor被持有后，它将处于锁定状态。线程执行到monitorenter指令时，将会尝试获取对象所对应的monitor的所有权，即尝试获得对象的锁。​ 12345678910111213141516/** * @author 二十 * @since 2021/8/26 4:33 下午 */public class SyncTest &#123; &#123; synchronized (Object.class)&#123; try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 执行javap -v SyncTest 命令​ 三，锁的存储​ 分析：要实现多线程的互斥状态，那这把锁需要哪些因素？ 锁需要有一个东西来表示，比如获得锁是什么状态，无锁状态是什么状态？ 这个状态需要对多个线程共享 ​ 观察synchronized的整个语法发现，synchronized(lock)是基于lock这个对象的生命周期来控制锁粒度的，那是不是锁的存储和这个lock对象有关系呢？以对象在jvm内存中是如何存储的作为切入点，分析对象里面有什么特性能够实现锁。 1.对象在内存中的布局在 Hotspot 虚拟机中，对象在内存中的存储布局，可以分为三个区域:对象头(Header)、实例数据(Instance Data)、对齐填充(Padding)。 2.JVM源码实现当我们在java代码里面使用new创建一个对象实例的时候，jvm层面实际上会创建一个instanceOopDesc对象。​ Hotspot虚拟机采用OOP-Klass用来描述对象实例Java对象实例，OOP指的是普通对象指针，Klass用来描述对象实例的具体类型。Hotspot采用instanceOopDesc和arrayOopDesc对象用来描述数组类型。​ instanceOopDesc的定义在Hotspot源码中的instanceOop.hpp文件中，另外，arrayOopDesc的定义对应arrayOop.hpp。 1234567891011121314151617181920212223#ifndef SHARE_OOPS_INSTANCEOOP_HPP#define SHARE_OOPS_INSTANCEOOP_HPP#include &quot;oops/oop.hpp&quot;// An instanceOop is an instance of a Java Class// Evaluating &quot;new HashTable()&quot; will create an instanceOop.class instanceOopDesc : public oopDesc &#123; public: // aligned header size. static int header_size() &#123; return sizeof(instanceOopDesc)/HeapWordSize; &#125; // If compressed, the offset of the fields of the instance may not be aligned. static int base_offset_in_bytes() &#123; return (UseCompressedClassPointers) ? klass_gap_offset_in_bytes() : sizeof(instanceOopDesc); &#125;&#125;;#endif // SHARE_OOPS_INSTANCEOOP_HPP 从 instanceOopDesc 代码中可以看到 instanceOopDesc继承自 oopDesc，oopDesc 的定义在Hotspot 源码中的oop.hpp 文件中。在普通实例对象中，oopDesc 的定义包含两个成员，分别是 mark 和 metadata 。_mark表示对象标记、属于 markOop 类型，也就是接下来要分析的 Mark World，它记录了对象和锁有关的信息。​ _metadata 表示类元信息，类元信息存储的是对象指向它的类元数据(Klass)的首地址，其中 Klass 表示普通指针、_compressed_klass 表示压缩类指针。 3.MarkWord在Hotspot中，markOop 的定义在markOop.hpp 文件中。​ Mark word 记录了对象和锁有关的信息，当某个对象被synchronized 关键字当成同步锁时，那么围绕这个锁的一系列操作都和 Mark word 有关系。Mark Word 在 32 位虚拟机的长度是 32bit、在 64 位虚拟机的长度是 64bit。Mark Word里面存储的数据会随着锁标志位的变化而变化，Mark Word 可能变化为存储以下 5 种情况。​ 4.为什么任何对象都可以实现锁 首先，java中的每个对象都派生自Object类，而每个Java Object 在JVM 内部都有一个native 的C++对象oop/oopDesc 进行对应。 线程在获取锁的时候，实际上就是获得了一个监视器对象（monitor），可以认为他是一个同步对象，所有的java对象天生携带monitor。在hotspot源码的markWord.hpp文件中，可以看到下面这段代码： ​ 12345ObjectMonitor* monitor() const &#123; assert(has_monitor(), &quot;check&quot;); // Use xor instead of &amp;~ to provide one extra tag-bit check. return (ObjectMonitor*) (value() ^ monitor_value);&#125; 多线程访问同步代码块时相当于去争抢对象监视器修改对象中的锁标识，上面的代码中ObjectMonitor这个对象和线程争抢锁的逻辑有密切的关系。​ 5.不同锁状态下MarkWord中存储的内容 四，锁升级使用锁能够实现数据的安全性，但是会带来性能的下降。不使用锁能够基于线程并行提升程序性能，但是却不能保证线程安全性。这两者之间似乎是没有办法达到既能满足性能也能满足安全性的要求。​ hotspot 虚拟机的作者经过调查发现，大部分情况下，加锁的代码不仅仅不存在多线程竞争，而且总是由同一个线程多次获得。所以基于这样一个概率，是的 synchronized 在JDK1.6 之后做了一些优化，为了减少获得锁和释放锁带来的性能开销，引入了偏向锁、轻量级锁的概念。因此在 synchronized 中，锁存在四种状态分别是：无锁、偏向锁、轻量级锁、重量级锁； 锁的状态根据竞争激烈的程度从低到高不断升级。​ 1，偏向锁的基本原理​ 怎么理解偏向锁？ 当一个线程访问加了同步锁的代码块时，会在对象头中存储当前线程的 ID，后续这个线程进入和退出这段加了同步锁的代码块时，不需要再次加锁和释放锁。而是直接比较对象头里面是否存储了指向当前线程的偏向锁。如果相等表示偏向锁是偏向于当前线程的，就不需要再尝试获得锁了。​ 对象一旦生成了hash码，他就无法进入偏向锁状态。也就是说，只要一个对象已经计算过hash码，她就无法进入偏向锁状态。当一个对象当前正处于偏向锁状态，并且需要计算其hash码，他的偏向锁会被撤销，并且锁会膨胀为重量级锁。 偏向锁其实主要解决的是无竞争情况下，锁的性能问题。 1）偏向锁的获取和撤销逻辑​ 首先获取锁 对象的 Markword，判断是否处于可偏向状态。（biased_lock=1、且 ThreadId 为空） ​ 如果是可偏向状态，则通过 CAS 操作，把当前线程的 ID写入到 MarkWord，锁标志位改成01，偏向标志位改成1. ​ 如果 cas 成功，那么 markword 就会变成这样。表示已经获得了锁对象的偏向锁，接着执行同步代码块 如果 cas 失败，说明有其他线程已经获得了偏向锁，这种情况说明当前锁存在竞争，需要撤销已获得偏向锁的线程，并且把它持有的锁升级为轻量级锁（这个操作需要等到全局安全点，也就是没有线程在执行字节码）才能执行 ​ 如果是已偏向状态，需要检查 markword 中存储的ThreadID 是否等于当前线程的 ThreadID ​ 如果相等，不需要再次获得锁，可直接执行同步代码块 ​ 如果不相等，说明当前锁偏向于其他线程，需要撤销偏向锁并升级到轻量级锁2) 偏向锁的撤销​ 偏向锁的撤销并不是把对象恢复到无锁可偏向状态（因为偏向锁并不存在锁释放的概念），而是在获取偏向锁的过程中，发现 cas 失败也就是存在线程竞争时，直接把被偏向的锁对象升级到被加了轻量级锁的状态。​ 对原持有偏向锁的线程进行撤销时，原获得偏向锁的线程有两种情况：​ 原获得偏向锁的线程如果已经退出了临界区，也就是同步代码块执行完了，那么这个时候会把对象头设置成无锁状态并且争抢锁的线程可以基于 CAS 重新偏向当前线程 ​ 如果原获得偏向锁的线程的同步代码块还没执行完，处于临界区之内，这个时候会把原获得偏向锁的线程升级为轻量级锁后继续执行同步代码块 ​ 在我们的应用开发中，绝大部分情况下一定会存在 2 个以上的线程竞争，那么如果开启偏向锁，反而会提升获取锁的资源消耗。所以可以通过 jvm 参数UseBiasedLocking 来设置开启或关闭偏向锁。​ 3）流程图分析 2，轻量级锁的基本原理1）轻量级锁的加锁和解锁逻辑​ 锁升级为轻量级锁之后，对象的 Markword 也会进行相应的变化。升级为轻量级锁的过程：​ 线程在自己的栈桢中创建锁记录 LockRecord。 ​ 将锁对象的对象头中的MarkWord复制到线程的刚刚创建的锁记录中。 ​ 将锁记录中的 Owner 指针指向锁对象。 ​ 将锁对象的对象头的MarkWord替换为指向锁记录的指针。2）自旋锁​ 轻量级锁在加锁过程中，用到了自旋锁。​ 所谓自旋，就是指当有另外一个线程来竞争锁时，这个线程会在原地循环等待，而不是把该线程给阻塞，直到那个获得锁的线程释放锁之后，这个线程就可以马上获得锁的。注意，锁在原地循环的时候，是会消耗 cpu 的，就相当于在执行一个啥也没有的 for 循环。​ 所以，轻量级锁适用于那些同步代码块执行的很快的场景，这样，线程原地等待很短的时间就能够获得锁了。​ 自旋锁的使用，其实也是有一定的概率背景，在大部分同步代码块执行的时间都是很短的。所以通过看似无异议的循环反而能提升锁的性能。​ 但是自旋必须要有一定的条件控制，否则如果一个线程执行同步代码块的时间很长，那么这个线程不断的循环反而会消耗 CPU 资源。默认情况下自旋的次数是 10 次，可以通过 preBlockSpin 来修改。​ 在 JDK1.6 之后，引入了自适应自旋锁，自适应意味着自旋的次数不是固定不变的，而是根据前一次在同一个锁上自旋的时间以及锁的拥有者的状态来决定。​ 如果在同一个锁对象上，自旋等待刚刚成功获得过锁，并且持有锁的线程正在运行中，那么虚拟机就会认为这次自旋也是很有可能再次成功，进而它将允许自旋等待持续相对更长的时间。如果对于某个锁，自旋很少成功获得过，那在以后尝试获取这个锁时将可能省略掉自旋过程，直接阻塞线程，避免浪费处理器资源。 为什么要将内置锁对象的MarkWord复制到锁记录里面？因为内置锁对象的MarkWord的结构会有所变化，MarkWord将会出现一个指向锁记录的指针，而不再存着无锁状态下锁对象哈希码等信息，所以必须将这些信息暂存起来，供后面在锁释放的时候使用。 JDK1.6使用的轻量级锁是普通自旋锁，且需要手动指定jvm参数开启。JDK1.7之后，轻量级锁使用自适应自旋锁，jvm启动的时候自动开启，且自旋时间由jvm自动控制。​ 轻量级锁也被称为非阻塞同步锁，乐观锁，因为这个过程并没有把线程阻塞挂起，而是让线程空循环等待。 3）轻量级锁的解锁​ 轻量级锁的锁释放逻辑其实就是获得锁的逆向逻辑，通过CAS 操作把线程栈帧中的 LockRecord 替换回到锁对象的MarkWord 中，如果成功表示没有竞争。如果失败，表示当前锁存在竞争，那么轻量级锁就会膨胀成为重量级锁。​ 4）流程图分析 3，重量级锁的基本原理​ 当轻量级锁膨胀到重量级锁之后，意味着线程只能被挂起阻塞来等待被唤醒了。​ 1）重量级锁的 monitor123456789public class SyncTest &#123; public static void main(String[] args) &#123; synchronized (SyncTest.class)&#123; &#125; test(); &#125; public static synchronized void test()&#123; &#125;&#125; 加 了 同 步 代 码 块 以 后 ， 在 字 节 码 中 会 看 到 一 个monitorenter 和 monitorexit。​ 每一个 JAVA 对象都会与一个监视器 monitor 关联，我们可以把它理解成为一把锁，当一个线程想要执行一段被synchronized 修饰的同步方法或者代码块时，该线程得先获取到 synchronized 修饰的对象对应的 monitor。monitorenter 表示去获得一个对象监视器。monitorexit 表示释放 monitor 监视器的所有权，使得其他被阻塞的线程可以尝试去获得这个监视器。​ monitor 依赖操作系统的 MutexLock(互斥锁)来实现的, 线程被阻塞后便进入内核（Linux）调度状态，这个会导致系统在用户态与内核态之间来回切换，严重影响锁的性能。​ 2）重量级锁的加锁的基本流程​ 任意线程对 Object（Object 由 synchronized 保护）的访问，首先要获得 Object 的监视器。如果获取失败，线程进入同步队列，线程状态变为 BLOCKED。当访问 Object 的前驱（获得了锁的线程）释放了锁，则该释放操作唤醒阻塞在同步队列中的线程，使其重新尝试对监视器的获取。​ 3）重量级锁核心原理jvm中每一个对象都会有一个监视器，监视器和对象一起创建，销毁。监视器相当于一个用来监视这些线程进入的特殊房间，其义务是保证同一时间只有一个线程可以访问被保护的临界区代码块。​ 本质上，监视器是一种同步工具，也可以说是一种同步机制，主要特点是：​ 同步：监视器所保护的临界区代码是互斥的执行的。一个监视器是一个运行许可，任一线程进入临界区代码都需要获得这个许可，离开时把许可归还。 协作：监视器提供Signal机制，允许正持有许可的线程暂时放弃许可进入阻塞等待状态，等待其他线程发送Signal去唤醒；其他拥有许可的线程可以发送Signal，唤醒正在阻塞等待的线程，让他可以重新获得许可并启动执行。 ​ 在HotSpot虚拟机中，监视器是由C++类ObjectMonitor实现的，ObjectMonitor类定义在ObjectMonitor.hpp文件中。​ ObjectMonitor的Owner，WaitSet，Cxq，EntryList这几个属性比较关键。​ ObjectMonitor的WaitSet，Cxq，EntryList这几个队列存放抢夺重量级锁的线程，而ObjectMonitor的Owner所指向的线程即为获得锁的线程。​ Cxq:竞争队列，所有请求锁的线程首先被放到这个竞争队列。 EntryList:Cxq中那些有资格成为候选资源的线程被移动到EntryList中。 WaitSet：某个拥有ObjectMonitor的线程在调用Object.wait()方法之后将被阻塞，然后该线程将会被放到WaitSet链表中。 ​ CxqCxq并不是一个真正的队列，只是一个虚拟队列，原因在于Cxq是由Node及其next指针逻辑构成的，并不存在一个队列的数据结构。每次新加入Node会在Cxq的队头进行，通过cas改变第一个节点的指针为新增节点，同时设置新增节点的next指向后续的节点；从Cxq取得元素时，会从队尾获取。显然，Cxq结构是一个无锁的结构。​ 因为只有Owner线程才能从队尾取元素，即线程出列操作无争用，当然就避免了cas的aba问题。​ 在线程进入Cxq前，抢锁线程会先尝试通过cas自旋锁获取锁，如果获取不到，就进入Cxq队列，这明显对于已经进入Cxq队列的线程是不公平的。所以，synchronized同步块所使用的重量级锁是不公平锁。​ EntryList​ EntryList与Cxq在逻辑上都属于等待队列。Cxq会被线程并发访问，为了降低对Cxq队尾的争用，而建立EntryList。在Owner线程释放锁时，jvm会从Cxq中迁移线程到EntryList，并会指定EntryList中的某个线程为OnDeck Thread。EntryList中的线程作为候选竞争线程而存在。​ OnDeck Thread &amp; Owner Threadjvm不直接把锁传递给Owner Thread，而是把锁竞争的权利交给OnDeck Thread，OnDeck需要重新竞争锁。这样虽然牺牲了一些公平性，但是能极大的提升系统的吞吐量，在jvm中，也把这种选择行为称为竞争切换。​ OnDeck Thread 获取到锁资源后会变成Owner Thread。无法获得锁的OnDeck Thread则会依然留在EntryList中，考虑到公平性，OnDeck Thread在EntryList中的位置不发生变化。​ 在OnDeck Thread成为Owner的过程中，还有一个不公平的事情，就是后来的新抢锁线程可能直接通过cas自旋成为Owner而抢到锁。​ WaitSet如果Owner线程被Object.wait()阻塞，就转移到WaitSet队列中，直到某个时刻通过Object.notify()唤醒，该线程就会重新进入EntryList中。​ 4）重量级锁的开销处于ContentionList，EntryList，WaitSet中的线程都处于阻塞状态，线程的阻塞或者唤醒都需要操作系统来帮忙，Linux内核下采用pthread_mutex_lock系统调用实现，进程需要从用户态切换到内核态。​ Linux系统体系架构分为用户态和内核态。​ Linux系统的内核是一组特殊的软件程序，负责控制计算机的硬件资源，例如协调cpu资源，分配内存资源，并且提供稳定的环境供应用程序运行。应用程序活动的空间为用户空间，应用程序的执行必须依托于内核提供的资源，包括cpu资源，存储资源，io资源等。​ 用户态与内核态有各自专用的内存空间，专用的寄存器等，进程从用户态切换到内核态需要传递许多变量，参数给内核，内核也需要保护好用户态在切换时的一些寄存器值，变量等，以便内核态调用结束后切换回用户态继续工作。​ 用户态的进程能够访问的资源受到了极大的控制，而运行在内核态的进程可以“为所欲为”。一个进程可以运行在用户态，也可以运行在内核态，那么肯定存在用户态和内核态切换的过程。进程从用户态到内核态切换主要包括以下三种方式:​ (1)硬件中断。硬件中断也称为外设中断，当外设完成用户的请求时会向CPU发送中断信号。​ (2)系统调用。其实系统调用本身就是中断，只不过是软件中断，跟硬件中断不同。​ (3)异常。如果当前进程运行在用户态，这个时候发生了异常事件(例如缺页异常)，就会触发切换。​ 用户态是应用程序运行的空间，为了能访问到内核管理的资源(例如CPU、内存、I/0)，可以通过内核态所提供的访问接口实现，这些接口就叫系统调用。pthredmutexlock系统调用是内核态为用户态进程提供的Linux内核态下互斥锁的访问机制，所以使用pthread_mutex_lock系统调用时，进程需要从用户态切换到内核态，而这种切换是需要消耗很多时间的，有可能比用户执行代码的时间还要长。​ 由于jvm轻量级锁使用cas进行自旋抢锁，这些cas操作都处于用户态下，进程不存在用户态和内核态之间的运行切换，因此jvm轻量级锁开销比较小。而jvm重量级锁使用了Linux内核态下的互斥锁，这是重量级锁开销很大的原因。 4，回顾线程的竞争​ 假如有这样一个同步代码块，存在 Thread#1、Thread#2 等多个线程：​ 123synchronized (lock) &#123;// do something&#125; 情况一：只有 Thread#1 会进入临界区；​ 情况二：Thread#1 和 Thread#2 交替进入临界区,竞争不激烈；​ 情况三：Thread#1/Thread#2/Thread3… 同时进入临界区，竞争激烈。​ 1）偏向锁​ 此时当 Thread#1 进入临界区时，JVM 会将 lockObject 的对象头 Mark Word 的锁标志位设为“01”，同时会用 CAS 操作把 Thread#1 的线程 ID 记录到 Mark Word 中，此时进入偏向模式。所谓“偏向”，指的是这个锁会偏向于 Thread#1，若接下来没有其他线程进入临界区，则 Thread#1 再出入临界区无需再执行任何同步操作。也就是说，若只有Thread#1 会进入临界区，实际上只有 Thread#1 初次进入临界区时需要执行 CAS 操作，以后再出入临界区都不会有同步操作带来的开销。​ 2）轻量级锁偏向锁的场景太过于理想化，更多的时候是 Thread#2 也会尝试进入临界区， 如果 Thread#2 也进入临界区但是Thread#1 还没有执行完同步代码块时，会暂停 Thread#1并且升级到轻量级锁。Thread#2 通过自旋再次尝试以轻量级锁的方式来获取锁。​ 3）重量级锁​ 如果 Thread#1 和 Thread#2 正常交替执行，那么轻量级锁基本能够满足锁的需求。但是如果 Thread#1 和 Thread#2同时进入临界区，那么轻量级锁就会膨胀为重量级锁，意味着 Thread#1 线程获得了重量级锁的情况下，Thread#2就会被阻塞。​ 五，结合wait，notify被阻塞的线程什么时候被唤醒，取决于获得锁的线程什么时候执行完同步代码块并且释放锁。那怎么做到显示控制呢？我们就需要借 助 一 个 信 号机 制 ： 在 Object 对 象 中 ，提 供 了wait/notify/notifyall，可以用于控制线程的状态 1，wait/notify/notifyall的基本概念​ wait：表示持有对象锁的线程 A 准备释放对象锁权限，释放 cpu 资源并进入等待状态。​ notify：表示持有对象锁的线程 A 准备释放对象锁权限，通知 jvm 唤醒某个竞争该对象锁的线程 X。线程 Asynchronized 代码执行结束并且释放了锁之后，线程 X 直接获得对象锁权限，其他竞争线程继续等待(即使线程 X 同步完毕，释放对象锁，其他竞争线程仍然等待，直至有新的 notify ,notifyAll 被调用)。​ notifyAll：notifyall 和 notify 的区别在于，notifyAll 会唤醒所有竞争同一个对象锁的所有线程，当已经获得锁的线程A 释放锁之后，所有被唤醒的线程都有可能获得对象锁权限。​ 三个方法都必须在 synchronized 同步关键字 所 限 定 的 作 用 域 中 调 用 ， 否 则 会 报 错java.lang.IllegalMonitorStateException ，意思是因为没有同步，所以线程对对象锁的状态是不确定的，不能调用这些方法。 通过同步机制来确保线程从 wait 方法返回时能够感知到 notify 线程对变量做出的修改 2，waity /notify 的基本使用12345678910111213141516171819202122232425262728293031323334353637public class SyncTest &#123; public static void main(String[] args) &#123; Share share = new Share(); new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) try &#123; share.add(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, &quot;AAA&quot;).start(); new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) try &#123; share.sub(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, &quot;BBB&quot;).start(); &#125;&#125;class Share &#123; private int num = 0; public synchronized void add()throws Exception&#123; //判断 if (num!=0) this.wait(); System.out.println(Thread.currentThread().getName()+&quot;执行了：&quot;+ ++num); this.notify(); &#125; public synchronized void sub()throws Exception&#123; if (num==0) this.wait(); System.out.println(Thread.currentThread().getName()+&quot;执行了：&quot;+ --num); this.notify(); &#125;&#125; 3，waity /notify 的基本原理 线程A执行MonitorEnter指令成功，获取到锁。 线程A执行Object.wait()，线程A将自己加入到等待队列并释放锁。 线程B执行MonitorEnter指令成功，获取到锁。 线程B执行Object.notify(),线程A从等待队列移动到同步队列，等到收到MonitorExit后，出队列。 ​ 六，synchronized的可重入实现原理每个锁对象拥有一个锁计数器和一个指向持有该锁的线程的指针。​ 当执行monitorenter时，如果目标锁对象的计数器为0，那么说明他没有被其他线程锁持有，Java虚拟机会将改锁对象的持有线程设置为当前线程，并且将其计数器+1.​ 在目标锁对象的计数器不为0的情况下，如果锁对象的持有线程是当前线程，那么Java虚拟机可以将其计数器+1，否则需要等待，直至持有线程释放锁。​ 当执行monitorexit时，Java虚拟机则需要将锁对象的计数器-1.计数器为0代表释放锁。​ 12345678910111213141516171819202122232425public class Demo3 &#123; private static Lock lock=new ReentrantLock(); public static void main(String[] args) &#123; m1(); &#125; public static void m1()&#123; new Thread(()-&gt;&#123; try &#123; lock.lock(); System.out.println(&quot;第一次加锁&quot;); try &#123; lock.lock(); System.out.println(&quot;第2次加锁&quot;); &#125;finally &#123; lock.unlock(); &#125; &#125;finally &#123; lock.unlock(); &#125; &#125;,&quot;m1&quot;).start(); &#125;&#125;","categories":[{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"Unsafe API详解","slug":"JUC/Unsafe API详解","date":"2022-01-11T11:05:27.570Z","updated":"2022-01-11T11:30:55.633Z","comments":true,"path":"2022/01/11/JUC/Unsafe API详解/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/JUC/Unsafe%20API%E8%AF%A6%E8%A7%A3/","excerpt":"","text":"一，Unsafe使用建议 unsafe有可能在未来几年的jdk版本中移除或者不允许java应用代码使用，这一点可能会导致使用了unsafe的应用无法运行在高版本的jdk中。 unsafe不少方法中必须提供原始地址（内存地址）和被替换对象的地址，偏移量要自己计算，一旦出现问题就是jvm级别的崩溃异常。 unsafe提供的直接内存访问的方法中使用的内存不受jvm管理（无法被gc），需要手动管理，一旦出现疏忽可能成为内存泄漏的源头。 ​ 二，unsafe详解unsafe类中一共有82个public native 修饰的方法，还有几十个基于这个82个方法的其他方法。​ 1.初始化123456789101112131415161718private static native void registerNatives();static &#123; registerNatives(); sun.reflect.Reflection.registerMethodsToFilter(Unsafe.class, &quot;getUnsafe&quot;);&#125;private Unsafe() &#123;&#125;private static final Unsafe theUnsafe = new Unsafe();@CallerSensitivepublic static Unsafe getUnsafe() &#123; Class&lt;?&gt; caller = Reflection.getCallerClass(); if (!VM.isSystemDomainLoader(caller.getClassLoader())) throw new SecurityException(&quot;Unsafe&quot;); return theUnsafe;&#125; 新建一个unsafe实例命名为theunsafe，通过静态方法getUnsafe获取，获取的时候需要做权限判断，由此可见，unsafe类的设计使用了单例设计模式，构造器私有化了。unsafe类做了限制，如果是普通的调用的话，他会抛出一个权限异常，只有由引导类加载器加载的类才能使用这个类的方法。最简单的方式是通过反射获取unsafe实例。​ 123Field f = Unsafe.class.getDeclaredField(&quot;theUnsafe&quot;);f.setAccessible(true);Unsafe unsafe = (Unsafe) f.get(null); 2.类，对象和变量相关方法2.1 getObject​ 通过给定的java变量获取引用值。这里实际上是获取一个java对象o中，获取偏移地址==offset的属性的值。​ 此方法可以突破修饰符的限制。类似的方法还有getInt，getDouble等。​ 2.2 putObject将引用值存储到给定的java变量中，这里实际上是设置一个java对象o中偏移地址为offset的属性的值为x。​ 此方法可以突破修饰符的限制。类似的方法还有putInt，putDouble等。​ 2.3 getObjectVolatile​ 此方法和上面的getObject功能类似，不过附加了volatile，强制从主存中获取属性值。​ 这个方法要求被使用的属性被volatile修饰，否则和getObject没有区别。​ 2.4 putObjectVolatile​ 此方法和上面的putObject功能类似，不过附加了volatile，也就是设置值得时候强制刷新的主存，从而保证这些变更对其他线程可见。​ 2.5 putOrderedObject设置o对象中offset偏移地址对应的Object型field的值为指定值x。这是一个有序或者有延迟的putObjectVolatile方法，并且不保证值得改变立马被其他线程看到。​ 只有在field被volatile修饰的时候期望被修改的时候才会生效​ 2.6 staticFieldOffset返回给定的静态属性在他的类的存储分配中的位置（偏移地址）。这个方法仅仅针对静态属性，非静态属性调用会抛出异常。​ 2.7 objectFieldOffset​ 返回给定的非静态属性在他的类的存储分配中的位置（偏移地址）。这个方法仅仅针对非静态属性，静态属性调用会抛出异常。​ 2.8 staticFieldBase​ 返回给定的静态属性的位置，配合staticFieldOffset使用。实际上这个方法的返回值就是静态属性所在的Class对象的一个内存快照。​ 2.9 shouldBeInitialized​ 检测给定的类是否需要初始化。通常需要使用在获取一个类的静态属性的时候（一个类如果没有初始化，他的静态属性也不会初始化）。​ 2.10 arrayIndexScale返回数组类型的比例因子（其实就是数据中元素偏移量地址的增量，因为数组中的元素地址是连续的）。​ 2.11 arrayBaseOffset​ 返回数组类型的第一个元素的偏移地址（基础偏移地址）。如果arrayIndexScale方法返回的比例因子不为0，可以通过结合基础偏移地址和比例因子访问数组的所有元素。​ 3.内存管理3.1 allocateMemory分配一块新的本地内存，通过bytes指定内存块的大小(单位是byte)，返回新开辟的内存的地址。如果内存块的内容不被初始化，那么它们一般会变成内存垃圾。​ 生成的本机指针永远不会为零，并将对所有值类型进行对齐。可以通过freelemory方法释放内存块 或者通过reallocatelemory方法调整内存块大小。​ bytes值为负数或者过大会抛出IIlegalArgumentException异常，如果系统拒绝分配内存会抛出Out( OfMemoryError异常。​ 3.2 reallocateMemory通过指定的内存地址address重新调整本地内存块的大小，调整后的内存块大小通过bytes指定(单立为byte)。可以通过freelemory方法释放内存块，或者通过reallocateMemory方法调整内存块大小。bytes值为负数或者过大会抛出IllegalArgumentEx ception异常，如果系统拒绝分配内存会抛出OutOfMemoryError异常。​ 3.3 freeMemory释放通过allocatelemory方法申请的内存​ 3.4 setMemory将给定内存块中的所有字节设置为固定值(通常是0)。内存块的地址由对象引用o和偏移地址共同决定，如果对象引用o为null，offset就是绝对地址。​ 第三个参数就是内存块的大小，如果使用allocatelenory进行内存开辟的话，这里的值应该和all ocatelemory的参数一致。value就是设置的固定值，一般为0。​ 4.多线程同步4.1 monitorEnter锁定对象，必须通过nonitorExit方法才能解锁。此方法经过实验是可以重入的，也就是可以多次调用，然后通过多次调用monitorExit进行解锁。​ ​ 4.2 monitorExit​ 解锁对象，前提是对象必须已经调用monitorEnter进行加锁，否则抛出IIlegalMonitorStateException异常。​ 4.3 tryMonitorEnter尝试锁定对象，如果加锁成功返回true，否则返回false。必须通过monitorExit方法才能解锁。​ 4.4 compareAndSwapObject针对Object对象进行CAS操作。即是对应Java变量引用o，原子性地更新o中偏移地址为offset的属性的值为x，当且仅当偏移地址为offset的属性的当前值为expected才会更新成功返回true，否则返回false。​ o:目标Java变量引用。​ offset:目标Java变量中的目标属性的偏移地址。​ expected:目标Java变量中的目标属性的期望的当前值。 x:目标Java变量中的目标属性的目标更新值。​ 类似的方法有compareAndSwapInt和compareAndSwapLong，在Jdk8中基于CAS扩展出来的方法有getAn daddInt、getAndAddLong、getAndSetInt、getAndSetLong、getAndSet0bject，它们的作用都是:通过CAS设置新的值，返回旧的值。​ 5.线程的挂起和恢复5.1 unpark恢复线程，必须制定要恢复的线程thread。​ 5.2 park​ 阻塞当前线程直到一个unpark方法出现(被调用)。​ 参数:isAbsolute true 表示绝对时间，阻塞时间按照纳秒计算。false表示相对时间，阻塞时间按照毫秒计算。​ 简单理解isAbsolute true时精度更高。​ 参数:time表示阻塞时间。值为0时表示无限期阻塞，直到有一个线程通过unsafeunpark(当前阻塞线程)才会唤醒。","categories":[{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"并发编程基础知识","slug":"JUC/并发编程基础知识","date":"2022-01-11T11:05:18.345Z","updated":"2022-01-11T11:21:07.732Z","comments":true,"path":"2022/01/11/JUC/并发编程基础知识/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/JUC/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/","excerpt":"","text":"一，线程的创建在java中，有多种方式来实现多线程。继承Thread类，实现Runnable接口，使用ExecutorService，Callable，Future实现带返回结果的多线程。 1.继承Thread类创建线程Thread类的本质是实现了Runnable接口的一个实例，代表一个线程的实例。启动线程的唯一方法就是通过Thread类的start()方法，这是一个本地方法，他会启动一个新线程，并执行run()。 1234567891011121314151617181920212223242526272829303132333435public class Thread implements Runnable&#123; private Runnable target; @Override public void run() &#123; if (target != null) &#123; target.run(); &#125; &#125; public synchronized void start() &#123; if (threadStatus != 0) throw new IllegalThreadStateException(); group.add(this); boolean started = false; try &#123; start0(); started = true; &#125; finally &#123; try &#123; if (!started) &#123; group.threadStartFailed(this); &#125; &#125; catch (Throwable ignore) &#123; /* do nothing. If start0 threw a Throwable then it will be passed up the call stack */ &#125; &#125; &#125; private native void start0();&#125; 2.实现Runnable接口创建多线程12345@FunctionalInterfacepublic interface Runnable &#123; public abstract void run();&#125; 为什么实现Runnable接口能够实现多线程？ 123456789101112131415161718/** * @author yhd * @description 使用Runnable接口创建多线程 * @email yinhuidong2@xiaomi.com * @since 2021/6/27 */public class RunnableTest implements Runnable&#123; public static void main(String[] args) &#123; RunnableTest runnableTest = new RunnableTest(); new Thread(runnableTest).start(); &#125; @Override public void run() &#123; System.out.println(Thread.currentThread().getName()); &#125;&#125; 在Thread类里面，有两个方法 12345678public Thread(Runnable target) &#123; init(null, target, &quot;Thread-&quot; + nextThreadNum(), 0);&#125;private void init(ThreadGroup g, Runnable target, String name, long stackSize, AccessControlContext acc) &#123; this.target = target; &#125; 3.实现callable接口来创建线程有的时候，我们可能需要让一个执行的线程在执行完以后，提供一个返回值给到当前的主线程，主线程需要依赖这个值进行后续的逻辑处理，那么这个时候，就需要用到带返回值的线程了。Java中提供了这样的实现方式： 12345678910111213141516171819202122232425262728293031/** * @author yhd * @description 使用Callable接口创建多线程 * @email yinhuidong2@xiaomi.com * @since 2021/6/27 */public class CallableTest implements Callable &#123; @Override public Object call() throws Exception &#123; System.out.println(&quot;正在计算结果&quot;); return &quot;123&quot;; &#125; public static void main(String[] args) &#123; try &#123; FutureTask task = new FutureTask&lt;&gt;(new CallableTest()); while (task.isDone())&#123; System.out.println(task.get()); &#125; &#125; catch (Exception e) &#123; &#125; finally &#123; &#125; &#125;&#125; 二，线程的生命周期1.生命周期线程一共有六种状态（NEW、RUNNABLE、BLOCKED、WAITING、TIME_WAITING、TERMINATED）New：初始化状态，线程被创建，但是还没有调用start()。Runnabled：运行状态，Java线程把操作系统中的运行状态和就绪状态统称为运行中状态。Blocked：阻塞状态，表示线程进入等待状态，也就是线程因为某种原因放弃了CPU的使用权。阻塞也分为几种情况： 等待阻塞：运行的线程执行wait(),jvm会把当前线程放入到等待队列 同步阻塞：运行的线程在获取对象的同步锁时，若该同步锁被其他线程占用了，那么JVM会把当前线程放入到锁池中，也就是同步队列 其他阻塞：运行中的线程执行thread.sleep()或者thread.join(),或者发出了IO请求时，JVM会把当前线程设置为阻塞状态，当sleep结束，join线程终止，io处理完毕则线程恢复 time——waiting：超时以后自动返回。terminated：终止状态，表示当前线程执行完毕。 2.代码演示线程状态12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/** * @author yhd * @description 代码演示线程状态 * @email yinhuidong2@xiaomi.com * @since 2021/6/27 */public class ThreadStatus &#123; public static void main(String[] args) &#123; //time_waiting new Thread(()-&gt;&#123; while (true)&#123; try &#123; TimeUnit.SECONDS.sleep(100); &#125; catch (Exception e) &#123; &#125; finally &#123; &#125; &#125; &#125;,&quot;time_waiting&quot;).start(); //waiting 线程拿到当前的类锁以后，执行wait() new Thread(()-&gt;&#123; while (true)&#123; synchronized (ThreadStatus.class)&#123; try &#123; ThreadStatus.class.wait(); &#125; catch (Exception e) &#123; &#125; finally &#123; &#125; &#125; &#125; &#125;).start(); //block 两个线程竞争锁 new Thread(new BlockedDemo(),&quot;block-01&quot;).start(); new Thread(new BlockedDemo(),&quot;block-02&quot;).start(); &#125; static class BlockedDemo extends Thread &#123; @Override public void run() &#123; synchronized (BlockedDemo.class) &#123; while (true) &#123; try &#123; TimeUnit.SECONDS.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; &#125;&#125; 3.通过工具查看线程的状态jstack是java虚拟机自带的一种堆栈跟踪工具。 jstack用于打印给定的java进程ID或者core file 或者远程调试服务的Java 堆栈信息。通过上面的代码演示，可以知道线程在整个生命周期中并不是固定处于某种状态，而是随着代码的执行在不同的状态之间进行切换。 三，线程相关方法1.启动调用start()去启动一个线程，当run()中的代码执行完毕以后，线程的生命周期也将终止。调用start()的语义是当前线程告诉jvm，启动调用start()方法的线程。 1）启动原理启动一个线程为什么是调用start()，而不是run()?调用start()实际上是调用一个native方法start0()来启动一个线程，首先start0()这个方法是在Thread.class的静态代码块中注册的。registerNatives的本地方法的定义在文件Thread.c,Thread.c定义了各个操作系统平台要用的关于线程的公共数据和操作。 1234567891011121314151617181920212223242526272829303132333435363738#include &quot;jni.h&quot;#include &quot;jvm.h&quot;#include &quot;java_lang_Thread.h&quot;#define THD &quot;Ljava/lang/Thread;&quot;#define OBJ &quot;Ljava/lang/Object;&quot;#define STE &quot;Ljava/lang/StackTraceElement;&quot;#define ARRAY_LENGTH(a) (sizeof(a)/sizeof(a[0]))static JNINativeMethod methods[] = &#123; &#123;&quot;start0&quot;, &quot;()V&quot;, (void *)&amp;JVM_StartThread&#125;, &#123;&quot;stop0&quot;, &quot;(&quot; OBJ &quot;)V&quot;, (void *)&amp;JVM_StopThread&#125;, &#123;&quot;isAlive&quot;, &quot;()Z&quot;, (void *)&amp;JVM_IsThreadAlive&#125;, &#123;&quot;suspend0&quot;, &quot;()V&quot;, (void *)&amp;JVM_SuspendThread&#125;, &#123;&quot;resume0&quot;, &quot;()V&quot;, (void *)&amp;JVM_ResumeThread&#125;, &#123;&quot;setPriority0&quot;, &quot;(I)V&quot;, (void *)&amp;JVM_SetThreadPriority&#125;, &#123;&quot;yield&quot;, &quot;()V&quot;, (void *)&amp;JVM_Yield&#125;, &#123;&quot;sleep&quot;, &quot;(J)V&quot;, (void *)&amp;JVM_Sleep&#125;, &#123;&quot;currentThread&quot;, &quot;()&quot; THD, (void *)&amp;JVM_CurrentThread&#125;, &#123;&quot;countStackFrames&quot;, &quot;()I&quot;, (void *)&amp;JVM_CountStackFrames&#125;, &#123;&quot;interrupt0&quot;, &quot;()V&quot;, (void *)&amp;JVM_Interrupt&#125;, &#123;&quot;isInterrupted&quot;, &quot;(Z)Z&quot;, (void *)&amp;JVM_IsInterrupted&#125;, &#123;&quot;holdsLock&quot;, &quot;(&quot; OBJ &quot;)Z&quot;, (void *)&amp;JVM_HoldsLock&#125;, &#123;&quot;getThreads&quot;, &quot;()[&quot; THD, (void *)&amp;JVM_GetAllThreads&#125;, &#123;&quot;dumpThreads&quot;, &quot;([&quot; THD &quot;)[[&quot; STE, (void *)&amp;JVM_DumpThreads&#125;,&#125;;#undef THD#undef OBJ#undef STEJNIEXPORT void JNICALLJava_java_lang_Thread_registerNatives(JNIEnv *env, jclass cls)&#123; (*env)-&gt;RegisterNatives(env, cls, methods, ARRAY_LENGTH(methods));&#125; 从这段代码可以看出，start0(),实际上会执行JVM_StartThread(),这个方法是干嘛的？从名字上来看，似乎是JVM层面去启动一个线程，如果真的是这样，那么在JVM层面，一定会调用Java中的run()。来到jvm.cpp文件（这个文件需要下载hotspot源码才能找到）。 12JVM_ENTRY(void, JVM_StartThread(JNIEnv* env, jobject jthread)) native_thread = new JavaThread(&amp;thread_entry, sz); JVM_ENTRY 是用来定义 JVM_StartThread 函数的，在这个函数里面创建了一个真正和平台有关的本地线程. 继续看看 newJavaThread 做了什么事情,继续寻找 JavaThread 的定义。 1234567891011JavaThread::JavaThread(ThreadFunction entry_point, size_t stack_sz) : JavaThread() &#123; _jni_attach_state = _not_attaching_via_jni; set_entry_point(entry_point); // Create the native thread itself. // %note runtime_23 os::ThreadType thr_type = os::java_thread; thr_type = entry_point == &amp;CompilerThread::thread_entry ? os::compiler_thread : os::java_thread; os::create_thread(this, thr_type, stack_sz); &#125; 这个方法有两个参数，第一个是函数名称，线程创建成功之后会根据这个函数名称调用对应的函数；第二个是当前进程内已经有的线程数量。最后重点关注一下os::create_thread,实际就是调用平台创建线程的方法来创建线程。接下来就是线程的启动，会调用 Thread.cpp 文件中的Thread::start(Thread* thread)方法，代码如下： 123456789void Thread::start(Thread* thread) &#123; if (thread-&gt;is_Java_thread()) &#123; java_lang_Thread::set_thread_status(thread-&gt;as_Java_thread()-&gt;threadObj(), JavaThreadStatus::RUNNABLE); &#125; os::start_thread(thread);&#125; start ()中有一个函数调用： os::start_thread(thread);，调用平台启动线程的方法，最终会调用 Thread.cpp 文件中的 JavaThread::run()方法。 2.终止线程的终止并不是简单的调用stop命令，虽然api仍然可以使用，但是和其他的控制线程的方法（`` suspend、resume ``）一样，都是不建议使用的。就拿stop来说，stop()在结束一个线程时并不会保证线程的资源正常释放，因此可能导致程序出现一些不确定的状态。要优雅的去中断一个线程，在线程中提供了一个**interrupt()**。 1）interrupt（）其他线程通过调用当前线程的interrupt(),表示像当前线程打个招呼，告诉他可以中断线程的执行了，至于什么时候中断，取决于当前线程自己。线程通过检查自身 是否被中断来进行响应，可以通过isInterrupted()来判断是否被中断。代码演示线程的优雅终止 12345678910111213141516171819202122232425262728/** * @author yhd * @description 代码演示线程的优雅终止 * @email yinhuidong2@xiaomi.com * @since 2021/6/27 */public class Stop &#123; private static int i; public static void main(String[] args) &#123; Thread t = new Thread(()-&gt;&#123; while (!Thread.currentThread().isInterrupted())&#123; i++; &#125; System.out.println(&quot;num:&quot;+i); &#125;,&quot;interrupt&quot;); t.start(); try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; t.interrupt(); &#125;&#125; 这种通过标识位或者中断操作的方式能够使线程在终止时有机会去清理资源，而不是武断地将线程停止，因此这种终止线程的做法显得更加安全和优雅。 2）Thread.interrupted()上面的案例中，通过 interrupt，设置了一个标识告诉线程可 以 终 止 了 ， 线 程 中 还 提 供 了 静 态 方 法Thread.interrupted()对设置中断标识的线程复位。比如在上面的案例中，外面的线程调用 thread.interrupt 来设置中断标识，而在线程里面，又通过Thread.interrupted把线程的标识又进行了复位。 1234567891011121314151617181920public class InterruptDemo &#123; public static void main(String[] args) throws InterruptedException &#123; Thread thread = new Thread(() -&gt; &#123; while (true) &#123; if (Thread.currentThread().isInterrupted()) &#123; System.out.println(&quot;before:&quot; + Thread.currentThread().isInterrupted()) ; Thread.interrupted(); // 对线程进行复位，由 true 变成 false System.out.println(&quot;after:&quot; + Thread .currentThread().isInterrupted()); &#125; &#125; &#125;, &quot;interruptDemo&quot;); thread.start(); TimeUnit.SECONDS.sleep(1); thread.interrupt(); &#125;&#125; 3)其他线程的复位除了通过 Thread.interrupted 方法对线程中断标识进行复位 以 外 ， 还 有 一 种 被 动 复 位 的 场 景 ， 就 是 对 抛 出InterruptedException 异 常 的 方 法 ， 在InterruptedException 抛出之前，JVM 会先把线程的中断标识位清除，然后才会抛出InterruptedException，这个时候如果调用 isInterrupted()，将会返回 false。 12345678910111213141516171819202122public class InterruptDemo &#123; private static int i; public static void main(String[] args) throws InterruptedException &#123; Thread thread = new Thread(() -&gt; &#123; while (!Thread.currentThread().isInterrupted()) &#123; try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; System.out.println(&quot;Num:&quot; + i); &#125;, &quot;interruptDemo&quot;); thread.start(); TimeUnit.SECONDS.sleep(1); thread.interrupt(); System.out.println(thread.isInterrupted()); &#125;&#125; 4)为什么要复位Thread.interrupted()是属于当前线程的，是当前线程对外界中断信号的一个响应，表示自己已经得到了中断信号，但不会立刻中断自己，具体什么时候中断由自己决定，让外界知道在自身中断前，他的中断状态仍然是 false，这就是复位的原因。 5)线程的终止原理1234567891011121314public void interrupt() &#123; if (this != Thread.currentThread()) checkAccess(); synchronized (blockerLock) &#123; Interruptible b = blocker; if (b != null) &#123; interrupt0(); // Just to set the interrupt flag b.interrupt(this); return; &#125; &#125; interrupt0();&#125; 这个方法里面，调用了 interrupt0()，这个方法在前面分析start()的时候见过，是一个 native 方法，同样，找到 jvm.cpp 文件，找到JVM_Interrupt 的定义，这个方法比较简单，直接调用了 Thread::interrupt(thr)，这个方法的定义在 Thread.cpp 文件中，Thread::interrupt() 调用了 os::interrupt()，这个是调用平台的 interrupt()，这个方法的实现是在 os_*.cpp文件中，其中星号代表的是不同平台，因为 jvm 是跨平台的，所以对于不同的操作平台，线程的调度方式都是不一样的。以 os_linux.cpp 文件为例：set_interrupted(true)实际上就是调用 osThread.hpp 中的set_interrupted()，在 osThread 中定义了一个成员属性 volatile jint _interrupted。 通过上面的代码分析可以知道，thread.interrupt()实际就是设置一个 interrupted 状态标识为 true、并且通过ParkEvent 的 unpark()来唤醒线程。 对于 synchronized 阻塞的线程，被唤醒以后会继续尝试获取锁，如果失败仍然可能被 park 在调用 ParkEvent 的 park 方法之前，会先判断线程的中断状态，如果为 true，会清除当前线程的中断标识 Object.wait 、 Thread.sleep 、 Thread.join 会 抛 出InterruptedException 为什么 Object.wait、Thread.sleep 和 Thread.join 都 会 抛 出InterruptedException?​ 这几个方法有一个共同点，都是属于阻塞的方法，而阻塞方法的释放会取决于一些外部的事件，但是阻塞方法可能因为等不到外部的触发事件而导致无法终止，所以它允许一个线程请求自己来停止它正在做的事情。当一个方法抛出 InterruptedException 时，它是在告诉调用者如果执行该方法的线程被中断，它会尝试停止正在做的事情并且通过抛出 InterruptedException 表示提前返回。所以，这个异常的意思是表示一个阻塞被其他线程中断了。然后，由于线程调用了 interrupt()中断方法，那么Object.wait、Thread.sleep 等被阻塞的线程被唤醒以后会通过 is_interrupted 方法判断中断标识的状态变化，如果发现中断标识为 true，则先清除中断标识，然后抛出InterruptedException。 InterruptedException 异常的抛出并不意味着线程必须终止，而是提醒当前线程有中断的操作发生，至于接下来怎么处理取决于线程本身， 直接捕获异常不做任何处理 将异常往外抛出 停止当前线程，并打印异常信息​ 总结：如果当前线程正处于wait，sleep，join状态，然后当前线程被打断，如果当前线程中断标识为true，清除当前线程的中端标识，并且当前线程会收到中断异常。​ 6)代码时间①打断sleep，wait，join的线程这几个方法都会让线程进入阻塞状态打断sleep的线程，会清空打断状态，并抛出异常 123456789101112131415161718192021222324252627282930313233/** * @author yhd * @description interrupt打断sleep的线程 * @email yinhuidong2@xiaomi.com * @since 2021/6/27 */public class Sleep &#123; public static void test() throws Exception&#123; Thread t = new Thread(()-&gt;&#123; try &#123; Thread.sleep(10000); &#125; catch (Exception e) &#123; System.out.println(&quot;抛出异常&quot;); &#125; finally &#123; &#125; &#125;); t.start(); Thread.sleep(1000); t.interrupt(); System.out.println(&quot;打断状态：&quot;+t.isInterrupted()); &#125; public static void main(String[] args) &#123; try &#123; test(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; ②打断正常运行的线程打断正常运行的线程，不会清空打断状态 1234567891011121314151617181920212223242526272829/** * @author yhd * @description 打断正常运行的线程 * @email yinhuidong2@xiaomi.com * @since 2021/6/27 */public class InterruptTest &#123; private static void test1() throws InterruptedException &#123; Thread t2 = new Thread(()-&gt;&#123; while(true) &#123; Thread current = Thread.currentThread(); boolean interrupted = current.isInterrupted(); if(interrupted) &#123; System.out.println(&quot; 打断状态: &#123;&#125;&quot;+ Thread.currentThread().isInterrupted()); break; &#125; &#125; &#125;, &quot;t2&quot;); t2.start(); sleep(1); t2.interrupt(); &#125; public static void main(String[] args) throws Exception &#123; test1(); &#125;&#125; ③打断park线程1234567891011121314151617181920212223242526/** * @author yhd * @description 打断park线程 * @email yinhuidong2@xiaomi.com * @since 2021/6/27 */public class ParkTest &#123; private static void test1() throws InterruptedException &#123; Thread t1 = new Thread(() -&gt; &#123; System.out.println(&quot;park...&quot;); LockSupport.park(); System.out.println(&quot;unpark...&quot;); System.out.println(&quot;打断状态：&#123;&#125;&quot;+ Thread.currentThread().isInterrupted()); &#125;, &quot;t1&quot;); t1.start(); sleep(1); t1.interrupt(); &#125; public static void main(String[] args) throws Exception &#123; test1(); &#125;&#125; 如果打断标记已经是true，则park会失效。 12345678910111213141516171819202122232425/** * @author yhd * @description park的第二个案例演示 * @email yinhuidong2@xiaomi.com * @since 2021/6/27 */public class ParkTest2 &#123; private static void test4() throws InterruptedException &#123; Thread t1 = new Thread(() -&gt; &#123; for (int i = 0; i &lt; 5; i++) &#123; System.out.println(&quot;park...&quot;); LockSupport.park(); System.out.println(&quot;打断状态：&#123;&#125;&quot;+Thread.currentThread().isInterrupted()); &#125; &#125;); t1.start(); sleep(1); t1.interrupt(); &#125; public static void main(String[] args) throws Exception &#123; test4(); &#125;&#125; 可以使用 Thread.interrupted() 清除打断状态。 3.sleep() &amp; yield()1)sleep() 调用sleep会让当前线程从running状态进入timedwaiting状态（阻塞） 其他线程可以使用interrupt打断正在睡眠的线程，这时sleep()会抛出中断异常 睡眠结束后的线程未必会立即得到执行 建议使用TimeUnit.second.sleep()替代sleep获得更好的可读性 2)yield() 调用yield会让当前线程从running进入runnable就绪状态，然后调度执行其他线程 具体的实现依赖于操作系统的任务调度器 4.join()1)为什么需要join下面的代码，打印r是什么？ 1234567891011121314151617181920212223242526/** * @author yhd */public class JoinDemo &#123; static int r = 0; public static void main(String[] args) throws InterruptedException &#123; test1(); &#125; private static void test1() throws InterruptedException &#123; System.out.println(&quot;开始&quot;); Thread t1 = new Thread(() -&gt; &#123; System.out.println(&quot;开始&quot;); try &#123; sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;结束&quot;); r = 10; &#125;); t1.start(); System.out.println(&quot;结果为:&#123;&#125;&quot;+r); System.out.println(&quot;结束&quot;); &#125;&#125; 分析：因为主线程和线程t1是并行执行的，t1需要1s之后才能算出r=10而主线程一开始就要打印r的结果，所以只能打印r=0 解决方法： sleep为什么不行？因为主线程无法准确捕捉到t1线程什么时候结束 用join，加在t1.start()之后2）有时效的join()等够时间1234567891011121314151617181920212223public class JoinDemo &#123; static int r1 = 0; static int r2 = 0; public static void main(String[] args) throws InterruptedException &#123; test3(); &#125; public static void test3() throws InterruptedException &#123; Thread t1 = new Thread(() -&gt; &#123; try &#123; sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; r1 = 10; &#125;); long start = System.currentTimeMillis(); t1.start();// 线程执行结束会导致 join 结束 t1.join(1500); long end = System.currentTimeMillis(); System.out.println(&quot;r1: &#123;&#125; r2: &#123;&#125; cost: &#123;&#125;&quot;+ r1 +&quot; &quot;+ r2+&quot; &quot;+ (end - start)); &#125;&#125; t1线程的结束会导致调用了t1.join(1500)的主线程结束。 没等够时间 1234567891011121314151617181920212223public class JoinDemo &#123; static int r1 = 0; static int r2 = 0; public static void main(String[] args) throws InterruptedException &#123; test3(); &#125; public static void test3() throws InterruptedException &#123; Thread t1 = new Thread(() -&gt; &#123; try &#123; sleep(2); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; r1 = 10; &#125;); long start = System.currentTimeMillis(); t1.start(); t1.join(1500); long end = System.currentTimeMillis(); System.out.println(&quot;r1: &#123;&#125; r2: &#123;&#125; cost: &#123;&#125;&quot;+ r1 +&quot; &quot;+ r2+&quot; &quot;+ (end - start)); &#125;&#125; join时间到了线程会尝试重新获取CPU执行权。 3）原理调用者轮训检查alive状态 1t1.join(); 等价于 123456synchronized (t1) &#123;// 调用者线程进入 t1 的 waitSet 等待, 直到 t1 运行结束 while (t1.isAlive()) &#123; t1.wait(0); &#125;&#125; 当前线程调用其他线程的wait(),会让当前线程处于阻塞状态，直到其他线程的wait()执行结束。join()能够让线程顺序执行的原因就是底层实际上是wait(),也就是主线程调用t1线程的join(),会让主线程阻塞，直到t1线程的join()执行结束。 5.java中操作线程的方法​ 方法名 static 功能说明 注意 start() 启动一个新线程，在新的线程运行 run 方法中的代码 start 方法只是让线程进入就绪，里面代码不一定立刻运行（CPU 的时间片还没分给它）。每个线程对象的start方法只能调用一次，如果调用了多次会出现IllegalThreadStateException。 run() 新线程启动后会调用的方法 如果在构造 Thread 对象时传递了 Runnable 参数，则线程启动后会调用 Runnable 中的 run 方法，否则默认不执行任何操作。但可以创建 Thread 的子类对象，来覆盖默认行为 join() 等待线程运行结束 join(long n) 等待线程运行结束,最多等待 n毫秒 getId() 获取线程长整型的 id 唯一 getName() 获取线程名 setName(String) 修改线程名 getPriority() 获取线程优先级 setPriority(int) 修改线程优先级 java中规定线程优先级是1~10 的整数，较大的优先级能提高该线程被 CPU 调度的机率 getState() 获取线程状态 Java 中线程状态是用 6 个 enum 表示，分别为：NEW, RUNNABLE, BLOCKED, WAITING,TIMED_WAITING, TERMINATED isInterrupted() 判断是否被打断 不会清除 打断标记 isAlive() 线程是否存活（还没有运行完毕） interrupt() 打断线程 如果被打断线程正在 sleep，wait，join 会导致被打断的线程抛出 InterruptedException，并清除 打断标记 ；如果打断的正在运行的线程，则会设置 打断标记 ；park 的线程被打断，也会设置 打断标记 interrupted() static 判断当前线程是否被打断 会清除 打断标记 currentThread() static 获取当前正在执行的线程 sleep(long n) static 让当前执行的线程休眠n毫秒，休眠时让出 cpu的时间片给其它线程 yield() static 提示线程调度器让出当前线程对CPU的使用 主要是为了测试和调试 6.LockSupport​ 用于创建锁和其他同步类的基本线程阻塞原语。​ 本质就是线程阻塞和唤醒wait notify的加强版​ park() unpark() 1）线程阻塞唤醒的三种方法①使用Object中的wait()让线程等待，使用Object的notify()唤醒线程。1234567891011121314151617181920212223242526272829303132/** * @author yhd * @description 阻塞&amp;唤醒 * @email yinhuidong2@xiaomi.com * @since 2021/6/28 */public class WaitNotify &#123; private static Object lock = new Object(); public static void main(String[] args) &#123; new Thread(()-&gt;&#123; synchronized (lock)&#123; try &#123; lock.wait(); System.out.println(Thread.currentThread().getName()+&quot;继续执行!&quot;); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;,&quot;t1&quot;).start(); new Thread(()-&gt;&#123; synchronized (lock)&#123; lock.notify(); System.out.println(Thread.currentThread().getName()+&quot;唤醒&quot;); &#125; &#125;,&quot;t2&quot;).start(); &#125;&#125; 存在的问题：必须在synchronized代码块里，顺序不能反。​ ②使用Condition的await和signal方法123456789101112131415161718192021222324252627282930313233343536373839/** * @author yhd * @description 阻塞&amp;唤醒 * @email yinhuidong2@xiaomi.com * @since 2021/6/28 */public class AwaitSignal &#123; private static Lock lock = new ReentrantLock(); private static Condition condition =lock.newCondition(); public static void main(String[] args) &#123; new Thread(()-&gt;&#123; lock.lock(); try &#123; condition.await(); &#125; catch (Exception e) &#123; &#125; finally &#123; lock.unlock(); &#125; &#125;,&quot;t1&quot;).start(); new Thread(()-&gt;&#123; lock.lock(); try &#123; condition.signal(); &#125; catch (Exception e) &#123; &#125; finally &#123; lock.unlock(); &#125; &#125;,&quot;t2&quot;).start(); &#125;&#125; 存在的问题：必须在lock代码块里面使用，使用顺序不能反。​ ③LockSupport类park()可以阻塞当前线程以及unpark(Thread)唤醒指定被阻塞的线程。LockSupport类使用了一种名为Permit（许可）的概念来做到阻塞和唤醒线程的功能，每个线程都有一个许可，permit只有1和0两个值，默认是0.​ permit默认是0，所以一开始调用park()方法，当前线程就会阻塞，直到别的线程将当前线程的permit设置为1时，park方法会被唤醒，然后将permit再次设置为0并返回。​ 1234567891011121314151617/** * @author yhd * @description 阻塞&amp;唤醒 * @email yinhuidong2@xiaomi.com * @since 2021/6/28 */public class ParkUnpark &#123; public static void main(String[] args) &#123; LockSupport.unpark(Thread.currentThread()); LockSupport.park(); LockSupport.park(); &#125;&#125; 2）总结LockSupport是一个线程阻塞工具类，所有的方法都是静态方法，可以让线程在任意位置阻塞，阻塞之后也有对应的唤醒方法。归根结底，LockSupport调用的Unsafe中的native代码。 LockSupport提供park()和unpark()方法实现阻塞线程和解除线程阻塞的过程​ LockSupport和每个使用它的线程都有一个许可(permit)关联。permit相当于1，0的开关，默认是0，调用一次unpark就加1变成1，调用一次park会消费permit，也就是将1变成o，同时park立即返回。​ 如再次调用park会变成阻塞(因为permit为零了会阻塞在这里，一直到permit变为1)，这时调用unpark会把permit置为1。​ 每个线程都有一个相关的permit, permit最多只有一个，重复调用unpark也不会积累凭证。​ 为什么可以先唤醒线程后阻塞线程? 因为unpark获得了一个凭证，之后再调用park方法，就可以名正言顺的凭证消费，故不会阻塞。 为什么唤醒两次后阻塞两次，但最终结果还会阻塞线程? 因为凭证的数量最多为1，连续调用两次unpark和调用一次unpark效果一样，只会增加一个凭证;而调用两次park却需要消费两个凭证，证不够，不能放行。 四，多线程相关概念1.并发？并行单核 cpu 下，线程实际还是 串行执行 的。操作系统中有一个组件叫做任务调度器，将 cpu 的时间片（windows下时间片最小约为 15 毫秒）分给不同的程序使用，只是由于 cpu 在线程间（时间片很短）的切换非常快，人类感觉是 同时运行的 。总结为一句话就是： 微观串行，宏观并行 ，一般会将这种 线程轮流使用 CPU 的做法称为并发， concurrent cpu 时间片1 时间片2 时间片3 时间片4 core 线程1 线程2 线程3 线程4 多核 cpu下，每个 核（core） 都可以调度运行线程，这时候线程可以是并行的。​ cpu 时间片1 时间片2 时间片3 时间片4 core1 线程1 线程1 线程3 线程3 core2 线程2 线程2 线程4 线程4 并发是同一时间应对多件事情的能力。并行是同一时间动手做多件事情的能力。 单核CPU下，多线程不能实际提高运行效率，只是为了能够在不同的任务之间切换，不同的线程轮流使用CPU，不至于一个线程总占用CPU，别的线程没法干活。 多核CPU可以并行跑多个线程，但能否提高运行效率还是要看具体情况的。 IO操作不占用CPU，只是一般拷贝文件使用的是阻塞IO，这时相当于线程虽然不用CPU，但是需要一直等待IO结束，没能充分利用线程。所以后面才有非阻塞IO和异步IO的优化。 2.线程上下文的切换因为以下一些原因导致 cpu 不再执行当前的线程，转而执行另一个线程的代码[1]。 线程的cpu时间片用完 垃圾回收 有更高优先级的线程需要运行 线程自己调用了 sleep、yield、wait、join、park、synchronized、lock 等方法。 当 Context Switch 发生时，需要由操作系统保存当前线程的状态，并恢复另一个线程的状态[2]，Java 中对应的概念就是程序计数器（Program Counter Register），它的作用是记住下一条 jvm 指令的执行地址，是线程私有的 3.主线程与守护线程默认情况下，Java 进程需要等待所有线程都运行结束，才会结束。有一种特殊的线程叫做守护线程，只要其它非守护线程运行结束了，即使守护线程的代码没有执行完，也会强制结束。 垃圾回收器线程就是一种守护线程。 Tomcat 中的 Acceptor 和 Poller 线程都是守护线程，所以 Tomcat 接收到 shutdown 命令后，不会等待它们处理完当前请求。 4.临界区的概念一个程序运行多个线程本身是没有问题的，问题出在多个线程访问共享资源。 多个线程读共享资源其实也没有问题，在多个线程对共享资源读写操作时发生指令交错，就会出现问题。 一段代码块内如果存在对共享资源的多线程读写操作，称这段代码块为临界区。 5.死锁1）死锁有这样的情况：一个线程需要同时获取多把锁，这时就容易发生死锁 t1 线程 获得 A对象 锁，接下来想获取 B对象 的锁 t2 线程 获得 B对象 锁，接下来想获取 A对象 的锁 例： 2）定位死锁检测死锁可以使用 jconsole工具，或者使用 jps 定位进程 id，再用 jstack 定位死锁。 避免死锁要注意加锁顺序。 另外如果由于某个线程进入了死循环，导致其它线程一直等待，对于这种情况 linux 下可以通过 top 先定位到CPU 占用高的 Java 进程，再利用 top -Hp 进程id来定位是哪个线程，最后再用 jstack 排查。 6.活锁活锁出现在两个线程互相改变对方的结束条件，最后谁也无法结束。 7.final原理1）设置 final 变量的原理字节码 发现 final 变量的赋值也会通过 putfield 指令来完成，同样在这条指令之后也会加入写屏障，保证在其它线程读到它的值时不会出现为 0 的情况。 Context Switch 频繁发生会影响性能. ↩︎ 状态包括程序计数器、虚拟机栈中每个栈帧的信息，如局部变量、操作数栈、返回地址等. ↩︎ 8.可重入锁可重入锁又名递归锁 是指在同一个线程在外层方法获取锁的时候，再进去该线程的内层方法会自动获取锁（前提：同一个锁对象），不会因为之前已经获取过还没释放而阻塞。​ java中的ReentrantLock和Synchronized都是可重入锁，可重入锁的一个优点是可一定程度避免死锁。​ 12345678910111213141516171819202122public class Demo2 &#123; private static Object lock = new Object(); public static void main(String[] args) &#123; test(); &#125; public static void test() &#123; new Thread(() -&gt; &#123; synchronized (lock) &#123; System.out.println(Thread.currentThread().getName() + &quot;外&quot;); synchronized (lock) &#123; System.out.println(Thread.currentThread().getName() + &quot;中&quot;); synchronized (lock) &#123; System.out.println(Thread.currentThread().getName() + &quot;内&quot;); &#125; &#125; &#125; &#125;).start(); &#125;&#125; 五，线程通信1.两个线程交替打印题目： i=0,a:i++,b:i–,交替打印10次​ 1.1 使用synchronized12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/** * @author 二十 * @since 2021/8/31 9:01 下午 */public class DemoA &#123; static CountDownLatch count=new CountDownLatch(2); public static void main(String[] args)throws Exception &#123; Lock lock = new Lock(); new Thread(()-&gt;&#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; lock.add(); &#125;catch (Exception e)&#123; &#125; &#125; count.countDown(); &#125;,&quot;A&quot;).start(); new Thread(()-&gt;&#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; lock.del(); &#125;catch (Exception e)&#123; &#125; &#125; count.countDown(); &#125;,&quot;B&quot;).start(); count.await(); &#125; private static class Lock&#123; static int num = 0; public synchronized void add()throws Exception&#123; if (num!=0)&#123; this.wait(); &#125; System.out.println(Thread.currentThread().getName()+&quot;num = &quot; + ++num); this.notify(); &#125; public synchronized void del()throws Exception&#123; if (num!=1)&#123; this.wait(); &#125; System.out.println(Thread.currentThread().getName()+&quot;num = &quot; + --num); this.notify(); &#125; &#125;&#125; 1.2 两个线程可以正常执行，现在增加到四个123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475/** * @author 二十 * @since 2021/8/31 9:24 下午 */public class DemoB &#123; static CountDownLatch count=new CountDownLatch(4); public static void main(String[] args)throws Exception &#123; Lock lock = new Lock(); new Thread(()-&gt;&#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; lock.add(); &#125;catch (Exception e)&#123; &#125; &#125; count.countDown(); &#125;,&quot;A&quot;).start(); new Thread(()-&gt;&#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; lock.del(); &#125;catch (Exception e)&#123; &#125; &#125; count.countDown(); &#125;,&quot;B&quot;).start(); new Thread(()-&gt;&#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; lock.add(); &#125;catch (Exception e)&#123; &#125; &#125; count.countDown(); &#125;,&quot;C&quot;).start(); new Thread(()-&gt;&#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; lock.del(); &#125;catch (Exception e)&#123; &#125; &#125; count.countDown(); &#125;,&quot;D&quot;).start(); count.await(); &#125; private static class Lock&#123; static int num = 0; public synchronized void add()throws Exception&#123; if (num!=0)&#123; this.wait(); &#125; System.out.println(Thread.currentThread().getName()+&quot;num = &quot; + ++num); this.notify(); &#125; public synchronized void del()throws Exception&#123; if (num!=1)&#123; this.wait(); &#125; System.out.println(Thread.currentThread().getName()+&quot;num = &quot; + --num); this.notify(); &#125; &#125;&#125; 1.3 线程间调用化定制通信查看jdkAPI wait（）；​ 注意：判断一定要while循环判断，不能用if，防止多线程虚假唤醒。​ 1.4 使用lock12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970/** * @author 二十 * @since 2021/8/31 9:30 下午 */public class DemoC &#123; private static CountDownLatch count = new CountDownLatch(2); public static void main(String[] args)throws Exception &#123; Data data = new Data(); new Thread(()-&gt;&#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; data.add(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; count.countDown(); &#125;,&quot;A&quot;).start(); new Thread(()-&gt;&#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; data.del(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; count.countDown(); &#125;,&quot;B&quot;).start(); count.await(); &#125; private static class Data &#123; static volatile int num =0; static ReentrantLock lock = new ReentrantLock(); static Condition c =lock.newCondition(); public void add()throws Exception&#123; lock.lock(); try &#123; while (num!=0)&#123; c.await(); &#125; System.out.println(Thread.currentThread().getName()+&quot; num = &quot; + ++num); c.signal(); &#125;finally &#123; lock.unlock(); &#125; &#125; public void del()throws Exception&#123; lock.lock(); try &#123; while (num!=1)&#123; c.await(); &#125; System.out.println(Thread.currentThread().getName()+&quot; num = &quot; + --num); c.signal(); &#125;finally &#123; lock.unlock(); &#125; &#125; &#125;&#125; 2.多个线程按顺序打印题目：​ 多线程之间按照顺序调用，实现A-B-C​ 三个线程启动，要求如下：​ AA打印5次，BB打印10次，CC打印15次​ 接着循环10轮 分析：​ 有顺序通知，需要有标识位 有一个锁，lock，3把钥匙condition 判断标识位 输出线程名+第几次+第几轮 修改标识位，通知下一个 ​ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100/** * @author 二十 * @since 2021/8/31 9:48 下午 */public class DemoD &#123; static CountDownLatch c = new CountDownLatch(3); public static void main(String[] args) throws Exception &#123; Data data = new Data(); for (int i = 0; i &lt; 10; i++) &#123; new Thread(() -&gt; &#123; data.p1(); c.countDown(); &#125;, &quot;A&quot;).start(); new Thread(() -&gt; &#123; data.p2(); c.countDown(); &#125;, &quot;B&quot;).start(); new Thread(() -&gt; &#123; data.p3(); c.countDown(); &#125;, &quot;C&quot;).start(); &#125; c.await(); &#125; private static class Data &#123; static int flag = 0; ReentrantLock lock = new ReentrantLock(); Condition c1 = lock.newCondition(); Condition c2 = lock.newCondition(); Condition c3 = lock.newCondition(); public void printf() &#123; System.out.println(Thread.currentThread().getName() + &quot; flag&quot; + flag); &#125; public void p1() &#123; lock.lock(); try &#123; while (flag != 0) &#123; try &#123; c1.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; for (int i = 0; i &lt; 5; i++) &#123; printf(); &#125; flag++; c2.signal(); &#125; finally &#123; lock.unlock(); &#125; &#125; public void p2() &#123; lock.lock(); try &#123; while (flag != 1) &#123; try &#123; c2.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; for (int i = 0; i &lt; 10; i++) &#123; printf(); &#125; flag++; c3.signal(); &#125; finally &#123; lock.unlock(); &#125; &#125; public void p3() &#123; lock.lock(); try &#123; while (flag != 2) &#123; try &#123; c3.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; for (int i = 0; i &lt; 15; i++) &#123; printf(); &#125; flag = 0; c1.signal(); &#125; finally &#123; lock.unlock(); &#125; &#125; &#125;&#125; 六，线程与集合1.如何证明集合是线程不安全的123456789101112131415161718 public static void main(String[] args) &#123; List&lt;String&gt; list = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 30; i++) &#123; new Thread(() -&gt; &#123; list.add(UUID.randomUUID().toString().substring(8)); System.out.println(list); &#125;, String.valueOf(i)).start(); &#125; &#125;//java.util.ConcurrentModificationException 2.如何让集合变的安全2.1 调用工具类12List&lt;String&gt; list= Arrays.asList(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;);List&lt;String&gt; list2= Collections.synchronizedList(list); 2.2 使用JUC copyOnWriteArrayList ConcurrentLinkedQueue ConcurrentHashMap ​ 1234567891011121314151617public static void main(String[] args) &#123; CopyOnWriteArrayList&lt;String&gt; list = new CopyOnWriteArrayList&lt;&gt;(); for (int i = 0; i &lt; 30; i++) &#123; new Thread(()-&gt;&#123; list.add(UUID.randomUUID().toString().substring(8)); System.out.println(list); &#125;,String.valueOf(i)).start(); &#125;&#125; 源码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576/** * @author 二十 * @since 2021/8/31 10:44 下午 */public class DemoF &#123; public static void main(String[] args) &#123; Printers printers = new Printers(); new Thread(() -&gt; &#123; for (int i = 1; i &lt;= 26; i++) printers.printNum(); &#125;, &quot;数字打印线程&quot;).start(); new Thread(() -&gt; &#123; for (int i = 1; i &lt;= 26; i++) printers.printLetter(i + 64); &#125;, &quot;字母打印线程&quot;).start(); &#125; private static class Printers &#123; private int num = 1; private int a = 0; private ReentrantLock lock = new ReentrantLock(); private Condition cd1 = lock.newCondition(); private Condition cd2 = lock.newCondition(); public void printNum() &#123; try &#123; lock.lock(); while (num != 1) cd1.await(); for (int i = 0; i &lt; 2; i++) &#123; System.out.println(Thread.currentThread().getName() + &quot;打印了：&quot; + ++a); &#125; num++; cd2.signal(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; public void printLetter(int aa) &#123; try &#123; lock.lock(); while (num != 2) cd2.await(); System.out.println(Thread.currentThread().getName() + &quot;打印了：&quot; + (char) aa); num--; cd1.signal(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; &#125;&#125;","categories":[{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"高并发核心模式之异步回调","slug":"JUC/同步队列","date":"2022-01-11T11:05:09.782Z","updated":"2022-01-11T11:22:06.594Z","comments":true,"path":"2022/01/11/JUC/同步队列/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/JUC/%E5%90%8C%E6%AD%A5%E9%98%9F%E5%88%97/","excerpt":"","text":"二，源码1.内部类和成员变量12345678910111213141516171819202122232425262728293031323334353637abstract static class Transferer&lt;E&gt; &#123; /** * * @param e 可以为null，null的时候表示这个是一个request类型的请求， * 如果不是null，说明当前请求是一个data类型的请求 * @param timed true 表示指定了超时时间， false 表示不支持超时，直到匹配到为止 * @param nanos 超时限制 单位：纳秒 * @return E 如果当前请求是一个request类型的请求 * 返回值！=null 表示匹配成功 * 返回值==null 表示超时或者被中断 * 如果当前请求是data类型的请求， * 返回值！=null 表示匹配成功，返回当前线程put的数据 * 返回值 ==null 表示data类型的请求超时 或者被中断 */ abstract E transfer(E e, boolean timed, long nanos);&#125;/** * 表示获取当前系统所拥有的的cpu核心数 */static final int NCPUS = Runtime.getRuntime().availableProcessors();/** * 表示指定了超时时间的话，最大的自旋次数 * 为什么需要自旋操作？因为线程挂起唤醒站在cpu角度，是比较耗费资源的， * 涉及到用户态和内核太的切换浪费性能，自旋期间线程会一直检查自己的 * 状态是否被匹配到，如果自旋期间被匹配到，直接返回，如果 * 未被匹配到，达到某一指标后，还是会挂起。 */static final int maxTimedSpins = (NCPUS &lt; 2) ? 0 : 32;/** * 表示没有指定超时限制的时候，线程等待匹配时，自旋的次数。 */static final int maxUntimedSpins = maxTimedSpins * 16;/** * 如果请求是指定超时限制的话，如果超时参数小于1000纳秒的时候，禁止挂起 */static final long spinForTimeoutThreshold = 1000L; 2.非公平模式TODO","categories":[{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"SimpleDateFormat在多线程下存在并发安全问题","slug":"JUC/SimpleDateFormat类的线程安全问题","date":"2022-01-11T11:05:01.751Z","updated":"2022-01-11T11:32:38.766Z","comments":true,"path":"2022/01/11/JUC/SimpleDateFormat类的线程安全问题/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/JUC/SimpleDateFormat%E7%B1%BB%E7%9A%84%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E9%97%AE%E9%A2%98/","excerpt":"","text":"SimpleDateFormat在多线程下存在并发安全问题。​ 1.重现SimpleDateFormat类的线程安全问题12345678910111213141516171819202122232425262728293031323334353637383940414243/** * * 重现SimpleDateFormat类的线程安全问题 * * @author 二十 * @since 2021/9/12 10:05 下午 */public class SdfTest &#123; /**执行总次数*/ private static final int EXECUTE_COUNT=1000; /**并发线程数*/ private static final int THREAD_COUNT=100; private static SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;); @Test public void test()throws Exception&#123; final Semaphore semaphore =new Semaphore(THREAD_COUNT); final CountDownLatch cdl = new CountDownLatch(EXECUTE_COUNT); ExecutorService executor = Executors.newCachedThreadPool(); for (int i = 0; i &lt; EXECUTE_COUNT; i++) &#123; executor.execute(()-&gt;&#123; try &#123; semaphore.acquire(); sdf.parse(&quot;2021-09-15&quot;); &#125;catch (Exception e)&#123; System.out.println(Thread.currentThread().getName()+&quot;格式化时间失败！&quot;); &#125;finally &#123; semaphore.release(); cdl.countDown(); &#125; &#125;); &#125; cdl.await(); executor.shutdown(); &#125;&#125; 说明：在高并发下使用SimpleDateFormat类格式化日期的时候抛出了异常，SimpleDateFormat类不是线程安全的！为什么SimpleDateFormat不是线程安全的？ 2.SimpleDateFormat为什么不是线程安全的？12345678910public Date parse(String source) throws ParseException &#123; ParsePosition pos = new ParsePosition(0); //调用了parse方法，最终的实现类在SimpleDateFormat类中 Date result = parse(source, pos); if (pos.index == 0) throw new ParseException(&quot;Unparseable date: \\&quot;&quot; + source + &quot;\\&quot;&quot; , pos.errorIndex); return result;&#125; 点进去parse()，这是一个抽象的方法。​ 1public abstract Date parse(String source, ParsePosition pos); 最终的实现类还是在SimpleDateFormat类中。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105public Date parse(String text, ParsePosition pos)&#123; checkNegativeNumberExpression(); int start = pos.index; int oldStart = start; int textLength = text.length(); boolean[] ambiguousYear = &#123;false&#125;; CalendarBuilder calb = new CalendarBuilder(); for (int i = 0; i &lt; compiledPattern.length; ) &#123; int tag = compiledPattern[i] &gt;&gt;&gt; 8; int count = compiledPattern[i++] &amp; 0xff; if (count == 255) &#123; count = compiledPattern[i++] &lt;&lt; 16; count |= compiledPattern[i++]; &#125; switch (tag) &#123; case TAG_QUOTE_ASCII_CHAR: if (start &gt;= textLength || text.charAt(start) != (char)count) &#123; //破坏了线程的安全性 pos.index = oldStart; //破坏了线程的安全性 pos.errorIndex = start; return null; &#125; start++; break; case TAG_QUOTE_CHARS: while (count-- &gt; 0) &#123; if (start &gt;= textLength || text.charAt(start) != compiledPattern[i++]) &#123; pos.index = oldStart;//破坏了线程的安全性 pos.errorIndex = start;//破坏了线程的安全性 return null; &#125; start++; &#125; break; default: boolean obeyCount = false; boolean useFollowingMinusSignAsDelimiter = false; if (i &lt; compiledPattern.length) &#123; int nextTag = compiledPattern[i] &gt;&gt;&gt; 8; if (!(nextTag == TAG_QUOTE_ASCII_CHAR || nextTag == TAG_QUOTE_CHARS)) &#123; obeyCount = true; &#125; if (hasFollowingMinusSign &amp;&amp; (nextTag == TAG_QUOTE_ASCII_CHAR || nextTag == TAG_QUOTE_CHARS)) &#123; int c; if (nextTag == TAG_QUOTE_ASCII_CHAR) &#123; c = compiledPattern[i] &amp; 0xff; &#125; else &#123; c = compiledPattern[i+1]; &#125; if (c == minusSign) &#123; useFollowingMinusSignAsDelimiter = true; &#125; &#125; &#125; start = subParse(text, start, tag, count, obeyCount, ambiguousYear, pos, useFollowingMinusSignAsDelimiter, calb); if (start &lt; 0) &#123; //破坏了线程的安全性 pos.index = oldStart; return null; &#125; &#125; &#125; //破坏了线程的安全性 pos.index = start; Date parsedDate; try &#123; parsedDate = calb.establish(calendar).getTime(); if (ambiguousYear[0]) &#123; if (parsedDate.before(defaultCenturyStart)) &#123; parsedDate = calb.addYear(100).establish(calendar).getTime(); &#125; &#125; &#125; catch (IllegalArgumentException e) &#123; //破坏了线程的安全性 pos.errorIndex = start; //破坏了线程的安全性 pos.index = oldStart; return null; &#125; return parsedDate;&#125; 通过对SimpleDateFormat类中的parse()进行分析可以得知：parse()中存在几处为ParsePosition类中的索引赋值的操作。​ 一旦将SimpleDateFormat类定义成全局静态变量，那么SimpleDateFormat类在多线程之间是共享的，这就会导致ParsePosition类在多线程之间共享。​ 在高并发场景下，一个线程对ParsePosition类中的索引进行修改，一定会影响到其他线程对ParsePosition类中索引的读。这就造成了线程的安全问题。​ 那么在确定了SimpleDateFormat线程不安全的原因以后，如何来解决这个问题。​ 3.SimpleDateFormat类线程安全的解决3.1 局部变量法12345678910111213141516171819202122232425262728293031323334public class SdfTest &#123; /**执行总次数*/ private static final int EXECUTE_COUNT=1000; /**并发线程数*/ private static final int THREAD_COUNT=100; @Test public void test()throws Exception&#123; SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;); final Semaphore semaphore =new Semaphore(THREAD_COUNT); final CountDownLatch cdl = new CountDownLatch(EXECUTE_COUNT); ExecutorService executor = Executors.newCachedThreadPool(); for (int i = 0; i &lt; EXECUTE_COUNT; i++) &#123; executor.execute(()-&gt;&#123; try &#123; semaphore.acquire(); sdf.parse(&quot;2021-09-15&quot;); &#125;catch (Exception e)&#123; System.out.println(Thread.currentThread().getName()+&quot;格式化时间失败！&quot;); &#125;finally &#123; semaphore.release(); cdl.countDown(); &#125; &#125;); &#125; cdl.await(); executor.shutdown(); &#125;&#125; 这种方式在高并发场景下会创建大量的SimpleDateFormat对象，影响程序的性能，所以，这种方式在实际生产环境不太推荐。​ 3.2 synchronized1234567891011121314151617181920212223242526272829303132333435public class SdfTest &#123; /**执行总次数*/ private static final int EXECUTE_COUNT=1000; /**并发线程数*/ private static final int THREAD_COUNT=100; private static SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;);; @Test public void test()throws Exception&#123; final Semaphore semaphore =new Semaphore(THREAD_COUNT); final CountDownLatch cdl = new CountDownLatch(EXECUTE_COUNT); ExecutorService executor = Executors.newCachedThreadPool(); for (int i = 0; i &lt; EXECUTE_COUNT; i++) &#123; executor.execute(()-&gt;&#123; try &#123; semaphore.acquire(); synchronized (sdf)&#123; sdf.parse(&quot;2021-09-15&quot;); &#125; &#125;catch (Exception e)&#123; System.out.println(Thread.currentThread().getName()+&quot;格式化时间失败！&quot;); &#125;finally &#123; semaphore.release(); cdl.countDown(); &#125; &#125;); &#125; cdl.await(); executor.shutdown(); &#125;&#125; 这种方式解决了他的线程安全问题，但是程序在执行的过程中，为SimpleDateFormat类对象加了synchronized锁，导致在同一时刻只能由一个线程执行格式化时间的方法。此时会影响程序的性能，在要求高并发的生产环境下，此种方式也是不太推荐使用的。​ 3.3 Lock锁方式1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class SdfTest &#123; /** * 执行总次数 */ private static final int EXECUTE_COUNT = 1000; /** * 并发线程数 */ private static final int THREAD_COUNT = 100; private static SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;); private static Lock lock = new ReentrantLock(); @Test public void test() throws Exception &#123; final Semaphore semaphore = new Semaphore(THREAD_COUNT); final CountDownLatch cdl = new CountDownLatch(EXECUTE_COUNT); ExecutorService executor = Executors.newCachedThreadPool(); for (int i = 0; i &lt; EXECUTE_COUNT; i++) &#123; executor.execute(() -&gt; &#123; try &#123; semaphore.acquire(); lock.lock(); try &#123; sdf.parse(&quot;2021-09-15&quot;); &#125; finally &#123; lock.unlock(); &#125; &#125; catch (Exception e) &#123; System.out.println(Thread.currentThread().getName() + &quot;格式化时间失败！&quot;); &#125; finally &#123; semaphore.release(); cdl.countDown(); &#125; &#125;); &#125; cdl.await(); executor.shutdown(); &#125;&#125; 通过代码得知：首先定义了Lock类型的全局静态变量作为加锁和释放锁的句柄。然后再SimpleDateFormat.parse()代码执行之前加锁。​ 这里需要注意的一点是：为了防止程序抛出异常而导致锁不能被释放，一定要将释放锁的操作放到finally代码块。​ 此种方式在并发下同样影响性能，不太推荐在高并发生产中使用。​ 3.4ThreadLocal方式123456789101112131415161718192021222324252627282930313233343536373839public class SdfTest &#123; /** * 执行总次数 */ private static final int EXECUTE_COUNT = 1000; /** * 并发线程数 */ private static final int THREAD_COUNT = 100; private static ThreadLocal&lt;DateFormat&gt; threadLocal = ThreadLocal.withInitial(() -&gt; new SimpleDateFormat(&quot;yyyy-MM-dd&quot;)); @Test public void test() throws Exception &#123; final Semaphore semaphore = new Semaphore(THREAD_COUNT); final CountDownLatch cdl = new CountDownLatch(EXECUTE_COUNT); ExecutorService executor = Executors.newCachedThreadPool(); for (int i = 0; i &lt; EXECUTE_COUNT; i++) &#123; executor.execute(() -&gt; &#123; try &#123; semaphore.acquire(); threadLocal.get().parse(&quot;2021-09-15&quot;); &#125; catch (Exception e) &#123; System.out.println(Thread.currentThread().getName() + &quot;格式化时间失败！&quot;); &#125; finally &#123; threadLocal.remove(); semaphore.release(); cdl.countDown(); &#125; &#125;); &#125; cdl.await(); executor.shutdown(); &#125;&#125; 使用ThreadLocal将每个线程使用的SimpleDateFormat副本保存在ThreadLocal中，各个线程在使用时互不干扰，从而解决了线程安全的问题。​ 此种方式运行效率比较高，推荐在高并发场景下使用。​ 3.5 DateTimeFormatterjdk8线程安全的时间日期API。​ ​","categories":[{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"ScheduledThreadPoolExecutor与Timer的区别","slug":"JUC/ScheduledThreadPoolExecutor与Timer的区别","date":"2022-01-11T11:04:52.884Z","updated":"2022-01-11T11:33:09.480Z","comments":true,"path":"2022/01/11/JUC/ScheduledThreadPoolExecutor与Timer的区别/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/JUC/ScheduledThreadPoolExecutor%E4%B8%8ETimer%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"JDK1.5开始提供ScheduledThreadPoolExecutor类，ScheduledThreadPoolExecutor类继承ThreadPoolExecutor类重用线程池实现了任务的周期性调度功能。在IDK1.5之前，实现任务的周期性调度主要使用的是Timer类和TimerTask类。​ 1.1线程角度​ Timer是单线程模式，如果某个TimerTask任务的执行时间比较久，会影响到其他任务的调度执行。 ScheduledThreadPoolExecutor是多线程模式，并且重用线程池，某个ScheduledFutureTask任务执行的时间比较久，不会影响到其他任务的调度执行。 ​ 1.2系统时间敏感度 Timer调度是基于操作系统的绝对时间的，对操作系统的时间敏感，一旦操作系统的时间改变，则Timer的调度不再精确。 ScheduledThreadPoolExecutor调度是基于相对时间的，不受操作系统时间改变的影响。 ​ 1.3是否捕获异常​ Timer不会捕获TimerTask抛出的异常，加上Timer又是单线程的。一旦某个调度任务出现异常则整个线程就会终止，其他需要调度的任务也不再执行。 ScheduledThreadPoolExecutor基于线程池来实现调度功能，某个任务抛出异常后，其他任务仍能正常执行。 ​ 1.4任务是否具备优先级​ Timer中执行的TimerTask任务整体上没有优先级的概念，只是按照系统的绝对时间来执行任务。 ScheduledThreadPoolExecutor中执行的ScheduledFutureTask类实现了iavalang.Comparable接口和java.utilconcurrentDelayed接口，这也就说明了ScheduledFutureTask类中实现了两个非常重要的方法，一个是javalangComparable接口的compareTo方法，一个是java.util.concurrentDelayed接口的getDelay方法。在ScheduledFutureTask类中compareTo方法方法实现了任务的比较，距离下次执行的时间间隔短的任务会排在前面，也就是说，距离下次执行的时间间隔短的任务的优先级比较高。而getDelay方法则能够返回距离下次任务执行的时间间隔。 ​ 1.5是否支持对任务排序 Timer不支持对任务的排序。 ScheduledThreadPoolExecutor类中定义了一个静态内部类DelayedWorkQueue,DelayedWorkQueue类本质上是一个有序队列，为需要调度的每个任务按照距离下次执行时间间隔的大小来排序 ​ 1.6能否获取返回的结果​ Timer中执行的TimerTask类只是实现了iavaangRunnable接口，无法从TimerTask中获取返回的结果。 ScheduledThreadPoolExecutor中执行的ScheduledFutureTask类继承了FutureTask类，能够通过遍Future来获取返回的结果。 ​ 通过以上对ScheduledThreadPoolExecutor类和Timer类的对比，相信在JDK1.5之后，就没有使用Timer来实现定时任务调度的必要了。​ ​ ​ ​","categories":[{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"线程间传递上下文信息","slug":"JUC/线程间传递上下文信息","date":"2022-01-11T11:04:43.563Z","updated":"2022-01-11T11:24:01.784Z","comments":true,"path":"2022/01/11/JUC/线程间传递上下文信息/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/JUC/%E7%BA%BF%E7%A8%8B%E9%97%B4%E4%BC%A0%E9%80%92%E4%B8%8A%E4%B8%8B%E6%96%87%E4%BF%A1%E6%81%AF/","excerpt":"","text":"1.复制父线程变量到子线程只有父线程里面创建一个线程的时候才会去调用init才会有inheritableThreadLocals的拷贝动作。​ 1234567891011121314151617181920212223242526272829303132333435public class InheriableThreadLocalTest &#123; private static ThreadLocal&lt;Integer&gt; context = new InheritableThreadLocal&lt;&gt;(); private static ExecutorService pool = Executors.newFixedThreadPool(2); static class MainThread extends Thread &#123; private int index; public MainThread(int index) &#123; this.index = index; &#125; @Override public void run() &#123; context.set(index); //用线程池的线程就会出现传递信息错乱 //pool.execute(() -&gt; System.out.println(Thread.currentThread().getName()+&quot;:&quot; + context.get())); new Thread(()-&gt;System.out.println(Thread.currentThread().getName()+&quot;:&quot; + context.get())).start(); &#125; &#125; /** * 当使用线程池来运行我们的子线程的任务的时候， * 采用InheriableThreadLocal是无法解决变量传递的问题的 */ public static void main(String[] args) &#123; for (int i = 0; i &lt; 10; i++) &#123; new MainThread(i).start(); &#125; pool.shutdown(); &#125;&#125; ​ 注意：要求必须是new的线程，不能使用线程池的线程，线程复用会导致信息传递出错。 2.解决无法使用线程池的问题12345678910111213141516171819202122232425262728293031/** * @author 二十 * @since 2021/9/14 11:15 上午 */public class TransmittableTest &#123; private static TransmittableThreadLocal&lt;String&gt; context = new TransmittableThreadLocal&lt;&gt;(); private static ExecutorService pool = TtlExecutors.getTtlExecutorService(Executors.newFixedThreadPool(5)); static class MainThread extends Thread &#123; private int index; public MainThread(int index) &#123; this.index = index; &#125; @Override public void run() &#123; context.set(String.valueOf(index)); pool.execute(() -&gt; System.out.println(Thread.currentThread().getName()+&quot;:&quot; + context.get())); // new Thread(()-&gt;System.out.println(Thread.currentThread().getName()+&quot;:&quot; + context.get())).start(); &#125; &#125; public static void main(String[] args) &#123; for (int i = 0; i &lt; 10; i++) &#123; new MainThread(i).start(); &#125; pool.shutdown(); &#125;&#125; 3.源码TransmittableThreadLocal继承了InheritableThreadLocal，先点一下InheritableThreadLocal瞅瞅。​ 1234567891011121314151617181920public class InheritableThreadLocal&lt;T&gt; extends ThreadLocal&lt;T&gt; &#123; /** * 新建线程时，如果当前inheritableThreadLocals非空，则会获取当前inheritableThreadLocals传递给新线程 */ protected T childValue(T parentValue) &#123; return parentValue; &#125; /** * InheritableThreadLocal变量的set/get/remove操作都是在inheritableThreadLocals上 */ ThreadLocalMap getMap(Thread t) &#123; return t.inheritableThreadLocals; &#125; /** * 创建inheritableThreadLocals */ void createMap(Thread t, T firstValue) &#123; t.inheritableThreadLocals = new ThreadLocalMap(this, firstValue); &#125;&#125; 对TL进行了一层包装和增强。​ Thread类中有两个ThreadLocal相关的ThreadLocalMap属性，如下： 12ThreadLocal.ThreadLocalMap threadLocals：ThreadLocal变量使用ThreadLocal.ThreadLocalMap inheritableThreadLocals：InheritableThreadLocal变量使用 新建线程时，将当前线程的inheritableThreadLocals传递给新线程，这里的传递是对InheritableThreadLocal变量的数据做浅拷贝（引用复制），这样新线程可以使用同一个InheritableThreadLocal变量查看上一个线程的数据。​ 下面以TtlRunnable.get()为起点分析TTL的设计实现，TtlRunnable.get源码如下（TtlRunnable.get流程对应的初始化时capture操作，保存快照。TtlCallable和TtlRunnable流程类似）：​ 12345678910111213141516171819202122232425public static TtlRunnable get(@Nullable Runnable runnable) &#123; return get(runnable, false, false);&#125;public static TtlRunnable get(@Nullable Runnable runnable, boolean releaseTtlValueReferenceAfterRun, boolean idempotent) &#123; if (runnable instanceof TtlEnhanced) &#123; // 幂等时直接返回，否则执行会产生问题，直接抛异常 if (idempotent) return (TtlRunnable) runnable; else throw new IllegalStateException(&quot;Already TtlRunnable!&quot;); &#125; return new TtlRunnable(runnable, releaseTtlValueReferenceAfterRun);&#125;private TtlRunnable(@Nonnull Runnable runnable, boolean releaseTtlValueReferenceAfterRun) &#123; this.capturedRef = new AtomicReference&lt;Object&gt;(capture()); this.runnable = runnable; this.releaseTtlValueReferenceAfterRun = releaseTtlValueReferenceAfterRun;&#125;public static Object capture() &#123; Map&lt;TransmittableThreadLocal&lt;?&gt;, Object&gt; captured = new HashMap&lt;TransmittableThreadLocal&lt;?&gt;, Object&gt;(); // 从holder获取所有threadLocal，存到captured，这里相当于对当前线程holder做一个快照保存 // 到TtlRunnable实例属性中，在执行TtlRunnable时进行回放 for (TransmittableThreadLocal&lt;?&gt; threadLocal : holder.get().keySet()) &#123; captured.put(threadLocal, threadLocal.copyValue()); &#125; return captured;&#125; 在新建TtlRunnable过程中，会保存下TransmittableThreadLocal.holder到captured，记录到TtlRunnable实例中的capturedRef字段，TransmittableThreadLocal.holder类型是： 123456789101112131415// Note about holder:// 1. The value of holder is type Map&lt;TransmittableThreadLocal&lt;?&gt;, ?&gt; (WeakHashMap implementation),// but it is used as *set*. 因为没有WeakSet的原因// 2. WeakHashMap support null value.private static InheritableThreadLocal&lt;Map&lt;TransmittableThreadLocal&lt;?&gt;, ?&gt;&gt; holder = new InheritableThreadLocal&lt;Map&lt;TransmittableThreadLocal&lt;?&gt;, ?&gt;&gt;() &#123; @Override protected Map&lt;TransmittableThreadLocal&lt;?&gt;, ?&gt; initialValue() &#123; return new WeakHashMap&lt;TransmittableThreadLocal&lt;?&gt;, Object&gt;(); &#125; @Override protected Map&lt;TransmittableThreadLocal&lt;?&gt;, ?&gt; childValue(Map&lt;TransmittableThreadLocal&lt;?&gt;, ?&gt; parentValue) &#123; return new WeakHashMap&lt;TransmittableThreadLocal&lt;?&gt;, Object&gt;(parentValue); &#125; &#125;; 从上面代码我们知道初始化TtlRunnable时已经将TransmittableThreadLocal保存下来了，那么什么时候应用到当前线程ThreadLocal中呢，这是就需要看下TtlRunnable.run方法： 123456789101112131415public void run() &#123; Object captured = capturedRef.get(); // captured不应该为空，releaseTtlValueReferenceAfterRun为true时设置capturedRef为null，防止当前Runnable重复执行 if (captured == null || releaseTtlValueReferenceAfterRun &amp;&amp; !capturedRef.compareAndSet(captured, null)) &#123; throw new IllegalStateException(&quot;TTL value reference is released after run!&quot;); &#125; // captured进行回放，应用到当前线程中 Object backup = replay(captured); try &#123; runnable.run(); &#125; finally &#123; restore(backup); &#125;&#125; 注意，TTL中的replay操作是以captured为当前inheritableThreadLocals的（处理逻辑是在TtlRunable run时，会以TtlRunnable.get时间点获取的captured（类似TTL快照）为准，holder中不在captured的先移除，在的会被替换）。回放captured和执行完runnable.run之后，再restore恢复到原来inheritableThreadLocals的状态。 4.自己写一个轻量级的123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168/** * @author 二十 * @since 2021/9/14 3:13 下午 */public class EsThreadLocal&lt;T&gt; extends InheritableThreadLocal&lt;T&gt; &#123; private static InheritableThreadLocal&lt;WeakHashMap&lt;EsThreadLocal&lt;Object&gt;, ?&gt;&gt; holder = new InheritableThreadLocal&lt;WeakHashMap&lt;EsThreadLocal&lt;Object&gt;, ?&gt;&gt;() &#123; @Override protected WeakHashMap&lt;EsThreadLocal&lt;Object&gt;, ?&gt; childValue(WeakHashMap&lt;EsThreadLocal&lt;Object&gt;, ?&gt; parentValue) &#123; return new WeakHashMap&lt;&gt;(parentValue); &#125; @Override protected WeakHashMap&lt;EsThreadLocal&lt;Object&gt;, ?&gt; initialValue() &#123; return new WeakHashMap&lt;&gt;(); &#125; &#125;; @Override public int hashCode() &#123; return super.hashCode(); &#125; @Override public boolean equals(Object obj) &#123; return super.equals(obj); &#125; @Override public T get() &#123; T value = super.get(); if (null != value) addToHolder(); return value; &#125; @Override public void set(T value) &#123; if (null == value) &#123; removeFromHolder(); super.remove(); &#125; else &#123; super.set(value); addToHolder(); &#125; &#125; private void removeFromHolder() &#123; holder.get().remove(this); &#125; private void addToHolder() &#123; if (!holder.get().containsKey(this)) holder.get().put((EsThreadLocal&lt;Object&gt;) this, null); &#125; static class SnapShot &#123; final WeakHashMap&lt;EsThreadLocal&lt;Object&gt;, Object&gt; ctlValue; private SnapShot(WeakHashMap&lt;EsThreadLocal&lt;Object&gt;, Object&gt; ctlValue) &#123; this.ctlValue = ctlValue; &#125; &#125; static class Transmitter &#123; public static SnapShot capture() &#123; return new SnapShot(captureCtlValues()); &#125; private static WeakHashMap&lt;EsThreadLocal&lt;Object&gt;, Object&gt; captureCtlValues() &#123; return holder.get().keySet().stream().collect(Collectors.toMap(ctlItem -&gt; ctlItem, EsThreadLocal::get, (a, b) -&gt; b, WeakHashMap::new)); &#125; public static SnapShot replay(SnapShot snapShot) &#123; WeakHashMap&lt;EsThreadLocal&lt;Object&gt;, Object&gt; capture = snapShot.ctlValue; WeakHashMap&lt;EsThreadLocal&lt;Object&gt;, Object&gt; backValue = new WeakHashMap&lt;&gt;(); /* * 从holder中获取当前线程持有的threadLocal的Map,进行迭代保存 */ Iterator&lt;EsThreadLocal&lt;Object&gt;&gt; iterator = holder.get().keySet().iterator(); while (iterator.hasNext()) &#123; EsThreadLocal&lt;Object&gt; threadLocal = iterator.next(); backValue.put(threadLocal, threadLocal.get()); if (!capture.containsKey(threadLocal)) &#123; iterator.remove(); threadLocal.remove(); &#125; &#125; /* 设置上capture */ setThreadLocal(capture); return new SnapShot(backValue); &#125; public static void setThreadLocal(WeakHashMap&lt;EsThreadLocal&lt;Object&gt;, Object&gt; ctlValues) &#123; ctlValues.forEach(EsThreadLocal::set); &#125; public static void restore(EsThreadLocal.SnapShot backUp) &#123; Iterator&lt;EsThreadLocal&lt;Object&gt;&gt; iterator = holder.get().keySet().iterator(); while (iterator.hasNext()) &#123; EsThreadLocal&lt;Object&gt; threadLocal = iterator.next(); if (!backUp.ctlValue.containsKey(threadLocal)) &#123; iterator.remove(); threadLocal.remove(); &#125; &#125; setThreadLocal(backUp.ctlValue); &#125; &#125; static class EsRunnable implements Runnable &#123; private AtomicReference&lt;SnapShot&gt; captureRef; private Runnable runnable; public EsRunnable(Runnable runnable) &#123; this.runnable = runnable; captureRef = new AtomicReference&lt;&gt;(Transmitter.capture()); &#125; @Override public void run() &#123; SnapShot capture = captureRef.get(); SnapShot backUp = Transmitter.replay(capture); try &#123; runnable.run(); &#125; finally &#123; Transmitter.restore(backUp); &#125; &#125; public static EsRunnable getRunnable(Runnable runnable) &#123; return new EsRunnable(runnable); &#125; &#125;&#125;class Test &#123; private static ThreadLocal&lt;String&gt; context = new EsThreadLocal&lt;&gt;(); private static ExecutorService pool = Executors.newFixedThreadPool(5); public static void main(String[] args)throws Exception &#123; for (int i = 1; i &lt;=10; i++) &#123; context.set(String.valueOf(i)); pool.execute(new EsThreadLocal.EsRunnable(()-&gt;System.out.println(Thread.currentThread().getName()+&quot; : &quot; + context.get() ))); TimeUnit.SECONDS.sleep(1); &#125; pool.shutdown(); &#125;&#125; 运行结果：","categories":[{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"高并发设计模式","slug":"JUC/高并发设计模式","date":"2022-01-11T11:04:33.684Z","updated":"2022-01-11T11:23:00.096Z","comments":true,"path":"2022/01/11/JUC/高并发设计模式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/JUC/%E9%AB%98%E5%B9%B6%E5%8F%91%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"并发场景下，常见的设计模式可能存在线程安全问题，比如单例模式就是一个典型。另外，为了充分发挥多核的优势，高并发程序通常会将大的任务分割成一些规模比较小的任务，分而治之，这就出现了高并发下特有的一些设计模式，比如ForkJoin模式等等。 一，线程安全的单例模式1.双重检查锁12345678910111213141516171819202122232425262728public class SingletonTestA &#123; private static volatile SingletonTestA instance; private SingletonTestA() &#123; &#125; public static SingletonTestA getInstance() &#123; /**判断对象是否已经初始化，如果已经初始化，直接返回。如果尚未初始化，加锁*/ if (instance == null) synchronized (SingletonTestA.class) &#123; /**再次判断是否已经初始化，通过第一层判断的线程可能有很多，但是能够获得锁的线程只有一个。*/ if (instance == null) /* 分配一块内存M 在内存M上初始化Singleton对象 M的地址赋值给instance变量 指令重排后可能会出现问题 */ instance = new SingletonTestA(); &#125; return instance; &#125;&#125; 2.静态内部类1234567891011121314151617public class SingletonTestA &#123; private static class LazyHolder&#123; private static final SingletonTestA instance = new SingletonTestA(); &#125; private SingletonTestA() &#123; &#125; /** * 只有在方法被调用的时候，才会去加载内部类并且初始化单例。 * @return */ public static SingletonTestA getInstance() &#123; return LazyHolder.instance; &#125;&#125; 二，Master-Worder模式Master-Worker模式是一种常见的高并发模式，它的核心思想是任务的调度和执行分离，调度任务的角包五为Master，执行任务的角色为 Worker，Master负责接收、分配任务务和合并(Merge)任务结果， Worker负责执行任务。Master-Work er模式是一种归并类型的模式。 举一个例子，在TCP服务端的请青求处理过程中，大量的客户端连接相当于大量的任务，Master需要将这些任务存储在一个任务队列中，然后分发给各个Worker，每个Worker是一个工作线程，负责完成连接的传输处理。 假设一个场景，需要执行N个任务，将这些任务的结果进行累加求和，如果任务太多，就可以采用Master-Worker模式来实现。Master持有workerCount个Worker，并且负责接收任务，然后分发给Worker，最后在回调函数中对Worker的结果进行归并求和。 1.代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183/** * @author 二十 * @since 2021/9/14 9:52 下午 */public class MasterWorkerTest &#123; static class SimpleTask extends Task&lt;Integer&gt; &#123; @Override protected Integer doExecute() &#123; System.out.println(&quot;task &quot; + getId() + &quot; is done !&quot;); return getId(); &#125; &#125; public static void main(String[] args) &#123; //创建master，包含4个worker，并启动master的执行线程 Master&lt;SimpleTask, Integer&gt; master = new Master&lt;&gt;(4); //定期向master提交任务 ScheduledExecutorService executorService = Executors.newScheduledThreadPool(10); executorService.scheduleAtFixedRate(() -&gt; master.submit(new SimpleTask()), 2L, 2L, TimeUnit.SECONDS); //定期从master提取结果 executorService.scheduleAtFixedRate(() -&gt; master.printResult(), 5, 2, TimeUnit.SECONDS); &#125;&#125;/** * 异步任务类在执行子类任务的doExecute()之后， * 回调一下Master传递过来的回调函数， * 将执行完成后的任务进行回填。 * * @param &lt;R&gt; */@Dataclass Task&lt;R&gt; &#123; static AtomicInteger index = new AtomicInteger(1); //任务的回调函数 public Consumer&lt;Task&lt;R&gt;&gt; resultAction; //任务的id private int id; //worker id private int workerId; //计算结果 R result = null; public Task() &#123; this.id = index.getAndIncrement(); &#125; public void execute() &#123; this.result = this.doExecute(); resultAction.accept(this); &#125; //钩子方法，交给子类实现 protected R doExecute() &#123; return null; &#125;&#125;/** * Master 负责接收客户端提交额任务，然后通过阻塞队列对任务进行缓存。 * Master所拥有的线程作为阻塞队列的消费者，不断从阻塞队列获取任务并轮流分给Worker。 * * @param &lt;T&gt; * @param &lt;R&gt; */class Master&lt;T extends Task, R&gt; &#123; //worker集合 private Map&lt;String, Worker&lt;T, R&gt;&gt; workers = new HashMap&lt;&gt;(); //任务集合 protected LinkedBlockingQueue&lt;T&gt; taskQueue = new LinkedBlockingQueue&lt;&gt;(); //任务处理结果集合 protected Map&lt;String, R&gt; resultMap = new ConcurrentHashMap&lt;&gt;(); //Master的任务调度线程 private Thread thread = null; //保持最终的和 private AtomicLong sum = new AtomicLong(0); public Master(int workerCount) &#123; //每一个worker对象都需要持有队列的引用，用于领取任务和提交结果 for (int i = 0; i &lt; workerCount; i++) &#123; Worker&lt;T, R&gt; worker = new Worker&lt;&gt;(); workers.put(&quot;子节点：&quot; + i, worker); &#125; thread = new Thread(() -&gt; this.execute()); thread.start(); &#125; //提交任务 public void submit(T task) &#123; taskQueue.add(task); &#125; //获取worker结果处理的回调函数 private void resultCallback(Object o) &#123; Task&lt;R&gt; task = (Task&lt;R&gt;) o; String taskName = &quot;Worker:&quot; + task.getWorkerId() + &quot;-&quot; + &quot;Task:&quot; + task.getId(); R result = task.getResult(); resultMap.put(taskName, result); sum.getAndAdd((Integer) result); &#125; //启动所有的子任务 public void execute() &#123; for (; ; ) &#123; for (Map.Entry&lt;String, Worker&lt;T, R&gt;&gt; entry : workers.entrySet()) &#123; T task = null; try &#123; //获取任务 task = this.taskQueue.take(); //获取节点 Worker worker = entry.getValue(); //分配任务 worker.submit(task, this::resultCallback); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; //获取最终结果 public void printResult() &#123; System.out.println(&quot;sum is : &quot; + sum.get()); for (Map.Entry&lt;String, R&gt; entry : resultMap.entrySet()) System.out.println(entry.getKey() + &quot; : &quot; + entry.getValue()); &#125;&#125;/** * Worker接收Master分配的任务，同样也通过阻塞队列对局部任务进行缓存。 * Worker所拥有的的线程作为拒不任务的阻塞队列的消费者， * 不断从阻塞队列获取任务并执行，执行完成后回调Master传递过来的回调函数。 * * @param &lt;T&gt; * @param &lt;R&gt; */class Worker&lt;T extends Task, R&gt; &#123; //接受任务的阻塞队列 private LinkedBlockingQueue&lt;T&gt; taskQueue = new LinkedBlockingQueue&lt;&gt;(); //Worker的编号 private static AtomicInteger index = new AtomicInteger(1); private int workerId; //执行任务的线程 private Thread thread = null; public Worker() &#123; this.workerId = index.getAndIncrement(); thread = new Thread(() -&gt; this.run()); thread.start(); &#125; //轮训执行任务 public void run() &#123; //轮训启动任务 for (; ; ) &#123; try &#123; //从阻塞队列提取任务 T task = this.taskQueue.take(); task.setWorkerId(workerId); task.execute(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; //接收任务到异步队列 public void submit(T task, Consumer&lt;R&gt; action) &#123; //设置任务的回调方法 task.resultAction = action; try &#123; this.taskQueue.put(task); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 2.Netty中Master-Worker模式的实现Master-Worker模式的核心思想是分而治之，Master角色负责接收和分配任务，Worker角色负责执行任务和结果回填。 实际上，高性能传输模式Reactor模式就是Master-Worker模式在传输领域的一种应用。基于Java的NIO技术，Nettv设计了一套优秀的、高性能Reactor(反应器)模式的具体实现。在Netty中，EventLoop反应器内部有一个线程负责JavaNIO选择器的事件轮询，然后进行对应的事件分发。事件分发的目标就是Netty的Handler处理程序(含用户定义的业务处理程序)。 Netty服务器程序中需要设置两个EventLoopGroup轮询组，一个组负责新连接的监听和接收，另一个组负责IO传输事件的轮询与分发，两个轮询组的职责具体如下: (1)负责新连接的监听和接收的EventLoopGroup轮询组中的反应器完成查询通道的新连接IO事件查询，这些反应器有点像负责招工的包工头，因此该轮询组可以形象地称为“包工头”(Boss)轮询组。 (2)另一个轮询组中的反应器完成查询所有子通道的IO事件，并且执行对应的Handler处理程序完成I0处理，例如数据的输入和输出(有点像搬砖)，这个轮询组可以形象地称为“工人”(Worker)轮询组。 Netty是基于Reactor模式的具体实现，体现了Master-Worker模式的思想。Netty的EventLoop(Reactor角色)可以对应到Master-Worker模式的Worker角色，而Netty的EventLoopGroup轮询组则可以对应到Master-Worker模式的Master角色。 3.Nginx中Master-Worker模式的实现大名鼎鼎的Nginx服务器是Master-Worker模式(更准确地说是Reactor模式)在高性能服务器领域的一种应用。Nginx是一个高性能的 HTTP和反向代理Web服务器，是由伊戈尔·赛索耶夫为俄罗斯访问量第二的Ramblerru站点开发的Web服务器。Nginx源代码以类BSD许可证的形式发布，它的第一个公开版本010发布于2004年10月4日，2011年6月1日发布了1.0.4版本。Nginx因其高稳定性、丰富的功能集、内存消耗少、并发能力强而闻名全球，目前得到非常广泛的使用，比如百度、京东、新浪、网易、腾讯、淘宝等都是它的用户。 Nginx在启动后会以daemon方式在后台运行，它的后台进程有两类:一类称为Master进程(相当于管理进程)，另一类称为Worker进程(工作进程)。Nginx的进程结构图如图。 Nginx的Master进程主要负责调度Worker进程，比如加载配置、启动工作进程、接收来自外界的信号、向各Worker进程发送信号、监控 Worker进程的运行状态等。Master进程负责创建监听套接口，交由Worker进程进行连接监听。Worker进程主要用来处理网络事件，当一个 Worker进程在接收一条连接通道之后，就开始读取请求、解析请求、处理请求，处理完成产生的数据后，再返回给客户端，最后断开连接通道。 Nginx的架构非常直观地体现了Master-Worker模式的思想。Nginx的 Master进程可以对应到Master-Worker模式的Master角色，Nginx的 Worker进程可以对应到Master-Worker模式的Worker角色。 三，ForkJoin模式“分而治之”是一种思想，所谓“分而治之”，就是把一个复杂的算法问题按一定的“分解”方法分为规模较小的若干部分，然后逐个解决，分别找出各部分的解，最后把各部分的解组成整个问题的解。“分而治之”思想在软件体系结构设计、模块化设计、基础算法中得到了非常广泛的应用。许多基础算法都运用了“分而治之”的思想，比如二分查找、快速排序等。 Master-Worker模式是“分而治之”思想的一种应用，ForkJoin模式则是“分而治之”思想的另一种应用。与Master-Worker模式不同，ForkJoin模式没有Master角色，其所有的角色都是Worker,ForkJoin模式中的Worker将大的任务分割成小的任务，一直到任务的规模足够小，可以使用很简单、直接的方式来完成。 1.ForkJoin模式的原理ForkJoin模式先把一个大任务分解成许多个独立的子任务，然后开启多个线程并行去处理这些子任务。有可能子任务还是很大而需要进一步分解，最终得到足够小的任务。ForkJoin模式的任务分解和执行过程如下： ForkJoin模式借助了现代计算机多核的优势并行处理数据。通常情况下，ForkJoin模式将分解出来的子任务放入双端队列中，然后几个启动线程从双端队列中获取任务并执行。子任务执行的结果放到一个队列中，各个线程从队列中获取数据，然后进行局部结果的合并，得到最终结果。 2.ForkJoin框架JUC包提供了一套ForkJoin框架的实现，具体以ForkJoinPool线程池的形式提供，并且该线程池在Java8的Lambda并行流框架中充当着底层框架的角色。JUC包的ForkJoin框架包含如下组件: (1)ForkJoinPool:执行任务的线程池，继承了 AbstractExecutorService类。 (2)ForkJoinWorkerThread:执行任务的工作线程(ForkJoinPool线程池中的线程)。每个线程都维护着一个内部队列，用于存放“内部任务”该类继承了Thread类。 (3)ForkJoinTask:用于ForkJoinPool的任务抽象类，实现了Future接口。 (4)RecursiveTask:带返回结果的递归执行任务，是ForkJoinTask的子类，在子任务带返回结果时使用。 (5)RecursiveAction:不返回结果的递归执行任务，是 ForkJoinTask的子类，在子任务不带返回结果时使用。 因为ForkJoinTask比较复杂，并日其抽象方法比较多，故在日常使用时一般不会直接继承ForkJoinTask来实现自定义的任务类，而是通过继承ForkJoinTask两个子类RecursiveTask或者RecursiveAction之一趋势线自定义任务类，自定义任务类需要实现这些子类的compute(),改方法的执行流程一般如下： 123456if 任务足够小 直接返回结果else 分割成N个子任务 依次调用每个子任务的fork方法执行子任务 依次调用每个子任务的join方法，等待子任务完成，然后合并执行结果 3.ForkJoin框架使用假设需要计算0-100的累加求和，可以使用ForkJoin框架完成。首先需要设计一个可以递归执行的异步任务子类。 3.1 可递归执行的异步任务类AccumulateTask123456789101112131415161718192021222324252627282930313233343536373839404142434445public class AccumulateTask extends RecursiveTask&lt;Integer&gt; &#123; private static final int threshold = 2; private int start; private int end; public AccumulateTask(int start, int end) &#123; this.start = start; this.end = end; &#125; @Override protected Integer compute() &#123; int sum = 0; //判断任务的规模：若规模小可以直接计算 boolean canCompute = (end - start) &lt;= threshold; //若任务已经足够小，则可以直接计算 if (canCompute) &#123; //直接计算并返回结果，Recursive结束 for (int i = start; i &lt;= end; i++) sum += i; System.out.println(&quot;执行任务，计算：&quot; + start + &quot;到&quot; + end + &quot;的和，结果是： &quot; + sum); &#125; else &#123; //任务过大，需要切割，Recursive 递归计算 System.out.println(&quot;切割任务：将&quot; + start + &quot;到&quot; + end + &quot;的和一分为二&quot;); int mid = (start + end) / 2; //切割成2个子任务 AccumulateTask lTask = new AccumulateTask(start, mid); AccumulateTask rTask = new AccumulateTask(mid + 1, end); //依次调用每个子任务的fork方法执行子任务 lTask.fork(); rTask.fork(); //等待子任务完成，依次调用每个子任务的join()合并执行结果 int lResult = lTask.join(); int rResult = rTask.join(); //合并子任务的执行结果 sum = lResult + rResult; &#125; return sum; &#125;&#125; 自定义的异步任务子类AccumulateTask继承自RecursiveTask，每一次执行可以携带返回值。AccumulateTask通过THRESHOLD常量设置子任务分解的阈值，并在它的computeO方法中进行阈值判断，判断的逻辑如下: (1)若当前的计算规模(这里为求和的数字个数)大于THRESHOLD，就当前子任务需要进一步分解，若当前的计算规模没有大于THRESHOLD，则直接计算(这里为求和)。 (2)如果子任务可以直接执行，就进行求和操作，并返回结果。如果任务进行了分解，就需要等待所有的子任务执行完毕、然后对各个分解结果求和。如果一个任务分解为多个子任务(含两个)，就依次调用每个子任务的fork方法执行子任务，然后依次调用每个子任务的join方法合并执行结果。 3.2 使用ForkJoinPool调度AccmulateTask()12345678910@Testpublic void testAccumulateTask()throws Exception&#123; ForkJoinPool forkJoinPool = new ForkJoinPool(); //创建一个累加任务，计算从1 到 10 AccumulateTask countTask = new AccumulateTask(1,100); Future&lt;Integer&gt; future = forkJoinPool.submit(countTask); Integer sum = future.get(1, TimeUnit.SECONDS); System.out.println(&quot;最终计算结果是：&quot;+sum); Assert.assertTrue(sum==5050);&#125; 切割任务：将1到100的和一分为二切割任务：将51到100的和一分为二切割任务：将1到50的和一分为二切割任务：将1到25的和一分为二切割任务：将1到13的和一分为二切割任务：将1到7的和一分为二切割任务：将1到4的和一分为二执行任务，计算：1到2的和，结果是： 3执行任务，计算：3到4的和，结果是： 7执行任务，计算：5到7的和，结果是： 18切割任务：将8到13的和一分为二执行任务，计算：8到10的和，结果是： 27执行任务，计算：11到13的和，结果是： 36切割任务：将26到50的和一分为二切割任务：将26到38的和一分为二切割任务：将26到32的和一分为二切割任务：将26到29的和一分为二执行任务，计算：26到27的和，结果是： 53执行任务，计算：28到29的和，结果是： 57执行任务，计算：30到32的和，结果是： 93切割任务：将33到38的和一分为二执行任务，计算：33到35的和，结果是： 102执行任务，计算：36到38的和，结果是： 111切割任务：将39到50的和一分为二切割任务：将39到44的和一分为二执行任务，计算：39到41的和，结果是： 120执行任务，计算：42到44的和，结果是： 129切割任务：将45到50的和一分为二执行任务，计算：45到47的和，结果是： 138执行任务，计算：48到50的和，结果是： 147切割任务：将76到100的和一分为二切割任务：将76到88的和一分为二切割任务：将76到82的和一分为二切割任务：将76到79的和一分为二执行任务，计算：76到77的和，结果是： 153执行任务，计算：78到79的和，结果是： 157执行任务，计算：80到82的和，结果是： 243切割任务：将83到88的和一分为二执行任务，计算：83到85的和，结果是： 252执行任务，计算：86到88的和，结果是： 261切割任务：将89到100的和一分为二切割任务：将89到94的和一分为二执行任务，计算：89到91的和，结果是： 270执行任务，计算：92到94的和，结果是： 279切割任务：将95到100的和一分为二执行任务，计算：95到97的和，结果是： 288执行任务，计算：98到100的和，结果是： 297切割任务：将51到75的和一分为二切割任务：将51到63的和一分为二切割任务：将51到57的和一分为二切割任务：将51到54的和一分为二执行任务，计算：51到52的和，结果是： 103执行任务，计算：53到54的和，结果是： 107执行任务，计算：55到57的和，结果是： 168切割任务：将58到63的和一分为二执行任务，计算：58到60的和，结果是： 177执行任务，计算：61到63的和，结果是： 186切割任务：将64到75的和一分为二切割任务：将64到69的和一分为二执行任务，计算：64到66的和，结果是： 195执行任务，计算：67到69的和，结果是： 204切割任务：将70到75的和一分为二执行任务，计算：70到72的和，结果是： 213执行任务，计算：73到75的和，结果是： 222切割任务：将14到25的和一分为二切割任务：将14到19的和一分为二执行任务，计算：14到16的和，结果是： 45执行任务，计算：17到19的和，结果是： 54切割任务：将20到25的和一分为二执行任务，计算：20到22的和，结果是： 63执行任务，计算：23到25的和，结果是： 72最终计算结果是：5050 4.ForkJoin框架的核心APIForkJoin框架的核心是ForkJoinPool线程池。该线程池使用一个无锁的栈来管理空闲线程，如果一个工作线程暂时取不到可用的任务，则可能被挂起，而挂起的线程将被压入由ForkJoinPool维护的栈中，等到有新的任务到来的时候，再从栈中唤醒这些线程。 4.1 构造器123456789101112private ForkJoinPool(int parallelism, //并行度，默认为cpu数，最小为1 ForkJoinWorkerThreadFactory factory, //线程创建工厂 UncaughtExceptionHandler handler, //异常处理程序 int mode, String workerNamePrefix) &#123; this.workerNamePrefix = workerNamePrefix; this.factory = factory; this.ueh = handler; this.config = (parallelism &amp; SMASK) | mode; long np = (long)(-parallelism); // offset ctl counts this.ctl = ((np &lt;&lt; AC_SHIFT) &amp; AC_MASK) | ((np &lt;&lt; TC_SHIFT) &amp; TC_MASK);&#125; (1) parallelism:可并行级别 ForkJoin框架将依据parallelism设定的级别决定框架内并行执行的线程数量。并行的每一个任务都会有一个线程进行处理，但parallelism属性并不是ForkJoin框架中最大的线程数量，该属性和ThreadPoolExecutor线程池中的corePoolSize、maximumPoolSize属性有区别，因为 ForkJoinPool的结构和工作方式与ThreadPoolExecutor完全不一样。 ForkJoin框架中可存在的线程数量和parallelism参数值并不是绝对关联的。 (2)factory:线程创建工厂 当ForkJoin框架创建一个新的线程时，同样会用到线程创建工厂。只不过这个线程工厂不再需要实现ThreadFactorv接口，而是需要实现ForkJoinWorkerThreadFactory接口。后者是一个函数式接口，只需要实现一个名叫newThreadO的方法。在ForkJoin框架中有一个默认的ForkJoinWorkerThreadFactory接口实现 DefaultForkJoinWorkerThreadFactory。 (3)handler:异常捕获处理程序 当执行的任务中出现异常，并从任务中被抛出时，就会被handler捕获。 (4)asyncMode:异步模式 asyncMode参数表示任务是否为异步模式，其默认值为false。如果 asyncMode为true，就表示子任务的执行遵循FIFO(先进先出)顺序，并且子任务不能被合并;如果asyncMode为false，就表示子任务的执行遵循FIFO(后进先!)顺序，并日子任务可以被合并。虽然从字面意思来看asyncMode是指异步模式，它并不是指ForkJoin框架的调度模式采用是同步模式还是异步模式工作，仅仅指任务的调度方式。ForkJoin框架中为每一个独立工作的线程准备了对应的待执行任务队列，这个任务队列是使用数组进行组合的双向队列。asyncMode模式的主要意思指的是待执行任务可以使用FIFO(先进先出)的工作模式，也可以使用 FIFO(后进先出)的工作模式，工作模式为FIFO(先进先出)的任务适用于工作线程只负责运行异步事件，不需要合并结果的异步任务。 ForkJoinPool无参数的，默认的构造器如下： 12345static final int MAX_CAP = 0x7fff; // max #workers - 1public ForkJoinPool() &#123; this(Math.min(MAX_CAP, Runtime.getRuntime().availableProcessors()), defaultForkJoinWorkerThreadFactory, null, false);&#125; 该构造器的parallelism值为CPU核心数，factory值为defaultForkJoinWorkerThreadFactory默认的线程工厂，异常捕获处理程序handler值为null；表示不进行异常处理；异步模式asyncMode值为false，使用LIFO的，可以合并子任务的模式。 4.2 common通用池很多场景可以直接使用ForkJoinPool定义的common通用池，调用ForkJoinPool.commonPool()可以获取该ForkJoin线程池，该线程池通过makeCommonPool()来构造。 1234567891011121314151617181920212223242526272829303132333435363738private static ForkJoinPool makeCommonPool() &#123; int parallelism = -1; ForkJoinWorkerThreadFactory factory = null; UncaughtExceptionHandler handler = null; try &#123; //并行度 String pp = System.getProperty (&quot;java.util.concurrent.ForkJoinPool.common.parallelism&quot;); //线程工厂 String fp = System.getProperty (&quot;java.util.concurrent.ForkJoinPool.common.threadFactory&quot;); //异常处理类 String hp = System.getProperty (&quot;java.util.concurrent.ForkJoinPool.common.exceptionHandler&quot;); if (pp != null) parallelism = Integer.parseInt(pp); if (fp != null) factory = ((ForkJoinWorkerThreadFactory)ClassLoader. getSystemClassLoader().loadClass(fp).newInstance()); if (hp != null) handler = ((UncaughtExceptionHandler)ClassLoader. getSystemClassLoader().loadClass(hp).newInstance()); &#125; catch (Exception ignore) &#123; &#125; if (factory == null) &#123; if (System.getSecurityManager() == null) factory = new DefaultCommonPoolForkJoinWorkerThreadFactory(); else // use security-managed default factory = new InnocuousForkJoinWorkerThreadFactory(); &#125; //默认并行度为cores-1 if (parallelism &lt; 0 &amp;&amp; // default 1 less than #cores (parallelism = Runtime.getRuntime().availableProcessors() - 1) &lt;= 0) parallelism = 1; if (parallelism &gt; MAX_CAP) parallelism = MAX_CAP; return new ForkJoinPool(parallelism, factory, handler, LIFO_QUEUE, &quot;ForkJoinPool.commonPool-worker-&quot;);&#125; 使用common池子的优点是可以通过指定系统属性的方式定义”并行度，线程工厂和异常处理类“，并且common池使用的是同步模式，也就是说可以支持任务合并。 通过系统属性的方式指定parallellism的值得示例如下： 1System.setPropert(&quot;java.util.concurrent.ForkJoinPool.common.parallelism&quot;,&quot;8&quot;); 除此之外，还可以通过Java指令的选项的方式指定parallellism值，具体的选项为： 1-Djava.util.concurrent.ForkJoinPool.common.parallelism=8 其他的参数值如异常处理程序handler，都可以通过以上两种方式指定。 4.3 向线程池提交任务的方式可以向ForkJoinPool线程池提交一下两类任务： 外部任务（External/Submissions Task） 向ForkJoinPool提交外部任务有三种方式:方式一是调用invoke()方法，该方法提交任务后线程会等待，等到任务计算完毕返回结果;方式二是调用execute方法提交一个任务来异步执行，无返回结果;方式三是调用submit方法提交一个任务，并且会返回一个ForkJoinTask实例，之后适当的时候可通过ForkJoinTask实例获取执行结果。 子任务（Worker Task）提交 向ForkJoinPool提交子任务的方法相对比较简单，由任务实例的 fork方法完成。当任务被分割之后，内部会调用ForkJoinPool.WorkQueuepush()方法直接把任务放到内部队列中等待被执行。 5.工作窃取算法ForkJoinPool线程池的任务分为“外部任务”和“内部任务”，两种任务的存放位置不同: (1)外部任务存放在ForkJoinPool的全局队列中。 (2)子任务会作为“内部任务”放到内部队列中，ForkJoinPool池中的每个线程都维护着一个内部队列，用于存放这些“内部任务”。 由于ForkJoinPool线程池通常有多个工作线程，与之相对应的就会有多个任务队列，这就会出现任务分配不均衡的问题:有的队列任务多，忙得不停，有的队列没有任务，一直空闲。那么有没有一种机制帮忙将任务从繁忙的线程分摊给空闲的线程呢?答案是使用工作窃取算法。 工作窃取算法的核心思想是:工作线程自己的活干完了之后，会去看看别人有没有没干完的活，如果有就拿过来帮忙干。工作窃取算法的主要逻辑:每个线程拥有一个双端队列(本地队列)，用于存放需要执行的任务，当自己的队列没有任务时，可以从其他线程的任务队列中获得一个任务继续执行。 在实际进行任务窃取操作的时候，操作线程会进行其他线程的任务队列的扫描和任务的出队尝试。为什么说是尝试?因为完全有可能操作失败，主要原因是并行执行肯定涉及线程安全的问题，假如在窃取过程中该任务已经开始执行，那么任务的窃取操作就会失败。 如何尽量避免在任务窃取中发生的线程安全问题呢?一种简单的优化方法是:在线程自己的本地队列采取LIFO(后进先出)策略，窃取其他任务队列的任务时采用FIFO(先进先出)策略。简单来说，获取自己队列的任务时从头开始，窃取其他队列的任务时从尾开始。由于窃取的动作十分快速，会大量降低这种冲突，也是一种优化方式。 6.ForkJoin框架的原理核心原理大致如下： (1)ForkJoin框架的线程池ForkJoinPool的任务分为“外部任务”和“内部任务”。 (2)“外部任务”放在ForkJoinPool的全局队列中。 (3)ForkJoinPool池中的每个线程都维护着一个任务队列，用于存放“内部任务”，线程切割任务得到的子任务会作为“内部任务”放到内部队列中。 (4)当工作线程想要拿到子任务的计算结果时，先判断子任务有没有完成，如果没有完成，再判断子任务有没有被其他线程“窃取”，如果子任务没有被窃取，就由本线程来完成;一旦子任务被窃取了，就去执行本线程“内部队列”的其他任务，或者扫描其他的任务队列并窃取任务。 (5)当工作线程完成其“内部任务”，处于空闲状态时，就会扫描其他的任务队列窃取任务，尽可能不会阻塞等待。 总之，ForkJoin线程在等待一个任务完成时，要么自己来完成这个任务，要么在其他线程窃取了这个任务的情况下，去执行其他任务，是不会阻塞等待的，从而避免资源浪费，除非所有任务队列都为空。 工作窃取算法的优点： (1)线程是不会因为等待某个子任务的执行或者没有内部任务要执行而被阻塞等待、挂起的，而是会扫描所有的队列窃取任务，直到所有队列都为空时才会被挂起。 (2)ForkJoin框架为每个线程维护着一个内部任务队列以及一个全局的任务队列，而且任务队列都是双向队列，可从首尾两端来获取任务，极大地减少了竞争的可能性，提高并行的性能。 ForkJoinPool适合需要“分而治之”的场景，特别是分治之后递归调用的函数，例如快速排序、二分搜索、大整数乘法、矩阵乘法、棋盘覆盖、归并排序、线性时间选择、汉诺塔问题等。ForkJoinPool适合调度的任务为CPU密集型任务，如果任务存在I/0操作、线程同步操作、sleep睡眠等较长时间阻塞的情况，最好配合使用ManagedBlocker进行阻塞管理。总体来说，ForkJoinPool不适合进行I0密集型、混合型的任务调度。 四，生产者-消费者模式生产者-消费者模式是一个经典的多线程设计模式，它为多线程间的协作提供了良好的解决方案，是高并发编程过程中常用的一种设计模式。 在实际的软件开发过程中，经常会碰到如下场景:某些模块负责产生数据，另一些模块负责消费数据(此处的模块可以是类、承数、线程、进程等)。产生数据的模块可以形象地称为生产者，而消费数据的模块可以称为消费者。然而，仅仅抽象出来生产者和消费者还不够，该模式还需要有一个数据缓冲区作为生产者和消费者之间的中介:生产者把数据放入缓冲区，而消费者从缓冲区取出数据。 数据缓冲区的作用主要在于能使生产者和消费者解耦。如果没有数据缓冲区，让生产者直接调用消费者的某个方法，那么生产者对于消费者就会产生依赖(也就是耦合)。将来如果消费者的代码发生变化，可能会影响到生产者。而如果两者都依赖于某个缓冲区，两者之间不直接依赖，耦合也就相应降低了。生产者-消费者模式天生就是用来处理并发问题的。生产者和消费者是两个独立的并发主体，生产者把制造出来的数据往缓冲区一放，就可以再去生产下一个数据了。生产者基本上不用依赖消费者的处理速度。尤其是在生成者的速度时快时慢时，生产者-消费者模式的好处就体现出来了。当数据制造快的时候，消费者来不及处理，未处理的数据可以暂时存在缓冲区中。等生产者的制造速度慢下来，消费者再慢慢处理掉。 在生产者-消费者模式中，缓冲区是性能的关键，缓冲区可以基于 ArrayList、LinkedList、BlockingQueue、环形队列等各种不同的数据存储组件去设计，所使用的组件不同，生产者-消费者模式实现的性能当然也就不同。 五，Future模式Future模式是高并发设计与开发过程中常见的设计模式，它的核心思想是异步调用。对于Future模式来说，它不是立即返回我们所需要的数据，但是它会返回一个契约(或异步任务)，将来我们可以凭借这个契约(或异步任务)获取需要的结果。 在进行传统的RPC(远程调用)时，同步调用RPC是一段耗时的过程。当客户端发出RPC请求后，服务端完成请求处理需要很长的一段时间才会返回，这个过程中客户端一直在等待，直到数据返回后，再进行其他任务的处理。现有一个Client同步对三个Server分别进行一次RPC调用。 假设一次远程调用的时间为500毫秒，则一个Client同步对三个Server分别进行一次RPC调用的总时间需要耗费1500毫秒。如果要节省这个总时间，可以使用Future模式对其进行改造，将同步的RPC调用改为异步并发的RPC调用，一个Client异步并发对三个Server分别进行一次 RPC调用。 一个Client同步对三个Server分别进行一次RPC调用 一个Client异步并发对三个Server分别进行一次RPC调用 假设一次远程调用的时间为500毫秒，则一个Client异步并发对三个 Server分别进行一次RPC调用的总时间只要耗费500毫秒。使用Future模式异步并发地进行RPC调用，客户端在得到一个RPC的返回结果前并不急于获取该结果，而是充分利用等待时间去执行其他的耗时操作(如其他RPC调用)，这就是Future模式的核心所在。 Future模式的核心思想是异步调用，有点类似于异步的Ajax请求。当调用某个耗时方法时，可以不急于立刻获取结果，而是让被调用者立刻返回一个契约(或异步任务)，并且将耗时的方法放到另外的线程中执行，后续凭契约再去获取异步执行的结果。 在具体的实现上，Future模式和异步回调模式既有区别，又有联系。Java的Future实现类并没有支持异步回调，仍然需要主动获取耗时任务的结果;而Java8中的CompletableFuture组件实现了异步回调模式。","categories":[{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"高并发核心模式之异步回调","slug":"JUC/高并发核心模式之异步回调","date":"2022-01-11T11:04:22.500Z","updated":"2022-01-11T11:21:32.266Z","comments":true,"path":"2022/01/11/JUC/高并发核心模式之异步回调/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/JUC/%E9%AB%98%E5%B9%B6%E5%8F%91%E6%A0%B8%E5%BF%83%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%BC%82%E6%AD%A5%E5%9B%9E%E8%B0%83/","excerpt":"","text":"一，Guava的异步回调模式Guava是Google提供的Java扩展包，它提供了一种异步回调的解决方案。Guava中与异步回调相关的源码处于com.google.commonutilconcurrent包中。包中的很多类都用于对javautilconcurrent的能力扩展和能力增强。比如，Guava的异步任务接口ListenableFuture扩展了Java的Future接口，实现了异步回调的能力。​ 1.FutureCallback总体来说，Guava主要增强了Java而不是另起炉灶。为了实现异步回调方式获取异步线程的结果，Guava做了以下增强:​ 引入了一个新的接口ListenableFuture，继承了Java的Future接口，使得Java的Future异步任务在Guava中能被监控和非阻塞获取异步结果。 引入了一个新的接口FutureCallback，这是一个独立的新接口。该接口的目的是在异步任务执行完成后，根据异步结果完成不同的回调处理，并且可以处理异步结果。 ​ FutureCallback是一个新增的接口，用来填写异步任务执行完后的监听逻辑。FutureCallback拥有两个日调方法:​ onSuccess()方法，在异步任务执行成功后被回调。调用时，异步任务的执行结果作为onSuccess方法的参数被传入。 onFailure0方法，在异步任务执行过程中抛出异常时被回调。调用时，异步任务所抛出的异常作为onFailure方法的参数被传入。 ​ Guava的FutureCallback与Java的Callable名字相近，实质不同，存在本质的区别:​ (1)Java的Callable接口代表的是异步执行的逻辑。​ (2)Guava的FutureCallback接口代表的是Callable异步逻辑执行完成之后，根据成功或者异常两种情形所需要执行的善后工作。​ Guava是对JavaFuture异步回调的增强，使用Guava异步回调也需要用到Java的Callable接口。简单地说，只有在Java的Callable任务执行结果出来后，才可能执行Guava中的FutureCallback结果回调。Guava如何实现异步任务Callable和结果回调FutureCallback之间的监控关系呢?Guava引入了一个新接口ListenableFuture，它继承了Java的 Future接口，增强了被监控的能力。 ​ 2.ListenableFutureGuava的ListenableFuture接口是对Java的Future接口的扩展，可以理解为异步任务实例：​ 1234public interface ListenableFuture&lt;V&gt; extends Future&lt;V&gt;&#123; //此方法由guava内部调用 void addListener(Runnable r , Executor e) &#125; ListenableFuture仅仅增加了一个addListener方法。它的作用就是将FutureCallback善后回调逻辑封装成一个内部的Runnable异步口调任务，在Callable异步任务完成后回调FutureCallback善后逻辑。​ 注意，此addListener(方法只在Guava内部调用，在实际编程中，addListener(）不会使用到。​ 在实际编程中，如何将FutureCallback回调逻辑绑定到异步的 ListenableFuture任务呢?可以使用Guava的Futures工具类，它有一个 addCallback0静态方法，可以将FutureCallback的回调实例绑定到 ListenableFuture异步任务。下面是一个简单的绑定实例:​ 123456789Futures.addCallback(listenableFuture,new FutureCallback&lt;Boolean&gt;)&#123; public void onSuccess(Boolean r)&#123; // listenableFuture内的Callable 成功时回调此方法 &#125; public void onFailure(Throwable t)&#123; // listenableFuture内的Callable 异常时回调此方法 &#125;&#125; ​ Guava的ListenableFuture接口是对Java的Future接口的扩展，都表示异步任务，那么Guava的异步任务实例从何而来?​ 3.ListenableFuture异步任务如果要获取Guava的ListenableFuture异步任务实例，主要通过向线程池(ThreadPool)提交Callable任务的方式获取。不过，这里所说的线程池不是Java的线程池，而是经过Guava自己定制过的Guava线程池。​ Guava线程池是对Java线程池的一种装饰。创建Guava线程池的方法如下:​ 12345//java线程池ExecutorService jPool = Executors.newFixedThreadPool(10);//Guava线程池ListeningExecutorService gPool = MoreExecutors.listeningDecorator(jPool); ​ 首先创建Java线程池，然后以其作为Guava线程池的参数再构造一个Guava线程池。有了Guava的线程池之后，就可以通过submit()方法来提交任务了，任务提交之后的返回结果就是我们所要的ListenableFuture异步任务实例。​ 简单来说，获取异步任务实例的方式是通过向线程池提交Callable业务逻辑来实现，代码如下:​ 123456//submit()用来提交任务，返回异步任务实例ListenableFuture&lt;Boolean&gt; hFuture = gPool.submit(hJob);//绑定回调实例Futures.addCallback(listenableFuture, new FutureCallback&lt;Boolean&gt;()&#123; //有两种实现回调的方法&#125;); ​ 取到了ListenableFuture实例后，通过Futures.addCallback0方法将 FutureCallback回调逻辑的实例绑定到ListenableFuture异步任务实例，实现异步执行完成后的回调。​ 总结一下，Guava异步回调的流程如下:​ 实现Java的Callable接口，创建异步执行逻辑。还有一种情况，如果不需要返回值，异步执行逻辑也可以实现Runnable接口。 创建Guava线程池。 将(1)创建的Callable/Runnable异步执行逻辑的实例提交到 Guava线程池，从而获取ListenableFuture异步任务实例。 创建FutureCallback回调实例，通过FuturesaddCallback将回调实例绑定到ListenableFuture异步任务上。 ​ 完成以上4步，当Callable/Runnable异步执行逻辑完成后，就会回调 FutureCallback实例的回调方法onSuccess()/onFailure()。​ 4.使用Guava实现泡茶喝的实例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109public class GuavaFutureDemo &#123; public static final int SLEEP_GAP=3; static class HotWaterJob implements Callable&lt;Boolean&gt;&#123; @Override public Boolean call() throws Exception &#123; try &#123; System.out.println(&quot;洗好水壶&quot;); System.out.println(&quot;烧开水&quot;); TimeUnit.SECONDS.sleep(SLEEP_GAP); System.out.println(&quot;水开了&quot;); &#125;catch (Exception e)&#123; e.printStackTrace(); return false; &#125; return true; &#125; &#125; static class WashJob implements Callable&lt;Boolean&gt;&#123; @Override public Boolean call() throws Exception &#123; try&#123; System.out.println(&quot;洗茶杯&quot;); TimeUnit.SECONDS.sleep(SLEEP_GAP); System.out.println(&quot;洗完了&quot;); &#125;catch (Exception e)&#123; System.out.println(&quot;清洗工作发生异常！&quot;); return false; &#125; System.out.println(&quot;清洗工作运行结束！&quot;); return true; &#125; &#125; static class DrinkJob&#123; boolean waterOk =false; boolean cupOk =false; public void drinkTea()&#123; if (waterOk&amp;&amp;cupOk)&#123; System.out.println(&quot;泡茶喝，茶喝完！&quot;); this.waterOk=false; &#125; &#125; &#125; public static void main(String[] args)throws Exception &#123; Thread.currentThread().setName(&quot;泡茶喝线程&quot;); //新启动一个线程，作为泡茶主线程 DrinkJob drinkJob = new DrinkJob(); //烧水的业务逻辑 Callable&lt;Boolean&gt; hotJob = new HotWaterJob(); //清晰的业务逻辑 Callable&lt;Boolean&gt; washJob = new HotWaterJob(); //创建java线程池 ExecutorService jPool = Executors.newFixedThreadPool(10); //包装java线程池，构造guava线程池 ListeningExecutorService gPool = MoreExecutors.listeningDecorator(jPool); //烧水的回调 FutureCallback&lt;Boolean&gt; hotWaterHook = new FutureCallback&lt;Boolean&gt;() &#123; @Override public void onSuccess(Boolean r) &#123; if (r)&#123; drinkJob.waterOk=true; drinkJob.drinkTea(); &#125; &#125; @Override public void onFailure(Throwable throwable) &#123; System.out.println(&quot;喝nm！&quot;); &#125; &#125;; //启动烧水线程 ListenableFuture&lt;Boolean&gt; hotFuture = gPool.submit(hotJob); //设置烧水任务的回调钩子 Futures.addCallback(hotFuture, hotWaterHook); //启动清洗线程 ListenableFuture&lt;Boolean&gt; washFuture = gPool.submit(washJob); //使用匿名实例，作为清洗之后的回调钩子 Futures.addCallback(washFuture, new FutureCallback&lt;Boolean&gt;() &#123; @Override public void onSuccess(Boolean r) &#123; if (r)&#123; drinkJob.cupOk=true; //执行回调 drinkJob.drinkTea(); &#125; &#125; @Override public void onFailure(Throwable throwable) &#123; System.out.println(&quot;喝nm！&quot;); &#125; &#125;); System.out.println(&quot;干点其他事！&quot;); TimeUnit.SECONDS.sleep(1); System.out.println(&quot;执行完成！&quot;); &#125;&#125; ​ 以上结果，烧水线程为pool-1-thread-1，清洗线程为pool-1-thread-2，在二者完成之前，泡茶喝线程已经执行完了。泡茶喝的工作在异步回调方法drinkTea(中执行，执行的线程并不是“泡茶喝”线程，而是烧水线程和清洗线程。​ 5.Guava异步回调和Java异步回调的区别总结一下Guava异步回调和Java的FutureTask异步调用的区别，具体如下:​ FutureTask是主动调用的模式，“调用线程”主动获得异步结果，在获取异步结果时处于阻塞状态，并且会一直阻塞，直到拿到异步线程的结果。 Guava是异步回调模式，“调用线程”不会主动获得异步结果，而是准备好回调函数，并设置好回调钩子，执行回调函数的并不是“调用线程”自身，回调承数的执行者是“被调用线程”，“调用线程”在执行完自己的业务逻辑后就已经结束了，当回调采数被执行时，“调用线程”可能已经结束很久了。 ​ 二，Netty的异步回调模式Netty官方文档说明Netty的网络操作都是异步的。Netty源码中大量使用了异步回调处理模式。在Netty的业务开发层面，处于Netty应用的 Handler处理程序中的业务处理代码也都是异步执行的。所以，了解 Netty的异步回调，无论是Netty应用开发还是源码级开发都是十分重要的。​ Netty和Guava一样，实现了自己的异步回调体系:Netty继承和扩展了JDKFuture系列异步回调的API，定义了自身的Future系列接口和类，实现了异步任务的监控、异步执行结果的获取。​ 总体来说，Netty对JavaFuture异步任务的扩展如下:​ 继承Java的Future接口得到了一个新的属于Netty自己的Future异步任务接口，该接口对原有的接口进行了增强，使得Netty异步任务能够非阻塞地处理回调结果。注意，Netty没有修改Future的名称，只是调整了所在的包名，Netty的Future类的包名和Java的Future接口的包不同。​ 引入了一个新接口–GenericFutureListener，用于表示异步执行完成的监听器。这个接口和Guava的FutureCallback回调接口不同。Nettv使用了监听器的模式，异步任务执行完成后的回调逻辑抽象成了Listener监听器接口。可以将Netty的GenericFutureListener监听器接口加入Netty异步任务Future中，实现对异步任务执行状态的事件监听。​ 总体来说，在异步非阻塞回调的设计思路上，Netty和Guava是一致的。对应关系为:​ Netty的Future接口可以对应到Guava的ListenableFuture接口。 ​ Netty的GenericFutureListener接口可以对应到Guava的 FutureCallback接口。 ​ 1.GenericFutureListener前面提到，和Guava的FutureCallback一样，Netty新增了一个接口，用来封装异步非阻塞回调的逻辑，那就是GenericFutureListener接口。​ GenericFutureListener位于io.netty.util.concurrent包中，源码如下:​ 123456package io.netty.util.concurrent; import java.util.EventListenerpublic interface GenericFutureListener&lt;F extends Future&lt;?&gt;&gt; extends Eventlistener&#123; //监听器的回调方法 void operationComplete(F var1) throws Exception;&#125; ​ GenericFutureListener拥有一个回调方法operationCompleteO，表示异步任务操作完成。在Future异步任务执行完成后将回调此方法。大多数情况下，Netty的异步回调代码编写在GenericFutureListener接口的实现类的operationComplete方法中。​ 说明一下，GenericFutureListener的父接口EventListener是一个空接口，没有任何抽象方法，是一个仅仅具有标识作用的接口。​ 2.Netty的Future接口Netty也对Java的Future接口进行了扩展，并且名称没有变，还是叫作Future接口，实现在io.nettyutil.concurrent包中。​ 和Guava的ListenableFuture一样，Netty的Future接口扩展了一系列方法，对执行的过程进行监控，对异步回调完成事件进行Listen监听并且回调。​ Netty的Future接口一般不会直接使用，使用过程中会使用它的子接口。Netty有一系列子接口，代表不同类型的异步任务，如ChannelFuture接口。​ ChannelFuture子接口表示Channel通道I/0操作的异步任务，如果在 Channel的异步I/0操作完成后需要执行回调操作，就需要使用到 ChannelFuture接口。​ 3.ChannelFuture在Netty网络编程中，网络连接通道的输入、输出处理都是异步进行的，都会返回一个ChannelFuture接口的实例。通过返回的异步任务实例可以为其增加异步回调的监听器。在异步任务真正完成后，回调执行。​ Netty的网络连接的异步回调实例代码如下:​ 1234567891011121314//connect是异步的，仅仅是提交异步任务ChannelFuture future = bootstrap.connect(new InetSocketAddress(&quot;www.manning.com,80));//connect的异步任务真正执行完成后，future回调监听器会执行future.addListener(new ChannelFutureListener()&#123; @Override public void operationComplete(ChannelFuture channelFuture) throws Exception &#123; if(channelFuture.isSuccess)) System.out.println(&quot;Connection established&quot;); else &#123; System.err.println(&quot;Connection attempt failed&quot;); channelFuture.cause().printStackTrace); &#125; &#125;&#125; ​ GenericFutureListener接口在Netty中是一个基础类型接口。在网络编程的异步回调中，一般使用Netty中提供的某个子接口，如ChannelFutureListener接口。在上面的代码中，使用到的是这个子接口。​ 4.Netty的出站和入站异步回调Netty的出站和入站操作都是异步的。异步回调的方法和前面Netty建立的异步回调是一样的。​ 下面以经典的NIO出站操作write为例说明ChannelFuture的使用。​ 在write操作调用后，Netty并没有立即完成对JavaNIO底层连接的写入操作，底层的写入操作是异步执行的，代码如下:​ 123456789//write()输出方法，返回的是一个异步任务ChannelFuture future=ctx.channel0.write(msg);//为异步任务加上监听器 future.addListener( new ChannelFutureListener()&#123; @Override public void operationComplete(ChannelFuture future)&#123; // write操作完成后的回调代码 &#125; &#125;); ​ 在write操作完成后立即返回，返回的是一个ChannelFuture接口的实例。通过这个实例可以绑定异步回调监听器，编写异步回调的逻辑。​ ​","categories":[{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"RabbitMQ","slug":"消息队列/RabbitMQ","date":"2022-01-11T09:08:03.557Z","updated":"2022-01-11T09:11:01.252Z","comments":true,"path":"2022/01/11/消息队列/RabbitMQ/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/RabbitMQ/","excerpt":"","text":"一.rabbitMQ概述1.搜索与商品服务的问题假设我们已经完成了商品详情和搜索系统的开发。我们思考一下，是否存在问题？ 商品的原始数据保存在数据库中，增删改查都在数据库中完成。搜索服务数据来源是索引库，如果数据库商品发生变化，索引库数据能否及时更新。如果我们在后台修改了商品的价格，搜索页面依然是旧的价格，这样显然不对。该如何解决？ 方案1：每当后台对商品做增删改操作，同时要修改索引库数据 方案2：搜索服务对外提供操作接口，后台在商品增删改后，调用接口 以上两种方式都有同一个严重问题：就是代码耦合，后台服务中需要嵌入搜索和商品页面服务，违背了微服务的独立原则。所以，我们会通过另外一种方式来解决这个问题：消息队列 2.消息队列2.1什么是消息队列消息队列，即MQ，Message Queue。 “消息”是在两台计算机间传送的数据单位。消息可以非常简单，例如只包含文本字符串；也可以更复杂，可能包含嵌入对象。​ 消息被发送到队列中。“消息队列”是在消息的传输过程中保存消息的容器。消息队列管理器在将消息从它的源中继到它的目标时充当中间人。队列的主要目的是提供路由并保证消息的传递；如果发送消息时接收者不可用，消息队列会保留消息，直到可以成功地传递它。 消息队列是典型的：生产者、消费者模型。生产者不断向消息队列中生产消息，消费者不断的从队列中获取消息。因为消息的生产和消费都是异步的，而且只关心消息的发送和接收，没有业务逻辑的侵入，这样就实现了生产者和消费者的解耦。 结合前面所说的问题： · 商品服务对商品增删改以后，无需去操作索引库，只是发送一条消息，也不关心消息被谁接收。 · 搜索服务接收消息，去处理索引库。 如果以后有其它系统也依赖商品服务的数据，同样监听消息即可，商品服务无需任何代码修改。 2.2AMQP和JMSMQ是消息通信的模型，并不是具体实现。现在实现MQ的有两种主流方式：AMQP、JMS。 两者间的区别和联系： · JMS是定义了统一接口，对消息操作进行统一；AMQP通过规定协议统一数据交互的格式； · JMS限定了必须使用Java语言；AMQP只是协议，不规定实现方式，因此是跨语言的 · JMS规定了两种消息模型（queue, topic）；而AMQP的消息模型更加丰富 2.3常见MQ产品· ActiveMQ：基于JMS · RabbitMQ：基于AMQP协议，erlang语言开发，稳定性好 · RocketMQ：基于JMS，阿里巴巴产品，目前交由Apache基金会 · Kafka：分布式消息系统，高吞吐量，处理日志，Scala和Java编写，Apache 2.4 RabbitMQ官网： http://www.rabbitmq.com/ 官方教程：http://www.rabbitmq.com/getstarted.html 2.5 MQ 三大主要功能 异步 解耦 削峰 2.6 RabbitMQ工作模型 1.Broker我们要使用RabbitMQ来收发消息，必须要安装个RabbitMQ的服务，可以安装在Windows上面也可以安装在Linux 上面，默认是5672的端口。这台RabbitMQ的服务器我们把它叫做 Broker， MQ 服务器帮助我们做的事情就是存储、转发消息。 2.Connection无论是生产者发送消息，还是消费者接收消息，都必须要跟 Broker 之间建立一个连接，这个连接是一个 TCP 的长连接。 3.Channel如果所有的生产者发送消息和消费者接收消息，都直接创建和释放 TCP 长连接的话，对于 Broker 来说肯定会造成很大的性能损耗，因为 TCP 连接是非常宝贵的资源，创建和释放也要消耗时间。所以在 AMQP 里面引入了 Channel 的概念，它是一个虚拟的连接。这样我们就可以在保持的 TCP 长连接里面去创建和释放Channel，大大了减少了资源消耗。 4.Queue队列是真正用来存储消息的，是一个独立运行的进程，有自己的数据库（Mnesia）。 我们可以基于事件机制，实现消费者对队列的监听。 由于队列有 FIFO 的特性，只有确定前一条消息被消费者接收之后，才会把这条消息从数据库删除，继续投递下一条消息。 5.Exchange在 RabbitMQ 里面永远不会出现消息直接发送到队列的情况。因为在 AMQP 里面引入了交换机（Exchange）的概念，用来实现消息的灵活路由。 交换机是一个绑定列表，用来查找匹配的绑定关系。 队列使用绑定键（Binding Key）跟交换机建立绑定关系。 生产者发送的消息需要携带路由键（Routing Key），交换机收到消息时会根据它保存的绑定列表，决定将消息路由到哪些与它绑定的队列上。 注意：交换机与队列、队列与消费者都是多对多的关系。 6.VhostVHOST 除了可以提高硬件资源的利用率之外，还可以实现资源的隔离和权限的控制。 不同的 VHOST 中可以有同名的 Exchange 和 Queue，它们是完全透明的。 这个时候，我们可以为不同的业务系统创建不同的用户（User），然后给这些用户分配 VHOST 的权限。 2.7 使用rabbitMQ会带来的一些问题？系统可用性降低：原来是两个节点的通信，现在还需要独立运行一个服务，如果 MQ服务器或者通信网络出现问题，就会导致请求失败。 系统复杂性提高： 为什么说复杂？第一个就是你必须要理解相关的模型和概念，才能正确地配置和使用 MQ。第二个，使用 MQ 发送消息必须要考虑消息丢失和消息重复消费的问题。一旦消息没有被正确地消费，就会带来数据一致性的问题。 所以，我们在做系统架构的时候一定要根据实际情况来分析，不要因为我们说了这么多的 MQ 能解决的问题，就盲目地引入 MQ。 3.下载和安装3.1 下载RabbitMQ是Erlang语言编写，所以Erang环境必须要有，注：Erlang环境一定要与RabbitMQ版本匹配：https://www.rabbitmq.com/which-erlang.html Erlang下载地址：https://www.rabbitmq.com/releases/erlang/（根据自身需求及匹配关系，下载对应rpm包） https://dl.bintray.com/rabbitmq-erlang/rpm/erlang/21/el/7/x86_64/erlang-21.3.8.9-1.el7.x86_64.rpm rabbitmq安装依赖于socat，所以需要下载socat。 socat下载地址：http://repo.iotti.biz/CentOS/7/x86_64/socat-1.7.3.2-5.el7.lux.x86_64.rpm RabbitMQ下载地址：https://www.rabbitmq.com/download.html（根据自身需求及匹配关系，下载对应rpm包）`rabbitmq-server-3.8.1-1.el7.noarch.rpm` 3.2安装1rpm -ivh erlang-21.3.8.9-1.el7.x86_64.rpm 1rpm -ivh socat-1.7.3.2-1.el6.lux.x86_64.rpm 1rpm -ivh rabbitmq-server-3.8.1-1.el7.noarch.rpm 启用管理插件 1rabbitmq-plugins enable rabbitmq_management 启动RabbitMQ 1234systemctl start rabbitmq-server.servicesystemctl status rabbitmq-server.servicesystemctl restart rabbitmq-server.servicesystemctl stop rabbitmq-server.service 查看进程 1ps -ef | grep rabbitmq 启用延时队列插件 1rabbitmq-plugins enable rabbitmq_delayed_message_exchange 3.3 测试o 关闭防火墙：systemctl stop firewalld.service o 在web浏览器中输入地址：http://虚拟机ip:15672/ o 输入默认账号密码:guest : guest，guest用户默认不允许远程连接。 添加用户 1rabbitmqctl add_user root root 分配角色 1rabbitmqctl set_user_tags root administrator 修改密码 1rabbitmqctl change_password root root 查看所有用户 1rabbitmqctl list_users 3.4卸载12rpm -qa | grep rabbitmqrpm -e rabbitmq-server 4管理界面4.1 添加用户 超级管理员(administrator) 可登录管理控制台，可查看所有的信息，并且可以对用户，策略(policy)进行操作。 监控者(monitoring) 可登录管理控制台，同时可以查看rabbitmq节点的相关信息(进程数，内存使用情况，磁盘使用情况等) 策略制定者(policymaker) 可登录管理控制台, 同时可以对policy进行管理。但无法查看节点的相关信息(上图红框标识的部分)。 普通管理者(management) 仅可登录管理控制台，无法看到节点信息，也无法对策略进行管理。 其他 无法登录管理控制台，通常就是普通的生产者和消费者。 4.2 创建 Virtual Hosts虚拟主机：类似于mysql中的database。他们都是以“/”开头 二，五种消息模型RabbitMQ提供了6种消息模型，但是第6种其实是RPC，并不是MQ，因此不予学习。那么也就剩下5种。但是其实3、4、5这三种都属于订阅模型，只不过进行路由的方式不同。​ 准备代码环境 12345678910111213141516171819202122232425262728&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.2.6.RELEASE&lt;/version&gt;&lt;/parent&gt;&lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt; &lt;version&gt;3.3.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.rabbitmq&lt;/groupId&gt; &lt;artifactId&gt;amqp-client&lt;/artifactId&gt; &lt;version&gt;5.4.3&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 抽取一个获取连接的工具类 1234567891011121314151617181920212223242526272829public class ConnectionUtil &#123; /** * 建立与RabbitMQ的连接 * @return * @throws Exception */ public static Connection getConnection() throws Exception &#123; //定义连接工厂 ConnectionFactory factory = new ConnectionFactory(); //设置服务地址 factory.setHost(&quot;121.199.31.160&quot;); //端口 factory.setPort(5672); //设置账号信息，用户名、密码、vhost factory.setVirtualHost(&quot;/shopping&quot;); factory.setUsername(&quot;root&quot;); factory.setPassword(&quot;root&quot;); // 通过工程获取连接 Connection connection = factory.newConnection(); return connection; &#125; public static void main(String[] args) throws Exception &#123; Connection con = ConnectionUtil.getConnection(); System.out.println(con); con.close(); &#125;&#125; 1.基本消息模型​ RabbitMQ是一个消息代理：它接受和转发消息。 你可以把它想象成一个邮局：当你把邮件放在邮箱里时，你可以确定邮差先生最终会把邮件发送给你的收件人。RabbitMQ与邮局的主要区别是它不处理纸张，而是接受，存储和转发数据消息的二进制数据块。​ 1234567891011121314151617181920212223242526/** * 生产者 */public class Send &#123; private final static String QUEUE_NAME = &quot;simple_queue&quot;; public static void main(String[] argv) throws Exception &#123; // 获取到连接 Connection connection = ConnectionUtil.getConnection(); // 从连接中创建通道，使用通道才能完成消息相关的操作 Channel channel = connection.createChannel(); // 声明（创建）队列 channel.queueDeclare(QUEUE_NAME, false, false, false, null); // 消息内容 String message = &quot;Hello World!&quot;; // 向指定的队列中发送消息 channel.basicPublish(&quot;&quot;, QUEUE_NAME, null, message.getBytes()); System.out.println(&quot; [x] Sent &#x27;&quot; + message + &quot;&#x27;&quot;); //关闭通道和连接 channel.close(); connection.close(); &#125;&#125; 12345678910111213141516171819202122232425262728/** * 消费者 */public class Recv &#123; private final static String QUEUE_NAME = &quot;simple_queue&quot;; public static void main(String[] argv) throws Exception &#123; // 获取到连接 Connection connection = ConnectionUtil.getConnection(); // 创建通道 Channel channel = connection.createChannel(); // 声明队列 channel.queueDeclare(QUEUE_NAME, false, false, false, null); // 定义队列的消费者 DefaultConsumer consumer = new DefaultConsumer(channel) &#123; // 获取消息，并且处理，这个方法类似事件监听，如果有消息的时候，会被自动调用 @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; // body 即消息体 String msg = new String(body); System.out.println(&quot; [x] received : &quot; + msg + &quot;!&quot;); &#125; &#125;; // 监听队列，第二个参数：是否自动进行消息确认。 channel.basicConsume(QUEUE_NAME, true, consumer); &#125;&#125; 1.1 消息确认机制-ACKRabbitMQ有一个ACK机制。当消费者获取消息后，会向RabbitMQ发送回执ACK，告知消息已经被接收。不过这种回执ACK分两种情况： 自动ACK：消息一旦被接收，消费者自动发送ACK 手动ACK：消息接收后，不会发送ACK，需要手动调用 选择哪种要看消息的重要性： 如果消息不太重要，丢失也没有影响，那么自动ACK会比较方便 如果消息非常重要，不容丢失。那么最好在消费完成后手动ACK，否则接收消息后就自动ACK，RabbitMQ就会把消息从队列中删除。如果此时消费者宕机，那么消息就丢失了 我们之前的测试都是自动ACK的，如果要手动ACK，需要改动我们的代码：​ 12345678910111213141516171819202122232425262728293031/** * 消费者,手动进行ACK */public class Recv2 &#123; private final static String QUEUE_NAME = &quot;simple_queue&quot;; public static void main(String[] argv) throws Exception &#123; // 获取到连接 Connection connection = ConnectionUtil.getConnection(); // 创建通道 final Channel channel = connection.createChannel(); // 声明队列 channel.queueDeclare(QUEUE_NAME, false, false, false, null); // 定义队列的消费者 DefaultConsumer consumer = new DefaultConsumer(channel) &#123; // 获取消息，并且处理，这个方法类似事件监听，如果有消息的时候，会被自动调用 @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; // body 即消息体 String msg = new String(body); //int i = 1/0 ; System.out.println(&quot; [x] received : &quot; + msg + &quot;!&quot;); // 手动进行ACK channel.basicAck(envelope.getDeliveryTag(), false); &#125; &#125;; // 监听队列，第二个参数false，手动进行ACK channel.basicConsume(QUEUE_NAME, false, consumer); &#125;&#125; 2.work消息模型避免执行资源密集型任务时，必须等待它执行完成。相反我们稍后完成任务，我们将任务封装为消息并将其发送到队列。 在后台运行的工作进程将获取任务并最终执行作业。当运行许多消费者时，任务将在他们之间共享，但是一个消息只能被一个消费者获取。​ 一定程度上，避免消息的堆积。 ​ 平均分摊​ 消费者2与消费者1基本类似，就是没有设置消费耗时时间。这里是模拟有些消费者快，有些比较慢。 接下来，两个消费者一同启动，然后发送50条消息：可以发现，两个消费者各自消费了25条消息，而且各不相同，这就实现了任务的分发。 能者多劳 现在的状态属于是把任务平均分配，正确的做法应该是消费越快的人，消费的越多。怎么实现呢？ 我们可以使用basicQos方法和prefetchCount = 1设置。这告诉RabbitMQ一次不要向工作人员发送多于一条消息。或者换句话说，不要向工作人员发送新消息，直到它处理并确认了前一个消息。相反，它会将其分派给不是仍然忙碌的下一个工作人员。 12345678910111213141516171819202122232425// 生产者public class Send &#123; private final static String QUEUE_NAME = &quot;test_work_queue&quot;; public static void main(String[] argv) throws Exception &#123; // 获取到连接 Connection connection = ConnectionUtil.getConnection(); // 获取通道 Channel channel = connection.createChannel(); // 声明队列 channel.queueDeclare(QUEUE_NAME, false, false, false, null); // 循环发布任务 for (int i = 0; i &lt; 50; i++) &#123; // 消息内容 String message = &quot;task .. &quot; + i; channel.basicPublish(&quot;&quot;, QUEUE_NAME, null, message.getBytes()); System.out.println(&quot; [x] Sent &#x27;&quot; + message + &quot;&#x27;&quot;); //Thread.sleep(i * 2); &#125; // 关闭通道和连接 channel.close(); connection.close(); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435// 消费者1public class Recv &#123; private final static String QUEUE_NAME = &quot;test_work_queue&quot;; public static void main(String[] argv) throws Exception &#123; // 获取到连接 Connection connection = ConnectionUtil.getConnection(); // 获取通道 final Channel channel = connection.createChannel(); // 声明队列 channel.queueDeclare(QUEUE_NAME, false, false, false, null); // 设置每个消费者同时只能处理一条消息 //channel.basicQos(1); // 定义队列的消费者 DefaultConsumer consumer = new DefaultConsumer(channel) &#123; // 获取消息，并且处理，这个方法类似事件监听，如果有消息的时候，会被自动调用 @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; // body 即消息体 String msg = new String(body); System.out.println(&quot; [消费者1] received : &quot; + msg + &quot;!&quot;); try &#123; // 模拟完成任务的耗时：1000ms Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; &#125; // 手动ACK channel.basicAck(envelope.getDeliveryTag(), false); &#125; &#125;; // 监听队列。 channel.basicConsume(QUEUE_NAME, false, consumer); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435//消费者2public class Recv2 &#123; private final static String QUEUE_NAME = &quot;test_work_queue&quot;; public static void main(String[] argv) throws Exception &#123; // 获取到连接 Connection connection = ConnectionUtil.getConnection(); // 获取通道 final Channel channel = connection.createChannel(); // 声明队列 channel.queueDeclare(QUEUE_NAME, false, false, false, null); // 设置每个消费者同时只能处理一条消息 //channel.basicQos(1); // 定义队列的消费者 DefaultConsumer consumer = new DefaultConsumer(channel) &#123; // 获取消息，并且处理，这个方法类似事件监听，如果有消息的时候，会被自动调用 @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; // body 即消息体 String msg = new String(body); System.out.println(&quot; [消费者2] received : &quot; + msg + &quot;!&quot;); try &#123; // 模拟完成任务的耗时：1000ms Thread.sleep(200); &#125; catch (InterruptedException e) &#123; &#125; // 手动ACK channel.basicAck(envelope.getDeliveryTag(), false); &#125; &#125;; // 监听队列。 channel.basicConsume(QUEUE_NAME, false, consumer); &#125;&#125; 3.订阅模型分类​ 在之前的模式中，我们创建了一个工作队列。 工作队列背后的假设是：每个任务只被传递给一个工作人员。在这一部分，我们将做一些完全不同的事情 - 我们将会传递一个信息给多个消费者。 这种模式被称为“发布/订阅”。 1个生产者，多个消费者 每一个消费者都有自己的一个队列 生产者没有将消息直接发送到队列，而是发送到了交换机 每个队列都要绑定到交换机 生产者发送的消息，经过交换机到达队列，实现一个消息被多个消费者获取的目的 X（Exchanges）： 交换机一方面：接收生产者发送的消息。另一方面：知道如何处理消息，例如递交给某个特别队列、递交给所有队列、或是将消息丢弃。到底如何操作，取决于Exchange的类型。 Exchange（交换机）只负责转发消息，不具备存储消息的能力，因此如果没有任何队列与Exchange绑定，或者没有符合路由规则的队列，那么消息会丢失！ Exchange类型有以下几种： Fanout：广播，将消息交给所有绑定到交换机的队列 Direct：定向，把消息交给符合指定routing key 的队列 Topic：通配符，把消息交给符合routing pattern（路由模式） 的队列 ​ 4.订阅模型-Fanout也称为广播。在广播模式下，消息发送流程是这样的： 可以有多个消费者 每个消费者有自己的queue（队列） 每个队列都要绑定到Exchange（交换机） ​生产者发送的消息，只能发送到交换机，交换机来决定要发给哪个队列，生产者无法决定。 交换机把消息发送给绑定过的所有队列 队列的消费者都能拿到消息。实现一条消息被多个消费者消费 ​ 1234567891011121314151617181920212223public class Send &#123; private final static String EXCHANGE_NAME = &quot;fanout_exchange_test&quot;; public static void main(String[] argv) throws Exception &#123; // 获取到连接 Connection connection = ConnectionUtil.getConnection(); // 获取通道 Channel channel = connection.createChannel(); // 声明exchange，指定类型为fanout channel.exchangeDeclare(EXCHANGE_NAME, &quot;fanout&quot;); // 消息内容 String message = &quot;Hello everyone&quot;; // 发布消息到Exchange channel.basicPublish(EXCHANGE_NAME, &quot;&quot;, null, message.getBytes()); System.out.println(&quot; [生产者] Sent &#x27;&quot; + message + &quot;&#x27;&quot;); channel.close(); connection.close(); &#125;&#125; 1234567891011121314151617181920212223242526272829303132//消费者1public class Recv &#123; private final static String QUEUE_NAME = &quot;fanout_exchange_queue_1&quot;; private final static String EXCHANGE_NAME = &quot;fanout_exchange_test&quot;; public static void main(String[] argv) throws Exception &#123; // 获取到连接 Connection connection = ConnectionUtil.getConnection(); // 获取通道 Channel channel = connection.createChannel(); // 声明队列 channel.queueDeclare(QUEUE_NAME, false, false, false, null); // 绑定队列到交换机 channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, &quot;&quot;); // 定义队列的消费者 DefaultConsumer consumer = new DefaultConsumer(channel) &#123; // 获取消息，并且处理，这个方法类似事件监听，如果有消息的时候，会被自动调用 @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; // body 即消息体 String msg = new String(body); System.out.println(&quot; [消费者1] received : &quot; + msg + &quot;!&quot;); &#125; &#125;; // 监听队列，自动返回完成 channel.basicConsume(QUEUE_NAME, true, consumer); &#125;&#125; 1234567891011121314151617181920212223242526272829303132// 消费者2public class Recv2 &#123; private final static String QUEUE_NAME = &quot;fanout_exchange_queue_2&quot;; private final static String EXCHANGE_NAME = &quot;fanout_exchange_test&quot;; public static void main(String[] argv) throws Exception &#123; // 获取到连接 Connection connection = ConnectionUtil.getConnection(); // 获取通道 Channel channel = connection.createChannel(); // 声明队列 channel.queueDeclare(QUEUE_NAME, false, false, false, null); // 绑定队列到交换机 channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, &quot;&quot;); // 定义队列的消费者 DefaultConsumer consumer = new DefaultConsumer(channel) &#123; // 获取消息，并且处理，这个方法类似事件监听，如果有消息的时候，会被自动调用 @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; // body 即消息体 String msg = new String(body); System.out.println(&quot; [消费者2] received : &quot; + msg + &quot;!&quot;); &#125; &#125;; // 监听队列，手动返回完成 channel.basicConsume(QUEUE_NAME, true, consumer); &#125;&#125; 5.订阅模型-Direct​ 在某些场景下，我们希望不同的消息被不同的队列消费。这时就要用到Direct类型的Exchange。 在Direct模型下，队列与交换机的绑定，不能是任意绑定了，而是要指定一个RoutingKey（路由key） 消息的发送方在向Exchange发送消息时，也必须指定消息的routing key。 P：生产者，向Exchange发送消息，发送消息时，会指定一个routing key。 X：Exchange（交换机），接收生产者的消息，然后把消息递交给 与routing key完全匹配的队列 C1：消费者，其所在队列指定了需要routing key 为 error 的消息 C2：消费者，其所在队列指定了需要routing key 为 info、error、warning 的消息 此处我们模拟商品的增删改，发送消息的RoutingKey分别是：insert、update、delete​ 1234567891011121314151617181920212223/** * 生产者，模拟为商品服务 */public class Send &#123; private final static String EXCHANGE_NAME = &quot;direct_exchange_test&quot;; public static void main(String[] argv) throws Exception &#123; // 获取到连接 Connection connection = ConnectionUtil.getConnection(); // 获取通道 Channel channel = connection.createChannel(); // 声明exchange，指定类型为direct channel.exchangeDeclare(EXCHANGE_NAME, &quot;direct&quot;); // 消息内容 String message = &quot;商品删除了， id = 1001&quot;; // 发送消息，并且指定routing key 为：insert ,代表新增商品 channel.basicPublish(EXCHANGE_NAME, &quot;delete&quot;, null, message.getBytes()); System.out.println(&quot; [商品服务：] Sent &#x27;&quot; + message + &quot;&#x27;&quot;); channel.close(); connection.close(); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334/** * 消费者1 */public class Recv &#123; private final static String QUEUE_NAME = &quot;direct_exchange_queue_1&quot;; private final static String EXCHANGE_NAME = &quot;direct_exchange_test&quot;; public static void main(String[] argv) throws Exception &#123; // 获取到连接 Connection connection = ConnectionUtil.getConnection(); // 获取通道 Channel channel = connection.createChannel(); // 声明队列 channel.queueDeclare(QUEUE_NAME, false, false, false, null); // 绑定队列到交换机，同时指定需要订阅的routing key。假设此处需要update和delete消息 channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, &quot;update&quot;); channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, &quot;delete&quot;); // 定义队列的消费者 DefaultConsumer consumer = new DefaultConsumer(channel) &#123; // 获取消息，并且处理，这个方法类似事件监听，如果有消息的时候，会被自动调用 @Override public void handleDelivery(String consumerTag, Envelope envelope, BasicProperties properties, byte[] body) throws IOException &#123; // body 即消息体 String msg = new String(body); System.out.println(&quot; [消费者1] received : &quot; + msg + &quot;!&quot;); &#125; &#125;; // 监听队列，自动ACK channel.basicConsume(QUEUE_NAME, true, consumer); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435/** * 消费者2 */public class Recv2 &#123; private final static String QUEUE_NAME = &quot;direct_exchange_queue_2&quot;; private final static String EXCHANGE_NAME = &quot;direct_exchange_test&quot;; public static void main(String[] argv) throws Exception &#123; // 获取到连接 Connection connection = ConnectionUtil.getConnection(); // 获取通道 Channel channel = connection.createChannel(); // 声明队列 channel.queueDeclare(QUEUE_NAME, false, false, false, null); // 绑定队列到交换机，同时指定需要订阅的routing key。订阅 insert、update、delete channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, &quot;insert&quot;); channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, &quot;update&quot;); channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, &quot;delete&quot;); // 定义队列的消费者 DefaultConsumer consumer = new DefaultConsumer(channel) &#123; // 获取消息，并且处理，这个方法类似事件监听，如果有消息的时候，会被自动调用 @Override public void handleDelivery(String consumerTag, Envelope envelope, BasicProperties properties, byte[] body) throws IOException &#123; // body 即消息体 String msg = new String(body); System.out.println(&quot; [消费者2] received : &quot; + msg + &quot;!&quot;); &#125; &#125;; // 监听队列，自动ACK channel.basicConsume(QUEUE_NAME, true, consumer); &#125;&#125; 6.订阅模型-Topic Topic类型的Exchange与Direct相比，都是可以根据RoutingKey把消息路由到不同的队列。只不过Topic类型Exchange可以让队列在绑定Routing key 的时候使用通配符！ Routingkey 一般都是有一个或多个单词组成，多个单词之间以”.”分割，例如： item.insert 通配符规则： #：匹配0或多个词（含零个）​ *：匹配不多不少恰好1个词（不含零个） 在这个例子中，我们将发送所有描述动物的消息。消息将使用由三个字（两个点）组成的routing key发送。路由关键字中的第一个单词将描述速度，第二个颜色和第三个种类：“**..**”。 我们创建三个绑定： Q1绑定了绑定键“_ .orange.”，Q2绑定了“._.rabbit”和“lazy.＃”。 · Q1匹配所有的橙色动物。 · Q2匹配关于兔子以及懒惰动物的消息。 练习，生产者发送如下消息，会进入那个队列： quick.orange.rabbit Q1 Q2lazy.orange.elephant Q1 Q2quick.orange.fox Q1lazy.pink.rabbit Q2quick.brown.fox 都不能quick.orange.male.rabbit 都不能orange 都不能 123456789101112131415161718192021222324/** * 生产者，模拟为商品服务 */public class Send &#123; private final static String EXCHANGE_NAME = &quot;topic_exchange_test&quot;; public static void main(String[] argv) throws Exception &#123; // 获取到连接 Connection connection = ConnectionUtil.getConnection(); // 获取通道 Channel channel = connection.createChannel(); // 声明exchange，指定类型为topic channel.exchangeDeclare(EXCHANGE_NAME, &quot;topic&quot;,true); // 消息内容 String message = &quot;更新商品 : id = 1001&quot;; // 发送消息，并且指定routing key 为：insert ,代表新增商品 channel.basicPublish(EXCHANGE_NAME, &quot;item.update&quot;, MessageProperties.PERSISTENT_TEXT_PLAIN, message.getBytes()); System.out.println(&quot; [商品服务：] Sent &#x27;&quot; + message + &quot;&#x27;&quot;); channel.close(); connection.close(); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334/** * 消费者1 */public class Recv &#123; private final static String QUEUE_NAME = &quot;topic_exchange_queue_1&quot;; private final static String EXCHANGE_NAME = &quot;topic_exchange_test&quot;; public static void main(String[] argv) throws Exception &#123; // 获取到连接 Connection connection = ConnectionUtil.getConnection(); // 获取通道 Channel channel = connection.createChannel(); // 声明队列 channel.queueDeclare(QUEUE_NAME, true, false, false, null); // 绑定队列到交换机，同时指定需要订阅的routing key。需要 update、delete channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, &quot;item.update&quot;); channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, &quot;item.delete&quot;); // 定义队列的消费者 DefaultConsumer consumer = new DefaultConsumer(channel) &#123; // 获取消息，并且处理，这个方法类似事件监听，如果有消息的时候，会被自动调用 @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; // body 即消息体 String msg = new String(body); System.out.println(&quot; [消费者1] received : &quot; + msg + &quot;!&quot;); &#125; &#125;; // 监听队列，自动ACK channel.basicConsume(QUEUE_NAME, true, consumer); &#125;&#125; 123456789101112131415161718192021222324252627282930313233/** * 消费者2 */public class Recv2 &#123; private final static String QUEUE_NAME = &quot;topic_exchange_queue_2&quot;; private final static String EXCHANGE_NAME = &quot;topic_exchange_test&quot;; public static void main(String[] argv) throws Exception &#123; // 获取到连接 Connection connection = ConnectionUtil.getConnection(); // 获取通道 Channel channel = connection.createChannel(); // 声明队列 channel.queueDeclare(QUEUE_NAME, false, false, false, null); // 绑定队列到交换机，同时指定需要订阅的routing key。订阅 insert、update、delete channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, &quot;item.*&quot;); // 定义队列的消费者 DefaultConsumer consumer = new DefaultConsumer(channel) &#123; // 获取消息，并且处理，这个方法类似事件监听，如果有消息的时候，会被自动调用 @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; // body 即消息体 String msg = new String(body); System.out.println(&quot; [消费者2] received : &quot; + msg + &quot;!&quot;); &#125; &#125;; // 监听队列，自动ACK channel.basicConsume(QUEUE_NAME, true, consumer); &#125;&#125; 三，可靠性投递使用 RabbitMQ 实现异步通信的时候，消息丢了怎么办，消息重复消费怎么办？ 在 RabbitMQ 里面提供了很多保证消息可靠投递的机制，这个也是 RabbitMQ 的一个特性。 首先要明确一个问题，因为效率与可靠性是无法兼得的，如果要保证每一个环节都成功，势必会对消息的收发效率造成影响。所以如果是一些业务实时一致性要求不是特别高的场合，可以牺牲一些可靠性来换取效率。比如发送通知或者记录日志的这种场景，如果用户没有收到通知，不会造成业务影响，只要再次发送就可以了。 在我们使用 RabbitMQ 收发消息的时候，有几个主要环节 ①代表消息从生产者发送到 Broker 生产者把消息发到 Broker 之后，怎么知道自己的消息有没有被 Broker 成功接收？ ②代表消息从 Exchange 路由到 Queue Exchange 是一个绑定列表，如果消息没有办法路由到正确的队列，会发生什么事情？应该怎么处理？ ③代表消息在 Queue 中存储 队列是一个独立运行的服务，有自己的数据库（Mnesia），它是真正用来存储消息的。如果还没有消费者来消费，那么消息要一直存储在队列里面。如果队列出了问题，消息肯定会丢失。怎么保证消息在队列稳定地存储呢？ ④代表消费者订阅 Queue 并消费消息 队列的特性是什么？FIFO。队列里面的消息是一条一条的投递的，也就是说，只有上一条消息被消费者接收以后，才能把这一条消息从数据库删掉，继续投递下一条消息。那么问题来了，Broker 怎么知道消费者已经接收了消息呢？ 1.消息发送到rabbitMQ服务器这个环节可能因为网络或者 Broker 的问题导致消息发送失败，生产者不能确定 Broker 有没有正确的接收。 在 RabbitMQ 里面提供了两种机制服务端确认机制，也就是在生产者发送消息给RabbitMQ 的服务端的时候，服务端会通过某种方式返回一个应答，只要生产者收到了这个应答，就知道消息发送成功了。第一种是 Transaction（事务）模式，第二种 Confirm（确认）模式。 1.1 Transaction模式事务模式怎么使用呢？ 我们通过一个 channel.txSelect()的方法把信道设置成事务模式，然后就可以发布消息给 RabbitMQ 了，如果 channel.txCommit();的方法调用成功，就说明事务提交成功，则消息一定到达了 RabbitMQ 中。 如果在事务提交执行之前由于 RabbitMQ 异常崩溃或者其他原因抛出异常，这个时候我们便可以将其捕获，进而通过执行 channel.txRollback()方法来实现事务回滚。 在事务模式里面，只有收到了服务端的 Commit-OK 的指令，才能提交成功。所以可以解决生产者和服务端确认的问题。但是事务模式有一个缺点，它是阻塞的，一条消息没有发送完毕，不能发送下一条消息，它会榨干 RabbitMQ 服务器的性能。所以不建议大家在生产环境使用。 Spring Boot 中的设置 1rabbitTemplate.setChannelTransacted(true); 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/** * @author yhd * @createtime 2021/1/24 23:29 * rabbitmq事务 */@Componentpublic class MqTx &#123; private static final String EXCHANGE = &quot;exchange.tx&quot;; private static final String QUEUE = &quot;queue.tx&quot;; private static final String ROUTING_KEY = &quot;routing.tx&quot;; @Resource private RabbitTemplate rabbitTemplate; public boolean sendMessage() &#123; rabbitTemplate.setChannelTransacted(true); rabbitTemplate.setConfirmCallback((correlationData, flag, cause) -&gt; &#123; if (flag) &#123; System.out.println(&quot;发送成功！&quot;); &#125; else &#123; System.out.println(&quot;发送失败&quot; + cause); &#125; &#125;); return true; &#125; @RabbitListener(bindings = @QueueBinding( value = @Queue(value = QUEUE, autoDelete = &quot;false&quot;, durable = &quot;true&quot;), exchange = @Exchange(value = EXCHANGE, autoDelete = &quot;true&quot;, durable = &quot;true&quot;), key = &#123;ROUTING_KEY&#125; ) ) public void receiveMessage(String msg, Message message, Channel channel) &#123; try &#123; channel.txSelect(); System.out.println(&quot;msg = &quot; + msg); channel.basicAck(message.getMessageProperties().getDeliveryTag(), false); channel.txCommit(); &#125; catch (IOException e) &#123; try &#123; channel.txRollback(); &#125; catch (IOException ioException) &#123; ioException.printStackTrace(); &#125; &#125; &#125;&#125; 那么有没有其他可以保证消息被 Broker 接收，但是又不大量消耗性能的方式呢？这就是第二种模式，叫做确认（Confirm）模式。 1.2 Confirm （ 确认 ）模式确认模式有三种，一种是普通确认模式。​ 普通确认模式 在生产者这边通过调用 channel.confirmSelect()方法将信道设置为 Confirm 模式，然后发送消息。一旦消息被投递到所有匹配的队列之后，RabbitMQ 就会发送一个确认（Basic.Ack）给生产者，也就是调用 channel.waitForConfirms()返回 true，这样生产者就知道消息被服务端接收了。 批量确认 这种发送 1 条确认 1 条的方式消息还不是太高，所以我们还有一种批量确认的方式。批 量 确 认 ， 就 是 在 开 启 Confirm 模 式 后 ， 先 发 送 一 批 消 息 。 只 要channel.waitForConfirmsOrDie();方法没有抛出异常，就代表消息都被服务端接收了。 批量确认的方式比单条确认的方式效率要高，但是也有两个问题，第一个就是批量的数量的确定。对于不同的业务，到底发送多少条消息确认一次？数量太少，效率提升不上去。数量多的话，又会带来另一个问题，比如我们发 1000 条消息才确认一次，如果前面 999 条消息都被服务端接收了，如果第 1000 条消息被拒绝了，那么前面所有的消息都要重发。 异步确认 ​ 有没有一种方式，可以一边发送一边确认的呢？这个就是异步确认模式。 异步确认模式需要添加一个 ConfirmListener，并且用一个 SortedSet 来维护没有被确认的消息。 Confirm 模式是在 Channel 上开启的，因为 RabbitTemplate 对 Channel 进行了封装，叫做 ConfimrCallback。 123456rabbitTemplate.setConfirmCallback((correlationData, ack, cause) -&gt; &#123; if (!ack) &#123; System.out.println(&quot;发送消息失败：&quot; + cause); throw new RuntimeException(&quot;发送异常：&quot; + cause); &#125;&#125;); 2. 消息从交换机路由到队列在什么情况下，消息会无法路由到正确的队列？可能因为路由键错误，或者队列不存在。 有两种方式处理无法路由的消息，一种就是让服务端重发给生产者，一种是让交换机路由到另一个备份的交换机。 消息回发的方式：使用 mandatory 参数和 ReturnListener（在 Spring AMQP 中是ReturnCallback）。 1234567891011121314rabbitTemplate.setMandatory(true);rabbitTemplate.setReturnCallback((Message message, int replyCode, String replyText, String exchange, String routingKey) -&gt; &#123; // 反序列化对象输出 log.info(&quot;消息主体: &#123;&#125;&quot;, new String(message.getBody())); log.info(&quot;应答码: &#123;&#125;&quot;, replyCode); log.info(&quot;描述：&#123;&#125;&quot;, replyText); log.info(&quot;消息使用的交换器 exchange : &#123;&#125;&quot;, exchange); log.info(&quot;消息使用的路由键 routing : &#123;&#125;&quot;, routingKey);&#125;); 消息路由到备份交换机的方式：在创建交换机的时候，从属性中指定备份交换机。 123456789101112131415161718192021222324252627private static final String EXCHANGE_NAME = &quot;amqp.yhd.exchange&quot;;private static final String EXCHANGE_NAME_COPY = &quot;amqp.yhd.exchange.copy&quot;;private static final String QUEUE_NAME = &quot;amqp.yhd.queue&quot;;private static final String ROUTING_KEY = &quot;amqp.admin&quot;;/** * AmqpAdmin * * @param factory * @return */@Beanpublic AmqpAdmin amqpAdmin(ConnectionFactory factory) &#123; RabbitAdmin admin = new RabbitAdmin(factory); //给交换机指定备份交换机 Map&lt;String,Object&gt; arguments = new HashMap(); arguments.put(&quot;alternate-exchange&quot;,EXCHANGE_NAME_COPY); //声明一个交换机 交换机名 是否持久化 是否自动删除 admin.declareExchange(new DirectExchange(EXCHANGE_NAME, true, false,arguments)); //队列名 持久化 是否批处理 自动删除 admin.declareQueue(new Queue(QUEUE_NAME, true, false, false)); //声明一个绑定 队列名 ，绑定类型，交换机名，路由键 参数 admin.declareBinding(new Binding(QUEUE_NAME, Binding.DestinationType.QUEUE, EXCHANGE_NAME, ROUTING_KEY, null)); return admin;&#125; 队列可以指定死信交换机；交换机可以指定备份交换机 3.消息在队列存储如果没有消费者的话，队列一直存在在数据库中。 如果 RabbitMQ 的服务或者硬件发生故障，比如系统宕机、重启、关闭等等，可能会导致内存中的消息丢失，所以我们要把消息本身和元数据（队列、交换机、绑定）都保存到磁盘。 解决方案队列持久化+交换机持久化+消息持久化 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849@Slf4j@Componentpublic class MQProducerAckTest &#123; @Autowired private RabbitTemplate rabbitTemplate; private static final String EXCHANGE = &quot;exchange.confirm&quot;; private static final String QUEUE = &quot;queue.confirm&quot;; private static final String ROUTING_KEY = &quot;routing.confirm&quot;; @Bean public AmqpAdmin amqpAdmin(ConnectionFactory factory) &#123; RabbitAdmin admin = new RabbitAdmin(factory); //声明一个交换机 交换机名 是否持久化 是否自动删除 admin.declareExchange(new DirectExchange(EXCHANGE, true, false, null)); //队列名 持久化 是否批处理 自动删除 admin.declareQueue(new org.springframework.amqp.core.Queue(QUEUE, true, false, false)); //声明一个绑定 队列名 ，绑定类型，交换机名，路由键 参数 admin.declareBinding(new Binding(QUEUE, Binding.DestinationType.QUEUE, EXCHANGE, ROUTING_KEY, null)); return admin; &#125; /** * 发送消息 * * @param exchange 交换机 * @param routingKey 路由键 * @param message 消息 */ public boolean sendMessage(String exchange, String routingKey, String message) &#123; MessageProperties messageProperties = new MessageProperties(); messageProperties.setDeliveryMode(MessageDeliveryMode.PERSISTENT); Message msg = new Message(message.getBytes(), messageProperties); rabbitTemplate.convertAndSend(exchange, routingKey, msg); return true; &#125; @SneakyThrows @RabbitListener(bindings = @QueueBinding( value = @Queue(value = QUEUE, autoDelete = &quot;false&quot;, durable = &quot;true&quot;), exchange = @Exchange(value = EXCHANGE, autoDelete = &quot;true&quot;, durable = &quot;true&quot;), key = &#123;ROUTING_KEY&#125;)) public void process(Message message, Channel channel) &#123; log.info(&quot;RabbitListener:&#123;&#125;&quot;, new String(message.getBody())); channel.basicAck(message.getMessageProperties().getDeliveryTag(), false); &#125;&#125; 集群 如果只有一个 RabbitMQ 的节点，即使交换机、队列、消息做了持久化，如果服务崩溃或者硬件发生故障，RabbitMQ 的服务一样是不可用的，所以为了提高 MQ 服务的可用性，保障消息的传输，我们需要有多个 RabbitMQ 的节点。 4.消息投递到消费者如果消费者收到消息后没来得及处理即发生异常，或者处理过程中发生异常，会导致接收消息失败。服务端应该以某种方式得知消费者对消息的接收情况，并决定是否重新投递这条消息给其他消费者。RabbitMQ 提供了消费者的消息确认机制（message acknowledgement），消费者可以自动或者手动地发送 ACK 给服务端。 没有收到 ACK 的消息，消费者断开连接后，RabbitMQ 会把这条消息发送给其他消费者。如果没有其他消费者，消费者重启后会重新消费这条消息，重复执行业务逻辑。 消费者在订阅队列时，可以指定autoAck参数，当autoAck等于false时，RabbitMQ会等待消费者显式地回复确认信号后才从队列中移去消息。 如何设置手动 ACK？ SimpleRabbitListenerContainer 或者 SimpleRabbitListenerContainerFactory 1factory.setAcknowledgeMode(AcknowledgeMode.MANUAL); application.properties 12spring.rabbitmq.listener.direct.acknowledge-mode=manualspring.rabbitmq.listener.simple.acknowledge-mode=manual 注意这三个值的区别：NONE：自动 ACK，MANUAL： 手动 ACK，AUTO：如果方法未抛出异常，则发送 ack。 当抛出 AmqpRejectAndDontRequeueException 异常的时候，则消息会被拒绝，且不重新入队。当抛出 ImmediateAcknowledgeAmqpException 异常，则消费者会发送 ACK。其他的异常，则消息会被拒绝，且 requeue = true 会重新入队。 在 Spring Boot 中，消费者又怎么调用 ACK，或者说怎么获得 Channel 参数呢？ 123456789@SneakyThrows@RabbitListener(bindings = @QueueBinding( value = @Queue(value = QUEUE, autoDelete = &quot;false&quot;, durable = &quot;true&quot;), exchange = @Exchange(value = EXCHANGE, autoDelete = &quot;true&quot;, durable = &quot;true&quot;), key = &#123;ROUTING_KEY&#125;))public void process(Message message, Channel channel) &#123; log.info(&quot;RabbitListener:&#123;&#125;&quot;, new String(message.getBody())); channel.basicAck(message.getMessageProperties().getDeliveryTag(), false);&#125; 如果消息无法处理或者消费失败，也有两种拒绝的方式，Basic.Reject()拒绝单条，Basic.Nack()批量拒绝。如果 requeue 参数设置为 true，可以把这条消息重新存入队列，以便发给下一个消费者（当然，只有一个消费者的时候，这种方式可能会出现无限循环重复消费的情况。可以投递到新的队列中，或者只打印异常日志）。 服务端收到了 ACK 或者 NACK，即使消费者没有接收到消息，或者消费时出现异常，生产者也是完全不知情的。 5.消费者回调 调用生产者 API 发送响应消息给生产者 6.补偿机制如果生产者的 API 就是没有被调用，也没有收到消费者的响应消息，怎么办？ 可能是消费者处理时间太长或者网络超时。 生产者与消费者之间应该约定一个超时时间，比如 5 分钟，对于超出这个时间没有得到响应的消息，可以设置一个定时重发的机制，但要发送间隔和控制次数，比如每隔 2分钟发送一次，最多重发 3 次，否则会造成消息堆积。 重发可以通过（本地消息表）消息落库+（异步）定时任务来实现。 7.消息幂等性如果消费者每一次接收生产者的消息都成功了，只是在响应或者调用 API 的时候出了问题，会不会出现消息的重复处理？ 为了避免相同消息的重复处理，必须要采取一定的措施。RabbitMQ 服务端是没有这种控制的（同一批的消息有个递增的 DeliveryTag），它不知道你是不是就要把一条消息发送两次，只能在消费端控制。 导致消息的重复消费的原因： 生产者的问题，环节①重复发送消息，比如在开启了 Confirm 模式但未收到确认，消费者重复投递。 环节④出了问题，由于消费者未发送 ACK 或者其他原因，消息重复投递。 生产者代码或者网络问题。 对于重复发送的消息，可以对每一条消息生成一个唯一的业务 ID，通过日志或者消息落库来做重复控制。 8.最终一致性如果确实是消费者宕机了，或者代码出现了 BUG 导致无法正常消费，在我们尝试多次重发以后，消息最终也没有得到处理，怎么办？手动处理。 9.消息的顺序性消息的顺序性指的是消费者消费消息的顺序跟生产者生产消息的顺序是一致的。 比如：1、发表微博；2、发表评论；3、删除微博。顺序不能颠倒。 在 RabbitMQ 中，一个队列有多个消费者时，由于不同的消费者消费消息的速度是不一样的，顺序无法保证。只有一个队列仅有一个消费者的情况才能保证顺序消费（不同的业务消息发送到不同的专用的队列）。​ 10.代码123456789101112spring.rabbitmq.host=121.199.31.160spring.rabbitmq.port=5672spring.rabbitmq.username=rootspring.rabbitmq.password=root#交换机确认spring.rabbitmq.publisher-confirms=true#队列确认spring.rabbitmq.publisher-returns=true#默认情况下消息消费者是自动确认消息的，如果要手动确认消息则需要修改确认模式为manualspring.rabbitmq.listener.simple.cknowledge-mode=manual# 消费者每次从队列获取的消息数量。此属性当不设置时为：轮询分发，设置为1为：公平分发spring.rabbitmq.listener.simple.prefetch=1 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * @author yhd * @createtime 2021/1/23 1:38 * @description 消息发送确认 * 关于实现的这两个类： * ConfirmCallback：只确认消息是否正确到达Exchange中 * 1.如果消息没有到exchange,则confirm回调,ack=false * 2.如果消息到达exchange,则confirm回调,ack=true * ReturnCallback：消息没有正确到达队列时触发回调，如果正确到达队列不执行 * 1.exchange到queue成功,则不回调return * 2.exchange到queue失败,则回调return */@Component@Slf4jpublic class MQProducerAckConfig implements RabbitTemplate.ConfirmCallback, RabbitTemplate.ReturnCallback&#123; @Resource private RabbitTemplate rabbitTemplate; /** * 修饰一个非静态的void（）方法,在服务器加载Servlet的时候运行， * 并且只会被服务器执行一次。 * 在构造函数之后执行，init（）方法之前执行。 */ @PostConstruct public void init() &#123; rabbitTemplate.setConfirmCallback(this); //指定 ConfirmCallback rabbitTemplate.setReturnCallback(this); //指定 ReturnCallback &#125; @Override public void confirm(CorrelationData correlationData, boolean ack, String cause) &#123; if (ack) &#123; log.info(&quot;消息发送成功：&quot; + GsonUtil.toJson(correlationData)); &#125; else &#123; log.info(&quot;消息发送失败：&quot; + cause + &quot; 数据：&quot; + GsonUtil.toJson(correlationData)); &#125; &#125; @Override public void returnedMessage(Message message, int replyCode, String replyText, String exchange, String routingKey) &#123; // 反序列化对象输出 log.info(&quot;消息主体: &#123;&#125;&quot;,new String(message.getBody())); log.info(&quot;应答码: &#123;&#125;&quot;,replyCode); log.info(&quot;描述：&#123;&#125;&quot;,replyText); log.info(&quot;消息使用的交换器 exchange : &#123;&#125;&quot;,exchange); log.info(&quot;消息使用的路由键 routing : &#123;&#125;&quot;,routingKey); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859/** * @author yhd * @createtime 2021/1/23 1:59 * 测试消息发送确认 */@Slf4j@Componentpublic class MQProducerAckTest &#123; @Autowired private RabbitTemplate rabbitTemplate; private static final String EXCHANGE = &quot;exchange.confirm&quot;; private static final String QUEUE = &quot;queue.confirm&quot;; private static final String ROUTING_KEY = &quot;routing.confirm&quot;; /** * 发送消息 * * @param exchange 交换机 * @param routingKey 路由键 * @param message 消息 */ public boolean sendMessage(String exchange, String routingKey, Object message) &#123; rabbitTemplate.convertAndSend(exchange, routingKey, message); return true; &#125; //接收消息 @SneakyThrows @RabbitListener(bindings = @QueueBinding( value = @Queue(value = QUEUE, autoDelete = &quot;false&quot;), exchange = @Exchange(value = EXCHANGE, autoDelete = &quot;true&quot;), key = &#123;ROUTING_KEY&#125;)) public void process(Message message, Channel channel) &#123; log.info(&quot;RabbitListener:&#123;&#125;&quot;, new String(message.getBody())); // 采用手动应答模式, 手动确认应答更为安全稳定 //如果手动确定了，再出异常，mq不会通知；如果没有手动确认，抛异常mq会一直通知 try &#123; int i = 1 / 0; // false 确认一个消息，true 批量确认 channel.basicAck(message.getMessageProperties().getDeliveryTag(), false); &#125; catch (Exception e) &#123; // 消息是否再次被拒绝！ System.out.println(&quot;come on!&quot;); // getRedelivered() 判断是否已经处理过一次消息！ if (message.getMessageProperties().getRedelivered()) &#123; System.out.println(&quot;消息已重复处理,拒绝再次接收&quot;); // 拒绝消息，requeue=false 表示不再重新入队，如果配置了死信队列则进入死信队列 channel.basicReject(message.getMessageProperties().getDeliveryTag(), false); &#125; else &#123; System.out.println(&quot;消息即将再次返回队列处理&quot;); // 参数二：是否批量， 参数三：为是否重新回到队列，true重新入队 channel.basicNack(message.getMessageProperties().getDeliveryTag(), false, true); &#125; &#125; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263/** * @author yhd * @createtime 2021/1/24 23:29 * rabbitmq事务 */@Componentpublic class MqTx &#123; private static final String EXCHANGE = &quot;exchange.tx&quot;; private static final String QUEUE = &quot;queue.tx&quot;; private static final String ROUTING_KEY = &quot;routing.tx&quot;; @Resource private RabbitTemplate rabbitTemplate; @Resource private TransactionTemplate transactionTemplate; public boolean sendMessage() &#123; rabbitTemplate.setConfirmCallback((correlationData, flag, cause) -&gt; &#123; if (flag) &#123; System.out.println(&quot;发送成功！&quot;); &#125; else &#123; System.out.println(&quot;发送失败&quot; + cause); &#125; &#125;); rabbitTemplate.setReturnCallback((RabbitTemplate.ReturnsCallback) returnedMessage -&gt; &#123; &#125;); transactionTemplate.execute(transactionStatus -&gt; &#123; rabbitTemplate.convertAndSend(EXCHANGE, ROUTING_KEY, &quot;rabbitmq-tx&quot;); return rabbitTemplate.receiveAndConvert(); &#125;); return true; &#125; @RabbitListener(bindings = @QueueBinding( value = @Queue(value = QUEUE, autoDelete = &quot;false&quot;, durable = &quot;true&quot;), exchange = @Exchange(value = EXCHANGE, autoDelete = &quot;true&quot;, durable = &quot;true&quot;), key = &#123;ROUTING_KEY&#125; ) ) public void receiveMessage(String msg, Message message, Channel channel) &#123; try &#123; channel.txSelect(); System.out.println(&quot;msg = &quot; + msg); channel.basicAck(message.getMessageProperties().getDeliveryTag(), false); channel.txCommit(); &#125; catch (IOException e) &#123; try &#123; channel.txRollback(); &#125; catch (IOException ioException) &#123; ioException.printStackTrace(); &#125; &#125; &#125;&#125; 四，消息流控1.服务端流控当生产 MQ 消息的速度远大于消费消息的速度时，会产生大量的消息堆积，占用系统资源，导致机器的性能下降。我们想要控制服务端接收的消息的数量，应该怎么做呢？ 1.1设置队列长度队列有两个控制长度的属性 12x-max-length：队列中最大存储最大消息数，超过这个数量，队头的消息会被丢弃。x-max-length-bytes：队列中存储的最大消息容量（单位 bytes），超过这个容量，队头的消息会被丢弃。 设置队列长度只在消息堆积的情况下有意义，而且会删除先入队的消息，不能真正地实现服务端限流。 1.2.内存控制RabbitMQ 会在启动时检测机器的物理内存数值。默认当 MQ 占用 40% 以上内存时，MQ 会主动抛出一个内存警告并阻塞所有连接。可以通过修改rabbitmq.config 文件来调整内存阈值，默认值是 0.4。 1[&#123;rabbit, [&#123;vm_memory_high_watermark, 0.4&#125;]&#125;] 也可以用命令动态设置，如果设置成 0，则所有的消息都不能发布。 1rabbitmqctl set_vm_memory_high_watermark 0.3 1.3磁盘控制通过磁盘来控制消息的发布。当磁盘空间低于指定的值时（默认50MB），触发流控措施。 例如：指定为磁盘的 30%或者 2GB 12disk_free_limit.relative = 3.0disk_free_limit.absolute = 2GB 2.消费端限流2.1官网描述默认情况下，如果不进行配置，RabbitMQ 会尽可能快速地把队列中的消息发送到消费者。因为消费者会在本地缓存消息，如果消息数量过多，可能会导致 OOM 或者影响其他进程的正常运行。 在消费者处理消息的能力有限，例如消费者数量太少，或者单条消息的处理时间过长的情况下，如果我们希望在一定数量的消息消费完之前，不再推送消息过来，就要用到消费端的流量限制措施。 可以基于 Consumer 或者 channel 设置 prefetch count 的值，含义为 Consumer端的最大的 unacked messages 数目。当超过这个数值的消息未被确认，RabbitMQ 会停止投递新的消息给该消费者。 2.2代码配置RabbitMQ12channel.basicQos(2); // 如果超过 2 条消息没有发送 ACK，当前消费者不再接受队列消息channel.basicConsume(QUEUE_NAME, false, consumer); SimpleMessageListenerContainer 1container.setPrefetchCount(2); Spring Boot 配置1spring.rabbitmq.listener.simple.prefetch=2 channel 的 prefetch count 设置为 5。当消费者有 5 条消息没有给 Broker 发送 ACK后，RabbitMQ 不再给这个消费者投递消息。 3.消息积压，丢失生产环境中，如果消息在队列和交换机发生积压，并已经开始丢失，应该怎么处理？ 临时扩容消费者，先保证现有的业务逻辑，丢失的消息，等待流量高峰期过后，利用程序排查出来，重新灌入MQ队列。也可以考虑将消息临时写入到一个新的topic里，缓解原本的队列压力。 其实还有broker，消息都是存磁盘，但是MQ高吞吐量一个很重要的原因是利用了page Cache ，数据量没特别大的情况下，mq发消息到broker磁盘，此时broker的page cache 中其实也是有这份消息的，当生产者正常消费时，大概率是直接可以从page cache 中拉消息，这个速度是内存级别，page cache没有拉到消息采取磁盘，当消息堆积在broker时，说明生产者生产速度过快，消费者消费不过来，这时broker的page cache被大量的更新，导致消费者拉消息都是去磁盘去读取，page cache失效了，所以扩容消费者数量有用，但是还需要扩容broker的数量。 五，集群与高可用1.为什么要做集群集群主要用于实现高可用与负载均衡。 高可用：如果集群中的某些 MQ 服务器不可用，客户端还可以连接到其他 MQ 服务器。 负载均衡：在高并发的场景下，单台 MQ 服务器能处理的消息有限，可以分发给多台 MQ 服务器。 RabbitMQ 有两种集群模式：普通集群模式和镜像队列模式。 2.RabbitMQ 如何支持集群应用做集群，需要面对数据同步和通信的问题。因为 Erlang 天生具备分布式的特性，所以 RabbitMQ 天然支持集群，不需要通过引入 ZK 或者数据库来实现数据同步。 RabbitMQ 通过/var/lib/rabbitmq/.erlang.cookie 来验证身份，需要在所有节点上保持一致。 3.rabbitMQ的节点类型集群有两种节点类型，一种是磁盘节点（Disc Node），一种是内存节点（RAMNode）。 磁盘节点：将元数据（包括队列名字属性、交换机的类型名字属性、绑定、vhost）放在磁盘中。 内存节点：将元数据放在内存中。 内存节点会将磁盘节点的地址存放在磁盘（不然重启后就没有办法同步数据了）。如果是持久化的消息，会同时存放在内存和磁盘。 集群中至少需要一个磁盘节点用来持久化元数据，否则全部内存节点崩溃时，就无从同步元数据。未指定类型的情况下，默认为磁盘节点。 我们一般把应用连接到内存节点（读写快），磁盘节点用来备份。 集群通过 25672 端口两两通信，需要开放防火墙的端口。 RabbitMQ 集群无法搭建在广域网上 集群的配置步骤 配置 hosts 同步 erlang.cookie 加入集群（join cluster） 4.普通集群普通集群模式下，不同的节点之间只会相互同步元数据。 为什么不直接把队列的内容（消息）在所有节点上复制一份？ 主要是出于存储和同步数据的网络开销的考虑，如果所有节点都存储相同的数据，就无法达到线性地增加性能和存储容量的目的（堆机器）。 假如生产者连接的是节点 3，要将消息通过交换机 A 路由到队列 1，最终消息还是会转发到节点 1 上存储，因为队列 1 的内容只在节点 1 上。 同理，如果消费者连接是节点 2，要从队列 1 上拉取消息，消息会从节点 1 转发到节点 2。其它节点起到一个路由的作用，类似于指针。 普通集群模式不能保证队列的高可用性，因为队列内容不会复制。如果节点失效将导致相关队列不可用，因此我们需要第二种集群模式。 5.镜像集群第二种集群模式叫做镜像队列。 镜像队列模式下，消息内容会在镜像节点间同步，可用性更高。不过也有一定的副作用，系统性能会降低，节点过多的情况下同步的代价比较大。 6.高可用集群搭建成功后，如果有多个内存节点，那么生产者和消费者应该连接到哪个内存节点？如果在我们的代码中根据一定的策略来选择要使用的服务器，那每个地方都要修改，客户端的代码就会出现很多的重复，修改起来也比较麻烦。 所以需要一个负载均衡的组件（例如 HAProxy，LVS，Nignx），由负载的组件来做路由。这个时候，只需要连接到负载组件的 IP 地址就可以了。 负载分为四层负载和七层负载。 四层负载：工作在 OSI 模型的第四层，即传输层（TCP 位于第四层），它是根据 IP端口进行转发（LVS 支持四层负载）。RabbitMQ 是 TCP 的 5672 端口。 （修改报文中目标地址和原地址） 七层负载：工作在第七层，应用层（HTTP 位于第七层）。可以根据请求资源类型分配到后端服务器（Nginx 支持七层负载；HAProxy 支持四层和七层负载）。 （处理请求，代理至服务器） 但是，如果这个负载的组件也挂了呢？ 我们应该需要这样一个组件 它本身有路由（负载）功能，可以监控集群中节点的状态（比如监控HAProxy），如果某个节点出现异常或者发生故障，就把它剔除掉。 为了提高可用性，它也可以部署多个服务，但是只有一个自动选举出来的 MASTER 服务器（叫做主路由器），通过广播心跳消息实现。 MASTER 服务器对外提供一个虚拟 IP，提供各种网络功能。也就是谁抢占到 VIP，就由谁对外提供网络服务。应用端只需要连接到这一个 IP 就行了。 这个协议叫做 VRRP 协议（虚拟路由冗余协议 Virtual Router RedundancyProtocol），这个组件就是 Keepalived，它具有 Load Balance 和 High Availability的功能。​ 六，MQ延迟消息的实现​ mq实现延迟消息有两种方式，一种是基于死信队列，一种是基于延迟插件。​ 1.基于死信队列1.1.理论 消息的TTL 消息的存活时间。RabbitMQ可以对队列和消息分别设置TTL。对队列设置就是队列没有消费者连着的保留时间，也可以对每一个单独的消息做单独的设置。超过了这个时间，我们认为这个消息就死了，称之为死信。如何设置TTL 我们创建一个队列queue.temp，在Arguments 中添加x-message-ttl 为5000 （单位是毫秒），那所在压在这个队列的消息在5秒后会消失。死信交换机 Dead Letter Exchange其实就是一种普通的exchange，和创建其他exchange没有两样。只是在某一个设置Dead Letter Exchange的队列中有消息过期了，会自动触发消息的转发，发送到Dead Letter Exchange中去。 何时进入死信路由 一个消息被Consumer拒收了，并且reject方法的参数里requeue是false。也就是说不会被再次放在队列里，被其他消费者使用。 上面的消息的TTL到了，消息过期了。 队列的长度限制满了。排在前面的消息会被丢弃或者扔到死信路由上。 1.2 代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/** * @author yhd * @createtime 2021/1/23 2:26 * @description 死信交换机配置类 */@SpringBootConfigurationpublic class DeadLetterMqConfig &#123; public static final String EXCHANGE_DEAD = &quot;exchange.dead&quot;; public static final String ROUTING_DEAD_1 = &quot;routing.dead.1&quot;; public static final String ROUTING_DEAD_2 = &quot;routing.dead.2&quot;; public static final String QUEUE_DEAD_1 = &quot;queue.dead.1&quot;; public static final String QUEUE_DEAD_2 = &quot;queue.dead.2&quot;; // 定义交换机 @Bean public DirectExchange exchange()&#123; return new DirectExchange(EXCHANGE_DEAD,true,false,null); &#125; @Bean public Queue queue1()&#123; // 设置如果队列一 出现问题，则通过参数转到EXCHANGE_DEAD，ROUTING_DEAD_2 上！ Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(); // 参数绑定 此处的key 固定值，不能随意写 map.put(&quot;x-dead-letter-exchange&quot;,EXCHANGE_DEAD); map.put(&quot;x-dead-letter-routing-key&quot;,ROUTING_DEAD_2); // 设置延迟时间 map.put(&quot;x-message-ttl&quot;, 10 * 1000); // 队列名称，是否持久化，是否独享、排外的【true:只可以在本次连接中访问】，是否自动删除，队列的其他属性参数 return new Queue(QUEUE_DEAD_1,true,false,false,map); &#125; @Bean public Binding binding()&#123; // 将队列一 通过ROUTING_DEAD_1 key 绑定到EXCHANGE_DEAD 交换机上 return BindingBuilder.bind(queue1()).to(exchange()).with(ROUTING_DEAD_1); &#125; // 这个队列二就是一个普通队列 @Bean public Queue queue2()&#123; return new Queue(QUEUE_DEAD_2,true,false,false,null); &#125; // 设置队列二的绑定规则 @Bean public Binding binding2()&#123; // 将队列二通过ROUTING_DEAD_2 key 绑定到EXCHANGE_DEAD交换机上！ return BindingBuilder.bind(queue2()).to(exchange()).with(ROUTING_DEAD_2); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package com.yhd.rabbitmq.dead;import com.rabbitmq.client.Channel;import lombok.SneakyThrows;import lombok.extern.slf4j.Slf4j;import org.springframework.amqp.core.Message;import org.springframework.amqp.rabbit.annotation.RabbitListener;import org.springframework.amqp.rabbit.core.RabbitTemplate;import org.springframework.stereotype.Component;import javax.annotation.Resource;/** * @author yhd * @createtime 2021/1/23 2:28 */@Component@Slf4jpublic class DeadLetterMqTest &#123; @Resource private RabbitTemplate rabbitTemplate; /** * 消息发送 */ public boolean sendMsg()&#123; rabbitTemplate.convertAndSend(DeadLetterMqConfig.EXCHANGE_DEAD, DeadLetterMqConfig.ROUTING_DEAD_1, &quot;ok&quot;); return true; &#125; /** * 消息接收 * 正常交换机 */ @SneakyThrows @RabbitListener(queues = DeadLetterMqConfig.QUEUE_DEAD_2) public void get(String msg, Message message, Channel channel) &#123; channel.basicNack(message.getMessageProperties().getDeliveryTag(),false,false); log.info(&quot;正常队列接受消息： &#123;&#125;&quot; , msg); &#125; /** * 消息接收 * 死信交换机 * @param msg * @param message * @param channel */ @SneakyThrows @RabbitListener(queues = DeadLetterMqConfig.QUEUE_DEAD_1) public void get2(String msg, Message message, Channel channel) &#123; log.info(&quot;死信队列接收消息: &#123;&#125;&quot; , msg); channel.basicAck(message.getMessageProperties().getDeliveryTag(),false); &#125;&#125; 2.基于延迟插件2.1.理论Rabbitmq实现了一个插件x-delay-message来实现延时队列。​ 基于插件的延迟消息可能有一个小bug（不影响业务），就是生产者发送消息时会回调returnedMessage方法（消息确认时我们配置的回调方法，表示交换机到队列发送失败），其实基于插件的延迟消息是发送成功了的，如果发生该bug，我们可以根据交换机或队列过滤掉该消息，别让他加入重试队列；如果不能接受后续业务我们可以使用死信的方式发送延迟消息。 插件安装​https://www.rabbitmq.com/community-plugins.html 将插件拷贝到plugins目录下 进入plugins目录 执行 rabbitmq-plugins enable rabbitmq_delayed_message_exchange 命令启用插件 重启 rabbitmq 队列不要在RabbitListener上面做绑定，否则不会成功，必须在配置类绑定。 2.2 代码12345678910111213141516171819202122232425262728293031323334/** * @author yhd * @createtime 2021/1/22 14:37 */@SpringBootApplicationpublic class DelayConfig &#123; //延时交换机 public static final String EXCHANGE_DIRECT_ORDER_CANCEL = &quot;spring.boot.test.delay.exchange&quot;; //路由键 public static final String ROUTING_ORDER_CANCEL = &quot;spring.boot.test.delay.routing&quot;; //延迟队列 public static final String QUEUE_ORDER_CANCEL = &quot;spring.boot.test.delay.queue&quot;; // 延迟时间 单位：秒 public static final int DELAY_TIME = 60; @Bean //声明死信队列 public Queue delayQueue() &#123; // 第一个参数是创建的queue的名字，第二个参数是是否支持持久化 return new Queue(QUEUE_ORDER_CANCEL, true); &#125; @Bean //声明私信交换机 public CustomExchange delayExchange() &#123; Map&lt;String, Object&gt; args = new HashMap&lt;String, Object&gt;(); args.put(&quot;x-delayed-type&quot;, &quot;direct&quot;); return new CustomExchange(EXCHANGE_DIRECT_ORDER_CANCEL, &quot;x-delayed-message&quot;, true, false, args); &#125; @Bean //死信交换机绑定死信队列并设置路由键 public Binding bindingDelay() &#123; return BindingBuilder.bind(delayQueue()).to(delayExchange()).with(ROUTING_ORDER_CANCEL).noargs(); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839/** * @author yhd * @createtime 2021/1/22 14:34 * 测试springboot整合mq利用死信队列发送消息并接收 */@Component@Slf4jpublic class SpringBootDelayQueueTest &#123; @Resource private RabbitTemplate rabbitTemplate; @Resource private AmqpTemplate amqpTemplate; /** * 发送消息 */ public void sendMessage() &#123; amqpTemplate.convertAndSend( DelayConfig.EXCHANGE_DIRECT_ORDER_CANCEL,DelayConfig.ROUTING_ORDER_CANCEL, &quot;try send message to delay queue !&quot;, msg -&gt; &#123; msg.getMessageProperties().setDelay(DelayConfig.DELAY_TIME * 1000); return msg; &#125;); &#125; /** * 接收消息 */ @RabbitListener(queues = DelayConfig.QUEUE_ORDER_CANCEL) public void receiveMessage(String msg, Message message, Channel channel) throws Exception &#123; log.info(&quot;the delaty queue received message : &#123;&#125;&quot;, msg); //log.info(&quot;the delaty queue received message : &#123;&#125;&quot;, new String(message.getBody())); channel.basicAck(message.getMessageProperties().getDeliveryTag(), false); &#125;&#125; 七，面试经验总结1.到底在消费者创建还是在生产者创建？如果A项目和B项目有相互发送和接收消息，应该创建几个vhost，几个Exchange？ 交换机和队列，实际上是作为资源，由运维管理员创建的。 2.信息落库+ 定时任务将需要发送的消息保存在数据库中，可以实现消息的可追溯和重复控制，需要配合定时任务来实现。 将需要发送的消息登记在消息表中。 定时任务一分钟或半分钟扫描一次，将未发送的消息发送到 MQ 服务器，并且修改状态为已发送。 如果需要重发消息，将指定消息的状态修改为未发送即可。 副作用：降低效率，浪费存储空间。 3.日志追踪RabbitMQ 可以通过 Firehose 功能来记录消息流入流出的情况，用于调试，排错。 它是通过创建一个 TOPIC 类型的交换机（amq.rabbitmq.trace），把生产者发送给Broker 的消息或者 Broker 发送给消费者的消息发到这个默认的交换机上面来实现的。 另外 RabbitMQ 也提供了一个 Firehose 的 GUI 版本，就是 Tracing 插件。 启用 Tracing 插件后管理界面右侧选项卡会多一个 Tracing，可以添加相应的策略。 RabbitMQ 还提供了其他的插件来增强功能。 4.如何减少连接数在发送大批量消息的情况下，创建和释放连接依然有不小的开销。我们可以跟接收方约定批量消息的格式，比如支持 JSON 数组的格式，通过合并消息内容，可以减少生产者/消费者与 Broker 的连接。 比如：活动过后，要全范围下线产品，通过 Excel 导入模板，通常有几万到几十万条解绑数据，合并发送的效率更高。 建议单条消息不要超过 4M（4096KB），一次发送的消息数需要合理地控制。 5.无法被路由的消息，去了哪里？直接丢弃。可用备份交换机（alternate-exchange）接收。 6.大量消息堆积怎么办？ 重启（不是开玩笑的） 多创建几个消费者同时消费 直接清空队列，重发消息 7.设计一个 MQ，你的思路是什么？存储与转发。 存储：内存：用什么数据结构？ 磁盘：文件系统？数据库？ 通信：通信协议（TCP HTTP AMQP ）？一对一？一对多？一对多 推模式？拉模式？后者 其他特性……​ 八，Spring AMQP1.简单使用Spring-amqp是对AMQP协议的抽象实现，而spring-rabbit是对协议的具体实现，也是目前的唯一实现。底层使用的就是RabbitMQ。 1.1依赖1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt;&lt;/dependency&gt; 1.2 配置1234567spring: rabbitmq: host: 121.199.31.160 username: root password: root virtual-host: /shopping port: 5672 1.3 监听者在SpringAmqp中，对消息的消费者进行了封装和抽象，一个普通的JavaBean中的普通方法，只要通过简单的注解，就可以成为一个消费者。​ 1234567891011121314151617@Componentpublic class Listener &#123; @RabbitListener(bindings = @QueueBinding( //value：队列名 value = @Queue(value = &quot;spring.test.queue&quot;, durable = &quot;true&quot;), exchange = @Exchange( //交换机名 value = &quot;spring.test.exchange&quot;, ignoreDeclarationExceptions = &quot;true&quot;, type = ExchangeTypes.TOPIC ), key = &#123;&quot;#.#&quot;&#125;)) public void listen(String msg)&#123; System.out.println(&quot;接收到消息：&quot; + msg); &#125;&#125; @Componet：类上的注解，注册到Spring容器 @RabbitListener：方法上的注解，声明这个方法是一个消费者方法，需要指定下面的属性： bindings：指定绑定关系，可以有多个。值是@QueueBinding的数组。@QueueBinding包含下面属性： value：这个消费者关联的队列。值是@Queue，代表一个队列 exchange：队列所绑定的交换机，值是@Exchange类型 key：队列和交换机绑定的RoutingKey durable = “true” 代表持久化 ignoreDeclarationExceptions = “true”, 异常情况是否忽略 类似listen这样的方法在一个类中可以写多个，就代表多个消费者。​ 1.4AmqpTemplateSpring为AMQP提供了统一的消息处理模板：AmqpTemplate，非常方便的发送消息，其发送方法红框圈起来的是比较常用的3个方法，分别是： 指定消息 指定RoutingKey和消息，会向默认的交换机发送消息 指定交换机、RoutingKey和消息体 测试代码：​ 123456789101112131415@RunWith(SpringRunner.class)@SpringBootTest(classes = Application.class)public class MqDemo &#123; @Autowired private AmqpTemplate amqpTemplate; @Test public void testSend() throws InterruptedException &#123; String msg = &quot;hello, Spring boot amqp&quot;; this.amqpTemplate.convertAndSend(&quot;spring.test.exchange&quot;, &quot;a.b&quot;, msg); // 等待10秒后再结束 Thread.sleep(10000); &#125;&#125; 2.思考​ Java API 方式编程，有什么问题？ Spring 封装 RabbitMQ 的时候，它做了什么事情？ 管理对象（队列、交换机、绑定） 封装方法（发送消息、接收消息） Spring AMQP 是对 Spring 基于 AMQP 的消息收发解决方案，它是一个抽象层，不依赖于特定的 AMQP Broker 实现和客户端的抽象，所以可以很方便地替换。比如我们可以使用 spring-rabbit 来实现。​ 3.Spring AMQP核心组件3.1 ConnectionFactorySpring AMQP 的连接工厂接口，用于创建连接。CachingConnectionFactory 是ConnectionFactory 的一个实现类。 3.2 RabbitAdminRabbitAdmin 是 AmqpAdmin 的实现，封装了对 RabbitMQ 的基础管理操作，比如对交换机、队列、绑定的声明和删除等。 123456789101112131415161718192021222324252627282930313233343536/** * @author yhd * @createtime 2021/1/28 19:24 * @description Spring AMQP configuration class */@SpringBootConfigurationpublic class AMQPConfig &#123; private static final String EXCHANGE_NAME = &quot;amqp.yhd.exchange&quot;; private static final String QUEUE_NAME = &quot;amqp.yhd.queue&quot;; private static final String ROUTING_KEY = &quot;amqp.admin&quot;; @Bean public ConnectionFactory factory() &#123; CachingConnectionFactory factory = new CachingConnectionFactory(); factory.setAddresses(&quot;121.199.31.160&quot;); factory.setPort(5672); factory.setUsername(&quot;root&quot;); factory.setPassword(&quot;root&quot;); return factory; &#125; @Bean public AmqpAdmin amqpAdmin( ConnectionFactory factory) &#123; RabbitAdmin admin = new RabbitAdmin(factory); //声明一个交换机 交换机名 是否持久化 是否自动删除 admin.declareExchange(new DirectExchange(EXCHANGE_NAME, true, false)); //队列名 持久化 是否批处理 自动删除 admin.declareQueue(new Queue(QUEUE_NAME, true, false, false)); //声明一个绑定 队列名 ，绑定类型，交换机名，路由键 参数 admin.declareBinding(new Binding(QUEUE_NAME, Binding.DestinationType.QUEUE, EXCHANGE_NAME, ROUTING_KEY, null)); return admin; &#125;&#125; 为什么我们在配置文件（Spring）或者配置类（SpringBoot）里面定义了交换机、队列、绑定关系，并没有直接调用 Channel 的 declare 的方法，Spring 在启动的时候就可以帮我们创建这些元数据？这些事情就是由 RabbitAdmin 完成的。 RabbitAdmin 实 现 了 InitializingBean 接 口 ， 里 面 有 唯 一 的 一 个 方 法afterPropertiesSet()，这个方法会在 RabbitAdmin 的属性值设置完的时候被调用。 在 afterPropertiesSet ()方法中，调用了一个 initialize()方法。这里面创建了三个Collection，用来盛放交换机、队列、绑定关系。 最后依次声明返回类型为 Exchange、Queue 和 Binding 这些 Bean，底层还是调用了 Channel 的 declare 的方法。 123declareExchanges(channel, exchanges.toArray(new Exchange[exchanges.size()]));declareQueues(channel, queues.toArray(new Queue[queues.size()]));declareBindings(channel, bindings.toArray(new Binding[bindings.size()])); 3.3 MessageMessage 是 Spring AMQP 对消息的封装。两个重要的属性：body：消息内容。 messageProperties：消息属性。 3.4 RabbitTemplate 消息模板RabbitTemplate 是 AmqpTemplate 的一个实现（目前为止也是唯一的实现），用来简化消息的收发，支持消息的确认（Confirm）与返回（Return）。跟 JDBCTemplate一 样 ， 它 封 装 了 创 建 连 接 、 创 建 消 息 信 道 、 收 发 消 息 、 消 息 格 式 转 换（ConvertAndSend→Message）、关闭信道、关闭连接等等操作。 针对于多个服务器连接，可以定义多个 Template。可以注入到任何需要收发消息的地方使用。 123456789101112131415161718192021222324252627/** * return callback &amp;&amp; confirm callable * * @param factory * @return */@Beanpublic RabbitTemplate rabbitTemplate(ConnectionFactory factory) &#123; RabbitTemplate template = new RabbitTemplate(factory); template.setMandatory(true); template.setReturnCallback((Message message, int replyCode, String replyText, String exchange, String routingKey) -&gt; &#123; &#125;); template.setConfirmCallback((CorrelationData correlationData, boolean ack, String cause) -&gt; &#123; if (ack) &#123; log.info(&quot;消息确认成功！&quot;); &#125; else &#123; log.info(&quot;消息确认失败！&quot;); &#125; &#125;); return template;&#125; 3.5 Messager Listener 消息监听MessageListenerMessageListener 是 Spring AMQP 异步消息投递的监听器接口，它只有一个方法onMessage，用于处理消息队列推送来的消息，作用类似于 Java API 中的 Consumer。 MessageListenerContainerMessageListenerContainer可以理解为MessageListener的容器，一个Container只有一个 Listener，但是可以生成多个线程使用相同的 MessageListener 同时消费消息。 Container 可以管理 Listener 的生命周期，可以用于对于消费者进行配置。 例如：动态添加移除队列、对消费者进行设置，例如 ConsumerTag、Arguments、并发、消费者数量、消息确认模式等等。 1234567891011121314151617181920212223/** * 消息监听器容器 * @param connectionFactory * @return */@Beanpublic SimpleMessageListenerContainer messageContainer(ConnectionFactory connectionFactory) &#123; SimpleMessageListenerContainer container = new SimpleMessageListenerContainer(connectionFactory); //监听的队列 container.setQueues(new Queue(QUEUE_NAME, true, false, false)); // 最小消费者数 container.setConcurrentConsumers(1); // 最大的消费者数量 container.setMaxConcurrentConsumers(5); //是否重回队列 container.setDefaultRequeueRejected(false); //签收模式 container.setAcknowledgeMode(AcknowledgeMode.AUTO); container.setExposeListenerChannel(true); //消费端的标签策略 container.setConsumerTagStrategy(queue -&gt; queue + &quot;_&quot; + UUID.randomUUID().toString()); return container;&#125; 在 SpringBoot2.0 中新增了一个 DirectMessageListenerContainer。 MessageListenerContainerFactory Spring 去整合 IBM MQ、JMS、Kafka 也是这么做的。 1234567891011121314/** * * @param connectionFactory * @return */@Beanpublic SimpleRabbitListenerContainerFactory rabbitListenerContainerFactory(ConnectionFactory connectionFactory) &#123; SimpleRabbitListenerContainerFactory factory = new SimpleRabbitListenerContainerFactory(); factory.setConnectionFactory(connectionFactory); factory.setMessageConverter(new Jackson2JsonMessageConverter()); factory.setAcknowledgeMode(AcknowledgeMode.NONE); factory.setAutoStartup(true); return factory;&#125; 可以在消费者上指定，当我们需要监听多个 RabbitMQ 的服务器的时候，指定不同的 MessageListenerContainerFactory。​ 1234567891011@Slf4j@Component@PropertySource(&quot;classpath:application.properties&quot;)@RabbitListener(queues = &quot;$&#123;amqp.yhd.queue&#125;&quot;, containerFactory = &quot;rabbitListenerContainerFactory&quot;)public class FirstConsumer &#123; @RabbitHandler public void process(@Payload String message) &#123; log.info(&quot;First Queue received msg : &#123;&#125;&quot;, message); &#125;&#125; 3.6 转换器 MessageConvertorMessageConvertor 的 作用？ RabbitMQ 的消息在网络传输中需要转换成 byte[]（字节数组）进行发送，消费者需要对字节数组进行解析。 在 Spring AMQP 中，消息会被封装为 org.springframework.amqp.core.Message对象。消息的序列化和反序列化，就是处理 Message 的消息体 body 对象。 如果消息已经是 byte[]格式，就不需要转换。 如果是 String，会转换成 byte[]。 如果是 Java 对象，会使用 JDK 序列化将对象转换为 byte[]（体积大，效率差）。 在 调 用 RabbitTemplate 的 convertAndSend() 方 法 发 送 消 息 时 ， 会 使 用MessageConvertor 进行消息的序列化，默认使用 SimpleMessageConverter。 在某些情况下，我们需要选择其他的高效的序列化工具。如果我们不想在每次发送消息时自己处理消息，就可以直接定义一个 MessageConvertor。 123456@Beanpublic RabbitTemplate rabbitTemplate(final ConnectionFactory connectionFactory) &#123; final RabbitTemplate rabbitTemplate = new RabbitTemplate(connectionFactory); rabbitTemplate.setMessageConverter(new Jackson2JsonMessageConverter()); return rabbitTemplate;&#125; MessageConvertor 如何 工作？ 调 用 了 RabbitTemplate 的 convertAndSend() 方 法 时 会 使 用 对 应 的MessageConvertor 进行消息的序列化和反序列化。 序列化：Object —— Json —— Message(body) —— byte[] 反序列化：byte[] ——Message —— Json —— Object 有 哪些 MessageConvertor ？ 在 Spring 中提供了一个默认的转换器：SimpleMessageConverter。 Jackson2JsonMessageConverter（RbbitMQ 自带）：将对象转换为 json，然后再转换成字节数组进行传递。 如何 自定义 MessageConverter ？ 例如：我们要使用 Gson 格式化消息： 创建一个类，实现 MessageConverter 接口，重写 toMessage()和 fromMessage()方法。 12toMessage(): Java 对象转换为 MessagefromMessage(): Message 对象转换为 Java 对象 4.SpringBoot集成RabbitMQ为什么没有定义 Spring AMQP 的任何一个对象，也能实现消息的收发？Spring Boot 做了什么？ 老套路 源码：RabbitAutoConfiguration​","categories":[{"name":"消息队列","slug":"消息队列","permalink":"https://yinhuidong.github.io/categories/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"https://yinhuidong.github.io/tags/RabbitMQ/"}]},{"title":"Spring Data Jpa使用篇","slug":"MyBatis/Spring Data Jpa使用篇","date":"2022-01-11T06:37:31.534Z","updated":"2022-01-11T06:41:57.741Z","comments":true,"path":"2022/01/11/MyBatis/Spring Data Jpa使用篇/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/MyBatis/Spring%20Data%20Jpa%E4%BD%BF%E7%94%A8%E7%AF%87/","excerpt":"","text":"一，orm思想和hibernate以及jpa1.orm思想 主要目的：操作实体类就相当于操作数据库表 建立两个映射关系 `实体类`和 `表` 实体类中的属性 和 表中字段的映射关系 不再重点关注： SQL语句 实现了ORM思想的框架：mybatis，hibernate 2. hibernate框架介绍Hibernate是一个开放源代码的对象关系映射框架，它对JDBC进行了非常轻量级的对象封装，它将POJO与数据库表建立映射关系，是一个全自动的orm框架，hibernate可以自动生成SQL语句，自动执行，使得Java程序员可以随心所欲的使用对象编程思维来操纵数据库。 3.jpa规范 JPA的全称是Java Persistence API， 即Java 持久化API，是SUN公司推出的一套基于ORM的规范，内部是由一系列的接口和抽象类构成。 JPA通过JDK 5.0注解描述对象－关系表的映射关系，并将运行期的实体对象持久化到数据库中。 JPA与hibernate的关系 JPA和Hibernate的关系就像JDBC和JDBC驱动的关系，JPA是规范，Hibernate除了作为ORM框架之外，它也是一种JPA实现。JPA怎么取代Hibernate呢？JDBC规范可以驱动底层数据库吗？答案是否定的，也就是说，如果使用JPA规范进行数据库操作，底层需要hibernate作为其实现类完成数据持久化工作。 4.jpa的入门案例1.案例：是客户的相关操作（增删改查） 客户：就是一家公司 2.创建项目导入依赖123456789101112131415161718192021222324252627282930313233343536373839404142&lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.hibernate.version&gt;5.0.7.Final&lt;/project.hibernate.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;!-- junit --&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- hibernate对jpa的支持包 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.hibernate&lt;/groupId&gt; &lt;artifactId&gt;hibernate-entitymanager&lt;/artifactId&gt; &lt;version&gt;$&#123;project.hibernate.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- c3p0 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.hibernate&lt;/groupId&gt; &lt;artifactId&gt;hibernate-c3p0&lt;/artifactId&gt; &lt;version&gt;$&#123;project.hibernate.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- log日志 --&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.17&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Mysql and MariaDB --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.6&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 3.配置文件位置：配置到类路径下的一个叫做 META-INF 的文件夹下命名：persistence.xml 12345678910111213141516171819202122232425262728293031323334&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;persistence xmlns=&quot;http://java.sun.com/xml/ns/persistence&quot; version=&quot;2.0&quot;&gt; &lt;!--配置持久化单元 name：持久化单元名称 transaction-type：事务类型 RESOURCE_LOCAL：本地事务管理 JTA：分布式事务管理 --&gt; &lt;persistence-unit name=&quot;myJpa&quot; transaction-type=&quot;RESOURCE_LOCAL&quot;&gt; &lt;!--配置JPA规范的服务提供商 --&gt; &lt;provider&gt;org.hibernate.jpa.HibernatePersistenceProvider&lt;/provider&gt; &lt;properties&gt; &lt;!-- 数据库驱动 --&gt; &lt;property name=&quot;javax.persistence.jdbc.driver&quot; value=&quot;com.mysql.jdbc.Driver&quot;/&gt; &lt;!-- 数据库地址 --&gt; &lt;property name=&quot;javax.persistence.jdbc.url&quot; value=&quot;jdbc:mysql://localhost:3306/jpa&quot;/&gt; &lt;!-- 数据库用户名 --&gt; &lt;property name=&quot;javax.persistence.jdbc.user&quot; value=&quot;root&quot;/&gt; &lt;!-- 数据库密码 --&gt; &lt;property name=&quot;javax.persistence.jdbc.password&quot; value=&quot;root&quot;/&gt; &lt;!--jpa提供者的可选配置：我们的JPA规范的提供者为hibernate，所以jpa的核心配置中兼容hibernate的配 --&gt; &lt;!--显示sql语句--&gt; &lt;property name=&quot;hibernate.show_sql&quot; value=&quot;true&quot;/&gt; &lt;property name=&quot;hibernate.format_sql&quot; value=&quot;true&quot;/&gt; &lt;!-- 建表方式 create:每次执行都创建表 update:没有表才创建 none:不创建表 --&gt; &lt;property name=&quot;hibernate.hbm2ddl.auto&quot; value=&quot;update&quot;/&gt; &lt;/properties&gt; &lt;/persistence-unit&gt;&lt;/persistence&gt; 4.实体类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107/** * @author yinhuidong * @createTime 2020-06-01-21:43 *///作用：指定当前类是实体类。@Entity/** * 作用：指定实体类和表之间的对应关系。 * 属性： * name：指定数据库表的名称 */@Table(name = &quot;t_person&quot;)public class Person &#123; //作用：指定当前字段是主键。 /** * @Id * 作用：指定主键的生成方式。。 * 属性： * strategy ：指定主键生成策略。 */ @GeneratedValue(strategy = GenerationType.IDENTITY) /** * @Column * 作用：指定实体类属性和数据库表之间的对应关系 * 属性： * name：指定数据库表的列名称。 * unique：是否唯一 * nullable：是否可以为空 * inserttable：是否可以插入 * updateable：是否可以更新 * columnDefinition: 定义建表时创建此列的DDL * secondaryTable: 从表名。如果此列不建在主表上（默认建在主表）， * 该属性定义该列所在从表的名字搭建开发环境[重点] */ @Column(name = &quot;id&quot;) private Integer id; @Column(name = &quot;p_name&quot;) private String name; @Column(name = &quot;p_age&quot;) private Integer age; @Column(name = &quot;p_email&quot;) private String email; @Column(name = &quot;p_birth&quot;) private Date birth; public Person() &#123; &#125; public Person(Integer id, String name, Integer age, String email, Date birth) &#123; this.id = id; this.name = name; this.age = age; this.email = email; this.birth = birth; &#125; public Integer getId() &#123; return id; &#125; public void setId(Integer id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public Integer getAge() &#123; return age; &#125; public void setAge(Integer age) &#123; this.age = age; &#125; public String getEmail() &#123; return email; &#125; public void setEmail(String email) &#123; this.email = email; &#125; public Date getBirth() &#123; return birth; &#125; public void setBirth(Date birth) &#123; this.birth = birth; &#125; @Override public String toString() &#123; return &quot;Person&#123;&quot; + &quot;id=&quot; + id + &quot;, name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, age=&quot; + age + &quot;, email=&#x27;&quot; + email + &#x27;\\&#x27;&#x27; + &quot;, birth=&quot; + birth + &#x27;&#125;&#x27;; &#125;&#125; 5.测试类jpa操作的操作步骤12345678910111213141516171819202122232425262728293031323334351.加载配置文件创建实体管理器工厂 Persisitence：静态方法（根据持久化单元名称创建实体管理器工厂） createEntityMnagerFactory（持久化单元名称） 作用：创建实体管理器工厂2.根据实体管理器工厂，创建实体管理器 EntityManagerFactory ：获取EntityManager对象 方法：createEntityManager * 内部维护的很多的内容 内部维护了数据库信息， 维护了缓存信息 维护了所有的实体管理器对象 再创建EntityManagerFactory的过程中会根据配置创建数据库表 * EntityManagerFactory的创建过程比较浪费资源 特点：线程安全的对象 多个线程访问同一个EntityManagerFactory不会有线程安全问题 * 如何解决EntityManagerFactory的创建过程浪费资源（耗时）的问题？ 思路：创建一个公共的EntityManagerFactory的对象 * 静态代码块的形式创建EntityManagerFactory3.创建事务对象，开启事务 EntityManager对象：实体类管理器 beginTransaction : 创建事务对象 presist ： 保存 merge ： 更新 remove ： 删除 find/getRefrence ： 根据id查询 Transaction 对象 ： 事务 begin：开启事务 commit：提交事务 rollback：回滚4.增删改查操作5.提交事务6.释放资源 测试12345678910@Testpublic void testSave()&#123; EntityManagerFactory factory = Persistence.createEntityManagerFactory(&quot;myJpa&quot;); EntityManager em = factory.createEntityManager(); EntityTransaction tx = em.getTransaction(); tx.begin(); Person person = new Person(null,&quot;aaa&quot;,22,&quot;aaa@163.com&quot;,new Date()); em.persist(person); tx.commit();&#125; 5.主键生成策略@GeneratedValue:配置主键的生成策略strategyGenerationType.IDENTITY ：自增，mysql底层数据库必须支持自动增长（底层数据库支持的自动增长方式，对id自增）GenerationType.SEQUENCE : 序列，oracle底层数据库必须支持序列GenerationType.TABLE : jpa提供的一种机制，通过一张数据库表的形式帮助我们完成主键自增GenerationType.AUTO ： 由程序自动的帮助我们选择主键生成策略 6.JPA的API介绍1.Persistence对象Persistence对象主要作用是用于获取EntityManagerFactory对象的 。通过调用该类的createEntityManagerFactory静态方法，根据配置文件中持久化单元名称创建EntityManagerFactory。 1EntityManagerFactory factory = Persistence.createEntityManagerFactory(&quot;myJpa&quot;); 2.EntityManagerFactoryEntityManagerFactory 接口主要用来创建 EntityManager 实例 1EntityManager em = factory.createEntityManager(); 由于EntityManagerFactory 是一个线程安全的对象（即多个线程访问同一个EntityManagerFactory 对象不会有线程安全问题），并且EntityManagerFactory 的创建极其浪费资源，所以在使用JPA编程时，我们可以对EntityManagerFactory 的创建进行优化，只需要做到一个工程只存在一个EntityManagerFactory 即可。 3. EntityManager在 JPA 规范中,EntityManager是完成持久化操作的核心对象。实体类作为普通 java对象，只有在调用 EntityManager将其持久化后才会变成持久化对象。EntityManager对象在一组实体类与底层数据源之间进行 O/R 映射的管理。它可以用来管理和更新 Entity Bean, 根椐主键查找 Entity Bean, 还可以通过JPQL语句查询实体。我们可以通过调用EntityManager的方法完成获取事务，以及持久化数据库的操作 12345getTransaction : 获取事务对象persist ： 保存操作merge ： 更新操作remove ： 删除操作find/getReference ： 根据id查询 4.EntityTransaction在 JPA 规范中, EntityTransaction是完成事务操作的核心对象，对于EntityTransaction在我们的java代码中承接的功能比较简单 123begin：开启事务commit：提交事务rollback：回滚事务 7. 抽取JPAUtil工具类1234567891011121314151617/** * @author yinhuidong * @createTime 2020-06-02-0:13 */public class JpaUtils &#123; private static EntityManagerFactory factory; static &#123; factory = Persistence.createEntityManagerFactory(&quot;myJpa&quot;); &#125; public static EntityManager getEm()&#123; return factory.createEntityManager(); &#125;&#125; 8. 使用JPA完成增删改查操作1.保存1234567891011121314/** * 添加 */@Testpublic void testSave()&#123; EntityManager em = JpaUtils.getEm(); EntityTransaction tx = em.getTransaction(); tx.begin(); Person person = new Person(null,&quot;aaa&quot;,22,&quot;aaa@163.com&quot;,new Date()); em.persist(person); tx.commit(); em.close();&#125; 2.删除12345678910111213/** * 删除：remove */ @Test public void testRemove()&#123; EntityManager em = JpaUtils.getEm(); EntityTransaction tx = em.getTransaction(); tx.begin(); Person person = em.getReference(Person.class, 1); em.remove(person); tx.commit(); em.close(); &#125; 3.更新12345678910111213/** * 更新:merge */@Testpublic void testmerge()&#123; EntityManager em = JpaUtils.getEm(); EntityTransaction tx = em.getTransaction(); tx.begin(); Person person = new Person(1,&quot;bbb&quot;,22,&quot;aaa@163.com&quot;,new Date()); em.merge(person); tx.commit(); em.close();&#125; 4.根据ID查询1）立即加载1234567891011/** * 查询:find * 立即加载，执行find()立刻发出sql */@Testpublic void testFind()&#123; EntityManager em = JpaUtils.getEm(); Person person = em.find(Person.class, 1); System.out.println(person); em.close();&#125; 2）懒加载123456789101112/** * 查询：getReference * 懒加载机制，什么时候调用查询结果什么时候发出sql语句 * 得到的是一个动态代理对象 */@Testpublic void testgetReference()&#123; EntityManager em = JpaUtils.getEm(); Person person = em.getReference(Person.class, 1); System.out.println(person); em.close();&#125; 5.复杂查询JPQL全称Java Persistence Query Language 123Java持久化查询语言(JPQL)是一种可移植的查询语言，旨在以面向对象表达式语言的表达式，将SQL语法和简单查询语义绑定在一起·使用这种语言编写的查询是可移植的，可以被编译成所有主流数据库服务器上的SQL。其特征与原生SQL语句类似，并且完全面向对象，通过类名和属性访问，而不是表名和表的属性 1)查询全部1234567891011/** * 查询所有 */@Testpublic void test1()&#123; EntityManager em = JpaUtils.getEm(); String jpql =&quot;from Person&quot;; Query query = em.createQuery(jpql); query.getResultList().forEach(System.out::println); em.close();&#125; 2）分页查询12345678910111213/** * 分页查询 */@Testpublic void test1()&#123; EntityManager em = JpaUtils.getEm(); String jpql =&quot;from Person&quot;; Query query = em.createQuery(jpql); query.setFirstResult(0); query.setMaxResults(2); query.getResultList().forEach(System.out::println); em.close();&#125; 3）条件查询1234567891011121314/** * 条件查询 */@Testpublic void test1()&#123; EntityManager em = JpaUtils.getEm(); String jpql =&quot;from Person where name like ?&quot;; Query query = em.createQuery(jpql); query.setParameter(1,&quot;aa%&quot;); //获取唯一结果集 //System.out.println(query.getSingleResult()); query.getResultList().forEach(System.out::println); em.close();&#125; 4）排序查询1234567891011/** * 排序查询 */@Testpublic void test1()&#123; EntityManager em = JpaUtils.getEm(); String jpql =&quot;from Person order by id desc&quot;; Query query = em.createQuery(jpql); query.getResultList().forEach(System.out::println); em.close();&#125; 5）统计查询1234567891011/** * 统计查询 */@Testpublic void test1()&#123; EntityManager em = JpaUtils.getEm(); String jpql =&quot;select count(id) from Person&quot;; Query query = em.createQuery(jpql); System.out.println(query.getSingleResult()); em.close();&#125; 二，springdatajpa的运行原理以及基本操作1.springDataJpa概述1.1SpringDataJpa概述Spring Data JPA 是 Spring 基于 ORM 框架、JPA 规范的基础上封装的一套JPA应用框架，可使开发者用极简的代码即可实现对数据库的访问和操作。它提供了包括增删改查等在内的常用功能，且易于扩展！学习并使用 Spring Data JPA 可以极大提高开发效率！ Spring Data JPA 让我们解脱了DAO层的操作，基本上所有CRUD都可以依赖于它来实现,在实际的工作工程中，推荐使用Spring Data JPA + ORM（如：hibernate）完成操作，这样在切换不同的ORM框架时提供了极大的方便，同时也使数据库层操作更加简单，方便解耦 1.2springdatajpa的特性SpringData Jpa 极大简化了数据库访问层代码。 如何简化的呢？ 使用了SpringDataJpa，我们的dao层中只需要写接口，就自动具有了增删改查、分页查询等方法。 1.3Spring Data JPA 与 JPA和hibernate之间的关系JPA是一套规范，内部是有接口和抽象类组成的。hibernate是一套成熟的ORM框架，而且Hibernate实现了JPA规范，所以也可以称hibernate为JPA的一种实现方式，我们使用JPA的API编程，意味着站在更高的角度上看待问题（面向接口编程） Spring Data JPA是Spring提供的一套对JPA操作更加高级的封装，是在JPA规范下的专门用来进行数据持久化的解决方案。 2.SpringDataJpa快速入门2.1需求说明Spring Data JPA完成客户的基本CRUD操作 2.2搭建环境项目依赖1234567891011121314151617181920212223242526272829303132&lt;!--hibernate--&gt;&lt;dependency&gt; &lt;groupId&gt;org.hibernate&lt;/groupId&gt; &lt;artifactId&gt;hibernate-core&lt;/artifactId&gt; &lt;version&gt;5.4.15.Final&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.hibernate&lt;/groupId&gt; &lt;artifactId&gt;hibernate-entitymanager&lt;/artifactId&gt; &lt;version&gt;5.0.7.Final&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.hibernate&lt;/groupId&gt; &lt;artifactId&gt;hibernate-validator&lt;/artifactId&gt; &lt;version&gt;5.3.6.Final&lt;/version&gt;&lt;/dependency&gt;&lt;!--springdatajpa--&gt;&lt;dependency&gt; &lt;groupId&gt;javax.el&lt;/groupId&gt; &lt;artifactId&gt;javax.el-api&lt;/artifactId&gt; &lt;version&gt;2.2.4&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.glassfish.web&lt;/groupId&gt; &lt;artifactId&gt;javax.el&lt;/artifactId&gt; &lt;version&gt;2.2.4&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.data&lt;/groupId&gt; &lt;artifactId&gt;spring-data-jpa&lt;/artifactId&gt; &lt;version&gt;2.3.0.RELEASE&lt;/version&gt;&lt;/dependency&gt; Spring整合SpringDataJPA1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:jdbc=&quot;http://www.springframework.org/schema/jdbc&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xmlns:jpa=&quot;http://www.springframework.org/schema/data/jpa&quot; xmlns:task=&quot;http://www.springframework.org/schema/task&quot; xsi:schemaLocation=&quot; http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/jdbc http://www.springframework.org/schema/jdbc/spring-jdbc.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd http://www.springframework.org/schema/data/jpa http://www.springframework.org/schema/data/jpa/spring-jpa.xsd&quot;&gt;&lt;!--组件扫描--&gt; &lt;context:component-scan base-package=&quot;com.example&quot;&gt; &lt;!-- 配置要忽略的注解 --&gt; &lt;context:exclude-filter type=&quot;annotation&quot; expression=&quot;org.springframework.stereotype.Controller&quot;/&gt; &lt;/context:component-scan&gt; &lt;!--Spring整合MyBatis--&gt; &lt;!--配置数据源--&gt; &lt;!--引入外部属性文件--&gt; &lt;context:property-placeholder location=&quot;classpath:jdbc.properties&quot;/&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot;&gt; &lt;property name=&quot;username&quot; value=&quot;$&#123;jdbc.username&#125;&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;$&#123;jdbc.password&#125;&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;$&#123;jdbc.url&#125;&quot;/&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;$&#123;jdbc.DriverName&#125;&quot;/&gt; &lt;/bean&gt; &lt;!--配置事务管理器--&gt; &lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.orm.jpa.JpaTransactionManager&quot;&gt; &lt;property name=&quot;entityManagerFactory&quot; ref=&quot;entityManagerFactory&quot;/&gt; &lt;/bean&gt; &lt;jpa:repositories base-package=&quot;com.example.mapper&quot; transaction-manager-ref=&quot;transactionManager&quot; entity-manager-factory-ref=&quot;entityManagerFactory&quot;&gt;&lt;/jpa:repositories&gt; &lt;!--事务管理--&gt; &lt;tx:advice id=&quot;txAdvice&quot; transaction-manager=&quot;transactionManager&quot;&gt; &lt;tx:attributes&gt; &lt;tx:method name=&quot;save*&quot; propagation=&quot;REQUIRED&quot;/&gt; &lt;tx:method name=&quot;insert*&quot; propagation=&quot;REQUIRED&quot;/&gt; &lt;tx:method name=&quot;update*&quot; propagation=&quot;REQUIRED&quot;/&gt; &lt;tx:method name=&quot;delete*&quot; propagation=&quot;REQUIRED&quot;/&gt; &lt;tx:method name=&quot;get*&quot; read-only=&quot;true&quot;/&gt; &lt;tx:method name=&quot;find*&quot; read-only=&quot;true&quot;/&gt; &lt;tx:method name=&quot;*&quot; propagation=&quot;REQUIRED&quot;/&gt; &lt;/tx:attributes&gt; &lt;/tx:advice&gt; &lt;aop:config&gt; &lt;!--配置切入点表达式--&gt; &lt;aop:pointcut id=&quot;my&quot; expression=&quot;execution(* com.example.service.impl.*.* (..))&quot;&gt;&lt;/aop:pointcut&gt; &lt;aop:advisor advice-ref=&quot;txAdvice&quot; pointcut-ref=&quot;my&quot;&gt;&lt;/aop:advisor&gt; &lt;/aop:config&gt; &lt;bean id=&quot;entityManagerFactory&quot; class=&quot;org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;&gt;&lt;/property&gt; &lt;property name=&quot;packagesToScan&quot; value=&quot;com.example.domain&quot;&gt;&lt;/property&gt; &lt;property name=&quot;persistenceProvider&quot;&gt; &lt;bean class=&quot;org.hibernate.jpa.HibernatePersistenceProvider&quot;/&gt; &lt;/property&gt; &lt;!--jpa的供应商适配器--&gt; &lt;property name=&quot;jpaVendorAdapter&quot;&gt; &lt;bean class=&quot;org.springframework.orm.jpa.vendor.HibernateJpaVendorAdapter&quot;&gt; &lt;property name=&quot;generateDdl&quot; value=&quot;false&quot;&gt;&lt;/property&gt; &lt;property name=&quot;database&quot; value=&quot;MYSQL&quot;&gt;&lt;/property&gt; &lt;property name=&quot;databasePlatform&quot; value=&quot;org.hibernate.dialect.MySQLDialect&quot;&gt;&lt;/property&gt; &lt;property name=&quot;showSql&quot; value=&quot;true&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;/property&gt; &lt;property name=&quot;jpaDialect&quot;&gt; &lt;bean class=&quot;org.springframework.orm.jpa.vendor.HibernateJpaDialect&quot;&gt;&lt;/bean&gt; &lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; 关联映射实体类1234567891011121314151617@Entity@Table(name = &quot;t_person&quot;)public class Person &#123; @Id @GeneratedValue(strategy = IDENTITY) @Column(name = &quot;id&quot;) private Long id; @Column(name = &quot;p_name&quot;) private String name; @Column(name = &quot;p_age&quot;) private Integer age; @Column(name = &quot;p_email&quot;) private String email; @Column(name = &quot;p_birth&quot;) private Date birth;&#125; mapper接口12345678/** * @author yinhuidong * @createTime 2020-06-04-15:46 * JpaRepository&lt;Person, Long&gt; 用来完成基本CRUD * JpaSpecificationExecutor&lt;Person&gt; 用来完成复杂查询 */public interface PersonMapper extends JpaRepository&lt;Person, Long&gt;, JpaSpecificationExecutor&lt;Person&gt; &#123;&#125; 测试类123456789101112131415161718192021222324252627282930313233343536373839404142@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(locations = &quot;classpath:applicationContext.xml&quot;)public class TestCRUD &#123; @Autowired private PersonMapper mapper; //保存操作 @Test public void testSave()&#123; mapper.save(new Person(&quot;aaa&quot;,20,&quot;aaa@qq.com&quot;,new Date())); &#125; /** * 修改操作： * 修改与保存共用一个方法save * 当传入的实体类有主键就是修改，没有主键就是保存 */ @Test public void testUpdate()&#123; Person person = new Person(&quot;aaa&quot;,20,&quot;bbb@qq.com&quot;,new Date()); person.setId(8l); mapper.save(person); &#125; //删除 @Test public void testDelete()&#123; Person person = new Person(&quot;aaa&quot;,20,&quot;bbb@qq.com&quot;,new Date()); person.setId(8l); mapper.delete(person); &#125; //根据ID查询 @Test public void testFindById()&#123; System.out.println(mapper.findById(7l).get()); &#125; //查询所有 @Test public void testFindAll()&#123; mapper.findAll().forEach(System.out::println); &#125;&#125; 3.SpringDataJPA的底层代码分析3.1常用接口分析在自定义mapper接口中，并没有提供任何方法就可以使用其中的方法，那么这些方法究竟是怎么来的？，由于我们的接口继承了JpaRepository和JpaSpecificationExecutor，所以我们可以使用这两个接口的所有方法。 在使用Spring Data JPA时，一般实现JpaRepository和JpaSpecificationExecutor接口，这样就可以使用这些接口中定义的方法，但是这些方法都只是一些声明，没有具体的实现方式，那么在 Spring Data JPA中它又是怎么实现的呢？ 3.2SpringDataJPA的实现过程通过刚才的入门案例，打断点来分析SpringDataJPA的执行过程。 以findById（）为例。 断点执行到方法上时，我们可以发现注入的是personmapper对象，本质上是通过jdk的动态代理生成的一个对象。 查看具体过程： 当程序执行的时候，会通过JdkDynamicAopProxy的invoke方法，对Dao对象生成动态代理对象。根据对Spring Data JPA介绍而知，要想进行findOne查询方法，最终还是会出现JPA规范的API完成操作，那么这些底层代码存在于何处呢？答案很简单，都隐藏在通过JdkDynamicAopProxy生成的动态代理对象当中，而这个动态代理对象就是SimpleJpaRepository,通过 SimpleJpaRepository的源码分析，定位到了findOne方法，在此方法中，返回em.find()的返回结果，那么em又是什么呢？ 继续查找em对象，我们发现em就是EntityManager对象，而他是JPA原生的实现方式，所以我们得到结论Spring Data JPA只是对标准JPA操作进行了进一步封装，简化了Dao层代码的开发。 由此可得：SpringDataJpa完整执行流程： SpringDataJpa–&gt;JPA–&gt;hibernate–&gt;数据库 4.SpringDataJPA的查询方式4.1使用jpql的方式查询使用Spring Data JPA提供的查询方法已经可以解决大部分的应用场景，但是对于某些业务来说，我们还需要灵活的构造查询条件，这时就可以使用@Query注解，结合JPQL的语句方式完成查询 **@Query **** 注解的使用非常简单，只需在方法上面标注该注解，同时提供一个JPQL查询语句即可 ** 1234567891011/** * 测试使用JPQL的方式进行查询 */@Query(&quot;from Person &quot;)List&lt;Person&gt; MyfindAll();@Query(&quot;from Person where id=?1&quot;)Person MyfindById(Long id);@Query(value = &quot;update Person set name=?1 where id=?2&quot;)@Modifying@Transactionalvoid MyUpdate(String name,Long id); 4.2使用SQL语句查询1234567891011/** * nativeQuery : 使用本地sql的方式查询 */@Query(value = &quot;select * from t_person&quot;,nativeQuery = true)List&lt;Person&gt; MyfindAll();@Query(value = &quot;select * from t_person where id=?1&quot;,nativeQuery = true)Person MyfindById(Long id);@Query(value = &quot;update t_person set p_name=?1 where id=?2&quot;,nativeQuery = true)@Modifying@Transactionalvoid MyUpdate(String name,Long id); 4.3方法命名规则查询方法命名规则查询就是根据方法的名字，就能创建查询。只需要按照Spring Data JPA提供的方法命名规则定义方法的名称，就可以完成查询工作。Spring Data JPA在程序执行的时候会根据方法名称进行解析，并自动生成查询语句进行查询 按照Spring Data JPA 定义的规则，查询方法以findBy开头，涉及条件查询时，条件的属性用条件关键字连接，要注意的是：条件属性首字母需大写。框架在进行方法名解析时，会先把方法名多余的前缀截取掉，然后对剩下部分进行解析。 Keyword Sample JPQL And findByLastnameAndFirstname … where x.lastname = ?1 and x.firstname = ?2 Or findByLastnameOrFirstname … where x.lastname = ?1 or x.firstname = ?2 Is,Equals findByFirstnameIs, findByFirstnameEquals … where x.firstname = ?1 Between findByStartDateBetween … where x.startDate between ?1 and ?2 LessThan findByAgeLessThan … where x.age &lt; ?1 LessThanEqual findByAgeLessThanEqual … where x.age ⇐ ?1 GreaterThan findByAgeGreaterThan … where x.age &gt; ?1 GreaterThanEqual findByAgeGreaterThanEqual … where x.age &gt;= ?1 After findByStartDateAfter … where x.startDate &gt; ?1 Before findByStartDateBefore … where x.startDate &lt; ?1 IsNull findByAgeIsNull … where x.age is null IsNotNull,NotNull findByAge(Is)NotNull … where x.age not null Like findByFirstnameLike … where x.firstname like ?1 NotLike findByFirstnameNotLike … where x.firstname not like ?1 StartingWith findByFirstnameStartingWith … where x.firstname like ?1 (parameter bound with appended %) EndingWith findByFirstnameEndingWith … where x.firstname like ?1 (parameter bound with prepended %) Containing findByFirstnameContaining … where x.firstname like ?1 (parameter bound wrapped in %) OrderBy findByAgeOrderByLastnameDesc … where x.age = ?1 order by x.lastname desc Not findByLastnameNot … where x.lastname &lt;&gt; ?1 In findByAgeIn(Collection ages) … where x.age in ?1 NotIn findByAgeNotIn(Collection age) … where x.age not in ?1 TRUE findByActiveTrue() … where x.active = true FALSE findByActiveFalse() … where x.active = false IgnoreCase findByFirstnameIgnoreCase … where UPPER(x.firstame) = UPPER(?1) 三，springdataJpa的动态查询与多表查询1.动态查询有时我们在查询某个实体的时候，给定的条件是不固定的，这时就需要动态构建相应的查询语句，在Spring Data JPA中可以通过JpaSpecificationExecutor接口查询。相比JPQL,其优势是类型安全,更加的面向对象。 12345678910111213141516171819202122import java.util.List;import org.springframework.data.domain.Page;import org.springframework.data.domain.Pageable;import org.springframework.data.domain.Sort;import org.springframework.data.jpa.domain.Specification;/** * JpaSpecificationExecutor中定义的方法 **/ public interface JpaSpecificationExecutor&lt;T&gt; &#123; //根据条件查询一个对象 T findOne(Specification&lt;T&gt; spec); //根据条件查询集合 List&lt;T&gt; findAll(Specification&lt;T&gt; spec); //根据条件分页查询 Page&lt;T&gt; findAll(Specification&lt;T&gt; spec, Pageable pageable); //排序查询查询 List&lt;T&gt; findAll(Specification&lt;T&gt; spec, Sort sort); //统计查询 long count(Specification&lt;T&gt; spec);&#125; 对于JpaSpecificationExecutor，这个接口基本是围绕着Specification接口来定义的。我们可以简单的理解为，Specification构造的就是查询条件。 Specification接口中只定义了如下一个方法： 1234567//构造查询条件 /** * root ：Root接口，代表查询的根对象，可以通过root获取实体中的属性 * query ：代表一个顶层查询对象，用来自定义查询 * cb ：用来构建查询，此对象里有很多条件方法 **/ public Predicate toPredicate(Root&lt;T&gt; root, CriteriaQuery&lt;?&gt; query, CriteriaBuilder cb); 1.1 使用Specifications完成条件查询1234567891011121314//动态条件查询@Testpublic void test7()&#123; //使用匿名了内部类的方式，创建一个Specification的实现类，并实现toPredicate（） Specification&lt;Person&gt; sp=new Specification&lt;Person&gt;() &#123; @Override public Predicate toPredicate(Root&lt;Person&gt; root, CriteriaQuery&lt;?&gt; criteriaQuery, CriteriaBuilder criteriaBuilder) &#123; //criteriaBuilder:构建查询，添加查询方式 like：模糊匹配 return criteriaBuilder.like(root.get(&quot;name&quot;).as(String.class),&quot;少妇白洁&quot;); &#125; &#125;; mapper.findAll(sp).forEach(System.out::println);&#125; 1.2基于Specification的分页查询1234567891011121314151617181920212223242526272829//动态分页查询@Testpublic void test8()&#123; //构建查询条件 Specification&lt;Person&gt; sp=new Specification&lt;Person&gt;() &#123; @Override public Predicate toPredicate(Root&lt;Person&gt; root, CriteriaQuery&lt;?&gt; criteriaQuery, CriteriaBuilder criteriaBuilder) &#123; return criteriaBuilder.like(root.get(&quot;name&quot;).as(String.class),&quot;少妇%&quot;); &#125; &#125;; /** * 构建分页参数： * Pageable：接口 * PageRequet实现了Pageable接口，调用of方法的形式构造。 * 第一个参数：页码（从0开始） * 第二个参数：每页查询条数 */ Pageable request = PageRequest.of(0,1); /** * 分页查询：封装为SpringDataJpa内部的pageBean * 此重载findAlll方法为分页方法需要两个参数 * 第一个参数、；查询条件 * 第二个参数：分页参数 */ Page&lt;Person&gt; page = mapper.findAll(sp, request); page.forEach(System.out::println);&#125; 1.3方法对应关系 方法名称 Sql对应关系 equle filed = value gt（greaterThan ） filed &gt; value lt（lessThan ） filed &lt; value ge（greaterThanOrEqualTo ） filed &gt;= value le（ lessThanOrEqualTo） filed &lt;= value notEqule filed != value like filed like value notLike filed not like value 2.多表设计2.1表之间关系的划分数据库中多表之间存在着三种关系，如图所示。 从图可以看出，系统设计的三种实体关系分别为：多对多、一对多和一对一关系。注意：一对多关系可以看为两种： 即一对多，多对一。所以说四种更精确。 2.2 在JPA框架中表关系的分析步骤在实际开发中，我们数据库的表难免会有相互的关联关系，在操作表的时候就有可能会涉及到多张表的操作。而在这种实现了ORM思想的框架中（如JPA），可以让我们通过操作实体类就实现对数据库表的操作。所以学习重点是：掌握配置实体之间的关联关系。 第一步：首先确定两张表之间的关系。 如果关系确定错了，后面做的所有操作就都不可能正确。 第二步：在数据库中实现两张表的关系 第三步：在实体类中描述出两个实体的关系 第四步：配置出实体类和数据库表的关系映射（重点） 3.jpa中的一对多3.1情景模拟上个案例我创建的是一张Person表，此时在创建一张Dog表，每个人都可以对应着多个宠物狗。 此时从人的角度就是一对多的关系。 3.2表关系建立在一对多关系中，我们习惯把一的一方称之为主表，把多的一方称之为从表。在数据库中建立一对多的关系，需要使用数据库的外键约束。 什么是外键？ 指的是从表中有一列，取值参照主表的主键，这一列就是外键。 一对多数据库关系的建立，如下图所示 3.3 实体类关系建立以及映射配置数据库建表语句 123456789101112CREATE TABLE t_person(id INT PRIMARY KEY AUTO_INCREMENT,p_age INT ,p_birth DATE,p_email VARCHAR(20),p_name VARCHAR(20));CREATE TABLE t_dog(d_id INT PRIMARY KEY AUTO_INCREMENT,d_name VARCHAR(20),p_id INT REFERENCES t_person(id) ); 实体类关系映射 12345678910111213141516171819@Entity@Table(name = &quot;t_person&quot;)public class Person &#123; @Id @GeneratedValue(strategy = IDENTITY) @Column(name = &quot;id&quot;) private Long id; @Column(name = &quot;p_name&quot;) private String name; @Column(name = &quot;p_age&quot;) private Integer age; @Column(name = &quot;p_email&quot;) private String email; @Column(name = &quot;p_birth&quot;) private Date birth; @OneToMany(mappedBy=&quot;person&quot;) private List&lt;Dog&gt; dogs=new ArrayList&lt;&gt;();&#125; 1234567891011121314@Entity@Table(name = &quot;t_dog&quot;)public class Dog &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) @Column(name = &quot;d_id&quot;) private Long id; @Column(name = &quot;d_name&quot;) private String name; @ManyToOne(targetEntity = Person.class) @JoinColumn(name = &quot;p_id&quot;,referencedColumnName = &quot;id&quot;) private Person person;&#125; 3.4映射的注解说明@OneToMany:作用：建立一对多的关系映射 属性： targetEntityClass：指定多的多方的类的字节码 mappedBy：指定从表实体类中引用主表对象的名称。 cascade：指定要使用的级联操作 fetch：指定是否采用延迟加载 orphanRemoval：是否使用孤儿删除 **@ManyToOne **** **作用：建立多对一的关系 属性： targetEntityClass：指定一的一方实体类字节码 cascade：指定要使用的级联操作 fetch：指定是否采用延迟加载 optional：关联是否可选。如果设置为false，则必须始终存在非空关系。 **@JoinColumn **** **作用：用于定义主键字段和外键字段的对应关系。 属性： name：指定外键字段的名称 referencedColumnName：指定引用主表的主键字段名称 unique：是否唯一。默认值不唯一 nullable：是否允许为空。默认值允许。 insertable：是否允许插入。默认值允许。 updatable：是否允许更新。默认值允许。 columnDefinition：列的定义信息。 3.5一对多的操作1）添加1234567891011121314151617181920212223242526@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(locations = &quot;classpath:applicationContext.xml&quot;)public class OneToManyTest &#123; @Autowired private PersonMapper personMapper; @Autowired private DogMapper dogMapper; /** * 测试保存： * 同时保存一个狗主人和一只狗的信息 */ @Test @Transactional @Rollback(value = false) public void testSave()&#123; Person person = new Person(&quot;张敏&quot;,26,&quot;baijie@qq.com&quot;,new Date()); Dog dog = new Dog(); dog.setName(&quot;金毛&quot;); person.getDogs().add(dog); dog.setPerson(person); personMapper.save(person); dogMapper.save(dog); &#125;&#125; 2）删除1234567891011/** * 删除操作 */@Testpublic void testDelete()&#123; //删除主表数据，可以直接删除，对应从表外键字段还是删除的主表ID personMapper.deleteById(4l); //此时在删除从表会报错 dogMapper.deleteById(4l); //如果先删除从表再删除主表则没有问题&#125; 3）级联操作级联操作：指操作一个对象同时操作它的关联对象 使用方法：只需要在操作主体的注解上配置cascade 1@OneToMany(mappedBy=&quot;person&quot;,cascade = CascadeType.ALL) cascade:配置级联操作CascadeType.MERGE 级联更新CascadeType.PERSIST 级联保存：CascadeType.REFRESH 级联刷新：CascadeType.REMOVE 级联删除：CascadeType.ALL 包含所有 4.jpa中的多对多4.1.情景模拟用户和角色：一个用户可以有多个角色，一个角色可以对应多个用户 4.2表关系建立 123456789CREATE TABLE t_role(r_id INT PRIMARY KEY AUTO_INCREMENT,NAME VARCHAR(20));CREATE TABLE t_person_role(id INT PRIMARY KEY AUTO_INCREMENT,r_id INT REFERENCES t_role(r_id),p_id INT REFERENCES t_person(id)) 4.3实体关系建立以及映射配置12345678910111213141516171819202122@Entity@Table(name = &quot;t_person&quot;)public class Person &#123; @Id @GeneratedValue(strategy = IDENTITY) @Column(name = &quot;id&quot;) private Long id; @Column(name = &quot;p_name&quot;) private String name; @Column(name = &quot;p_age&quot;) private Integer age; @Column(name = &quot;p_email&quot;) private String email; @Column(name = &quot;p_birth&quot;) private Date birth; @OneToMany(mappedBy=&quot;person&quot;,cascade = CascadeType.ALL) private List&lt;Dog&gt; dogs=new ArrayList&lt;&gt;(); //多对多关系映射 @ManyToMany(mappedBy =&quot;persons&quot;) private List&lt;Role&gt; roles=new ArrayList&lt;&gt;(); &#125; 12345678910111213141516@Entity@Table(name = &quot;t_role&quot;)public class Role &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) @Column(name = &quot;r_id&quot;) private Integer id; @Column(name = &quot;name&quot;) private String name; @ManyToMany @JoinTable(name=&quot;t_person_role&quot;,//中间表的名称 joinColumns=&#123;@JoinColumn(name=&quot;r_id&quot;,referencedColumnName=&quot;r_id&quot;)&#125;, inverseJoinColumns=&#123;@JoinColumn(name=&quot;p_id&quot;,referencedColumnName=&quot;id&quot;)&#125; ) private List&lt;Person&gt; persons=new ArrayList&lt;&gt;(); &#125; 4.4映射的注解说明**@ManyToMany **** ** 作用：用于映射多对多关系 属性： cascade：配置级联操作。 fetch：配置是否采用延迟加载。 targetEntity：配置目标的实体类。映射多对多的时候不用写。 **@JoinTable **** **作用：针对中间表的配置 属性： nam：配置中间表的名称 joinColumns：中间表的外键字段关联当前实体类所对应表的主键字段 inverseJoinColumn：中间表的外键字段关联对方表的主键字段 ​ **@JoinColumn **** **作用：用于定义主键字段和外键字段的对应关系。 属性： name：指定外键字段的名称 referencedColumnName：指定引用主表的主键字段名称 unique：是否唯一。默认值不唯一 nullable：是否允许为空。默认值允许。 insertable：是否允许插入。默认值允许。 updatable：是否允许更新。默认值允许。 columnDefinition：列的定义信息。 4.5多对多的操作1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * @author yinhuidong * @createTime 2020-06-04-21:19 */@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(locations = &quot;classpath:applicationContext.xml&quot;)public class ManyToManyTest &#123; @Autowired private PersonMapper personMapper; @Autowired private RoleMapper roleMapper; /** * 保存 */ @Test @Transactional @Rollback(false) public void test1()&#123; Person person = new Person(); person.setName(&quot;尹会东&quot;); Person person2 = new Person(); person2.setName(&quot;张贝贝&quot;); Role r1 = new Role(); r1.setName(&quot;学生&quot;); Role r2 = new Role(); r2.setName(&quot;丈夫&quot;); person.getRoles().add(r1); person.getRoles().add(r2); person2.getRoles().add(r1); person2.getRoles().add(r2); r1.getPersons().add(person); r2.getPersons().add(person); r1.getPersons().add(person2); r2.getPersons().add(person2); personMapper.save(person); personMapper.save(person2); roleMapper.save(r1); roleMapper.save(r2); &#125; @Test @Transactional @Rollback(false) public void test2()&#123; personMapper.deleteById(10l); &#125;&#125; 5.SpringDataJPA的多表查询对象导航图查询对象图导航检索方式是根据已经加载的对象，导航到他的关联对象。它利用类与类之间的关系来检索对象。例如：我们通过ID查询方式查出一个客户，可以调用Customer类中的getLinkMans()方法来获取该客户的所有联系人。对象导航查询的使用要求是：两个对象之间必须存在关联关系。 查询一个Person获取它对应的所有Role 1234567@Transactional@Testpublic void test3()&#123; Person person = personMapper.findById(11l).get(); System.out.println(&quot;person = &quot; + person); person.getRoles().forEach(System.out::println);&#125; 查询一个角色，获取角色对应的所有person 1234567@Transactional@Testpublic void test3()&#123; Role role = roleMapper.findById(5l).get(); System.out.println(&quot;role = &quot; + role); role.getPersons().forEach(System.out::println);&#125; 对象导航查询的问题分析 问题1：我们查询客户时，要不要把联系人查询出来？ 分析：如果我们不查的话，在用的时候还要自己写代码，调用方法去查询。如果我们查出来的，不使用时又会白白的浪费了服务器内存。 解决：采用延迟加载的思想。通过配置的方式来设定当我们在需要使用时，发起真正的查询。 配置方式： 12//LAZY @ManyToMany(mappedBy =&quot;persons&quot;,fetch = FetchType.EAGER) FetchType.EAGER 立即加载 FetchType.LAZY 懒加载 12345678910111213141516 r1.getPersons().add(person); r2.getPersons().add(person); r1.getPersons().add(person2); r2.getPersons().add(person2); personMapper.save(person); personMapper.save(person2); roleMapper.save(r1); roleMapper.save(r2);&#125;@Test@Transactional@Rollback(false)public void test2()&#123; personMapper.deleteById(10l);&#125; } 1234567891011121314151617## 5.SpringDataJPA的多表查询### 5.1对象导航图查询对象图导航检索方式是根据已经加载的对象，导航到他的关联对象。它利用类与类之间的关系来检索对象。例如：我们通过ID查询方式查出一个客户，可以调用Customer类中的getLinkMans()方法来获取该客户的所有联系人。对象导航查询的使用要求是：两个对象之间必须存在关联关系。**查询一个Person获取它对应的所有Role**```java @Transactional @Test public void test3()&#123; Person person = personMapper.findById(11l).get(); System.out.println(&quot;person = &quot; + person); person.getRoles().forEach(System.out::println); &#125; 查询一个角色，获取角色对应的所有person 1234567@Transactional@Testpublic void test3()&#123; Role role = roleMapper.findById(5l).get(); System.out.println(&quot;role = &quot; + role); role.getPersons().forEach(System.out::println);&#125; 对象导航查询的问题分析 问题1：我们查询客户时，要不要把联系人查询出来？ 分析：如果我们不查的话，在用的时候还要自己写代码，调用方法去查询。如果我们查出来的，不使用时又会白白的浪费了服务器内存。 解决：采用延迟加载的思想。通过配置的方式来设定当我们在需要使用时，发起真正的查询。 配置方式： 12//LAZY @ManyToMany(mappedBy =&quot;persons&quot;,fetch = FetchType.EAGER) FetchType.EAGER 立即加载 FetchType.LAZY 懒加载","categories":[{"name":"MyBatis","slug":"MyBatis","permalink":"https://yinhuidong.github.io/categories/MyBatis/"}],"tags":[{"name":"MyBatis","slug":"MyBatis","permalink":"https://yinhuidong.github.io/tags/MyBatis/"}]},{"title":"MyBatis应用分析与最佳实践","slug":"MyBatis/MyBatis应用分析与最佳实践","date":"2022-01-11T06:37:23.826Z","updated":"2022-01-11T06:40:09.343Z","comments":true,"path":"2022/01/11/MyBatis/MyBatis应用分析与最佳实践/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/MyBatis/MyBatis%E5%BA%94%E7%94%A8%E5%88%86%E6%9E%90%E4%B8%8E%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/","excerpt":"","text":"写在前面：全文代码git地址：https://gitee.com/yin_huidong/mybatis-use.git 一，MyBatis入门持久层技术的解决方案： JDBC Spring的JdbcTemplate Apache的DbUtils 以上都不是框架，JDBC是规范，Spring的JdbcTemplate和Apache的DBUtils都只是工具类。​ MyBatis概述：一个用Java编写的持久层框架，封装了JDBC操作的很多细节，是开发者只需要关注SQL语句本身，不需要关注整个请求过程。 ORM：Object Relational Mappging 对象关系映射。简单的说：就是把数据库表和实体类及实体类的属性对应起来，让我们可以操作实体类就实现操作数据库表。 t_user TUser id id name name ​ 1.mybatis环境搭建 创建maven工程，导入坐标 创建实体类和mapper接口 创建mybatis的主配置文件和日志文件 创建映射配置文件 ​ 2.注意事项 在idea中创建目录的时候，它和包是不一样的。 包在创建时：com.itheima.dao它是三级结构 目录在创建时：com.itheima.dao是一级目录 mybatis的映射配置文件位置必须和dao接口的包结构相同 映射配置文件的mapper标签namespace属性的取值必须是dao接口的全限定类名 映射配置文件的操作配置（select），id属性的取值必须是dao接口的方法名 ​ 3.基于XML形式的配置代码：mybatis-01​ 4.基于注解形式的配置代码：mybatis-02​ 5.思考自己实现mybatis框架 5.1 需求分析 它需要实现对连接资源的自动管理，也就是把创建 Connection、Statement、关闭 Connection、Statement、ResultSet 这些操作封装到底层的对象中，不需要在应用层手动调用。 它需要把 SQL 语句抽离出来实现集中管理，开发人员不用在业务代码里面写 SQL语句。 它需要实现对结果集的转换，也就是我们指定了映射规则之后，这个框架会自动帮我们把 ResultSet 映射成实体类对象。 需要提供一个 API 来给我们操作数据库，这里面封装了对数据库的操作的常用的方法。 5.2 概要设计① 核心对象 存放参数和结果映射关系、存放 SQL 语句，我们需要定义一个配置类 执行对数据库的操作，处理参数和结果集的映射，创建和释放资源，我们需要定义一个执行器 有了这个执行器以后，我们不能直接调用它，而是定义一个给应用层使用的 API，它可以根据 SQL 的 id 找到 SQL 语句，交给执行器执行 直接使用 id 查找 SQL 语句太麻烦了，我们干脆把存放 SQL 的命名空间定义成一个接口，把 SQL 的 id 定义成方法，这样只要调用接口方法就可以找到要执行的 SQL。这个时候我们需要引入一个代理类。 ② 流程分析 定义接口 Mapper 和方法，用来调用数据库操作。Mapper 接口操作数据库需要通过代理类。 定义配置类对象 Configuration。 定义应用层的 API SqlSession。它有一个 getMapper()方法，我们会从配置类Configuration 里面使用 Proxy.newProxyInatance()拿到一个代理对象 MapperProxy。 有了代理对象 MapperProxy 之后，我们调用接口的任意方法，就是调用代理对象的 invoke()方法。 代理对象 MapperProxy 的 invoke()方法调用了 SqlSession 的 selectOne()。 SqlSession 只是一个 API，还不是真正的 SQL 执行者，所以接下来会调用执行器 Executor 的 query()方法。 执行器 Executor 的 query()方法里面就是对 JDBC 底层的 Statement 的封装，最终实现对数据库的操作，和结果的返回。 5.3 代码&amp;思考mybatis-0 在 Executor 中，对参数、语句和结果集的处理是耦合的，没有实现职责分离； 参数：没有实现对语句的预编译，只有简单的格式化（format），效率不高，还存在 SQL 注入的风险； 语句执行：数据库连接硬编码； 结果集：还只能处理 Blog 类型，没有实现根据实体类自动映射。确实有点搓，拿不出手。 展望 支持参数预编译 支持结果集的自动处理（通过反射） 对 Executor 的职责进行细化 在方法上使用注解配置 SQL 查询带缓存功能 支持自定义插件6.Mybatis与jdbc编程比较 数据库链接创建、释放频繁造成系统资源浪费从而影响系统性能，如果使用数据库链接池可解决此问题。 解决：在 SqlMapConfig.xml 中配置数据链接池，使用连接池管理数据库链接。 Sql 语句写在代码中造成代码不易维护，实际应用 sql 变化的可能较大，sql 变动需要改变 java 代码。 解决：将 Sql 语句配置在 XXXXmapper.xml 文件中与 java 代码分离。 向 sql 语句传参数麻烦，因为 sql 语句的 where 条件不一定，可能多也可能少，占位符需要和参数对应。 解决：Mybatis 自动将 java 对象映射至 sql 语句，通过 statement 中的 parameterType 定义输入参数的类型。 对结果集解析麻烦，sql 变化导致解析代码变化，且解析前需要遍历，如果能将数据库记录封装成 pojo 对象解析比较方便。 解决：Mybatis 自动将 sql 执行结果映射至 java 对象，通过 statement 中的 resultType 定义输出结果的类型 二，基于Mybatis实现基本的增删改查1.代码mybatis-03​ 2.Mapper映射文件详解 映射器里面最主要的是配置了 SQL 语句，也解决了我们的参数映射和结果集映射的问题。一共有 8 个标签： cache – 给定命名空间的缓存配置（是否开启二级缓存）。 cache-ref – 其他命名空间缓存配置的引用。 resultMap – 是最复杂也是最强大的元素，用来描述如何从数据库结果集中来加载对象。 sql – 可被其他语句引用的可重用语句块。 insert – 映射插入语句 update – 映射更新语句 delete – 映射删除语句 select – 映射查询语句 2.1 select 标签 resultType 属性：用于指定结果集的类型。 parameterType 属性：用于指定传入参数的类型。 sql 语句中使用#{}字符： 它代表占位符，相当于原来 jdbc 部分所学的?，都是用于执行语句时替换实际的数据。具体的数据是由#{}里面的内容决定的。#{}中内容的写法：由于数据类型是基本类型，所以此处可以随意写。​ 2.2 insert 标签 parameterType 属性：代表参数的类型，因为我们要传入的是一个类的对象，所以类型就写类的全名称。 sql 语句中使用#{}字符： 它代表占位符，相当于原来 jdbc 部分所学的?，都是用于执行语句时替换实际的数据。具体的数据是由#{}里面的内容决定的。 #{}中内容的写法：由于我们保存方法的参数是 一个 User 对象，此处要写 User 对象中的属性名称。它用的是 ognl 表达式。 ​ 2.3 OGNL 表达式ognl 表达式：它是 apache 提供的一种表达式语言，全称是：Object Graphic Navigation Language 对象图导航语言，它是按照一定的语法格式来获取数据的。​ 语法格式就是使用 #{对象.对象}的方式：#{user.username}它会先去找 user 对象，然后在 user 对象中找到 username 属性，并调用getUsername()方法把值取出来。但是我们在 parameterType 属性上指定了实体类名称，所以可以省略 user.而直接写 username。​ 2.4 插入后主键返回的三种方式新增用户后，同时还要返回当前新增用户的 id 值，因为 id 是由数据库的自动增长来实现的，所以就相​当于我们要在新增后将自动增长 auto_increment 的值返回。 ①方法一123&lt;insert id=&quot;add&quot; useGeneratedKeys=&quot;true&quot; keyProperty=&quot;id&quot; parameterType=&quot;com.yhd.domain.Account&quot;&gt; insert into account(name,money) values (#&#123;name&#125;,#&#123;money&#125;);&lt;/insert&gt; ②方法二123456&lt;insert id=&quot;add&quot; &gt; &lt;selectKey order=&quot;AFTER&quot; keyProperty=&quot;id&quot; resultType=&quot;int&quot;&gt; select last_insert_id(); &lt;/selectKey&gt; insert into account(name,money) values (#&#123;name&#125;,#&#123;money&#125;);&lt;/insert&gt; ③方法三可以把查询回来的多条数据封装为Map，Map的键是我们指定的唯一键的值，Map的value是每一行记录转换的对象。 12@MapKey(&quot;id&quot;)Map&lt;Integer,Account&gt; findAllByMap(); 123&lt;select id=&quot;findAllByMap&quot; resultType=&quot;account&quot;&gt; select * from account;&lt;/select&gt; 2.5 根据名称模糊查询①第一种方式我们在配置文件中没有加入%来作为模糊查询的条件，所以在传入字符串实参时，就需要给定模糊查询的标识%。配置文件中的#{username}也只是一个占位符，所以 SQL 语句显示为“？”。​ 123&lt;!-- 根据名称模糊查询 --&gt; &lt;select id=&quot;findByName&quot; resultType=&quot;com.yhd.domain.User&quot; parameterType=&quot;String&quot;&gt; select * from user where username like % #&#123;username&#125; %&lt;/select&gt; ②第二种方式123456第一步：修改 SQL 语句的配置，配置如下：&lt;!-- 根据名称模糊查询 --&gt; &lt;select id=&quot;findByName&quot; parameterType=&quot;string&quot; resultType=&quot;com.yhd.domain.User&quot;&gt; select * from user where username like &#x27;%$&#123;value&#125;%&#x27;&lt;/select&gt; 我们在上面将原来的#{}占位符，改成了${value}。注意如果用模糊查询的这种写法，那么${value}的写法就是固定的，不能写成其它名字。​ ③ #{}和${}的区别 #{}表示一个占位符号 通过#{}可以实现 preparedStatement 向占位符中设置值，自动进行 java 类型和 jdbc 类型转换， #{}可以有效防止 sql 注入。 #{}可以接收简单类型值或 pojo 属性值。 如果 parameterType 传输单个简单 类型值，#{}括号中可以是 value 或其它名称。${}表示拼接 sql 串 通过${}可以将 parameterType 传入的内容拼接在 sql 中且不进行 jdbc 类型转换， ${}可以接收简 单类型值或 pojo 属性值，如果 parameterType 传输单个简单类型值，${}括号中只能是 value。 ④模糊查询${value}的源码12345678910111213@Override public String handleToken(String content) &#123; Object parameter = context.getBindings().get(&quot;_parameter&quot;); if (parameter == null) &#123; context.getBindings().put(&quot;value&quot;, null); &#125; else if (SimpleTypeRegistry.isSimpleType(parameter.getClass())) &#123; context.getBindings().put(&quot;value&quot;, parameter); &#125; Object value = OgnlCache.getValue(content, context.getBindings()); String srtValue = (value == null ? &quot;&quot; : String.valueOf(value)); checkInjection(srtValue); return srtValue; &#125; 这就说明了源码中指定了读取的 key 的名字就是”value”，所以我们在绑定参数时就只能叫 value 的名字了。​ 2.6 resultType 配置结果类型resultType配置结果类型:当他为实体类全限定类名，必须让实体类的属性名与数据库表的列名对应，否则，数据会封装不进去，当然，也存在解决办法。解决办法： 起别名，在sql语句中给数据库表的列名起别名，别名与实体类的属性名一致 优点：执行效率高 缺点：开发效率低 配置resultMap 自定义一个resultMap，在select标签中进行引用 优点：开发效率高 缺点：执行效率低​ 123456789101112131415&lt;resultMap id=&quot;map&quot; type=&quot;com.atguigu.domain.User&quot;&gt; &lt;!--配置主键--&gt; &lt;id column=&quot;id&quot; property=&quot;id&quot;&gt;&lt;/id&gt; &lt;!--配置其他列--&gt; &lt;result column=&quot;birthday&quot; property=&quot;birthday&quot;&gt;&lt;/result&gt; &lt;result column=&quot;sex&quot; property=&quot;sex&quot;&gt;&lt;/result&gt; &lt;result column=&quot;address&quot; property=&quot;address&quot;&gt;&lt;/result&gt; &lt;result column=&quot;username&quot; property=&quot;username&quot;&gt;&lt;/result&gt; &lt;!-- id 标签：用于指定主键字段 result 标签：用于指定非主键字段 column 属性：用于指定数据库列名 property 属性：用于指定实体类属性名称 --&gt;&lt;/resultMap&gt; 1234&lt;!-- 配置查询所有操作 --&gt; &lt;select id=&quot;findAll&quot; resultMap=&quot;userMap&quot;&gt; select * from user&lt;/select&gt; 3.核心配置解读configurationMyBatis 全局配置文件顺序是固定的，否则启动的时候会报错。 properties配置参数信息，比如最常见的数据库连接信息。 为了避免直接把参数写死在 xml 配置文件中，我们可以把这些参数单独放在properties 文件中，用 properties 标签引入进来，然后在 xml 配置文件中用${}引用就可以了。 可以用 resource 引用应用里面的相对路径，也可以用 url 指定本地服务器或者网络的绝对路径。 settings 属性名 含义 简介 有效值 默认值 cacheEnabled 是否使用缓存 是整个工程中所有映射器配置缓存的开关，即是一个全局缓存开关 true/false true lazyLoadingEnabled 是否开启延迟加载 控制全局是否使用延迟加载（association、collection）。当有特殊关联关系需要单独配置时，可以使用 fetchType 属性来覆盖此配置 true/false false aggressiveLazyLoading 是否需要侵入式延迟加载 开启时，无论调用什么方法加载某个对象，都会加载该对象的所有属性，关闭之后只会按需加载 true/false false defaultExecutorType 设置默认的执行器 有三种执行器：SIMPLE 为普通执行器；REUSE 执行器会重用与处理语句；BATCH 执行器将重用语句并执行批量更新 SIMPLE/REUSE/BATCH SIMPLE lazyLoadTriggerMethods 指定哪个对象的方法触发一次延迟加载 配置需要触发延迟加载的方法的名字，该方法就会触发一次延迟加载 一个逗号分隔的方法名称列表 equals，clone，hashCode，toString localCacheScope MyBatis 利用本地缓存机制（LocalCache）防止循环引用（circularreferences）和加速重复嵌套查询 默认值为 SESSION，这种情况下会缓存一个会话中执行的所有查询。若设置值为 STATEMENT，本地会话仅用在语句执行上，对相同 SqlSession 的不同调用将不会共享数据 SESSION/STATEMENT SESSION logImpl 日志实现 指定 MyBatis 所用日志的具体实现，未指定时将自动查找 SLF4J、LOG4J、LOG4J2、JDK_LOGGING、COMMONS_LOGGING、STDOUT_LOGGING、NO_LOGGING 无 multipleResultSetsEnabled 是否允许单一语句返回多结果集 即 Mapper 配置中一个单一的 SQL 配置是否能返回多个结果集 true/false true useColumnLabel 使用列标签代替列 设置是否使用列标签代替列名 true/false true useGeneratedKeys 是否支持 JDBC 自动生成主键 设置之后，将会强制使用自动生成主键的策略 true/false false autoMappingBehavior 指定 MyBatis 自动映射字段或属性的方式 有三种方式，NONE 时将取消自动映射；PARTIAL 时只会自动映射没有定义结果集的结果映射；FULL 时会映射任意复杂的结果集 NONE/PARTIAL/FULL PARTIAL autoMappingUnknownColumnBehavior 设置当自动映射时发现未知列的动作 有三种动作：NONE 时不做任何操作；WARNING 时会输出提醒日志；FAILING时会抛出 SqlSessionException 异常表示映射失败 NONE/WARNING/FAILING NONE defaultStatementTimeout 设置超时时间 该超时时间即数据库驱动连接数据库时，等待数据库回应的最大秒数 任意正整数 无 defaultFetchSize 设置驱动的结果集获取数量（fetchSize）的提示值 为了防止从数据库查询出来的结果过多，而导致内存溢出，可以通过设置fetchSize 参数来控制结果集的数量 任意正整数 无 safeRowBoundsEnabled 允许在嵌套语句中使用分页（RowBound，即行内嵌套语句） 如果允许在 SQL 的行内嵌套语句中使用分页，就设置该值为 false true/false false safeResultHandlerEnabled 允许在嵌套语句中使用分页（ResultHandler，即结果集处理） 如果允许在 SQL 的结果集使用分页，就设置该值为 false true/false false mapUnderscoreToCamelCase 是否开启驼峰命名规则（camel case）映射 表明数据库中的字段名称与工程中Java 实体类的映射是否采用驼峰命名规则校验 true/false false jdbcTypeForNull JDBC类型的默认设置 当没有为参数提供特定的 JDBC 类型时，为空值指定 JDBC 类型。某些驱动需要指定列的 JDBC 类型，多数情况直接用一般类型即可，比如 NULL、VARCHAR 或 OTHER 常用 NULL、VARCHAR、OTHER OTHER defaultScriptingLanguage 动态 SQL 默认语言 指定动态 SQL 生成的默认语言 一个类型别名或者一个类的全路径名 org.apache.ibatis.scripting.xmltags.XMLLanguageDriver callSettersOnNulls 是否在控制情况下调用 Set 方法 指定当结果集中值为 null 时是否调用映射对象的 setter （map对象时为put）方法，这对于有 Map.keySet()依赖或null 值初始化时是有用的。注意基本类型是不能设置成 null 的 true/false false returnInstanceForEmptyRow 返回空实体集对象 当返回行的所有列都是空时，MyBatis默认返回 null。当开启这个设置时，MyBatis 会返回一个空实例。请注意，它也适用于嵌套的结果集（从MyBatis3.4.2 版本开始） true/false false logPrefix 日志前缀 指定 MyBatis 所用日志的具体实现，未指定时将自动查找 任意字符串 无 vfsImpl vfs 实现 指定 vfs 的实现 自定义 VFS 的实现的类的全限定名，以逗号分隔 无 useActualParamName 使用方法签名 允许使用方法签名中的名称作为语句参数名称。要使用该特性，工程必须采用 Java8 编译，并且加上-parameters选项（从 MyBatis3.4.1 版本开始） 自定义 VFS 的实现的类的全限定名，以逗号分隔 无 configurationFactory 配置工厂 指定提供配置示例的类。返回的配置实例用于加载反序列化的懒加载参数。这个类必须有一个签名的静态配置getconfiguration()方法（从MyBatis3.2.3 版本开始） 一个类型别名或者一个类型的全路径名 无 typeAliasesTypeAlias 是类型的别名，跟 Linux 系统里面的 alias 一样，主要用来简化全路径类名的拼写。比如我们的参数类型和返回值类型都可能会用到我们的 Bean，如果每个地方都配置全路径的话，那么内容就比较多，还可能会写错。 我们可以为自己的 Bean 创建别名，既可以指定单个类，也可以指定一个 package，自动转换。配置了别名以后，只需要写别名就可以了，比如 com.gupaoedu.domain.Blog都只要写 blog 就可以了。 MyBatis 里面有系统预先定义好的类型别名，在 TypeAliasRegistry 中。 typeHandlers由于 Java 类型和数据库的 JDBC 类型不是一一对应的（比如 String 与 varchar），所以我们把 Java 对象转换为数据库的值，和把数据库的值转换成 Java 对象，需要经过一定的转换，这两个方向的转换就要用到 TypeHandler。 MyBatis 已经内置了很多 TypeHandler（在 type 包下），它们全部注册在 TypeHandlerRegistry 中，他们都继承了抽象类 BaseTypeHandler，泛型就是要处理的 Java 数据类型。 当我们做数据类型转换的时候，就会自动调用对应的 TypeHandler 的方法。 如果我们需要自定义一些类型转换规则，或者要在处理类型的时候做一些特殊的动作，就可以编写自己的 TypeHandler，跟系统自定义的 TypeHandler 一样，继承抽象类BaseTypeHandler。有 4 个抽象方法必须实现，我们把它分成两类：set 方法从 Java 类型转换成 JDBC 类型的，get 方法是从 JDBC 类型转换成 Java 类型的。 objectFactory当我们把数据库返回的结果集转换为实体类的时候，需要创建对象的实例，由于我们不知道需要处理的类型是什么，有哪些属性，所以不能用 new 的方式去创建。在MyBatis 里面，它提供了一个工厂类的接口，叫做 ObjectFactory，专门用来创建对象的实例，里面定义了 4 个方法。 方法 作用 void setProperties(Properties properties); 设置参数时调用 T create(Class type); 创建对象（调用无参构造函数） T create(Class type, List&lt;Class&lt;?&gt;constructorArgTypes, List 创建对象（调用带参数构造函数） n boolean isCollection(Class type) 判断是否集合 ObjectFactory 有一个默认的实现类 DefaultObjectFactory，创建对象的方法最终都调用了 instantiateClass()，是通过反射来实现的。 如果想要修改对象工厂在初始化实体类的时候的行为，就可以通过创建自己的对象工厂，继承 DefaultObjectFactory 来实现（不需要再实现 ObjectFactory 接口）。 plugins插件是 MyBatis 的一个很强大的机制，跟很多其他的框架一样，MyBatis 预留了插件的接口，让 MyBatis 更容易扩展。 environments 、environmentenvironments 标签用来管理数据库的环境，比如我们可以有开发环境、测试环境、生产环境的数据库。可以在不同的环境中使用不同的数据库地址或者类型。 1234567891011&lt;environments default=&quot;dev&quot;&gt; &lt;environment id=&quot;dev&quot;&gt; &lt;transactionManager type=&quot;JDBC&quot;/&gt; &lt;dataSource type=&quot;POOLED&quot;&gt; &lt;property name=&quot;driver&quot; value=&quot;com.mysql.jdbc.Driver&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://127.0.0.1:3306/aaa&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;Alibaba741&quot;/&gt; &lt;/dataSource&gt; &lt;/environment&gt;&lt;/environments&gt; 一个 environment 标签就是一个数据源，代表一个数据库。这里面有两个关键的标签，一个是事务管理器，一个是数据源。 transactionManager如果配置的是 JDBC，则会使用 Connection 对象的 commit()、rollback()、close()管理事务。 如果配置成 MANAGED，会把事务交给容器来管理，比如 JBOSS，Weblogic。因为我们跑的是本地程序，如果配置成 MANAGE 不会有任何事务。 如 果 是 Spring + MyBatis ， 则 没 有 必 要 配 置 ， 因 为 我 们 会 直 接 在applicationContext.xml 里面配置数据源，覆盖 MyBatis 的配置。 dataSourcemappers标签配置的是我们的映射器，也就是 Mapper.xml 的路径。这里配置的目的是让 MyBatis 在启动的时候去扫描这些映射器，创建映射关系。 使用相对于类路径的资源引用（resource） 使用完全限定资源定位符（绝对路径）（URL） 使用映射器接口实现类的完全限定类名 将包内的映射器接口实现全部注册为映射器（最常用） 完整的配置文件12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE configuration PUBLIC &quot;-//mybatis.org//DTD Config 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-config.dtd&quot;&gt;&lt;configuration&gt; &lt;!-- MyBatis标签 --&gt; &lt;!-- properties： 通过properties属性指定数据源的配置 resource=&quot;jdbc.properties&quot; 通过resource属性引入外部属性文件 --&gt; &lt;properties&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot;&gt;&lt;/property&gt; &lt;property name=&quot;password&quot; value=&quot;root&quot;&gt;&lt;/property&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql:///ssm&quot;&gt;&lt;/property&gt; &lt;property name=&quot;driverName&quot; value=&quot;com.mysql.jdbc.Driver&quot;&gt;&lt;/property&gt; &lt;/properties&gt; &lt;!-- setting mybatis运行时的重要设置，谨慎修改 --&gt; &lt;settings&gt; &lt;!--开启驼峰命名法--&gt; &lt;setting name=&quot;mapUnderscoreToCamelCase&quot; value=&quot;true&quot;/&gt; &lt;/settings&gt; &lt;!-- typeAliases：类型别名 --&gt; &lt;typeAliases&gt; &lt;!--为一个实体类起别名，代替子映射文件的实体类全限定类名--&gt; &lt;!--&lt;typeAlias type=&quot;com.atguigu.pojo.User&quot; alias=&quot;user&quot;/&gt;--&gt; &lt;!--直接为一个包下的所有类起别名，默认类名首字母小写--&gt; &lt;package name=&quot;com.atguigu.pojo&quot;/&gt; &lt;/typeAliases&gt; &lt;!--typeHandlers：自己注册类型处理器--&gt; &lt;!--&lt;typeHandlers&gt;--&gt; &lt;!--&lt;typeHandler handler=&quot;&quot; javaType=&quot;&quot; jdbcType=&quot;&quot;/&gt;--&gt; &lt;!--&lt;/typeHandlers&gt;--&gt; &lt;!--ObjectFactory:配置对象工厂--&gt; &lt;!--plugins:插件--&gt; &lt;plugins&gt; &lt;!--5.0版本之前写pagehelper，5.0以后写PageInterceptor--&gt; &lt;plugin interceptor=&quot;com.github.pagehelper.PageInterceptor&quot;&gt; &lt;!--分页合理化参数--&gt; &lt;property name=&quot;reasonable&quot; value=&quot;true&quot;/&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;environments default=&quot;mysql&quot;&gt; &lt;!--配置环境- 此处不光可以配置mysql 还可以配置SQLserver，db2，oracle 使用时在标签上指定databaseId --&gt; &lt;environment id=&quot;mysql&quot;&gt; &lt;!--配置事务--&gt; &lt;transactionManager type=&quot;JDBC&quot;/&gt; &lt;!--配置连接池： type=&quot;POOLED&quot; 使用连接池 type=&quot;UNPOOLED&quot; 不使用连接池 --&gt; &lt;dataSource type=&quot;POOLED&quot;&gt; &lt;property name=&quot;driver&quot; value=&quot;$&#123;driver&#125;&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;$&#123;url&#125;&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;$&#123;username&#125;&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;$&#123;password&#125;&quot;/&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; &lt;!--映射文件处理器--&gt; &lt;mappers&gt; &lt;!--使用相对于类路径下的资源--&gt; &lt;!--&lt;mapper resource=&quot;com/atguigu/mapper/AccountMapper.xml&quot;/&gt;--&gt; &lt;!--此种方法要求接口与映射文件在同一包下且同名--&gt; &lt;mapper class=&quot;com.atguigu.mapper.UserMapper&quot;/&gt; &lt;!--批量注册：依此制定一个包 此种方法要求接口与映射文件在同一包下且同名 --&gt; &lt;package name=&quot;com/atguigu/mapper&quot;/&gt; &lt;/mappers&gt;&lt;/configuration&gt; 4.关于传递参数是多个基本数据类型在传递参数是多个普通类型的情况下，有两种方案可以传递参数的值到占位符。​ 1234567891011121314第一种情况： args0 表示第一个参数 args1 表示第二个参数 。。。以此类推 argsn 表示第n+1个参数 第二种情况： param1 表示第一个参数 param2 表示第二个参数 。。。以此类推 paramn 表示第n个参数 第三种情况： @Param(&quot;name&quot;) String name, @Param(&quot;sex&quot;) Integer sex 如果传递的参数是Map类型，则在#{}中需要写上map的key值表示传递相应key的值到sql的占位符中。mybatis底层传递参数就是使用的map集合。​ 三，Mybatis连接池和事务深入1.连接池Mybatis 中也有连接池技术，但是它采用的是自己的连接池技术。在 Mybatis 的 主 配置文件中，通过来实现 Mybatis 中连接池的配置。​ Mybatis 将它自己的数据源分为三类：​ UNPOOLED 不使用连接池的数据源 POOLED 使用连接池的数据源 JNDI 使用 JNDI 实现的数据源 ​ 相应地，MyBatis 内部分别定义了实现了 java.sql.DataSource 接口的 UnpooledDataSource，PooledDataSource 类来表示 UNPOOLED、POOLED 类型的数据源。一般采用的是 POOLED 数据源（很多时候我们所说的数据源就是为了更好的管理数据库连接，也就是我们所说的连接池技术）连接池其实就是一个容器，容器就可以用集合来充当，而且他必须具有，队列的特性，先进先出。还得是线程安全的，不能让多个线程拿到同一个连接。​ 1.1 MyBatis中数据源的配置1234567&lt;!-- 配置数据源（连接池）信息 --&gt; &lt;dataSource type=&quot;POOLED&quot;&gt; &lt;property name=&quot;driver&quot; value=&quot;$&#123;jdbc.driver&#125;&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;$&#123;jdbc.url&#125;&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;$&#123;jdbc.username&#125;&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;$&#123;jdbc.password&#125;&quot;/&gt;&lt;/dataSource&gt; ​ MyBatis 在初始化时，根据的 type 属性来创建相应类型的的数据源 DataSource，即：type=”POOLED”：MyBatis 会创建 PooledDataSource 实例type=”UNPOOLED” ： MyBatis 会创建 UnpooledDataSource 实例type=”JNDI”：MyBatis 会从 JNDI 服务上查找 DataSource 实例，然后返回使用 1.2 MyBatis中Datasource的存取MyBatis 是 通 过 工 厂 模 式 来 创 建 数 据 源 DataSource 对 象 的 ， MyBatis 定 义 了 抽 象 的 工 厂 接口:org.apache.ibatis.datasource.DataSourceFactory,通过其 getDataSource()方法返回数据源DataSource。​ MyBatis 创建了 DataSource 实例后，会将其放到 Configuration 对象内的 Environment 对象中， 供以后使用。（具体可以查看一个类，叫 XMLConfigBuilder）。​ 1.3 Mybatis中连接的获取过程分析123456789101112131415161718192021222324252627282930313233343536373839当我们需要创建 SqlSession 对象并需要执行 SQL 语句时，这时候 MyBatis 才会去调用 dataSource 对象来创建java.sql.Connection对象。也就是说，java.sql.Connection对象的创建一直延迟到执行SQL语句的时候。 @Test public void testSql() throws Exception &#123; InputStream in = Resources.getResourceAsStream(&quot;SqlMapConfig.xml&quot;); SqlSessionFactory factory = new SqlSessionFactoryBuilder().build(in); SqlSession sqlSession = factory.openSession(); List&lt;User&gt; list = sqlSession.selectList(&quot;findUserById&quot;,41); System.out.println(list.size()); &#125;只有当第 4 句 sqlSession.selectList(&quot;findUserById&quot;)，才会触发 MyBatis 在底层执行下面这个方法来创建 java.sql.Connection 对象。查看加载过程：通过断点调试，在 PooledDataSource 中找到如下 popConnection()方法，如下所示 **386行左右 if (!state.idleConnections.isEmpty()) &#123; // Pool has available connection //代表池子里有可用连接 conn = state.idleConnections.remove(0); if (log.isDebugEnabled()) &#123; log.debug(&quot;Checked out connection &quot; + conn.getRealHashCode() + &quot; from pool.&quot;); &#125; &#125; else &#123; // Pool does not have available connection if (state.activeConnections.size() &lt; poolMaximumActiveConnections) &#123; // Can create new connection conn = new PooledConnection(dataSource.getConnection(), this); if (log.isDebugEnabled()) &#123; log.debug(&quot;Created connection &quot; + conn.getRealHashCode() + &quot;.&quot;); &#125; &#125; else &#123; // Cannot create new connection PooledConnection oldestActiveConnection = state.activeConnections.get(0); long longestCheckoutTime = oldestActiveConnection.getCheckoutTime(); .... //此处省略一部分，暂时看不懂 &#125;大概意思就是：底层有两个池子，一个是空闲池，一个是存放活跃连接的池子，当获取连接的时候，首先去空闲池子看看有没有空闲的连接，有的话直接拿走，没有的话，就去活跃池子，看看到没到最大连接数，如果到了，就把最先活跃的连接拿过来，把里面绑定的数据都清了，然后拿来用。排序规则就是，拿走最先进来的，然后后面的依次向前补位，比如说拿走0，那么1就会向前补位，变成0，然后后面的依次向前补位。 2.事务控制2.1 JDBC事务回顾在 JDBC 中我们可以通过手动方式将事务的提交改为手动方式，通过 setAutoCommit()方法就可以调整。​ 查找jdk文档：默认为自动提交，当做单个事务处理，可以通过设置true，false来改变。​ 2.2 mybatis事务分析框架的本质也是调用jdk的这个方法，只是进行了一些处理。对于之前的增删该方法：通过查看控制台的日志，可以发现，mybatis对于增删改，默认提交方式是false，我们要在提交之后将他的提交方式设置为true，或者在session.getCommit()方法的（）里面直接传一个true。​ 四，动态SQL我们根据实体类的不同取值，使用不同的 SQL 语句来进行查询。比如在 id 如果不为空时可以根据 id 查询， 如果 username 不同空时还要加入用户名作为条件。这种情况在我们的多条件组合查询中经常会碰到。​ 1.抽取重复代码片段123456&lt;!-- 抽取重复的语句代码片段 --&gt;&lt;sql id=&quot;defaultSql&quot;&gt; select * from user&lt;/sql&gt;&lt;!-- 然后就可以在标签中对抽取出来的语句进行引用 --&gt;&lt;include refid=&quot;defaultSql&quot;/&gt; 2.if1234/** * 根据id查询 */User findById(User user); 12345678&lt;!--根据id查询--&gt;&lt;select id=&quot;findById&quot; parameterType=&quot;com.yhd.domain.User&quot; resultType=&quot;com.yhd.domain.User&quot;&gt; &lt;include refid=&quot;defaultSql&quot;/&gt; where 1=1 &lt;if test=&quot;id!=null and id!=&#x27;&#x27;&quot;&gt; and id = #&#123;id&#125;; &lt;/if&gt;&lt;/select&gt; 注意 where 1=1 的作用~！ 1234567&lt;!--Account selectByIdSelective(Integer id);--&gt;&lt;select id=&quot;selectByIdSelective&quot; parameterType=&quot;int&quot; resultMap=&quot;baseAccountMap&quot;&gt; select id ,name ,money from account where 1=1 &lt;if test=&quot;_parameter!=null and _parameter !=&#x27;&#x27;&quot;&gt; and id = #&#123;_parameter&#125; &lt;/if&gt;&lt;/select&gt; 3. where1234 /** * 根据id查询 */User findById(User user); 123456789&lt;!--根据id查询--&gt; &lt;select id=&quot;findById&quot; parameterType=&quot;com.yhd.domain.User&quot; resultType=&quot;com.yhd.domain.User&quot;&gt; &lt;include refid=&quot;defaultSql&quot;/&gt; &lt;where&gt; &lt;if test=&quot;id!=null and id!=&#x27;&#x27;&quot;&gt; and id = #&#123;id&#125; &lt;/if&gt; &lt;/where&gt; &lt;/select&gt; 有了where就不用写1=1了。​ 4.foreach1List&lt;Account&gt; selectByIds(Integer []ids); 12345678910111213&lt;!--List&lt;Account&gt; selectByIds(Integer []ids);--&gt;&lt;select id=&quot;selectByIds&quot; parameterType=&quot;int&quot; resultMap=&quot;baseAccountMap&quot;&gt; select id ,name ,money from account &lt;where&gt; &lt;if test=&quot;_parameter!=null and _parameter.size()&gt;0&quot;&gt; id in &lt;foreach collection=&quot;array&quot; item=&quot;id&quot; open=&quot;(&quot; close=&quot;)&quot; separator=&quot;,&quot;&gt; #&#123;id&#125; &lt;/foreach&gt; &lt;/if&gt; &lt;/where&gt;&lt;/select&gt; SQL 语句：select 字段 from user where id in (?)标签用于遍历集合，它的属性：collection:代表要遍历的集合元素，注意编写时不要写#{}open:代表语句的开始部分close:代表结束部分item:代表遍历集合的每个元素，生成的变量名sperator:代表分隔符 五，Mybatis多表查询1.一对一查询代码：mybatis-04 2.一对多查询代码：mybatis-05 3.多对多查询代码：mybatis-06​ 六，延迟加载延迟加载：就是在需要用到数据时才进行加载，不需要用到数据时就不加载数据。延迟加载也称懒加载. 好处：先从单表查询，需要时再从关联表去关联查询，大大提高数据库性能，因为查询单表要比关联查询多张表速度要快。​ 坏处：因为只有当需要用到数据时，才会进行数据库查询，这样在大批量数据查询时，因为查询工作也要消耗时间，所以可能造成用户等待时间变长，造成用户体验下降。 ​ 如何开启懒加载策略？​ 12345&lt;!-- 开启延迟加载的支持 --&gt;&lt;settings&gt; &lt;setting name=&quot;lazyLoadingEnabled&quot; value=&quot;true&quot;/&gt; &lt;setting name=&quot;aggressiveLazyLoading&quot; value=&quot;false&quot;/&gt;&lt;/settings&gt; 1.一对一懒加载​ mybatis-07​ 2.多对一懒加载mybatis-08​ 七，缓存机制1.一级缓存​ 一级缓存也叫本地缓存，MyBatis 的一级缓存是在会话（SqlSession）层面进行缓存的。MyBatis 的一级缓存是默认开启的，不需要任何的配置。 要在同一个会话里面共享一级缓存，这个对象肯定是在 SqlSession 里面创建的，作为 SqlSession 的一个属性。 DefaultSqlSession 里面只有两个属性，Configuration 是全局的，所以缓存只可能放在 Executor 里面维护——SimpleExecutor/ReuseExecutor/BatchExecutor 的父类BaseExecutor 的构造函数中持有了 PerpetualCache。 在同一个会话里面，多次执行相同的 SQL 语句，会直接从内存取到缓存的结果，不会再发送 SQL 到数据库。但是不同的会话里面，即使执行的 SQL 一模一样（通过一个Mapper 的同一个方法的相同参数调用），也不能使用到一级缓存。​ 一级缓存在 BaseExecutor 的 query()——queryFromDatabase()中存入。在queryFromDatabase()之前会 get()。 一级缓存是在 BaseExecutor 中的 update()方法中调用 clearLocalCache()清空的（无条件），query 中会判断。 一级缓存的 不足 使用一级缓存的时候，因为缓存不能跨会话共享，不同的会话之间对于相同的数据可能有不一样的缓存。在有多个会话或者分布式环境下，会存在脏数据的问题。如果要解决这个问题，就要用到二级缓存。​ 虽然在上面的代码中我们查询了两次，但最后只执行了一次数据库操作，这就是 Mybatis 提供给我们的一级缓存在起作用了。因为一级缓存的存在，导致第二次查询 id 为 41 的记录时，并没有发出 sql 语句从数据库中查询数据，而是从一级缓存中查询。​ 如何清空一级缓存？​ 一级缓存是 SqlSession 范围的缓存，当调用 SqlSession 的修改，添加，删除，commit()，close()等方法时，就会清空一级缓存。​ 第一次发起查询用户 id 为 1 的用户信息，先去找缓存中是否有 id 为 1 的用户信息，如果没有，从数据库查询用户信息。得到用户信息，将用户信息存储到一级缓存中。​ 如果 sqlSession 去执行 commit 操作（执行插入、更新、删除），清空 SqlSession 中的一级缓存，这样做的目的为了让缓存中存储的是最新的信息，避免脏读。​ 第二次发起查询用户 id 为 1 的用户信息，先去找缓存中是否有 id 为 1 的用户信息，缓存中有，直接从缓存中获取用户信息。​ 2.二级缓存二级缓存是用来解决一级缓存不能跨会话共享的问题的，范围是 namespace 级别的，可以被多个 SqlSession 共享（只要是同一个接口里面的相同方法，都可以共享），生命周期和应用同步。 作为一个作用范围更广的缓存，它肯定是在 SqlSession 的外层，否则不可能被多个SqlSession 共享。而一级缓存是在 SqlSession 内部的，所以，肯定是工作在一级缓存之前，也就是只有取不到二级缓存的情况下才到一个会话中去取一级缓存。 要跨会话共享的话，SqlSession 本身和它里面的 BaseExecutor 已经满足不了需求了，那我们应该在 BaseExecutor 之外创建一个对象。 实际上 MyBatis 用了一个装饰器的类来维护，就是 CachingExecutor。如果启用了二级缓存，MyBatis 在创建 Executor 对象的时候会对 Executor 进行装饰。CachingExecutor 对于查询请求，会判断二级缓存是否有缓存结果，如果有就直接返回，如果没有委派交给真正的查询器 Executor 实现类，比如 SimpleExecutor 来执行查询，再走到一级缓存的流程。最后会把结果缓存起来，并且返回给用户。​ ​ 二级缓存的开启与关闭​ 主配置文件12345678&lt;!-- 声明这个 namespace 使用二级缓存 --&gt;&lt;cache type=&quot;org.apache.ibatis.cache.impl.PerpetualCache&quot; size=&quot;1024&quot; eviction=&quot;LRU&quot; flushInterval=&quot;120000&quot; readOnly=&quot; false&quot;&gt; &lt;!--自动刷新时间 ms，未配置时只有调用时刷新 --&gt; &lt;!-- 回收策略--&gt; &lt;!-- 最多缓存对象个数，默认 1024--&gt; &lt;!-- 默认是 false（安全），改为 true 可读写时，对象必须支持序列化--&gt;&lt;/cache&gt; 因为 cacheEnabled 的取值默认就为 true，所以这一步可以省略不配置。为 true 代表开启二级缓存；为false 代表不开启二级缓存。​ mapper映射文件123456789&lt;mapper namespace=&quot;com.itheima.dao.IUserDao&quot;&gt; &lt;!-- 开启二级缓存的支持 --&gt; &lt;cache&gt;&lt;/cache&gt; &lt;!-- 根据 id 查询 --&gt; &lt;!-- 在此处将userCache属性设置为true --&gt; &lt;select id=&quot;findById&quot; resultType=&quot;user&quot; parameterType=&quot;int&quot; useCache=&quot;true&quot;&gt; select * from user where id = #&#123;uid&#125; &lt;/select&gt;&lt;/mapper&gt; 标签表示当前这个 mapper 映射将使用二级缓存，区分的标准就看 mapper 的 namespace 值。 将 UserDao.xml 映射文件中的标签中设置 useCache=”true”代表当前这个 statement 要使用 二级缓存，如果不使用二级缓存可以设置为 false。 注意：针对每次查询都需要最新的数据 sql，要设置成 useCache=false，禁用二级缓存。​ 当我们在使用二级缓存时，所缓存的类一定要实现 java.io.Serializable 接口，这种就可以使用序列化方式来保存对象。否则会报java,io.SerializableException。​ Mapper.xml 配置了之后，select()会被缓存。update()、delete()、insert()会刷新缓存。 如果 cacheEnabled=true，Mapper.xml 没有配置标签，还有二级缓存吗？还会出现 CachingExecutor 包装对象吗？ 只要 cacheEnabled=true 基本执行器就会被装饰。有没有配置，决定了在启动的时候会不会创建这个 mapper 的 Cache 对象，最终会影响到 CachingExecutorquery 方法里面的判断： 1if (cache != null) 如果某些查询方法对数据的实时性要求很高，不需要二级缓存，怎么办？ 可以在单个 Statement ID 上显式关闭二级缓存（默认是 true） 12&lt;select id=&quot;selectBlog&quot; resultMap=&quot;BaseResultMap&quot; useCache=&quot;false&quot;&gt;&lt;/select&gt; 为什么事务不提交，二级缓存不生效？ 因为二级缓存使用 TransactionalCacheManager（TCM）来管理，最后又调用了TransactionalCache的getObject()、putObject和commit()方法，TransactionalCache里面又持有了真正的 Cache 对象，比如是经过层层装饰的 PerpetualCache。在 putObject 的时候，只是添加到了 entriesToAddOnCommit 里面，只有它的commit()方法被调用的时候才会调用 flushPendingEntries()真正写入缓存。它就是在DefaultSqlSession 调用 commit()的时候被调用的。 为什么增删改操作会清空缓存？ 在 CachingExecutor 的 update()方法里面会调用 flushCacheIfRequired(ms)，isFlushCacheRequired 就是从标签里面渠道的 flushCache 的值。而增删改操作的flushCache 属性默认为 true。 第三方缓存 做 二级缓存 除了 MyBatis 自带的二级缓存之外，我们也可以通过实现 Cache 接口来自定义二级缓存。 MyBatis 官方提供了一些第三方缓存集成方式，比如 ehcache 和 redis。​ 八，Mybatis扩展1.批量操作在 MyBatis 里面是支持批量的操作的，包括批量的插入、更新、删除。我们可以直接传入一个 List、Set、Map 或者数组，配合动态 SQL 的标签，MyBatis 会自动帮我们生成语法正确的 SQL 语句。 1.1 批量插入批量插入的语法是这样的，只要在 values 后面增加插入的值就可以了。 1insert into tbl_emp (emp_id, emp_name, gender,email, d_id) values ( ?,?,?,?,? ) , ( ?,?,?,?,? ) , ( ?,?,?,?,? ) , ( ?,?,?,?,? ) ,( ?,?,?,?,? ) , ( ?,?,?,?,? ) , ( ?,?,?,?,? ) , ( ?,?,?,?,? ) , ( ?,?,?,?,? ) , ( ?,?,?,?,? ) 在 Mapper 文件里面，我们使用 foreach 标签拼接 values 部分的语句： 12345678910&lt;!-- 批量插入 --&gt;&lt;insert id=&quot;batchInsert&quot; parameterType=&quot;java.util.List&quot; useGeneratedKeys=&quot;true&quot;&gt; &lt;selectKey resultType=&quot;long&quot; keyProperty=&quot;id&quot; order=&quot;AFTER&quot;&gt; SELECT LAST_INSERT_ID() &lt;/selectKey&gt; insert into tbl_emp (emp_id, emp_name, gender,email, d_id) values &lt;foreach collection=&quot;list&quot; item=&quot;emps&quot; index=&quot;index&quot; separator=&quot;,&quot;&gt; (#&#123;emps.empId&#125;,#&#123;emps.empName&#125;,#&#123;emps.gender&#125;,#&#123;emps.email&#125;,#&#123;emps.dId&#125;) &lt;/foreach&gt;&lt;/insert&gt; Java 代码里面，直接传入一个 List 类型的参数。 效率要比循环发送 SQL 执行要高得多。最关键的地方就在于减少了跟数据库交互的次数，并且避免了开启和结束事务的时间消耗。 1.2 批量更新批量更新的语法是这样的，通过 case when，来匹配 id 相关的字段值。 1234567891011121314151617update tbl_emp setemp_name = case emp_id when ? then ? when ? then ? when ? then ? end , gender = case emp_id when ? then ? when ? then ? when ? then ? end , email = case emp_id when ? then ? when ? then ? when ? then ? endwhere emp_id in ( ? , ? , ? ) 所以在 Mapper 文件里面最关键的就是 case when 和 where 的配置。需要注意一下 open 属性和 separator 属性。 12345678910111213141516171819202122&lt;update id=&quot;updateBatch&quot;&gt; update tbl_emp set emp_name = &lt;foreach collection=&quot;list&quot; item=&quot;emps&quot; index=&quot;index&quot; separator=&quot; &quot; opene=&quot;case emp_id&quot; close=&quot;end&quot;&gt; when #&#123;emps.empId&#125; then #&#123;emps.empName&#125; &lt;/ foreach&gt; ,gender = &lt;foreach collection=&quot;list&quot; item=&quot;emps&quot; index=&quot;index&quot; separator=&quot; &quot; opene=&quot;case emp_id&quot; close=&quot;end&quot;&gt; when #&#123;emps.empId&#125; then #&#123;emps.gender&#125; &lt;/foreach&gt; ,email = &lt;foreach collection=&quot;list&quot; item=&quot;emps&quot; index=&quot;index&quot; separator=&quot; &quot; opene=&quot;case emp_id&quot; close=&quot;end&quot;&gt; when #&#123;emps.empId&#125; then #&#123;emps.email&#125; &lt;/foreach&gt; where emp_id in &lt;foreach collection=&quot;list&quot; item=&quot;emps&quot; index=&quot;index&quot; separator=&quot;,&quot; open=&quot;(&quot; close=&quot;)&quot;&gt; #&#123;emps.empId&#125; &lt;/foreach&gt;&lt;/update&gt; 1.3 BatchExecutor当然 MyBatis 的动态标签的批量操作也是存在一定的缺点的，比如数据量特别大的时候，拼接出来的 SQL 语句过大。 MySQL 的服务端对于接收的数据包有大小限制，max_allowed_packet 默认是4M，需要修改默认配置才可以解决这个问题。在我们的全局配置文件中，可以配置默认的 Executor 的类型。其中有一种BatchExecutor。 1&lt;setting name =&quot;defaultExecutorType&quot; value =&quot;BATCH&quot; /&gt; 也可以在创建会话的时候指定执行器类型 1SqlSession session = sqlSessionFactory.openSession(ExecutorType. BATCH ); BatchExecutor 底层是对 JDBC ps.addBatch()的封装，原理是攒一批 SQL 以后再发。 2.翻页在我们查询数据库的操作中，有两种翻页方式，一种是逻辑翻页（假分页），一种是物理翻页（真分页）。逻辑翻页的原理是把所有数据查出来，在内存中删选数据。 物理翻页是真正的翻页，比如 MySQL 使用 limit 语句，Oracle 使用 rownum 语句，SQLServer 使用 top 语句。 2.1逻辑翻页MyBatis 里面有一个逻辑分页对象 RowBounds，里面主要有两个属性，offset 和limit（从第几条开始，查询多少条）。 我们可以在Mapper接口的方法上加上这个参数，不需要修改xml里面的SQL语句。 1public List&lt;Blog&gt; selectBlogList(RowBounds rowBounds); 它的底层其实是对 ResultSet 的处理。它会舍弃掉前面 offset 条数据，然后再取剩下的数据的 limit 条。 如果数据量大的话，这种翻页方式效率会很低（跟查询到内存中再使用subList(start,end)没什么区别）。所以我们要用到物理翻页。 2.2物理翻页物理翻页是真正的翻页，它是通过数据库支持的语句来翻页。 第一种简单的办法就是传入参数（或者包装一个 page 对象），在 SQL 语句中翻页。 123&lt;select id=&quot;selectBlogPage&quot; parameterType=&quot;map&quot; resultMap=&quot;BaseResultMap&quot;&gt; select * from blog limit #&#123;curIndex&#125; , #&#123;pageSize&#125;&lt;/ select&gt; 第一个问题是我们要在 Java 代码里面去计算起止序号；第二个问题是：每个需要翻页的 Statement 都要编写 limit 语句，会造成 Mapper 映射器里面很多代码冗余。 需要一种通用的方式，不需要去修改配置的任何一条 SQL 语句，只要在我们需要翻页的地方封装一下翻页对象就可以了。 使用翻页的插件，这个是基于 MyBatis 的拦截器实现的，比如 PageHelper。 123456// pageSize 每一页几条PageHelper. startPage (pn, 10);List&lt;Employee&gt; emps = employeeService.getAll();// navigatePages 导航页码数PageInfo page = w new PageInfo(emps, 10);return Msg. success ().add( &quot;pageInfo&quot;, page); 3.通用Mapper问题：当我们的表字段发生变化的时候，我们需要修改实体类和 Mapper 文件定义的字段和方法。如果是增量维护，那么一个个文件去修改。如果是全量替换，我们还要去对比用 MBG 生成的文件。字段变动一次就要修改一次，维护起来非常麻烦。 解决这个问题，我们有两种思路。 第 一 个 ， 因 为 MyBatis 的 Mapper 是 支 持 继 承 的 。 所 以 我 们 可 以 把 我 们 的Mapper.xml 和 Mapper 接口都分成两个文件。一个是 MBG 生成的，这部分是固定不变的。然后创建 DAO 类继承生成的接口，变化的部分就在 DAO 里面维护。 GitHub地址 4.MyBatis-PlusMyBatis-Plus 是原生 MyBatis 的一个增强工具，可以在使用原生 MyBatis 的所有功能的基础上，使用 plus 特有的功能。​ 九，Mybatis注解开发1.常用注解 @Insert:实现新增@Update:实现更新@Delete:实现删除@Select:实现查询@Result:实现结果集封装@Results:可以与@Result 一起使用，封装多个结果集@ResultMap:实现引用@Results 定义的封装@One:实现一对一结果集封装@Many:实现一对多结果集封装@SelectProvider: 实现动态 SQL 映射@CacheNamespace:实现注解二级缓存的使用 ​ 2.使用Mybatis注解实现CRUD2.1 复杂映射实现复杂关系映射之前我们可以在映射文件中通过配置来实现，在使用注解开发时我们需要借助@Results 注解，@Result 注解，@One 注解，@Many 注解。​ @Results 注解代替的是标签该注解中可以使用单个@Result 注解，也可以使用@Result 集合@Results（{@Result（），@Result（）}）或@Results（@Result（））@Resutl 注解代替了 标签和标签@Result 中 属性介绍：id 是否是主键字段column 数据库的列名property 需要装配的属性名one 需要使用的@One 注解（@Result（one=@One）（）））many 需要使用的@Many 注解（@Result（many=@many）（）））@One 注解（一对一）代替了标签，是多表查询的关键，在注解中用来指定子查询返回单一对象。@One 注解属性介绍：select 指定用来多表查询的 sqlmapperfetchType 会覆盖全局的配置参数 lazyLoadingEnabled。。使用格式：@Result(column=” “,property=””,one=@One(select=””))@Many 注解（多对一）代替了标签,是是多表查询的关键，在注解中用来指定子查询返回对象集合。注意：聚集元素用来处理“一对多”的关系。需要指定映射的 Java 实体类的属性，属性的 javaType（一般为 ArrayList）但是注解中可以不定义；使用格式：@Result(property=””,column=””,many=@Many(select=””)) 2.2 二级缓存 主配置文件 ​ 12345&lt;!-- 配置二级缓存 --&gt; &lt;settings&gt; &lt;!-- 开启二级缓存的支持 --&gt; &lt;setting name=&quot;cacheEnabled&quot; value=&quot;true&quot;/&gt;&lt;/settings&gt; 映射文件1234@CacheNamespace(blocking=true)//mybatis 基于注解方式实现配置二级缓存public interface UserMapper &#123; &#125;","categories":[{"name":"MyBatis","slug":"MyBatis","permalink":"https://yinhuidong.github.io/categories/MyBatis/"}],"tags":[{"name":"MyBatis","slug":"MyBatis","permalink":"https://yinhuidong.github.io/tags/MyBatis/"}]},{"title":"MyBatis源码分析","slug":"MyBatis/MyBatis源码分析","date":"2022-01-11T06:37:14.818Z","updated":"2022-01-11T06:41:11.784Z","comments":true,"path":"2022/01/11/MyBatis/MyBatis源码分析/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/MyBatis/MyBatis%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","excerpt":"","text":"MyBatis SQL Mapper Framework for Java The MyBatis SQL mapper framework makes it easier to use a relational database with object-oriented applications.MyBatis couples objects with stored procedures or SQL statements using an XML descriptor or annotations.Simplicity is the biggest advantage of the MyBatis data mapper over object relational mapping tools. 一，Mybatis原生使用方式1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public class EsTest &#123; /** * 字节流 */ private InputStream in; /** * 数据库连接工厂 */ private SqlSessionFactory sqlSessionFactory; /** * 数据库连接 */ private SqlSession sqlSession; &#123; try &#123; //将配置文件以二进制字节流的方式加载到内存 in = Resources.getResourceAsStream(&quot;mybatis-config.xml&quot;); //通过构建者模式创建一个数据库连接工厂 sqlSessionFactory = new SqlSessionFactoryBuilder().build(in); //通过工厂来管理和获取数据库连接 sqlSession = sqlSessionFactory.openSession(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; public void query() &#123; //获取一个指定类型的mapper代理对象 UserMapper userMapper = sqlSession.getMapper(UserMapper.class); //代理对象执行目标方法 List&lt;Object&gt; users = userMapper.queryUser(1L); users.forEach(System.out::println); &#125; private static interface UserMapper &#123; List&lt;Object&gt; queryUser(Long id); &#125; /** * 利用对象的finalize 方法进行资源回收 * @throws Throwable */ @Override protected void finalize() throws Throwable &#123; sqlSession.commit(true); assert sqlSession != null; sqlSession.close(); assert in != null; try &#123; in.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 分析下流程： 通过Resources对象加载Mybatis主配置文件到内存形成一个二进制的字节流。 将上一步生成的二进制字节流当做参数传递到SqlSessionFactoryBuilder 的build()，用来构建数据库连接工厂 SqlSessionFactory。 通过生成的数据库连接工厂获取数据库连接。 通过数据库连接获取指定类型的Mapper的代理对象。 通过代理对象调用方法，获得结果。 关闭数据库连接，关闭字节流。 二，Mybatis源码执行流程分析 首先，通过Resources对象里面的方法去加载配置文件，这里会默认传入一个类加载器数组，循环尝试使用各种类加载器加载配置文件，直到获取到二进制字节流，如果最终仍然没有获取到，会抛出异常。 如果成功加载配置文件生成了二进制字节流，那么会将二进制字节流传入到SqlSessionFactoryBuilder的build方法，为生成一个数据库连接工厂对象 SqlSessionFactory 对象赋能。 首先在build方法里面会去构建一个xml解析器对象 XMLConfigBuilder，用来解析主配置文件。 通过解析器的parse()返回了一个Configuration对象，用来构建数据库连接工厂。 判断是否已经加载过主配置文件，如果已经加载过，会抛出异常。 从configuration标签开始解析配置文件。 mybatis的主配置文件里面的标签是有顺序的，他会按照顺序来解析配置文件中的标签，重点在于解析mappers标签。 他会循环获取到所有的mapper.xml文件，利用mapper解析器解析mapper.xml文件，解析封装的到Configuration对象中。 具体的解析过程：首先去定位根标签mapper，然后绑定mapper的命名空间，把所有的命名空间放到configuration对象的一个set集合里面，把当前mapper的类型通过MapperRegistry对象添加到一个map里面。 具体放到Mapper注册中心的其实是Mapper对象的类型和Mapper接口的代理工厂。 最终解析完mappers标签以后，返回了一个Configuration对象。 最终通过build方法返回了一个默认的数据库连接工厂对象，DefaultSqlSessionFactory，这个工厂里面持有一个Configuration对象。 获取到数据库连接工厂以后就是去通过openSession()去获取数据库连接。 SqlSession的获取主要是通过SqlSessionFactory的默认实现类DefaultSqlSessionFactory的openSessionFromDataSource封装一个DefaultSqlSession(实现SqlSession接口）返回。 首先通过Configuration对象获取环境信息。 再通过环境信息获取事务工厂，事务工厂主要是看配置文件有没有配置，没有配置的话就创建一个新的。 通过事务工厂来获取事务对象。 通过一个Configuration对象来创建一个Executor对象，使用它来执行SQL。 判断executorType的类型，分为批处理，可复用和普通三种类型的Executor对象。 SimpleExecutor：每执行一次 update 或 select，就开启一个 Statement 对象，用完立刻关闭 Statement 对象 ReuseExecutor：用完后，不关闭 Statement 对象，而是放置于 Map 内，供下一次使用 BatchExecutor：里面缓存了多个statement用来做批处理 上面选择完具体的Executor对象后，判断是否开启了二级缓存，如果开启了二级缓存的话，使用装饰器模式对Executor进行一个包装，生成一个CachingExecutor对象，里面持有一个Executor对象。 在此处会执行插件的拦截器链，这个拦截器链是mybatis的一个很核心的扩展点机制，最终会返回一个executor对象。 回头看一下Configuration对象，这个Configuration对象很有意思，里面间接的持有几个对象 ，为什么是间接？因为类里面没有这几个对象的属性，但是却可以通过当前类创建这几个对象。 Executor 对象 StatementHandler 对象 PameterHandler 对象 ResultSetHandler 对象 mybatis的插件拦截器会对这四个接口进行拦截，也就是说会对这四种对象生成代理对象，mybatis 的拦截器用到了责任链+代理+反射机制。（通过源码可以知道：所有可能被拦截的处理类都会生成一个代理类，如果有N个拦截器，就会有N个代理，层层生成动态代理是比较消耗性能的。而且虽然能指定插件拦截的位置，但这个是在执行方法的时候利用反射动态判断的，初始化的时候就是简单的把拦截器插入到了所有可以拦截的地方。所以尽量不要编写不必要的拦截器。）其实mybatis的插件实现原理和spring的aop的实现原理是一样的，就是一个多重的代理，多重的代理有两种实现方式，一个是通过责任链 返回一个默认的 SQLSession ，这个DefaultSQLSession里面持有 Configuration对象 executor对象 ，executor对象里面持有一个事务对象。 通过SqlSession对象的getMapper方法获取一个指定类型的mapper代理对象。 数据库连接对象里面的getMapper实际上调用了configuration对象的getMapper方法。 通过mapperRegistry去获取一个mapper的代理对象。 前面解析配置文件的时候，将mapper对象类型和mapper的代理工厂封装到了一个map，这里实际上从这个map里面拿出来了一个mapper的代理工厂。 使用代理工厂去创建对象，通过传递数据库连接去创建一个mapperProxy对象，这个mapperProxy实现了InvocationHandler接口。 通过mapperProxy返回一个代理对象，实际上就是使用JDK的动态代理创建一个代理对象。 当代理对象执行目标方法的时候：实际上就是执行mapperProxy的invoke方法。 这里对目标方法进行一个包装，生成一个invoker，通过invoker执行invoke()。 实际上这里调用了MapperMethod的execute方法。 在execute方法里面实际上就是判断执行的增删改查的类型，然后调用SqlSession的crud方法。(动态代理实际上就是生成了一个statement的字符串，然后调用SqlSession的crud方法。) 以sqlSession.selectOne()进行分析 从configuration对象构建一个MappedStatement对象，然后执行executor的query方法进行查询，executor分为三种： 一个是批处理的 一个是走二级缓存的 一个是BaseExecutor，直接执行的 接下来看executor的query方法： 组装构建待执行的SQL。 创建一级缓存的缓存key，一级缓存默认是开启的。 方法重载query() 判断如果命中一级缓存的话，直接返回。 否则的话，queryFromDatabase 直接去查询数据库 委派给子类取走真正的查询逻辑，然后将查询结果房放到一级缓存。 在子类里面通过原生jdbc的prepareStatement执行查询sql，查询之后通过ResultHandler对象去处理结果，最终返回。 二十中文注释版源码地址：https://gitee.com/yin_huidong/mybatis-3.git 三，mybatis整体架构分析mybatis的整体架构分为三层，分别是基础支持层，核心处理层，接口层。 接口层核心对象是 SqlSession，它是上层应用和 MyBatis打交道的桥梁，SqlSession 上定义了非常多的对数据库的操作方法。接口层在接收到调用请求的时候，会调用核心处理层的相应模块来完成具体的数据库操作。 核心处理层既然叫核心处理层，也就是跟数据库操作相关的动作都是在这一层完成的。 核心处理层主要做了这几件事： 把接口中传入的参数解析并且映射成 JDBC 类型； 解析 xml 文件中的 SQL 语句，包括插入参数，和动态 SQL 的生成； 执行 SQL 语句； 处理结果集，并映射成 Java 对象。 插件也属于核心层，这是由它的工作方式和拦截的对象决定的。 基础支持层基础支持层主要是一些抽取出来的通用的功能（实现复用），用来支持核心处理层的功能。比如数据源、缓存、日志、xml 解析、反射、IO、事务等等这些功能。 2.再看mybatis的SQL执行流程 SQL语句的执行设涉及到很多个组件，其中比较重要的就是Executor，StatementHandler，ParameterHandler，ResultSetHandler。Executor主要负责维护一级缓存和二级缓存，并提供事务管理的相关操作。他会将数据库相关的操作交给StatementHandler完成。StatementHandler首先通过ParameterHandler完成SQL语句的实参绑定，然后通过jdk内置的Statement对象执行SQL语句并得到结果集，最后通过ResultSetHandler完成结果集的映射，得到结果对象并返回。 3. 核心对象生命周期3.1 SqlSessionFactoryBuiler它 是 用 来 构 建 SqlSessionFactory 的 ， 而SqlSessionFactory 只需要一个，所以只要构建了这一个 SqlSessionFactory，它的使命就完成了，也就没有存在的意义了。所以它的生命周期只存在于方法的局部。 3.2 SqlSessionFactorySqlSessionFactory 是用来创建 SqlSession 的，每次应用程序访问数据库，都需要创建一个会话。因为我们一直有创建会话的需要，所以 SqlSessionFactory 应该存在于应用的整个生命周期中（作用域是应用作用域）。创建 SqlSession 只需要一个实例来做这件事就行了，否则会产生很多的混乱，和浪费资源。所以我们要采用单例模式。 3.3 SqlSessionSqlSession 是一个会话，因为它不是线程安全的，不能在线程间共享。所以我们在请求开始的时候创建一个 SqlSession 对象，在请求结束或者说方法执行完毕的时候要及时关闭它（一次请求或者操作中）。 3.4 MapperMapper（实际上是一个代理对象）是从 SqlSession 中获取的。它的作用是发送 SQL 来操作数据库的数据。它应该在一个 SqlSession 事务方法之内。 对象 生命周期 SqlSessionFactoryBuiler 方法局部（method） SqlSessionFactory（单例） 应用级别（application） SqlSession 请求和操作（request/method） Mapper 方法（method） 四，扩展：PageHelper原理上面分析源码的时候其实已经分析过，mybatis的拦截器实际上就是代理模式加拦截器来实现的（同AOP），而pagehelper实际上是基于插件机制实现的。 先看 PageHelper jar 包中 PageInterceptor 的源码。拦截的是 Executor 的两个query()方法。在这里对 SQL 进行了改写。 跟踪到最后，是在 MySqlDialect.getPageSql()对 SQL 进行了改写，翻页参数是从一个 Page 对象中拿到的，那么 Page 对象是怎么传到这里的呢？ 上一步，AbstractHelperDialect.getPageSql()中：Page 对象是从一个 ThreadLocal&lt;&gt;变量中拿到的，那它是什么时候赋值的？ PageHelper.startPage()方法，把分页参数放到了 ThreadLocal&lt;&gt;变量中。 扩展：插件机制的应用场景： 作用 实现方式 水平分表 对 query update 方法进行拦截在接口上添加注解，通过反射获取接口注解，根据注解上配置的参数进行分表，修改原 SQL，例如 id 取模，按月分表 数据加解密 update——加密；query——解密获得入参和返回值 菜单权限控制 对 query 方法进行拦截在方法上添加注解，根据权限配置，以及用户登录信息，在 SQL 上加上权限过滤条件 五，整合Spring大部分时候我们不会在项目中单独使用 MyBatis 的工程，而是集成到 Spring 里面使用，但是却没有看到这三个对象在代码里面的出现。我们直接注入了一个 Mapper 接口，调用它的方法。 SqlSessionFactory 是什么时候创建的？ SqlSession 去哪里了？为什么不用它来 getMapper？ 为什么@Autowired 注入一个接口，在使用的时候却变成了代理对象？在 IOC的容器里面我们注入的是什么？ 注入的时候发生了什么事情？ 1.关键配置12345&lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt; &lt;version&gt;2.0.6&lt;/version&gt;&lt;/dependency&gt; 然后在 Spring 的 applicationContext.xml 里面配置 SqlSessionFactoryBean，它是用来帮助我们创建会话的，其中还要指定全局配置文件和 mapper 映射器文件的路径。 12345&lt;bean id=&quot;sqlSessionFactory&quot; class=&quot;org.mybatis.spring.SqlSessionFactoryBean&quot;&gt; &lt;property name=&quot;configLocation&quot; value=&quot;classpath:mybatis-config.xml&quot;&gt;&lt;/ property&gt; &lt;property name=&quot;mapperLocations&quot; value=&quot;classpath:mapper/*.xml&quot;&gt;&lt;/ property&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt;&lt;/ bean&gt; 然后在 applicationContext.xml 配置需要扫描 Mapper 接口的路径。 123&lt;bean id=&quot;mapperScanner&quot; class=&quot;org.mybatis.spring.mapper.MapperScannerConfigurer&quot;&gt; &lt;property name=&quot;basePackage&quot; value=&quot;com.yhd.crud.dao&quot;/&gt;&lt;/ bean&gt; 1&lt;mybatis-springn:scan base-package =&quot;com.yhd.crud.dao&quot;/&gt; 1@MapperScan( &quot;com.yhd.crud.dao&quot;) Spring 对 MyBatis 的对象进行了管理，但是并不会替换 MyBatis 的核心对象。也就意味着：MyBatis jar 包中的 SqlSessionFactory、SqlSession、MapperProxy 这些都会用到。而 mybatis-spring.jar 里面的类只是做了一些包装或者桥梁的工作。 2.创建会话工厂我们在 Spring 的配置文件中配置了一个 SqlSessionFactoryBean，我们来看一下这个类。 它实现了 InitializingBean 接口，所以要实现 afterPropertiesSet()方法，这个方法会在 bean 的属性值设置完的时候被调用 另外它实现了 FactoryBean 接口，所以它初始化的时候，实际上是调用 getObject()方法，它里面调用的也是 afterPropertiesSet()方法。 在 afterPropertiesSet()方法里面：解析配置文件，指定事务工厂。 3.创建SqlSession3.1可以直接使用 DefaultSqlSession 吗？现在已经有一个 DefaultSqlSessionFactory，按照编程式的开发过程，我们接下来就会创建一个 SqlSession 的实现类，但是在 Spring 里面，我们不是直接使用DefaultSqlSession 的，而是对它进行了一个封装，这个 SqlSession 的实现类就是SqlSessionTemplate。这个跟 Spring 封装其他的组件是一样的，比如 JdbcTemplate，RedisTemplate 等等，也是 Spring 跟 MyBatis 整合的最关键的一个类。 为什么不用 DefaultSqlSession？它是线程不安全的，注意看类上的注解：而 SqlSessionTemplate 是线程安全的。 1Note that this class is not Thread-Safe. 3.2怎么拿到一个 SqlSessionTemplate ？MyBatis提供了一个 SqlSessionDaoSupport，里面持有一个SqlSessionTemplate 对象，并且提供了一个 getSqlSession()方法，让我们获得一个SqlSessionTemplate。 123456public abstract class SqlSessionDaoSupport extends DaoSupport &#123; private SqlSessionTemplate sqlSessionTemplate; public SqlSession getSqlSession() &#123; return this.sqlSessionTemplate; &#125;&#125; 先创建一个 BaseDao 继承 SqlSessionDaoSupport。在BaseDao 里面封装对数据库的操作，包括 selectOne()、selectList()、insert()、delete()这些方法，子类就可以直接调用。 然后让我们的实现类继承 BaseDao 并且实现我们的 DAO 层接口，这里就是我们的Mapper 接口。实现类需要加上@Repository 的注解。 在实现类的方法里面，我们可以直接调用父类（BaseDao）封装的 selectOne()方法，那么它最终会调用 sqlSessionTemplate 的 selectOne()方法。 3.3有没有更好的拿到 SqlSessionTemplate我们的每一个 DAO 层的接口（Mapper 接口也属于），如果要拿到一个 SqlSessionTemplate，去操作数据库，都要创建实现一个实现类，加上@Repository 的注解，继承 BaseDao，这个工作量也不小。 另外一个，我们去直接调用 selectOne()方法，还是出现了 Statement ID 的硬编码，MapperProxy 在这里根本没用上。 4.接口的扫描注册在 applicationContext.xml 里 面 配 置 了 一 个MapperScannerConfigurer。 MapperScannerConfigurer 实现了 BeanDefinitionRegistryPostProcessor 接口，BeanDefinitionRegistryPostProcessor 是 BeanFactoryPostProcessor 的子类，可以通过编码的方式修改、新增或者删除某些 Bean 的定义。 我们只需要重写 postProcessBeanDefinitionRegistry()方法，在这里面操作 Bean就可以了。 在这个方法里面： scanner.scan() 方 法 是 ClassPathBeanDefinitionScanner 中 的 ， 而 它 的 子 类ClassPathMapperScanner 覆 盖 了 doScan() 方 法 ， 在 doScan() 中 调 用 了processBeanDefinitions，它先调用父类的 doScan()扫描所有的接口。 processBeanDefinitions 方法里面，在注册 beanDefinitions 的时候，BeanClass被改为 MapperFactoryBean（注意灰色的注释）。 为什么要把 BeanClass 修改成 MapperFactoryBean，这个类有什么作用？ MapperFactoryBean 继 承 了 SqlSessionDaoSupport ， 可 以 拿 到SqlSessionTemplate。 5.接口注入使用我们使用 Mapper 的时候，只需要在加了 Service 注解的类里面使用@Autowired注入 Mapper 接口就好了。 123456789 @Service public class EmployeeService &#123; @Autowired EmployeeMapper employeeMapper; public List&lt;Employee&gt; getAll() &#123; return employeeMapper.selectByMap( null); &#125;&#125; Spring 在启动的时候需要去实例化 EmployeeService。 EmployeeService 依赖了 EmployeeMapper 接口（是 EmployeeService 的一个属性）。 Spring 会根据 Mapper 的名字从 BeanFactory 中获取它的 BeanDefination，再从BeanDefination 中 获 取 BeanClass ，EmployeeMapper 对 应 的 BeanClass 是MapperFactoryBean（上一步已经分析过）。 接下来就是创建 MapperFactoryBean，因为实现了 FactoryBean 接口，同样是调用 getObject()方法。 1234// MapperFactoryBean.java public T getObject() throws Exception &#123; return getSqlSession().getMapper( this. mapperInterface);&#125; 因为 MapperFactoryBean 继 承 了 SqlSessionDaoSupport ， 所 以 这 个getSqlSession()就是调用父类的方法，返回 SqlSessionTemplate。 1234// SqlSessionDaoSupport.javapublic SqlSession getSqlSession() &#123; return this. sqlSessionTemplate;&#125; 我们注入到 Service 层的接口，实际上还是一个 MapperProxy 代理对象。所以最后调用 Mapper 接口的方法，也是执行 MapperProxy 的 invoke()方法。 DaoSupport ， 所 以 这 个getSqlSession()就是调用父类的方法，返回 SqlSessionTemplate。 1234// SqlSessionDaoSupport.javapublic SqlSession getSqlSession() &#123; return this. sqlSessionTemplate;&#125; 我们注入到 Service 层的接口，实际上还是一个 MapperProxy 代理对象。所以最后调用 Mapper 接口的方法，也是执行 MapperProxy 的 invoke()方法。 6.总结​ Mybatis整合Spring框架首先利用的是Spring框架的SPI机制，在项目的META-INF目录下有一个文件【spring.handlers】，里面给Spring容器中导入了一个类【NamespaceHandler】。【NamespaceHandler】 继承关系上：实现了spring的扩展点接口 【init】给容器中注册了一个【BeanDefinitionParser】bean定义信息的解析器 【MapperScannerBeanDefinitionParser】【MapperScannerBeanDefinitionParser】 他会在spring容器创建过程中去解析【mapperScanner】标签 解析出来的属性会给【MapperScannerConfigurer】赋能另一个需要配置的bean就是【SqlSessionFactoryBean】 继承关系 FactoryBean InitializingBean 初始化的时候会执行【afterPropertiesSet()】 ApplicationListener afterPropertiesSet() buildSqlSessionFactory() 创建Mybatis的主配置文件解析器，解析主配置文件 创建mapper映射文件的解析器，解析mapper映射文件 构建出一个Configuration对象传入到【SqlSessionFactoryBuilder】的【build()】 最终返回了一个【SqlSessionFactory】对象实例【MapperScannerConfigurer】 继承关系 【BeanDefinitionRegistryPostProcessor】 【postProcessBeanDefinitionRegistry()】 在系统初始化的过程中被调用，扫描了配置文件中配置的basePackage 下的所有 Mapper 类，最终生成 Spring 的 Bean 对象，注册到容器中 这里面调用了包扫描的方法【scanner.scan()】经过一系列调用，调用到了 【ClassPathMapperScanner】的【doScan】 调用父类的doScan()方法，遍历basePackages中指定的所有包，扫描每个包下的Java文件并进行解析。 使用之前注册的过滤器进行过滤，得到符合条件的BeanDefinitionHolder对象 【processBeanDefinitions()】处理扫描得到的BeanDefinitionHolder集合 循环 将BeanDefinition中记录的Bean类型修改为【MapperFactoryBean】 将扫描到的接口类型作为构造方法的参数 构造MapperFactoryBean的属性，将sqlSessionFactory、sqlSessionTemplate 等信息填充到BeanDefinition中，修改自动注入方式 重新注册到容器 ClassPathMapperScanner 在处理 Mapper 接口的时候用到了 MapperFactoryBean 类，动态代理的实现，可以直接将 Mapper 接口注入到 Service 层的 Bean 中，这样就不需要编写任何 DAO 实现的代码。【MapperFactoryBean】 继承关系 InitializingBean DaoSupport SqlSessionDaoSupport FactoryBean MapperFactoryBean MapperFactoryBean 类的动态代理功能是通过实现了 Spring 提供的 FactoryBean 接口实现的，该接口是一个用于创建 Bean 对象的工厂接口，通过 getObject() 方法获取真实的对象。 【getObject()】 【getSqlSession().getMapper(this.mapperInterface)】 【getSqlSession()】是她父类【SqlSessionDaoSupport】的方法 【getMapper()】 通过sqlSession获取到mapper代理对象这里面涉及到了一个类 【SqlSessionDaoSupport】 构造器 通过 Spring 容器自动注入 sqlSessionFactory 属性 【createSqlSessionTemplate()】 创建了一个【SqlSessionTemplate】对象并且里面持有【SqlSessionFactory】【SqlSessionTemplate】 SqlSessionTemplate 实现了 SqlSession 接口，在 MyBatis 与 Spring 集成开发时，用来代替 MyBatis 中的 DefaultSqlSession 的功能。 SqlSessionTemplate 是线程安全的，可以在 DAO 之间共享使用，比如上面生成的 Mapper 对象会持有一个 SqlSessionTemplate 对象，每次请求都会共用该对象。 在 MyBatis 中 SqlSession 的 Scope 是会话级别，请求结束后需要关闭本次会话，怎么集成了 Spring 后，可以共用了？ 首先，在集成 Spring 后，Mapper 对象是单例，由 Spring 容器管理，供 Service 层使用，SqlSessionTemplate 在设计的时候，功能分成了如下两部分： 1. 获取 MapperProxy 代理对象； 2. 执行 SQL 操作，该部分功能通过代理对象 SqlSessionInterceptor 实现； 【构造器】 sqlSession通过动态代理来创建的，【SqlSessionInterceptor】实现了 【InvocationHandler】当调用mapper里面的方法的时候，就会执行【SqlSessionInterceptor】的【invoke()】 通过SqlSessionUtils.getSqlSession()获取SqlSession对象，同一个事务共享SqlSession 通过【invoke】调用SqlSession对象的相应方法 检测事务是否由Spring进行管理，并据此决定是否提交事务。​ Mybatis-Spring 中文注释源码地址：https://gitee.com/yin_huidong/mybatis-spring.git\u0000","categories":[{"name":"MyBatis","slug":"MyBatis","permalink":"https://yinhuidong.github.io/categories/MyBatis/"}],"tags":[{"name":"MyBatis","slug":"MyBatis","permalink":"https://yinhuidong.github.io/tags/MyBatis/"}]},{"title":"Spring[十一]注解版Aop流程分析","slug":"Spring/Spring[十一]注解版Aop流程分析","date":"2022-01-11T06:13:44.508Z","updated":"2022-01-11T06:18:16.601Z","comments":true,"path":"2022/01/11/Spring/Spring[十一]注解版Aop流程分析/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/Spring/Spring[%E5%8D%81%E4%B8%80]%E6%B3%A8%E8%A7%A3%E7%89%88Aop%E6%B5%81%E7%A8%8B%E5%88%86%E6%9E%90/","excerpt":"","text":"上一篇介绍了实现AOP的两种方式，本篇我们通过分析源码流程，来看一下注解版AOP的实现。具体的源码细节，会在后面的篇章一行行翻译。 1.开启AOP的功能读源码需要找到入口或者抓手，AOP的源码我们如何入手呢？想要使用AOP的功能就需要在Spring的配置类上加上**@EnableAspectJAutoProxy**注解。​ 先分析一下这个注解：\u0000 12345678910111213141516171819@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Import(AspectJAutoProxyRegistrar.class)public @interface EnableAspectJAutoProxy &#123; /** * 是否要创建基于子类 (CGLIB) 的代理，而不是基于标准 Java 接口的代理。 默认值为false 。 * @return */ boolean proxyTargetClass() default false; /** * 代理应由 AOP 框架公开为ThreadLocal以通过AopContext类进行检索。 * 默认关闭，即不保证AopContext访问将起作用。 */ boolean exposeProxy() default false;&#125; 可以看到这个注解的底层有一个**@Import(AspectJAutoProxyRegistrar.class)**，他往容器中导入了一个组件**AspectJAutoProxyRegistrar**。​ 看一下这个组件：​ 12345678910111213141516171819202122class AspectJAutoProxyRegistrar implements ImportBeanDefinitionRegistrar &#123; @Override public void registerBeanDefinitions( AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) &#123; /*导入组件到容器中 AnnotationAwareAspectJAutoProxyCreator*/ AopConfigUtils.registerAspectJAnnotationAutoProxyCreatorIfNecessary(registry); AnnotationAttributes enableAspectJAutoProxy = AnnotationConfigUtils.attributesFor(importingClassMetadata, EnableAspectJAutoProxy.class); if (enableAspectJAutoProxy != null) &#123; if (enableAspectJAutoProxy.getBoolean(&quot;proxyTargetClass&quot;)) &#123; AopConfigUtils.forceAutoProxyCreatorToUseClassProxying(registry); &#125; if (enableAspectJAutoProxy.getBoolean(&quot;exposeProxy&quot;)) &#123; AopConfigUtils.forceAutoProxyCreatorToExposeProxy(registry); &#125; &#125; &#125;&#125; 12345678910@Nullablepublic static BeanDefinition registerAspectJAnnotationAutoProxyCreatorIfNecessary( BeanDefinitionRegistry registry, @Nullable Object source) &#123; /* * 参数一：固定类型 * 参数二：spring容器 * 参数三：标签 * */ return registerOrEscalateApcAsRequired(AnnotationAwareAspectJAutoProxyCreator.class, registry, source);&#125; \u0000\u0000**AspectJAutoProxyRegistrar** 实现了 **ImportBeanDefinitionRegistrar**接口，往容器中注册了一个bd信息。**AnnotationAwareAspectJAutoProxyCreator**​ 2.加载时机\u0000上一节我们分析到，在Spring的配置类上打一个注解，最后总会往容器中导入一个类：**AnnotationAwareAspectJAutoProxyCreator**。​ 我们来分析一下这个类的继承关系：​ 按照顺序分析一下这个类的加载时机和加载的时候，里面的 **initBeanFactory()** &amp; **setBeanFactory()** 是什么时候执行的。​ ​ \u0000 首先执行的是**AbstractAdvisorAutoProxyCreator**的**setBeanFactory()**。 123456789@Overridepublic void setBeanFactory(BeanFactory beanFactory) &#123; super.setBeanFactory(beanFactory); if (!(beanFactory instanceof ConfigurableListableBeanFactory)) &#123; throw new IllegalArgumentException( &quot;AdvisorAutoProxyCreator requires a ConfigurableListableBeanFactory: &quot; + beanFactory); &#125; initBeanFactory((ConfigurableListableBeanFactory) beanFactory);&#125; 接下来执行的是**AbstractAutoProxyCreator**的**setBeanFactory()**。 \u0000 1234@Overridepublic void setBeanFactory(BeanFactory beanFactory) &#123; this.beanFactory = beanFactory;&#125; ​ 然后执行的是**AnnotationAwareAspectJAutoProxyCreator**的**initBeanFactory()** 。 ​ 123456789@Overrideprotected void initBeanFactory(ConfigurableListableBeanFactory beanFactory) &#123; super.initBeanFactory(beanFactory); if (this.aspectJAdvisorFactory == null) &#123; this.aspectJAdvisorFactory = new ReflectiveAspectJAdvisorFactory(beanFactory); &#125; this.aspectJAdvisorsBuilder = new BeanFactoryAspectJAdvisorsBuilderAdapter(beanFactory, this.aspectJAdvisorFactory);&#125; 最终执行的是**AbstractAdvisorAutoProxyCreator**的**initBeanFactory()**。 ​ 123protected void initBeanFactory(ConfigurableListableBeanFactory beanFactory) &#123; this.advisorRetrievalHelper = new BeanFactoryAdvisorRetrievalHelperAdapter(beanFactory);&#125; 接下来分析**AnnotationAwareAspectJAutoProxyCreator**的执行时机。​ 3.创建代理对象 4.获取拦截器 5.链式调用通知方法 6.流程总结 AOP其实就是往容器中导入了一个组件，这个组件是一个后置处理器，他会在对象创建之前尝试返回一个代理对象，如果不能成功返回，会在对象创建之后，init方法执行前后去判断当前对象是否需要被代理，如果需要被代理则根据各种条件去选择代理方式，创建代理对象，同时会去判断哪些切面和方法需要增强代理对象里面的方法，生成一条拦截器链。\u0000\u0000在代理对象执行目标方法前后，通过拦截器对目标方法进行拦截，执行增强逻辑。","categories":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/tags/Spring/"}]},{"title":"Spring[扩展]模拟核心原理","slug":"Spring/Spring[扩展]模拟核心原理","date":"2022-01-11T06:11:42.407Z","updated":"2022-01-11T06:20:30.143Z","comments":true,"path":"2022/01/11/Spring/Spring[扩展]模拟核心原理/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/Spring/Spring[%E6%89%A9%E5%B1%95]%E6%A8%A1%E6%8B%9F%E6%A0%B8%E5%BF%83%E5%8E%9F%E7%90%86/","excerpt":"","text":"一，实现思路1，配置阶段 配置web.xml DispatcherServlet 设定init-param contextConfigLocation=classpath:application.xml 设定url-pattern /* 配置Annotation @Controller @Service @Autowrited @RequestMapping 2,初始化阶段 调用init方法 加载配置文件 IOC容器初始化 MAP 扫描相关的类 scan-package=”” 创建实例化并保存至容器 通过反射机制将类实例化放入IOC容器 进行DI操作 扫描IOC容器的实例，给没有赋值的属性自动填充 初始化HandlerMapping 讲一个URL和一个Method进行一对一的映射 3，运行阶段 调用doGet/doPost web容器调用doget、dopost，获取req和resp对象 匹配HandlerMapping 从req对象获取输入的URL，找到其对应的method 反射调用method.invoker() 利用反射调用方法并返回结果 response.getWrite().write() 将返回结果输出到浏览器 二，自定义配置1，配置 application.properties 文件为了解析方便，用 application.properties 来代替 application.xml 文件，具体配置内容如下： 1scanPackage=com.yhd.spring01 2，配置web.xml文件12345678910111213141516171819202122&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;web-app xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns=&quot;http://java.sun.com/xml/ns/j2ee&quot; xmlns:javaee=&quot;http://java.sun.com/xml/ns/javaee&quot; xmlns:web=&quot;http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd&quot; xsi:schemaLocation=&quot;http://java.sun.com/xml/ns/j2eehttp://java.sun.com/xml/ns/j2ee/web-app_2_4.xsd&quot; version=&quot;2.4&quot;&gt; &lt;display-name&gt;YHD Web Application&lt;/display-name&gt; &lt;servlet&gt; &lt;servlet-name&gt;yhdmvc&lt;/servlet-name&gt; &lt;servlet-class&gt;com.yhd.spring01.servlet.HdDispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;application.properties&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;yhdmvc&lt;/servlet-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/servlet-mapping&gt;&lt;/web-app&gt; 3，自定义注解123456@Target(&#123;ElementType.FIELD&#125;)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface HdAutowired &#123; String value() default &quot;&quot;;&#125; 1234567import java.lang.annotation.*;@Target(&#123;ElementType.TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface HdController &#123; String value() default &quot;&quot;;&#125; 123456@Target(&#123;ElementType.TYPE, ElementType.METHOD&#125;)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface HdRequestMapping &#123; String value() default &quot;&quot;;&#125; 123456@Target(&#123;ElementType.PARAMETER&#125;)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface HdRequestParam &#123; String value() default &quot;&quot;;&#125; 123456@Target(&#123;ElementType.TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface HdService &#123; String value() default &quot;&quot;;&#125; 4，编写模拟业务1234567@HdServicepublic class DemoService implements IDemoService &#123; @Override public String get(String name) &#123; return &quot;My name is &quot; + name; &#125;&#125; 12345678910111213141516171819202122232425262728293031@HdController@HdRequestMapping(&quot;/demo&quot;)public class DemoController &#123; @HdAutowired private IDemoService demoService; @HdRequestMapping(&quot;/query&quot;) public void query(HttpServletRequest req, HttpServletResponse resp, @HdRequestParam(&quot;name&quot;) String name)&#123; String result = demoService.get(name); try &#123; resp.getWriter().write(result); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; @HdRequestMapping(&quot;/add&quot;) public void add(HttpServletRequest req, HttpServletResponse resp, @HdRequestParam(&quot;a&quot;) Integer a, @HdRequestParam(&quot;b&quot;) Integer b)&#123; try &#123; resp.getWriter().write(a + &quot;+&quot; + b + &quot;=&quot; + (a + b)); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; @HdRequestMapping(&quot;/remove&quot;) public void remove(HttpServletRequest req,HttpServletResponse resp, @HdRequestParam(&quot;id&quot;) Integer id)&#123; &#125;&#125; 三，容器初始化1.0版本流程分析1.首先在doGet方法里面调用doDispatcher方法，根据请求路径判断路径是否存在，如果不存在就返回404存在就从容器中拿到路径对应的方法，通过动态代理执行对应的方法 2.在类加载阶段，用流来加载配置文件，从配置文件读取配置的包扫描路径根据包扫描路径进行迭代遍历，利用反射创建所有类上标有controller注解的类加入到容器，并下钻到类中，将类中每一个方法的绝对访问路径和方法加入到容器，迭代遍历创建所有标有service注解的类，如果该类实现了接口，将该接口的全限定类型名和类实例对象也放入容器，达到根据接口注入的效果。 3.属性赋值，遍历容器中所有类，如果类中标有@autowried注解，将属性对应的值设置进去。 重要方法1.clazz.isAnnotationPresent(HdController.class)判断clazz上有没有HdController注解 2.field.set(mappings.get(clazz.getName()), mappings.get(beanName));属性赋值：args1：给哪个属性设值，args2：设置的什么值 3.method.invoke(mappings.get(method.getDeclaringClass().getName()), new Object[]{req, resp, params.get(“name”)[0]});通过动态代理执行方法，方法所在类名，方法参数 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139/** * @author yhd * @createtime 2021/1/31 15:49 * @description 模拟IOC容器的创建 */public class HdDispatcherServlet extends HttpServlet &#123; //映射关系 访问路径-方法名 全限定类名-实例对象 private Map&lt;String, Object&gt; mappings = new HashMap&lt;&gt;(); @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; this.doPost(req, resp); &#125; @SneakyThrows @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; doDispatch(req, resp); &#125; private void doDispatch(HttpServletRequest req, HttpServletResponse resp) throws IOException, InvocationTargetException, IllegalAccessException &#123; //组装路径 String url = req.getRequestURI(); String contextPath = req.getContextPath(); url = url.replace(contextPath, &quot;&quot;).replaceAll(&quot;/+&quot;, &quot;/&quot;); //判断路径是否存在 if (!this.mappings.containsKey(url)) &#123; resp.getWriter().write(&quot;404 NotFound!&quot;); return; &#125; //获取路径对应的方法参数，通过动态代理进行增强 Method method = (Method) this.mappings.get(url); Map&lt;String, String[]&gt; params = req.getParameterMap(); method.invoke(mappings.get(method.getDeclaringClass().getName()), new Object[]&#123;req, resp, params.get(&quot;name&quot;)[0]&#125;); &#125; @Override public void init(ServletConfig config) throws ServletException &#123; InputStream is = null; try &#123; //加载配置文件 Properties configContext = new Properties(); is = this.getClass().getClassLoader().getResourceAsStream(config.getInitParameter(&quot;contextConfigLocation&quot;)); configContext.load(is); //获取扫描路径 String scanPackage = configContext.getProperty(&quot;scanPackage&quot;); doScanner(scanPackage); for (String className : mappings.keySet()) &#123; if (!className.contains(&quot;.&quot;)) &#123; continue; &#125; Class&lt;?&gt; clazz = Class.forName(className); //当前这个类上有没有controller注解 if (clazz.isAnnotationPresent(HdController.class)) &#123; mappings.put(className, clazz.newInstance()); String baseUrl = &quot;&quot;; //判断有没有一级访问路径 if (clazz.isAnnotationPresent(HdRequestMapping.class)) &#123; HdRequestMapping requestMapping = clazz.getAnnotation(HdRequestMapping.class); baseUrl = requestMapping.value(); &#125; Method[] methods = clazz.getMethods(); for (Method method : methods) &#123; if (!method.isAnnotationPresent(HdRequestMapping.class)) &#123; continue; &#125; HdRequestMapping requestMapping = method.getAnnotation(HdRequestMapping.class); //拼装路径 String url = (baseUrl + &quot;/&quot; + requestMapping.value()).replaceAll(&quot;/+&quot;, &quot;/&quot;); //map放的是：controller里面一个方法的访问绝对路径，这个对应的方法 mappings.put(url, method); System.out.println(&quot;Mapped &quot; + url + &quot;,&quot; + method); &#125; &#125; else if (clazz.isAnnotationPresent(HdService.class)) &#123; HdService service = clazz.getAnnotation(HdService.class); String beanName = service.value(); if (&quot;&quot;.equals(beanName)) &#123; beanName = clazz.getName(); &#125; Object instance = clazz.newInstance(); //map里面放的是类名和实例对象 mappings.put(beanName, instance); //将这个类实现的接口和实例对象放进去 for (Class&lt;?&gt; i : clazz.getInterfaces()) &#123; mappings.put(i.getName(), instance); &#125; &#125; else &#123; continue; &#125; &#125; //属性注入 for (Object object : mappings.values()) &#123; if (object == null) &#123; continue; &#125; Class clazz = object.getClass(); if (clazz.isAnnotationPresent(HdController.class)) &#123; Field[] fields = clazz.getDeclaredFields(); for (Field field : fields) &#123; if (!field.isAnnotationPresent(HdAutowired.class)) &#123; continue; &#125; HdAutowired autowired = field.getAnnotation(HdAutowired.class); String beanName = autowired.value(); if (&quot;&quot;.equals(beanName)) &#123; beanName = field.getType().getName(); &#125; field.setAccessible(true); try &#123; field.set(mappings.get(clazz.getName()), mappings.get(beanName)); &#125; catch (IllegalAccessException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; System.out.print(&quot;Diy MVC Framework is init&quot;); &#125; catch (Exception e) &#123; &#125; &#125; private void doScanner(String scanPackage) &#123; URL url = this.getClass().getClassLoader().getResource(&quot;/&quot; + scanPackage.replaceAll(&quot;\\\\.&quot;, &quot;/&quot;)); File classDir = new File(url.getFile()); Arrays.stream(classDir.listFiles()).forEach(file -&gt; &#123; if (file.isDirectory()) &#123; doScanner(scanPackage + &quot;.&quot; + file.getName()); &#125; else &#123; if (!file.getName().endsWith(&quot;.class&quot;)) &#123; return; &#125; String clazzName = (scanPackage + &quot;.&quot; + file.getName().replace(&quot;.class&quot;, &quot;&quot;)); mappings.put(clazzName, null); &#125; &#125;); &#125;&#125; 2.0版本分析1.0版本的所有代码都写在了一个方法里面，代码耦合度 十分高，不符合开发规范 思路采用设计模式（工厂模式、单例模式、委派模式、策略模式），改造业务逻辑。 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202/** * @author yhd * @createtime 2021/2/1 11:29 */public class HdDispatcherServlet2 extends HttpServlet &#123; private Map&lt;String, Object&gt; ioc = new ConcurrentHashMap&lt;&gt;(); private Map&lt;String, Method&gt; handlerMappings = new ConcurrentHashMap&lt;&gt;(); private List&lt;String&gt; classNames = new CopyOnWriteArrayList&lt;&gt;(); private Properties configContext = new Properties(); private static final String CONFIG_LOCATION = &quot;contextConfigLocation&quot;; @Override public void init(ServletConfig config) throws ServletException &#123; //1.加载配置文件 loadConfig(config.getInitParameter(CONFIG_LOCATION)); //2.扫描所有的组件 doScanPackages(configContext.getProperty(&quot;scanPackage&quot;)); //3.将组件加入到容器 refersh(); //4.属性设值 population(); //5.建立方法与路径的映射 routingAndMapping(); &#125; /** * 建立方法与路径的映射 */ private void routingAndMapping() &#123; classNames.forEach(className -&gt; &#123; Object instance = ioc.get(className); if (instance.getClass().isAnnotationPresent(HdController.class)) &#123; String baseUrl = &quot;&quot;; if (instance.getClass().isAnnotationPresent(HdRequestMapping.class)) &#123; baseUrl += instance.getClass().getAnnotation(HdRequestMapping.class).value().trim(); &#125; String finalBaseUrl = baseUrl; Arrays.asList(instance.getClass().getDeclaredMethods()).forEach(method -&gt; &#123; if (method.isAnnotationPresent(HdRequestMapping.class)) &#123; String methodUrl = finalBaseUrl; methodUrl += method.getAnnotation(HdRequestMapping.class).value().trim(); handlerMappings.put(methodUrl, method); &#125; &#125;); &#125; &#125;); &#125; /** * 属性设值 */ private void population() &#123; Set&lt;String&gt; keySet = ioc.keySet(); keySet.forEach(key -&gt; &#123; Field[] fields = ioc.get(key).getClass().getFields(); Arrays.asList(fields).forEach(field -&gt; &#123; if (field.isAnnotationPresent(HdAutowired.class)) &#123; HdAutowired autowired = field.getAnnotation(HdAutowired.class); String name = autowired.value().trim(); if (&quot;&quot;.equals(autowired.value().trim())) &#123; name = field.getType().getName(); &#125; try &#123; field.setAccessible(true); field.set(name, ioc.get(name)); &#125; catch (IllegalAccessException e) &#123; &#125; &#125; &#125;); &#125;); &#125; /** * 容器刷新 * 组件加入到容器中 */ @SneakyThrows private void refersh() &#123; if (classNames == null || classNames.isEmpty()) &#123; throw new RuntimeException(&quot;组件扫描出现异常！&quot;); &#125; for (String className : classNames) &#123; Class&lt;?&gt; clazz = Class.forName(className); if (clazz.isAnnotationPresent(HdController.class)) &#123; //TODO 类名处理 ioc.put(clazz.getSimpleName(), clazz.newInstance()); &#125; else if (clazz.isAnnotationPresent(HdService.class)) &#123; Object instance = clazz.newInstance(); ioc.put(clazz.getSimpleName(), instance); Class&lt;?&gt;[] interfaces = clazz.getInterfaces(); for (Class&lt;?&gt; inter : interfaces) &#123; ioc.put(inter.getSimpleName(), clazz); &#125; &#125; else &#123; continue; &#125; &#125; &#125; /** * 组件扫描 * * @param scanPackage */ private void doScanPackages(String scanPackage) &#123; URL url = getClass().getClassLoader().getResource(&quot;/&quot; + scanPackage.replaceAll(&quot;\\\\.&quot;, &quot;/&quot;)); File files = new File(url.getFile()); for (File file : files.listFiles()) &#123; if (file.isDirectory()) &#123; doScanPackages(scanPackage + &quot;.&quot; + file.getName()); &#125; else &#123; if (!file.getName().endsWith(&quot;.class&quot;)) &#123; continue; &#125; String className = scanPackage + &quot;.&quot; + file.getName().replace(&quot;.class&quot;, &quot;&quot;); classNames.add(className); &#125; &#125; &#125; /** * 加载配置文件 * * @param initParameter */ @SneakyThrows private void loadConfig(String initParameter) &#123; InputStream is = getClass().getClassLoader().getResourceAsStream(initParameter); configContext.load(is); &#125; @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; doPost(req, resp); &#125; @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; try &#123; doDispatcher(req, resp); &#125; catch (Exception e) &#123; throw new RuntimeException(&quot; 500 server error!&quot;); &#125; &#125; @SneakyThrows private void doDispatcher(HttpServletRequest req, HttpServletResponse resp) &#123; String realPath = req.getRequestURI().replace(req.getContextPath(), &quot;&quot;); Map&lt;String, String[]&gt; parameterMap = req.getParameterMap(); if (!handlerMappings.containsKey(realPath)) &#123; throw new RuntimeException(&quot;404 Not Found!&quot;); &#125; Method method = handlerMappings.get(realPath); Class&lt;?&gt;[] parameterTypes = method.getParameterTypes(); Object[] paramValues = new Object[parameterTypes.length]; for (int i = 0; i &lt; parameterTypes.length - 1; i++) &#123; Class param = parameterTypes[i]; if (param == HttpServletRequest.class) &#123; paramValues[i] = req; &#125; if (param == HttpServletResponse.class) &#123; paramValues[i] = resp; &#125; if (param == String.class) &#123; HdRequestParam requestParam = parameterTypes[i].getAnnotation(HdRequestParam.class); String value = requestParam.value(); String[] realParam = parameterMap.get(value); paramValues[i] = Arrays.toString(realParam) .replaceAll(&quot;\\\\[|\\\\]&quot;, &quot;&quot;) .replaceAll(&quot;\\\\s&quot;, &quot;,&quot;); &#125; &#125; method.invoke(method.getDeclaringClass().getSimpleName(), paramValues); &#125; private Object convertParamType() &#123; return null; &#125;&#125; 3.0版本分析HandlerMapping还不能像SpringMVC一样支持正则，url参数还不支持强制类型转换，反射调用之前还需要重新获取bean的name。 改造 HandlerMapping，在真实的 Spring 源码中，HandlerMapping 其实是一个 List 而非 Map。List 中的元素是一个自定义的类型。 思路使用内部类维护requestMapping和url之间的关系。 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282public class HdDispatcherServlet3 extends HttpServlet &#123; private Map&lt;String, Object&gt; ioc = new ConcurrentHashMap&lt;&gt;(); private Map&lt;String, Method&gt; handlerMappings = new ConcurrentHashMap&lt;&gt;(); private List&lt;String&gt; classNames = new CopyOnWriteArrayList&lt;&gt;(); private Properties configContext = new Properties(); private static final String CONFIG_LOCATION = &quot;contextConfigLocation&quot;; private List&lt;Handler&gt; handlerMapping = new ArrayList&lt;&gt;(); /** * */ @Data private class Handler &#123; //保存方法对应的实例 private Object controller; //保存映射的方法 private Method method; //正则匹配 private Pattern pattern; //参数顺序 private Map&lt;String, Integer&gt; paramIndexMapping = new ConcurrentHashMap&lt;&gt;(); public Handler(Pattern pattern, Object controller, Method method) &#123; this.controller = controller; this.method = method; this.pattern = pattern; paramIndexMapping = new HashMap&lt;String, Integer&gt;(); putParamIndexMapping(method); &#125; private void putParamIndexMapping(Method method) &#123; //提取方法中加了注解的参数 Annotation[][] pa = method.getParameterAnnotations(); for (int i = 0; i &lt; pa.length; i++) &#123; for (Annotation a : pa[i]) &#123; if (a instanceof HdRequestParam) &#123; String paramName = ((HdRequestParam) a).value(); if (!&quot;&quot;.equals(paramName.trim())) &#123; paramIndexMapping.put(paramName, i); &#125; &#125; &#125; &#125; //提取方法中的req和resp Class&lt;?&gt;[] parameterTypes = method.getParameterTypes(); for (int i = 0; i &lt; parameterTypes.length; i++) &#123; Class&lt;?&gt; type = parameterTypes[i]; if (type == HttpServletRequest.class || type == HttpServletResponse.class) &#123; paramIndexMapping.put(type.getName(), i); &#125; &#125; &#125; &#125; @Override public void init(ServletConfig config) throws ServletException &#123; //1.加载配置文件 loadConfig(config.getInitParameter(CONFIG_LOCATION)); //2.扫描所有的组件 doScanPackages(configContext.getProperty(&quot;scanPackage&quot;)); //3.将组件加入到容器 refersh(); //4.属性设值 population(); //5.建立方法与路径的映射 routingAndMapping(); &#125; /** * 建立方法与路径的映射 */ private void routingAndMapping() &#123; if (ioc.isEmpty()) &#123; return; &#125; for (Map.Entry&lt;String, Object&gt; entry : ioc.entrySet()) &#123; Class&lt;?&gt; clazz = entry.getValue().getClass(); if (!clazz.isAnnotationPresent(HdController.class)) &#123; continue; &#125; String url = &quot;&quot;; if (clazz.isAnnotationPresent(HdRequestMapping.class)) &#123; HdRequestMapping requestMapping = clazz.getAnnotation(HdRequestMapping.class); url = requestMapping.value(); &#125; for (Method method : clazz.getMethods()) &#123; if (!method.isAnnotationPresent(HdRequestMapping.class)) &#123; continue; &#125; HdRequestMapping requestMapping = method.getAnnotation(HdRequestMapping.class); String regex = (&quot;/&quot; + url + requestMapping.value()).replaceAll(&quot;/+&quot;, &quot;/&quot;); Pattern pattern = Pattern.compile(regex); handlerMapping.add(new Handler(pattern, entry.getValue(), method)); &#125; &#125; &#125; /** * 属性设值 */ private void population() &#123; Set&lt;String&gt; keySet = ioc.keySet(); keySet.forEach(key -&gt; &#123; Field[] fields = ioc.get(key).getClass().getFields(); Arrays.asList(fields).forEach(field -&gt; &#123; if (field.isAnnotationPresent(HdAutowired.class)) &#123; HdAutowired autowired = field.getAnnotation(HdAutowired.class); String name = autowired.value().trim(); if (&quot;&quot;.equals(autowired.value().trim())) &#123; name = field.getType().getName(); &#125; try &#123; field.setAccessible(true); field.set(name, ioc.get(name)); &#125; catch (IllegalAccessException e) &#123; &#125; &#125; &#125;); &#125;); &#125; /** * 容器刷新 * 组件加入到容器中 */ @SneakyThrows private void refersh() &#123; if (classNames == null || classNames.isEmpty()) &#123; throw new RuntimeException(&quot;组件扫描出现异常！&quot;); &#125; for (String className : classNames) &#123; Class&lt;?&gt; clazz = Class.forName(className); if (clazz.isAnnotationPresent(HdController.class)) &#123; //TODO 类名处理 ioc.put(clazz.getSimpleName(), clazz.newInstance()); &#125; else if (clazz.isAnnotationPresent(HdService.class)) &#123; Object instance = clazz.newInstance(); ioc.put(clazz.getSimpleName(), instance); Class&lt;?&gt;[] interfaces = clazz.getInterfaces(); for (Class&lt;?&gt; inter : interfaces) &#123; ioc.put(inter.getSimpleName(), clazz); &#125; &#125; else &#123; continue; &#125; &#125; &#125; /** * 组件扫描 * * @param scanPackage */ private void doScanPackages(String scanPackage) &#123; URL url = getClass().getClassLoader().getResource(&quot;/&quot; + scanPackage.replaceAll(&quot;\\\\.&quot;, &quot;/&quot;)); File files = new File(url.getFile()); for (File file : files.listFiles()) &#123; if (file.isDirectory()) &#123; doScanPackages(scanPackage + &quot;.&quot; + file.getName()); &#125; else &#123; if (!file.getName().endsWith(&quot;.class&quot;)) &#123; continue; &#125; String className = scanPackage + &quot;.&quot; + file.getName().replace(&quot;.class&quot;, &quot;&quot;); classNames.add(className); &#125; &#125; &#125; /** * 加载配置文件 * * @param initParameter */ @SneakyThrows private void loadConfig(String initParameter) &#123; InputStream is = getClass().getClassLoader().getResourceAsStream(initParameter); configContext.load(is); &#125; @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; doPost(req, resp); &#125; @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; try &#123; doDispatcher(req, resp); &#125; catch (Exception e) &#123; throw new RuntimeException(&quot; 500 server error!&quot;); &#125; &#125; @SneakyThrows private void doDispatcher(HttpServletRequest req, HttpServletResponse resp) &#123; Handler handler = getHandler(req); if (handler == null) &#123; throw new RuntimeException(&quot;404 Not Found!&quot;); &#125; Class&lt;?&gt;[] parameterTypes = handler.getMethod().getParameterTypes(); Object[] paramValues = new Object[parameterTypes.length]; Map&lt;String, String[]&gt; params = req.getParameterMap(); for (Map.Entry&lt;String, String[]&gt; param : params.entrySet()) &#123; String value = Arrays.toString(param.getValue()).replaceAll(&quot;\\\\[|\\\\]&quot;, &quot;&quot;) .replaceAll(&quot;\\\\s&quot;, &quot;,&quot;); if (!handler.getParamIndexMapping().containsKey(param.getKey())) &#123; continue; &#125; Integer index = handler.getParamIndexMapping().get(param.getKey()); paramValues[index] = this.convert(parameterTypes[index], value); &#125; if (handler.paramIndexMapping.containsKey(HttpServletRequest.class.getName())) &#123; int reqIndex = handler.paramIndexMapping.get(HttpServletRequest.class.getName()); paramValues[reqIndex] = req; &#125; if (handler.paramIndexMapping.containsKey(HttpServletResponse.class.getName())) &#123; int respIndex = handler.paramIndexMapping.get(HttpServletResponse.class.getName()); paramValues[respIndex] = resp; &#125; Object returnValue = handler.getMethod().invoke(handler.getController(), paramValues); if (returnValue == null || returnValue instanceof Void) &#123; return; &#125; resp.getWriter().write(returnValue.toString()); &#125; private Object convert(Class&lt;?&gt; parameterType, String value) &#123; if (Integer.class == parameterType) &#123; return Integer.parseInt(value); &#125; return value; &#125; private Handler getHandler(HttpServletRequest req) &#123; if (handlerMapping.isEmpty()) &#123; return null; &#125; String url = req.getRequestURI(); String contextPath = req.getContextPath(); url = url.replace(contextPath, &quot;&quot;) .replaceAll(&quot;/+&quot;, &quot;/&quot;); for (Handler handler : handlerMapping) &#123; try &#123; Matcher matcher = handler.pattern.matcher(url); //如果没有匹配上继续下一个匹配 if (!matcher.matches()) &#123; continue; &#125; return handler; &#125; catch (Exception e) &#123; throw e; &#125; &#125; return null; &#125;&#125;","categories":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/tags/Spring/"}]},{"title":"Spring[扩展]MVC使用篇","slug":"Spring/Spring[扩展]MVC使用篇","date":"2022-01-11T06:11:34.427Z","updated":"2022-01-11T06:20:52.215Z","comments":true,"path":"2022/01/11/Spring/Spring[扩展]MVC使用篇/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/Spring/Spring[%E6%89%A9%E5%B1%95]MVC%E4%BD%BF%E7%94%A8%E7%AF%87/","excerpt":"","text":"一，springmvc基本概念1.三层架构开发架构一般基于两种形式，一种是c/s架构，也就是客户端服务器，另一种是b/s架构，也就是浏览器服务器。javaee的开发基本都是b/s架构。在b/s架构中，系统表转的三层架构包括：表现层，业务层，持久层。 表现层：web层。负责接收客户端请求，向客户端响应结果。依赖于业务层，接受请求调用业务层进行业务处理，并将处理结果响应回客户端。 展示层：展示结果。 控制层：接受请求 表现层的设计一般都是使用MVC设计模式。 业务层：service层。负责业务逻辑处理。 业务层可能会依赖于持久层，如果需要对数据持久化，需要保证事物的一致性。 持久层：dao层。负责数据持久化。 数据库：对数据进行持久化的载体。 数据访问层：业务层和持久层交互的接口 持久层就是和数据库交互，对数据库表进行增删改查。 2.MVC模型MVC 全名是 Model View Controller，是模型(model)－视图(view)－控制器(controller)的缩写，是一种用于设计创建 Web 应用程序表现层的模式。MVC 中每个部分各司其职： Model（模型）：通常指的就是我们的数据模型。作用一般情况下用于封装数据。 View（视图）：通常指的就是我们的 jsp 或者 html。作用一般就是展示数据的。通常视图是依据模型数据创建的。 Controller（控制器）：是应用程序中处理用户交互的部分。作用一般就是处理程序逻辑的。 3.springmvc是什么SpringMVC 是一种基于 Java 的实现 MVC 设计模型的请求驱动类型的轻量级 Web 框架，属于 SpringFrameWork 的后续产品，已经融合在 Spring Web Flow 里面。Spring 框架提供了构建 Web 应用程序的全功能 MVC 模块。使用 Spring 可插入的 MVC 架构，从而在使用 Spring 进行 WEB 开发时，可以选择使用 Spring的 Spring MVC 框架或集成其他 MVC 开发框架，如 Struts1(现在一般不用)，Struts2 等。​ SpringMVC 已经成为目前最主流的 MVC 框架之一，并且随着 Spring3.0 的发布，全面超越 Struts2，成为最优秀的 MVC 框架。​ 它通过一套注解，让一个简单的 Java 类成为处理请求的控制器，而无须实现任何接口。同时它还支持RESTful 编程风格的请求。 4.springmvc和struts2的优劣对比共同点： 它们都是表现层框架，都是基于 MVC 模型编写的。 它们的底层都离不开原始 ServletAPI。 它们处理请求的机制都是一个核心控制器。 区别： Spring MVC 的入口是 Servlet, 而 Struts2 是 Filter Spring MVC 是基于方法设计的，而 Struts2 是基于类，Struts2 每次执行都会创建一个动作类。所 以 Spring MVC 会稍微比 Struts2 快些。 Spring MVC 使用更加简洁,同时还支持 JSR303, 处理 ajax 的请求更方便 (JSR303 是一套 JavaBean 参数校验的标准，它定义了很多常用的校验注解，我们可以直接将这些注解加在我们 JavaBean 的属性上面，就可以在需要校验的时候进行校验了。) Struts2 的 OGNL 表达式使页面的开发效率相比 Spring MVC 更高些，但执行效率并没有比 JSTL 提 升，尤其是 struts2 的表单标签，远没有 html 执行效率高。 二，Springmvc入门1.入门案例需求：点击页面超链接，跳转到成功页面 1&lt;a href=&quot;success/success&quot;&gt;testSuccess&lt;/a&gt;&lt;br/&gt; 12345678910111213@Controller@RequestMapping(&quot;/success&quot;)public class success &#123; /** * 入门案例 * @return */ @RequestMapping(&quot;/success&quot;) public String testSuccess()&#123; System.out.println(&quot;testSuccess()...&quot;); return &quot;success&quot;; &#125; &#125; web.xml配置核心控制器：一个Servlet 12345678910111213141516&lt;!-- 配置 DispatcherServlet --&gt; &lt;servlet&gt; &lt;servlet-name&gt;springDispatcherServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:springmvc.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;!-- 映射地址 --&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;springDispatcherServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; springmvc.xml1234567891011121314151617181920212223&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot; http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;!--组件扫描--&gt; &lt;context:component-scan base-package=&quot;com.atguigu&quot;&gt;&lt;/context:component-scan&gt; &lt;!--配置视图解析器--&gt; &lt;bean id=&quot;internalResourceViewResolver&quot; class=&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;&gt; &lt;!--前缀解析器--&gt; &lt;property name=&quot;prefix&quot; value=&quot;/WEB-INF/views/&quot;&gt;&lt;/property&gt; &lt;!--后缀解析器--&gt; &lt;property name=&quot;suffix&quot; value=&quot;.jsp&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!--开启注解支持--&gt; &lt;mvc:annotation-driven /&gt; &lt;/beans&gt; 2.入门案例流程分析 服务器启动，加载应用。读取web.xml中的配置创建spring容器并且初始化容器中的对象。从案例中可以发现创建的是successController和InternalResourceViewResolver，实际上远不止这些。 浏览器发送请求，经过前端控制器，被其捕获，他并不处理请求，只是根据路径的URL匹配有没有对应的，如果匹配到@RequestMapping中的内容，就转发。 转发到控制层执行对应的方法，该方法有一个返回值。 根据方法的返回值，借助视图解析器对象找到对应的视图结果。 渲染结果视图，响应浏览器 3.请求响应流程 1234567891011请求相应流程： 请求先来到springDispatcherServlet看Springmvc是否存在对应的映射？ 如果不存在，看springmvc的配置文件是否配置了&lt;mvc:default-servlet-handler/&gt; 如果不存在，页面响应404，控制台打印no mapping found 如果存在，转发到目标资源。 如果存在，请求交给handlerMapping（处理器映射器），由它获取HandlerExecutionChain对象， 再交给HandlerAdapter（处理器适配器）对象，调用拦截器的PreHandle方法， 然后调用目标Handler方法得到ModelAndview对象，调用拦截器的PostHnadle方法， 查看是否存在异常 如果存在，由异常处理器处理异常，得到新的ModelAndView对象， 如果不存在异常，由视图解析器解析视图，得到实际的View，渲染视图，调用拦截器的afterCompletion方法 4.案例中涉及的组件 DispatcherServlet：前端控制器:用户请求到达前端控制器，它就相当于 mvc 模式中的 c，dispatcherServlet 是整个流程控制的中心，由它调用其它组件处理用户的请求，dispatcherServlet 的存在降低了组件之间的耦合性 HandlerMapping：处理器映射器:HandlerMapping 负责根据用户请求找到 Handler 即处理器，SpringMVC 提供了不同的映射器实现不同的映射方式，例如：配置文件方式，实现接口方式，注解方式等。 Handler：处理器:开发中要编写的具体业务控制器。由 DispatcherServlet 把用户请求转发到 Handler。由Handler 对具体的用户请求进行处理。 HandlAdapter：处理器适配器:通过适配器对处理器进行执行，通过拓展适配器可以处理更多类型。 View Resolver：视图解析器:View Resolver 负责将处理结果生成 View 视图，View Resolver 首先根据逻辑视图名解析成物理视图名.即具体的页面地址，再生成 View 视图对象，最后对 View 进行渲染将处理结果通过页面展示给用户 View：视图:SpringMVC 框架提供了很多的 View 视图类型的支持，包括：jstlView、freemarkerView、pdfView等。我们最常用的视图就是 jsp。一般情况下需要通过页面标签或页面模版技术将模型数据通过页面展示给用户，需要由程序员根据业务需求开发具体的页面。 mvc:annotation-driven说明:在 SpringMVC 的各个组件中，处理器映射器、处理器适配器、视图解析器称为 SpringMVC 的三大组件。使 用 mvc:annotation-driven 自动加载 RequestMappingHandlerMapping （处理映射器） 和RequestMappingHandlerAdapter （ 处 理 适 配 器 ） ， 可 用 在 SpringMVC.xml 配 置 文 件 中 使 用mvc:annotation-driven/替代注解处理器和适配器的配置。 5.RequestMapping注解123456789101112131415161718192021222324源码：@Target(&#123;ElementType.METHOD, ElementType.TYPE&#125;)//可以加到类上和方法上@Retention(RetentionPolicy.RUNTIME)@Documented@Mappingpublic @interface RequestMapping &#123; &#125;作用： 用于建立请求 URL 和处理请求方法之间的对应关系。出现位置： 1.类上 请求 URL 的第一级访问目录，便于模块化管理。 2.方法上 请求 URL 的第二级访问目录。属性： value：用于指定请求的 URL。它和 path 属性的作用是一样的。 method：用于指定请求的方式。 params：用于指定限制请求参数的条件。它支持简单的表达式。要求请求参数的 key 和 value 必须和配置的一模一样。例如：params = &#123;&quot;accountName&quot;&#125;，表示请求参数必须有 accountNameparams = &#123;&quot;moeny!100&quot;&#125;，表示请求参数中 money 不能是 100。 headers：用于指定限制请求消息头的条件。注意：以上四个属性只要出现 2 个或以上时，他们的关系是与的关系。 三，请求参数的绑定1.绑定的机制123456789101112131415表单中请求参数都是基于 key=value 的。SpringMVC 绑定请求参数的过程是通过把表单提交请求参数，作为控制器中方法参数进行绑定的。例如：&lt;a href=&quot;account/findAccount?accountId=10&quot;&gt;查询账户&lt;/a&gt;中请求参数是： accountId=10/*** 查询账户* @return*/@RequestMapping(&quot;/findAccount&quot;)public String findAccount(Integer accountId) &#123; System.out.println(&quot;查询了账户。。。。&quot;+accountId); return &quot;success&quot;;&#125; 2.支持的数据类型 基本类型参数： 包括基本类型和 String 类型 POJO 类型参数： 包括实体类，以及关联的实体类 数组和集合类型参数： 包括 List 结构和 Map 结构的集合（包括数组） SpringMVC 绑定请求参数是自动实现的，但是要想使用，必须遵循使用要求。 3.使用要求 如果是基本类型或者 String 类型： 要求我们的参数名称必须和控制器中方法的形参名称保持一致。(严格区分大小写) 如果是 POJO 类型，或者它的关联对象： 要求表单中参数名称和 POJO 类的属性名称保持一致。并且控制器方法的参数类型是 POJO 类型。 如果是集合类型,有两种方式： 第一种： 要求集合类型的请求参数必须在 POJO 中。在表单中请求参数名称要和 POJO 中集合属性名称相同。 给 List 集合中的元素赋值，使用下标。 给 Map 集合中的元素赋值，使用键值对。 第二种： 接收的请求参数是 json 格式数据。需要借助一个注解实现 注意: 它还可以实现一些数据类型自动转换。如遇特殊类型转换要求，需要我们自己编写自定义类型转换器。 4.代码示例1234567891011121314151617181920/** * 参数绑定1: * bean中包含bean */ @RequestMapping(&quot;/getBean&quot;) public String getBean(Person person)&#123; System.out.println(&quot;getBean()...&quot;); System.out.println(person); return &quot;success&quot;; &#125; /** * 参数绑定2 * 集合类型 */ @RequestMapping(&quot;/getCollection&quot;) public String getList(MyCollection collection)&#123; System.out.println(&quot;getList()....&quot;); System.out.println(collection); return &quot;success&quot;; &#125; 12345678910111213&lt;form action=&quot;success/getBean&quot; method=&quot;post&quot;&gt; id: &lt;input type=&quot;text&quot; name=&quot;id&quot;/&gt;&lt;br/&gt; name: &lt;input type=&quot;text&quot; name=&quot;user.name&quot;/&gt;&lt;br/&gt; age:&lt;input type=&quot;text&quot; name=&quot;user.age&quot;/&gt;&lt;br/&gt; &lt;input type=&quot;submit&quot; value=&quot;submit&quot;/&gt;&lt;br/&gt;&lt;/form&gt;&lt;form method=&quot;post&quot; action=&quot;success/getCollection&quot;&gt; name: &lt;input type=&quot;text&quot; name=&quot;list[0].name&quot;/&gt;&lt;br/&gt; age: &lt;input type=&quot;text&quot; name=&quot;list[0].age&quot;/&gt;&lt;br/&gt; name: &lt;input type=&quot;text&quot; name=&quot;map[1].name&quot;/&gt;&lt;br/&gt; age: &lt;input type=&quot;text&quot; name=&quot;map[1].age&quot;/&gt;&lt;br/&gt; &lt;input type=&quot;submit&quot; value=&quot;submit&quot;/&gt;&lt;/form&gt; 5.请求参数乱码问题idea控制台输出中文乱码：-Dfile.encoding=UTF-8 1234567891011121314&lt;!-- 编码过滤器,必须放在web.xml最上面，防止缓存 --&gt; &lt;filter&gt; &lt;filter-name&gt;CharacterEncodingFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;CharacterEncodingFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;!-- 此处/*代表过滤所有请求 --&gt; &lt;/filter-mapping&gt; 6.关于静态资源处理在 springmvc 的配置文件中可以配置，静态资源不过滤： 1234&lt;!-- location 表示路径，mapping 表示文件，**表示该目录下的文件以及子目录的文件 --&gt;&lt;mvc:resources location=&quot;/css/&quot; mapping=&quot;/css/**&quot;/&gt;&lt;mvc:resources location=&quot;/images/&quot; mapping=&quot;/images/**&quot;/&gt;&lt;mvc:resources location=&quot;/scripts/&quot; mapping=&quot;/javascript/**&quot;/&gt; 7.关于get请求tomacat 对 GET 和 POST 请求处理方式是不同的，GET 请求的编码问题，要改 tomcat 的 server.xml 配置文件，如下： 123456789&lt;Connector connectionTimeout=&quot;20000&quot; port=&quot;8080&quot;protocol=&quot;HTTP/1.1&quot; redirectPort=&quot;8443&quot;/&gt;改为：&lt;Connector connectionTimeout=&quot;20000&quot; port=&quot;8080&quot;protocol=&quot;HTTP/1.1&quot; redirectPort=&quot;8443&quot;useBodyEncodingForURI=&quot;true&quot;/&gt;如果遇到 ajax 请求仍然乱码，请把：useBodyEncodingForURI=&quot;true&quot;改为 URIEncoding=&quot;UTF-8&quot;即可 8.自定义类型转换器代码：jsp页面 1234&lt;form action=&quot;success/date&quot; method=&quot;post&quot;&gt; &lt;input type=&quot;text&quot; name=&quot;date&quot;/&gt;&lt;br/&gt; &lt;input type=&quot;submit&quot; value=&quot;submit&quot;/&gt;&lt;/form&gt; 转换器 123456789101112131415public class StringToDate implements Converter&lt;String, Date&gt; &#123; @Override public Date convert(String s) &#123; if (s==null||&quot;&quot;.equals(s))&#123; throw new RuntimeException(&quot;输入不能为空&quot;); &#125; try &#123; SimpleDateFormat format = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;); return format.parse(s); &#125; catch (Exception e) &#123; throw new ClassCastException(&quot;类型转换异常&quot;); &#125; &#125;&#125; 自定义类型转换器 spring 配置类型转换器的机制是，将自定义的转换器注册到类型转换服务中去。 1234567891011&lt;!--自定义类型转换--&gt;&lt;!-- 配置类型转换器工厂 --&gt; &lt;bean id=&quot;conversionServiceFactoryBean&quot; class=&quot;org.springframework.context.support.ConversionServiceFactoryBean&quot;&gt; &lt;!-- 给工厂注入一个新的类型转换器 --&gt; &lt;property name=&quot;converters&quot;&gt; &lt;set&gt;&lt;!-- 配置自定义类型转换器 --&gt; &lt;bean class=&quot;com.atguigu.utils.StringToDate&quot;&gt;&lt;/bean&gt; &lt;/set&gt; &lt;/property&gt; &lt;/bean&gt;&lt;mvc:annotation-driven conversion-service=&quot;conversionServiceFactoryBean&quot;/&gt; 控制器 1234567891011/** * 自定义数据类型转换 * 1.编写类型转换类，实现 Converter 接口，该接口有两个泛型。 * 2.配置文件中配置转换器,并在注解支持里面注册。 * 3.控制层参数列表传入转换类 */ @RequestMapping(&quot;/date&quot;) public String getDate(Date date)&#123; System.out.println(date.toString()); return &quot;success&quot;; &#125; 9.使用servletAPI对象作为方法参数springMVC 还支持使用原始 ServletAPI 对象作为控制器方法的参数。支持原始 ServletAPI 对象有： 12345678/** * 获取原生ServletAPI */@RequestMapping(&quot;/getServlet&quot;)public String getServlet(HttpServletRequest request, HttpServletResponse response)&#123; System.out.println(&quot;getServlet()...&quot;); return &quot;success&quot;;&#125; 1&lt;a href=&quot;success/getServlet&quot;&gt;getServlet&lt;/a&gt;&lt;br/&gt; 四，常用注解1.RequestParam作用： 把请求中指定名称的参数给控制器中的形参赋值。属性： value：请求参数中的名称。 required：请求参数中是否必须提供此参数。默认值：true。表示必须提供，如果不提供将报错。 1234567891011/** * @RequestParam 当表单name和形参列表名字不一致时，使用此注解。 * 属性： * name：指定表单的name * required：指定是否为必须 */ @RequestMapping(&quot;/test1&quot;) public String test1(@RequestParam(name = &quot;name&quot;, required = false) String username, Integer age) &#123; System.out.println(username + &quot; &quot; + age); return &quot;success&quot;; &#125; 12345&lt;form method=&quot;post&quot; action=&quot;anno/test1&quot;&gt; &lt;input type=&quot;text&quot; name=&quot;name&quot;/&gt;&lt;br/&gt; &lt;input type=&quot;text&quot; name=&quot;age&quot;/&gt;&lt;br/&gt; &lt;input type=&quot;submit&quot; value=&quot;submit&quot;/&gt;&lt;/form&gt; 2.RequestBody作用： 用于获取请求体内容。直接使用得到是 key=value&amp;key=value…结构的数据。 get 请求方式不适用。属性： required：是否必须有请求体。默认值是:true。当取值为 true 时,get 请求方式会报错。如果取值 为 false，get 请求得到是 null。 12345678910111213/** * @RequestBody 用于获取请求体内容 * 属性： * 属性： * required：指定是否为必须 * 当请求方式为get时，指定为true会报错， * 指定为false时，拿到的是null。 */ @RequestMapping(&quot;/test2&quot;) public String test2(@RequestBody(required = false) String name) &#123; System.out.println(name); return &quot;success&quot;; &#125; 1234&lt;form method=&quot;post&quot; action=&quot;anno/test2&quot;&gt; &lt;input type=&quot;text&quot; name=&quot;name&quot;/&gt;&lt;br/&gt; &lt;input type=&quot;submit&quot; value=&quot;submit&quot;/&gt;&lt;/form&gt; 3.PathVariable作用： 用于绑定 url 中的占位符。例如：请求 url 中 /delete/{id}，这个{id}就是 url 占位符。 url 支持占位符是 spring3.0 之后加入的。是 springmvc 支持 rest 风格 URL 的一个重要标志。属性： value：用于指定 url 中占位符名称。 required：是否必须提供占位符。 1&lt;a href=&quot;anno/test3/张三&quot;&gt;test3&lt;/a&gt;&lt;br/&gt; 12345678910/** * @PathVariable restful风格，spring3.0新特性 * 以/参数方式传递参数 * 次注解的value属性指定传递的参数名 */@RequestMapping(&quot;/test3/&#123;name&#125;&quot;)public String test3(@PathVariable(&quot;name&quot;) String name) &#123; System.out.println(name); return &quot;success&quot;;&#125; 关于rest风格url123456789101112131415161718192021222324252627*什么是 rest：REST（英文：Representational State Transfer，简称 REST）描述了一个架构样式的网络系统，比如 web 应用程序。它首次出现在 2000 年 Roy Fielding 的博士论文中，他是 HTTP 规范的主要编写者之一。在目前主流的三种 Web 服务交互方案中，REST 相比于 SOAP（Simple Object Access protocol，简单对象访问协议）以及 XML-RPC 更加简单明了，无论是对 URL 的处理还是对 Payload 的编码，REST 都倾向于用更加简单轻量的方法设计和实现。值得注意的是 REST 并没有一个明确的标准，而更像是一种设计的风格。它本身并没有什么实用性，其核心价值在于如何设计出符合 REST 风格的网络接口。*restful 的优点它结构清晰、符合标准、易于理解、扩展方便，所以正得到越来越多网站的采用。*restful 的特性：*资源（Resources）：网络上的一个实体，或者说是网络上的一个具体信息。它可以是一段文本、一张图片、一首歌曲、一种服务，总之就是一个具体的存在。可以用一个 URI（统一资源定位符）指向它，每种资源对应一个特定的 URI 。要获取这个资源，访问它的 URI 就可以，因此 URI 即为每一个资源的独一无二的识别符。 表现层（Representation）：把资源具体呈现出来的形式，叫做它的表现层 （Representation）。比如，文本可以用 txt 格式表现，也可以用 HTML 格式、XML 格式、JSON 格式表现，甚至可以采用二进制格式。*状态转化（State Transfer）：每 发出一个请求，就代表了客户端和服务器的一次交互过程。*HTTP 协议，是一个无状态协议，即所有的状态都保存在服务器端。因此，如果客户端想要操作服务器，必须通过某种手段，让服务器端发生“状态转化”（State Transfer）。而这种转化是建立在表现层之上的，所以就是 “表现层状态转化”。具体说，就是 HTTP 协议里面，四个表示操作方式的动词：GET 、POST 、PUT、DELETE。它们分别对应四种基本操作：GET 用来获取资源，POST 用来新建资源，PUT 用来更新资源，DELETE 用来删除资源。*restful 的示例： /account/1 HTTP GET ： 得到 id = 1 的 account /account/1 HTTP DELETE： 删除 id = 1 的 account /account/1 HTTP PUT： 更新 id = 1 的 account /account HTTP POST： 新增 account 4.requestHeader作用： 用于获取请求消息头。属性： value：提供消息头名称 required：是否必须有此消息头注： 在实际开发中一般不怎么用。 1234567891011/** * @RequestHeader 获取请求头信息，不常用 * 属性： * value: * required: */@RequestMapping(&quot;/test4&quot;)public String test4(@RequestHeader(value = &quot;Accept-Language&quot;, required = false) String value) &#123; System.out.println(value); return &quot;success&quot;;&#125; 1&lt;a href=&quot;anno/test4&quot;&gt;test4&lt;/a&gt;&lt;br/&gt; 5.CookieValue作用： 用于把指定 cookie 名称的值传入控制器方法参数。属性： value：指定 cookie 的名称。 required：是否必须有此 cookie。 1234567891011/** * @CookieValue 获取cookie里面的信息 * 属性： * value： * required： */@RequestMapping(&quot;/test5&quot;)public String test5(@CookieValue(value = &quot;JSESSIONID&quot;, required = false) String value) &#123; System.out.println(value); return &quot;success&quot;;&#125; 1&lt;a href=&quot;anno/test5&quot;&gt;test5&lt;/a&gt;&lt;br/&gt; 6.ModelAttribute作用： 该注解是 SpringMVC4.3 版本以后新加入的。它可以用于修饰方法和参数。 出现在方法上，表示当前方法会在控制器的方法执行之前，先执行。它可以修饰没有返回值的方法，也可 以修饰有具体返回值的方法。 出现在参数上，获取指定的数据给参数赋值。属性： value：用于获取数据的 key。key 可以是 POJO 的属性名称，也可以是 map 结构的 key。应用场景： 当表单提交数据不是完整的实体类数据时，保证没有提交数据的字段使用数据库对象原来的数据。 例如： 我们在编辑一个用户时，用户有一个创建信息字段，该字段的值是不允许被修改的。在提交表单数 据是肯定没有此字段的内容，一旦更新会把该字段内容置为 null，此时就可以使用此注解解决问题。 12345678910111213141516171819202122/** * @ModelAttribute * 1.夹在方法上： * 1.没有返回值 * 可以应用在从表单获取的值不全，在返回方法前先给其将值补全 * 2.有返回值 * 可以根据表单提交的一个值在到达控制层之前先去从数据库查询， * *严重怀疑这个方法利用动态代理对方法进行增强。 * 2.加在方法参数上： * 可以为指定的属性赋值 * */ @RequestMapping(&quot;/test6&quot;) public String test6(User user) &#123; System.out.println(user.getName() + &quot;...&quot; + user.getAge()); return &quot;success&quot;; &#125; @ModelAttribute public void test11(User user)&#123; System.out.println(user.getName()+&quot; &quot;+user.getAge()+&quot;......&quot;); user.setAge(20); &#125; 1234&lt;form action=&quot;anno/test6&quot;&gt; &lt;input type=&quot;text&quot; name=&quot;name&quot;/&gt;&lt;br/&gt; &lt;input type=&quot;submit&quot; value=&quot;submit&quot;/&gt;&lt;/form&gt; 场景二：123456789101112131415161718 @RequestMapping(&quot;/test6&quot;) public String test6(User user) &#123; System.out.println(user.getName() + &quot;...&quot; + user.getAge()); return &quot;success&quot;; &#125; @ModelAttribute public User test22(String name) &#123; //模拟从数据库查询数据 User user=findUserByname(name); return user; &#125; //模拟服务层dao层方法 private User findUserByname(String name) &#123; User user = new User(); user.setName(name); user.setAge(20); return user; &#125; 1234&lt;form action=&quot;anno/test6&quot;&gt; &lt;input type=&quot;text&quot; name=&quot;name&quot;/&gt;&lt;br/&gt; &lt;input type=&quot;submit&quot; value=&quot;submit&quot;/&gt;&lt;/form&gt; 场景三123456789101112131415161718//模拟服务层dao层方法private User findUserByname(String name) &#123; User user = new User(); user.setName(name); user.setAge(20); return user;&#125;@RequestMapping(&quot;/test7&quot;)public String test7(@ModelAttribute(value=&quot;1&quot;) User user)&#123; System.out.println(user); return &quot;success&quot;;&#125;@ModelAttributepublic void test33(String name, Map&lt;String,User&gt;map)&#123; //模拟从数据库查询 User user=findUserByname(name); map.put(&quot;1&quot;,user);&#125; 1234&lt;form action=&quot;anno/test7&quot;&gt; &lt;input type=&quot;text&quot; name=&quot;name&quot;/&gt;&lt;br/&gt; &lt;input type=&quot;submit&quot; value=&quot;submit&quot;/&gt;&lt;/form&gt; 7.SessionAttribute作用： 用于多次执行控制器方法间的参数共享。属性： value：用于指定存入的属性名称 type：用于指定存入的数据类型 123&lt;a href=&quot;session/put&quot;&gt;put&lt;/a&gt;&lt;br/&gt;&lt;a href=&quot;session/get&quot;&gt;get&lt;/a&gt;&lt;br/&gt;&lt;a href=&quot;session/delete&quot;&gt;delete&lt;/a&gt;&lt;br/&gt; 123456789101112131415161718192021222324252627282930313233@Controller@RequestMapping(&quot;/session&quot;)@SessionAttributes(value = &#123;&quot;name&quot;,&quot;age&quot;&#125;)public class Session &#123; /** * 存入Session * Model 是 spring 提供的一个接口，该接口有一个实现类 ExtendedModelMap * 该类继承了 ModelMap，而 ModelMap 就是 LinkedHashMap 子类 */ @RequestMapping(&quot;/put&quot;) public String put(Model model)&#123; model.addAttribute(&quot;name&quot;,&quot;尹会东&quot;); model.addAttribute(&quot;age&quot;,23); return &quot;success&quot;; &#125; /** * 取出Session */ @RequestMapping(&quot;/get&quot;) public String get(ModelMap map)&#123; System.out.println(map.get(&quot;name&quot;)); System.out.println(map.get(&quot;age&quot;)); return &quot;success&quot;; &#125; /** * 清除Session */ @RequestMapping(&quot;/delete&quot;) public String delete(SessionStatus status)&#123; status.setComplete(); return &quot;success&quot;; &#125;&#125; 五，响应数据和结果视图1.返回值分类①void在 controller 方法形参上可以定义 request 和 response，使用 request 或 response 指定响应结果： 123456781、使用 request 转向页面，如下：request.getRequestDispatcher(&quot;/WEB-INF/pages/success.jsp&quot;).forward(request, response);2、也可以通过 response 页面重定向：response.sendRedirect(&quot;testRetrunString&quot;) 3、也可以通过 response 指定响应结果，例如响应 json 数据：response.setCharacterEncoding(&quot;utf-8&quot;);response.setContentType(&quot;application/json;charset=utf-8&quot;);response.getWriter().write(&quot;json 串&quot;); ②ModelAndViewModelAndView 是 SpringMVC 为我们提供的一个对象，该对象也可以用作控制器方法的返回值。 该对象中有两个方法： ③转发和重定向forward 转发controller 方法在提供了 String 类型的返回值之后，默认就是请求转发。​ 需要注意的是，如果用了 formward：则路径必须写成实际视图 url，不能写逻辑视图。​ 它相当于“request.getRequestDispatcher(“url”).forward(request,response)”。使用请求转发，既可以转发到 jsp，也可以转发到其他的控制器方法。 Redirect 重定向contrller 方法提供了一个 String 类型返回值之后，它需要在返回值里使用:redirect:它相当于“response.sendRedirect(url)”。需要注意的是，如果是重定向到 jsp 页面，则 jsp 页面不能写在 WEB-INF 目录中，否则无法找到 1234567891011121314151617181920212223242526272829303132333435363738394041424344@Controller@RequestMapping(&quot;/return&quot;)public class returnController &#123; /** * 返回值类型为String，在request作用于存放值，并显示到页面 * @param model * @return */ @RequestMapping(&quot;/string&quot;) public String test1(Model model)&#123; model.addAttribute(&quot;name&quot;,&quot;尹会东&quot;); model.addAttribute(&quot;age&quot;,20); return &quot;success&quot;; &#125; /** * 返回值类型为void */ @RequestMapping(&quot;/void&quot;) public void test2(HttpServletResponse response, HttpServletRequest request)throws Exception&#123;// //request.getRequestDispatcher(&quot;/WEB-INF/views/success.jsp&quot;).forward(request,response); //response.sendRedirect(&quot;/index.jsp&quot;); response.getWriter().write(&quot;aaaa&quot;); &#125; /** * 返回值类型为ModelAndView */ @RequestMapping(&quot;/model&quot;) public ModelAndView test3()&#123; ModelAndView mv = new ModelAndView(); mv.addObject(&quot;name&quot;,&quot;zhangsan&quot;); mv.addObject(&quot;age&quot;,20); mv.setViewName(&quot;success&quot;); return mv; &#125; /** * 关键字：forward和redirect */ @RequestMapping(&quot;/fr&quot;) public String test4()&#123; System.out.println(&quot;..................&quot;); //return &quot;forward:/WEB-INF/views/success.jsp&quot;; return &quot;redirect:/index.jsp&quot;; &#125;&#125; 12345678910111213&lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;return&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h3&gt;return返回值类型&lt;/h3&gt;&lt;a href=&quot;return/string&quot; &gt;string&lt;/a&gt;&lt;br/&gt;&lt;a href=&quot;return/void&quot; &gt;void&lt;/a&gt;&lt;br/&gt;&lt;a href=&quot;return/model&quot; &gt;model&lt;/a&gt;&lt;br/&gt;&lt;a href=&quot;return/fr&quot; &gt;fr&lt;/a&gt;&lt;br/&gt;&lt;/body&gt;&lt;/html&gt; ④@ResponseBody 注解响应json数据作用：​ 该注解用于将 Controller 的方法返回的对象，通过 HttpMessageConverter 接口转换为指定格式的数据如：json,xml 等，通过 Response 响应给客户端 123456789101112131415161718192021222324@Controller@RequestMapping(&quot;/ajax&quot;)public class Ajax &#123; /** * 发送Ajax异步请求 * 1.静态资源处理：在配置文件中加入&lt;mvc:resource /&gt;标签，指定放行的资源。 * 2.导入jackson的依赖 * 3. * @RequestBody * 接受请求体消息 * @ResponseBody * 发送响应体消息 * 4.springmvc框架已经为我们封装好了处理json数据的方法，底层会自动执行。 * @param user * @return */ @RequestMapping(&quot;/ajax&quot;) public @ResponseBody User testAjax(@RequestBody User user) &#123; user.setName(&quot;yinhuidong&quot;); user.setAge(23); return user; &#125;&#125; 123456789101112131415161718192021222324252627&lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;Ajax&lt;/title&gt; &lt;script src=&quot;js/jquery-3.3.1.min.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot;&gt; $(function () &#123; $(&quot;#btn&quot;).click(function () &#123; $.ajax(&#123; url: &quot;ajax/ajax&quot;, contentType:&quot;application/json;charset=UTF-8&quot;, data: &#x27;&#123;&quot;name&quot;:&quot;aa&quot;,&quot;age&quot;:20&#125;&#x27;, dataType: &quot;json&quot;, type: &quot;post&quot;, success: function (data) &#123; alert(data.name); alert(data.age); &#125; &#125;); &#125;); &#125;); &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;input type=&quot;button&quot; id=&quot;btn&quot; value=&quot;别点我&quot;/&gt;&lt;/body&gt;&lt;/html&gt; 12//data:JSON.stringify(&#123;&quot;name&quot;:&quot;张三&quot;,&quot;msg&quot;:message&#125;),data:&#x27;&#123;&quot;name&quot;:&quot;张三&quot;,&quot;msg&quot;:&quot;123&quot;&#125;&#x27;, 六，文件上传和下载1.文件上传前提条件 form 表单的 enctype 取值必须是：multipart/form-data (默认值是:application/x-www-form-urlencoded) enctype:是表单请求正文的类型 method 属性取值必须是 Post 提供一个文件选择域 原理分析1234567891011121314当 form 表单的 enctype 取值不是默认值后，request.getParameter()将失效。 enctype=”application/x-www-form-urlencoded”时，form 表单的正文内容是： key=value&amp;key=value&amp;key=value当 form 表单的 enctype 取值为 Mutilpart/form-data 时，请求正文内容就变成： 每一部分都是 MIME 类型描述的正文-----------------------------7de1a433602ac 分界符Content-Disposition: form-data; name=&quot;userName&quot; 协议头aaa 协议的正文-----------------------------7de1a433602acContent-Disposition: form-data; name=&quot;file&quot;; filename=&quot;C:\\Users\\zhy\\Desktop\\fileupload_demofile\\b.txt&quot;Content-Type: text/plain 协议的类型（MIME 类型）bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb-----------------------------7de1a433602ac-- ①传统模式的文件上传123456&lt;h3&gt;文件上传&lt;/h3&gt;&lt;form action=&quot;file/fileupload&quot; method=&quot;post&quot; enctype=&quot;multipart/form-data&quot;&gt; 择文件：&lt;input type=&quot;file&quot; name=&quot;upload&quot;/&gt;&lt;br/&gt; &lt;input type=&quot;submit&quot; value=&quot;上传文件&quot;/&gt;&lt;/form&gt;&lt;hr/&gt; 12345678910111213141516171819202122232425262728293031323334353637383940@Controller@RequestMapping(&quot;/file&quot;)public class upload &#123; /** * 文件上传1: * 传统的文件上传 * */ @RequestMapping(value = &quot;/fileupload&quot;) public String fileupload(HttpServletRequest request) throws Exception &#123; // 先获取到要上传的文件目录 String path = request.getSession().getServletContext().getRealPath(&quot;/uploads&quot;); // 创建File对象，一会向该路径下上传文件 File file = new File(path); // 判断路径是否存在，如果不存在，创建该路径 if (!file.exists()) &#123; file.mkdirs(); &#125; // 创建磁盘文件项工厂 DiskFileItemFactory factory = new DiskFileItemFactory(); ServletFileUpload fileUpload = new ServletFileUpload(factory); // 解析request对象 List&lt;FileItem&gt; list = fileUpload.parseRequest(request); // 遍历 for (FileItem fileItem : list) &#123; // 判断文件项是普通字段，还是上传的文件 if (fileItem.isFormField()) &#123; &#125; else &#123; // 上传文件项 &#125; // 获取到上传文件的名称 String filename = fileItem.getName(); // 上传文件 fileItem.write(new File(file, filename)); // 删除临时文件 fileItem.delete(); &#125; return &quot;success&quot;; &#125; &#125; ②springmvc文件上传12345&lt;form action=&quot;file/upload&quot; method=&quot;post&quot; enctype=&quot;multipart/form-data&quot;&gt; 择文件：&lt;input type=&quot;file&quot; name=&quot;upload&quot;/&gt;&lt;br/&gt; &lt;input type=&quot;submit&quot; value=&quot;上传文件&quot;/&gt;&lt;/form&gt;&lt;hr/&gt; 12345678910111213141516171819202122232425262728/** * springmvc文件上传 * 1.导入依赖 * commons-upload * commons-io * 2.配置文件解析器 * 3.编写jsp页面 * 4.代码实现 */ @RequestMapping(&quot;/upload&quot;) public String fileupload2(HttpSession session, MultipartFile upload)throws Exception&#123; //获取文件上传路径 String path = session.getServletContext().getRealPath(&quot;/img&quot;); File file = new File(path); //判断不存在该目录就创建 if (!file.exists())&#123; file.mkdirs(); &#125; //获取文件名 String filename = upload.getOriginalFilename(); System.out.println(filename); //起别名 String s = UUID.randomUUID().toString().replace(&quot;_&quot;, &quot;&quot;).toUpperCase(); filename=s+filename; //开始上传 upload.transferTo(new File(file,filename)); return &quot;success&quot;; &#125; 2.文件下载12345678910&lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h3&gt;文件下载&lt;/h3&gt;&lt;a href=&quot;down/down?name=6.jpg&quot;&gt;点击下载&lt;/a&gt;&lt;/body&gt;&lt;/html&gt; 123456789101112131415161718192021222324252627282930313233@Controller@RequestMapping(&quot;/down&quot;)public class down &#123; /** * 文件下载 * 1.获取文件名 * 2.获得文件下载路径 * 3.拼接 * 4.用流来加载文件到字节数组 * 5.设置头信息以附件形式打开 * 6.设置响应状态吗 * 7.下载 */ @RequestMapping(&quot;/down&quot;) public ResponseEntity&lt;byte[]&gt; down(HttpSession session,String name)throws Exception&#123; //获取文件下载路径 String path = session.getServletContext().getRealPath(&quot;/img&quot;); //加上文件名 String finalpath = path + File.separator + name; FileInputStream is = new FileInputStream(finalpath); //is.available()获取流的字节数 byte[] bytes = new byte[is.available()]; is.read(bytes); HttpHeaders headers = new HttpHeaders(); //设置有附件形式打开 headers.add(&quot;Content-Disposition&quot;, &quot;attachment;filename=&quot;+name); //设置响应吗 HttpStatus status=HttpStatus.OK; ResponseEntity&lt;byte[]&gt; entity = new ResponseEntity&lt;&gt;(bytes, headers, status); is.close(); return entity; &#125;&#125; 七，springmvc中的异常处理1.异常处理的思路系统中异常包括两类：预期异常和运行时异常 RuntimeException，前者通过捕获异常从而获取异常信息，后者主要通过规范代码开发、测试通过手段减少运行时异常的发生。​ 系统的 dao、service、controller 出现都通过 throws Exception 向上抛出，最后由 springmvc 前端控制器交由异常处理器进行异常处理。 2.代码自定义异常类1234567891011121314151617181920212223/** * @author yinhuidong * @createTime 2020-03-09-17:18 */public class MyException extends Exception&#123; private String message; public MyException() &#123; &#125; public MyException(String message) &#123; this.message = message; &#125; @Override public String getMessage() &#123; return message; &#125; public void setMessage(String message) &#123; this.message = message; &#125;&#125; 123456789101112131415161718192021/** * 自定义异常处理器 * @author yinhuidong * @createTime 2020-03-09-17:20 */public class HandlerException implements HandlerExceptionResolver &#123; @Override public ModelAndView resolveException(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o, Exception e) &#123; MyException mye=null; if (e instanceof MyException)&#123; mye= (MyException) e; &#125;else&#123; mye=new MyException(); &#125; mye.setMessage(&quot;系统繁忙，请稍后再试！&quot;); ModelAndView mv = new ModelAndView(); mv.addObject(&quot;message&quot;,mye.getMessage()); mv.setViewName(&quot;/error&quot;); return mv; &#125;&#125; 12&lt;!--异常处理类--&gt; &lt;bean id=&quot;handlerException&quot; class=&quot;com.atguigu.exception.HandlerException&quot;&gt;&lt;/bean&gt; 123456789101112131415161718192021/** * @author yinhuidong * @createTime 2020-03-09-17:27 */@Controller@RequestMapping(&quot;/exception&quot;)public class TestException &#123; /** * 测试异常 * 1.编写自定义异常类，继承Exception类 * 2.编写异常处理类， * 3.在配置文件中配置异常处理类 * 4.模拟发生异常 * @return */ @RequestMapping(&quot;/test&quot;) public String test1()throws MyException &#123; int i=1/0; return &quot;success&quot;; &#125;&#125; 1&lt;a href=&quot;exception/test&quot;&gt;测试异常&lt;/a&gt; 全局异常处理器 123456789101112131415161718192021222324252627282930313233343536public class ExceptionHandler implements HandlerExceptionResolver &#123; @Override public ModelAndView resolveException (HttpServletRequest request, HttpServletResponse response, Object o, Exception e) &#123; boolean isAjax = JudgeRequestType.judgeIsAjax(request); if (isAjax)&#123; try &#123; String message=e.getMessage(); Gson gson = new Gson(); String json = gson.toJson(message); response.getWriter().write(json); &#125; catch (IOException e1) &#123; e1.printStackTrace(); &#125; return null; &#125; ModelAndView mv = new ModelAndView(); mv.addObject(&quot;message&quot;,e.getMessage()); mv.setViewName(&quot;error&quot;); return mv; &#125;&#125;class JudgeRequestType &#123; public static boolean judgeIsAjax(HttpServletRequest request)&#123; String accept = request.getHeader(&quot;Accept&quot;); String header = request.getHeader(&quot;X-Requested-With&quot;); return (accept!=null &amp;&amp;accept.length()&gt;0&amp;&amp;accept.contains(&quot;application/json&quot;)) || (header!=null&amp;&amp;header.length()&gt;0&amp;&amp;header.equals(&quot;XMLHttpRequest&quot;)); &#125;&#125; 八，springmvc中的拦截器拦截器的作用Spring MVC 的处理器拦截器类似于 Servlet 开发中的过滤器 Filter，用于对处理器进行预处理和后处理。用户可以自己定义一些拦截器来实现特定的功能。​ 谈到拦截器，还要向大家提一个词——拦截器链（Interceptor Chain）。拦截器链就是将拦截器按一定的顺序联结成一条链。在访问被拦截的方法或字段时，拦截器链中的拦截器就会按其之前定义的顺序被调用。​ 说到这里，可能大家脑海中有了一个疑问，这不是我们之前学的过滤器吗？是的它和过滤器是有几分相似，但是也有区别，接下来我们就来说说他们的区别： 过滤器是 servlet 规范中的一部分，任何 java web 工程都可以使用。 拦截器是 SpringMVC 框架自己的，只有使用了 SpringMVC 框架的工程才能用。 过滤器在 url-pattern 中配置了/*之后，可以对所有要访问的资源拦截。 拦截器它是只会拦截访问的控制器方法，如果访问的是 jsp，html,css,image 或者 js 是不会进行拦 截的。​ 它也是 AOP 思想的具体应用。我们要想自定义拦截器， 要求必须实现：HandlerInterceptor 接口。 自定义拦截器步骤1.写一个类继承HandlerInterceptor接口12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/** * @author yinhuidong * @createTime 2020-03-09-17:33 */public class Intercepter1 implements HandlerInterceptor &#123; /** * 1. preHandle方法是controller方法执行前拦截的方法 * 1. 可以使用request或者response跳转到指定的页面 * 2. return true放行，执行下一个拦截器，如果没有拦截器， * 执行controller中的方法。 * 3. return false不放行，不会执行controller中的方法 * @param request * @param response * @param handler * @return * @throws Exception */ public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; System.out.println(&quot;preHandle()....&quot;); //request.getRequestDispatcher(&quot;/WEB-INF/views/error.jsp&quot;).forward(request, response); return true; &#125; /** * 2. postHandle是controller方法执行后执行的方法，在JSP视图执行前。 * 1. 可以使用request或者response跳转到指定的页面 * 2. 如果指定了跳转的页面，那么controller方法跳转的页面将不会显示。 * @param request * @param response * @param handler * @param modelAndView * @throws Exception */ public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, @Nullable ModelAndView modelAndView) throws Exception &#123; System.out.println(&quot;postHandle()....&quot;); &#125; /** * 3. postHandle方法是在JSP执行后执行 * request或者response不能再跳转页面了 * @param request * @param response * @param handler * @param ex * @throws Exception */ public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, @Nullable Exception ex) throws Exception &#123; System.out.println(&quot;afterCompletion()...&quot;); &#125;&#125; 2.在springmvc的配置文件中配置拦截器1234567891011121314151617181920&lt;!--配置拦截器--&gt; &lt;mvc:interceptors&gt; &lt;!--配置一个拦截器--&gt; &lt;mvc:interceptor&gt; &lt;!--设置拦截路径--&gt; &lt;mvc:mapping path=&quot;/**&quot;/&gt; &lt;!--设置哪些不拦截--&gt; &lt;!--&lt;mvc:exclude-mapping path=&quot;&quot;/&gt;--&gt; &lt;!--配置bean--&gt; &lt;bean class=&quot;com.atguigu.intercept.Intercepter1&quot;/&gt; &lt;/mvc:interceptor&gt; &lt;mvc:interceptor&gt; &lt;!--设置拦截路径--&gt; &lt;mvc:mapping path=&quot;/**&quot;/&gt; &lt;!--设置哪些不拦截--&gt; &lt;!--&lt;mvc:exclude-mapping path=&quot;&quot;/&gt;--&gt; &lt;!--配置bean--&gt; &lt;bean class=&quot;com.atguigu.intercept.Intercepter2&quot;/&gt; &lt;/mvc:interceptor&gt; &lt;/mvc:interceptors&gt; 3.编写测试类和jsp页面12345678910111213141516171819/** * @author yinhuidong * @createTime 2020-03-09-17:37 */@Controller@RequestMapping(&quot;/intercepter&quot;)public class TestIntercepter &#123; /** * 1.编写拦截器 * 2.在配置文件中配置拦截器 * 3.测试 * 4.多个拦截器执行顺序 */ @RequestMapping(&quot;/test&quot;) public String test()&#123; System.out.println(&quot;controller().....&quot;); return &quot;success&quot;; &#125;&#125; 1&lt;a href=&quot;intercepter/test&quot;&gt;测试拦截器&lt;/a&gt; 4.定义多个拦截器12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/** * @author yinhuidong * @createTime 2020-03-09-17:33 */public class Intercepter2 implements HandlerInterceptor &#123; /** * 1. preHandle方法是controller方法执行前拦截的方法 * 1. 可以使用request或者response跳转到指定的页面 * 2. return true放行，执行下一个拦截器，如果没有拦截器， * 执行controller中的方法。 * 3. return false不放行，不会执行controller中的方法 * @param request * @param response * @param handler * @return * @throws Exception */ public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; System.out.println(&quot;preHandle2()....&quot;); //request.getRequestDispatcher(&quot;/WEB-INF/views/error.jsp&quot;).forward(request, response); return true; &#125; /** * 2. postHandle是controller方法执行后执行的方法，在JSP视图执行前。 * 1. 可以使用request或者response跳转到指定的页面 * 2. 如果指定了跳转的页面，那么controller方法跳转的页面将不会显示。 * @param request * @param response * @param handler * @param modelAndView * @throws Exception */ public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, @Nullable ModelAndView modelAndView) throws Exception &#123; System.out.println(&quot;postHandle()2....&quot;); &#125; /** * 3. postHandle方法是在JSP执行后执行 * request或者response不能再跳转页面了 * @param request * @param response * @param handler * @param ex * @throws Exception */ public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, @Nullable Exception ex) throws Exception &#123; System.out.println(&quot;afterCompletion()2...&quot;); &#125;&#125; 拦截器的简单应用需求：123456781、有一个登录页面，需要写一个 controller 访问页面 2、登录页面有一提交表单的动作。需要在 controller 中处理。 2.1、判断用户名密码是否正确 2.2、如果正确 向 session 中写入用户信息 2.3、返回登录成功。3、拦截用户请求，判断用户是否登录 3.1、如果用户已经登录。放行 3.2、如果用户未登录，跳转到登录页面 代码实现：1.控制器代码1234567891011121314151617181920//登陆页面@RequestMapping(&quot;/login&quot;)public String login(Model model)throws Exception&#123; return &quot;login&quot;; &#125;//登陆提交//userid：用户账号，pwd：密码@RequestMapping(&quot;/loginsubmit&quot;)public String loginsubmit(HttpSession session,String userid,String pwd)throws Exception&#123; //向 session 记录用户身份信息 session.setAttribute(&quot;activeUser&quot;, userid); return &quot;redirect:/main.jsp&quot;; &#125;//退出@RequestMapping(&quot;/logout&quot;)public String logout(HttpSession session)throws Exception&#123; //session 过期 session.invalidate(); return &quot;redirect:index.jsp&quot;;&#125; 2.拦截器代码123456789101112131415161718public class LoginInterceptor implements HandlerInterceptor&#123;@OverridePublic boolean preHandle(HttpServletRequest request,HttpServletResponse response, Object handler) throws Exception &#123; //如果是登录页面则放行 if(request.getRequestURI().indexOf(&quot;login.action&quot;)&gt;=0)&#123; return true; &#125; HttpSession session = request.getSession(); //如果用户已登录也放行 if(session.getAttribute(&quot;user&quot;)!=null)&#123; return true; &#125; //用户没有登录挑战到登录页面 request.getRequestDispatcher(&quot;/WEB-INF/jsp/login.jsp&quot;).forward(request,response); return false; &#125; &#125;","categories":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/tags/Spring/"}]},{"title":"Spring[扩展]基本使用篇","slug":"Spring/Spring[扩展]基本使用篇","date":"2022-01-11T06:11:20.219Z","updated":"2022-01-11T06:20:10.278Z","comments":true,"path":"2022/01/11/Spring/Spring[扩展]基本使用篇/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/Spring/Spring[%E6%89%A9%E5%B1%95]%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%E7%AF%87/","excerpt":"","text":"一，Spring的概述1.Spring是什么？Spring 是分层的 Java SE/EE 应用 full-stack 轻量级开源框架，以 IoC（Inverse Of Control：反转控制）和 AOP（Aspect Oriented Programming：面向切面编程）为内核，提供了展现层 Spring MVC 和持久层 Spring JDBC 以及业务层事务管理等众多的企业级应用技术，还能整合开源世界众多著名的第三方框架和类库，逐渐成为使用最多的 Java EE 企业应用开源框架。​ 2.Spring的优势？ 方便解耦，简化开发 AOP编程的支持 声明式事务的支持 方便程序的测试 方便集成各种优秀框架 降低JavaEE API 的使用难度 源码是经典的学习范例 ​ 3.Spring的体系结构图 二，IOC1.程序的耦合和解耦耦合是影响软件复杂程度和设计质量的一个重要因素，在设计上我们应采用以下原则：如果模块间必须 存在耦合，就尽量使用数据耦合，少用控制耦合，限制公共耦合的范围，尽量避免使用内容耦合。​ 2.解决耦合的思路Class.forName(&quot;com.mysql.jdbc.Driver&quot;);//此处只是一个字符串，此时的好处是，我们的类中不再依赖具体的驱动类，此时就算删除 mysql 的驱动 jar 包，依然可以编译（运行就不要想了，没有驱动不可能运行成功的）。​ 同时，也产生了一个新的问题，mysql 驱动的全限定类名字符串是在 java 类中写死的，一旦要改还是要修改源码。解决这个问题也很简单，使用配置文件配置。​ 3.工厂模式解耦在实际开发中可以把三层的对象都使用配置文件配置起来，当启动服务器应用加载的时候，让一个类中的方法通过读取配置文件，把这些对象创建出来并存起来。在接下来的使用的时候，直接拿过来用就好了。那么，这个读取配置文件，创建和获取三层对象的类就是工厂。​ 4.代码案例4.1 AccountMapper123456789101112131415161718/** * @author 二十 * @since 2021/9/21 3:23 下午 */public class AccountMapper &#123; public void update(Account account) &#123; System.out.println(&quot;调用了update()...&quot;); &#125; public void add(Account account) &#123; System.out.println(&quot;调用了add()...&quot;); &#125; public void delete(Integer id) &#123; System.out.println(&quot;调用了delete()...&quot;); &#125;&#125; 4.2 AccountService123456789101112131415161718192021222324252627/** * @author 二十 * @since 2021/9/21 3:25 下午 */public class AccountService &#123; private Account account = (Account) BeanFactory.getBean(&quot;account&quot;); private AccountMapper accountDao = (AccountMapper) BeanFactory.getBean(&quot;accountMapper&quot;); int i = 1; public void update(Account account) &#123; accountDao.update(account); &#125; public void add(Account account) &#123; accountDao.add(account); System.out.println(i + 1); &#125; public void delete(Integer id) &#123; System.out.println(accountDao); accountDao.delete(id); System.out.println(account.toString()); System.out.println(i + 1); &#125;&#125; 4.3 BeanFactory1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * @author 二十 * @since 2021/9/21 3:27 下午 */public class BeanFactory &#123; private static Properties prop; private static Map&lt;String, Object&gt; beans; private static InputStream in; private final static String configFile = &quot;beans.properties&quot;; static &#123; try &#123; in = BeanFactory.class.getClassLoader().getResourceAsStream(configFile); prop = new Properties(); prop.load(in); beans = new HashMap&lt;&gt;(); Enumeration&lt;Object&gt; keys = prop.keys(); while (keys.hasMoreElements()) &#123; String key = keys.nextElement().toString(); String path = prop.getProperty(key); Object value = Class.forName(path).newInstance(); beans.put(key, value); &#125; &#125; catch (Exception e) &#123; &#125; finally &#123; try &#123; assert in != null; in.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; public static Object getBean(String name) &#123; assert name != null; return beans.get(name); &#125;&#125; 4.4 beans.properties123accountService=com.es.service.AccountServiceaccountMapper=com.es.mapper.AccountMapperaccount=com.es.domain.Account 4.5 单元测试123456@Testpublic void test()&#123;//AccountService service=new AccountServiceImpl();AccountService service = (AccountService) BeanFactory.getBean(&quot;accountService&quot;);service.delete(1);&#125; 5.控制反转 存哪去？ 分析：由于我们是很多对象，肯定要找个集合来存。这时候有 Map 和 List 供选择。到底选 Map 还是 List 就看我们有没有查找需求。有查找需求，选 Map。所以我们的答案就是在应用加载时，创建一个 Map，用于存放三层对象。我们把这个 map 称之为容器。 还是没解释什么是工厂？ 工厂就是负责给我们从容器中获取指定对象的类。这时候我们获取对象的方式发生了改变。原来：我们在获取对象时，都是采用 new 的方式。是主动的。现在：我们获取对象时，同时跟工厂要，有工厂为我们查找或者创建对象。是被动的。 这种被动接收的方式获取对象的思想就是控制反转，它是 spring 框架的核心之一。 明确 ioc 的作用： 削减计算机程序的耦合(解除我们代码中的依赖关系)。 三，使用IOC解决程序的耦合1.基于xml形式的装配1.1 步骤​ 创建maven工程，导入相关依赖 创建spring的配置文件，applicationContext.xml配置文件 让spring管理资源，在spring的配置文件中配置service和mapper ​ 1.2 代码12345678910/** * 测试基于xml形式的spring ioc获取对象 * */@Testpublic void test3()&#123; ApplicationContext ioc=new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); User user= (User) ioc.getBean(&quot;user&quot;);//在此处打断点验证对象是什么时候被创建的。 user.show();&#125; 12&lt;!-- 基于xml形式装配bean --&gt;&lt;bean id=&quot;user&quot; class=&quot;com.es.domain.User&quot;&gt;&lt;/bean&gt; 2.细节2.1 BeanFactory和ApplicationContext的区别BeanFactory 才是 Spring 容器中的顶层接口。​ ApplicationContext 是它的子接口。​ BeanFactory 和 ApplicationContext 的区别：创建对象的时间点不一样。 ApplicationContext：只要一读取配置文件，默认情况下就会创建对象。 BeanFactory：什么使用什么时候创建对象 ​ 2.2 ApplicationContext接口的实现类ClasspathXmlApplicationContext：从类的根路径下加载配置文件，推荐使用这种方式。​ FileSystemXmlApplicationContext：从磁盘路径上加载配置文件，可以指定在任意位置。​ AnnotationConfigApplicationContext：当使用注解配置容器或者对象的时候，需要使用此类来创建 spring 容器，用来读取注解。​ 2.3 bean标签作用：用于配置对象让 spring 来创建的。默认情况下它调用的是类中的无参构造函数。如果没有无参构造函数则不能创建成功。 属性： id：给对象在容器中提供一个唯一标识。用于获取对象。 class：指定类的全限定类名。用于反射创建对象。默认情况下调用无参构造函数。 scope：指定对象的作用范围。 singleton :默认值，单例的. prototype :多例的. request :WEB 项目中,Spring 创建一个 Bean 的对象,将对象存入到 request 域中. session :WEB 项目中,Spring 创建一个 Bean 的对象,将对象存入到 session 域中. global session :WEB 项目中,应用在 Portlet 环境.如果没有 Portlet 环境那么globalSession 相当于 session. init-method：指定类中的初始化方法名称。 destroy-method：指定类中销毁方法名称。 2.4 bean的生命周期和作用范围单例对象：scope=&quot;singleton&quot; 一个应用只有一个对象的实例。它的作用范围就是整个引用。​ 生命周期： 对象出生：当应用加载，创建容器时，对象就被创建了。 对象活着：只要容器在，对象一直活着。 对象死亡：当应用卸载，销毁容器时，对象就被销毁了。 ​ 多例对象：scope=&quot;prototype&quot; 每次访问对象时，都会重新创建对象实例。​ 生命周期： 对象出生：当使用对象时，创建新的对象实例。 对象活着：只要对象在使用中，就一直活着。 对象死亡：当对象长时间不用时，被 java 的垃圾回收器回收了。 2.5 实例化bean的三种方式2.5.1.使用无参构造器默认情况下：它会根据默认无参构造器来创建类对象。如果bean中没有默认无参构造器，将会创建失败。​ 1&lt;bean id=&quot;accountService&quot; class=&quot;com.itheima.service.impl.AccountServiceImpl&quot;/&gt; 2.5.2.静态工厂此种方式是使用StaticFactory类中的静态方法创建对象，并存入Spring容器。​ id属性：指定bean的id，用于从容器中获取。​ class属性：指定静态工厂的全限定类名。​ factory-method属性：指定生产对象的静态方法。​ 案例：​ 12345678/*** 模拟一个静态工厂，创建业务层实现类*/public class StaticFactory &#123; public static IAccountService createAccountService()&#123; return new AccountServiceImpl(); &#125; &#125; 1&lt;bean id=&quot;accountService&quot; class=&quot;com.itheima.factory.StaticFactory&quot; factory-method=&quot;createAccountService&quot;&gt;&lt;/bean&gt; 2.5.3 实例工厂此种方式先把工厂的创建交给spring容器来管理。然后再使用工厂的bean来调用里面的方法。​ factory-bean属性：用于指定实例工厂bean的id。​ factory-method属性：用于指定实例工厂中创建对象的方法。​ 案例：​ 123456789/*** 模拟一个实例工厂，创建业务层实现类* 此工厂创建对象，必须现有工厂实例对象，再调用方法*/public class InstanceFactory &#123; public IAccountService createAccountService()&#123; return new AccountServiceImpl(); &#125; &#125; 12&lt;bean id=&quot;instancFactory&quot; class=&quot;com.itheima.factory.InstanceFactory&quot;&gt;&lt;/bean&gt; &lt;bean id=&quot;accountService&quot;factory-bean=&quot;instancFactory&quot;factory- method=&quot;createAccountService&quot;&gt;&lt;/bean&gt; 3.依赖注入依赖注入（Dependency Injection）他是spring框架的核心，ioc的具体实现。​ 编写程序的时候，通过控制反转，把对象的创建交给spring容器，但是代码中不可能出现没有依赖的情况。​ ioc解耦只是降低他们的依赖关系，但是不会消除。比如我们的业务层仍然会调用持久层的方法。​ 那么这种业务层和持久层的依赖关系，在使用spring框架之后，就让spring来维护了。​ 简单地说，就是坐等框架把持久层的对象传入业务层，不需要开发人员手动去获取。​ 3.1 构造函数注入就是使用类中的构造函数给成员变量赋值。 注意：赋值的操作不是我们自己做的，而是通过配置的方式，让spring框架来为我们注入。 ​ 3.2 set()注入在类中提供需要注入的成员的set方法。​ 3.3 注入集合属性给类中的集合成员传递值，他用的也是set方法的注入方式，只不过变量的数据类型都是集合。​ 3.4 案例3.4.1 User123456789101112131415161718192021222324252627282930313233public class User &#123; private String name; private Integer age; private Date birth; public void setName(String name) &#123; this.name = name; &#125; public void setAge(Integer age) &#123; this.age = age; &#125; public void setBirth(Date birth) &#123; this.birth = birth; &#125; public User()&#123; System.out.println(&quot;我被创建了...&quot;); &#125; public void show()&#123; System.out.println(&quot;user中的show方法背调用了。。。&quot;); &#125; @Override public String toString() &#123; return &quot;User&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, age=&quot; + age + &quot;, birth=&quot; + birth + &#x27;&#125;&#x27;; &#125;&#125; 3.4.2 Person1234567891011121314151617181920public class Person &#123; private String name; private int age; public Person() &#123; &#125; public Person(String name, int age) &#123; this.name = name; this.age = age; &#125; @Override public String toString() &#123; return &quot;Person&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, age=&quot; + age + &#x27;&#125;&#x27;; &#125;&#125; 3.4.3 CollectionDemo1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class CollectionDemo &#123; private String [] arr; private List&lt;String&gt; myList; private Set&lt;String&gt; mySet; private Map&lt;String,String&gt; myMap; private Properties myProp; public void setArr(String[] arr) &#123; this.arr = arr; &#125; public void setMyList(List&lt;String&gt; myList) &#123; this.myList = myList; &#125; public void setMySet(Set&lt;String&gt; mySet) &#123; this.mySet = mySet; &#125; public void setMyMap(Map&lt;String, String&gt; myMap) &#123; this.myMap = myMap; &#125; public void setMyProp(Properties myProp) &#123; this.myProp = myProp; &#125; public String[] getArr() &#123; return arr; &#125; public List&lt;String&gt; getMyList() &#123; return myList; &#125; public Set&lt;String&gt; getMySet() &#123; return mySet; &#125; public Map&lt;String, String&gt; getMyMap() &#123; return myMap; &#125; public Properties getMyProp() &#123; return myProp; &#125;&#125; 3.4.4配置文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354&lt;!-- 基于xml形式装配bean --&gt;&lt;bean id=&quot;user&quot; class=&quot;com.es.java1.User&quot;&gt;&lt;/bean&gt;&lt;!--使用get方法创建bean--&gt;&lt;bean id=&quot;user2&quot; class=&quot;com.es.java1.User&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;张&quot;&gt;&lt;/property&gt; &lt;property name=&quot;age&quot;&gt; &lt;value&gt;20&lt;/value&gt; &lt;/property&gt; &lt;property name=&quot;birth&quot; ref=&quot;now&quot;&gt;&lt;/property&gt;&lt;/bean&gt;&lt;bean id=&quot;now&quot; class=&quot;java.util.Date&quot;&gt;&lt;/bean&gt;&lt;!--集合和数组类型的依赖注入--&gt;&lt;bean id=&quot;demo&quot; class=&quot;com.es.java1.CollectionDemo&quot;&gt; &lt;property name=&quot;arr&quot;&gt; &lt;array&gt; &lt;value&gt;111&lt;/value&gt; &lt;value&gt;222&lt;/value&gt; &lt;value&gt;333&lt;/value&gt; &lt;/array&gt; &lt;/property&gt; &lt;property name=&quot;myList&quot;&gt; &lt;list&gt; &lt;value&gt;111&lt;/value&gt; &lt;value&gt;222&lt;/value&gt; &lt;value&gt;333&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;property name=&quot;mySet&quot;&gt; &lt;set&gt; &lt;value&gt;111&lt;/value&gt; &lt;value&gt;222&lt;/value&gt; &lt;value&gt;333&lt;/value&gt; &lt;/set&gt; &lt;/property&gt; &lt;property name=&quot;myMap&quot;&gt; &lt;map&gt; &lt;entry key=&quot;aaa&quot; value=&quot;aaa&quot;&gt;&lt;/entry&gt; &lt;entry key=&quot;bbb&quot; value=&quot;bbb&quot;&gt;&lt;/entry&gt; &lt;entry key=&quot;ccc&quot; value=&quot;ccc&quot;&gt;&lt;/entry&gt; &lt;/map&gt; &lt;/property&gt; &lt;property name=&quot;myProp&quot;&gt; &lt;props&gt; &lt;prop key=&quot;aaa&quot;&gt;aaa&lt;/prop&gt; &lt;prop key=&quot;bbb&quot;&gt;bbb&lt;/prop&gt; &lt;prop key=&quot;ccc&quot;&gt;ccc&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt;&lt;/bean&gt;&lt;!--使用默认构造器创建bean--&gt;&lt;bean id=&quot;person&quot; class=&quot;com.es.java1.Person&quot;&gt; &lt;constructor-arg name=&quot;name&quot; value=&quot;张&quot;&gt;&lt;/constructor-arg&gt; &lt;constructor-arg name=&quot;age&quot; value=&quot;20&quot;&gt;&lt;/constructor-arg&gt;&lt;/bean&gt; 3.4.5 测试类12345678910111213141516171819202122232425262728293031323334353637383940414243/** * 测试基于xml形式的spring ioc获取对象 * */@Testpublic void test3()&#123; ApplicationContext ioc=new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); User user= (User) ioc.getBean(&quot;user&quot;);//在此处打断点验证对象是什么时候被创建的。 user.show();&#125;/** * 采用默认构造器的形式创建bean对象 */@Testpublic void test()&#123; ApplicationContext ioc=new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); Person p= (Person) ioc.getBean(&quot;person&quot;); Person p2= (Person) ioc.getBean(&quot;person&quot;); System.out.println(p.toString());&#125;/** * 使用get方法进行依赖注入 */@Testpublic void test4()&#123; ApplicationContext ioc=new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); User user= (User) ioc.getBean(&quot;user2&quot;);//在此处打断点验证对象是什么时候被创建的。 System.out.println(user.toString());&#125;/** * 集合和数组的依赖注入 */@Testpublic void test5()&#123; ApplicationContext ioc=new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); CollectionDemo demo= (CollectionDemo) ioc.getBean(&quot;demo&quot;); System.out.println(Arrays.toString(demo.getArr())); System.out.println(demo.getMyList()); System.out.println(demo.getMySet()); System.out.println(demo.getMyMap()); System.out.println(demo.getMyProp());&#125; 四，使用ioc实现账户CRUD1.基于xml形式1.1 引用外部属性文件12345678910&lt;!-- 引用外部属性文件 --&gt; &lt;context:property-placeholder location=&quot;classpath:druid.properties&quot;/&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot;&gt; &lt;property name=&quot;username&quot; value=&quot;$&#123;jdbc.username&#125;&quot;&gt;&lt;/property&gt; &lt;property name=&quot;url&quot; value=&quot;$&#123;jdbc.url&#125;&quot;&gt;&lt;/property&gt; &lt;property name=&quot;password&quot; value=&quot;$&#123;jdbc.password&#125;&quot;&gt;&lt;/property&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;$&#123;driverClassName&#125;&quot;&gt;&lt;/property&gt; &lt;property name=&quot;initialSize&quot; value=&quot;$&#123;initialSize&#125;&quot;&gt;&lt;/property&gt; &lt;property name=&quot;maxActive&quot; value=&quot;$&#123;maxActive&#125;&quot;&gt;&lt;/property&gt; &lt;/bean&gt; 1.2 SPEL表达式1.2.1 简介​ Spring Expression Language，Spring表达式语言，简称SpEL。支持运行时查询并可以操作对象图。​ 和JSP页面上的EL表达式、Struts2中用到的OGNL表达式一样，SpEL根据JavaBean风格的getXxx()、setXxx()方法定义的属性访问对象图，完全符合我们熟悉的操作习惯。​ 1.2.2 基本语法​ SpEL使用#{…}作为定界符，所有在大框号中的字符都将被认为是SpEL表达式。​ 1.2.3 使用字面量●整数：&lt;property name=&quot;count&quot; value=&quot;#&#123;5&#125;&quot;/&gt; ●小数：&lt;property name=&quot;frequency&quot; value=&quot;#&#123;89.7&#125;&quot;/&gt; ●科学计数法：&lt;property name=&quot;capacity&quot; value=&quot;#&#123;1e4&#125;&quot;/&gt; ●String类型的字面量可以使用单引号或者双引号作为字符串的定界符号 &lt;property name=”name” value=&quot;#&#123;&#39;Chuck&#39;&#125;&quot;/&gt; &lt;property name=&#39;name&#39; value=&#39;#&#123;&quot;Chuck&quot;&#125;&#39;/&gt; ●Boolean：&lt;property name=&quot;enabled&quot; value=&quot;#&#123;false&#125;&quot;/&gt; 1.2.4 引用其他bean123456&lt;bean id=&quot;emp04&quot; class=&quot;com.es.parent.bean.Employee&quot;&gt; &lt;property name=&quot;empId&quot; value=&quot;1003&quot;/&gt; &lt;property name=&quot;empName&quot; value=&quot;Kate&quot;/&gt; &lt;property name=&quot;age&quot; value=&quot;21&quot;/&gt; &lt;property name=&quot;detp&quot; value=&quot;#&#123;dept&#125;&quot;/&gt;&lt;/bean&gt; 1.2.5 引用其他bean的属性值作为自己某个属性的值123456&lt;bean id=&quot;emp05&quot; class=&quot;com.es.parent.bean.Employee&quot;&gt; &lt;property name=&quot;empId&quot; value=&quot;1003&quot;/&gt; &lt;property name=&quot;empName&quot; value=&quot;Kate&quot;/&gt; &lt;property name=&quot;age&quot; value=&quot;21&quot;/&gt; &lt;property name=&quot;deptName&quot; value=&quot;#&#123;dept.deptName&#125;&quot;/&gt;&lt;/bean&gt; 1.2.6调用非静态方法1234567&lt;!-- 创建一个对象，在SpEL表达式中调用这个对象的方法 --&gt;&lt;bean id=&quot;salaryGenerator&quot; class=&quot;com.es.spel.bean.SalaryGenerator&quot;/&gt;&lt;bean id=&quot;employee&quot; class=&quot;com.es.spel.bean.Employee&quot;&gt; &lt;!-- 通过对象方法的返回值为属性赋值 --&gt; &lt;property name=&quot;salayOfYear&quot; value=&quot;#&#123;salaryGenerator.getSalaryOfYear(5000)&#125;&quot;/&gt;&lt;/bean&gt; 1.2.7调用静态方法1234&lt;bean id=&quot;employee&quot; class=&quot;com.es.spel.bean.Employee&quot;&gt; &lt;!-- 在SpEL表达式中调用类的静态方法 --&gt; &lt;property name=&quot;circle&quot; value=&quot;#&#123;T(java.lang.Math).PI*20&#125;&quot;/&gt;&lt;/bean&gt; 1.2.8 运算符①算术运算符：+、-、*、/、%、^②字符串连接：+③比较运算符：&lt;、&gt;、==、&lt;=、&gt;=、lt、gt、eq、le、ge④逻辑运算符：and, or, not, |⑤三目运算符：判断条件?判断结果为true时的取值:判断结果为false时的取值⑥正则表达式：matches​ 1.3 Spring的#和$的区别$&#123;key名称&#125;：​ 用户获取外部文件中指定key的值； ​ 可以出现在xml配置文件中，也可以出现在注解@Value中； ​ 一般用户获取数据库配置文件的内容信息等。 #&#123;表达式&#125;：​ SpEL表达式的格式，详情(https://blog.csdn.net/xingfei_work/article/details/76058178))； ​ 可以出现在xml配置文件中，也可以出现在注解@Value中 ​ 可以任意表达式，支持运算符等。 ​ 1.4 案例1.4.1 配置文件12345678910111213141516171819202122232425262728293031&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:jdbc=&quot;http://www.springframework.org/schema/jdbc&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/jdbc http://www.springframework.org/schema/jdbc/spring-jdbc-4.2.xsd http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.2.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-4.2.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-4.2.xsd&quot;&gt; &lt;!--设置自动扫描的包--&gt; &lt;context:component-scan base-package=&quot;com.es&quot;&gt;&lt;/context:component-scan&gt; &lt;bean id=&quot;accountDao&quot; class=&quot;com.es.dao.impl.AccountDaoImpl&quot;&gt; &lt;property name=&quot;runner&quot; ref=&quot;runner&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;bean id=&quot;accountService&quot; class=&quot;com.es.service.impl.AccountServiceImpl&quot;&gt; &lt;property name=&quot;accountDao&quot; ref=&quot;accountDao&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;bean id=&quot;account&quot; class=&quot;com.es.domain.Account&quot;&gt;&lt;/bean&gt; &lt;bean id=&quot;runner&quot; class=&quot;org.apache.commons.dbutils.QueryRunner&quot; scope=&quot;prototype&quot;&gt; &lt;constructor-arg name=&quot;ds&quot; ref=&quot;dataSource&quot;&gt;&lt;/constructor-arg&gt; &lt;/bean&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot;&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:3306/eesy&quot;&gt;&lt;/property&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot;&gt;&lt;/property&gt; &lt;property name=&quot;password&quot; value=&quot;yhd666&quot;&gt;&lt;/property&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;com.mysql.jdbc.Driver&quot;&gt;&lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; 1.4.2 持久层12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970package com.es.dao.impl;import com.es.dao.IAccountDao;import com.es.domain.Account;import org.apache.commons.dbutils.QueryRunner;import org.apache.commons.dbutils.handlers.BeanHandler;import org.apache.commons.dbutils.handlers.BeanListHandler;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Repository;import java.util.List;/** * 账户的持久层实现类 */public class AccountDaoImpl implements IAccountDao &#123; private QueryRunner runner; public void setRunner(QueryRunner runner) &#123; this.runner = runner; &#125; public List&lt;Account&gt; findAllAccount() &#123; try&#123; return runner.query(&quot;select * from account&quot;,new BeanListHandler&lt;Account&gt;(Account.class)); &#125;catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; public Account findAccountById(Integer accountId) &#123; try&#123; return runner.query(&quot;select * from account where id = ? &quot;,new BeanHandler&lt;Account&gt;(Account.class),accountId); &#125;catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; public void saveAccount(Account account) &#123; try&#123; runner.update(&quot;insert into account(name,money)values(?,?)&quot;,account.getName(),account.getMoney()); &#125;catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; public void updateAccount(Account account) &#123; try&#123; runner.update(&quot;update account set name=?,money=? where id=?&quot;,account.getName(),account.getMoney(),account.getId()); &#125;catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; public void deleteAccount(Integer accountId) &#123; try&#123; runner.update(&quot;delete from account where id=?&quot;,accountId); &#125;catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125;&#125; 1.4.3 业务层12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package com.es.service.impl;import com.es.dao.IAccountDao;import com.es.domain.Account;import com.es.domain.Account;import com.es.service.IAccountService;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import java.util.List;/** * 账户的业务层实现类 */public class AccountServiceImpl implements IAccountService&#123; private IAccountDao accountDao; public void setAccountDao(IAccountDao accountDao) &#123; this.accountDao = accountDao; &#125; public List&lt;Account&gt; findAllAccount() &#123; return accountDao.findAllAccount(); &#125; public Account findAccountById(Integer accountId) &#123; return accountDao.findAccountById(accountId); &#125; public void saveAccount(Account account) &#123; accountDao.saveAccount(account); &#125; public void updateAccount(Account account) &#123; accountDao.updateAccount(account); &#125; public void deleteAccount(Integer acccountId) &#123; accountDao.deleteAccount(acccountId); &#125;&#125; 1.4.4 测试类12345678public class Test1 &#123; ApplicationContext ioc=new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); @Test public void test1()&#123; IAccountService service= (IAccountService) ioc.getBean(&quot;accountService&quot;); service.deleteAccount(2); &#125;&#125; 2.xml和注解混搭2.1 用于创建对象他们的作用就和在XML配置文件中编写一个标签实现的功能是一样的。 ​Component: 作用：用于把当前类对象存入spring容器中 属性： value：用于指定bean的id。当我们不写时，它的默认值是当前类名，且首字母改小写。 ​Controller：一般用在表现层 ​Service：一般用在业务层 ​Repository：一般用在持久层 ​ 以上个注解他们的作用和属性与Component是一模一样。​ 他们是spring框架为我们提供明确的层使用的注解，使我们的层对象更加清晰。​ 2.2 用于注入数据他们的作用就和在xml配置文件中的bean标签中写一个标签的作用是一样的。 Autowired: 作用：自动照类型注入。只要容器中唯一的一个bean对象类型和要注入的变量类型匹配，就可以注入成功 如果ioc容器中没任何bean的类型和要注入的变量类型匹配，则报错。 如果Ioc容器中多个类型匹配时： 出现位置： 可以是变量上，也可以是方法上 细节： 在使用注解注入时，set方法就不是必须的了。 ​Qualifier: 作用：在照类型注入的基础之上再照名称注入。它在给类成员注入时不能单独使用。但是在给方法参数注入时可以 属性： value：用于指定注入bean的id。 ​Resource 作用：直接照bean的id注入。它可以独立使用 属性： name：用于指定bean的id。以上个注入都只能注入其他bean类型的数据，而基本类型和String类型无法使用上述注解实现。另外，集合类型的注入只能通过XML来实现。​ ​Value 作用：用于注入基本类型和String类型的数据 属性： value：用于指定数据的值。它可以使用spring中SpEL(也就是spring的el表达式，SpEL的写法：${表达式}​ 2.3.用于改变作用范围他们的作用就和在bean标签中使用scope属性实现的功能是一样的。 ​Scope 作用：用于指定bean的作用范围 属性： value：指定范围的取值。常用取值：singleton prototype​ 2.4 和生命周期相关他们的作用就和在bean标签中使用init-method和destroy-methode的作用是一样的。​ ​PreDestroy 作用：用于指定销毁方法 ​PostConstruct 作用：用于指定初始化方法​ 2.5 案例2.5.1 配置文件123456789101112131415161718192021222324&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:jdbc=&quot;http://www.springframework.org/schema/jdbc&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/jdbc http://www.springframework.org/schema/jdbc/spring-jdbc-4.2.xsd http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.2.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-4.2.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-4.2.xsd&quot;&gt; &lt;!--设置自动扫描的包--&gt; &lt;context:component-scan base-package=&quot;com.es&quot;&gt;&lt;/context:component-scan&gt; &lt;bean id=&quot;runner&quot; class=&quot;org.apache.commons.dbutils.QueryRunner&quot; scope=&quot;prototype&quot;&gt; &lt;constructor-arg name=&quot;ds&quot; ref=&quot;dataSource&quot;&gt;&lt;/constructor-arg&gt; &lt;/bean&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot;&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:3306/eesy&quot;&gt;&lt;/property&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot;&gt;&lt;/property&gt; &lt;property name=&quot;password&quot; value=&quot;yhd666&quot;&gt;&lt;/property&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;com.mysql.jdbc.Driver&quot;&gt;&lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; 2.5.2 持久层1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768package com.es.dao.impl;import com.es.dao.IAccountDao;import com.es.domain.Account;import org.apache.commons.dbutils.QueryRunner;import org.apache.commons.dbutils.handlers.BeanHandler;import org.apache.commons.dbutils.handlers.BeanListHandler;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Repository;import java.util.List;/** * 账户的持久层实现类 */@Repository(value = &quot;accountDao&quot;)public class AccountDaoImpl implements IAccountDao &#123; @Autowired private QueryRunner runner; public List&lt;Account&gt; findAllAccount() &#123; try&#123; return runner.query(&quot;select * from account&quot;,new BeanListHandler&lt;Account&gt;(Account.class)); &#125;catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; public Account findAccountById(Integer accountId) &#123; try&#123; return runner.query(&quot;select * from account where id = ? &quot;,new BeanHandler&lt;Account&gt;(Account.class),accountId); &#125;catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; public void saveAccount(Account account) &#123; try&#123; runner.update(&quot;insert into account(name,money)values(?,?)&quot;,account.getName(),account.getMoney()); &#125;catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; public void updateAccount(Account account) &#123; try&#123; runner.update(&quot;update account set name=?,money=? where id=?&quot;,account.getName(),account.getMoney(),account.getId()); &#125;catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; public void deleteAccount(Integer accountId) &#123; try&#123; runner.update(&quot;delete from account where id=?&quot;,accountId); &#125;catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125;&#125; 2.5.3 业务层1234567891011121314151617181920212223242526272829303132333435363738394041424344package com.es.service.impl;import com.es.dao.IAccountDao;import com.es.domain.Account;import com.es.domain.Account;import com.es.service.IAccountService;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import java.util.List;/** * 账户的业务层实现类 */@Service(&quot;accountService&quot;)public class AccountServiceImpl implements IAccountService&#123; @Autowired private IAccountDao accountDao; public List&lt;Account&gt; findAllAccount() &#123; return accountDao.findAllAccount(); &#125; public Account findAccountById(Integer accountId) &#123; return accountDao.findAccountById(accountId); &#125; public void saveAccount(Account account) &#123; accountDao.saveAccount(account); &#125; public void updateAccount(Account account) &#123; accountDao.updateAccount(account); &#125; public void deleteAccount(Integer acccountId) &#123; accountDao.deleteAccount(acccountId); &#125;&#125; 2.5.4 测试类1234567891011121314151617import com.es.service.IAccountService;import org.junit.Test;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;/** * @author yinhuidong * @createTime 2020-03-01-11:03 */public class Test1 &#123; ApplicationContext ioc=new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); @Test public void test1()&#123; IAccountService service= (IAccountService) ioc.getBean(&quot;accountService&quot;); service.deleteAccount(2); &#125;&#125; 3.纯注解配置3.1 注解 ​Configuration 作用：指定当前类是一个配置类细节：当配置类作为AnnotationConfigApplicationContext对象创建的参数时，该注解可以不写。 ​ComponentScan 作用：用于通过注解指定spring在创建容器时要扫描的包属性： value：它和basePackages的作用是一样的，都是用于指定创建容器时要扫描的包。 我们使用此注解就等同于在xml中配置了: &lt;context:component-scan base-package=&quot;com.itheima&quot;&gt;&lt;/context:component-scan&gt; ​Bean 作用：用于把当前方法的返回值作为bean对象存入spring的ioc容器中属性: name:用于指定bean的id。当不写时，默认值是当前方法的名称细节： 当我们使用注解配置方法时，如果有方法参数，spring框架会去容器中查找没可用的bean对象。 查找的方式和Autowired注解的作用是一样的 ​Import 作用：用于导入其他的配置类 属性： value：用于指定其他配置类的字节码。 当我们使用Import的注解之后，Import注解的类就父配置类，而导入的都是子配置类 ​PropertySource 作用：用于指定properties文件的位置 属性： value：指定文件的名称和路径。 关键字：classpath，表示类路径下 ​ 3.2 spring整合junit4123456789101112131415161718192021222324252627/**1、应用程序的入口 main方法2、junit单元测试中，没有main方法也能执行 junit集成了一个main方法 该方法就会判断当前测试类中哪些方法有 @Test注解 junit就让有Test注解的方法执行3、junit不会管我们是否采用spring框架 在执行测试方法时，junit根本不知道我们是不是使用了spring框架 所以也就不会为我们读取配置文件/配置类创建spring核心容器4、由以上三点可知 当测试方法执行时，没有Ioc容器，就算写了Autowired注解，也无法实现注入------------------------------------------------------------------------- * 使用Junit单元测试：测试我们的配置 * Spring整合junit的配置 * 1、导入spring整合junit的jar(坐标) * 2、使用Junit提供的一个注解把原有的main方法替换了，替换成spring提供的 * @Runwith(SpringJUnit4ClassRunner.class) * 3、告知spring的运行器，spring和ioc创建是基于xml还是注解的，并且说明位置 * @ContextConfiguration * locations：指定xml文件的位置，加上classpath关键字，表示在类路径下 * classes：指定注解类所在地位置 * * 当我们使用spring 5.x版本的时候，要求junit的jar必须是4.12及以上 */ 3.3 案例3.3.1 配置类1234567891011/** * @author yinhuidong * @createTime 2020-03-01-16:43 */@Configuration@ComponentScan(&quot;com.es&quot;)@Import(JdbcConfig.class)@PropertySource(&quot;classpath:c3p0.properties&quot;)public class SpringConfig &#123;&#125; 3.3.2 配置子类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/** * @author yinhuidong * @createTime 2020-03-01-16:56 */public class JdbcConfig &#123; @Bean(name=&quot;runner&quot;) @Scope(value = &quot;prototype&quot;) public QueryRunner getRunner(@Qualifier(&quot;ds1&quot;) DataSource dataSource) &#123; QueryRunner runner = new QueryRunner(dataSource); return runner; &#125; private static DataSource dataSource = null; @Bean(name=&quot;ds1&quot;) public DataSource getDataSource() &#123; try &#123; Properties prop = new Properties(); InputStream is = JdbcConfig.class.getClassLoader().getResourceAsStream(&quot;jdbc.properties&quot;); prop.load(is); dataSource = DruidDataSourceFactory.createDataSource(prop); return dataSource; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return null; &#125; @Value(&quot;$&#123;jdbc.driver&#125;&quot;) private String driver; @Value(&quot;$&#123;jdbc.url&#125;&quot;) private String url; @Value(&quot;$&#123;jdbc.username&#125;&quot;) private String username; @Value(&quot;$&#123;jdbc.password&#125;&quot;) private String password; @Bean(name=&quot;ds2&quot;) public DataSource getDataSource2()&#123; try &#123; ComboPooledDataSource dataSource=new ComboPooledDataSource(); dataSource.setDriverClass(driver); dataSource.setJdbcUrl(url); dataSource.setUser(username); dataSource.setPassword(password); return dataSource; &#125; catch (PropertyVetoException e) &#123; e.printStackTrace(); &#125; return null; &#125;&#125; 3.3.3 测试类1234567891011@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(classes = com.es.java1.SpringConfig.class)public class Test1 &#123; @Autowired private IAccountService service = null; @Test public void test1() &#123; service.deleteAccount(5); &#125;&#125; 五，AOP面向切面编程1.动态代理1.1 基于接口的动态代理1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public class Test1 &#123; @Test public void test1()&#123; //被代理类对象要声明为最终的 final Producer producer=new Producer(); /** * 动态代理： * 特点：字节码随用随创建，随用随加载 * 作用：不修改源码的基础上对方法增强 * 分类： * 基于接口的动态代理 * 基于子类的动态代理 * 基于接口的动态代理： * 涉及的类：Proxy * 提供者：JDK官方 * 如何创建代理对象： * 使用Proxy类中的newProxyInstance方法 * 创建代理对象的要求： * 被代理类最少实现一个接口，如果没有则不能使用 * newProxyInstance方法的参数： * ClassLoader：类加载器 * 它是用于加载代理对象字节码的。和被代理对象使用相同的类加载器。固定写法。 * Class[]：字节码数组 * 它是用于让代理对象和被代理对象相同方法。固定写法。 * InvocationHandler：用于提供增强的代码 * 它是让我们写如何代理。我们一般都是些一个该接口的实现类，通常情况下都是匿名内部类，但不是必须的。 * 此接口的实现类都是谁用谁写。 */ //代理对象和被代理类对象要实现同一个接口 IProducer proxyProducer = (IProducer) Proxy.newProxyInstance(producer.getClass().getClassLoader(), producer.getClass().getInterfaces(), new InvocationHandler() &#123; /** * 作用：执行被代理对象的任何接口方法都会经过该方法 * 方法参数的含义 * @param proxy 代理对象的引用 * 1. 可以使用反射获取代理对象的信息（也就是proxy.getClass().getName()。 * 2. 可以将代理对象返回以进行连续调用，这就是proxy存在的目的，因为this并不是代理对象。 * @param method 当前执行的方法 * @param args 当前执行方法所需的参数 * @return 和被代理对象方法相同的返回值 * @throws Throwable */ public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; Object value=null; //获取方法执行的参数 //判断当前方法是不是销售 if (&quot;saleProduct&quot;.equals(method.getName()))&#123; Float money= (Float) args[0]; //两个参数：被代理类对象，方法增强的参数 value=method.invoke(producer,money*0.8f); &#125; return value; &#125; &#125;); proxyProducer.saleProduct(10000f); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243package com.es.java1;/** * 一个生产者 */public class Producer implements IProducer&#123; /** * 销售 * @param money */ public void saleProduct(float money)&#123; System.out.println(&quot;销售产品，并拿到钱：&quot;+money); &#125; /** * 售后 * @param money */ public void afterService(float money)&#123; System.out.println(&quot;提供售后服务，并拿到钱：&quot;+money); &#125;&#125;-------------------------------------------------------------------/** * 对生产厂家要求的接口 */public interface IProducer &#123; /** * 销售 * @param money */ public void saleProduct(float money); /** * 售后 * @param money */ public void afterService(float money);&#125; 1.2 基于子类的动态代理123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657/** * @author yinhuidong * @createTime 2020-03-02-1:08 */public class Test4 &#123; @Test public void test() &#123; final Producer producer = new Producer(); /** * 动态代理： * 特点：字节码随用随创建，随用随加载 * 作用：不修改源码的基础上对方法增强 * 分类： * 基于接口的动态代理 * 基于子类的动态代理 * 基于子类的动态代理： * 涉及的类：Enhancer * 提供者：第方cglib库 * 如何创建代理对象： * 使用Enhancer类中的create方法 * 创建代理对象的要求： * 被代理类是最终类 * create方法的参数： * Class：字节码 * 它是用于指定被代理对象的字节码。 * * Callback：用于提供增强的代码 * 它是让我们写如何代理。我们一般都是些一个该接口的实现类，通常情况下都是匿名内部类，但不是必须的。 * 此接口的实现类都是谁用谁写。 * 我们一般写的都是该接口的子接口实现类：MethodInterceptor */ Producer cglibProducer = (Producer) Enhancer.create(producer.getClass(), new MethodInterceptor() &#123; /** * 执行被代理对象的任何方法都会经过该方法 * @param proxy * @param method * @param args * 以上个参数和基于接口的动态代理中invoke方法的参数是一样的 * @param methodProxy ：当前执行方法的代理对象 * @return * @throws Throwable */ public Object intercept(Object proxy, Method method, Object[] args, MethodProxy methodProxy) throws Throwable &#123; //提供增强的代码 Object returnValue = null; //1.获取方法执行的参数 Float money = (Float) args[0]; //2.判断当前方法是不是销售 if (&quot;saleProduct&quot;.equals(method.getName())) &#123; returnValue = method.invoke(producer, money * 0.8f); &#125; return returnValue; &#125; &#125;); cglibProducer.saleProduct(12000f); &#125;&#125; 1234567891011121314151617181920212223package com.es.java2;/** * 一个生产者 */public class Producer &#123; /** * 销售 * @param money */ public void saleProduct(float money)&#123; System.out.println(&quot;销售产品，并拿到钱：&quot;+money); &#125; /** * 售后 * @param money */ public void afterService(float money)&#123; System.out.println(&quot;提供售后服务，并拿到钱：&quot;+money); &#125;&#125; 1.3 使用动态代理对spring进行方法增强接口和实现类 12345678910111213141516171819202122public interface MyInterface &#123; public int add(int a,int b); public int del(int a,int b); public int che(int a,int b); public int div(int a,int b);&#125;----------------------------------------------------------public class java1 implements MyInterface&#123; public int add(int a,int b)&#123; return a+b; &#125; public int del(int a,int b)&#123; return a-b; &#125; public int che(int a,int b)&#123; return a*b; &#125; public int div(int a,int b)&#123; return a/b; &#125;&#125; BeanFactory 123456789101112131415161718192021222324public class BeanFactory &#123; private java1 java; public void setJava(java1 java) &#123; this.java = java; &#125; public MyInterface getBean()&#123; MyInterface proxyJava = (MyInterface) Proxy.newProxyInstance( java.getClass().getClassLoader(), java.getClass().getInterfaces(), new InvocationHandler() &#123; public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; Object value = null; System.out.println(&quot;方法执行前....&quot;); value = method.invoke(java, args); System.out.println(&quot;方法执行之后....&quot;); return value; &#125; &#125; ); return proxyJava; &#125;&#125; 测试类 1234567891011121314151617181920212223@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(&quot;classpath:applicationContext.xml&quot;)public class Test1 &#123; @Autowired @Qualifier(&quot;proxyJava&quot;) private MyInterface myInterface; @Test public void test1()&#123; System.out.println(myInterface.add(1, 2)); &#125; @Test public void test2()&#123; System.out.println(myInterface.del(1, 2)); &#125; @Test public void test3()&#123; System.out.println(myInterface.che(1, 2)); &#125; @Test public void test4()&#123; System.out.println(myInterface.div(1, 2)); &#125;&#125; 配置文件 1234567&lt;bean id=&quot;factory&quot; class=&quot;com.es.factory.BeanFactory&quot;&gt; &lt;property name=&quot;java&quot; ref=&quot;java&quot;&gt;&lt;/property&gt;&lt;/bean&gt;&lt;bean id=&quot;java&quot; class=&quot;com.es.java1.java1&quot;&gt;&lt;/bean&gt;&lt;bean id=&quot;proxyJava&quot; factory-bean=&quot;factory&quot; factory-method=&quot;getBean&quot;&gt;&lt;/bean&gt; 2.AOP2.1 AOP相关概念AOP：全称是 Aspect Oriented Programming 即：面向切面编程。就是把我们程序重复的代码抽取出来，在需要执行的时候，使用动态代理的技术，在不修改源码的基础上，对我们的已有方法进行增强。 作用： 在程序运行期间，不修改源码对已有方法进行增强。 优势： 减少重复代码 提高开发效率 维护方便 AOP 相关术语 Joinpoint(连接点): 所谓连接点是指那些被拦截到的点。在 spring 中,这些点指的是方法,因为 spring 只支持方法类型的连接点。 Pointcut(切入点): 所谓切入点是指我们要对哪些 Joinpoint 进行拦截的定义。 Advice(通知/增强): 所谓通知是指拦截到 Joinpoint 之后所要做的事情就是通知。 通知的类型：前置通知,后置通知,异常通知,最终通知,环绕通知。 Introduction(引介): 引介是一种特殊的通知在不修改类代码的前提下, Introduction 可以在运行期为类动态地添加一些方法或 Field。 Target(目标对象):被代理对象 代理的目标对象。 Weaving(织入): 是指把增强应用到目标对象来创建新的代理对象的过程。 spring 采用动态代理织入，而 AspectJ 采用编译期织入和类装载期织入。 Proxy（代理: 一个类被 AOP 织入增强后，就产生一个结果代理类。 Aspect(切面): 是切入点和通知（引介的结合 ​ Spring 框架监控切入点方法的执行。一旦监控到切入点方法被运行，使用代理机制，动态创建目标对象的代理对象，根据通知类别，在代理对象的对应位置，将通知对应的功能织入，完成完整的代码逻辑运行。​ 2.2 基于xml形式的配置2.2.1 配置步骤12345678910111213141516171819202122232425262728293031323334353637383940&lt;!--spring中基于XML的AOP配置步骤 1、把通知Bean也交给spring来管理 2、使用aop:config标签表明开始AOP的配置 3、使用aop:aspect标签表明配置切面 id属性：是给切面提供一个唯一标识 ref属性：是指定通知类bean的Id。 4、在aop:aspect标签的内部使用对应标签来配置通知的类型 我们现在示例是让printLog方法在切入点方法执行之前之前：所以是前置通知 aop:before：表示配置前置通知 method属性：用于指定Logger类中哪个方法是前置通知 pointcut属性：用于指定切入点表达式，该表达式的含义指的是对业务层中哪些方法增强 切入点表达式的写法： 关键字：execution(表达式) 表达式： 访问修饰符 返回值 包名.包名.包名...类名.方法名(参数列表) 标准的表达式写法： public void com.itheima.service.impl.AccountServiceImpl.saveAccount() 访问修饰符可以省略 void com.itheima.service.impl.AccountServiceImpl.saveAccount() 返回值可以使用通配符，表示任意返回值 * com.itheima.service.impl.AccountServiceImpl.saveAccount() 包名可以使用通配符，表示任意包。但是几级包，就需要写几个*. * *.*.*.*.AccountServiceImpl.saveAccount()) 包名可以使用..表示当前包及其子包 * *..AccountServiceImpl.saveAccount() 类名和方法名都可以使用*来实现通配 * *..*.*() 参数列表： 可以直接写数据类型： 基本类型直接写名称 int 引用类型写包名.类名的方式 java.lang.String 可以使用通配符表示任意类型，但是必须参数 可以使用..表示无参数均可，参数可以是任意类型 全通配写法： * *..*.*(..) 实际开发中切入点表达式的通常写法： 切到业务层实现类下的所方法 * com.itheima.service.impl.*.*(..)--&gt; 配置文件 1234567891011121314151617181920212223242526&lt;!--配置bean--&gt;&lt;bean id=&quot;java&quot; class=&quot;com.es.java1.java1&quot;&gt;&lt;/bean&gt;&lt;bean id=&quot;logging&quot; class=&quot;com.es.java2.Logging&quot;&gt;&lt;/bean&gt;&lt;!-- 配置aop --&gt;&lt;aop:config&gt; &lt;!-- 配置切入点表达式 id属性用于指定表达式的唯一标识。expression属性用于指定表达式内容 此标签写在aop:aspect标签内部只能当前切面使用。 它还可以写在aop:aspect外面，此时就变成了所切面可用 --&gt; &lt;aop:pointcut id=&quot;a&quot; expression=&quot;execution(public int com.es.java1.java1.*(int,int))&quot;&gt;&lt;/aop:pointcut&gt; &lt;!--配置切面--&gt; &lt;aop:aspect id=&quot;log&quot; ref=&quot;logging&quot;&gt; &lt;!-- 配置通知的类型，并且建立通知方法和切入点方法的关联--&gt; &lt;!-- 配置前置通知：在切入点方法执行之前执行 --&gt; &lt;aop:before method=&quot;before&quot; pointcut=&quot;execution(public int com.es.java1.java1.*(int,int))&quot;&gt;&lt;/aop:before&gt; &lt;!-- 配置后置通知：在切入点方法正常执行之后值。它和异常通知永远只能执行一个--&gt; &lt;aop:after-returning method=&quot;afterReturn&quot; pointcut-ref=&quot;a&quot;&gt;&lt;/aop:after-returning&gt; &lt;!-- 配置异常通知：在切入点方法执行产生异常之后执行。它和后置通知永远只能执行一个--&gt; &lt;aop:after-throwing method=&quot;afterThrowing&quot; pointcut-ref=&quot;a&quot;&gt;&lt;/aop:after-throwing&gt; &lt;!-- 配置最终通知：无论切入点方法是否正常执行它都会在其后面执行 --&gt; &lt;aop:after method=&quot;after&quot; pointcut-ref=&quot;a&quot;&gt;&lt;/aop:after&gt; &lt;!-- 配置环绕通知 详细的注释 :Logger类中--&gt; &lt;aop:around method=&quot;around&quot; pointcut-ref=&quot;a&quot;&gt;&lt;/aop:around&gt; &lt;/aop:aspect&gt;&lt;/aop:config&gt; 实现类 1234567891011121314public class java1 implements MyInterface&#123; public int add(int a,int b)&#123; return a+b; &#125; public int del(int a,int b)&#123; return a-b; &#125; public int che(int a,int b)&#123; return a*b; &#125; public int div(int a,int b)&#123; return a/b; &#125;&#125; 测试类 1234567891011@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(locations = &quot;classpath:applicationContext.xml&quot;)public class Test1 &#123; @Autowired @Qualifier(&quot;java&quot;) private MyInterface java; @Test public void test1()&#123; java.div(1, 1); &#125;&#125; 日志类 123456789101112131415161718192021222324252627282930313233343536373839404142434445public class Logging &#123; public void before()&#123; System.out.println(&quot;★★★前置★★★&quot;); &#125; public void afterReturn()&#123; System.out.println(&quot;★★★后置★★★&quot;); &#125; public void afterThrowing()&#123; System.out.println(&quot;★★★异常★★★&quot;); &#125; public void after()&#123; System.out.println(&quot;★★★最终★★★&quot;); &#125; /** * 环绕通知 * 问题： * 当我们配置了环绕通知之后，切入点方法没执行，而通知方法执行了。 * 分析： * 通过对比动态代理中的环绕通知代码，发现动态代理的环绕通知明确的切入点方法调用，而我们的代码中没。 * 解决： * Spring框架为我们提供了一个接口：ProceedingJoinPoint。该接口一个方法proceed()，此方法就相当于明确调用切入点方法。 * 该接口可以作为环绕通知的方法参数，在程序执行时，spring框架会为我们提供该接口的实现类供我们使用。 * * spring中的环绕通知： * 它是spring框架为我们提供的一种可以在代码中手动控制增强方法何时执行的方式。 */ public Object around(ProceedingJoinPoint pjp)&#123; Object value=null; //获取方法执行的参数 Object []args=pjp.getArgs(); //获取方法名 String methodName = pjp.getSignature().getName(); try &#123; System.out.println(&quot;★★★前置★★★&quot;+methodName+&quot;...&quot;+Arrays.toString(args)); value=pjp.proceed(args); System.out.println(&quot;★★★后置★★★&quot;+methodName+&quot;...&quot;+Arrays.toString(args)+&quot;...&quot;+value); return value; &#125; catch (Throwable throwable) &#123; System.out.println(&quot;★★★异常★★★&quot;+methodName+&quot;...&quot;+Arrays.toString(args)); throw new RuntimeException(throwable); &#125;finally &#123; System.out.println(&quot;★★★最终★★★&quot;+methodName+&quot;...&quot;+Arrays.toString(args)+&quot;...&quot;+value); &#125; &#125;&#125; 2.3 基于注解的配置2.3.1 步骤 首先在配置文件里开启声明式aop注解支持 12&lt;!--开启声明式事务注解--&gt;&lt;aop:aspectj-autoproxy&gt;&lt;/aop:aspectj-autoproxy&gt; 在logging类上声明其为一个切面类 1@Aspect//声明一个切面 在类中声明一个方法作为切入点表达式 1234@Pointcut(&quot;execution(public * com.es.java1.java1.*(..))&quot;)public void pointcut()&#123;&#125; 在各个方法上添加注解 设置切面优先级 1@Order(2)//通过@Order(2)注解指定切面优先级，value值越小，优先级越高，默认是int最大值。 2.3.2 案例 配置文件 1234 &lt;!--开启声明式事务注解--&gt;&lt;aop:aspectj-autoproxy&gt;&lt;/aop:aspectj-autoproxy&gt; &lt;!--设置自动扫描的包--&gt;&lt;context:component-scan base-package=&quot;com.es&quot;&gt;&lt;/context:component-scan&gt; 日志类 1234567891011121314151617181920212223242526272829303132333435363738394041package com.es.utils;import org.aspectj.lang.JoinPoint;import org.aspectj.lang.ProceedingJoinPoint;import org.aspectj.lang.annotation.*;import org.springframework.stereotype.Component;import java.util.Arrays;/** * @author yinhuidong * @createTime 2020-03-04-16:37 */@Component(&quot;logging&quot;)@Aspectpublic class Logging &#123; @Before(value=&quot;execution(* com.es.java1.*.*(..))&quot;) public void before(JoinPoint jp)&#123; Object[] args = jp.getArgs(); String name = jp.getSignature().getName(); System.out.println(&quot;before...&quot;+name+&quot;...&quot;+Arrays.toString(args)); &#125; @AfterReturning(value=&quot;execution(* com.es.java1.*.*(..))&quot;,returning = &quot;result&quot;) public void afterReturning(JoinPoint jp,Object result)&#123; Object[] args = jp.getArgs(); String name = jp.getSignature().getName(); System.out.println(&quot;afterReturning...&quot;+name+&quot;...&quot;+Arrays.toString(args)+&quot;...&quot;+result); &#125; @AfterThrowing(value=&quot;execution(* com.es.java1.*.*(..))&quot;,throwing = &quot;e&quot;) public void afterThrowing(JoinPoint jp,Exception e)&#123; Object[] args = jp.getArgs(); String name = jp.getSignature().getName(); System.out.println(&quot;afterThrowing...&quot;+name+&quot;...&quot;+Arrays.toString(args)+&quot;...&quot;+e); &#125; @After(&quot;execution(* com.es.java1.*.*(..))&quot;) public void after(JoinPoint jp)&#123; Object[] args = jp.getArgs(); String name = jp.getSignature().getName(); System.out.println(&quot;after...&quot;+name+&quot;...&quot;+Arrays.toString(args)); &#125;&#125; 实现类 123456789@Component(&quot;add&quot;)public class Add &#123; public void add(int a ,int b)&#123; System.out.println(a+b); &#125; public int del(int a, int b)&#123; return a-b; &#125;&#125; 测试类 1234567891011@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(locations = &quot;classpath:applicationContext.xml&quot;)public class Test1 &#123; @Autowired private Add add; @Test public void test1()&#123; add.add(1,1); System.out.println(add.del(1, 1)); &#125;&#125; 补充： 12345678910//配置通用的切入点表达式@Pointcut(value = &quot;execution(* com.es.dao.impl.ComputerDaoImpl.*(..))&quot;)public void pointCut()&#123;&#125;//引用通用的切入点表达式@Before(&quot;pointCut()&quot;)public void before(JoinPoint point)&#123; System.out.println(&quot;前置通知--&gt;&quot;+&quot;方法名：&quot;+point.getSignature().getName()+&quot;参数列表：&quot;+ Arrays.asList(point.getArgs()));&#125; 六，spring的事务1.基于AOP的事务处理模拟1.1 步骤 创建一个链接工具类，负责从线程获取连接，并实现与线程的绑定。 创建和事务管理相关的工具类，负责处理事务操作 利用aop实现事务处理 ​ 1.2 案例配置文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:jdbc=&quot;http://www.springframework.org/schema/jdbc&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/jdbc http://www.springframework.org/schema/jdbc/spring-jdbc-4.2.xsd http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.2.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-4.2.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-4.2.xsd&quot;&gt; &lt;!--配置dao--&gt; &lt;bean id=&quot;accountDao&quot; class=&quot;com.es.dao.impl.AccountDaoImpl&quot;&gt; &lt;property name=&quot;runner&quot; ref=&quot;runner&quot;&gt;&lt;/property&gt; &lt;property name=&quot;connectionUtils&quot; ref=&quot;connection&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!--配置service--&gt; &lt;bean id=&quot;accountService&quot; class=&quot;com.es.service.impl.AccountServiceImpl&quot;&gt; &lt;property name=&quot;accountDao&quot; ref=&quot;accountDao&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!--配置queryRunner--&gt; &lt;bean id=&quot;runner&quot; class=&quot;org.apache.commons.dbutils.QueryRunner&quot;&gt; &lt;!--&lt;constructor-arg name=&quot;ds&quot; ref=&quot;dataSource&quot;&gt;&lt;/constructor-arg&gt;--&gt; &lt;/bean&gt; &lt;!--配置数据源--&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot;&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;com.mysql.jdbc.Driver&quot;&gt;&lt;/property&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:3306/eesy&quot;&gt;&lt;/property&gt; &lt;property name=&quot;password&quot; value=&quot;yhd666&quot;&gt;&lt;/property&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!--配置transactionManager--&gt; &lt;bean id=&quot;transactionManager&quot; class=&quot;com.es.utils.TransactionManager&quot;&gt; &lt;property name=&quot;connectionUtils&quot; ref=&quot;connection&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!--配置ConnectionUtils--&gt; &lt;bean id=&quot;connection&quot; class=&quot;com.es.utils.ConnectionUtils&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!--配置aop--&gt; &lt;aop:config&gt; &lt;!--配置切入点表达式--&gt; &lt;aop:pointcut id=&quot;txAdvice&quot; expression=&quot;execution(* com.es.service.impl.*.*(..))&quot;&gt;&lt;/aop:pointcut&gt; &lt;aop:aspect id=&quot;txA&quot; ref=&quot;transactionManager&quot;&gt; &lt;aop:before method=&quot;beginTransaction&quot; pointcut-ref=&quot;txAdvice&quot;&gt;&lt;/aop:before&gt; &lt;aop:after-returning method=&quot;commit&quot; pointcut-ref=&quot;txAdvice&quot;&gt;&lt;/aop:after-returning&gt; &lt;aop:after-throwing method=&quot;rollback&quot; pointcut-ref=&quot;txAdvice&quot;&gt;&lt;/aop:after-throwing&gt; &lt;aop:after method=&quot;release&quot; pointcut-ref=&quot;txAdvice&quot;&gt;&lt;/aop:after&gt; &lt;/aop:aspect&gt; &lt;/aop:config&gt;&lt;/beans&gt; 持久层 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293package com.es.dao.impl;import com.es.dao.IAccountDao;import com.es.domain.Account;import com.es.utils.ConnectionUtils;import org.apache.commons.dbutils.QueryRunner;import org.apache.commons.dbutils.handlers.BeanHandler;import org.apache.commons.dbutils.handlers.BeanListHandler;import org.springframework.jdbc.core.BeanPropertyRowMapper;import org.springframework.jdbc.core.support.JdbcDaoSupport;import org.springframework.stereotype.Repository;import java.util.List;/** * 账户的持久层实现类 */public class AccountDaoImpl implements IAccountDao &#123; private QueryRunner runner; private ConnectionUtils connectionUtils; public void setRunner(QueryRunner runner) &#123; this.runner = runner; &#125; public void setConnectionUtils(ConnectionUtils connectionUtils) &#123; this.connectionUtils = connectionUtils; &#125; public List&lt;Account&gt; findAllAccount() &#123; try&#123; return runner.query(connectionUtils.getThreadConnection(),&quot;select * from account&quot;,new BeanListHandler&lt;Account&gt;(Account.class)); &#125;catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; public Account findAccountById(Integer accountId) &#123; try&#123; return runner.query(connectionUtils.getThreadConnection(),&quot;select * from account where id = ? &quot;,new BeanHandler&lt;Account&gt;(Account.class),accountId); &#125;catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; public void saveAccount(Account account) &#123; try&#123; runner.update(connectionUtils.getThreadConnection(),&quot;insert into account(name,money)values(?,?)&quot;,account.getName(),account.getMoney()); &#125;catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; public void updateAccount(Account account) &#123; try&#123; runner.update(connectionUtils.getThreadConnection(),&quot;update account set name=?,money=? where id=?&quot;,account.getName(),account.getMoney(),account.getId()); &#125;catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; public void deleteAccount(Integer accountId) &#123; try&#123; runner.update(connectionUtils.getThreadConnection(),&quot;delete from account where id=?&quot;,accountId); &#125;catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; public Account findAccountByName(String accountName) &#123; try&#123; List&lt;Account&gt; accounts = runner.query(connectionUtils.getThreadConnection(),&quot;select * from account where name = ? &quot;,new BeanListHandler&lt;Account&gt;(Account.class),accountName); if(accounts == null || accounts.size() == 0)&#123; return null; &#125; if(accounts.size() &gt; 1)&#123; throw new RuntimeException(&quot;结果集不唯一，数据有问题&quot;); &#125; return accounts.get(0); &#125;catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125;&#125; 服务层 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package com.es.service.impl;import com.es.dao.IAccountDao;import com.es.domain.Account;import com.es.service.IAccountService;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;/** * 账户的业务层实现类 * * 事务控制应该都是在业务层 */public class AccountServiceImpl implements IAccountService&#123; private IAccountDao accountDao; public void setAccountDao(IAccountDao accountDao) &#123; this.accountDao = accountDao; &#125; public Account findAccountById(Integer accountId) &#123; return accountDao.findAccountById(accountId); &#125; public void transfer(String sourceName, String targetName, Float money) &#123; System.out.println(&quot;transfer....&quot;); //2.1根据名称查询转出账户 Account source = accountDao.findAccountByName(sourceName); //2.2根据名称查询转入账户 Account target = accountDao.findAccountByName(targetName); //2.3转出账户减钱 source.setMoney(source.getMoney()-money); //2.4转入账户加钱 target.setMoney(target.getMoney()+money); //2.5更新转出账户 accountDao.updateAccount(source); int i=1/0; //2.6更新转入账户 accountDao.updateAccount(target); &#125;&#125; 连接工具类 12345678910111213141516171819202122232425262728293031323334353637383940414243444546package com.es.utils;import javax.sql.DataSource;import java.sql.Connection;/** * 连接的工具类，它用于从数据源中获取一个连接，并且实现和线程的绑定 */public class ConnectionUtils &#123; private ThreadLocal&lt;Connection&gt; tl = new ThreadLocal&lt;Connection&gt;(); private DataSource dataSource; public void setDataSource(DataSource dataSource) &#123; this.dataSource = dataSource; &#125; /** * 获取当前线程上的连接 * @return */ public Connection getThreadConnection() &#123; try&#123; //1.先从ThreadLocal上获取 Connection conn = tl.get(); //2.判断当前线程上是否连接 if (conn == null) &#123; //3.从数据源中获取一个连接，并且存入ThreadLocal中 conn = dataSource.getConnection(); tl.set(conn); &#125; //4.返回当前线程上的连接 return conn; &#125;catch (Exception e)&#123; throw new RuntimeException(e); &#125; &#125; /** * 把连接和线程解绑 */ public void removeConnection()&#123; tl.remove(); &#125;&#125; 事务处理类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162package com.es.utils;import org.springframework.stereotype.Component;/** * 和事务管理相关的工具类，它包含了，开启事务，提交事务，回滚事务和释放连接 */public class TransactionManager &#123; private ConnectionUtils connectionUtils; public void setConnectionUtils(ConnectionUtils connectionUtils) &#123; this.connectionUtils = connectionUtils; &#125; /** * 开启事务 */ public void beginTransaction()&#123; try &#123; connectionUtils.getThreadConnection().setAutoCommit(false); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; &#125; /** * 提交事务 */ public void commit()&#123; try &#123; connectionUtils.getThreadConnection().commit(); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; &#125; /** * 回滚事务 */ public void rollback()&#123; try &#123; connectionUtils.getThreadConnection().rollback(); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; &#125; /** * 释放连接 */ public void release()&#123; try &#123; connectionUtils.getThreadConnection().close();//还回连接池中 connectionUtils.removeConnection(); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; &#125;&#125; 2.spring的事务配置2.1 spring中基于xml的事务配置2.2.1 配置步骤spring中基于XML的声明式事务控制配置步骤​ 配置事务管理器 配置事务的通知 此时我们需要导入事务的约束 tx名称空间和约束，同时也需要aop的 使用tx:advice标签配置事务通知 属性： id：给事务通知起一个唯一标识 transaction-manager：给事务通知提供一个事务管理器引用 配置AOP中的通用切入点表达式 建立事务通知和切入点表达式的对应关系 配置事务的属性 是在事务的通知tx:advice标签的内部 ​ 配置事务的属性​ isolation：用于指定事务的隔离级别。默认值是DEFAULT，表示使用数据库的默认隔离级别。 propagation：用于指定事务的传播行为。默认值是REQUIRED，表示一定会有事务，增删改的选择。查询方法可以选择SUPPORTS。 read-only：用于指定事务是否只读。只查询方法才能设置为true。默认值是false，表示读写。 timeout：用于指定事务的超时时间，默认值是-1，表示永不超时。如果指定了数值，以秒为单位。 rollback-for：用于指定一个异常，当产生该异常时，事务回滚，产生其他异常时，事务不回滚。没有默认值。表示任何异常都回滚。 no-rollback-for：用于指定一个异常，当产生该异常时，事务不回滚，产生其他异常时事务回滚。没有默认值。表示任何异常都回滚。 ​ 2.2.2 案例配置文件 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:jdbc=&quot;http://www.springframework.org/schema/jdbc&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/jdbc http://www.springframework.org/schema/jdbc/spring-jdbc-4.2.xsd http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.2.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-4.2.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-4.2.xsd&quot;&gt; &lt;context:component-scan base-package=&quot;com.es&quot;&gt;&lt;/context:component-scan&gt; &lt;bean id=&quot;accountDao&quot; class=&quot;com.es.dao.impl.AccountDaoImpl&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;bean id=&quot;account&quot; class=&quot;com.es.domain.Account&quot;&gt;&lt;/bean&gt; &lt;bean id=&quot;accountService&quot; class=&quot;com.es.service.impl.AccountServiceImpl&quot;&gt; &lt;property name=&quot;accountDao&quot; ref=&quot;accountDao&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot;&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot;&gt;&lt;/property&gt; &lt;property name=&quot;password&quot; value=&quot;yhd666&quot;&gt;&lt;/property&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql:///eesy&quot;&gt;&lt;/property&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;com.mysql.jdbc.Driver&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- spring中基于XML的声明式事务控制配置步骤 1、配置事务管理器 2、配置事务的通知 此时我们需要导入事务的约束 tx名称空间和约束，同时也需要aop的 使用tx:advice标签配置事务通知 属性： id：给事务通知起一个唯一标识 transaction-manager：给事务通知提供一个事务管理器引用 3、配置AOP中的通用切入点表达式 4、建立事务通知和切入点表达式的对应关系 5、配置事务的属性 是在事务的通知tx:advice标签的内部 --&gt; &lt;!--配置事务管理器--&gt; &lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!--配置事务的通知--&gt; &lt;tx:advice id=&quot;txAdvice&quot; transaction-manager=&quot;transactionManager&quot;&gt; &lt;!-- 配置事务的属性 isolation：用于指定事务的隔离级别。默认值是DEFAULT，表示使用数据库的默认隔离级别。 propagation：用于指定事务的传播行为。默认值是REQUIRED，表示一定会有事务，增删改的选择。查询方法可以选择SUPPORTS。 read-only：用于指定事务是否只读。只查询方法才能设置为true。默认值是false，表示读写。 timeout：用于指定事务的超时时间，默认值是-1，表示永不超时。如果指定了数值，以秒为单位。 rollback-for：用于指定一个异常，当产生该异常时，事务回滚，产生其他异常时，事务不回滚。没有默认值。表示任何异常都回滚。 no-rollback-for：用于指定一个异常，当产生该异常时，事务不回滚，产生其他异常时事务回滚。没有默认值。表示任何异常都回滚。 --&gt; &lt;tx:attributes&gt; &lt;tx:method name=&quot;*&quot; isolation=&quot;DEFAULT&quot; propagation=&quot;REQUIRED&quot; timeout=&quot;-1&quot; read-only=&quot;false&quot;/&gt; &lt;tx:method name=&quot;find*&quot; isolation=&quot;DEFAULT&quot; propagation=&quot;SUPPORTS&quot; read-only=&quot;true&quot; timeout=&quot;-1&quot;/&gt; &lt;/tx:attributes&gt; &lt;/tx:advice&gt; &lt;aop:config&gt; &lt;aop:pointcut id=&quot;pointCut&quot; expression=&quot;execution(* com.es.service.impl.*.*(..))&quot;&gt;&lt;/aop:pointcut&gt; &lt;aop:advisor advice-ref=&quot;txAdvice&quot; pointcut-ref=&quot;pointCut&quot;&gt;&lt;/aop:advisor&gt; &lt;/aop:config&gt;&lt;/beans&gt; 持久层 12345678910111213141516171819202122232425262728293031323334353637package com.es.dao.impl;import com.es.dao.IAccountDao;import com.es.domain.Account;import org.springframework.jdbc.core.BeanPropertyRowMapper;import org.springframework.jdbc.core.support.JdbcDaoSupport;import java.util.List;/** * 账户的持久层实现类 */public class AccountDaoImpl extends JdbcDaoSupport implements IAccountDao &#123; public Account findAccountById(Integer accountId) &#123; List&lt;Account&gt; accounts = super.getJdbcTemplate().query(&quot;select * from account where id = ?&quot;,new BeanPropertyRowMapper&lt;Account&gt;(Account.class),accountId); return accounts.isEmpty()?null:accounts.get(0); &#125; public Account findAccountByName(String accountName) &#123; List&lt;Account&gt; accounts = super.getJdbcTemplate().query(&quot;select * from account where name = ?&quot;,new BeanPropertyRowMapper&lt;Account&gt;(Account.class),accountName); if(accounts.isEmpty())&#123; return null; &#125; if(accounts.size()&gt;1)&#123; throw new RuntimeException(&quot;结果集不唯一&quot;); &#125; return accounts.get(0); &#125; public void updateAccount(Account account) &#123; super.getJdbcTemplate().update(&quot;update account set name=?,money=? where id=?&quot;,account.getName(),account.getMoney(),account.getId()); &#125;&#125; 服务层 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package com.es.service.impl;import com.es.dao.IAccountDao;import com.es.domain.Account;import com.es.service.IAccountService;/** * 账户的业务层实现类 * * 事务控制应该都是在业务层 */public class AccountServiceImpl implements IAccountService&#123; private IAccountDao accountDao; public void setAccountDao(IAccountDao accountDao) &#123; this.accountDao = accountDao; &#125; public Account findAccountById(Integer accountId) &#123; return accountDao.findAccountById(accountId); &#125; public void transfer(String sourceName, String targetName, Float money) &#123; System.out.println(&quot;transfer....&quot;); //2.1根据名称查询转出账户 Account source = accountDao.findAccountByName(sourceName); //2.2根据名称查询转入账户 Account target = accountDao.findAccountByName(targetName); //2.3转出账户减钱 source.setMoney(source.getMoney()-money); //2.4转入账户加钱 target.setMoney(target.getMoney()+money); //2.5更新转出账户 accountDao.updateAccount(source); // int i=1/0; //2.6更新转入账户 accountDao.updateAccount(target); &#125;&#125; 测试类 123456789101112131415161718192021import com.es.service.IAccountService;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.test.context.ContextConfiguration;import org.springframework.test.context.junit4.SpringJUnit4ClassRunner;/** * @author yinhuidong * @createTime 2020-03-03-12:03 */@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(locations = &quot;classpath:applicationContext.xml&quot;)public class Test1 &#123; @Autowired private IAccountService service; @Test public void test1()&#123; service.transfer(&quot;aaa&quot;,&quot;bbb&quot;,1000f); &#125;&#125; 2.2 spring中基于注解的事务配置2.2.1 配置步骤 在配置文件配置事务管理器 开始声明式事务的支持 在对应的方法上加@Transcational注解，事务声明注解:该注解可以添加到类或者方法上 属性： propagation:用来设置事务的传播行为：一个方法运行在一个开启了事务的方法中，当前方法是使用原有的事务还是开启新事物 required:如果有事务就使用，没有就开启一个新的（默认） required_new:必须开启新事物 supports：如果有事务就运行，否则也不开启新的事务 no_supports:即使有事务也不用​ 2.2.2 案例配置文件 123456789101112131415161718192021222324252627282930313233343536&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:jdbc=&quot;http://www.springframework.org/schema/jdbc&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/jdbc http://www.springframework.org/schema/jdbc/spring-jdbc-4.2.xsd http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.2.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-4.2.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-4.2.xsd&quot;&gt; &lt;bean id=&quot;jdbcTemplate&quot; class=&quot;org.springframework.jdbc.core.JdbcTemplate&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;context:property-placeholder location=&quot;classpath:druid.properties&quot;&gt;&lt;/context:property-placeholder&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot;&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;$&#123;driverClassName&#125;&quot;&gt;&lt;/property&gt; &lt;property name=&quot;url&quot; value=&quot;$&#123;url&#125;&quot;&gt;&lt;/property&gt; &lt;property name=&quot;username&quot; value=&quot;$&#123;jdbc.username&#125;&quot;&gt;&lt;/property&gt; &lt;property name=&quot;password&quot; value=&quot;$&#123;jdbc.password&#125;&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- spring中基于注解 的声明式事务控制配置步骤 1、配置事务管理器 2、开启spring对注解事务的支持 3、在需要事务支持的地方使用@Transactional注解 --&gt; &lt;!-- 配置事务管理器 --&gt; &lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 开启spring对注解事务的支持--&gt; &lt;tx:annotation-driven&gt;&lt;/tx:annotation-driven&gt; &lt;!--组件扫描--&gt; &lt;context:component-scan base-package=&quot;com.es&quot;&gt;&lt;/context:component-scan&gt;&lt;/beans&gt; 持久层 1234567891011121314151617181920212223242526272829/** * 账户的持久层实现类 */@Repository(&quot;accountDao&quot;)public class AccountDaoImpl implements IAccountDao &#123; @Autowired private JdbcTemplate jdbcTemplate; public Account findAccountById(Integer accountId) &#123; List&lt;Account&gt; accounts = jdbcTemplate.query(&quot;select * from account where id = ?&quot;,new BeanPropertyRowMapper&lt;Account&gt;(Account.class),accountId); return accounts.isEmpty()?null:accounts.get(0); &#125; public Account findAccountByName(String accountName) &#123; List&lt;Account&gt; accounts = jdbcTemplate.query(&quot;select * from account where name = ?&quot;,new BeanPropertyRowMapper&lt;Account&gt;(Account.class),accountName); if(accounts.isEmpty())&#123; return null; &#125; if(accounts.size()&gt;1)&#123; throw new RuntimeException(&quot;结果集不唯一&quot;); &#125; return accounts.get(0); &#125; public void updateAccount(Account account) &#123; jdbcTemplate.update(&quot;update account set name=?,money=? where id=?&quot;,account.getName(),account.getMoney(),account.getId()); &#125;&#125; 服务层 12345678910111213141516171819202122232425262728293031323334353637383940414243/** * 账户的业务层实现类 * * 事务控制应该都是在业务层 */@Service(&quot;accountService&quot;)@Transactional(readOnly = false,propagation = Propagation.SUPPORTS)public class AccountServiceImpl implements IAccountService&#123; @Autowired private IAccountDao accountDao; public void setAccountDao(IAccountDao accountDao) &#123; this.accountDao = accountDao; &#125; public Account findAccountById(Integer accountId) &#123; return accountDao.findAccountById(accountId); &#125; @Transactional(readOnly = false,propagation = Propagation.REQUIRED) public void transfer(String sourceName, String targetName, Float money) &#123; System.out.println(&quot;transfer....&quot;); //2.1根据名称查询转出账户 Account source = accountDao.findAccountByName(sourceName); //2.2根据名称查询转入账户 Account target = accountDao.findAccountByName(targetName); //2.3转出账户减钱 source.setMoney(source.getMoney()-money); //2.4转入账户加钱 target.setMoney(target.getMoney()+money); //2.5更新转出账户 accountDao.updateAccount(source); int i=1/0; //2.6更新转入账户 accountDao.updateAccount(target); &#125;&#125; 测试类​ 12345678910@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(locations = &quot;classpath:applicationContext.xml&quot;)public class Test1 &#123; @Autowired private IAccountService service; @Test public void test1()&#123; service.transfer(&quot;aaa&quot;,&quot;bbb&quot;,1000f); &#125;&#125;","categories":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/tags/Spring/"}]},{"title":"Spring[十五]事务增强器源码分析","slug":"Spring/Spring[十五]事务增强器源码分析","date":"2022-01-11T06:11:10.808Z","updated":"2022-01-11T06:18:09.756Z","comments":true,"path":"2022/01/11/Spring/Spring[十五]事务增强器源码分析/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/Spring/Spring[%E5%8D%81%E4%BA%94]%E4%BA%8B%E5%8A%A1%E5%A2%9E%E5%BC%BA%E5%99%A8%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","excerpt":"","text":"在上一篇回顾了一下传统的JDBC事务，并分析了Spring解析事务标签，创建事务代理对象的源码，本篇我们来分析在目标方法执行的前后，事务增强器是如何工作的。 1.相关接口 在分析事务增强器的源码之前，先来过一些接口，明确接口的作用和接口之间的关系。 java jdbc规范接口：DataSource、 Connection 事务属性承载对象：TransactionAttribute 事务管理器：TransactionManager、PlatformTransactionManager 数据库链接holder：ConnectionHolder Spring抽象出来的事务对象：JdbcTransactionObjectSupport、DataSourceTransactionObject Spring抽象出来的事务状态（包装“事务对象”，做了一些增强）：AbstractTransactionStatus、DefaultTransactionStatus Spring抽象出来的事务信息（集大成者“txMgr”、“txStatus”、“txAttr”…）：TransactionInfo 2.事务增强器 接下来我们来分析事务增强器源码。 ​ 事务增强器的入口自然是**invoke()**。​ ​ 1234567891011121314151617181920212223242526272829303132333435363738/** * 事务增强器的入口 * invocation：后续事务增强器向后驱动事务拦截器的时候还需要使用它 * @param invocation the method invocation joinpoint * @return * @throws Throwable */@Override@Nullablepublic Object invoke(MethodInvocation invocation) throws Throwable &#123; // Work out the target class: may be &#123;@code null&#125;. // The TransactionAttributeSource should be passed the target class // as well as the method, which may be from an interface. /*需要被事务增强器增强的目标类型 * invocation.getThis() 拿到目标对象 * */ Class&lt;?&gt; targetClass = (invocation.getThis() != null ? AopUtils.getTargetClass(invocation.getThis()) : null); // 参数一：目标方法 //参数二：目标对象类型 //参数三： return invokeWithinTransaction(invocation.getMethod(), targetClass, new CoroutinesInvocationCallback() &#123; @Override @Nullable public Object proceedWithInvocation() throws Throwable &#123; return invocation.proceed(); &#125; @Override public Object getTarget() &#123; return invocation.getThis(); &#125; @Override public Object[] getArguments() &#123; return invocation.getArguments(); &#125; &#125;);&#125; 我们直接往下看**invokeWithinTransaction()**。​ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162&#123; Object result; final ThrowableHolder throwableHolder = new ThrowableHolder(); // It&#x27;s a CallbackPreferringPlatformTransactionManager: pass a TransactionCallback in. try &#123; result = ((CallbackPreferringPlatformTransactionManager) ptm).execute(txAttr, status -&gt; &#123; TransactionInfo txInfo = prepareTransactionInfo(ptm, txAttr, joinpointIdentification, status); try &#123; Object retVal = invocation.proceedWithInvocation(); if (retVal != null &amp;&amp; vavrPresent &amp;&amp; VavrDelegate.isVavrTry(retVal)) &#123; // Set rollback-only in case of Vavr failure matching our rollback rules... retVal = VavrDelegate.evaluateTryFailure(retVal, txAttr, status); &#125; return retVal; &#125; catch (Throwable ex) &#123; if (txAttr.rollbackOn(ex)) &#123; // A RuntimeException: will lead to a rollback. if (ex instanceof RuntimeException) &#123; throw (RuntimeException) ex; &#125; else &#123; throw new ThrowableHolderException(ex); &#125; &#125; else &#123; // A normal return value: will lead to a commit. throwableHolder.throwable = ex; return null; &#125; &#125; finally &#123; cleanupTransactionInfo(txInfo); &#125; &#125;); &#125; catch (ThrowableHolderException ex) &#123; throw ex.getCause(); &#125; catch (TransactionSystemException ex2) &#123; if (throwableHolder.throwable != null) &#123; logger.error(&quot;Application exception overridden by commit exception&quot;, throwableHolder.throwable); ex2.initApplicationException(throwableHolder.throwable); &#125; throw ex2; &#125; catch (Throwable ex2) &#123; if (throwableHolder.throwable != null) &#123; logger.error(&quot;Application exception overridden by commit exception&quot;, throwableHolder.throwable); &#125; throw ex2; &#125; // Check result state: It might indicate a Throwable to rethrow. if (throwableHolder.throwable != null) &#123; throw throwableHolder.throwable; &#125; return result; &#125; &#125; 这个方法的内容很多：​ 获取事务注解解析器，解析事务注解 获取事务名称 判断如果是声明式事务 **createTransactionIfNecessary()** 创建一个最上层的事务上下文，包含所有的事务资源 驱动方法增强逻辑继续往下执行 **completeTransactionAfterThrowing(txInfo, ex)**执行业务代码出现异常时的逻辑 **cleanupTransactionInfo(txInfo)**还原现场逻辑 **commitTransactionAfterReturning(txInfo)**提交事务逻辑 判断如果是编程式事务，走编程式事务的逻辑…. ​ 接下来我们来分析这几个核心逻辑。​ 3.创建最上层的事务上下文​ 这里是事务的核心逻辑，涉及到事务嵌套和传播行为，隔离级别。​ 1234567891011121314151617181920212223242526272829303132protected TransactionInfo createTransactionIfNecessary(@Nullable PlatformTransactionManager tm, @Nullable TransactionAttribute txAttr, final String joinpointIdentification) &#123; // If no name specified, apply method identification as transaction name. if (txAttr != null &amp;&amp; txAttr.getName() == null) &#123; //进行一个包装，提供事务名 txAttr = new DelegatingTransactionAttribute(txAttr) &#123; @Override public String getName() &#123; return joinpointIdentification; &#125; &#125;; &#125; //事务状态对象 TransactionStatus status = null; if (txAttr != null) &#123; if (tm != null) &#123; //根据事物属性创建事务状态对象，事务状态：一般情况下包装着事务对象；特殊情况 status.Transaction 有可能为空 //方法上的注解为 @Transactional上的注解 传播行为设置为了 NOT_SUPPORTED || NEVER //具体看一下这个方法 status = tm.getTransaction(txAttr); &#125; else &#123; if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Skipping transactional joinpoint [&quot; + joinpointIdentification + &quot;] because no transaction manager has been configured&quot;); &#125; &#125; &#125; //包装成一个上层的事务上下文对象 return prepareTransactionInfo(tm, txAttr, joinpointIdentification, status);&#125; 获取事务状态对象 包装成事务上下文对象 主要的逻辑在这里**getTransaction()**。\u0000 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859@Overridepublic final TransactionStatus getTransaction(@Nullable TransactionDefinition definition) throws TransactionException &#123; // 事务属性信息 TransactionDefinition def = (definition != null ? definition : TransactionDefinition.withDefaults()); //获取事务对象，非常关键 Object transaction = doGetTransaction(); boolean debugEnabled = logger.isDebugEnabled(); //条件成立说明当前是重入的事务的情况 //a开启事务 a调用b ，b也加了事务注解 ，开启了事务的情况 if (isExistingTransaction(transaction)) &#123; //事务重入的分支逻辑 ，这里就涉及到了传播行为 // Existing transaction found -&gt; check propagation behavior to find out how to behave. return handleExistingTransaction(def, transaction, debugEnabled); &#125; //执行到这里说明当前线程没有绑定连接资源，没开启事务 // Check definition settings for new transaction. if (def.getTimeout() &lt; TransactionDefinition.TIMEOUT_DEFAULT) &#123; throw new InvalidTimeoutException(&quot;Invalid transaction timeout&quot;, def.getTimeout()); &#125; //使用当前事务，没有事务就会抛出异常 // No existing transaction found -&gt; check propagation behavior to find out how to proceed. if (def.getPropagationBehavior() == TransactionDefinition.PROPAGATION_MANDATORY) &#123; throw new IllegalTransactionStateException( &quot;No existing transaction found for transaction marked with propagation &#x27;mandatory&#x27;&quot;); &#125; //PROPAGATION_REQUIRED PROPAGATION_REQUIRES_NEW PROPAGATION_NESTED 则进入 else if (def.getPropagationBehavior() == TransactionDefinition.PROPAGATION_REQUIRED || def.getPropagationBehavior() == TransactionDefinition.PROPAGATION_REQUIRES_NEW || def.getPropagationBehavior() == TransactionDefinition.PROPAGATION_NESTED) &#123; //挂起了个寂寞，因为线程并没有绑定事务 SuspendedResourcesHolder suspendedResources = suspend(null); if (debugEnabled) &#123; logger.debug(&quot;Creating new transaction with name [&quot; + def.getName() + &quot;]: &quot; + def); &#125; try &#123; //线程未开启事务时，这三种情况都会去开启一个新的事物 //开启事务的逻辑 return startTransaction(def, transaction, debugEnabled, suspendedResources); &#125; catch (RuntimeException | Error ex) &#123; resume(null, suspendedResources); throw ex; &#125; &#125; else &#123; //走到这里是什么情况？ support || not support || never //没有使用新的事务 // Create &quot;empty&quot; transaction: no actual transaction, but potentially synchronization. if (def.getIsolationLevel() != TransactionDefinition.ISOLATION_DEFAULT &amp;&amp; logger.isWarnEnabled()) &#123; logger.warn(&quot;Custom isolation level specified but no actual transaction initiated; &quot; + &quot;isolation level will effectively be ignored: &quot; + def); &#125; boolean newSynchronization = (getTransactionSynchronization() == SYNCHRONIZATION_ALWAYS); return prepareTransactionStatus(def, null, true, newSynchronization, debugEnabled, null); &#125;&#125; 获取事务属性信息&amp;事务对象 如果是事务重入的逻辑，就做事务重入逻辑的处理 判断当前事务如果超时了，抛出异常 PROPAGATION_MANDATORY，有就使用当前事务，没有就抛异常 PROPAGATION_REQUIRED PROPAGATION_REQUIRES_NEW PROPAGATION_NESTED 开启一个新的事物 support || not support || never 没有使用新的事务，不会主动开启 最终构建事务状态对象 **doGetTransaction() 获取事务对象。**​ **handleExistingTransaction()处理事务重入的逻辑。**​ **startTransaction()开启新事物的逻辑。**​ 3.1 获取事务对象1234567891011121314151617@Overrideprotected Object doGetTransaction() &#123; //先创建事务对象 DataSourceTransactionObject txObject = new DataSourceTransactionObject(); //事务的保存点，这个由事务管理器控制 txObject.setSavepointAllowed(isNestedTransactionAllowed()); //TransactionSynchronizationManager 事务同步管理器 //从tl中获取连接资源，有可能拿到null，也有可能不是null //什么时候是null，什么时候不是null？ //==null:事务方法a调用了非事务方法b //!=null:事务方法a调用了事务方法b ConnectionHolder conHolder = (ConnectionHolder) TransactionSynchronizationManager.getResource(obtainDataSource()); //为事务对象赋能，参数二传递的false， 表示当前事务是否新分配了连接资源，而不是和上层事务共享，默认是false，表示共享。 txObject.setConnectionHolder(conHolder, false); return txObject;&#125; 3.2处理事务重入123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105private TransactionStatus handleExistingTransaction( TransactionDefinition definition, /*事务属性*/Object transaction/*事务对象*/, boolean debugEnabled) throws TransactionException &#123; /* 进入这个方法的时候说明当前线程已经持有一个事务了，需要根据新方法的事务注解传播行为，走不同的逻辑 */ //PROPAGATION_NEVER：需要抛异常 if (definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_NEVER) &#123; throw new IllegalTransactionStateException( &quot;Existing transaction found for transaction marked with propagation &#x27;never&#x27;&quot;); &#125; //PROPAGATION_NOT_SUPPORTED：有事务就挂起当前事务 if (definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_NOT_SUPPORTED) &#123; if (debugEnabled) &#123; logger.debug(&quot;Suspending current transaction&quot;); &#125; //看一下挂起的逻辑 Object suspendedResources = suspend(transaction); boolean newSynchronization = (getTransactionSynchronization() == SYNCHRONIZATION_ALWAYS); //将挂起事务返回的 挂起持有者对象(这里面持有上一个事务对象的连接资源和线程上下文参数) 给事务状态对象赋能 //step into 创建一个新的 事务状态对象 ，这里的第二个参数 事务 为null ；表示线程执行到当前方法执行到事务增强的后置处理逻辑的时候 //提交事务的时候会检查事务状态的事务是否有值，如果没有值，Spring就不会做提交的操作。 //参数6：suspendedResources 线程执行到后置处理的逻辑的时候，执行到恢复现场的时候会检查这个参数是否有值，如果有值会进行恢复现场的操作。 return prepareTransactionStatus( definition, null/*说明当前线程未手动开启事务，连接是直接从数据源拿的，不需要手动提交事务了*/, false, newSynchronization, debugEnabled, suspendedResources); &#125; //PROPAGATION_REQUIRES_NEW：挂起当前事务，开启一个新的事务 if (definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_REQUIRES_NEW) &#123; if (debugEnabled) &#123; logger.debug(&quot;Suspending current transaction, creating new transaction with name [&quot; + definition.getName() + &quot;]&quot;); &#125; //挂起上层事务，新建事务 SuspendedResourcesHolder suspendedResources = suspend(transaction); try &#123; //开启一个专属于当前方法的新事务，因为当前方法被挂起的事务执行完当前方法后还要回到上层继续执行，所以 //suspendedResources用来恢复现场 return startTransaction(definition, transaction, debugEnabled, suspendedResources); &#125; catch (RuntimeException | Error beginEx) &#123; resumeAfterBeginException(transaction, suspendedResources, beginEx); throw beginEx; &#125; &#125; //嵌套事务的逻辑，如果当前存在事务，则在嵌套事务内执行 ， //如果当前没有事务，则与required的操作类似 if (definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_NESTED) &#123; //默认情况下，spring是不开启这种传播行为的，除非手动开启，所以默认是false。 if (!isNestedTransactionAllowed()) &#123; throw new NestedTransactionNotSupportedException( &quot;Transaction manager does not allow nested transactions by default - &quot; + &quot;specify &#x27;nestedTransactionAllowed&#x27; property with value &#x27;true&#x27;&quot;); &#125; if (debugEnabled) &#123; logger.debug(&quot;Creating nested transaction with name [&quot; + definition.getName() + &quot;]&quot;); &#125; //一般成立，默认是true if (useSavepointForNestedTransaction()) &#123; // 为当前方法创建一个 事务状态对象，共享的上层事务，执行的扩展点是外层的，也不需要挂起事务。 DefaultTransactionStatus status = prepareTransactionStatus(definition, transaction, false, false, debugEnabled, null); //创建一个保存点 ，重要！！！ status.createAndHoldSavepoint(); return status; &#125; else &#123; // 开启新事物 return startTransaction(definition, transaction, debugEnabled, null); &#125; &#125; if (debugEnabled) &#123; logger.debug(&quot;Participating in existing transaction&quot;); &#125; //是否需要验证，默认是false if (isValidateExistingTransaction()) &#123; if (definition.getIsolationLevel() != TransactionDefinition.ISOLATION_DEFAULT) &#123; Integer currentIsolationLevel = TransactionSynchronizationManager.getCurrentTransactionIsolationLevel(); if (currentIsolationLevel == null || currentIsolationLevel != definition.getIsolationLevel()) &#123; Constants isoConstants = DefaultTransactionDefinition.constants; throw new IllegalTransactionStateException(&quot;Participating transaction with definition [&quot; + definition + &quot;] specifies isolation level which is incompatible with existing transaction: &quot; + (currentIsolationLevel != null ? isoConstants.toCode(currentIsolationLevel, DefaultTransactionDefinition.PREFIX_ISOLATION) : &quot;(unknown)&quot;)); &#125; &#125; if (!definition.isReadOnly()) &#123; if (TransactionSynchronizationManager.isCurrentTransactionReadOnly()) &#123; throw new IllegalTransactionStateException(&quot;Participating transaction with definition [&quot; + definition + &quot;] is not marked as read-only but existing transaction is&quot;); &#125; &#125; &#125; /* 执行到这里就剩下 required &amp; supports */ boolean newSynchronization = (getTransactionSynchronization() != SYNCHRONIZATION_NEVER); //prepareTransactionStatus（）上面已经分析过了，属于构造器重载逻辑 //参数二：事务对象 -&gt; connHodler 是 doGetTransaction()的时候从西安城上下文内获取的上层事务的连接资源 //参数六：是空，因为我们没有挂起任何事务 return prepareTransactionStatus(definition, transaction/*事务也是使用上层的*/, false/*表示并不是一个为自己创建的事务，与上层方法共享*/, newSynchronization, debugEnabled, null);&#125; PROPAGATION_NEVER：需要抛异常 PROPAGATION_NOT_SUPPORTED：有事务就挂起当前事务 PROPAGATION_REQUIRES_NEW：挂起当前事务，开启一个新的事务 嵌套事务的逻辑，如果当前存在事务，则在嵌套事务内执行 ，如果当前没有事务，则与required的操作类似 required &amp; supports，不会创建事务对象 最终创建事务状态对象并返回 ​ 看一下创建事务保存点的逻辑，**createAndHoldSavepoint()**。 123456public void createAndHoldSavepoint() throws TransactionException &#123; /* getSavepointManager()：这里实际上就执行到了jdbc的方法，创建保存点，然后保存。 */ setSavepoint(getSavepointManager().createSavepoint());&#125; 3.3 开启新事物1234567891011121314private TransactionStatus startTransaction(TransactionDefinition definition, Object transaction, boolean debugEnabled, @Nullable SuspendedResourcesHolder suspendedResources) &#123; //一般情况下是true ，这个值控制是否执行事务的扩展逻辑，这东西有点类似ioc的后置处理器 boolean newSynchronization = (getTransactionSynchronization() != SYNCHRONIZATION_NEVER); //创建默认的事务状态对象，第三个参数为true，会为当前事务分配 连接资源，就是事务是专门为了当前方法开启的 //suspendedResources:表示挂起的事务 ，从上面过来这里实际上是null。 DefaultTransactionStatus status = newTransactionStatus( definition, transaction, true, newSynchronization, debugEnabled, suspendedResources); //开启事务 核心逻辑 ，参数一：事务对象；参数二：事务属性 doBegin(transaction, definition); //处理TransactionSynchronization prepareSynchronization(status, definition); return status;&#125; 核心逻辑在这里**doBegin()**。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263@Overrideprotected void doBegin(Object transaction, TransactionDefinition definition) &#123; //事务对象 DataSourceTransactionObject txObject = (DataSourceTransactionObject) transaction; Connection con = null; try &#123;/*判断当前事务对象是不是有线程资源，没有就会走if的逻辑，有就说明需要为当前方法分为连接资源*/ if (!txObject.hasConnectionHolder() || txObject.getConnectionHolder().isSynchronizedWithTransaction()) &#123; //通过数据源拿到真实的数据库连接 Connection newCon = obtainDataSource().getConnection(); if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Acquired Connection [&quot; + newCon + &quot;] for JDBC transaction&quot;); &#125; //重点：将上一步创建的数据库连接包装为ConnectionHolder，并且为事务对象赋能 //参数二很关键，给事务新申请的而连接资源，那么就将事务对象的 newConnectionHolder 设置为true。表示当前目标方法开启了一个自己的事务。 txObject.setConnectionHolder(new ConnectionHolder(newCon), true); &#125; txObject.getConnectionHolder().setSynchronizedWithTransaction(true); //获取事务对象商的数据库连接 con = txObject.getConnectionHolder().getConnection(); //修改数据库连接上的一些属性 step into Integer previousIsolationLevel = DataSourceUtils.prepareConnectionForTransaction(con, definition); //将连接原来的隔离级别保存到事务对象，方便释放连接的时候，设置回原来的状态 txObject.setPreviousIsolationLevel(previousIsolationLevel); txObject.setReadOnly(definition.isReadOnly()); //如果连接的自动提交是true，一般会成立 if (con.getAutoCommit()) &#123; //因为接下来就是设置自动提交为false，这里设置 must ，表示回头释放的时候要设置回去 txObject.setMustRestoreAutoCommit(true); if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Switching JDBC Connection [&quot; + con + &quot;] to manual commit&quot;); &#125; //源头，相当于在数据库开启了事务 con.setAutoCommit(false); &#125; //没啥实际的东西 prepareTransactionalConnection(con, definition); //激活holder的事务状态 txObject.getConnectionHolder().setTransactionActive(true); //获取超时时间 int timeout = determineTimeout(definition); //如果时间不相等，就设置 if (timeout != TransactionDefinition.TIMEOUT_DEFAULT) &#123; txObject.getConnectionHolder().setTimeoutInSeconds(timeout); &#125; // 如果是新开启的事务，分配了新的连接就会成立，这个时候需要将线程和连接进行一个绑定 tl if (txObject.isNewConnectionHolder()) &#123; TransactionSynchronizationManager.bindResource(obtainDataSource(), txObject.getConnectionHolder()); &#125; &#125; catch (Throwable ex) &#123; if (txObject.isNewConnectionHolder()) &#123; DataSourceUtils.releaseConnection(con, obtainDataSource()); txObject.setConnectionHolder(null, false); &#125; throw new CannotCreateTransactionException(&quot;Could not open JDBC Connection for transaction&quot;, ex); &#125;&#125; 判断当前事务对象是不是有线程资源，没有就会走if的逻辑，有就说明需要为当前方法分为连接资源 通过数据源拿到真实的数据库连接 将上一步创建的数据库连接包装为ConnectionHolder，并且为事务对象赋能 获取事务对象商的数据库连接 修改数据库连接上的一些属性 将连接原来的隔离级别保存到事务对象，方便释放连接的时候，设置回原来的状态 如果连接的自动提交是true，改成false 激活holder的事务状态 设置超时时间 如果是新开启的事务，分配了新的连接就会成立，这个时候需要将线程和连接进行一个绑定 tl 4.异常回滚逻辑12345678910111213141516171819202122232425262728293031323334353637383940414243protected void completeTransactionAfterThrowing(@Nullable TransactionInfo txInfo/*当前事务上下文*/, Throwable ex/*目标方法抛出的异常信息*/) &#123; if (txInfo != null &amp;&amp; txInfo.getTransactionStatus() != null) &#123; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Completing transaction for [&quot; + txInfo.getJoinpointIdentification() + &quot;] after exception: &quot; + ex); &#125; //条件一：一般都是成立的 //条件二：transactionAttribute.rollbackOn(ex) 判断目标方法抛出的异常是否需要回滚，条件成立，说明需要回滚。 if (txInfo.transactionAttribute != null &amp;&amp; txInfo.transactionAttribute.rollbackOn(ex)) &#123; try &#123; //如果需要回滚，就会走到事务管理器的回滚逻辑 txInfo.getTransactionManager().rollback(txInfo.getTransactionStatus()/*当前事务状态对象*/); &#125; catch (TransactionSystemException ex2) &#123; logger.error(&quot;Application exception overridden by rollback exception&quot;, ex); ex2.initApplicationException(ex); throw ex2; &#125; catch (RuntimeException | Error ex2) &#123; logger.error(&quot;Application exception overridden by rollback exception&quot;, ex); throw ex2; &#125; &#125; else &#123; //执行到这里，说明当前事务虽然抛出了异常，但是该异常并不会导致整个事务回滚 // We don&#x27;t roll back on this exception. // Will still roll back if TransactionStatus.isRollbackOnly() is true. try &#123; txInfo.getTransactionManager().commit(txInfo.getTransactionStatus()); &#125; catch (TransactionSystemException ex2) &#123; logger.error(&quot;Application exception overridden by commit exception&quot;, ex); ex2.initApplicationException(ex); throw ex2; &#125; catch (RuntimeException | Error ex2) &#123; logger.error(&quot;Application exception overridden by commit exception&quot;, ex); throw ex2; &#125; &#125; &#125;&#125; 判断目标方法的异常是否需要回滚？ 需要，**rollback()**。 不需要，**commit()**。 ​ 来看一下**rollback()**的逻辑。​ 1234567891011@Overridepublic final void rollback(TransactionStatus status) throws TransactionException &#123; if (status.isCompleted()) &#123; throw new IllegalTransactionStateException( &quot;Transaction is already completed - do not call commit or rollback more than once per transaction&quot;); &#125; DefaultTransactionStatus defStatus = (DefaultTransactionStatus) status; //看这个逻辑 processRollback(defStatus, false);&#125; 继续往下看**processRollback()**。​ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475private void processRollback(DefaultTransactionStatus status, boolean unexpected) &#123; try &#123; boolean unexpectedRollback = unexpected; try &#123; //事务扩展逻辑的调用点 triggerBeforeCompletion(status); //说明当前事务是一个内嵌事务 ，当前方法使用的事务是上层的事务，如果有保存点，就回滚到保存点 if (status.hasSavepoint()) &#123; if (status.isDebug()) &#123; logger.debug(&quot;Rolling back transaction to savepoint&quot;); &#125; //回滚到保存点的操作 status.rollbackToHeldSavepoint(); &#125; //条件成立：说明当前方法是一个开启了一个新的事物的方法 else if (status.isNewTransaction()) &#123; if (status.isDebug()) &#123; logger.debug(&quot;Initiating transaction rollback&quot;); &#125; //委派模式，核心逻辑 doRollback(status); &#125; else &#123; //执行到这里说明，这个事务不是当前方法开启的 （共享上层事务）|| 当前方法压根没开启事务（not_supports，never, supports） // Participating in larger transaction //说的是第一种情况：当前方法共享上层事务 if (status.hasTransaction()) &#123; //条件一：什么时候成立？当前方法共享上层事务，业务代码强制设置当前整个事务 需要回滚的话，可以通过 设置 status.isLocalRollbackOnly() = true //条件二：默认是true if (status.isLocalRollbackOnly() || isGlobalRollbackOnParticipationFailure()) &#123; if (status.isDebug()) &#123; logger.debug(&quot;Participating transaction failed - marking existing transaction as rollback-only&quot;); &#125; /* 一个共享上层事务的方法可以直接回滚嘛？不行的，需要将回滚的操作交给上层方法来做。 如何交给？设置status.isLocalRollbackOnly()=true，这样的话，线程回到上层事务提交逻辑的时候，会检查该字段，发现是true，就会执行回滚逻辑 */ //这里其实就是设置回滚字段为true 先拿到事务状态对象-&gt; 事务对象 -&gt; 连接持有者 -&gt; 设置rollbackOnly =true。 doSetRollbackOnly(status); &#125; else &#123; if (status.isDebug()) &#123; logger.debug(&quot;Participating transaction failed - letting transaction originator decide on rollback&quot;); &#125; &#125; &#125;//没有事务 else &#123; logger.debug(&quot;Should roll back transaction but cannot - no transaction available&quot;); &#125; // Unexpected rollback only matters here if we&#x27;re asked to fail early if (!isFailEarlyOnGlobalRollbackOnly()) &#123; unexpectedRollback = false; &#125; &#125; &#125; catch (RuntimeException | Error ex) &#123; //事务扩展调用点 triggerAfterCompletion(status, TransactionSynchronization.STATUS_UNKNOWN); throw ex; &#125; //事务扩展调用点 triggerAfterCompletion(status, TransactionSynchronization.STATUS_ROLLED_BACK); // Raise UnexpectedRollbackException if we had a global rollback-only marker if (unexpectedRollback) &#123; throw new UnexpectedRollbackException( &quot;Transaction rolled back because it has been marked as rollback-only&quot;); &#125; &#125; finally &#123; //看这里 cleanupAfterCompletion(status); &#125;&#125; 关注两个核心的方法**doRollback() &amp; rollbackToHeldSavepoint()**。 1234567891011121314public void rollbackToHeldSavepoint() throws TransactionException &#123; //获取事务的保存点 Object savepoint = getSavepoint(); if (savepoint == null) &#123; throw new TransactionUsageException( &quot;Cannot roll back to savepoint - no savepoint associated with current transaction&quot;); &#125; //回滚到保存点 getSavepointManager().rollbackToSavepoint(savepoint); //删除保存点 getSavepointManager().releaseSavepoint(savepoint); //清空保存点 setSavepoint(null);&#125; 1234567891011121314@Overrideprotected void doRollback(DefaultTransactionStatus status) &#123; DataSourceTransactionObject txObject = (DataSourceTransactionObject) status.getTransaction(); Connection con = txObject.getConnectionHolder().getConnection(); if (status.isDebug()) &#123; logger.debug(&quot;Rolling back JDBC transaction on Connection [&quot; + con + &quot;]&quot;); &#125; try &#123; con.rollback(); &#125; catch (SQLException ex) &#123; throw translateException(&quot;JDBC rollback&quot;, ex); &#125;&#125; 5.还原现场逻辑123456protected void cleanupTransactionInfo(@Nullable TransactionInfo txInfo) &#123; if (txInfo != null) &#123; //往下看 txInfo.restoreThreadLocalStatus(); &#125;&#125; 我们往下看 123456private void restoreThreadLocalStatus() &#123; // Use stack to restore old transaction TransactionInfo. // Will be null if none was set. //相当于事务出栈的逻辑 transactionInfoHolder.set(this.oldTransactionInfo);&#125; 这里很简单，就是让当前事务出栈。将栈内的事务设置成上一层事务。 6.提交事务逻辑123456789protected void commitTransactionAfterReturning(@Nullable TransactionInfo txInfo) &#123; if (txInfo != null &amp;&amp; txInfo.getTransactionStatus() != null) &#123; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Completing transaction for [&quot; + txInfo.getJoinpointIdentification() + &quot;]&quot;); &#125; //调用事务管理器的提交事务方法 txInfo.getTransactionManager().commit(txInfo.getTransactionStatus()/*事务状态*/); &#125;&#125; 调用了事务管理器的提交事务方法。​ 12345678910111213141516171819202122232425262728293031@Overridepublic final void commit(TransactionStatus status) throws TransactionException &#123; if (status.isCompleted()) &#123; throw new IllegalTransactionStateException( &quot;Transaction is already completed - do not call commit or rollback more than once per transaction&quot;); &#125; DefaultTransactionStatus defStatus = (DefaultTransactionStatus) status; //说明是业务强制回滚 if (defStatus.isLocalRollbackOnly()) &#123; if (defStatus.isDebug()) &#123; logger.debug(&quot;Transactional code has requested rollback&quot;); &#125; //处理回滚 processRollback(defStatus, false); return; &#125; //shouldCommitOnGlobalRollbackOnly() 默认是true //defStatus.isGlobalRollbackOnly() 其实就是defStatus -&gt;txObject-&gt;connHolder-&gt;rollbackOnly 字段 //下层事务使用上层事务的时候，想回滚，就会设置这个标记 if (!shouldCommitOnGlobalRollbackOnly() &amp;&amp; defStatus.isGlobalRollbackOnly()) &#123; if (defStatus.isDebug()) &#123; logger.debug(&quot;Global transaction is marked as rollback-only but transactional code requested commit&quot;); &#125; processRollback(defStatus, true); return; &#125; //处理提交 processCommit(defStatus);&#125; 核心的逻辑在**processCommit()**。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576private void processCommit(DefaultTransactionStatus status) throws TransactionException &#123; try &#123; boolean beforeCompletionInvoked = false; try &#123; boolean unexpectedRollback = false; prepareForCommit(status); triggerBeforeCommit(status); triggerBeforeCompletion(status); beforeCompletionInvoked = true; if (status.hasSavepoint()) &#123; if (status.isDebug()) &#123; logger.debug(&quot;Releasing transaction savepoint&quot;); &#125; unexpectedRollback = status.isGlobalRollbackOnly(); //有保存点就清理 status.releaseHeldSavepoint(); &#125; else if (status.isNewTransaction()) &#123; if (status.isDebug()) &#123; logger.debug(&quot;Initiating transaction commit&quot;); &#125; unexpectedRollback = status.isGlobalRollbackOnly(); //底层提交事务 doCommit(status); &#125; else if (isFailEarlyOnGlobalRollbackOnly()) &#123; unexpectedRollback = status.isGlobalRollbackOnly(); &#125; // Throw UnexpectedRollbackException if we have a global rollback-only // marker but still didn&#x27;t get a corresponding exception from commit. if (unexpectedRollback) &#123; throw new UnexpectedRollbackException( &quot;Transaction silently rolled back because it has been marked as rollback-only&quot;); &#125; &#125; catch (UnexpectedRollbackException ex) &#123; // can only be caused by doCommit triggerAfterCompletion(status, TransactionSynchronization.STATUS_ROLLED_BACK); throw ex; &#125; catch (TransactionException ex) &#123; // can only be caused by doCommit if (isRollbackOnCommitFailure()) &#123; doRollbackOnCommitException(status, ex); &#125; else &#123; triggerAfterCompletion(status, TransactionSynchronization.STATUS_UNKNOWN); &#125; throw ex; &#125; catch (RuntimeException | Error ex) &#123; if (!beforeCompletionInvoked) &#123; triggerBeforeCompletion(status); &#125; doRollbackOnCommitException(status, ex); throw ex; &#125; // Trigger afterCommit callbacks, with an exception thrown there // propagated to callers but the transaction still considered as committed. try &#123; triggerAfterCommit(status); &#125; finally &#123; triggerAfterCompletion(status, TransactionSynchronization.STATUS_COMMITTED); &#125; &#125; finally &#123; //清理资源，比如保存点 cleanupAfterCompletion(status); &#125;&#125; 逻辑很简单，清理保存点，提交事务，执行扩展点逻辑，清理资源。​ 我们来看一下资源的清理**cleanupAfterCompletion(status)**。​ 123456789101112131415161718192021private void cleanupAfterCompletion(DefaultTransactionStatus status) &#123; //设置当前方法的事务状态为完成状态 status.setCompleted(); //清理逻辑 线程上下文变量 &amp; 扩展点注册的东西 if (status.isNewSynchronization()) &#123; TransactionSynchronizationManager.clear(); &#125; //如果是开启的新事物，还原现场操作 最重要的就是解绑线程持有的连接 if (status.isNewTransaction()) &#123; doCleanupAfterCompletion(status.getTransaction()); &#125; //说明当前事务执行的时候，挂起了一个上层的事务 if (status.getSuspendedResources() != null) &#123; if (status.isDebug()) &#123; logger.debug(&quot;Resuming suspended transaction after completion of inner transaction&quot;); &#125; Object transaction = (status.hasTransaction() ? status.getTransaction() : null); //唤醒上层的事务 resume(transaction, (SuspendedResourcesHolder) status.getSuspendedResources()); &#125;&#125; \u0000看几个核心方法**doCleanupAfterCompletion()** &amp; **resume()**。​ 12345678910111213141516171819202122232425262728293031@Overrideprotected void doCleanupAfterCompletion(Object transaction) &#123; DataSourceTransactionObject txObject = (DataSourceTransactionObject) transaction; // Remove the connection holder from the thread, if exposed. if (txObject.isNewConnectionHolder()) &#123; TransactionSynchronizationManager.unbindResource(obtainDataSource()); &#125; // Reset connection. Connection con = txObject.getConnectionHolder().getConnection(); try &#123; if (txObject.isMustRestoreAutoCommit()) &#123; con.setAutoCommit(true); &#125; DataSourceUtils.resetConnectionAfterTransaction( con, txObject.getPreviousIsolationLevel(), txObject.isReadOnly()); &#125; catch (Throwable ex) &#123; logger.debug(&quot;Could not reset JDBC Connection after transaction&quot;, ex); &#125; if (txObject.isNewConnectionHolder()) &#123; if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Releasing JDBC Connection [&quot; + con + &quot;] after transaction&quot;); &#125; DataSourceUtils.releaseConnection(con, this.dataSource); &#125; txObject.getConnectionHolder().clear();&#125; 1234567891011121314151617181920protected final void resume(@Nullable Object transaction, @Nullable SuspendedResourcesHolder resourcesHolder) throws TransactionException &#123; if (resourcesHolder != null) &#123; Object suspendedResources = resourcesHolder.suspendedResources; if (suspendedResources != null) &#123; //重新绑定上一个事务的资源 doResume(transaction, suspendedResources); &#125; List&lt;TransactionSynchronization&gt; suspendedSynchronizations = resourcesHolder.suspendedSynchronizations; if (suspendedSynchronizations != null) &#123; //将线程上下文变量恢复为上一个事务的现场 TransactionSynchronizationManager.setActualTransactionActive(resourcesHolder.wasActive); TransactionSynchronizationManager.setCurrentTransactionIsolationLevel(resourcesHolder.isolationLevel); TransactionSynchronizationManager.setCurrentTransactionReadOnly(resourcesHolder.readOnly); TransactionSynchronizationManager.setCurrentTransactionName(resourcesHolder.name); doResumeSynchronization(suspendedSynchronizations); &#125; &#125;&#125; **doResume()**​ 1234@Overrideprotected void doResume(@Nullable Object transaction, Object suspendedResources) &#123; TransactionSynchronizationManager.bindResource(obtainDataSource(), suspendedResources);&#125; 补充一个逻辑**doSuspend()**。挂起逻辑​ 12345678910@Overrideprotected Object doSuspend(Object transaction) &#123; DataSourceTransactionObject txObject = (DataSourceTransactionObject) transaction; //将事务对象的 连接持有者设置为null，不想和上层事务共享连接资源... //当前方法有可能是不开启事务 || 要开启一个独立的事务 txObject.setConnectionHolder(null); //解绑线程上的事务，将连接持有者从tl移除掉，这样业务就不会再拿上层事务的连接资源了 return TransactionSynchronizationManager.unbindResource(obtainDataSource());&#125; 7.思考与沉淀事务的源码有一些琐碎，至此整个事务的源码其实已经分析完了，为了形成一个清晰，完整的链路，我画了一张事务源码的流程图。​ 我们再来回顾一下事务的流程。@EnableTransactionManagement 利用TransactionManagementConfigurationSelector给容器中会导入组件 给容器中导入了两个类： 1.AutoProxyRegistrar 往容器中导入了一个组件：InfrastructureAdvisorAutoProxyCreator 这个类是什么？利用后置处理器，包装对象，返回一个代理对象（增强器），代理对象执行方法的时候利用拦截器进行调用 2.ProxyTransactionManagementConfiguration 给容器中注册了三个bean： 1.事务增强器 ：advisor切面 2.事务注解解析器：解析事务注解，获取注解信息 3.事务增强器的拦截器：拦截事务相关的advisor切面 ctrl + H 查看类的继承关系：这个类是MethodInterceptor的子类 在目标方法执行的时候： 执行拦截器链条 事务拦截器： 1.先获取事务相关的属性 2.在获取事务管理器，如果事先没有添加和执行任何事务管理器，最终会从容器中拿出来一个默认的 3.执行目标方法 1.如果是异常，获取到事务管理器，利用事务管理器进行回滚 2.如果是正常执行，利用事务管理器，提交事务。 事务注解解析器和事务增强器的拦截器都包含在事务增强器中，为事务增强器赋能。\u0000 至此，事务源码分析完成，后续的篇章，我们会去分析web请求的相关流程，我是二十，熟读Java生态圈源码，精通Java高并发编程，欢迎点赞关注。","categories":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/tags/Spring/"}]},{"title":"Spring[十四]事务标签解析&创建代理对象","slug":"Spring/Spring[十四]事务标签解析&创建代理对象","date":"2022-01-11T06:11:03.124Z","updated":"2022-01-11T06:17:49.759Z","comments":true,"path":"2022/01/11/Spring/Spring[十四]事务标签解析&创建代理对象/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/Spring/Spring[%E5%8D%81%E5%9B%9B]%E4%BA%8B%E5%8A%A1%E6%A0%87%E7%AD%BE%E8%A7%A3%E6%9E%90&%E5%88%9B%E5%BB%BA%E4%BB%A3%E7%90%86%E5%AF%B9%E8%B1%A1/","excerpt":"","text":"本篇我们开始分析Spring的事务，在看源码之前，我们先来做一些铺垫。 1.JDBC&amp;Spring事务12345678910111213141516171819202122232425@Testpublic void test() &#123; Connection co = null; try &#123; co = JDBCUtils.getConnection(); co.setAutoCommit(false); String sql1 = &quot;update user_table set balance=balance-? where user=?&quot;; String sql2 = &quot;update user_table set balance=balance+? where user=?&quot;; update(co,sql1,100,&quot;AA&quot;); //System.out.println(10/0); update(co,sql2,100,&quot;BB&quot;); co.commit(); System.out.println(&quot;转账成功！&quot;); &#125; catch (SQLException e) &#123; try &#123; co.rollback(); System.out.println(&quot;转账失败！&quot;); &#125; catch (SQLException e1) &#123; e1.printStackTrace(); &#125; &#125; finally &#123; JDBCUtils.closeResource(co,null,null); &#125;&#125; 123456789101112131415public static int update(Connection co, String sql, Object... args) &#123; PreparedStatement ps = null; try &#123; ps = co.prepareStatement(sql); for (int i = 0; i &lt; args.length; i++) &#123; ps.setObject(i + 1, args[i]); &#125; return ps.executeUpdate(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; finally &#123; JDBCUtils.closeResource(null, ps, null); &#125; return 0;&#125; 接下来看一下Spring事务。 2.事务传播行为​ 所谓 spring 事务的传播属性，就是定义在存在多个事务同时存在的时候，spring 应该如何处理这些事务的行为。这些属性在 TransactionDefinition 中定义。​ 常量名称 常量解释 PROPAGATION_REQUIRED 支持当前事务，如果当前没有事务，就新建一个事务。这是最常见的选择，也是 Spring默认的事务的传播。 PROPAGATION_REQUIRES_NEW 新建事务，如果当前存在事务，把当前事务挂起。新建的事务将和被挂起的事务没有任何关系，是两个独立的事务，外层事务失败回滚之后，不能回滚内层事务执行的结果，内层事务失败抛出异常，外层事务捕获，也可以不处理回滚操作 PROPAGATION_SUPPORTS 支持当前事务，如果当前没有事务，就以非事务方式执行。 PROPAGATION_MANDATORY 支持当前事务，如果当前没有事务，就抛出异常。 PROPAGATION_NOT_SUPPORTED 以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。 PROPAGATION_NEVER 以非事务方式执行，如果当前存在事务，则抛出异常。 PROPAGATION_NESTED 如果一个活动的事务存在，则运行在一个嵌套的事务中。如果没有活动事务，则按REQUIRED 属性执行。它使用了一个单独的事务，这个事务拥有多个可以回滚的保存点。内部事务的回滚不会对外部事务造成影响。它只对DataSourceTransactionManager 事务管理器起效。 1234567891011121314151617181920PROPAGATION_REQUIREDclass ServiceA &#123; @Autowired ServiceB serviceB； @Transactional void a() &#123; .... serviceB.b(); .... &#125;&#125;class ServiceB &#123; @Transactional void b() &#123; ...... &#125;&#125; 线程执行到serviceA.a() 方法时，其实是执行的 代理serviceA对象的a方法。 执行代理serviceA对象的a方法 执行a方法的增强逻辑-&gt; 事务增强器 (环绕增强) 事务增强器会做什么事? 提取事务标签属性 检查当前线程有没有绑定 conn 数据库连接 资源？ 发现当前线程未绑定（TransactionSync…Manager#resources 是 ThreadLocal&lt;Map&lt;obj,obj&gt;&gt;，检查key:datasource 有没有数据） 因为未绑定conn资源，所以线程下一步就是 到 datasource.getConnection() 获取一个conn资源 因为新获取的conn资源的autocommit是true，所以这一步 修改 autocommit 为false，表示手动提交事务，这一步也表示 开启事务（修改conn其它 属性..） 绑定conn资源到 TransactionSync…Manager#resources，key：datasource 执行事务增强器后面的增强器.. 最后一个advice调用 target的目标方法 a() 方法 假设target a方法 需要访问数据库 执行SQL 的话，程序需要获取一个 conn 资源，到哪拿？ DataSourceUtils.getConnection(datasource) 这一步最终会拿到 事务增强器 前置增强逻辑 存放在 TransactionSync..Manager#resources 内的conn 资源 执行方法a逻辑…可能会执行一些 SQL 语句… 线程执行到这样一行代码：serviceB.b() serviceB 它是一个代理对象，因为它也使用了 @Transactional 注解了，Spring 会为它创建代理的。 执行代理serviceB对象的b方法 执行b方法的增强逻辑-&gt; 事务增强器（环绕增强） 事务增强器会做什么事? 提取事务标签属性 检查当前线程有没有绑定 conn 数据库连接 资源？发现当前线程已经绑定了 conn 数据库连接资源了 检查事务注解属性，发现自己打的propagation == REQUIRED，所以继续共享 conn 数据库链接资源 执行事务增强器后面的增强器.. 最后一个device调用 target (serviceB)的目标方法 b() 方法 假设target b方法 需要访问数据库 执行SQL 的话，程序需要获取一个 conn 资源，到哪拿？ DataSourceUtils.getConnection(datasource) 这一步最终会拿到 代理serviceA对象存放在 TransactionSync..Manager#resources 内的conn 资源 执行方法b逻辑…可能会执行一些 SQL 语句… 线程继续执行 事务增强器 环绕增强的后置逻辑 （代理serviceB.b() 方法的 后置增强） 检查发现，serviceB.b() 事务并不是 当前 b方法开启的，所以 基本不做什么事情.. 线程继续回到 目标 serviceA.a() 方法内，继续执行 执行方法a逻辑…可能会执行一些 SQL 语句… 线程继续回到 代理 serviceA.a() 方法内，继续执行 执行a方法的增强逻辑-&gt; 事务增强器 (环绕增强-后置增强逻辑) 提交事务/回滚事务 恢复连接状态 （将conn的autocommit 设置回 true…等等） 清理工作（将绑定的conn资源从TransactionSync…Manager#resources移除） conn 连接关闭 （归还连接到datasource） 1234567891011121314151617181920PROPAGATION_SUPPORTSclass ServiceA &#123; @Autowired ServiceB serviceB； @Transactional void a() &#123; .... serviceB.b(); .... &#125;&#125;class ServiceB &#123; @Transactional(propagation = SUPPORTS) void b() &#123; ...... &#125;&#125; 逻辑和上面完全一致。 12345678class ServiceA &#123; @Transactional(Propagation = SUPPORTS) void a() &#123; .... .... &#125;&#125; 线程在未绑定事务的情况下，去调用serviceA.a() 方法会发生什么呢？ 线程执行到serviceA.a() 方法时，其实是执行的 代理serviceA对象的a方法。 执行代理serviceA对象的a方法 执行a方法的增强逻辑-&gt; 事务增强器 (环绕增强) 事务增强器会做什么事? 提取事务标签属性 检查当前线程有没有绑定 conn 数据库连接 资源？ 发现当前线程未绑定（TransactionSync…Manager#resources 是 ThreadLocal&lt;Map&lt;obj,obj&gt;&gt;，检查key:datasource 有没有数据） 啥也不用做.. 执行事务增强器后面的增强器.. 最后一个advice调用 target的目标方法 a() 方法 假设target a方法 需要访问数据库 执行SQL 的话，程序需要获取一个 conn 资源，到哪拿？ DataSourceUtils.getConnection(datasource) ，因为事务增强器前置增强逻辑 并没有 向TransactionSync..Manager#resources 内绑定conn资源 因为 上一步未拿到 conn资源，所以 DataSourceUtils 通过 datasource.getConnection() 获取了一个全新的 conn 资源（注意：conn.autocommit == true,执行的每一条sql 都是一个 独立事务！！） 执行方法a逻辑…可能会执行一些 SQL 语句… 线程继续执行到代理serviceA对象的a方法 （事务增强器-后置增强逻辑） 检查发现 TrasactionSync..Manager#resources 并未绑定任何 conn 资源，所以 这一步啥也不做了… 1234567891011121314151617181920PROPAGATION_MANDATORY 很少使用..class ServiceA &#123; @Autowired ServiceB serviceB； @Transactional void a() &#123; .... serviceB.b(); .... &#125;&#125;class ServiceB &#123; @Transactional(propagation = MANDATORY) void b() &#123; ...... &#125;&#125; 如果是这样的话，情况和 PROPAGATION_REQUIRED 案例分析 完全一致。 12345678class ServiceA &#123; @Transactional(Propagation = MANDATORY) void a() &#123; .... .... &#125;&#125; 线程在未绑定事务的情况下，去调用serviceA.a() 方法会发生什么呢？ 线程执行到serviceA.a() 方法时，其实是执行的 代理serviceA对象的a方法。 执行代理serviceA对象的a方法 执行a方法的增强逻辑-&gt; 事务增强器 (环绕增强) 事务增强器会做什么事? 提取事务标签属性 检查当前线程有没有绑定 conn 数据库连接 资源？ 发现当前线程未绑定（TransactionSync…Manager#resources 是 ThreadLocal&lt;Map&lt;obj,obj&gt;&gt;，检查key:datasource 有没有数据） 直接抛出异常… … 123456789101112131415161718192021PROPAGATION_REQUIRES_NEWclass ServiceA &#123; @Autowired ServiceB serviceB； @Transactional void a() &#123; .... serviceB.b(); .... &#125;&#125;class ServiceB &#123; @Transactional(propagation = REQUIRES_NEW) void b() &#123; ...... &#125;&#125; 线程执行到serviceA.a() 方法时，其实是执行的 代理serviceA对象的a方法。 执行代理serviceA对象的a方法 执行a方法的增强逻辑-&gt; 事务增强器 (环绕增强) 事务增强器会做什么事? 提取事务标签属性 检查当前线程有没有绑定 conn 数据库连接 资源？ 发现当前线程未绑定（TransactionSync…Manager#resources 是 ThreadLocal&lt;Map&lt;obj,obj&gt;&gt;，检查key:datasource 有没有数据） 因为未绑定conn资源，所以线程下一步就是 到 datasource.getConnection() 获取一个conn资源 因为新获取的conn资源的autocommit是true，所以这一步 修改 autocommit 为false，表示手动提交事务，这一步也表示 开启事务（修改conn其它 属性..） 绑定conn资源到 TransactionSync…Manager#resources，key：datasource 执行事务增强器后面的增强器.. 最后一个advice调用 target的目标方法 a() 方法 假设target a方法 需要访问数据库 执行SQL 的话，程序需要获取一个 conn 资源，到哪拿？ DataSourceUtils.getConnection(datasource) 这一步最终会拿到 事务增强器 前置增强逻辑 存放在 TransactionSync..Manager#resources 内的conn 资源 执行方法a逻辑…可能会执行一些 SQL 语句… 线程执行到这样一行代码：serviceB.b() serviceB 它是一个代理对象，因为它也使用了 @Transactional 注解了，Spring 会为它创建代理的。 执行代理serviceB对象的b方法 执行b方法的增强逻辑-&gt; 事务增强器（环绕增强） 事务增强器会做什么事? 提取事务标签属性 检查发现当前线程已经绑定了conn资源（并且手动开启了事务..），又发现 当前方法的 传播行为：REQUIRES_NEW ，需要开启一个新的事务.. 将已经绑定的conn资源 保存到 suspand 变量内 因为 REQUIRES_NEW 不会和上层共享同一个事务，所以这一步 又到 datasource.getConnection() 获取了一个全新的 conn 数据库连接资源 因为新获取的conn资源的autocommit是true，所以这一步 修改 autocommit 为false，表示手动提交事务，这一步也表示 开启事务（修改conn其它 属性..） 绑定conn资源到 TransactionSync…Manager#resources，key：datasource 执行事务增强器后面的增强器.. 最后一个advice调用 target （serviceB）的目标方法 b() 方法 假设target b方法 需要访问数据库 执行SQL 的话，程序需要获取一个 conn 资源，到哪拿？ DataSourceUtils.getConnection(datasource) 这一步最终会拿到 事务增强器 前置增强逻辑 存放在 TransactionSync..Manager#resources 内的conn 资源 执行方法a逻辑…可能会执行一些 SQL 语句… 线程继续执行 事务增强器 环绕增强的后置逻辑 （代理serviceB.b() 方法的 后置增强） 检查发现，serviceB.b() 事务是 b方法开启的，所以 需要做一些事情了 执行b方法的增强逻辑-&gt; 事务增强器 (环绕增强-后置增强逻辑) 提交事务/回滚事务 恢复连接状态 （将conn的autocommit 设置回 true…等等） 清理工作（将绑定的conn资源从TransactionSync…Manager#resources移除） conn 连接关闭 （归还连接到datasource） 检查suspand 发现 该变量有值，需要执行 恢复现场的工作 resume() 恢复现场 将suspand 挂起的 conn 资源再次 绑定到 TransactionSync…Manager#resources 内，方便 serviceA 继续使用它的conn资源 （它自己的事务） 线程继续回到 serviceA.a() 方法内 继续执行一些sql …注意 这里它使用的 conn 是 serviceA 申请的 conn 线程继续执行 事务增强器 环绕增强的后置逻辑 （代理serviceA.a() 方法的 后置增强） 检查发现，serviceA.a() 事务是 a方法开启的，所以 需要做一些事情了 执行a方法的增强逻辑-&gt; 事务增强器 (环绕增强-后置增强逻辑) 提交事务/回滚事务 恢复连接状态 （将conn的autocommit 设置回 true…等等） 清理工作（将绑定的conn资源从TransactionSync…Manager#resources移除） conn 连接关闭 （归还连接到datasource） 3.隔离级别3.1 Mysql 隔离级别 隔离级别的值 导致的问题 Read-Uncommitted 0 导致脏读 Read-Committed 1 避免脏读，允许不可重复读和幻读 Repeatable-Read 2 避免脏读，不可重复读，允许幻读 Serializable 3 串行化读，事务只能一个一个执行，避免了脏读、不可重复读、幻读。执行效率慢，使用时慎重 脏读：一事务对数据进行了增删改，但未提交，另一事务可以读取到未提交的数据。如果第一个事务这时候回滚了，那么第二个事务就读到了脏数据。 不可重复读：一个事务中发生了两次读操作，第一次读操作和第二次操作之间，另外一个事务对数据进行了修改，这时候两次读取的数据是不一致的。 幻读：第一个事务对一定范围的数据进行批量修改，第二个事务在这个范围增加一条数据，这时候第一个事务就会丢失对新增数据的修改。 隔离级别越高，越能保证数据的完整性和一致性，但是对并发性能的影响也越大。大多数的数据库默认隔离级别为 Read Commited，比如 SqlServer、Oracle少数数据库默认隔离级别为：Repeatable Read 比如： MySQL InnoDB。​ 3.2 Spring 常量 解释 ISOLATION_DEFAULT 这是个 PlatfromTransactionManager 默认的隔离级别，使用数据库默认的事务隔离级别。另外四个与 JDBC 的隔离级别相对应。 ISOLATION_READ_UNCOMMITTED 这是事务最低的隔离级别，它允许另外一个事务可以看到这个事务未提交的数据。这种隔离级别会产生脏读，不可重复读和幻像读。 ISOLATION_READ_COMMITTED 保证一个事务修改的数据提交后才能被另外一个事务读取。另外一个事务不能读取该事务未提交的数据。 ISOLATION_REPEATABLE_READ 这种事务隔离级别可以防止脏读，不可重复读。但是可能出现幻像读。 ISOLATION_SERIALIZABLE 这是花费最高代价但是最可靠的事务隔离级别。事务被处理为顺序执行。 4.事务嵌套这里内容在实际开发中比较重要，因此我们再来做一下简单的回顾。 假设外层事务 Service A 的 Method A() 调用 内层 Service B 的 Method B() PROPAGATION_REQUIRED(Spring 默认) 如果 ServiceB.MethodB() 的事务级别定义为 PROPAGATION_REQUIRED，那么执行ServiceA.MethodA() 的时候 Spring 已经起了事务，这时调用 ServiceB.MethodB()，ServiceB.MethodB() 看到自己已经运行在 ServiceA.MethodA() 的事务内部，就不再起新的事务。 假如 ServiceB.MethodB() 运行的时候发现自己没有在事务中，他就会为自己分配一个事务。 这样，在 ServiceA.MethodA() 或者在 ServiceB.MethodB() 内的任何地方出现异常，事务都会被回滚。 PROPAGATION_REQUIRES_NEW 比如我们设计 ServiceA.MethodA() 的事务级别为 PROPAGATION_REQUIRED，ServiceB.MethodB() 的事务级别为 PROPAGATION_REQUIRES_NEW。那么当执行到 ServiceB.MethodB() 的时候，ServiceA.MethodA() 所在的事务就会挂起，ServiceB.MethodB() 会起一个新的事务，等待 ServiceB.MethodB() 的事务完成以后，它才继续执行。 他 与 PROPAGATION_REQUIRED 的 事 务 区 别 在 于 事 务 的 回 滚 程 度 了 。 因 为ServiceB.MethodB() 是新起一个事务，那么就是存在两个不同的事务。如果ServiceB.MethodB() 已 经 提 交 ， 那 么 ServiceA.MethodA() 失 败 回 滚 ，ServiceB.MethodB() 是不会回滚的。如果 ServiceB.MethodB() 失败回滚，如果他抛出的异常被 ServiceA.MethodA() 捕获，ServiceA.MethodA() 事务仍然可能提交(主要看 B 抛出的异常是不是 A 会回滚的异常)。 PROPAGATION_SUPPORTS假设 ServiceB.MethodB() 的事务级别为 PROPAGATION_SUPPORTS，那么当执行到ServiceB.MethodB()时，如果发现 ServiceA.MethodA()已经开启了一个事务，则加入当前的事务，如果发现 ServiceA.MethodA()没有开启事务，则自己也不开启事务。这种时候，内部方法的事务性完全依赖于最外层的事务。 PROPAGATION_NESTED现 在 的 情 况 就 变 得 比 较 复 杂 了 , ServiceB.MethodB() 的 事 务 属 性 被 配 置 为PROPAGATION_NESTED, 此时两者之间又将如何协作呢? ServiceB.MethodB() 如果 rollback, 那么内部事务(即 ServiceB.MethodB()) 将回滚到它执行前的 SavePoint而外部事务(即 ServiceA.MethodA()) 可以有以下两种处理方式: 捕获异常，执行异常分支逻辑 1234567void MethodA() &#123; try &#123; ServiceB.MethodB(); &#125; catch (SomeException) &#123; // 执行其他业务, 如 ServiceC.MethodC(); &#125;&#125; 这 种 方 式 也 是 嵌 套 事 务 最 有 价 值 的 地 方 , 它 起 到 了 分 支 执 行 的 效 果 , 如 果ServiceB.MethodB()失败, 那么执行 ServiceC.MethodC(), 而 ServiceB.MethodB()已经回滚到它执行之前的 SavePoint, 所以不会产生脏数据(相当于此方法从未执行过),这 种 特 性 可 以 用 在 某 些 特 殊 的 业 务 中 , 而 PROPAGATION_REQUIRED 和PROPAGATION_REQUIRES_NEW 都没有办法做到这一点。 外部事务回滚/提交 代码不做任何修改, 那么如果内部事务(ServiceB.MethodB())rollback, 那么首先 ServiceB.MethodB() 回滚到它执行之前的 SavePoint(在任何情况下都会如此), 外部事务(即 ServiceA.MethodA()) 将根据具体的配置决定自己是commit 还是 rollback。 ​ 接下来我们来看事务的源码。 5.Spring事务标签解析1234567891011121314151617&lt;bean id=&quot;dataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot;&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;$&#123;driverClassName&#125;&quot;&gt;&lt;/property&gt; &lt;property name=&quot;url&quot; value=&quot;$&#123;url&#125;&quot;&gt;&lt;/property&gt; &lt;property name=&quot;username&quot; value=&quot;$&#123;jdbc.username&#125;&quot;&gt;&lt;/property&gt; &lt;property name=&quot;password&quot; value=&quot;$&#123;jdbc.password&#125;&quot;&gt;&lt;/property&gt;&lt;/bean&gt;&lt;!-- spring中基于注解 的声明式事务控制配置步骤 1、配置事务管理器 2、开启spring对注解事务的支持 3、在需要事务支持的地方使用@Transactional注解 --&gt;&lt;!-- 配置事务管理器 --&gt;&lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;&gt;&lt;/property&gt;&lt;/bean&gt;&lt;!-- 开启spring对注解事务的支持--&gt;&lt;tx:annotation-driven&gt;&lt;/tx:annotation-driven&gt; 这是开启Spring事务需要在配置文件写的一些配置标签，主要就是最后一行，开启事务功能。​ 这个标签不是Spring的原生标签，所以需要额外的解析器来解析。在spring-tx包下的META-INF目录下，有一个spring.handlers配置文件，里面注册了事务标签的解析器。​ 我们来看下这个解析器。​ 12345678910111213141516171819202122232425/** * 这个类就是事务标签解析器，注意这里有一个init方法，加载一些当前解析器能够解析的bean标签，给解析器赋能。 */public class TxNamespaceHandler extends NamespaceHandlerSupport &#123; static final String TRANSACTION_MANAGER_ATTRIBUTE = &quot;transaction-manager&quot;; static final String DEFAULT_TRANSACTION_MANAGER_BEAN_NAME = &quot;transactionManager&quot;; static String getTransactionManagerName(Element element) &#123; return (element.hasAttribute(TRANSACTION_MANAGER_ATTRIBUTE) ? element.getAttribute(TRANSACTION_MANAGER_ATTRIBUTE) : DEFAULT_TRANSACTION_MANAGER_BEAN_NAME); &#125; @Override public void init() &#123; registerBeanDefinitionParser(&quot;advice&quot;, new TxAdviceBeanDefinitionParser()); //拉看这个标签的解析逻辑 registerBeanDefinitionParser(&quot;annotation-driven&quot;, new AnnotationDrivenBeanDefinitionParser()); registerBeanDefinitionParser(&quot;jta-transaction-manager&quot;, new JtaTransactionManagerBeanDefinitionParser()); &#125;&#125; 主要来看下**AnnotationDrivenBeanDefinitionParser**解析器的解析标签逻辑。​ 1234567891011121314151617181920@Override@Nullablepublic BeanDefinition parse(Element element, ParserContext parserContext) &#123; //向Spring容器注册一个BD：TransactionalEventListenerFactory registerTransactionalEventListenerFactory(parserContext); //从标签里面获取mode属性，一般我们不配置这个属性，所以走else的逻辑 String mode = element.getAttribute(&quot;mode&quot;); if (&quot;aspectj&quot;.equals(mode)) &#123; // mode=&quot;aspectj&quot; registerTransactionAspect(element, parserContext); if (ClassUtils.isPresent(&quot;jakarta.transaction.Transactional&quot;, getClass().getClassLoader())) &#123; registerJtaTransactionAspect(element, parserContext); &#125; &#125; else &#123; // mode=&quot;proxy&quot; 往下走 AopAutoProxyConfigurer.configureAutoProxyCreator(element, parserContext); &#125; return null;&#125; 继续往下走。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public static void configureAutoProxyCreator(Element element, ParserContext parserContext) &#123; //向Spring容器中注册BD -&gt; InfrastructureAdvisorAutoProxyCreator //BD的名称：internalAutoProxyCreator ，相当于往容器中加入了AOP的组件 AopNamespaceUtils.registerAutoProxyCreatorIfNecessary(parserContext, element); //事务切面的名称 ：org.springframework.transaction.config.internalTransactionAdvisor String txAdvisorBeanName = TransactionManagementConfigUtils.TRANSACTION_ADVISOR_BEAN_NAME; //如果容器中没有事务切面,就往容器中注册一个事务切面 if (!parserContext.getRegistry().containsBeanDefinition(txAdvisorBeanName)) &#123; //把标签包装成一个对象 Object eleSource = parserContext.extractSource(element); // Create the TransactionAttributeSource definition. RootBeanDefinition sourceDef = new RootBeanDefinition( &quot;org.springframework.transaction.annotation.AnnotationTransactionAttributeSource&quot;); sourceDef.setSource(eleSource); sourceDef.setRole(BeanDefinition.ROLE_INFRASTRUCTURE); String sourceName = parserContext.getReaderContext().registerWithGeneratedName(sourceDef); // Create the TransactionInterceptor definition. 事务增强器 RootBeanDefinition interceptorDef = new RootBeanDefinition(TransactionInterceptor.class); interceptorDef.setSource(eleSource); interceptorDef.setRole(BeanDefinition.ROLE_INFRASTRUCTURE); //往bd中注册信息 事务管理器 registerTransactionManager(element, interceptorDef); //往bd里面注册事务属性信息 interceptorDef.getPropertyValues().add(&quot;transactionAttributeSource&quot;, new RuntimeBeanReference(sourceName)); String interceptorName = parserContext.getReaderContext().registerWithGeneratedName(interceptorDef); // Create the TransactionAttributeSourceAdvisor definition. RootBeanDefinition advisorDef = new RootBeanDefinition(BeanFactoryTransactionAttributeSourceAdvisor.class); advisorDef.setSource(eleSource); advisorDef.setRole(BeanDefinition.ROLE_INFRASTRUCTURE); advisorDef.getPropertyValues().add(&quot;transactionAttributeSource&quot;, new RuntimeBeanReference(sourceName)); advisorDef.getPropertyValues().add(&quot;adviceBeanName&quot;, interceptorName); if (element.hasAttribute(&quot;order&quot;)) &#123; advisorDef.getPropertyValues().add(&quot;order&quot;, element.getAttribute(&quot;order&quot;)); &#125; parserContext.getRegistry().registerBeanDefinition(txAdvisorBeanName, advisorDef); CompositeComponentDefinition compositeDef = new CompositeComponentDefinition(element.getTagName(), eleSource); compositeDef.addNestedComponent(new BeanComponentDefinition(sourceDef, sourceName)); compositeDef.addNestedComponent(new BeanComponentDefinition(interceptorDef, interceptorName)); compositeDef.addNestedComponent(new BeanComponentDefinition(advisorDef, txAdvisorBeanName)); parserContext.registerComponent(compositeDef); &#125;&#125; 主要就是往容器中注册了几个bean。我们通过一张图来看一下。​ ​ 6.创建代理对象上面说到，解析事务标签的时候会往Spring容器中注入一个类**InfrastructureAdvisorAutoProxyCreator**。我们来看一下这个类的继承关系。​ \u0000这个是是一个抽象自动代理创建器，也是一个后置处理器。这里就和AOP的逻辑关联起来了。我们直接看AOP创建代理对象的后置处理器的方法。​ 1234567891011121314@Overridepublic Object postProcessAfterInitialization(@Nullable Object bean/*spring容器完全初始化完毕的对象*/, String beanName/*bean名称*/) &#123; if (bean != null) &#123; /*获取缓存建：大部分情况下都是beanName，如果是工厂bean对象，也有可能是 &amp; */ Object cacheKey = getCacheKey(bean.getClass(), beanName); /*防止重复代理某个bean实例*/ if (this.earlyProxyReferences.remove(cacheKey) != bean) &#123; /*判断是否需要包装 AOP操作的入口*/ return wrapIfNecessary(bean, beanName, cacheKey); &#125; &#125; return bean;&#125; 这里的逻辑前面已经解释过，直接往下走。​ 123456789101112131415161718192021222324252627282930313233343536373839404142protected Object wrapIfNecessary(Object bean, String beanName, Object cacheKey) &#123; /*条件一般不成立，因为正常情况下很少使用TargetSourceCreator 去创建对象。BeforeInstantiation阶段*/ if (StringUtils.hasLength(beanName) &amp;&amp; this.targetSourcedBeans.contains(beanName)) &#123; return bean; &#125; /* * 如果当前bean对象不需要增强处理 * 判断是在BeforeInstantiation阶段阶段做的 */ if (Boolean.FALSE.equals(this.advisedBeans.get(cacheKey))) &#123; return bean; &#125; /* * 条件一：判断当前bean类型是否是基础框架类型的实例，不能被增强 * 条件二：判断当前beanname是否是是忽略的bean，不需要被增强 */ if (isInfrastructureClass(bean.getClass()) || shouldSkip(bean.getClass(), beanName)) &#123; //进入这里表示不需要增强 this.advisedBeans.put(cacheKey, Boolean.FALSE); //直接返回上层 return bean; &#125; //查找适合当前类的通知 非常重要 ！！！ Object[] specificInterceptors = getAdvicesAndAdvisorsForBean(bean.getClass(), beanName, null); //判断当前查询出来的通知是不是空，如果不是空，说明走增强逻辑 if (specificInterceptors != DO_NOT_PROXY) &#123; //记得放在缓存true this.advisedBeans.put(cacheKey, Boolean.TRUE); /*真正去创建代理对象*/ Object proxy = createProxy( bean.getClass()/*目标对象*/, beanName/*beanName*/, specificInterceptors/*匹配当前目标对象class的拦截器*/, new SingletonTargetSource(bean)/*把当前bean进行了一个封装*/); //保存代理对象类型 this.proxyTypes.put(cacheKey, proxy.getClass()); //返回代理对象 return proxy; &#125; //执行到这里说明没查到这个类相关的通知，没法增强，直接返回 this.advisedBeans.put(cacheKey, Boolean.FALSE); return bean;&#125; 看一下**getAdvicesAndAdvisorsForBean()**。​ 12345678910111213@Override@Nullableprotected Object[] getAdvicesAndAdvisorsForBean( Class&lt;?&gt; beanClass, String beanName, @Nullable TargetSource targetSource) &#123; //查询合适当前类型的通知 List&lt;Advisor&gt; advisors = findEligibleAdvisors(beanClass, beanName); //通知为空返回空 if (advisors.isEmpty()) &#123; return DO_NOT_PROXY; &#125; //否则转成一个数组返回 return advisors.toArray();&#125; 继续往下。 123456789101112protected List&lt;Advisor&gt; findEligibleAdvisors(Class&lt;?&gt; beanClass, String beanName) &#123; /*获取到当前项目里面所有可以使用的增强器*/ List&lt;Advisor&gt; candidateAdvisors = findCandidateAdvisors(); /*将上一步获取到的全部增强器进行过滤，留下适合当前类的*/ List&lt;Advisor&gt; eligibleAdvisors = findAdvisorsThatCanApply(candidateAdvisors, beanClass, beanName); /*在这一步，会在index为0 的位置添加一个增强器*/ extendAdvisors(eligibleAdvisors); if (!eligibleAdvisors.isEmpty()) &#123; eligibleAdvisors = sortAdvisors(eligibleAdvisors); &#125; return eligibleAdvisors;&#125; 继续往下看过滤增强器的逻辑。 12345678910111213141516171819202122232425262728public static List&lt;Advisor&gt; findAdvisorsThatCanApply(List&lt;Advisor&gt; candidateAdvisors, Class&lt;?&gt; clazz) &#123; /*如果这个类全部可用的增强器为空，直接返回*/ if (candidateAdvisors.isEmpty()) &#123; return candidateAdvisors; &#125; //匹配当前class 的 advisor 信息 List&lt;Advisor&gt; eligibleAdvisors = new ArrayList&lt;&gt;(); //不考虑音阶增强 for (Advisor candidate : candidateAdvisors) &#123; if (candidate instanceof IntroductionAdvisor &amp;&amp; canApply(candidate, clazz)) &#123; eligibleAdvisors.add(candidate); &#125; &#125; //假设 值为false boolean hasIntroductions = !eligibleAdvisors.isEmpty(); for (Advisor candidate : candidateAdvisors) &#123; if (candidate instanceof IntroductionAdvisor) &#123; // already processed continue; &#125; //判断当前增强器是否匹配class if (canApply(candidate, clazz, hasIntroductions)) &#123; eligibleAdvisors.add(candidate); &#125; &#125; //返回的都是匹配当前class的advisor return eligibleAdvisors;&#125; 看这个重载的方法**canApply()**。​ 123456789101112131415public static boolean canApply(Advisor advisor, Class&lt;?&gt; targetClass, boolean hasIntroductions) &#123; if (advisor instanceof IntroductionAdvisor) &#123; return ((IntroductionAdvisor) advisor).getClassFilter().matches(targetClass); &#125; //大多数情况下是走这里，因为创建的增强器是 InstantiationModelAwarePointcutAdvisorImpl else if (advisor instanceof PointcutAdvisor) &#123; PointcutAdvisor pca = (PointcutAdvisor) advisor; //方法重载 return canApply(pca.getPointcut(), targetClass, hasIntroductions); &#125; else &#123; // It doesn&#x27;t have a pointcut so we assume it applies. return true; &#125;&#125; Spring事务导入到容器中的增强器是哪一个呢？回顾一下上面的图，**BeanFactoryTransactionAttributeSourceAdvisor**。​ 我们直接来到这个bean。​ 这个时候我们在来看**canApply()**。​ 123456789101112131415161718192021222324252627282930313233343536373839404142434445/*判断当前切点是否匹配当前class*/public static boolean canApply(Pointcut pc, Class&lt;?&gt; targetClass, boolean hasIntroductions) &#123; Assert.notNull(pc, &quot;Pointcut must not be null&quot;); //条件成立：说明当前class就不满足切点的定义 ，直接返回，因为后面是判断方法匹配的逻辑，直接返回 if (!pc.getClassFilter().matches(targetClass)) &#123; return false; &#125; //事务逻辑：BeanFactoryTransactionAttributeSourceAdvisor 里面有一个连接点。这个时候获取到的实际上就是事务增强器里面的连接点的方法匹配器。 //其他逻辑不用管，我们直接跳到方法匹配的逻辑。 //获取方法匹配器 MethodMatcher methodMatcher = pc.getMethodMatcher(); //如果是true，直接返回true，因为true不做判断，直接匹配所有方法 if (methodMatcher == MethodMatcher.TRUE) &#123; // No need to iterate the methods if we&#x27;re matching any method anyway... return true; &#125; //skip IntroductionAwareMethodMatcher introductionAwareMethodMatcher = null; if (methodMatcher instanceof IntroductionAwareMethodMatcher) &#123; introductionAwareMethodMatcher = (IntroductionAwareMethodMatcher) methodMatcher; &#125; //保存当前目标对象clazz + 目标对象 父类 爷爷类 ... 的接口 + 自身实现的接口 Set&lt;Class&lt;?&gt;&gt; classes = new LinkedHashSet&lt;&gt;(); //判断目标对象是不是代理对象，确保classes内存储的数据包括目标对象的class，而不是代理类class if (!Proxy.isProxyClass(targetClass)) &#123; classes.add(ClassUtils.getUserClass(targetClass)); &#125; classes.addAll(ClassUtils.getAllInterfacesForClassAsSet(targetClass)); //遍历classes，获取当前class定义的method，整个for循环会检查当前目标clazz 上级接口的所有方法 //看看是否会被方法匹配器匹配，如果有一个方法匹配成功，就说明目标class需要被AOP代理增强 for (Class&lt;?&gt; clazz : classes) &#123; Method[] methods = ReflectionUtils.getAllDeclaredMethods(clazz); for (Method method : methods) &#123; //事务注释：方法匹配：TransactionAttributeSourcePointcut 的 方法实现。 if (introductionAwareMethodMatcher != null ? introductionAwareMethodMatcher.matches(method, targetClass, hasIntroductions) : methodMatcher.matches(method, targetClass)) &#123; return true; &#125; &#125; &#125; //执行到这里，说明当前类的所有方法都没有匹配成功，当前类不需要AOP的增强。 return false;&#125; \u0000这里会调用到**TransactionAttributeSourcePointcut**类的**matches()**。我们来看一下。​ 12345@Overridepublic boolean matches(Method method, Class&lt;?&gt; targetClass) &#123; TransactionAttributeSource tas = getTransactionAttributeSource(); return (tas == null || tas.getTransactionAttribute(method, targetClass) != null);&#125; 继续往下看 **getTransactionAttribute()**， 来到了 **AbstractFallbackTransactionAttributeSource**。\u0000 12345678910111213141516171819202122232425262728293031323334353637383940414243@Override@Nullablepublic TransactionAttribute getTransactionAttribute(Method method, @Nullable Class&lt;?&gt; targetClass) &#123; if (method.getDeclaringClass() == Object.class) &#123; return null; &#125; //缓存的逻辑 // First, see if we have a cached value. Object cacheKey = getCacheKey(method, targetClass); TransactionAttribute cached = this.attributeCache.get(cacheKey); if (cached != null) &#123; // Value will either be canonical value indicating there is no transaction attribute, // or an actual transaction attribute. if (cached == NULL_TRANSACTION_ATTRIBUTE) &#123; return null; &#125; else &#123; return cached; &#125; &#125; else &#123; //真正去执行的逻辑。 // We need to work it out. 解析事务属性注解，获取事务属性信息。 TransactionAttribute txAttr = computeTransactionAttribute(method, targetClass); // Put it in the cache. 加缓存。 if (txAttr == null) &#123; this.attributeCache.put(cacheKey, NULL_TRANSACTION_ATTRIBUTE); &#125; else &#123; String methodIdentification = ClassUtils.getQualifiedMethodName(method, targetClass); if (txAttr instanceof DefaultTransactionAttribute) &#123; DefaultTransactionAttribute dta = (DefaultTransactionAttribute) txAttr; dta.setDescriptor(methodIdentification); dta.resolveAttributeStrings(this.embeddedValueResolver); &#125; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Adding transactional method &#x27;&quot; + methodIdentification + &quot;&#x27; with attribute: &quot; + txAttr); &#125; this.attributeCache.put(cacheKey, txAttr); &#125; return txAttr; &#125;&#125; 查找事物注解信息并加缓存。​ **computeTransactionAttribute()**\u0000 123456789101112131415161718192021222324252627282930313233343536373839404142434445@Nullableprotected TransactionAttribute computeTransactionAttribute(Method method, @Nullable Class&lt;?&gt; targetClass) &#123; // Don&#x27;t allow no-public methods as required. if (allowPublicMethodsOnly() &amp;&amp; !Modifier.isPublic(method.getModifiers())) &#123; return null; &#125; //获取目标类上的method，因为@Transactional注解可能是标记在接口上的 // The method may be on an interface, but we need attributes from the target class. // If the target class is null, the method will be unchanged. Method specificMethod = AopUtils.getMostSpecificMethod(method, targetClass); //获取目标类上的方法的注解信息。 // First try is the method in the target class. TransactionAttribute txAttr = findTransactionAttribute(specificMethod); if (txAttr != null) &#123; //获取到了就返回 return txAttr; &#125; // Second try is the transaction attribute on the target class. //到实现类的方法上去找 txAttr = findTransactionAttribute(specificMethod.getDeclaringClass()); if (txAttr != null &amp;&amp; ClassUtils.isUserLevelMethod(method)) &#123; //找到则返回 return txAttr; &#125; //此时说明注解是打在了接口上，到目标接口上提取method信息 if (specificMethod != method) &#123; // Fallback is to look at the original method. txAttr = findTransactionAttribute(method); if (txAttr != null) &#123; //找到则返回 return txAttr; &#125; //此时说明注解可能打在了目标接口的方法上，到接口的方法上提取注解信息 // Last fallback is the class of the original method. txAttr = findTransactionAttribute(method.getDeclaringClass()); if (txAttr != null &amp;&amp; ClassUtils.isUserLevelMethod(method)) &#123; return txAttr; &#125; &#125; //说明method并没有定义事务注解信息，不需要事务支持。 return null;&#125; 这个方法的主要逻辑就是获取到类，接口或者方法上的事务注解信息。我们来看一下具体的解析事务注解的逻辑。**findTransactionAttribute()**。​ 123456@Override@Nullableprotected TransactionAttribute findTransactionAttribute(Method method) &#123; return determineTransactionAttribute(method);&#125; 我们先来看这个类的属性，有一个事务注解解析器集合，这个集合是何时赋值的呢？是在创建这个类的时候，看构造器，我们只需要关注 **SpringTransactionAnnotationParser** 整一个解析器即可。 123456789101112131415161718private final Set&lt;TransactionAnnotationParser&gt; annotationParsers; public AnnotationTransactionAttributeSource(boolean publicMethodsOnly) &#123; this.publicMethodsOnly = publicMethodsOnly; if (jta12Present || ejb3Present) &#123; this.annotationParsers = new LinkedHashSet&lt;&gt;(4); this.annotationParsers.add(new SpringTransactionAnnotationParser()); if (jta12Present) &#123; this.annotationParsers.add(new JtaTransactionAnnotationParser()); &#125; if (ejb3Present) &#123; this.annotationParsers.add(new Ejb3TransactionAnnotationParser()); &#125; &#125; else &#123; this.annotationParsers = Collections.singleton(new SpringTransactionAnnotationParser()); &#125; &#125; 我们再回到提取事务注解信息的逻辑。**determineTransactionAttribute()**​ 1234567891011@Nullableprotected TransactionAttribute determineTransactionAttribute(AnnotatedElement element) &#123; for (TransactionAnnotationParser parser : this.annotationParsers) &#123; //我们来看 SpringTransactionAnnotationParser 里面的逻辑 TransactionAttribute attr = parser.parseTransactionAnnotation(element); if (attr != null) &#123; return attr; &#125; &#125; return null;&#125; 这里是循环所有的解析器，提取解析事务注解信息，我们来看 **SpringTransactionAnnotationParser** 里面的逻辑。​ 1234567891011121314@Override@Nullablepublic TransactionAttribute parseTransactionAnnotation(AnnotatedElement element) &#123; //从类或者方法上查找@Transactional这个注解 AnnotationAttributes attributes = AnnotatedElementUtils.findMergedAnnotationAttributes( element, Transactional.class, false, false); if (attributes != null) &#123; //解析注解阶段为事务属性TransactionAttribute return parseTransactionAnnotation(attributes); &#125; else &#123; return null; &#125;&#125; 首先是从类或者方法上查找到注解，然后通过**parseTransactionAnnotation()**解析注解为**TransactionAttribute**。​ 12345678910111213141516171819202122232425262728293031323334353637383940/** * 解析事务注解 * @param attributes * @return */protected TransactionAttribute parseTransactionAnnotation(AnnotationAttributes attributes) &#123; RuleBasedTransactionAttribute rbta = new RuleBasedTransactionAttribute(); Propagation propagation = attributes.getEnum(&quot;propagation&quot;); rbta.setPropagationBehavior(propagation.value()); Isolation isolation = attributes.getEnum(&quot;isolation&quot;); rbta.setIsolationLevel(isolation.value()); rbta.setTimeout(attributes.getNumber(&quot;timeout&quot;).intValue()); String timeoutString = attributes.getString(&quot;timeoutString&quot;); Assert.isTrue(!StringUtils.hasText(timeoutString) || rbta.getTimeout() &lt; 0, &quot;Specify &#x27;timeout&#x27; or &#x27;timeoutString&#x27;, not both&quot;); rbta.setTimeoutString(timeoutString); rbta.setReadOnly(attributes.getBoolean(&quot;readOnly&quot;)); rbta.setQualifier(attributes.getString(&quot;value&quot;)); rbta.setLabels(Arrays.asList(attributes.getStringArray(&quot;label&quot;))); List&lt;RollbackRuleAttribute&gt; rollbackRules = new ArrayList&lt;&gt;(); for (Class&lt;?&gt; rbRule : attributes.getClassArray(&quot;rollbackFor&quot;)) &#123; rollbackRules.add(new RollbackRuleAttribute(rbRule)); &#125; for (String rbRule : attributes.getStringArray(&quot;rollbackForClassName&quot;)) &#123; rollbackRules.add(new RollbackRuleAttribute(rbRule)); &#125; for (Class&lt;?&gt; rbRule : attributes.getClassArray(&quot;noRollbackFor&quot;)) &#123; rollbackRules.add(new NoRollbackRuleAttribute(rbRule)); &#125; for (String rbRule : attributes.getStringArray(&quot;noRollbackForClassName&quot;)) &#123; rollbackRules.add(new NoRollbackRuleAttribute(rbRule)); &#125; rbta.setRollbackRules(rollbackRules); return rbta;&#125; 这里就是具体解析事务注解信息的逻辑。​ 阶段性梳理一下，这里我们匹配到了事务相关的增强器，接下来我们要去为当前加了Transaction注解的bean创建代理对象。 至此，我们分析完了解析事务标签，创建打了Transaction注解的bean创建代理对象的源码流程分析，接下来就是分析，需要被事务增强的目标方法执行过程中，事务增强器是如何对目标方法加上事务的。 ​","categories":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/tags/Spring/"}]},{"title":"Spring[十三]Ioc整合Aop创建代理对象","slug":"Spring/Spring[十三]Ioc整合Aop创建代理对象","date":"2022-01-11T06:10:54.686Z","updated":"2022-01-11T06:17:27.836Z","comments":true,"path":"2022/01/11/Spring/Spring[十三]Ioc整合Aop创建代理对象/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/Spring/Spring[%E5%8D%81%E4%B8%89]Ioc%E6%95%B4%E5%90%88Aop%E5%88%9B%E5%BB%BA%E4%BB%A3%E7%90%86%E5%AF%B9%E8%B1%A1/","excerpt":"","text":"临时补充一篇内容，昨晚忽然看源码发现忽略了这Ioc整合Aop创建代理对象的过程，本篇我们来补充一下核心的逻辑。 1.解析xml标签之前我们说过了注解版开发AOP功能，如果是xml版的需要在配置文件配置_**&lt;aop:aspectj-autoproxy /&gt;**_。​ 我们先找到这个标签的解析器**AspectJAutoProxyBeanDefinitionParser**。​ 123456789101112@Override@Nullable/** * @param element 包装 &lt;aop:aspectj-autoproxy /&gt;标签数据 * @param parserContext 它持有一个 readerContext ， readerContext里面 有持有一个 registry ，也就是 bf。 */public BeanDefinition parse(Element element, ParserContext parserContext) &#123; /*解析标签，创建一个自动代理创建器*/ AopNamespaceUtils.registerAspectJAnnotationAutoProxyCreatorIfNecessary(parserContext, element); extendBeanDefinition(element, parserContext); return null;&#125; \u0000\u0000这个方法就是解析标签，创建抽象自动代理创建器，注册到容器中，是不是和前面的流程很像？​ 12345678910public static void registerAspectJAnnotationAutoProxyCreatorIfNecessary( ParserContext parserContext, Element sourceElement) &#123; /*拿到 bf ， 包装标签*/ BeanDefinition beanDefinition = AopConfigUtils.registerAspectJAnnotationAutoProxyCreatorIfNecessary( parserContext.getRegistry(), parserContext.extractSource(sourceElement)); /*执行到这里，spring容器中已经有了aop相关的bd信息，接下来的逻辑属于扩展 。 * 这里的主要逻辑就是对aop标签上可配置的属性进行解析。*/ useClassProxyingIfNecessary(parserContext.getRegistry(), sourceElement); registerComponentIfNecessary(beanDefinition, parserContext);&#125; 先看一下解析标签属性的逻辑**useClassProxyingIfNecessary()** &amp; **registerComponentIfNecessary()**。​ 123456789101112131415private static void useClassProxyingIfNecessary(BeanDefinitionRegistry registry, @Nullable Element sourceElement) &#123; /*属于扩展逻辑，判断aop的标签上有没有配置proxy-target-class属性，这个属性默认是关闭的，如果开启的话，目标对象不管有没有实现接口，都会使用cglib的代理方式*/ if (sourceElement != null) &#123; boolean proxyTargetClass = Boolean.parseBoolean(sourceElement.getAttribute(PROXY_TARGET_CLASS_ATTRIBUTE)); if (proxyTargetClass) &#123; /*这里的逻辑就是假如你配置了这个属性是true，就会拿到bd信息，往bd信息里面在添加一个属性，设置成true。*/ AopConfigUtils.forceAutoProxyCreatorToUseClassProxying(registry); &#125; /*exposeProxy：也是可以在aop标签配置的属性，就是判断当前代理对象是否要暴露在aop上下文，方便代理对象内部的真实对象拿到代理对象。*/ boolean exposeProxy = Boolean.parseBoolean(sourceElement.getAttribute(EXPOSE_PROXY_ATTRIBUTE)); if (exposeProxy) &#123; AopConfigUtils.forceAutoProxyCreatorToExposeProxy(registry); &#125; &#125;&#125; 这个方法主要就是决定用哪种方式创建代理对象。​ 123456private static void registerComponentIfNecessary(@Nullable BeanDefinition beanDefinition, ParserContext parserContext) &#123; if (beanDefinition != null) &#123; parserContext.registerComponent( new BeanComponentDefinition(beanDefinition, AopConfigUtils.AUTO_PROXY_CREATOR_BEAN_NAME)); &#125;&#125; 这个就是判断是不是需要将抽象自动代理创建器注册到解析器的上下文。​ 2.抽象自动代理创建器我们接下来回归主线，来看**AopConfigUtils.registerAspectJAnnotationAutoProxyCreatorIfNecessary()**的逻辑。​ 12345678910@Nullablepublic static BeanDefinition registerAspectJAnnotationAutoProxyCreatorIfNecessary( BeanDefinitionRegistry registry, @Nullable Object source) &#123; /* * 参数一：固定类型 * 参数二：spring容器 * 参数三：标签 * */ return registerOrEscalateApcAsRequired(AnnotationAwareAspectJAutoProxyCreator.class, registry, source);&#125; 再往下跟​ 1234567891011121314151617181920212223242526@Nullableprivate static BeanDefinition registerOrEscalateApcAsRequired( Class&lt;?&gt; cls, BeanDefinitionRegistry registry, @Nullable Object source) &#123; Assert.notNull(registry, &quot;BeanDefinitionRegistry must not be null&quot;); /*判断容器里面有没有这个名字的bean，如果有的话就拿出来*/ if (registry.containsBeanDefinition(AUTO_PROXY_CREATOR_BEAN_NAME)) &#123; BeanDefinition apcDefinition = registry.getBeanDefinition(AUTO_PROXY_CREATOR_BEAN_NAME); if (!cls.getName().equals(apcDefinition.getBeanClassName())) &#123; int currentPriority = findPriorityForClass(apcDefinition.getBeanClassName()); int requiredPriority = findPriorityForClass(cls); if (currentPriority &lt; requiredPriority) &#123; apcDefinition.setBeanClassName(cls.getName()); &#125; &#125; return null; &#125; /*通常情况下，其实走不到上面的逻辑，除非自己手写了aop*/ /*创建一个bd，并且注册到容器中*/ RootBeanDefinition beanDefinition = new RootBeanDefinition(cls); beanDefinition.setSource(source); beanDefinition.getPropertyValues().add(&quot;order&quot;, Ordered.HIGHEST_PRECEDENCE); beanDefinition.setRole(BeanDefinition.ROLE_INFRASTRUCTURE); registry.registerBeanDefinition(AUTO_PROXY_CREATOR_BEAN_NAME, beanDefinition); return beanDefinition;&#125; 这里就是给容器中注册了一个**beanDefinition**，这个**beanDefinition**就是**AbstractAutoProxyCreator**。​ 前面在注解流程里面我们分析了，Ioc在通过**getBean()**创建单实例bean对象的时候，执行到**initializeBean()**的时候，会执行bean的后置处理器，我们再来回顾一下这里的逻辑。​ 12345678910111213141516171819202122232425262728293031/** 初始化给定的 bean 实例，应用工厂回调以及 init 方法和 bean 后处理器。* 从createBean调用传统定义的 bean，从initializeBean调用现有 bean 实例。* */protected Object initializeBean(String beanName, Object bean, @Nullable RootBeanDefinition mbd) &#123; /*检查当前bean是否实现了aware接口，再具体判断实现的哪个aware接口，做一些赋能操作。*/ invokeAwareMethods(beanName, bean); Object wrappedBean = bean; if (mbd == null || !mbd.isSynthetic()) &#123; /*初始化之前，后置处理器的调用点*/ wrappedBean = applyBeanPostProcessorsBeforeInitialization(wrappedBean, beanName); &#125; try &#123; /*执行初始化方法*/ invokeInitMethods(beanName, wrappedBean, mbd); &#125; catch (Throwable ex) &#123; throw new BeanCreationException( (mbd != null ? mbd.getResourceDescription() : null), beanName, &quot;Invocation of init method failed&quot;, ex); &#125; if (mbd == null || !mbd.isSynthetic()) &#123; /*初始化后的后置处理器执行点*/ /*典型应用：AOP的具体实现*/ wrappedBean = applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName); &#125; return wrappedBean;&#125; \u0000通常都是在执行**invokeInitMethods()**,之后的后置处理器的after方法返回一个代理对象，我们继续往下走。​ 1234567891011121314151617@Overridepublic Object applyBeanPostProcessorsAfterInitialization(Object existingBean, String beanName) throws BeansException &#123; Object result = existingBean; for (BeanPostProcessor processor : getBeanPostProcessors()) &#123; Object current = processor.postProcessAfterInitialization(result, beanName); /*注意： * 一旦某个后置处理器返回的结果为空 * 就返回上一个后置处理器的结果，后面的后置处理器方法不在执行*/ if (current == null) &#123; return result; &#125; result = current; &#125; return result;&#125; 这里是后处理器的逻辑。在这里就会调用到我们前面注册到容器中的**AbstractAutoProxyCreator**。​ 1234567891011121314@Overridepublic Object postProcessAfterInitialization(@Nullable Object bean/*spring容器完全初始化完毕的对象*/, String beanName/*bean名称*/) &#123; if (bean != null) &#123; /*获取缓存建：大部分情况下都是beanName，如果是工厂bean对象，也有可能是 &amp; */ Object cacheKey = getCacheKey(bean.getClass(), beanName); /*防止重复代理某个bean实例*/ if (this.earlyProxyReferences.remove(cacheKey) != bean) &#123; /*判断是否需要包装 AOP操作的入口*/ return wrapIfNecessary(bean, beanName, cacheKey); &#125; &#125; return bean;&#125; 构建缓存key 判断如果**earlyProxyReferences**里面移除的对象和当前完全初始化好的对象不是同一个，说明什么？说明有其他地方通过**FactoryBean**的**getObject()**创建了当前bean的代理对象，所以需要移除。 判断是否需要包装**wrapIfNecessary(bean, beanName, cacheKey)** ​ 3.earlyProxyReferences我们先来分析下这个属性 1private final Map&lt;Object, Object&gt; earlyProxyReferences = new ConcurrentHashMap&lt;&gt;(16); 假设A，B两个类现在发生了循环依赖，创建A的时候发现需要B对象，然后A会把自己的代理对象放到三级缓存，然后递归去创建B对象。​ 1addSingletonFactory(beanName, () -&gt; getEarlyBeanReference(beanName, mbd, bean)) 就是这行代码，前面我们是有讲过的，然后我们继续往下跟。​ 1234567891011121314151617181920/** * 获取早期bean实例对象的引用，用来解决循环依赖。 * 这里说明了一个问题：为什么是三级缓存不是二级缓存。 * Obtain a reference for early access to the specified bean, * typically for the purpose of resolving a circular reference. * @param beanName the name of the bean (for error handling purposes) * @param mbd the merged bean definition for the bean * @param bean the raw bean instance * @return the object to expose as bean reference */protected Object getEarlyBeanReference(String beanName, RootBeanDefinition mbd, Object bean) &#123; Object exposedObject = bean; if (!mbd.isSynthetic() &amp;&amp; hasInstantiationAwareBeanPostProcessors()) &#123; for (SmartInstantiationAwareBeanPostProcessor bp : getBeanPostProcessorCache().smartInstantiationAware) &#123; /*判断要返回的早期单实例对象是否需要增强，如果需要增强，就进行包装，返回包装好的对象*/ exposedObject = bp.getEarlyBeanReference(exposedObject, beanName); &#125; &#125; return exposedObject;&#125; 这里面会遍历所有的**SmartInstantiationAwareBeanPostProcessor**，判断是否需要返回增强的对象。​ 而我们的**AbstractAutoProxyCreator**恰恰实现了**SmartInstantiationAwareBeanPostProcessor**。\u0000所以**getEarlyBeanReference()**会执行**AbstractAutoProxyCreator**里面的。​ 123456@Overridepublic Object getEarlyBeanReference(Object bean, String beanName) &#123; Object cacheKey = getCacheKey(bean.getClass(), beanName); this.earlyProxyReferences.put(cacheKey, bean); return wrapIfNecessary(bean, beanName, cacheKey);&#125; 这里会把目标对象本身A放入到缓存中，返回A的早期代理对象。​ 在我们递归创建B的的时候，为B进行属性赋值的时候，回去从缓存拿A。​ 1234567891011121314151617/*从一级缓存拿*/singletonObject = this.singletonObjects.get(beanName);if (singletonObject == null) &#123; /*从二级缓存拿*/ singletonObject = this.earlySingletonObjects.get(beanName); if (singletonObject == null) &#123; /*从三级缓存拿*/ ObjectFactory&lt;?&gt; singletonFactory = this.singletonFactories.get(beanName); /*条件成立：说明第三级缓存有数据。这里就涉及到了缓存的升级 ，很简单 ，从三级挪到二级 ，再反手干掉三级的。*/ if (singletonFactory != null) &#123; singletonObject = singletonFactory.getObject(); this.earlySingletonObjects.put(beanName, singletonObject); this.singletonFactories.remove(beanName); &#125; &#125;&#125; ​ 这里第二次调用到了**singletonFactory.getObject()**，（此时是B创建过程中获取A），然后又会走到**getEarlyBeanReference()**。​ 123456@Overridepublic Object getEarlyBeanReference(Object bean, String beanName) &#123; Object cacheKey = getCacheKey(bean.getClass(), beanName); this.earlyProxyReferences.put(cacheKey, bean); return wrapIfNecessary(bean, beanName, cacheKey);&#125; 这个时候，会将A的代理对象放到缓存，返回A的代理对象。​ 所以当我们递归创建B后，回头用B为A属性赋值之后，执行A的初始化方法，就会走到抽象自动代理创建器的后置处理器逻辑，然后再来看这个方法​ 1234567891011121314@Overridepublic Object postProcessAfterInitialization(@Nullable Object bean/*A的代理对象*/, String beanName/*bean名称*/) &#123; if (bean != null) &#123; /*获取缓存建：大部分情况下都是beanName，如果是工厂bean对象，也有可能是 &amp; */ Object cacheKey = getCacheKey(bean.getClass(), beanName); /*防止重复代理某个bean实例*/ if (this.earlyProxyReferences.remove(cacheKey)/*A本身*/ != bean) &#123; /*判断是否需要包装 AOP操作的入口*/ return wrapIfNecessary(bean, beanName, cacheKey); &#125; &#125; return bean;&#125; **earlyProxyReferences.remove(cacheKey) != bean**，因为此时缓存里面的是A的代理对象，传进来的也是A的代理对象，所以判断相等，直接返回，不需要再次创建A的代理对象。​ 至此，这里就解释清楚了。​ 接下来，我们来看代理对象的创建过程。**wrapIfNecessary()**​ 4.是否需要创建代理对象123456789101112131415161718192021222324252627282930313233343536373839404142protected Object wrapIfNecessary(Object bean, String beanName, Object cacheKey) &#123; /*条件一般不成立，因为正常情况下很少使用TargetSourceCreator 去创建对象。BeforeInstantiation阶段*/ if (StringUtils.hasLength(beanName) &amp;&amp; this.targetSourcedBeans.contains(beanName)) &#123; return bean; &#125; /* * 如果当前bean对象不需要增强处理 * 判断是在BeforeInstantiation阶段阶段做的 */ if (Boolean.FALSE.equals(this.advisedBeans.get(cacheKey))) &#123; return bean; &#125; /* * 条件一：判断当前bean类型是否是基础框架类型的实例，不能被增强 * 条件二：判断当前beanname是否是是忽略的bean，不需要被增强 */ if (isInfrastructureClass(bean.getClass()) || shouldSkip(bean.getClass(), beanName)) &#123; //进入这里表示不需要增强 this.advisedBeans.put(cacheKey, Boolean.FALSE); //直接返回上层 return bean; &#125; //查找适合当前类的通知 非常重要 ！！！ Object[] specificInterceptors = getAdvicesAndAdvisorsForBean(bean.getClass(), beanName, null); //判断当前查询出来的通知是不是空，如果不是空，说明走增强逻辑 if (specificInterceptors != DO_NOT_PROXY) &#123; //记得放在缓存true this.advisedBeans.put(cacheKey, Boolean.TRUE); /*真正去创建代理对象*/ Object proxy = createProxy( bean.getClass()/*目标对象*/, beanName/*beanName*/, specificInterceptors/*匹配当前目标对象class的拦截器*/, new SingletonTargetSource(bean)/*把当前bean进行了一个封装*/); //保存代理对象类型 this.proxyTypes.put(cacheKey, proxy.getClass()); //返回代理对象 return proxy; &#125; //执行到这里说明没查到这个类相关的通知，没法增强，直接返回 this.advisedBeans.put(cacheKey, Boolean.FALSE); return bean;&#125; ​ 通过**getAdvicesAndAdvisorsForBean()**查找适合当前类的通知。​ 然后通过**createProxy()**去创建代理对象，最终返回代理对象。​ 5.查找通知12345678910111213@Override@Nullableprotected Object[] getAdvicesAndAdvisorsForBean( Class&lt;?&gt; beanClass, String beanName, @Nullable TargetSource targetSource) &#123; //查询合适当前类型的通知 List&lt;Advisor&gt; advisors = findEligibleAdvisors(beanClass, beanName); //通知为空返回空 if (advisors.isEmpty()) &#123; return DO_NOT_PROXY; &#125; //否则转成一个数组返回 return advisors.toArray();&#125; 继续往下看**findEligibleAdvisors()**。\u0000 123456789101112protected List&lt;Advisor&gt; findEligibleAdvisors(Class&lt;?&gt; beanClass, String beanName) &#123; /*获取到当前项目里面所有可以使用的增强器*/ List&lt;Advisor&gt; candidateAdvisors = findCandidateAdvisors(); /*将上一步获取到的全部增强器进行过滤，留下适合当前类的*/ List&lt;Advisor&gt; eligibleAdvisors = findAdvisorsThatCanApply(candidateAdvisors, beanClass, beanName); /*在这一步，会在index为0 的位置添加一个增强器*/ extendAdvisors(eligibleAdvisors); if (!eligibleAdvisors.isEmpty()) &#123; eligibleAdvisors = sortAdvisors(eligibleAdvisors); &#125; return eligibleAdvisors;&#125; 获取全部增强器，过滤排序，返回。​ 先看一下如何获取的。​ 123456789protected List&lt;Advisor&gt; findCandidateAdvisors() &#123; Assert.state(this.advisorRetrievalHelper != null, &quot;No BeanFactoryAdvisorRetrievalHelper available&quot;); //查询出来通过 bean 的方式配置的 增强器 /* * advisorRetrievalHelper 是怎么初始化的？ * 这个类实现了 beanFactoryAware接口 ，在初始化beanFactory的时候， 创建了一个 helper 对象 */ return this.advisorRetrievalHelper.findAdvisorBeans();&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public List&lt;Advisor&gt; findAdvisorBeans() &#123; // Determine list of advisor bean names, if not cached already. String[] advisorNames = this.cachedAdvisorBeanNames; if (advisorNames == null) &#123; //通过 bf 查询出来 bd 配置的 class 是 增强器的 子类 的beanName advisorNames = BeanFactoryUtils.beanNamesForTypeIncludingAncestors( this.beanFactory, Advisor.class, true, false); this.cachedAdvisorBeanNames = advisorNames; &#125; //要是没拿到，就直接返回，没必要往下走了 if (advisorNames.length == 0) &#123; return new ArrayList&lt;&gt;(); &#125; List&lt;Advisor&gt; advisors = new ArrayList&lt;&gt;(); for (String name : advisorNames) &#123; //注意：当前的helper是适配器包装的，真正的逻辑在适配器里面，但是实际上，适配器里面的这个方法也是返回 true //这个方法的作用是判断当前给定名字的bean是否合格 if (isEligibleBean(name)) &#123; //当前bean如果是在创建中的话，那就打印个日志，记录下 if (this.beanFactory.isCurrentlyInCreation(name)) &#123; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Skipping currently created advisor &#x27;&quot; + name + &quot;&#x27;&quot;); &#125; &#125; else &#123; try &#123; //从 bf 查询出来 当前这个名字和类型的增强器实例加入到增强器列表中 advisors.add(this.beanFactory.getBean(name, Advisor.class)); &#125; catch (BeanCreationException ex) &#123; Throwable rootCause = ex.getMostSpecificCause(); if (rootCause instanceof BeanCurrentlyInCreationException) &#123; BeanCreationException bce = (BeanCreationException) rootCause; String bceBeanName = bce.getBeanName(); if (bceBeanName != null &amp;&amp; this.beanFactory.isCurrentlyInCreation(bceBeanName)) &#123; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Skipping advisor &#x27;&quot; + name + &quot;&#x27; with dependency on currently created bean: &quot; + ex.getMessage()); &#125; // Ignore: indicates a reference back to the bean we&#x27;re trying to advise. // We want to find advisors other than the currently created bean itself. continue; &#125; &#125; throw ex; &#125; &#125; &#125; &#125; return advisors;&#125; 再看一下如何过滤出当前类需要的增强器的。​ 123456789101112protected List&lt;Advisor&gt; findAdvisorsThatCanApply( List&lt;Advisor&gt; candidateAdvisors, Class&lt;?&gt; beanClass, String beanName) &#123; ProxyCreationContext.setCurrentProxiedBeanName(beanName); try &#123; /*核心逻辑*/ return AopUtils.findAdvisorsThatCanApply(candidateAdvisors, beanClass); &#125; finally &#123; ProxyCreationContext.setCurrentProxiedBeanName(null); &#125;&#125; 继续往下走 12345678910111213141516171819202122232425262728public static List&lt;Advisor&gt; findAdvisorsThatCanApply(List&lt;Advisor&gt; candidateAdvisors, Class&lt;?&gt; clazz) &#123; /*如果这个类全部可用的增强器为空，直接返回*/ if (candidateAdvisors.isEmpty()) &#123; return candidateAdvisors; &#125; //匹配当前class 的 advisor 信息 List&lt;Advisor&gt; eligibleAdvisors = new ArrayList&lt;&gt;(); //不考虑音阶增强 for (Advisor candidate : candidateAdvisors) &#123; if (candidate instanceof IntroductionAdvisor &amp;&amp; canApply(candidate, clazz)) &#123; eligibleAdvisors.add(candidate); &#125; &#125; //假设 值为false boolean hasIntroductions = !eligibleAdvisors.isEmpty(); for (Advisor candidate : candidateAdvisors) &#123; if (candidate instanceof IntroductionAdvisor) &#123; // already processed continue; &#125; //判断当前增强器是否匹配class if (canApply(candidate, clazz, hasIntroductions)) &#123; eligibleAdvisors.add(candidate); &#125; &#125; //返回的都是匹配当前class的advisor return eligibleAdvisors;&#125; 看一下_**canApply()**_。​ 123456789101112131415public static boolean canApply(Advisor advisor, Class&lt;?&gt; targetClass, boolean hasIntroductions) &#123; if (advisor instanceof IntroductionAdvisor) &#123; return ((IntroductionAdvisor) advisor).getClassFilter().matches(targetClass); &#125; //大多数情况下是走这里，因为创建的增强器是 InstantiationModelAwarePointcutAdvisorImpl else if (advisor instanceof PointcutAdvisor) &#123; PointcutAdvisor pca = (PointcutAdvisor) advisor; //方法重载 return canApply(pca.getPointcut(), targetClass, hasIntroductions); &#125; else &#123; // It doesn&#x27;t have a pointcut so we assume it applies. return true; &#125;&#125; \u0000方法重载​ 123456789101112131415161718192021222324252627282930313233343536373839404142/*判断当前切点是否匹配当前class*/public static boolean canApply(Pointcut pc, Class&lt;?&gt; targetClass, boolean hasIntroductions) &#123; Assert.notNull(pc, &quot;Pointcut must not be null&quot;); //条件成立：说明当前class就不满足切点的定义 ，直接返回，因为后面是判断方法匹配的逻辑，直接返回 if (!pc.getClassFilter().matches(targetClass)) &#123; return false; &#125; //获取方法匹配器 MethodMatcher methodMatcher = pc.getMethodMatcher(); //如果是true，直接返回true，因为true不做判断，直接匹配所有方法 if (methodMatcher == MethodMatcher.TRUE) &#123; // No need to iterate the methods if we&#x27;re matching any method anyway... return true; &#125; //skip IntroductionAwareMethodMatcher introductionAwareMethodMatcher = null; if (methodMatcher instanceof IntroductionAwareMethodMatcher) &#123; introductionAwareMethodMatcher = (IntroductionAwareMethodMatcher) methodMatcher; &#125; //保存当前目标对象clazz + 目标对象 父类 爷爷类 ... 的接口 + 自身实现的接口 Set&lt;Class&lt;?&gt;&gt; classes = new LinkedHashSet&lt;&gt;(); //判断目标对象是不是代理对象，确保classes内存储的数据包括目标对象的class，而不是代理类class if (!Proxy.isProxyClass(targetClass)) &#123; classes.add(ClassUtils.getUserClass(targetClass)); &#125; classes.addAll(ClassUtils.getAllInterfacesForClassAsSet(targetClass)); //遍历classes，获取当前class定义的method，整个for循环会检查当前目标clazz 上级接口的所有方法 //看看是否会被方法匹配器匹配，如果有一个方法匹配成功，就说明目标class需要被AOP代理增强 for (Class&lt;?&gt; clazz : classes) &#123; Method[] methods = ReflectionUtils.getAllDeclaredMethods(clazz); for (Method method : methods) &#123; if (introductionAwareMethodMatcher != null ? introductionAwareMethodMatcher.matches(method, targetClass, hasIntroductions) : methodMatcher.matches(method, targetClass)) &#123; return true; &#125; &#125; &#125; //执行到这里，说明当前类的所有方法都没有匹配成功，当前类不需要AOP的增强。 return false;&#125; **matches()**上一篇已经分析过了，这里不再赘述。​ 查找到匹配当前类的切面以后，我们再看一看如何创建的代理对象。​ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556protected Object createProxy(Class&lt;?&gt; beanClass, @Nullable String beanName, @Nullable Object[] specificInterceptors, TargetSource targetSource) &#123; /*类型断言，成立*/ if (this.beanFactory instanceof ConfigurableListableBeanFactory) &#123; /*给当前的bd添加了一个属性*/ AutoProxyUtils.exposeTargetClass((ConfigurableListableBeanFactory) this.beanFactory, beanName, beanClass); &#125; //创建一个代理对象的工厂，他必须持有创建AOP代理class的生产资料 // 1. 目标对象 //2. 增强信息 ProxyFactory proxyFactory = new ProxyFactory(); //从当前类赋值一些信息到工厂 proxyFactory.copyFrom(this); //说明咱们有使用 xml 配置修改过 aop ProxyTargetClass if (proxyFactory.isProxyTargetClass()) &#123; // Explicit handling of JDK proxy targets (for introduction advice scenarios) if (Proxy.isProxyClass(beanClass)) &#123; // Must allow for introductions; can&#x27;t just set interfaces to the proxy&#x27;s interfaces only. for (Class&lt;?&gt; ifc : beanClass.getInterfaces()) &#123; proxyFactory.addInterface(ifc); &#125; &#125; &#125; else &#123; // No proxyTargetClass flag enforced, let&#x27;s apply our default checks... //如果bd定义内有 preserverTargetClass = true ，那么bd对应的class创建代理对象的时候 //使用cglib，否则还得继续判断，判断需要代理的接口 if (shouldProxyTargetClass(beanClass, beanName)) &#123; proxyFactory.setProxyTargetClass(true); &#125; else &#123; //评估需要代理的接口，判断使用什么代理 evaluateProxyInterfaces(beanClass, proxyFactory); &#125; &#125; //构建切面集合 Advisor[] advisors = buildAdvisors(beanName, specificInterceptors); proxyFactory.addAdvisors(advisors); proxyFactory.setTargetSource(targetSource); //扩展点 customizeProxyFactory(proxyFactory); proxyFactory.setFrozen(this.freezeProxy); if (advisorsPreFiltered()) &#123; //传递给代理工厂的这些增强器信息做过基础匹配，也就是classFilter匹配 proxyFactory.setPreFiltered(true); &#125; // ClassLoader classLoader = getProxyClassLoader(); if (classLoader instanceof SmartClassLoader &amp;&amp; classLoader != beanClass.getClassLoader()) &#123; classLoader = ((SmartClassLoader) classLoader).getOriginalClassLoader(); &#125; return proxyFactory.getProxy(classLoader);&#125; 最终还是调用了**proxyFactory.getProxy(classLoader)**。​ 至此，整个AOP的逻辑算是正式完成，下一篇我将带领大家一起分析下事务的源码。​ ​","categories":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/tags/Spring/"}]},{"title":"Spring[十二]Aop源码分析","slug":"Spring/Spring[十二]Aop源码分析","date":"2022-01-11T06:10:47.608Z","updated":"2022-01-11T06:17:03.028Z","comments":true,"path":"2022/01/11/Spring/Spring[十二]Aop源码分析/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/Spring/Spring[%E5%8D%81%E4%BA%8C]Aop%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","excerpt":"","text":"上一篇通过简单的分析，我们大概清楚了整个AOP代码实现的大体流程。本篇我们将从代码入手，一点点分解AOP的实现代码。 1.使用AOP的代码先看一段代码，看看如何使用AOP的。​ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677public class CodeMain &#123; public static void main(String[] args) &#123; //默认代理所有方法 proxyAllMethods(); //定制代理 //proxyMethod(); &#125; /** * 默认情况。代理所有方法 */ private static void proxyAllMethods()&#123; //1.创建被代理对象 Tiger tiger = new Tiger(); //2.创建spring代理工厂对象 ProxyFactory //proxyFactory 是 config + factory 的存在 持有 aop 操作 的 所有的生产资料 ProxyFactory proxyFactory = new ProxyFactory(tiger); //3. 添加拦截器 proxyFactory.addAdvice(new MethodInterceptor01()); proxyFactory.addAdvice(new MethodInterceptor02()); //4.获取代理对象，分析如何获取的代理对象？ Animals proxy = (Animals) proxyFactory.getProxy(); proxy.eat(&quot;人&quot;); System.out.println(&quot;===================================&quot;); proxy.run(); &#125; /** * 代理指定的方法 */ private static void proxyMethod()&#123; //1.创建被代理对象 Tiger tiger = new Tiger(); //2.创建spring代理工厂对象 ProxyFactory //proxyFactory 是 config + factory 的存在 持有 aop 操作 的 所有的生产资料 ProxyFactory proxyFactory = new ProxyFactory(tiger); //3.添加方法拦截 MyPointCut pointCut = new MyPointCut(); proxyFactory.addAdvisor(new DefaultPointcutAdvisor(pointCut,new MethodInterceptor01())); proxyFactory.addAdvisor(new DefaultPointcutAdvisor(pointCut,new MethodInterceptor02())); //4.获取代理对象 分析如何获得代理对象 Animals proxy = (Animals) proxyFactory.getProxy(); proxy.eat(&quot;人肉&quot;); System.out.println(&quot;===================================&quot;); proxy.run(); &#125; /** * 方法拦截器 */ private static class MethodInterceptor01 implements MethodInterceptor &#123; @Nullable @Override public Object invoke(@NonNull MethodInvocation invocation) throws Throwable&#123; System.out.println(&quot;method interceptor begin!----1&quot;); Object result = invocation.proceed(); System.out.println(&quot;method interceptor end!---4&quot;); return result; &#125; &#125; private static class MethodInterceptor02 implements MethodInterceptor &#123; @Nullable @Override public Object invoke(@NonNull MethodInvocation invocation) throws Throwable&#123; System.out.println(&quot;method interceptor begin!----2&quot;); Object result = invocation.proceed(); System.out.println(&quot;method interceptor end!----3&quot;); return result; &#125; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445interface Animals&#123; void eat(String food); void run();&#125;class Tiger implements Animals&#123; @Override public void eat(String food) &#123; System.out.println(&quot;老虎吃&quot;+food); &#125; @Override public void run() &#123; System.out.println(&quot;跑的贼快！&quot;); &#125;&#125;public class MyPointCut implements Pointcut &#123; @Override public ClassFilter getClassFilter() &#123; return clazz -&gt; true; &#125; @Override public MethodMatcher getMethodMatcher() &#123; return new MethodMatcher() &#123; @Override public boolean matches(Method method, Class&lt;?&gt; targetClass) &#123; return method.getName().equals(&quot;eat&quot;); &#125; @Override public boolean isRuntime() &#123; return false; &#125; @Override public boolean matches(Method method, Class&lt;?&gt; targetClass, Object... args) &#123; return false; &#125; &#125;; &#125;&#125; 这是使用AOP的两种方式，默认是代理全部方法，另一个则是代理指定的方法。接下来，我们来对源码进行分析。​ 2.抓手123456789101112131415private static void proxyAllMethods()&#123; //1.创建被代理对象 Tiger tiger = new Tiger(); //2.创建spring代理工厂对象 ProxyFactory //proxyFactory 是 config + factory 的存在 持有 aop 操作 的 所有的生产资料 ProxyFactory proxyFactory = new ProxyFactory(tiger); //3. 添加拦截器 proxyFactory.addAdvice(new MethodInterceptor01()); proxyFactory.addAdvice(new MethodInterceptor02()); //4.获取代理对象，分析如何获取的代理对象？ Animals proxy = (Animals) proxyFactory.getProxy(); proxy.eat(&quot;人&quot;); System.out.println(&quot;===================================&quot;); proxy.run();&#125; 这里面首先首先是创建了一个代理工厂，然后给代理工厂添加拦截器，最后通过代理工厂来获取代理对象，最后通过代理对象执行目标方法。​ 接下来我们来看代理工厂。​ 3.代理工厂123456public ProxyFactory(Object target) &#123; //将目标对象封装成为 SingletonTargetSource 保存到父类字段内 setTarget(target); //获取目标对象class 的所有接口 ，保存到父类字段内 setInterfaces(ClassUtils.getAllInterfaces(target));&#125; 在**ProxyFactory**的构造器内将目标对象和目标对象实现的接口封装到了父类的字段里面。​ 上图是**ProxyFactory**的继承关系，可以先简单过一下，有一个印象。​ 4.添加切面**proxyFactory.addAdvice(new MethodInterceptor01())**\u0000这一行代码是往代理工厂添加拦截器/切面。​ 看一下添加的流程​ 12345@Overridepublic void addAdvice(Advice advice) throws AopConfigException &#123; int pos = this.advisors.size(); addAdvice(pos, advice);&#125; 首先是调用了**AdvisedSupport**类的添加切面方法。​ 在添加切面的方法里获取了当前类的增强器个数。然后和切面一起传递到重载的方法里。​ 12345678910111213141516171819@Overridepublic void addAdvice(int pos, Advice advice) throws AopConfigException &#123; Assert.notNull(advice, &quot;Advice must not be null&quot;); //不考虑，引介增强，很少用 if (advice instanceof IntroductionInfo) &#123; // We don&#x27;t need an IntroductionAdvisor for this kind of introduction: // It&#x27;s fully self-describing. addAdvisor(pos, new DefaultIntroductionAdvisor(advice, (IntroductionInfo) advice)); &#125; //不考虑，引介增强，很少用 else if (advice instanceof DynamicIntroductionAdvice) &#123; // We need an IntroductionAdvisor for this kind of introduction. throw new AopConfigException(&quot;DynamicIntroductionAdvice may only be added as part of IntroductionAdvisor&quot;); &#125; else &#123; //spring中Advice对应的接口就是Advisor，Spring使用Advisor包装着AOP的Advice实例 addAdvisor(pos, new DefaultPointcutAdvisor(advice)); &#125;&#125; ​ 然后调用了**addAdvisor()**，添加增强器。​ 因为我们没有指定切点，所以创建了一个默认的切点的增强器**DefaultPointcutAdvisor**。​ 123456789@Overridepublic void addAdvisor(int pos, Advisor advisor) throws AopConfigException &#123; //引介相关的逻辑，不考虑 if (advisor instanceof IntroductionAdvisor) &#123; validateIntroductionAdvisor((IntroductionAdvisor) advisor); &#125; //委派模式 addAdvisorInternal(pos, advisor);&#125; ​ 这里面调用了**addAdvisorInternal(pos, advisor)**，继续往下看。​ 12345678910111213141516private void addAdvisorInternal(int pos, Advisor advisor) throws AopConfigException &#123; Assert.notNull(advisor, &quot;Advisor must not be null&quot;); //如果当前AOP配置已经冻结了，不能在添加切面了，添加的话会抛出异常。 if (isFrozen()) &#123; throw new AopConfigException(&quot;Cannot add advisor: Configuration is frozen.&quot;); &#125; //如果传入的切面的位置大于当前切面的个数，抛异常，因为位置下标默认从-1开始。 if (pos &gt; this.advisors.size()) &#123; throw new IllegalArgumentException( &quot;Illegal position &quot; + pos + &quot; in advisor list with size &quot; + this.advisors.size()); &#125; //添加增强器 this.advisors.add(pos, advisor); //清理缓存 adviceChanged();&#125; 这里进行一些校验逻辑，然后添加增强器，清理缓存。​ 5.获取代理对象**proxyFactory.getProxy()**\u0000 1234567891011121314151617181920/** * 根据工厂的设置创建代理对象 * 可以反复的调用。 * 如果我们添加或者删除接口，效果会有所不同，可以添加和删除拦截器。 * 使用默认的类加载器：默认是线程上下文类加载器（如果需要创建代理） */public Object getProxy() &#123; /** * 创建AOP 的 代理 ，那么 AOP 的代理是什么 ？ * * AopProxy * * createAopProxy() ：去创建代理对象的逻辑 * getProxy()：获取创建好的代理对象，这里有两个实现分别是jdk的动态代理和cglib的动态代理。 * CglibAopProxy * JdkDynamicAopProxy * */ return createAopProxy().getProxy();&#125; 这里面先是利用**createAopProxy()**创建了一个代理对象，然后通过**getProxy()**来获取一个代理对象。​ 我们先来分析**createAopProxy()**，看一看代理对象是如何创建的？​ 12345678910111213141516171819/** * 子类应该调用它来获得一个新的 AOP 代理。 他们不应该创建一个AOP代理this作为参数。 */protected final synchronized AopProxy createAopProxy() &#123; /** * active属性实际上就是一个标记，在创建第一个代理对象的时候，会将他设置为true。 */ if (!this.active) &#123; activate(); &#125; /** * 分析一下下面这行代码的流程： * * 1. 获取aop的代理工厂 看一下AopProxyFactory * 2. 使用工厂创建一个aop的代理 ,如何创建代理的？ * */ return getAopProxyFactory().createAopProxy(this);&#125; 先是通过**getAopProxyFactory()**获取AOP的代理工厂，然后通过**createAopProxy(this)**传入当前类来获取一个代理对象。​ 123456789/** * 这个类里面已经持有了一个默认的aop的代理工厂 * ctrl + h 查看当前类的继承关系 * 当前类是 ProxyFactory的父类，所以里面的代理工厂会被默认的初始化加载 * @return */public AopProxyFactory getAopProxyFactory() &#123; return this.aopProxyFactory;&#125; 看一下上图类的继承关系，其实在创建**ProxyFactory**的时候，隐式调用父类的构造器的时候，就已经在**ProxyCreatorSupport**里面创建了一个默认的AOP代理工厂**DefaultAopProxyFactory**。​ 接下来再来看如何创建一个AOP代理对象的。​ 123456789101112131415161718192021222324252627282930313233343536373839/** * @param config 就是我们的ProxyFactory对象，ProxyFactory他是一个配置管理对象 * 保存着创建代理对象所有的生产资料。 * @return 返回一个AOP的代理对象 * @throws AopConfigException 如果某些不期望我们修改的配置被修改，就会抛出异常 */@Overridepublic AopProxy createAopProxy(AdvisedSupport config) throws AopConfigException &#123; /** * 条件一：暂且不管 * 条件二：true 表示强制使用cglib代理， * 条件三：true 表示被代理对象没有实现任何接口没有办法使用jdk的动态代理，只能使用cglib的动态代理 */ if (!NativeDetector.inNativeImage() &amp;&amp; //该条件不需要考虑 ( config.isOptimize() || //设置了这个属性，那么就是强制使用cglib的动态代理 config.isProxyTargetClass() || //设置了这个属性，那么就是强制使用cglib的动态代理 hasNoUserSuppliedProxyInterfaces(config) //判断被代理对象有没有实现接口，没有实现接口，那还用锤子jdk的动态代理 ) ) &#123; //走到这里的话，很大程度上就已经会使用cglib的动态代理 //获取目标对象的类型，为空的话肯定没法继续往下走了，直接异常中断 Class&lt;?&gt; targetClass = config.getTargetClass(); if (targetClass == null) &#123; throw new AopConfigException(&quot;TargetSource cannot determine target class: Either an interface or a target is required for proxy creation.&quot;); &#125; //目标对象是一个接口 或者 已经是一个被代理过得类型（此时是多重代理） ，只能使用jdk的动态代理 if (targetClass.isInterface() || Proxy.isProxyClass(targetClass)) &#123; return new JdkDynamicAopProxy(config); &#125; //走cglib的动态代理 return new ObjenesisCglibAopProxy(config); &#125; else &#123; //执行到这里的情况 ： 实现了接口,大多数情况我们都是面向接口编程，走这里 return new JdkDynamicAopProxy(config); &#125;&#125; 根据条件判断我们到底是创建JDK的代理对象还是创建Cglib的代理对象，因为我们的案例代码的目标类是实现了接口的，所以默认会走jdk的动态代理。​ 6.JdkDynamicAopProxy1234567891011121314151617/** * 使用给定的配置，通过构造器创建aop 的动jdk态代理对象 * 这里的config是啥？就是我们的代理工厂对象 */public JdkDynamicAopProxy(AdvisedSupport config) throws AopConfigException &#123; //非空断言 Assert.notNull(config, &quot;AdvisedSupport must not be null&quot;); //如果配置里面的切面数==0 &amp;&amp; 配置里面的目标对象是空对象，那么代理无法继续往下走了，直接抛异常中断 if (config.getAdvisorCount() == 0 &amp;&amp; config.getTargetSource() == AdvisedSupport.EMPTY_TARGET_SOURCE) &#123; throw new AopConfigException(&quot;No advisors and no TargetSource specified&quot;); &#125; this.advised = config; //获取当前被代理对象实现的接口数组 ,具体的实现逻辑？ this.proxiedInterfaces = AopProxyUtils.completeProxiedInterfaces(this.advised, true); //查找所有被代理的接口，如果有equals 和 hashcode就打个标 findDefinedEqualsAndHashCodeMethods(this.proxiedInterfaces);&#125; ​ 判断如果配置里面没有该被代理对象的切面，或者被代理对象是空，那就不能往下走了，抛出异常。​ 获取当前被代理对象实现的接口数组**AopProxyUtils.completeProxiedInterfaces(this.advised, true)**。​ 查找所有被代理的接口，如果有equals 和 hashcode就打个标。​ 看一下如何获取到当前被代理对象实现的接口数组的。​ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * 拿到被代理对象的所有接口 */static Class&lt;?&gt;[] completeProxiedInterfaces(AdvisedSupport advised, boolean decoratingProxy) &#123; /*从proxyFactory中获取所有的target提取出来的接口*/ Class&lt;?&gt;[] specifiedInterfaces = advised.getProxiedInterfaces(); /*如果接口长度是0*/ if (specifiedInterfaces.length == 0) &#123; //拿到目标对象的类型 Class&lt;?&gt; targetClass = advised.getTargetClass(); //如果目标对象的类型不为空 if (targetClass != null) &#123; //如果目标对象是一个接口，那么就将目标对象设置到接口列表里面 if (targetClass.isInterface()) &#123; advised.setInterfaces(targetClass); &#125; //如果目标对象是一个代理类，那么也将目标对象设置到接口列表 else if (Proxy.isProxyClass(targetClass)) &#123; advised.setInterfaces(targetClass.getInterfaces()); &#125; specifiedInterfaces = advised.getProxiedInterfaces(); &#125; &#125; /*创建一个新的接口数组，长度是原接口数量+spring追加的三个接口数量*/ List&lt;Class&lt;?&gt;&gt; proxiedInterfaces = new ArrayList&lt;&gt;(specifiedInterfaces.length + 3); for (Class&lt;?&gt; ifc : specifiedInterfaces) &#123; // 只有非密封接口实际上有资格进行JDK代理(在JDK 17上) if (!ifc.isSealed()) &#123; proxiedInterfaces.add(ifc); &#125; &#125; /*如果这个接口里面没有SpringProxy这个接口，那么就需要添加一个，打标，标识这个代理对象是Spring创建的*/ if (!advised.isInterfaceProxied(SpringProxy.class)) &#123; proxiedInterfaces.add(SpringProxy.class); &#125; /*判断目标对象的所有接口是否有advice接口，没有就手动添加*/ if (!advised.isOpaque() &amp;&amp; !advised.isInterfaceProxied(Advised.class)) &#123; proxiedInterfaces.add(Advised.class); &#125; //如果目标对象的所有接口里面，没有DecoratingProxy的接口，那就添加一个 if (decoratingProxy &amp;&amp; !advised.isInterfaceProxied(DecoratingProxy.class)) &#123; proxiedInterfaces.add(DecoratingProxy.class); &#125; //返回接口类型数组 return ClassUtils.toClassArray(proxiedInterfaces);&#125; 接下来我们来看**getProxy(**)的逻辑。​ 7.getProxy()12345@Overridepublic Object getProxy() &#123; //这里如果没有传类加载器，就使用默认的类加载器，默认是线程上下文类加载器 return getProxy(ClassUtils.getDefaultClassLoader());&#125; 方法重载，继续往下走。​ 123456789@Overridepublic Object getProxy(@Nullable ClassLoader classLoader) &#123; //打印日志的逻辑 if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Creating JDK dynamic proxy: &quot; + this.advised.getTargetSource()); &#125; //通过jdk的动态代理来创建代理对象 this == this::invoke 该方法最终会返回一个代理类对象 return Proxy.newProxyInstance(classLoader, this.proxiedInterfaces, this);&#125; 这里通过JDK的动态代理来创建代理对象，为啥会传入当前类**JdkDynamicAopProxy**，因为当前类实现了**InvocationHandler**接口。​ 因此，当代理对象调用目标方法的时候，就会执行该类的**invoke()**。​ 8.代理对象执行目标方法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293@Override@Nullablepublic Object invoke(Object proxy/*代理对象*/, Method method/*目标方法*/, Object[] args/*目标方法对应的参数*/) throws Throwable &#123; Object oldProxy = null; boolean setProxyContext = false; //advised 这里实际上就是proxyFactory的引用，targetSource 实际上就是上层传递封装的targetsource TargetSource targetSource = this.advised.targetSource; Object target = null; try &#123; /*如果代理类实现的接口里面有equals方法，就使用里面的，否则使用jdk提供的equals方法*/ if (!this.equalsDefined &amp;&amp; AopUtils.isEqualsMethod(method)) &#123; return equals(args[0]); &#125; //如果代理类实现的接口里面提供了hashcode方法，就是用里面的，否则用jdk的 else if (!this.hashCodeDefined &amp;&amp; AopUtils.isHashCodeMethod(method)) &#123; return hashCode(); &#125; /*暂时尚未用到，TODO*/ else if (method.getDeclaringClass() == DecoratingProxy.class) &#123; return AopProxyUtils.ultimateTargetClass(this.advised); &#125; else if (!this.advised.opaque &amp;&amp; method.getDeclaringClass().isInterface() &amp;&amp; method.getDeclaringClass().isAssignableFrom(Advised.class)) &#123; return AopUtils.invokeJoinpointUsingReflection(this.advised, method, args); &#125; //返回值 Object retVal; /** * 是否需要将当前的代理对象设置在aop上下文中 * aop上下文对象实际上就是一个threadLocal * 为什么要引入一个aop上下文？ * 目标对象A B * 通过代理的方式调用A.eat() * 这个eat方法里面有恰恰调用到了B的方法，这个时候B对象实际上并不是代理对象，所以 * b的方法执行前后并不会被增强，为了解决这个问题，就引入了aop的上下文 */ if (this.advised.exposeProxy) &#123; // 将当前代理对象设置到aop上下文中，并返回老的代理对象 oldProxy = AopContext.setCurrentProxy(proxy); setProxyContext = true; &#125; /*根据targetSource拿到目标对象*/ target = targetSource.getTarget(); /*根据目标对象拿到目标对象的类型*/ Class&lt;?&gt; targetClass = (target != null ? target.getClass() : null); // 这里是最关键的地方，查找适合该方法的增强 具体是如何查找的？ List&lt;Object&gt; chain = this.advised.getInterceptorsAndDynamicInterceptionAdvice(method, targetClass); /*查询出匹配当前方法拦截器的数量是0 说明当前方法不需要被增强，直接通过反射调用目标对象的目标方法。*/ if (chain.isEmpty()) &#123; Object[] argsToUse = AopProxyUtils.adaptArgumentsIfNecessary(method, args); /*调用目标对象的目标方法*/ retVal = AopUtils.invokeJoinpointUsingReflection(target, method, argsToUse); &#125; else &#123; /*说明有匹配当前method的方法拦截器，说明要做增强处理 */ MethodInvocation invocation = new ReflectiveMethodInvocation(proxy, target, method, args, targetClass, chain); /*核心驱动逻辑在ReflectiveMethodInvocation*/ retVal = invocation.proceed(); &#125; /*获取方法的返回值类型*/ Class&lt;?&gt; returnType = method.getReturnType(); /*如果目标方法返回目标对象 ，做一个替换 ，返回代理对象*/ if (retVal != null &amp;&amp; retVal == target &amp;&amp; returnType != Object.class &amp;&amp; returnType.isInstance(proxy) &amp;&amp; !RawTargetAccess.class.isAssignableFrom(method.getDeclaringClass())) &#123; retVal = proxy; &#125; //方法是void类型，但是返回值类型还不为空，说明有问题，抛异常 else if (retVal == null &amp;&amp; returnType != Void.TYPE &amp;&amp; returnType.isPrimitive()) &#123; throw new AopInvocationException( &quot;Null return value from advice does not match primitive return type for: &quot; + method); &#125; return retVal; &#125; finally &#123; if (target != null &amp;&amp; !targetSource.isStatic()) &#123; // Must have come from TargetSource. targetSource.releaseTarget(target); &#125; if (setProxyContext) &#123; // 将上次设置的proxy在此设置回去到aop上下文内 //因为当前代理对象的目标方法已经完成了，需要回到上一层逻辑 //属于恢复现场的逻辑 AopContext.setCurrentProxy(oldProxy); &#125; &#125;&#125; \u0000\u0000抛开一切不是很重要的逻辑​ 判断当前代理对象是否应该暴露出去，aop上下文**AopContext**实际上就是一个ThreadLocal。​ 为什么要引入AOP的上下文？​ 假设有目标对象A,B。​ 通过代理的方式调用A.eat()。​ 这个eat()方法里面恰恰调用了B的方法，这个时候对象实际上并不是代理对象，所以B的方法执行前后并不会被增强，为了解决这个问题，就引入了AOP上下文。​ 通过**getInterceptorsAndDynamicInterceptionAdvice()**查找到当前方法执行前后需要执行的增强器。\u0000如果当前方法匹配的增强器数量是0，那么直接通过反射调用目标方法。​ 否则说明有匹配的增强器，需要做增强处理。**retVal=ReflectiveMethodInvocation.proceed(）**​ retVal就是方法的返回值。​ 判断如果方法最终返回的目标对象，那就替换成代理对象。​ 判断如果方法是void类型，但是返回值类型还不为空，说明有问题，抛异常。​ 最终返回结果，并将AOP上下文的代理对象还原成里面原有的对象，因为当前代理对象的目标方法已经完成了，需要回到上一层逻辑。\u0000\u0000至此，AOP整个流程就分析完了，剩下的一些核心的细节：​ **getInterceptorsAndDynamicInterceptionAdvice()**查找到当前方法执行前后需要执行的增强器 **retVal=ReflectiveMethodInvocation.proceed(）**执行增强器逻辑 ​ 通过前面的分析，我们大体上了解了Spring的Aop的执行流程。接下来我们在看一些核心的细节，如何查找目标方法的增强器。 9.查找增强器**List&lt;Object&gt; chain = this.advised.getInterceptorsAndDynamicInterceptionAdvice(method, targetClass);**\u0000 1234567891011121314151617public List&lt;Object&gt; getInterceptorsAndDynamicInterceptionAdvice(Method method, @Nullable Class&lt;?&gt; targetClass) &#123; //先尝试从缓存拿 MethodCacheKey cacheKey = new MethodCacheKey(method); List&lt;Object&gt; cached = this.methodCache.get(cacheKey); /*如果缓存为空*/ if (cached == null) &#123; /*那就走查找逻辑，并刷新缓存 * advisorChainFactory什么时候创建的？ * 这个是在proxyFactory里面的一个变量，代理工厂创建出来，他就创建出来了 * */ cached = this.advisorChainFactory.getInterceptorsAndDynamicInterceptionAdvice( this, method, targetClass); this.methodCache.put(cacheKey, cached); &#125; /*最终返回查找到的值*/ return cached;&#125; 查找缓存，如果没命中则去查找并放入缓存放回。我们继续往下看查找逻辑**this.advisorChainFactory.getInterceptorsAndDynamicInterceptionAdvice()**。​ 这个**advisorChainFactory**是什么？看当前类的属性**AdvisorChainFactory advisorChainFactory = new DefaultAdvisorChainFactory();** 从这里我们就定位到了看哪个方法的逻辑\u0000 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879@Overridepublic List&lt;Object&gt; getInterceptorsAndDynamicInterceptionAdvice( Advised config/*代理工厂*/, Method method/*目标方法*/, @Nullable Class&lt;?&gt; targetClass/*目标对象类型*/) &#123; /** * 这个接口是切面适配器的注册中心 * 1.可以注册AdvisorAdapter 适配器目的：将非advisor类型的增强包装成advisor ，将advisor类型的增强提取出来对应的 方法拦截器 * * */ AdvisorAdapterRegistry registry = GlobalAdvisorAdapterRegistry.getInstance(); /** * 获取代理工厂内部持有的增强信息 * 1. addAdvice * 2. addAdvisor * 最终在代理工厂中都会包装成 advisor * */ Advisor[] advisors = config.getAdvisors(); /*创建一个拦截器列表，长度就是advisor的长度*/ List&lt;Object&gt; interceptorList = new ArrayList&lt;&gt;(advisors.length); /*真实的目标对象类型*/ Class&lt;?&gt; actualClass = (targetClass != null ? targetClass : method.getDeclaringClass()); /*引介增强相关*/ Boolean hasIntroductions = null; for (Advisor advisor : advisors) &#123; /*包含切点信息的增强，内部逻辑就是做匹配算法*/ if (advisor instanceof PointcutAdvisor) &#123; // 转换成 PointcutAdvisor 类型，可以获取到切点信息 PointcutAdvisor pointcutAdvisor = (PointcutAdvisor) advisor; /*条件二成立，说明当前被代理对象的class匹配当前advisor成功，可能被advisor增强，具体还要看方法匹配。 这里可以看一下 Pointcut 源码*/ if (config.isPreFiltered() || pointcutAdvisor.getPointcut().getClassFilter().matches(actualClass)) &#123; /*获取切点信息的方法匹配器，做方法级别的匹配*/ MethodMatcher mm = pointcutAdvisor.getPointcut().getMethodMatcher(); boolean match; /*引介相关的，不需要考虑*/ if (mm instanceof IntroductionAwareMethodMatcher) &#123; if (hasIntroductions == null) &#123; hasIntroductions = hasMatchingIntroductions(advisors, actualClass); &#125; match = ((IntroductionAwareMethodMatcher) mm).matches(method, actualClass, hasIntroductions); &#125; else &#123; /*进行方法匹配 目标方法匹配成功 ， match = true，当前的增强器可以应用到method*/ match = mm.matches(method, actualClass); &#125; /*判断是否还需要运行时的匹配*/ if (match) &#123; /*提取出advisor类持有的拦截器信息 registry里面包含三个默认的增强器*/ MethodInterceptor[] interceptors = registry.getInterceptors(advisor); if (mm.isRuntime()) &#123; /*如果是运行时匹配，那就走运行时匹配的逻辑*/ for (MethodInterceptor interceptor : interceptors) &#123; interceptorList.add(new InterceptorAndDynamicMethodMatcher(interceptor, mm)); &#125; &#125; else &#123; /*将方法拦截器追加到拦截器列表里面去*/ interceptorList.addAll(Arrays.asList(interceptors)); &#125; &#125; &#125; &#125; /*不考虑引介，所以直接跳过*/ else if (advisor instanceof IntroductionAdvisor) &#123; IntroductionAdvisor ia = (IntroductionAdvisor) advisor; if (config.isPreFiltered() || ia.getClassFilter().matches(actualClass)) &#123; Interceptor[] interceptors = registry.getInterceptors(advisor); interceptorList.addAll(Arrays.asList(interceptors)); &#125; &#125; /*适配所有方法的*/ else &#123; Interceptor[] interceptors = registry.getInterceptors(advisor); interceptorList.addAll(Arrays.asList(interceptors)); &#125; &#125; /*返回所有匹配当前方法的拦截器*/ return interceptorList;&#125; 方法有点长，我们挑重点慢慢分析。​ 首先获取到切面适配器的注册中心，切面适配器**AdvisorAdapter**是做什么的？​ 将非advisor类型的增强包装成advisor ，将advisor类型的增强提取出来对应的 方法拦截器。​ 获取代理工厂内部持有的增强信息​ Advice Advisor 最终在代理工厂中都会被包装成Advisor。​ 遍历所有的拦截器：​ 先是处理切点类型的增强器 先将增强器转化成**PointcutAdvisor**类型 判断如果当前被代理对象的class匹配增强器成功，说明可能增强成功，还要看具体的方法匹配，这里可以看一下**PointCut** 源码 获取切点信息的方法匹配器，准备做方法级别的匹配 **match = mm.matches(method, actualClass)**进行具体的方法匹配 判断是否需要运行时匹配 如果需要，提取出增强器持有的拦截器信息(registry里面默认持有三个增强器)，走运行时匹配的逻辑 -&gt; 将**InterceptorAndDynamicMethodMatcher**加入到拦截器列表 如果不需要，将方法拦截器添加到拦截器列表 处理引介类型的增强器 这里的逻辑我们不需要关注 处理适配所有方法的增强器 从增强器的适配中心获取所有的拦截器 最终返回匹配当前方法的所有拦截器​ 10.方法匹配这里我们再看一下具体的方法匹配逻辑。​ 看一下**AbstractRegexpMethodPointcut**，说实话，营养价值不大，有兴趣可以自行琢磨。 123456789101112131415161718192021222324252627@Overridepublic boolean matches(Method method, Class&lt;?&gt; targetClass) &#123; return (matchesPattern(ClassUtils.getQualifiedMethodName(method, targetClass)) || (targetClass != method.getDeclaringClass() &amp;&amp; matchesPattern(ClassUtils.getQualifiedMethodName(method, method.getDeclaringClass()))));&#125;/** * Match the specified candidate against the configured patterns. * @param signatureString &quot;java.lang.Object.hashCode&quot; style signature * @return whether the candidate matches at least one of the specified patterns */protected boolean matchesPattern(String signatureString) &#123; for (int i = 0; i &lt; this.patterns.length; i++) &#123; boolean matched = matches(signatureString, i); if (matched) &#123; for (int j = 0; j &lt; this.excludedPatterns.length; j++) &#123; boolean excluded = matchesExclusion(signatureString, j); if (excluded) &#123; return false; &#125; &#125; return true; &#125; &#125; return false;&#125; 11.拦截器的核心驱动获取到所有的匹配当前方法的拦截器后，最终我们是要驱动所有的拦截器去执行，接下来分析下拦截器的核心驱动逻辑。**invocation.proceed()**，核心逻辑在**ReflectiveMethodInvocation**中。​ 123456789101112131415161718192021222324252627282930@Override@Nullablepublic Object proceed() throws Throwable &#123; // 因为从-1开始，如果当前拦截器下标 == 拦截器数量-1 ，说明所有方法拦截器都执行过了，接下来需要执行目标对象的目标方法 if (this.currentInterceptorIndex == this.interceptorsAndDynamicMethodMatchers.size() - 1) &#123; /*调用连接点*/ return invokeJoinpoint(); &#125; /*获取下一个方法拦截器*/ Object interceptorOrInterceptionAdvice = this.interceptorsAndDynamicMethodMatchers.get(++this.currentInterceptorIndex); /*判断是否需要运行时匹配*/ if (interceptorOrInterceptionAdvice instanceof InterceptorAndDynamicMethodMatcher) &#123; InterceptorAndDynamicMethodMatcher dm = (InterceptorAndDynamicMethodMatcher) interceptorOrInterceptionAdvice; Class&lt;?&gt; targetClass = (this.targetClass != null ? this.targetClass : this.method.getDeclaringClass()); if (dm.methodMatcher.matches(this.method, targetClass, this.arguments)) &#123; return dm.interceptor.invoke(this); &#125; else &#123; return proceed(); &#125; &#125; /*大部分情况下会走到else这里静态匹配*/ else &#123; // 让当前方法拦截器执行invoke即可 ，并且将当前对象传递进去 return ((MethodInterceptor) interceptorOrInterceptionAdvice).invoke(this); &#125;&#125; 判断是否所有拦截器都执行完了，如果是的话，执行目标方法**invokeJoinpoint()**。​ \u0000获取下一个拦截器，判断是否需要做运行时匹配，大部分情况下，我们都是走静态匹配的逻辑。​ 让当前方法拦截器执行**invoke()**。​ 12345678910@Override@Nullablepublic Object invoke(MethodInvocation mi) throws Throwable &#123; try &#123; return mi.proceed(); &#125; finally &#123; invokeAdviceMethod(getJoinPointMatch(), null, null); &#125;&#125; ​ 通过这样类似递归的链式调用，每一个拦截器等待下一个拦截器执行完成返回以后在执行，拦截器的机制保证了通知方法与目标方法的执行顺序。​ 再来看下如何调用目标方法。**invokeJoinpoint()**​ 1234@Nullableprotected Object invokeJoinpoint() throws Throwable &#123; return AopUtils.invokeJoinpointUsingReflection(this.target, this.method, this.arguments);&#125; \u0000继续往下追。​ 12345678910111213141516171819202122@Nullablepublic static Object invokeJoinpointUsingReflection(@Nullable Object target, Method method, Object[] args) throws Throwable &#123; // Use reflection to invoke the method. try &#123; ReflectionUtils.makeAccessible(method); return method.invoke(target, args); &#125; catch (InvocationTargetException ex) &#123; // Invoked method threw a checked exception. // We must rethrow it. The client won&#x27;t see the interceptor. throw ex.getTargetException(); &#125; catch (IllegalArgumentException ex) &#123; throw new AopInvocationException(&quot;AOP configuration seems to be invalid: tried calling method [&quot; + method + &quot;] on target [&quot; + target + &quot;]&quot;, ex); &#125; catch (IllegalAccessException ex) &#123; throw new AopInvocationException(&quot;Could not access method [&quot; + method + &quot;]&quot;, ex); &#125;&#125; 最终通过暴力反射来调用目标方法执行。​ 12.切点表达式**PointCut**​ 123456789101112131415161718192021222324public interface Pointcut &#123; /** * Return the ClassFilter for this pointcut. * @return the ClassFilter (never &#123;@code null&#125;) * * 类过滤器：判断某个类是否符合切点位置 */ ClassFilter getClassFilter(); /** * Return the MethodMatcher for this pointcut. * @return the MethodMatcher (never &#123;@code null&#125;) * 方法匹配器：判断类中某个方法是否匹配条件，匹配条件的方法才会被增强 */ MethodMatcher getMethodMatcher(); /** * Canonical Pointcut instance that always matches. */ Pointcut TRUE = TruePointcut.INSTANCE;&#125; 前面案例代码中，有一个我们自己实现的切点，可以回顾一下。 最终回顾下开头的一张图，明确下**Advised --持有--&gt; Advisor --持有--&gt; Advice --子类--&gt;Interceptor --子类--&gt;MethodInterceptor**关系：​ 至此，整个AOP的全部流程已经梳理清晰。下一篇，我们将开始分析Spring的事务。​ ​","categories":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/tags/Spring/"}]},{"title":"Spring[十]Aop的两种实现方式","slug":"Spring/Spring[十]Aop的两种实现方式","date":"2022-01-11T06:00:15.970Z","updated":"2022-01-11T06:08:00.592Z","comments":true,"path":"2022/01/11/Spring/Spring[十]Aop的两种实现方式/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/Spring/Spring[%E5%8D%81]Aop%E7%9A%84%E4%B8%A4%E7%A7%8D%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F/","excerpt":"","text":"本篇开始Aop的相关的内容，aop的底层使用到了动态代理的，我们针对一个方法可以配置多个切面，也就实现了多重代理。本篇先不看源码，从开发者的角度观望如何实现多重代理。 ​ 一，代理模式代理模式的定义：由于某些原因需要给某对象提供一个代理以控制对该对象的访问。这时，访问对象不适合或者不能直接引用目标对象，代理对象作为访问对象和目标对象之间的中介。代理模式的结构比较简单，主要是通过定义一个继承抽象主题的代理来包含真实主题，从而实现对真实主题的访问。代理模式的主要角色如下。 抽象主题（Subject）类：通过接口或抽象类声明真实主题和代理对象实现的业务方法。 真实主题（Real Subject）类：实现了抽象主题中的具体业务，是代理对象所代表的真实对象，是最终要引用的对象。 代理（Proxy）类：提供了与真实主题相同的接口，其内部含有对真实主题的引用，它可以访问、控制或扩展真实主题的功能。 在代码中，一般代理会被理解为代码增强，实际上就是在原代码逻辑前后增加一些代码逻辑，而使调用者无感知。根据代理的创建时期，代理模式分为静态代理和动态代理。 静态：由程序员创建代理类或特定工具自动生成源代码再对其编译，在程序运行前代理类的 .class 文件就已经存在了。 动态：在程序运行时，运用反射机制动态创建而成。 1.JDK的动态代理 动态代理 特点：字节码随用随修改，随用随加载 作用：在不修改源码的基础上在运行时动态的对方法进行增强 分类： 基于接口的动态代理 基于子类的动态代理 基于接口的动态代理 涉及的类：Proxy 提供者：JDK官方 如何创建代理对象 使用Proxy类的**newProxyInstance()** 创建代理对象的要求 被代理类最少实现一个接口，如果没有则不能使用 **newProxyInstance()**的参数 ClassLoader：类加载器，用于加载代理对象字节码文件；和被代理对象使用相同的类加载器。 Class[]：字节码数组，用于让代理对象和被代理对象实现相同方法的。 InvocationHandler：用于提供增强的代码，他是让我们自定义如何代理，我们一般都是写一个该接口的实现类。 简单的实现12345678910111213141516171819202122232425262728293031323334353637public class Test1 &#123; @Test public void test1()&#123; //被代理类对象要声明为最终的 final Producer producer=new Producer(); //代理对象和被代理类对象要实现同一个接口 IProducer proxyProducer = (IProducer) Proxy.newProxyInstance(producer.getClass().getClassLoader(), producer.getClass().getInterfaces(), new InvocationHandler() &#123; /** * 作用：执行被代理对象的任何接口方法都会经过该方法 * 方法参数的含义 * @param proxy 代理对象的引用 * 1. 可以使用反射获取代理对象的信息（也就是proxy.getClass().getName()。 * 2. 可以将代理对象返回以进行连续调用，这就是proxy存在的目的，因为this并不是代理对象。 * @param method 当前执行的方法 * @param args 当前执行方法所需的参数 * @return 和被代理对象方法相同的返回值 * @throws Throwable */ public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; Object value=null; //获取方法执行的参数 //判断当前方法是不是销售 if (&quot;saleProduct&quot;.equals(method.getName()))&#123; Float money= (Float) args[0]; //两个参数：被代理类对象，方法增强的参数 value=method.invoke(producer,money*0.8f); &#125; return value; &#125; &#125;); proxyProducer.saleProduct(10000f); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243package com.es.java1;/** * 一个生产者 */public class Producer implements IProducer&#123; /** * 销售 * @param money */ public void saleProduct(float money)&#123; System.out.println(&quot;销售产品，并拿到钱：&quot;+money); &#125; /** * 售后 * @param money */ public void afterService(float money)&#123; System.out.println(&quot;提供售后服务，并拿到钱：&quot;+money); &#125;&#125;-------------------------------------------------------------------/** * 对生产厂家要求的接口 */public interface IProducer &#123; /** * 销售 * @param money */ public void saleProduct(float money); /** * 售后 * @param money */ public void afterService(float money);&#125; 2.cglib动态代理 基于子类的动态代理 涉及的类：Enhancer ，提供者：第方cglib库 如何创建代理对象： 使用Enhancer类中的create方法 创建代理对象的要求： 被代理类是最终类 create方法的参数： Class：字节码 ：它是用于指定被代理对象的字节码。 Callback：用于提供增强的代码 ：它是让我们写如何代理。我们一般都是些一个该接口的实现类。 我们一般写的都是该接口的子接口实现类：**MethodInterceptor** 简单的实现12345678910111213141516171819202122232425262728293031323334/** * @author yinhuidong * @createTime 2020-03-02-1:08 */public class Test4 &#123; @Test public void test() &#123; final Producer producer = new Producer(); Producer cglibProducer = (Producer) Enhancer.create(producer.getClass(), new MethodInterceptor() &#123; /** * 执行被代理对象的任何方法都会经过该方法 * @param proxy * @param method * @param args * 以上个参数和基于接口的动态代理中invoke方法的参数是一样的 * @param methodProxy ：当前执行方法的代理对象 * @return * @throws Throwable */ public Object intercept(Object proxy, Method method, Object[] args, MethodProxy methodProxy) throws Throwable &#123; //提供增强的代码 Object returnValue = null; //1.获取方法执行的参数 Float money = (Float) args[0]; //2.判断当前方法是不是销售 if (&quot;saleProduct&quot;.equals(method.getName())) &#123; returnValue = method.invoke(producer, money * 0.8f); &#125; return returnValue; &#125; &#125;); cglibProducer.saleProduct(12000f); &#125;&#125; 二，多重代理1.基于责任链模式的多重代理1.1 被代理的方法123456789public interface Animal &#123; void eat(String food);&#125;public class Cat implements Animal&#123; @Override public void eat(String food) &#123; System.out.println(&quot;猫吃&quot;+food+&quot;!&quot;); &#125;&#125; 1.2封装目标对象的目标方法12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class TargetMethod &#123; /** * 目标对象 */ private Object target; /** * 目标方法 */ private Method method; /** * 方法参数 */ private Object[] args; public TargetMethod(Object target, Method method, Object[] args) &#123; this.target = target; this.method = method; this.args = args; &#125; public Object getTarget() &#123; return target; &#125; public void setTarget(Object target) &#123; this.target = target; &#125; public Method getMethod() &#123; return method; &#125; public void setMethod(Method method) &#123; this.method = method; &#125; public Object[] getArgs() &#123; return args; &#125; public void setArgs(Object[] args) &#123; this.args = args; &#125;&#125; 1.3 抽象的责任链节点和驱动责任链往下执行的头节点1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public abstract class AbstractSlot &#123; TargetMethod targetMethod; AbstractSlot next; abstract Object invoke(TargetMethod targetMethod); private boolean hasNextSlot() &#123; return next != null; &#125; public Object proceed(TargetMethod targetMethod) throws Exception &#123; return hasNextSlot() ? next.invoke(targetMethod) : targetMethod.getMethod() .invoke( targetMethod.getTarget(), targetMethod.getArgs() ); &#125; public AbstractSlot(TargetMethod targetMethod, AbstractSlot next) &#123; this.targetMethod = targetMethod; this.next = next; &#125; public TargetMethod getTargetMethod() &#123; return targetMethod; &#125; public void setTargetMethod(TargetMethod targetMethod) &#123; this.targetMethod = targetMethod; &#125; public AbstractSlot getNext() &#123; return next; &#125; public void setNext(AbstractSlot next) &#123; this.next = next; &#125; public AbstractSlot() &#123; &#125; public static class Head extends AbstractSlot&#123; @Override Object invoke(TargetMethod targetMethod) &#123; return null; &#125; &#125;&#125; 1.4 对方法增强的类1234567891011121314151617181920212223242526public class JdkDynamic implements InvocationHandler &#123; Object target ; AbstractSlot head; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; return head.proceed(new TargetMethod( target,method,args )); &#125; public Object getProxy()&#123; return Proxy.newProxyInstance( target.getClass().getClassLoader(), target.getClass().getInterfaces(), this ); &#125; public JdkDynamic(Object target, AbstractSlot head) &#123; this.target = target; this.head = head; &#125;&#125; 1.5测试类12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class Main &#123; public static void main(String[] args) &#123; AbstractSlot.Head head = new AbstractSlot.Head(); AbstractSlot first = new First(); AbstractSlot second=new Second(); first.setNext(second); head.setNext(first); JdkDynamic jdkDynamic = new JdkDynamic(new Cat(),head); Animal proxy = (Animal) jdkDynamic.getProxy(); proxy.eat(&quot;事物&quot;); &#125; private static class First extends AbstractSlot&#123; @Override Object invoke(TargetMethod targetMethod) &#123; Object result = null; System.out.println(&quot;增强逻辑&quot;); try &#123; result= proceed(targetMethod); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;增强逻辑&quot;); return result; &#125; &#125; private static class Second extends AbstractSlot&#123; @Override Object invoke(TargetMethod targetMethod) &#123; Object result = null; System.out.println(&quot;增强逻辑&quot;); try &#123; result= proceed(targetMethod); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;增强逻辑&quot;); return result; &#125; &#125;&#125; 2.基于拦截器的多重代理2.1 拦截器12345678910111213141516171819202122232425262728public interface MyInterceptor &#123; Object invoke(MyInvocation myInvocation);&#125;class One implements MyInterceptor&#123; @Override public Object invoke(MyInvocation myInvocation) &#123; Object result = null; System.out.println(&quot;1&quot;); result=myInvocation.proceed(); System.out.println(&quot;4&quot;); return result; &#125;&#125;class Two implements MyInterceptor&#123; @Override public Object invoke(MyInvocation myInvocation) &#123; Object result = null; System.out.println(&quot;2&quot;); result=myInvocation.proceed(); System.out.println(&quot;3&quot;); return result; &#125;&#125; 2.2 增强器123456789101112131415161718192021222324252627282930313233343536public interface MyInvocation &#123; Object proceed();&#125;class MyInvocationImpl implements MyInvocation&#123; List&lt;MyInterceptor&gt; interceptors = new ArrayList&lt;&gt;(); int size =0; TargetMethod targetMethod; @Override public Object proceed() &#123; try &#123; return size == interceptors.size()? targetMethod.getMethod() .invoke( targetMethod.getTarget(), targetMethod.getArgs() ): interceptors.get(size++) .invoke(this); &#125; catch (IllegalAccessException | InvocationTargetException e) &#123; e.printStackTrace(); &#125; return null; &#125; public MyInvocationImpl(List&lt;MyInterceptor&gt; interceptors, TargetMethod targetMethod) &#123; this.interceptors = interceptors; this.targetMethod = targetMethod; &#125;&#125; 2.3 对方法的增强类12345678910111213141516171819202122232425262728293031323334public class JdkProxy implements InvocationHandler &#123; Object target; List&lt;MyInterceptor&gt; interceptors = new ArrayList&lt;&gt;(); public void add(MyInterceptor a) &#123; interceptors.add(a); &#125; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; return new MyInvocationImpl( interceptors, new TargetMethod( target, method, args ) ).proceed(); &#125; public Object getProxy()&#123; return Proxy.newProxyInstance( target.getClass().getClassLoader(), target.getClass().getInterfaces(), this ); &#125; public JdkProxy(Object target) &#123; this.target = target; &#125;&#125; 2.4 测试类12345678910public class Main &#123; public static void main(String[] args) &#123; JdkProxy jdkProxy = new JdkProxy(new Cat()); jdkProxy.add(new One()); jdkProxy.add(new Two()); ((Animal) jdkProxy.getProxy()).eat(&quot;事务&quot;); &#125;&#125; 三，AOP相关概念 AOP：全称是 Aspect Oriented Programming 即：面向切面编程。就是把我们程序重复的代码抽取出来，在需要执行的时候，使用动态代理的技术，在不修改源码的基础上，对我们的已有方法进行增强。​ 作用：在程序运行期间，不修改源码对已有方法进行增强。 优势：减少重复代码提高开发效率维护方便 AOP 相关术语 Joinpoint(连接点):所谓连接点是指那些被拦截到的点。在 spring 中,这些点指的是方法,因为 spring 只支持方法类型的连接点。 Pointcut(切入点):所谓切入点是指我们要对哪些 Joinpoint 进行拦截的定义。 Advice(通知/增强):所谓通知是指拦截到 Joinpoint 之后所要做的事情就是通知。通知的类型：前置通知,后置通知,异常通知,最终通知,环绕通知。 Introduction(引介):引介是一种特殊的通知在不修改类代码的前提下, Introduction 可以在运行期为类动态地添加一些方法或 Field。 Target(目标对象):被代理对象代理的目标对象。 Weaving(织入):是指把增强应用到目标对象来创建新的代理对象的过程。spring 采用动态代理织入，而 AspectJ 采用编译期织入和类装载期织入。 Proxy（代理:一个类被 AOP 织入增强后，就产生一个结果代理类。 Aspect(切面):是切入点和通知（引介的结合） ​ Spring 框架监控切入点方法的执行。一旦监控到切入点方法被运行，使用代理机制，动态创建目标对象的代理对象，根据通知类别，在代理对象的对应位置，将通知对应的功能织入，完成完整的代码逻辑运行。​ 至此，我们通过两种方式完成了多重代理实现AOP，也简单介绍了两种实现动态代理的方式，和AOP相关的一些概念。下一篇将开始分析注解版Aop源码。 ​","categories":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/tags/Spring/"}]},{"title":"Spring[九]三级缓存&循环依赖","slug":"Spring/Spring[九]三级缓存&循环依赖","date":"2022-01-11T06:00:05.852Z","updated":"2022-01-11T06:08:29.459Z","comments":true,"path":"2022/01/11/Spring/Spring[九]三级缓存&循环依赖/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/Spring/Spring[%E4%B9%9D]%E4%B8%89%E7%BA%A7%E7%BC%93%E5%AD%98&%E5%BE%AA%E7%8E%AF%E4%BE%9D%E8%B5%96/","excerpt":"","text":"上一篇补充了一下Spring的组件，注解和扩展点，在开发中如果清楚的了解这些东西，会对你的案例设计，产生意想不到的效果。我呢，只是对这些组件注解扩展点进行一个介绍分析，具体的如何为业务赋能还需要结合实际的开发场景。 什么是循环依赖？ 有几种循环依赖？ Spring是如何解决循环依赖的？ Spring为什么用三级缓存解决循环依赖，用二级可不可以？ 当目标对象产生代理对象时，Spring容器中(第一级缓存)到底存储的是谁？一，问题&amp;答案1.什么是循环依赖 类和类之间的依赖关系形成了闭环，就叫做循环依赖。​ 2.有几种循环依赖 通过构造方法进行依赖注入时产生的循环依赖问题。 通过setter方法进行依赖注入且是在多例(原型)模式下产生的循环依赖问题。 通过setter方法进行依赖注入且是在单例模式下产生的循环依赖问题。 ​ 注意：在Spring中，只有【第三种方式】的循环依赖问题被解决了，其他两种方式在遇到循环依赖问题时都会产生异常。 第一种构造方法注入的情况下，在new对象的时候就会堵塞住了，其实也就是”先有鸡还是先有蛋“的历史难题。 第二种setter方法&amp;&amp;多例的情况下，每一次getBean()时，都会产生一个新的Bean，如此反复下去就会有无穷无尽的Bean产生了，最终就会导致OOM问题的出现。 回顾一下单实例bean创建的过程​ 3.Spring是如何解决循环依赖的？ 下载原图可点击这里​ 1.A创建过程中需要B，于是A将自己放到三级缓存去实例化B 2.B实例化的时候发现需要A，于是B先查一级缓存，没有，再查二级缓存，没有，再查三级缓存，找到了A，然后把三级缓存里面的A放到二级缓存里面，并删除三级缓存里面的A。 3.B顺利初始化完毕，将自己放到一级缓存里面（此时B里面的A还未创建完），然后接着回来创建A，此时B已经创建结束，直接从一级缓存里面拿到B，然后完成创建，并将A自己放到一级缓存里面。 spring解决循环依赖依靠的是Bean的中间态这个概念，而这个中间态指的是已经实例化但还没初始化的状态—–&gt;半成品。​ 12345678910//DefaultSingletonBeanRegistry//一级缓存：实例化完的beanprivate final Map&lt;String, Object&gt; singletonObjects = new ConcurrentHashMap&lt;&gt;(256);//三级缓存：单例bean工厂private final Map&lt;String, ObjectFactory&lt;?&gt;&gt; singletonFactories = new HashMap&lt;&gt;(16);//二级缓存：早期暴露的beanprivate final Map&lt;String, Object&gt; earlySingletonObjects = new HashMap&lt;&gt;(16); 4.Spring为什么用三级缓存解决循环依赖，用二级可不可以？为什么第三级缓存要使用ObjectFactory？需要提前产生代理对象。​ ​ 什么时候将Bean的引用提前暴露给第三级缓存的ObjectFactory持有？时机就是在第一步实例化之后，第二步依赖注入之前，完成此操作。​ 至此，我就解释清楚了整个三级缓存和循环依赖。不得不感慨，在我大三的时候，这道题还号称是阿里P7的面试题，大四的时候，我就连续四场面试被问到，行业内卷啊。","categories":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/tags/Spring/"}]},{"title":"Spring[八]注解&组件篇","slug":"Spring/Spring[八]注解&组件篇","date":"2022-01-11T05:59:54.649Z","updated":"2022-01-11T06:08:57.690Z","comments":true,"path":"2022/01/11/Spring/Spring[八]注解&组件篇/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/Spring/Spring[%E5%85%AB]%E6%B3%A8%E8%A7%A3&%E7%BB%84%E4%BB%B6%E7%AF%87/","excerpt":"","text":"写在前面 写在前面：最近在编译最新版spring源码的时候，踩了写小坑。先来看一下最新的官方文档：https://github.com/spring-projects/spring-framework在build from source里面声明了最新的编译过程：https://github.com/spring-projects/spring-framework/wiki/Build-from-Source由于最近spring做了一些升级，想要在本地构建spring源码，需要使用jdk17。此外，最近gradle的语法也因为版本的原因，改动很大。想要在spring源码里面对gradle进行一些定制化配置的时候，可以参照gradle官网。https://gradle.org/releases/ 1.注解1.1总览 @Bean 容器中注册组件 @Primary 同类组件如果有多个，标注主组件 @DependsOn 组件之间声明依赖关系 @Lazy 组件懒加载（最后使用的时候才会去创建） @Scope 声明组件的作用范围 @Configuration 声明这是一个配置类，替换xml @Component @Controller @Service @Repository @Indexed 加速注解，所有标注了@Indexed的组件，直接会启动快速加载 @Order 数字越小，优先级越高，越先工作 @ComponentScan 包扫描 @Conditional 条件注入 @Import 导入第三方jar包中的组件，或者定制批量导入组件逻辑 @ImportResource 导入以前的xml配置文件，让其生效 @Profile 基于多环境激活 @PropertySource 外部properties配置文件和javaBean进行绑定，结合ConfigurationProperties @PropertySources @PropertySource的组合注解 @Autowired 自动装配 @Qualifier 精准指定 @Resource jsr250规范的jdk自带注解 @Value 取值，计算机环境变量，jvm系统 xxx @Lookup 单例组件依赖非单例组件，非单例组件获取需要使用方法1.2.案例com.yhd.annotation.AnnoMainTest​ 2.组件与SPI扩展点机制2.1总览 基础接口 Resource+ResourceLoader 将来自各种不同渠道的配置文件等进行一层抽象封装，让开发人员不必关注于底层的细节实现，通过资源加载器加载资源。 BeanFactory ioc容器顶层接口 ✅ BeanDefinition 从resource解析出bean的定义信息 ✅ BeanDefinitionReader bean定义信息读取器 ，从resource 读取解析 BeanDefinition BeanDefinitionRegistry bean定义信息的注册中心 解析出来的BeanDefinition信息会被注册到这里 ApplicationContext ioc核心接口 ✅ Aware 实现xxxAware接口是为了能够获取到xxx相关的一些属性 ✅ BeanNameAware BeanFactoryAware ApplicationEventPublisherAware ApplicationContextAware ApplicationStartupAware BeanClassLoaderAware ImportAware EnvironmentAware 生命周期-后置处理器 BeanFactoryPostProcessor ✅ BeanDefinitionRegistryPostProcessor ✅ InitializingBean ✅ DisposableBean ✅ BeanPostProcessor ✅ SmartInitializingSingleton ✅ 监听器 ApplicationListener ✅2.2案例 \u0000com.yhd.annotation.AnnoMainTest​ 补充：spring源码地址：https://gitee.com/yin_huidong/spring.git​ \u0000此源码main分支为手动翻译好的中文注释版spring源码。 一，组件注册1.Configuration&amp;Bean@Configuration注解标识的类标识这是一个Spring的配置类。​ @Bean注解：给容器中注册一个bean，id默认是方法名作为id。​ value：指定id名 initMethod：指定初始化方法 destoryMethod：指定销毁方法 ​ 1.1 SpringConfig12345678@Configurationpublic class SpringConfig &#123; @Bean(value = &quot;person&quot;,initMethod = &quot;init&quot;,destroyMethod = &quot;destroy&quot;) public Person person()&#123; return new Person(1,&quot;二十&quot;); &#125;&#125; 1.2 Person1234567891011121314151617@Data@AllArgsConstructor@NoArgsConstructorpublic class Person &#123; private Integer id; private String name ; public void init()&#123; System.out.println(&quot;init()...&quot;); &#125; public void destroy()&#123; System.out.println(&quot;destroy()...&quot;); &#125;&#125; 1.3 Test12345678910public class TestA &#123; ApplicationContext ioc = new AnnotationConfigApplicationContext(SpringConfig.class); @Test public void testBean()&#123; Person person = ioc.getBean(&quot;person&quot;, Person.class); System.out.println(&quot;person = &quot; + person); &#125;&#125; 2.RunWith&amp;ContextConfiguration@RunWith：Spring整合Junit4。​ @ContextConfiguration：指定配置类。​ 1234567891011@RunWith(SpringRunner.class)@ContextConfiguration(classes = &#123;SpringConfig.class&#125;)public class TestA &#123; @Autowired private Person person; @Test public void testBean()&#123; System.out.println(&quot;person = &quot; + person); &#125;&#125; 3.ComponentScan@ComponentScan(“com.yhd”) 组件扫描 value:指定扫描的包 includeFilters = &#123;@ComponentScan.Filter(type = FilterType.ANNOTATION,classes = Controller.class)&#125; 只包含哪些包（需要指明：useDefaultFilters = false） type type:FilterType.ANNOTATION:按照注解过滤type=FilterType.ASSIGNABLE_TYPE：按照类型过滤type=ASPECTJ,切面type=REGEX,正则type=CUSTOM，定制（需要实现TypeFilter接口） excludeFilters:指明排除哪些包不被扫描 ​ 3.1 SpringConfig1234567891011121314@Configuration@ComponentScan(value = &quot;com.yhd&quot;, includeFilters = &#123; @ComponentScan.Filter(type = FilterType.ANNOTATION, classes = Controller.class), @ComponentScan.Filter(type = FilterType.ASSIGNABLE_TYPE) &#125;, excludeFilters = &#123;&#125;, useDefaultFilters = true)public class SpringConfig &#123; @Bean(value = &quot;person&quot;, initMethod = &quot;init&quot;, destroyMethod = &quot;destroy&quot;) public Person person() &#123; return new Person(1, &quot;二十&quot;); &#125;&#125; 3.2 Test12345678910111213141516@RunWith(SpringRunner.class)@ContextConfiguration(classes = &#123;SpringConfig.class&#125;)public class TestA &#123; @Autowired private ApplicationContext ioc; @Test public void test()&#123; //查看容器中bean的名字 String[] names = ioc.getBeanDefinitionNames(); for (String name : names) &#123; System.out.println(&quot;name = &quot; + name); &#125; &#125;&#125; 4.Scope作用：设置组件作用域。​ @Scope 默认是单例的点击进入该注解，在此点击进入文档注释中的ConfigurableBeanFactory可以看到该注解的几个取值： singleton 单例 prototype 多例 request 同一次请求 session 同一个session作用域 在scope=singleton时，对象在容器已创建立即加入容器。在scope=prototype时，对象每次调用的时候都会添加到容器。​ 4.1 SpringConfig123456789@Configurationpublic class SpringConfig &#123; @Bean(value = &quot;person&quot;, initMethod = &quot;init&quot;, destroyMethod = &quot;destroy&quot;) @Scope(&quot;prototype&quot;) public Person person() &#123; return new Person(1, &quot;二十&quot;); &#125;&#125; 4.2 Test123456@Testpublic void test()&#123; Person p1 = ioc.getBean(&quot;person&quot;, Person.class); Person p2 = ioc.getBean(&quot;person&quot;, Person.class); System.out.println(p1==p2?&quot;true&quot;:&quot;false&quot;);//false&#125; 5.Lazy@Lazy 懒加载单实例bean，默认在容器创建时候添加对象。懒加载：容器启动不创建对象，第一次获取/使用时在创建对象并初始化。​ 5.1 SpringConfig12345678910@Configurationpublic class SpringConfig &#123; @Lazy @Bean(value = &quot;person&quot;, initMethod = &quot;init&quot;, destroyMethod = &quot;destroy&quot;) public Person person() &#123; System.out.println(&quot;person已经加载到容器中！&quot;); return new Person(1, &quot;二十&quot;); &#125;&#125; 5.2 Test1234@Testpublic void test()&#123; Person person = ioc.getBean(&quot;person&quot;, Person.class);&#125; 对比加上这个注解前后，容器中单实例bean的变化：​ 6.Conditional@Conditional点击进入Class&lt;? extends Condition&gt;[] value();发现这个注解里面的value属性需要传入一个Condition数组，点击进入Conditionboolean matches(ConditionContext context, AnnotatedTypeMetadata metadata);因此，可以实现Condition接口，来使用该注解​ 当该注解加在方法上，标识满足条件时，该方法会执行当该注解加在类上时，表示满足条件时，该类里面的所有方法才会执行，否则一个都不执行。​ 123456789101112@Target(&#123;ElementType.TYPE, ElementType.METHOD&#125;)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface Conditional &#123; /** * All &#123;@link Condition&#125;s that must &#123;@linkplain Condition#matches match&#125; * in order for the component to be registered. */ Class&lt;? extends Condition&gt;[] value();&#125; 1234567891011121314@FunctionalInterfacepublic interface Condition &#123; /** * Determine if the condition matches. * @param context the condition context * @param metadata metadata of the &#123;@link org.springframework.core.type.AnnotationMetadata class&#125; * or &#123;@link org.springframework.core.type.MethodMetadata method&#125; being checked * @return &#123;@code true&#125; if the condition matches and the component can be registered, * or &#123;@code false&#125; to veto the annotated component&#x27;s registration */ boolean matches(ConditionContext context, AnnotatedTypeMetadata metadata);&#125; 6.1 案例：根据当前系统环境将对应的bean加入到容器6.1.1 SpringConfig1234567891011121314151617@Configurationpublic class SpringConfig &#123; @Bean(&quot;person&quot;) @Conditional(&#123;Mac.class&#125;) public Person person() &#123; System.out.println(&quot;person已经加载到容器中！&quot;); return new Person(1, &quot;二十&quot;); &#125; @Bean(&quot;person2&quot;) @Conditional(&#123;Linux.class&#125;) public Person person2() &#123; System.out.println(&quot;person已经加载到容器中！&quot;); return new Person(1, &quot;二十&quot;); &#125;&#125; 6.1.2 条件类12345678public class Mac implements Condition &#123; @Override public boolean matches(ConditionContext context, AnnotatedTypeMetadata metadata) &#123; return context.getEnvironment().getProperty(&quot;os.name&quot;).contains(&quot;Mac&quot;); &#125;&#125; 123456public class Linux implements Condition &#123; @Override public boolean matches(ConditionContext context, AnnotatedTypeMetadata metadata) &#123; return context.getEnvironment().getProperty(&quot;os.name&quot;).contains(&quot;linux&quot;); &#125;&#125; 6.1.3 Test12345678910111213141516@RunWith(SpringRunner.class)@ContextConfiguration(classes = &#123;SpringConfig.class&#125;)public class TestA &#123; @Autowired private ApplicationContext ioc; @Test public void test()&#123; Person person = ioc.getBean(&quot;person&quot;, Person.class); Person person2 = ioc.getBean(&quot;person2&quot;, Person.class); System.out.println(person); System.out.println(person2); &#125;&#125; 7.Import用来快速导入一个bean，默认类名是该类的全限定类名，单例。​ 7.1 SpringConfig123456789101112131415161718@Configuration@Import(&#123;Dog.class&#125;)public class SpringConfig &#123; @Bean(&quot;person&quot;) @Conditional(&#123;Mac.class&#125;) public Person person() &#123; System.out.println(&quot;person已经加载到容器中！&quot;); return new Person(1, &quot;二十&quot;); &#125; @Bean(&quot;person2&quot;) @Conditional(&#123;Linux.class&#125;) public Person person2() &#123; System.out.println(&quot;person已经加载到容器中！&quot;); return new Person(1, &quot;二十&quot;); &#125;&#125; 7.2 Test12345678910@Testpublic void test()&#123; Dog dog = ioc.getBean(Dog.class); assert dog!=null; dog.setId(1); dog.setName(&quot;asdfg&quot;); System.out.println(&quot;dog = &quot; + dog);&#125; 8.ImportSelector123456789public @interface Import &#123; /** * &#123;@link Configuration&#125;, &#123;@link ImportSelector&#125;, &#123;@link ImportBeanDefinitionRegistrar&#125; * or regular component classes to import. */ Class&lt;?&gt;[] value();&#125; 点击查看@Import注解可以发现：​ 里面还可以传入{@link Configuration}, {@link ImportSelector}, {@link ImportBeanDefinitionRegistrar}​ importSelector其实就是一个接口，我们需要通过实现它来传入impot注解。​ 8.1 MyImportSelector123456public class MyImportSelector implements ImportSelector &#123; @Override public String[] selectImports(AnnotationMetadata importingClassMetadata) &#123; return new String[]&#123;&quot;com.yhd.pojo.Person&quot;,&quot;com.yhd.pojo.Dog&quot;&#125;; &#125;&#125; 8.2 SpringConfig12345@Configuration@Import(&#123;MyImportSelector.class&#125;)public class SpringConfig &#123; &#125; 8.3 Test12345678910111213@RunWith(SpringRunner.class)@ContextConfiguration(classes = &#123;SpringConfig.class&#125;)public class TestA &#123; @Autowired private ApplicationContext ioc; @Test public void test()&#123; String[] names = ioc.getBeanDefinitionNames(); for (String name : names) System.out.println(name); &#125;&#125; 9.ImportBeanDefinitionRegistrar1234567public interface ImportBeanDefinitionRegistrar &#123; public void registerBeanDefinitions( AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry);&#125; 9.1 MyImportBeanDefinitionRegistrar123456public class MyImportBeanDefinitionRegistrar implements ImportBeanDefinitionRegistrar &#123; @Override public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) &#123; registry.registerBeanDefinition(&quot;dog&quot;, new RootBeanDefinition(&quot;com.yhd.pojo.Dog&quot;)); &#125;&#125; 9.2 MyImportBeanDefinitionRegistrar12345@Configuration@Import(&#123;MyImportBeanDefinitionRegistrar.class&#125;)public class SpringConfig &#123;&#125; 9.3 Test12345678910111213@RunWith(SpringRunner.class)@ContextConfiguration(classes = &#123;SpringConfig.class&#125;)public class TestA &#123; @Autowired private ApplicationContext ioc; @Test public void test()&#123; String[] names = ioc.getBeanDefinitionNames(); for (String name : names) System.out.println(name); &#125;&#125; 10.FactoryBean​ spring的工厂模式造Bean 如果传入id获取到的是工厂造的bean 如果传入的是&amp;id获取到的是工厂本身 ​ 10.1 PersonFactory12345678910111213141516public class PersonFactory implements FactoryBean&lt;Person&gt; &#123; @Override public Person getObject() throws Exception &#123; return new Person(1,&quot;FactoryBean&quot;); &#125; @Override public Class&lt;?&gt; getObjectType() &#123; return Person.class; &#125; @Override public boolean isSingleton() &#123; return true; &#125;&#125; 10.2 SpringConfig12345678@Configurationpublic class SpringConfig &#123; @Bean public PersonFactory personFactory()&#123; return new PersonFactory(); &#125;&#125; 10.3 Test12345678910111213141516171819@RunWith(SpringRunner.class)@ContextConfiguration(classes = &#123;SpringConfig.class&#125;)public class TestA &#123; @Autowired private ApplicationContext ioc; @Test public void test()&#123; PersonFactory personFactory = (PersonFactory) ioc.getBean(&quot;&amp;personFactory&quot;); System.out.println(&quot;personFactory = &quot; + personFactory); Person person = (Person) ioc.getBean(&quot;personFactory&quot;); System.out.println(&quot;person = &quot; + person); /*personFactory = com.yhd.factory.PersonFactory@6e20b53a person = Person(id=1, name=FactoryBean)*/ &#125;&#125; 10.4 原理1234public interface BeanFactory &#123; //more能获取到工厂bean的方法就是在id前加上前缀&amp; String FACTORY_BEAN_PREFIX = &quot;&amp;&quot;; 11.LookUp​ 如果有一个类C,需要用到类B,如果使用@Autowired注解注入B,那么B每次调用都是同一个对象，即使B不是单例的，现在我希望每次调用B都是不一样的，那么实现方案有2个：​ 11.1 每次从容器中获取B123456789101112131415161718192021@Component@Scope(scopeName= ConfigurableBeanFactory.SCOPE_PROTOTYPE) //原型 也就是非单例public class B &#123; public void sayHi()&#123; System.out.println(&quot;hi&quot;); &#125;&#125;@Componentpublic class C implements ApplicationContextAware &#123; private ApplicationContext applicationContext; public void hello()&#123; B b = (B)applicationContext.getBean(&quot;b&quot;); b.sayHi(); &#125; @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; this.applicationContext=applicationContext; &#125;&#125; 11.2 使用@lookup注解12345678910111213141516171819@Component@Scope(scopeName= ConfigurableBeanFactory.SCOPE_PROTOTYPE) //原型 也就是非单例public class B &#123; public void sayHi()&#123; System.out.println(&quot;hi&quot;); &#125;&#125;@Componentpublic abstract class C &#123; public void hello()&#123; B b = getB(); b.sayHi(); &#125; @Lookup public abstract B getB(); //一般都是抽象方法&#125; 二，生命周期1.@Bean指定初始化和销毁方法bean的生命周期：创建，初始化，使用，销毁。​ 容器管理bean的生命周期：我们可以自定义初始化和销毁方法，容器在bean进行到当前生命周期的时候，来调用我们自定义的初始化和销毁方法。​ 单例模式：先是执行对象的无参构造，赋值后，执行初始化方法，在创建容器的时候加入对象，在容器关闭时，执行销毁方法。​ 多例模式：先创建容器，每次调用对象时，调用无参构造方法，赋值后，执行初始化方法。对象的销毁由java垃圾回收机制回收。​ 1.1 单例1234567891011121314151617181920@Data@AllArgsConstructorpublic class Person &#123; private Integer id; private String name ; public void init()&#123; System.out.println(&quot;init()...&quot;); &#125; public void destroy()&#123; System.out.println(&quot;destroy()...&quot;); &#125; public Person()&#123; System.out.println(&quot;constructor()....&quot;); &#125;&#125; 12345678@Configurationpublic class SpringConfig &#123; @Bean(value = &quot;person&quot;,initMethod = &quot;init&quot;,destroyMethod = &quot;destroy&quot;) public Person person()&#123; return new Person(); &#125;&#125; 12345678910111213@RunWith(SpringRunner.class)@ContextConfiguration(classes = &#123;SpringConfig.class&#125;)public class TestA &#123; @Autowired private ApplicationContext ioc; //constructor @Test public void test()&#123; Person person = ioc.getBean(&quot;person&quot;, Person.class); System.out.println(&quot;person = &quot; + person); &#125;&#125; 1.2 多例12345678@Configurationpublic class SpringConfig &#123; @Scope(value = &quot;prototype&quot;) @Bean(value = &quot;person&quot;,initMethod = &quot;init&quot;,destroyMethod = &quot;destroy&quot;) public Person person()&#123; return new Person(); &#125;&#125; 2.实现接口完成对象的初始化和销毁InitializingBean, DisposableBean​ 1234567891011121314151617181920212223@Data@AllArgsConstructorpublic class Person implements InitializingBean, DisposableBean &#123; private Integer id; private String name ; public Person()&#123; System.out.println(&quot;constructor()....&quot;); &#125; @Override public void destroy() throws Exception &#123; System.out.println(&quot;destroy()....&quot;); &#125; @Override public void afterPropertiesSet() throws Exception &#123; System.out.println(&quot;afterPropertiesSet()....&quot;); &#125; &#125; 12345678@Configurationpublic class SpringConfig &#123; @Bean(value = &quot;person&quot;) public Person person()&#123; return new Person(); &#125;&#125; 1234567891011121314@RunWith(SpringRunner.class)@ContextConfiguration(classes = &#123;SpringConfig.class&#125;)public class TestA &#123; @Autowired private ApplicationContext ioc; //constructor @Test public void test()&#123; System.out.println(&quot;容器创建成功！&quot;); Person person = ioc.getBean(&quot;person&quot;, Person.class); System.out.println(&quot;person = &quot; + person); &#125;&#125; 3.BeanPostProcessorbean的后置处理器，在bean的初始化前后做一些处理工作，需要加入到容器中。​ 1234567891011121314public class MyBeanPostProcessor implements BeanPostProcessor &#123; @Override public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException &#123; System.out.println(&quot;初始化前&quot;); return BeanPostProcessor.super.postProcessBeforeInitialization(bean, beanName); &#125; @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException &#123; System.out.println(&quot;初始化后&quot;); return BeanPostProcessor.super.postProcessAfterInitialization(bean, beanName); &#125;&#125; 123456789@Configuration@Import(MyBeanPostProcessor.class)public class SpringConfig &#123; @Bean(value = &quot;person&quot;) public Person person()&#123; return new Person(); &#125;&#125; 4.BeanFactoryPostProcessorbean工厂的后置处理器，可以在spring解析完配置文件，创建对象之前，对bean的定义信息进行修改。 12345678910111213@Componentpublic class Computer &#123; String name; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125;&#125; 12345678910@Componentpublic class MyBeanFactoryPostProcessor implements BeanFactoryPostProcessor &#123; @Override public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException &#123; System.out.println(&quot;=======BeanFactoryPostProcessor=========&quot;); BeanDefinition person = beanFactory.getBeanDefinition(&quot;computer&quot;); person.setScope(BeanDefinition.SCOPE_PROTOTYPE); System.out.println(&quot;=======BeanFactoryPostProcessor=========&quot;); &#125;&#125; 123456789101112/** * 默认Computer对象是单实例的， * 通过beanFactory的后置处理器处理之后，变成原型的 * &lt;p&gt; * 执行时机：打断点 */private static void testBeanFactoryPostProcessor() &#123; /*ioc容器创建的12个核心方法里面*/ Computer computer = ioc.getBean(Computer.class); Computer computer2 = ioc.getBean(Computer.class); System.out.println(computer == computer2);&#125; 5.BeanDefinitionRegistryPostProcessor他是bean工厂后置处理器的子类插播一条广告：BeanFactoryPostProcessor和BeanDefinitionRegistryPostProcessor之间的关系？BeanDefinitionRegistryPostProcessor 是 BeanFactoryPostProcessor的子类和扩展它里面 搞了一个新的方法 postProcessBeanDefinitionRegistry ，可以往容器中注册更多的bd信息。扩展点：①BeanFactoryPostProcessor 对bd信息进行修改②postProcessBeanDefinitionRegistry 添加更多的bd信息 12public class RegistryBean &#123;&#125; 123456789101112131415161718@Componentpublic class MyBeanDefinitionRegistryPostProcessor implements BeanDefinitionRegistryPostProcessor &#123; @Override public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException &#123; /*这个是她的父类里面的方法 对 bd信息进行修改*/ &#125; @Override public void postProcessBeanDefinitionRegistry(BeanDefinitionRegistry registry) throws BeansException &#123; /*这个是给他本身的方法，可以添加额外的 bd 信息*/ GenericBeanDefinition beanDefinition = new GenericBeanDefinition(); beanDefinition.setBeanClass(RegistryBean.class); //beanDefinition.setScope(); registry.registerBeanDefinition(&quot;registryBean&quot;,beanDefinition); &#125;&#125; 1234private static void testBeanDefinitionRegistryPostProcessor() &#123; RegistryBean registryBean = ioc.getBean(&quot;registryBean&quot;, RegistryBean.class); System.out.println(&quot;registryBean = &quot; + registryBean);&#125; 6.SmartInitializingSingleton在所有的单实例bean 通过getBean方法完成初始化之后,就会去查找这个类型的SmartInitializingSingleton 的 组件 ，执行里面的 方法. 1234567@Componentpublic class MySmartInitializingSingleton implements SmartInitializingSingleton &#123; @Override public void afterSingletonsInstantiated() &#123; System.out.println(&quot;断点调试。。。。&quot;); &#125;&#125; 7.Lifecycle &amp;&amp; SmartLifecycle容器创建完成之后的回调，相当于传递的参数autoStartUpOnly是干嘛的？表示只启动SmartLifeCycle生命周期对象，并且启动的对象autoStartUpOnly必须是true，不会启动普通的生命周期对象，false的时候，会启动全部的生命周期对象。 1234567891011121314151617181920public class DemoLifeCycle implements Lifecycle &#123; private boolean running =false; @Override public void start() &#123; this.running=true; System.out.println(&quot;demo one start!&quot;); &#125; @Override public void stop() &#123; this.running=false; System.out.println(&quot;demo one stop!&quot;); &#125; @Override public boolean isRunning() &#123; return running; &#125;&#125; 1234567891011121314151617181920public class DemoSmartLifeCycle implements SmartLifecycle &#123; private boolean running = false; @Override public void start() &#123; this.running=true; System.out.println(&quot;demo two start!&quot;); &#125; @Override public void stop() &#123; this.running=false; System.out.println(&quot;demo two stop!&quot;); &#125; @Override public boolean isRunning() &#123; return running; &#125;&#125; 三，属性赋值1.Value@Value注解可以给属性赋值，支持SPEL表达式，${},,基本数值。​ 12345678910111213@Data@AllArgsConstructorpublic class Person &#123; @Value(&quot;1&quot;) private Integer id; @Value(&quot;二十&quot;) private String name ; public Person()&#123; System.out.println(&quot;constructor()....&quot;); &#125; &#125; 2.PropertySource@PropertySource加载外部属性文件​ 支持一次写多个，标注在配置类上。​ 12person.name=二十person.age=20 123456789@Data@AllArgsConstructor@NoArgsConstructorpublic class Person &#123; @Value(&quot;$&#123;person.age&#125;&quot;) private Integer id; @Value(&quot;$&#123;person.name&#125;&quot;) private String name ;&#125; 123456789@Configuration@PropertySource(&quot;classpath:person.properties&quot;)public class SpringConfig &#123; @Bean(value = &quot;person&quot;) public Person person()&#123; return new Person(); &#125;&#125; 四，自动装配Spring利用依赖注入DI，完成对IOC容器中各个组件的依赖关系赋值。​ 1.@Autowired &amp;&amp; @Qualifier &amp;&amp; @Primary 默认优先按照类型去容器中找对应的组件，ioc.getBean(BookDao.class); 如果找到多个相同类型的组件，再讲属性的名称作为组件的id去容器中查找ioc.getBean(&quot;bookDao&quot;); @Qualifier(&quot;bookDao&quot;),使用@Qualifier指定要装配的组件的id，而不是使用属性名 自动装配默认一定要将属性赋值好，没有就会报错.可以使用@Autowired（required=false） @Primary 让Spring进行自动装配的时候，默认使用首选bean，也可以继续使用@Qualifier指定需要装配的bean的id ​ 2.@Resource &amp;&amp; @Injectspring还支持使用@Resource（jsr250）和@inject（jsr330）【java规范的注解】​ @Resource：可以和@Autowired一样实现自动装配，默认按照组件名称进行装配.不支持@Primary和required=false​ @Inject：需要导入javax.inject包，没有required=false​ 3.方法，构造器位置的自动注入@Autowired：构造器，参数，方法，属性，都是从容器中获取参数组件的值。 标注在方法上：@Bean+方法参数，参数从容器中获取，默认不写@Autowired效果是一样的，都能自动装配 标在构造器上：如果组件只有一个构造器，这个有参构造器的@Autowired可以省略 放在参数位置 ​ 3.1 构造器注入12345678@Component@Data@AllArgsConstructor@NoArgsConstructorpublic class Person &#123; private Integer id; private String name ;&#125; 12345678910111213@Component@Data@NoArgsConstructorpublic class Dog &#123; private Person person; @Autowired //此处注解可以省略 private Dog(Person person)&#123; this.person=person; &#125;&#125; 12345678910111213@RunWith(SpringRunner.class)@ContextConfiguration(classes = &#123;SpringConfig.class&#125;)public class TestA &#123; @Autowired private ApplicationContext ioc; //constructor @Test public void test()&#123; Dog dog = ioc.getBean(&quot;dog&quot;, Dog.class); System.out.println(&quot;dog = &quot; + dog); &#125;&#125; 3.2 set()注入12345678910111213@Component@Data@NoArgsConstructorpublic class Dog &#123; private Person person; @Autowired //此处的注解不能省略 public void setPerson(Person person)&#123; this.person=person; &#125;&#125; 3.3 @Bean的方式123456789101112131415@Configurationpublic class SpringConfig &#123; @Bean(value = &quot;person&quot;) public Person person() &#123; return new Person(); &#125; @Bean(value = &quot;dog&quot;) @Autowired //此处的注解可以省略不写 public Dog dog(Person person) &#123; return new Dog(person); &#125;&#125; 4.Aware接口，自定义组件使用Spring底层的组件自定义接口想要使用Spring底层的一些组件（ApplicationContext，BeanFactory，xxx）​ 自定义接口实现xxxAware，在创建对象的时候，会调用接口规定的方法注入相关组件，Aware吧Spring底层的一些接口注入到自定义的Bean中​ xxxAware：功能：使用xxxProcessor​ ApplicationContextAware--&gt;ApplicationContextAwareProcessor​ 1234567891011121314151617@Componentpublic class Dog implements ApplicationContextAware, BeanFactoryAware &#123; private ApplicationContext ioc; private BeanFactory beanFactory; @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; //自此处打断点 this.ioc=applicationContext; &#125; @Override public void setBeanFactory(BeanFactory beanFactory) throws BeansException &#123; //此处打断点 this.beanFactory=beanFactory; &#125;&#125; 5.@Profile根据环境装配@Profile:指定组件在那个环境的情况下才能被注册到容器中，不指定，任何环境下都能注册这个组件 加了环境标识的bean，只有这个环境被激活的时候才能注册到容器中，默认是default环境 写在配置类上，只有是指定的环境的时候，整个配置类里面的所有配置才能开始生效 没有标识环境标识的bean在任何环境下都是加载的。 ​ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748@Configuration@PropertySource(&quot;classpath:jdbc.properties&quot;)public class SpringConfig implements EmbeddedValueResolverAware &#123; @Value(&quot;$&#123;jdbc.username&#125;&quot;) private String name; private StringValueResolver resolver; private String driver; @Profile(&quot;dev&quot;) @Bean(&quot;dev&quot;) public DataSource dataSource1(@Value(&quot;$&#123;jdbc.password&#125;&quot;) String pwd)&#123; DruidDataSource source = new DruidDataSource(); source.setName(name); source.setPassword(pwd); source.setDriverClassName(driver); source.setUrl(&quot;jdbc:mysql:///test&quot;); return source; &#125; @Profile(&quot;pro&quot;) @Bean(&quot;pro&quot;) public DataSource dataSource2(@Value(&quot;$&#123;jdbc.password&#125;&quot;) String pwd)&#123; DruidDataSource source = new DruidDataSource(); source.setName(name); source.setPassword(pwd); source.setDriverClassName(driver); source.setUrl(&quot;jdbc:mysql:///ssm&quot;); return source; &#125; @Profile(&quot;test&quot;) @Bean(&quot;test&quot;) public DataSource dataSource3(@Value(&quot;$&#123;jdbc.password&#125;&quot;) String pwd)&#123; DruidDataSource source = new DruidDataSource(); source.setName(name); source.setPassword(pwd); source.setDriverClassName(driver); source.setUrl(&quot;jdbc:mysql:///zuoye&quot;); return source; &#125; @Override public void setEmbeddedValueResolver(StringValueResolver resolver) &#123; this.resolver=resolver; driver = resolver.resolveStringValue(&quot;$&#123;jdbc.driverClassName&#125;&quot;); &#125;&#125; 激活环境： 使用命令行动态参数：在虚拟机参数位置加载： -Dspring.profiles.active=test 代码方式激活 ​ 123456789101112@Testpublic void test() &#123; AnnotationConfigApplicationContext ioc = new AnnotationConfigApplicationContext(); ioc.getEnvironment().setActiveProfiles(&quot;dev&quot;); ioc.register(SpringConfig4.class); ioc.refresh(); String[] names = ioc.getBeanNamesForType(DataSource.class); for (String name : names) &#123; System.out.println(name); &#125;&#125; 五，事件驱动1.ApplicationListener &amp;&amp; ApplicationEvent通过自定义不同类型的事件，使用不同的监听器监听不同类型的事件，做到jvm进程内的消息队列，事件驱动，解耦。 1234567891011121314151617public class MyEvent extends ApplicationEvent &#123; String message; public MyEvent(Object source) &#123; super(source); &#125; public MyEvent(Object source,String message) &#123; super(source); this.message=message; &#125; public void print()&#123; System.out.println(&quot;发布了一个事件：&quot;+message); &#125;&#125; 12345678public class MyListener implements ApplicationListener&lt;MyEvent&gt; &#123; @Override public void onApplicationEvent(MyEvent event) &#123; event.print(); &#125;&#125; 1234567/** * 测试spring 的 ioc 容器的事件发布 */private static void testPublishEvent() &#123; ApplicationContext ioc = new ClassPathXmlApplicationContext(CONFIG_LOCATION); ioc.publishEvent(new MyEvent(&quot;&quot;, &quot;这是我自定义的一个事件&quot;));&#125; 2.@EventListener对上面写法的一个优化，更加简洁，开发量更少，懒人必备神器。 12345678private static void testEventListener() &#123; ioc.publishEvent(new ApplicationEvent(&quot;hello，spring&quot;) &#123; @Override public Object getSource() &#123; return super.getSource(); &#125; &#125;);&#125; 12345678@Componentpublic class MyEventListener &#123; @EventListener(classes = ApplicationEvent.class) public void listener(ApplicationEvent event)&#123; System.out.println(&quot;event = &quot; + event); &#125;&#125;","categories":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/tags/Spring/"}]},{"title":"Spring[七]initializeBean()","slug":"Spring/Spring[七]initializeBean()","date":"2022-01-11T05:59:43.828Z","updated":"2022-01-11T06:09:21.774Z","comments":true,"path":"2022/01/11/Spring/Spring[七]initializeBean()/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/Spring/Spring[%E4%B8%83]initializeBean()/","excerpt":"","text":"上一篇分析了单实例bean创建过程中的属性赋值，接下来我们分析，单实例bean属性赋值之后的初始化**initializeBean()**。 1. initializeBean()123456789101112131415161718192021222324252627protected Object initializeBean(String beanName, Object bean, @Nullable RootBeanDefinition mbd) &#123; /*检查当前bean是否实现了aware接口，再具体判断实现的哪个aware接口，做一些赋能操作。*/ invokeAwareMethods(beanName, bean); Object wrappedBean = bean; if (mbd == null || !mbd.isSynthetic()) &#123; /*初始化之前，后置处理器的调用点*/ wrappedBean = applyBeanPostProcessorsBeforeInitialization(wrappedBean, beanName); &#125; try &#123; /*执行初始化方法*/ invokeInitMethods(beanName, wrappedBean, mbd); &#125; catch (Throwable ex) &#123; throw new BeanCreationException( (mbd != null ? mbd.getResourceDescription() : null), beanName, &quot;Invocation of init method failed&quot;, ex); &#125; if (mbd == null || !mbd.isSynthetic()) &#123; /*初始化后的后置处理器执行点*/ /*典型应用：AOP的具体实现*/ wrappedBean = applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName); &#125; return wrappedBean;&#125; 在初始化之前，把当前bean实现的所有Aware接口都执行一遍。**invokeAwareMethods()**在初始化之前，调用后置处理器。**applyBeanPostProcessorsBeforeInitialization()**执行初始化逻辑。**invokeInitMethods()**执行后置处理器的调用点，典型的实现其实就是AOP。**applyBeanPostProcessorsAfterInitialization()**返回包装对象。​ 2.invokeAwareMethods()12345678910111213141516private void invokeAwareMethods(String beanName, Object bean) &#123; if (bean instanceof Aware) &#123; if (bean instanceof BeanNameAware) &#123; ((BeanNameAware) bean).setBeanName(beanName); &#125; if (bean instanceof BeanClassLoaderAware) &#123; ClassLoader bcl = getBeanClassLoader(); if (bcl != null) &#123; ((BeanClassLoaderAware) bean).setBeanClassLoader(bcl); &#125; &#125; if (bean instanceof BeanFactoryAware) &#123; ((BeanFactoryAware) bean).setBeanFactory(AbstractAutowireCapableBeanFactory.this); &#125; &#125;&#125; 执行相关的Aware接口方法。 3.applyBeanPostProcessorsBeforeInitialization()​ 123456789101112131415@Overridepublic Object applyBeanPostProcessorsBeforeInitialization(Object existingBean, String beanName) throws BeansException &#123; Object result = existingBean; /*beanPostProcessor的执行逻辑*/ for (BeanPostProcessor processor : getBeanPostProcessors()) &#123; Object current = processor.postProcessBeforeInitialization(result, beanName); if (current == null) &#123; return result; &#125; result = current; &#125; return result;&#125; 循环所有的后置处理器，执行前置方法，如果有一个返回结果为空，直接短路，后面的都不执行了。​ 4.invokeInitMethods()1234567891011121314151617181920212223protected void invokeInitMethods(String beanName, Object bean, @Nullable RootBeanDefinition mbd) throws Throwable &#123; /*如果是实现了InitializingBean接口*/ boolean isInitializingBean = (bean instanceof InitializingBean); if (isInitializingBean &amp;&amp; (mbd == null || !mbd.isExternallyManagedInitMethod(&quot;afterPropertiesSet&quot;))) &#123; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Invoking afterPropertiesSet() on bean with name &#x27;&quot; + beanName + &quot;&#x27;&quot;); &#125; /*执行afterPropertiesSet方法*/ ((InitializingBean) bean).afterPropertiesSet(); &#125; if (mbd != null &amp;&amp; bean.getClass() != NullBean.class) &#123; /*判断重写init方法没？*/ String initMethodName = mbd.getInitMethodName(); if (StringUtils.hasLength(initMethodName) &amp;&amp; !(isInitializingBean &amp;&amp; &quot;afterPropertiesSet&quot;.equals(initMethodName)) &amp;&amp; !mbd.isExternallyManagedInitMethod(initMethodName)) &#123; /*执行重写的init方法*/ invokeCustomInitMethod(beanName, bean, mbd); &#125; &#125;&#125; 判断实现了**InitializingBean**接口的，执行里面的方法。判断重写了初始化方法的，执行重写的方法。 4.1 invokeCustomInitMethod()1234567891011121314151617181920212223242526272829303132333435363738394041424344protected void invokeCustomInitMethod(String beanName, Object bean, RootBeanDefinition mbd) throws Throwable &#123; /*从bd获取init方法*/ String initMethodName = mbd.getInitMethodName(); Assert.state(initMethodName != null, &quot;No init method set&quot;); /* * isNonPublicAccessAllowed 非公开访问 * * 这里完成之后就会获取到通过init方法定义的方法对象。 * */ Method initMethod = (mbd.isNonPublicAccessAllowed() ? BeanUtils.findMethod(bean.getClass(), initMethodName) : ClassUtils.getMethodIfAvailable(bean.getClass(), initMethodName)); /*如果init方法为空*/ if (initMethod == null) &#123; if (mbd.isEnforceInitMethod()) &#123; throw new BeanDefinitionValidationException(&quot;Could not find an init method named &#x27;&quot; + initMethodName + &quot;&#x27; on bean with name &#x27;&quot; + beanName + &quot;&#x27;&quot;); &#125; else &#123; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;No default init method named &#x27;&quot; + initMethodName + &quot;&#x27; found on bean with name &#x27;&quot; + beanName + &quot;&#x27;&quot;); &#125; // Ignore non-existent default lifecycle methods. return; &#125; &#125; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Invoking init method &#x27;&quot; + initMethodName + &quot;&#x27; on bean with name &#x27;&quot; + beanName + &quot;&#x27;&quot;); &#125; /*将init方法转换成接口层面获取的initMethod*/ Method methodToInvoke = ClassUtils.getInterfaceMethodIfPossible(initMethod); try &#123; /*反射执行方法*/ ReflectionUtils.makeAccessible(methodToInvoke); methodToInvoke.invoke(bean); &#125; catch (InvocationTargetException ex) &#123; throw ex.getTargetException(); &#125;&#125; 经过一些列处理，反射调用初始化方法。 5.applyBeanPostProcessorsAfterInitialization()1234567891011121314151617@Overridepublic Object applyBeanPostProcessorsAfterInitialization(Object existingBean, String beanName) throws BeansException &#123; Object result = existingBean; for (BeanPostProcessor processor : getBeanPostProcessors()) &#123; Object current = processor.postProcessAfterInitialization(result, beanName); /*注意： * 一旦某个后置处理器返回的结果为空 * 就返回上一个后置处理器的结果，后面的后置处理器方法不在执行*/ if (current == null) &#123; return result; &#125; result = current; &#125; return result;&#125; 初始化之后，遍历所有的后置处理器，执行后置处理器的后置方法，如果有一个返回结果是空，直接返回，后面的后置处理器都不执行了。​ 至此，整个ioc容器的刷新流程就都分析完了，重点就在于单实例bean的创建流程，首先会去**getBean() doGetBean() getSinglton() createBean() doCreateBean() populateBean() initializeBean()**。下一篇我们将会去分析Spring的三级缓存和循环依赖。","categories":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/tags/Spring/"}]},{"title":"Spring[六]populateBean()","slug":"Spring/Spring[六]populateBean()","date":"2022-01-11T05:59:34.475Z","updated":"2022-01-11T06:09:48.766Z","comments":true,"path":"2022/01/11/Spring/Spring[六]populateBean()/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/Spring/Spring[%E5%85%AD]populateBean()/","excerpt":"","text":"在上一篇中我们分析完了**createBean()**，并详解讲解了三中创建单实例bean对象的方法，在bean实例创建出来之后，就是对bean的属性赋值和初始化操作，本篇我们继续往下分析。 1.populateBean()1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192protected void populateBean(String beanName, RootBeanDefinition mbd, @Nullable BeanWrapper bw) &#123; /*这里是判断假如beanWrapper是空，但是mbd中的属性还有值，那就说明需要抛出异常，因为没法给一个null赋值。*/ if (bw == null) &#123; if (mbd.hasPropertyValues()) &#123; throw new BeanCreationException( mbd.getResourceDescription(), beanName, &quot;Cannot apply property values to null instance&quot;); &#125; else &#123; // Skip property population phase for null instance. return; &#125; &#125; /*在设置属性之前，给任何实例化的bean后处理器修改bean状态的机会。例如，这可以用来支持各种类型的属性注入。*/ /* * mbd.isSynthetic()默认是false，取反成立。 * 判断有没有instantiationAwareBeanPostProcessors，条件成立说明有。 */ if (!mbd.isSynthetic() &amp;&amp; hasInstantiationAwareBeanPostProcessors()) &#123; /*这里又是后置处理器的一个调用点：实例化之后的调用，调用的是后置处理器的afterInstantiation方法。*/ for (InstantiationAwareBeanPostProcessor bp : getBeanPostProcessorCache().instantiationAware) &#123; /*返回值将决定当前实例需要在进行依赖注入处理，默认返回true*/ if (!bp.postProcessAfterInstantiation(bw.getWrappedInstance(), beanName)) &#123; return; &#125; &#125; &#125; /*下面是处理依赖注入的逻辑*/ /*三元运算符获取属性，有就把属性赋值给pvs ，没有就给一个null*/ PropertyValues pvs = (mbd.hasPropertyValues() ? mbd.getPropertyValues() : null); int resolvedAutowireMode = mbd.getResolvedAutowireMode(); /*这里判断依赖注入是按照名字注入还是按照类型注入，根据不同的选择走不同的处理逻辑。*/ if (resolvedAutowireMode == AUTOWIRE_BY_NAME || resolvedAutowireMode == AUTOWIRE_BY_TYPE) &#123; /*吧属性进行一个包装*/ MutablePropertyValues newPvs = new MutablePropertyValues(pvs); // Add property values based on autowire by name if applicable. /*按照名字注入，根据字段名称查找依赖bean完成注入*/ if (resolvedAutowireMode == AUTOWIRE_BY_NAME) &#123; autowireByName(beanName, mbd, bw, newPvs); &#125; // Add property values based on autowire by type if applicable. /*按照类型注入*/ if (resolvedAutowireMode == AUTOWIRE_BY_TYPE) &#123; autowireByType(beanName, mbd, bw, newPvs); &#125; /*相当于处理了依赖数据后的pvs*/ pvs = newPvs; &#125; /*表示当前是否拥有instantiationAwareBeanPostProcessors需要执行*/ boolean hasInstAwareBpps = hasInstantiationAwareBeanPostProcessors(); /*是否需要进行依赖检察，不重要*/ boolean needsDepCheck = (mbd.getDependencyCheck() != AbstractBeanDefinition.DEPENDENCY_CHECK_NONE); PropertyDescriptor[] filteredPds = null; if (hasInstAwareBpps) &#123; if (pvs == null) &#123; pvs = mbd.getPropertyValues(); &#125; /*后置处理器的调用点*/ for (InstantiationAwareBeanPostProcessor bp : getBeanPostProcessorCache().instantiationAware) &#123; /*典型应用：@Autowired 注解的注入*/ PropertyValues pvsToUse = bp.postProcessProperties(pvs, bw.getWrappedInstance(), beanName); if (pvsToUse == null) &#123; if (filteredPds == null) &#123; filteredPds = filterPropertyDescriptorsForDependencyCheck(bw, mbd.allowCaching); &#125; pvsToUse = bp.postProcessPropertyValues(pvs, filteredPds, bw.getWrappedInstance(), beanName); if (pvsToUse == null) &#123; return; &#125; &#125; pvs = pvsToUse; &#125; &#125; if (needsDepCheck) &#123; if (filteredPds == null) &#123; filteredPds = filterPropertyDescriptorsForDependencyCheck(bw, mbd.allowCaching); &#125; checkDependencies(beanName, mbd, filteredPds, pvs); &#125; if (pvs != null) &#123; /*将完成依赖注入（合并后的pvs）的信息应用到实例上去*/ /** * 1.对man进行属性赋值的过程 * 2.对women进行属性赋值 */ applyPropertyValues(beanName, mbd, bw, pvs); &#125;&#125; \u0000 上来一套组合拳，先判断假如bean包装对象是空的，但是mbd中的属性还有值，那就说明需要抛出异常，因为没法给一个null赋值。 接下来就是在属性赋值之前，给任何实例化的bean后置处理器修改bean状态的机会。例如，这可以用来支持各种类型的属性注入。**InstantiationAwareBeanPostProcessors** 这里又是一个后置处理器的调用点，实例化之后的调用，调用的是后置处理器的**afterInstantiation**方法。这个处理器的返回值将决定当前bean实例是否要进行属性注入，返回false则表示直接返回，默认返回true。\u0000 判断依赖注入按照名字注入还是按照类型注入，根据不同的选择走不同的处理逻辑。注意这里的处理并不是直接注入到bean中，而是解析到pvs中。 什么时候走这里？必须显式的指定了配置了依赖，才会走这里。 如果是按照名字注入，就执行**autowireByName()**。 如果是按照类型注入，就执行**autowireByType()**。 ​ 判断当前是否还有**InstantiationAwareBeanPostProcessors**需要执行。 判断当前是否需要进行依赖检查、 如果还有**InstantiationAwareBeanPostProcessors**需要执行，循环执行后置处理器的方法 如果需要依赖检查，就走依赖检查的逻辑 如果合并后的依赖信息（pvs）不为空，就执行**applyPropertyValues()**进行属性赋值。 ​ 接下来分析两种属性注入的方式，按照名字注入&amp;按照类型注入。 2.autowireByName()\u0000 1234567891011121314151617181920212223242526protected void autowireByName( String beanName, AbstractBeanDefinition mbd, BeanWrapper bw, MutablePropertyValues pvs) &#123; /*bean实例中有该字段，且有该字段的setter方法，但是在bd中没有property属性。*/ String[] propertyNames = unsatisfiedNonSimpleProperties(mbd, bw); /*便利查找出来的集合，完成依赖注入。*/ for (String propertyName : propertyNames) &#123; /*条件成立说明beanFactory存在当前属性的bean实例，可以注入，说明找到对应的依赖信息数据了*/ if (containsBean(propertyName)) &#123; /*拿到属性所对应的bean实例*/ Object bean = getBean(propertyName); /*在属性里面追加一个property*/ pvs.add(propertyName, bean); /*管理依赖信息*/ registerDependentBean(propertyName, beanName); if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Added autowiring by name from bean name &#x27;&quot; + beanName + &quot;&#x27; via property &#x27;&quot; + propertyName + &quot;&#x27; to bean named &#x27;&quot; + propertyName + &quot;&#x27;&quot;); &#125; &#125; else &#123; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Not autowiring property &#x27;&quot; + propertyName + &quot;&#x27; of bean &#x27;&quot; + beanName + &quot;&#x27; by name: no matching bean found&quot;); &#125; &#125; &#125;&#125; 总结：autowireByName主要完成以下逻辑：​ 获取需要填充对象得非简单类型得属性名； 遍历第一步获取得属性名，调用getBean方法从容器中获取此属性名对应的object； 然后，如果能找到，则把此属性名和object对象保存到pvs的propertyValueList里面。 3.autowireByType()\u0000 12345678910111213141516171819202122232425262728293031323334353637383940414243444546protected void autowireByType( String beanName, AbstractBeanDefinition mbd, BeanWrapper bw, MutablePropertyValues pvs) &#123; /*获取类型转换器*/ TypeConverter converter = getCustomTypeConverter(); if (converter == null) &#123; converter = bw; &#125; Set&lt;String&gt; autowiredBeanNames = new LinkedHashSet&lt;&gt;(4); /*拿到属性信息，循环遍历*/ String[] propertyNames = unsatisfiedNonSimpleProperties(mbd, bw); for (String propertyName : propertyNames) &#123; try &#123; PropertyDescriptor pd = bw.getPropertyDescriptor(propertyName); // Don&#x27;t try autowiring by type for type Object: never makes sense, // even if it technically is a unsatisfied, non-simple property. /*如果不是Object类型*/ if (Object.class != pd.getPropertyType()) &#123; /*拿到setter方法*/ MethodParameter methodParam = BeanUtils.getWriteMethodParameter(pd); // Do not allow eager init for type matching in case of a prioritized post-processor. boolean eager = !(bw.getWrappedInstance() instanceof PriorityOrdered); /*包装成一个依赖描述信息对象*/ DependencyDescriptor desc = new AutowireByTypeDependencyDescriptor(methodParam, eager); /*解析器根据依赖描述信息查找对象，或者 容器没有该对象实例的话，但是有该类型的bd的话，也会调用getBeanByType生成对象。*/ Object autowiredArgument = resolveDependency(desc, beanName, autowiredBeanNames, converter); /*查询出来的依赖对象加入到pvs里面*/ if (autowiredArgument != null) &#123; pvs.add(propertyName, autowiredArgument); &#125; /*将查询到的依赖对象注册*/ for (String autowiredBeanName : autowiredBeanNames) &#123; registerDependentBean(autowiredBeanName, beanName); if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Autowiring by type from bean name &#x27;&quot; + beanName + &quot;&#x27; via property &#x27;&quot; + propertyName + &quot;&#x27; to bean named &#x27;&quot; + autowiredBeanName + &quot;&#x27;&quot;); &#125; &#125; autowiredBeanNames.clear(); &#125; &#125; catch (BeansException ex) &#123; throw new UnsatisfiedDependencyException(mbd.getResourceDescription(), beanName, propertyName, ex); &#125; &#125;&#125; 获取当前对象的非简单类型的属性名数组propertyNames； 遍历属性名数组propertyNames，获取当前属性名多对应的类型filedType； 通过filedType找到匹配的候选Bean对象； 把属性名以及bean对象添加到pvs的propertyValueList里面。 可以看到，无论是按照名字注入还是按照类型的注入方式，其实里面都会调用同一个公共的方法。**unsatisfiedNonSimpleProperties()**，接下来分析这个方法是做什么的。 123456789101112131415161718192021protected String[] unsatisfiedNonSimpleProperties(AbstractBeanDefinition mbd, BeanWrapper bw) &#123; Set&lt;String&gt; result = new TreeSet&lt;&gt;(); /*拿到配置的properties集合*/ PropertyValues pvs = mbd.getPropertyValues(); /*拿到bean的所有字段信息，然后便利所有的字段信息*/ PropertyDescriptor[] pds = bw.getPropertyDescriptors(); /* * 条件1成立说明 当前字段有setter方法 * 条件2成立说明 当前字段类型是否在忽略自动注入列表中，条件成立，说明不在。 * 条件3成立说明 当前字段不在xml或者其他方式的配置中配置过。 * 条件4成立说明 当前字段类型是不是简单的八种基本数据类型。基本数据类型不允许自动注入。当前字段不是基本数据类型。 * */ for (PropertyDescriptor pd : pds) &#123; if (pd.getWriteMethod() != null &amp;&amp; !isExcludedFromDependencyCheck(pd) &amp;&amp; !pvs.contains(pd.getName()) &amp;&amp; !BeanUtils.isSimpleProperty(pd.getPropertyType())) &#123; result.add(pd.getName()); &#125; &#125; return StringUtils.toStringArray(result);&#125; 过滤出mbd中符合依赖注入条件的属性。 4.applyPropertyValues()大部分情况下，我们走属性注入的逻辑，其实都是对解析出来的依赖bean进行注入。我们来梳理一下大致的流程，看一下核心的逻辑即可。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697protected void applyPropertyValues(String beanName, BeanDefinition mbd, BeanWrapper bw, PropertyValues pvs) &#123; // 为空直接返回 if (pvs == null || pvs.isEmpty()) &#123; return; &#125; MutablePropertyValues mpvs = null; List&lt;PropertyValue&gt; original; if (System.getSecurityManager() != null) &#123; if (bw instanceof BeanWrapperImpl) &#123; ((BeanWrapperImpl) bw).setSecurityContext(getAccessControlContext()); &#125; &#125; if (pvs instanceof MutablePropertyValues) &#123; mpvs = (MutablePropertyValues) pvs; if (mpvs.isConverted()) &#123; // 如果已被设置转换完成，直接完成配置 try &#123; bw.setPropertyValues(mpvs); return; &#125; catch (BeansException ex) &#123; throw new BeanCreationException( mbd.getResourceDescription(), beanName, &quot;Error setting property values&quot;, ex); &#125; &#125; original = mpvs.getPropertyValueList(); &#125; else &#123; original = Arrays.asList(pvs.getPropertyValues()); &#125; TypeConverter converter = getCustomTypeConverter(); if (converter == null) &#123; converter = bw; &#125; // 创建BeanDefinitionValueResolver解析器，用来解析未被解析的PropertyValue。 BeanDefinitionValueResolver valueResolver = new BeanDefinitionValueResolver(this, beanName, mbd, converter); // 开始遍历检查original中的属性，对未被解析的先解析/已解析的直接加入deepCopy中，最后再填充到具体的Bean实例中 List&lt;PropertyValue&gt; deepCopy = new ArrayList&lt;PropertyValue&gt;(original.size()); boolean resolveNecessary = false; for (PropertyValue pv : original) &#123; // 如果属性已经转化，直接添加 if (pv.isConverted()) &#123; deepCopy.add(pv); &#125; else &#123; String propertyName = pv.getName(); Object originalValue = pv.getValue(); // 核心逻辑，解析获取实际的值 // 对于RuntimeReference，会解析拿到具体的beanName,最终通过getBean(beanName)拿到具体的对象 Object resolvedValue = valueResolver.resolveValueIfNecessary(pv, originalValue); Object convertedValue = resolvedValue; // 判断是否可以转换 boolean convertible = bw.isWritableProperty(propertyName) &amp;&amp; !PropertyAccessorUtils.isNestedOrIndexedProperty(propertyName); if (convertible) &#123; // 尝试进行转换 convertedValue = convertForProperty(resolvedValue, propertyName, bw, converter); &#125; // 避免需要重复转换，设定已转换 if (resolvedValue == originalValue) &#123; if (convertible) &#123; pv.setConvertedValue(convertedValue); &#125; deepCopy.add(pv); &#125; else if (convertible &amp;&amp; originalValue instanceof TypedStringValue &amp;&amp; !((TypedStringValue) originalValue).isDynamic() &amp;&amp; !(convertedValue instanceof Collection || ObjectUtils.isArray(convertedValue))) &#123; pv.setConvertedValue(convertedValue); deepCopy.add(pv); &#125; else &#123; resolveNecessary = true; deepCopy.add(new PropertyValue(pv, convertedValue)); &#125; &#125; &#125; if (mpvs != null &amp;&amp; !resolveNecessary) &#123; mpvs.setConverted(); &#125; // 完成设置 try &#123; bw.setPropertyValues(new MutablePropertyValues(deepCopy)); &#125; catch (BeansException ex) &#123; throw new BeanCreationException( mbd.getResourceDescription(), beanName, &quot;Error setting property values&quot;, ex); &#125;&#125; ​ 主要是关心下，解析出来的依赖，如果注入到当前正在执行属性赋值的bean对象。​ **valueResolver.resolveValueIfNecessary()** 利用值解析器解析当前bean实例所依赖的bean实例。​ **resolveReference()**通过这个方法去拿到当前属性赋值的bean需要注入的bean。​ **this.beanFactory.getBean()** 最终这里又递归回到了getBean()去获取所需要的bean对象。​ **registerDependentBean()**在获取到bean对象之后，会将获取到的bean对象注册到当前bean所依赖的bean集合。​ 至此，单实例bean的属性赋值大体流程我们就分析完了，下一篇我们将去分析单实例bean的初始化逻辑。 \u0000\u0000\u0000","categories":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/tags/Spring/"}]},{"title":"Spring[五]createBean()","slug":"Spring/Spring[五]createBean()","date":"2022-01-11T05:59:23.628Z","updated":"2022-01-11T06:10:12.056Z","comments":true,"path":"2022/01/11/Spring/Spring[五]createBean()/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/Spring/Spring[%E4%BA%94]createBean()/","excerpt":"","text":"在上一篇中，分析了getBean()方法的详细流程，里面涉及到了单实例bean的创建，createBean(),本篇将分析这个方法，分析一下Spring中单实例对象时如何创建出来的。 1.createBean1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677@Overrideprotected Object createBean(String beanName, RootBeanDefinition mbd,/*按照传递的参数构建bean的实例并且返回，一般很少使用到参数*/ @Nullable Object[] args) throws BeanCreationException &#123; /* * 创建实例使用的bean定义信息 * */ RootBeanDefinition mbdToUse = mbd; /* * 检查当前bean的定义信息是否有class信息，如果有的话返回class类型，如果没有的话，使用类加载器加载class实例加载到jvm，并返回class对象 * * */ Class&lt;?&gt; resolvedClass = resolveBeanClass(mbd, beanName); /* * 条件一：拿到了bean定义信息 实例化时候需要的真实class对象 * 条件二：说明bean定义信息在解析bean对象之前（resolveBeanClass()）是没有class对象的 * 条件三：说明mbd有className * * 都成立之后就会去重新初始化mbd对象，进行一层包装。 * */ if (resolvedClass != null &amp;&amp; !mbd.hasBeanClass() &amp;&amp; mbd.getBeanClassName() != null) &#123; mbdToUse = new RootBeanDefinition(mbd); mbdToUse.setBeanClass(resolvedClass); &#125; try &#123; /* * 预处理 方法重写的逻辑。。 给重写的方法打一个标。 * */ mbdToUse.prepareMethodOverrides(); &#125; catch (BeanDefinitionValidationException ex) &#123; throw new BeanDefinitionStoreException(mbdToUse.getResourceDescription(), beanName, &quot;Validation of method overrides failed&quot;, ex); &#125; try &#123; /* * 通过后置处理器在这里返回一个代理的实例对象 * 这里的代理对象不是spring 的 aop 的实现。 * 实例化之前 并不是init执行前后 后置处理器的执行机会 * */ Object bean = resolveBeforeInstantiation(beanName, mbdToUse); /*如果对象不为空，直接返回。*/ if (bean != null) &#123; return bean; &#125; &#125; catch (Throwable ex) &#123; throw new BeanCreationException(mbdToUse.getResourceDescription(), beanName, &quot;BeanPostProcessor before instantiation of bean failed&quot;, ex); &#125; try &#123; /* * 真正的创建bean的逻辑，spring里面真正干活的方法都是以do开头 * 创建bean实例 执行依赖注入 执行init 后置处理器的逻辑 * */ /** * 1.第一次创建man的逻辑 * 2.创建women */ Object beanInstance = doCreateBean(beanName, mbdToUse, args); return beanInstance; &#125; catch (BeanCreationException | ImplicitlyAppearedSingletonException ex) &#123; throw ex; &#125; catch (Throwable ex) &#123; throw new BeanCreationException( mbdToUse.getResourceDescription(), beanName, &quot;Unexpected exception during bean creation&quot;, ex); &#125;&#125; 首先使用**resolveBeanClass()**解析出当前bean的类型 如果解析出的bean类型不为空，并且mbd里面没有bean的类型，并且mbd的className不为空 创建一个新的**RootBeanDefinition**,将Class类型设置进去 处理方法重写的逻辑**prepareMethodOverrides()** **resolveBeforeInstantiation() **通过后置处理器在这里尝试返回一个代理对象，这里的代理并不是Aop的实现，此时是实例化之前，并不是**init()**执行前后 如果后置处理器成功返回了对象，直接返回 如果后置处理器没有返回一个代理对象，那就将创建bean的逻辑委派给**doCreateBean()**,返回创建好的bean实例对象。 ​ 按照执行流程来分析，我们先分析**resolveBeanClass()**方法。 2.resolveBeanClass()123456789101112131415161718@Nullableprotected Class&lt;?&gt; resolveBeanClass(RootBeanDefinition mbd, String beanName, Class&lt;?&gt;... typesToMatch) throws CannotLoadBeanClassException &#123; try &#123; if (mbd.hasBeanClass()) &#123; return mbd.getBeanClass(); &#125; return doResolveBeanClass(mbd, typesToMatch); &#125; catch (ClassNotFoundException ex) &#123; throw new CannotLoadBeanClassException(mbd.getResourceDescription(), beanName, mbd.getBeanClassName(), ex); &#125; catch (LinkageError err) &#123; throw new CannotLoadBeanClassException(mbd.getResourceDescription(), beanName, mbd.getBeanClassName(), err); &#125;&#125; 可以看到，这里其实又是一个委派模式。**doResolveBeanClass()**。​ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960@Nullableprivate Class&lt;?&gt; doResolveBeanClass(RootBeanDefinition mbd, Class&lt;?&gt;... typesToMatch) throws ClassNotFoundException &#123; ClassLoader beanClassLoader = getBeanClassLoader(); ClassLoader dynamicLoader = beanClassLoader; boolean freshResolve = false; if (!ObjectUtils.isEmpty(typesToMatch)) &#123; // When just doing type checks (i.e. not creating an actual instance yet), // use the specified temporary class loader (e.g. in a weaving scenario). ClassLoader tempClassLoader = getTempClassLoader(); if (tempClassLoader != null) &#123; dynamicLoader = tempClassLoader; freshResolve = true; if (tempClassLoader instanceof DecoratingClassLoader) &#123; DecoratingClassLoader dcl = (DecoratingClassLoader) tempClassLoader; for (Class&lt;?&gt; typeToMatch : typesToMatch) &#123; dcl.excludeClass(typeToMatch.getName()); &#125; &#125; &#125; &#125; String className = mbd.getBeanClassName(); if (className != null) &#123; Object evaluated = evaluateBeanDefinitionString(className, mbd); if (!className.equals(evaluated)) &#123; // A dynamically resolved expression, supported as of 4.2... if (evaluated instanceof Class) &#123; return (Class&lt;?&gt;) evaluated; &#125; else if (evaluated instanceof String) &#123; className = (String) evaluated; freshResolve = true; &#125; else &#123; throw new IllegalStateException(&quot;Invalid class name expression result: &quot; + evaluated); &#125; &#125; if (freshResolve) &#123; // When resolving against a temporary class loader, exit early in order // to avoid storing the resolved Class in the bean definition. if (dynamicLoader != null) &#123; try &#123; return dynamicLoader.loadClass(className); &#125; catch (ClassNotFoundException ex) &#123; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Could not load class [&quot; + className + &quot;] from &quot; + dynamicLoader + &quot;: &quot; + ex); &#125; &#125; &#125; return ClassUtils.forName(className, dynamicLoader); &#125; &#125; // Resolve regularly, caching the result in the BeanDefinition... return mbd.resolveBeanClass(beanClassLoader);&#125; 这里面没有什么核心的逻辑，不作具体分析。关注一下重要的逻辑，在bean实例创建之前尝试返回一个代理对象。​ 3.实例化之前尝试返回一个代理对象**resolveBeforeInstantiation()**​ 1234567891011121314151617181920212223242526protected Object resolveBeforeInstantiation(String beanName, RootBeanDefinition mbd) &#123; Object bean = null; if (!Boolean.FALSE.equals(mbd.beforeInstantiationResolved)) &#123; // hasInstantiationAwareBeanPostProcessors() 看当前需要执行的集合里面 //是否有相关的后置处理器，如果有的话，条件成立 ，才能执行if 里面的逻辑 if (!mbd.isSynthetic() &amp;&amp; hasInstantiationAwareBeanPostProcessors()) &#123; Class&lt;?&gt; targetType = determineTargetType(beanName, mbd); if (targetType != null) &#123; /* * 执行bean 实例化 前后的后置处理器 before 方法，默认是返回null，如果想要做特殊的逻辑处理 * 可以自己继承接口进行扩展。需要继承的接口：InstantiationAwareBeanPostProcessor * */ bean = applyBeanPostProcessorsBeforeInstantiation(targetType, beanName); /* * 如果bean真的在这里创建出来了，那么，下面就会执行bean实例化后的后置处理器逻辑 * */ if (bean != null) &#123; bean = applyBeanPostProcessorsAfterInitialization(bean, beanName); &#125; &#125; &#125; mbd.beforeInstantiationResolved = (bean != null); &#125; return bean;&#125; 如果有**InstantiationAwareBeanPostProcessors**这个类型的后置处理器 获取被代理对象的类型 如果类型不为空 执行bean实例化前后的后置处理器的before()，默认是返回null。如果想要做特殊的逻辑处理，可以自己继承接口进行扩展。**applyBeanPostProcessorsBeforeInstantiation()** \u0000 如果有特殊处理的逻辑，也就是返回了一个代理对象，那就执行bean实例化后的后置处理器方法。**applyBeanPostProcessorsAfterInitialization()** 返回后置处理器创建的bean对象3.1 applyBeanPostProcessorsBeforeInstantiation()12345678910、 @Nullable protected Object applyBeanPostProcessorsBeforeInstantiation(Class&lt;?&gt; beanClass, String beanName) &#123; for (InstantiationAwareBeanPostProcessor bp : getBeanPostProcessorCache().instantiationAware) &#123; Object result = bp.postProcessBeforeInstantiation(beanClass, beanName); if (result != null) &#123; return result; &#125; &#125; return null; &#125; 遍历所有的后置处理器，执行后置处理器的前置方法，如果某一个后置处理器返回了一个不为空的对象，直接返回该对象，后面的后置处理器的方法不在执行。3.2 applyBeanPostProcessorsAfterInitialization()1234567891011121314151617@Overridepublic Object applyBeanPostProcessorsAfterInitialization(Object existingBean, String beanName) throws BeansException &#123; Object result = existingBean; for (BeanPostProcessor processor : getBeanPostProcessors()) &#123; Object current = processor.postProcessAfterInitialization(result, beanName); /*注意： * 一旦某个后置处理器返回的结果为空 * 就返回上一个后置处理器的结果，后面的后置处理器方法不在执行*/ if (current == null) &#123; return result; &#125; result = current; &#125; return result;&#125; 遍历所有的后置处理器，执行后置处理器的后置方法，如果某一个后置处理器返回了null，直接返回null，后面的后置处理器的方法不在执行。​ **InstantiationAwareBeanPostProcessors**的逻辑可以看前面的Spring组件与注解篇，再次不再赘述。​ 4.doCreateBean()123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145protected Object doCreateBean(String beanName, RootBeanDefinition mbd, @Nullable Object[] args) throws BeanCreationException &#123; // BeanWrapper：一个包装的bean实例，继承了可配置的属性访问器 BeanWrapper instanceWrapper = null; /*如果是单实例bean对象，清理缓存并返回该对象，这里正常情况下是拿不到的，因为还未创建bean对象实例。*/ if (mbd.isSingleton()) &#123; instanceWrapper = this.factoryBeanInstanceCache.remove(beanName); &#125; /*如果上面的逻辑没拿到，说明还是正常的逻辑，执行创建bean实例的方法并且包装到wrapper中。*/ if (instanceWrapper == null) &#123; /** * 1.第一次走到这里是去创建man * 2.创建women */ instanceWrapper = createBeanInstance(beanName, mbd, args); &#125; /*获取包装好的bean实例对象*/ Object bean = instanceWrapper.getWrappedInstance(); /*拿到bean实例的Class类型*/ Class&lt;?&gt; beanType = instanceWrapper.getWrappedClass(); /*如果bean类型不为空，将bean类型设置到mergedBeanDefinition中*/ if (beanType != NullBean.class) &#123; mbd.resolvedTargetType = beanType; &#125; /*允许post-processors 在此刻对 merged bean definition 进行修改*/ synchronized (mbd.postProcessingLock) &#123; /*如果后置处理器的逻辑还尚未执行，那就在此刻执行，执行之后将是否执行过修改为true，这也就限定了后置处理器只能执行一次。*/ if (!mbd.postProcessed) &#123; try &#123; /*执行后置处理器的核心逻辑：合并bd信息，接下来就是populate处理依赖。*/ applyMergedBeanDefinitionPostProcessors(mbd, beanType, beanName); &#125; catch (Throwable ex) &#123; throw new BeanCreationException(mbd.getResourceDescription(), beanName, &quot;Post-processing of merged bean definition failed&quot;, ex); &#125; mbd.postProcessed = true; &#125; &#125; // Eagerly cache singletons to be able to resolve circular references // even when triggered by lifecycle interfaces like BeanFactoryAware. /* * 提前地缓存单例，以便能够解析循环引用，即使被生命周期接口(如BeanFactoryAware)触发。 * 其实这里的逻辑就是判断是否过早的暴露早期的bean实例，已经初始化，但是没实例化。 * */ boolean earlySingletonExposure = (mbd.isSingleton() &amp;&amp; this.allowCircularReferences &amp;&amp; isSingletonCurrentlyInCreation(beanName)); /*如果支持早期暴露单实例bean对象*/ if (earlySingletonExposure) &#123; /*日志打印*/ if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Eagerly caching bean &#x27;&quot; + beanName + &quot;&#x27; to allow for resolving potential circular references&quot;); &#125; /*加入到单例bean工厂，将早期bean&#x27;实例（已经实例化但是尚未初始化）的引用加入到第三级缓存 * getEarlyBeanReference（）*/ /** * 1.man现在刚创建出来，还没有进行属性赋值和初始化，此时将他放到第三级缓存 * 2.此时的women也是通过反射刚刚创建出来，还没有进行属性赋值和初始化的逻辑，此时把他放到了三级缓存中 */ addSingletonFactory(beanName, () -&gt; getEarlyBeanReference(beanName, mbd, bean)); &#125; // Initialize the bean instance. Object exposedObject = bean; try &#123; /*对bean进行属性赋值，依赖注入。*/ /** * 1.对man进行属性赋值 * 2.对women进行属性赋值 */ populateBean(beanName, mbd, instanceWrapper); /*生命周期中的初始化方法的调用*/ /** * 1.women先经过这里 */ exposedObject = initializeBean(beanName, exposedObject, mbd); &#125; catch (Throwable ex) &#123; if (ex instanceof BeanCreationException &amp;&amp; beanName.equals(((BeanCreationException) ex).getBeanName())) &#123; throw (BeanCreationException) ex; &#125; else &#123; throw new BeanCreationException( mbd.getResourceDescription(), beanName, &quot;Initialization of bean failed&quot;, ex); &#125; &#125; if (earlySingletonExposure) &#123; Object earlySingletonReference = getSingleton(beanName, false); //条件成立：说明当前bean实例是从二级缓存就获取到了 //说明产生了，循环依赖，三级缓存 当前对象的ObjectFactory.getObject()被调用过 if (earlySingletonReference != null) &#123; /*什么时候相等呢？ * 1.当前的真实实例不需要被代理 * 2.当前实例已经被代理过，，，是在ObjectFactory.getObject()方法调用的时候，实现的增强代理*/ if (exposedObject == bean) &#123; exposedObject = earlySingletonReference; &#125; else if (!this.allowRawInjectionDespiteWrapping &amp;&amp; hasDependentBean(beanName)) &#123; /*获取依赖当前bean的其他beanName*/ String[] dependentBeans = getDependentBeans(beanName); Set&lt;String&gt; actualDependentBeans = new LinkedHashSet&lt;&gt;(dependentBeans.length); for (String dependentBean : dependentBeans) &#123; /*如果依赖当前bean的bean已经创建完了，就把他加入到集合中去*/ if (!removeSingletonIfCreatedForTypeCheckOnly(dependentBean)) &#123; actualDependentBeans.add(dependentBean); &#125; &#125; /*当前bean正在创建，但是依赖当前bean的bean已经创建完了，那说明指定是有问题，不对劲。*/ /** * 为什么有问题？ * 因为当前对象的aop操作是在当前方法 initbean 完成的 * 在这之前 ，外部其他bean持有的当前bean实例 都是尚未增强的。 */ if (!actualDependentBeans.isEmpty()) &#123; throw new BeanCurrentlyInCreationException(beanName, &quot;Bean with name &#x27;&quot; + beanName + &quot;&#x27; has been injected into other beans [&quot; + StringUtils.collectionToCommaDelimitedString(actualDependentBeans) + &quot;] in its raw version as part of a circular reference, but has eventually been &quot; + &quot;wrapped. This means that said other beans do not use the final version of the &quot; + &quot;bean. This is often the result of over-eager type matching - consider using &quot; + &quot;&#x27;getBeanNamesForType&#x27; with the &#x27;allowEagerInit&#x27; flag turned off, for example.&quot;); &#125; &#125; &#125; &#125; // Register bean as disposable. try &#123; /*判断当前bean是否需要注册一个容器关闭时候的析构函数回调*/ registerDisposableBeanIfNecessary(beanName, bean, mbd); &#125; catch (BeanDefinitionValidationException ex) &#123; throw new BeanCreationException( mbd.getResourceDescription(), beanName, &quot;Invalid destruction signature&quot;, ex); &#125; return exposedObject;&#125; 如果是单实例bean对象，拿到该对象，并清理缓存，这里正常情况下是什么都拿不到的，因为还没有创建bean对象实例。 如果上面的逻辑什么也没有拿到，说明还是正常的情况，执行创建bean实例的方法并且包装到wrapper中。**createBeanInstance()** 获取包装好的bean实例对象，拿到bean实例对象的class类型，将他设置到mbd中。 在属性赋值之前，给尚未执行的后置处理器最后一次机会来执行，执行之后修改执行过状态为true，这也就限定了后置处理器只能执行一次。**applyMergedBeanDefinitionPostProcessors()** 判断是否允许过早的暴露早期bean实例的引用(已经初始化，但是尚未实例化) 将早期bean实例加入到三级缓存 对早期暴露的bean进行属性赋值**populateBean()** 在属性赋值之后执行bean对象的初始化方法**initializeBean()** 这里开始其实就是循环依赖的逻辑，首先判断是不是允许早期暴露的单实例bean对象，如果是的话 从缓存中拿到尝试获取bean，**getSingleton()** 如果从缓存拿到bean了，说明当前bean实例是从二级缓存就获取到了，说明产生了循环依赖 如果依赖于当前bean其他bean对象，，如果依赖当前bean的对象都已经创建完了，那就吧他们加入到集合中。 当前bean正在创建，但是依赖他的bean都创建完了，那就说明逻辑上出现问题了 为什么会出现问题？因为当前对象的aop操作是在当前方法的init方法里面执行的，在这之前，其他对象拿到的bean都是尚未增强的bean。 判断当前bean是否需要注册一个容器关闭时候执行的析构函数**registerDisposableBeanIfNecessary()** \u0000\u0000这里面其实主要关注五个点：**createBeanInstance()**，**applyMergedBeanDefinitionPostProcessors()**，**populateBean()**，**initializeBean()**，**registerDisposableBeanIfNecessary()**。​ 5.createBeanInstance()使用适当的实例化策略为指定的 bean 创建一个新实例：工厂方法，构造函数自动装配或简单实例化。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697protected BeanWrapper createBeanInstance(String beanName, RootBeanDefinition mbd, @Nullable Object[] args) &#123; // 解析bean定义信息中的 bean 的实例类型 Class&lt;?&gt; beanClass = resolveBeanClass(mbd, beanName); /* * 条件一：Class 实例不为空 * 条件二：Class实例的访问权限不是公开的 * 条件三：CLass实例的权限没打开，没打开就不能反射我记得。 * */ if (beanClass != null &amp;&amp; !Modifier.isPublic(beanClass.getModifiers()) &amp;&amp; !mbd.isNonPublicAccessAllowed()) &#123; throw new BeanCreationException(mbd.getResourceDescription(), beanName, &quot;Bean class isn&#x27;t public, and non-public access not allowed: &quot; + beanClass.getName()); &#125; /*spring5新特性，没研究*/ Supplier&lt;?&gt; instanceSupplier = mbd.getInstanceSupplier(); if (instanceSupplier != null) &#123; return obtainFromSupplier(instanceSupplier, beanName); &#125; /*说明bean标签配置了 factoryMethod属性：利用工厂创建对象的逻辑。*/ if (mbd.getFactoryMethodName() != null) &#123; return instantiateUsingFactoryMethod(beanName, mbd, args); &#125; // 创建相同bean的时候，可以走捷径。解析构造器比较耗时，所以如果一个对象创建两次，可以不用那么麻烦。 /* * resolved：bd对应的构造信息是否已经解析成可以发射调用的构造方法信息 * autowireNecessary：是否自动匹配构造方法 * * */ boolean resolved = false; boolean autowireNecessary = false; if (args == null) &#123; /* * mbd.resolvedConstructorOrFactoryMethod != null * 条件成立：说明bd的构造信息已经转化成可以反射调用的method，并且被缓存。 * */ synchronized (mbd.constructorArgumentLock) &#123; if (mbd.resolvedConstructorOrFactoryMethod != null) &#123; /*说明已经解析过*/ resolved = true; /*判断构造方法的参数是否已经被解析了，resolvedConstructorOrFactoryMethod 有值 * 且构造方法有参数，可以认为字段值就是true * 设么情况下为false？ * 1.resolvedConstructorOrFactoryMethod == null * 2.当resolvedConstructorOrFactoryMethod 表示的是默认的无参构造器的时候 * */ autowireNecessary = mbd.constructorArgumentsResolved; &#125; &#125; &#125; /* * 如果已经解析过， * * */ if (resolved) &#123; /*并且可以自动匹配构造方法*/ if (autowireNecessary) &#123; /*自动匹配有参构造器*/ return autowireConstructor(beanName, mbd, null, null); &#125; /*没有参数解析，也就是无参构造方法处理*/ else &#123; /** * 1.第一次去创建man * 2.创建women */ return instantiateBean(beanName, mbd); &#125; &#125; // Candidate constructors for autowiring? /** * 后置处理器的调用点：SmartInstantiationAwareBeanPostProcessor * 这个后置处理器里面默认是实现方法为空，想要执行这里的逻辑可以自己继承接口实现 * * 典型应用 @Autowired 打在了构造器方法上，就会用到后置处理器 AutowiredAnnotationBeanPostProcessor */ Constructor&lt;?&gt;[] ctors = determineConstructorsFromBeanPostProcessors(beanClass, beanName); /* * 条件1：后置处理器指定了构造方法数组 * 条件2：一般不会成立。bean标签里面配置了autowired=constructor 才会触发 * 条件3：说明bean标签中配置了构造参数信息 * 条件4：getBean时，args有参数 * */ if (ctors != null || mbd.getResolvedAutowireMode() == AUTOWIRE_CONSTRUCTOR || mbd.hasConstructorArgumentValues() || !ObjectUtils.isEmpty(args)) &#123; return autowireConstructor(beanName, mbd, ctors, args); &#125; /*判断是否在beanDefinition里面配置偏好构造器，默认实现是null。*/ // Preferred constructors for default construction? ctors = mbd.getPreferredConstructors(); if (ctors != null) &#123; return autowireConstructor(beanName, mbd, ctors, null); &#125; /*大部分情况下都是走这里：未指定构造参数，未设定偏好，使用无参构造器创建bean实例。*/ // No special handling: simply use no-arg constructor. return instantiateBean(beanName, mbd);&#125; 解析bd中的bean类型 对类型做一些校验 判断如果配置了bean标签里面的**factoryMethod**属性，走利用工厂创建对象的逻辑。**instantiateUsingFactoryMethod()** 进行一些属性判断，其实主要就是创建相同bean对象的时候，没有必要全部重新来做，可以走一些捷径 如果已经解析过&amp;可以自动匹配构造方法，那就走自动匹配有参构造器的逻辑。**autowireConstructor()** 如果是已经解析过&amp;无参构造器的处理方法，那就走**instantiateBean()** 接下来是一个后置处理器的调用点 _**SmartInstantiationAwareBeanPostProcessor**_，这个后置处理器里面默认实现为空，可以自己扩展。 典型的应用就是 @Autowired 注解打在了构造器上，就会用到后置处理器_**AutowiredAnnotationBeanPostProcessor**_。 判断是否需要执行构造器自动注入的逻辑 **autowireConstructor()** 判断是否在beandefinition里面配置了偏好的构造器，默认实现是null，如果是的话，就走**autowireConstructor()**。 大部分情况下，其实会走**instantiateBean()**。（未指定构造参数，未设置偏好，使用无参构造器创建bean对象实例） \u0000这里主要关心三个方法**instantiateBean()**&amp;**autowireConstructor()**&amp;**instantiateUsingFactoryMethod()**。​ 5.1 instantiateBean()\u0000 12345678910111213141516171819protected BeanWrapper instantiateBean(String beanName, RootBeanDefinition mbd) &#123; try &#123; /*获取实例化策略，调用实例化方法创建bean实例*/ /** * 1.反射去创建man * 2.反射去创建women */ Object beanInstance = getInstantiationStrategy().instantiate(mbd, beanName, this); /*根据实例创建bean实例的包装器*/ BeanWrapper bw = new BeanWrapperImpl(beanInstance); /*对包装器进行一些参数，工具设置*/ initBeanWrapper(bw); return bw; &#125; catch (Throwable ex) &#123; throw new BeanCreationException( mbd.getResourceDescription(), beanName, &quot;Instantiation of bean failed&quot;, ex); &#125;&#125; 首先获取到实例化的策略，反射去创建对象，根据bean去创建对象的包装器，对包装器进行赋能。**instantiate()** 12345678910111213141516171819202122232425262728293031@Overridepublic Object instantiate(RootBeanDefinition bd, @Nullable String beanName, BeanFactory owner) &#123; // Don&#x27;t override the class with CGLIB if no overrides. /*不考虑方法覆盖*/ if (!bd.hasMethodOverrides()) &#123; Constructor&lt;?&gt; constructorToUse; synchronized (bd.constructorArgumentLock) &#123; constructorToUse = (Constructor&lt;?&gt;) bd.resolvedConstructorOrFactoryMethod; if (constructorToUse == null) &#123; final Class&lt;?&gt; clazz = bd.getBeanClass(); if (clazz.isInterface()) &#123; throw new BeanInstantiationException(clazz, &quot;Specified class is an interface&quot;); &#125; try &#123; constructorToUse = clazz.getDeclaredConstructor(); bd.resolvedConstructorOrFactoryMethod = constructorToUse; &#125; catch (Throwable ex) &#123; throw new BeanInstantiationException(clazz, &quot;No default constructor found&quot;, ex); &#125; &#125; &#125; /*实例化*/ return BeanUtils.instantiateClass(constructorToUse); &#125; else &#123; // Must generate CGLIB subclass. /*方法覆盖的逻辑*/ return instantiateWithMethodInjection(bd, beanName, owner); &#125;&#125; **instantiateClass()** 12345678910111213141516171819202122232425262728293031323334353637383940414243public static &lt;T&gt; T instantiateClass(Constructor&lt;T&gt; ctor, Object... args) throws BeanInstantiationException &#123; Assert.notNull(ctor, &quot;Constructor must not be null&quot;); try &#123; ReflectionUtils.makeAccessible(ctor); if (KotlinDetector.isKotlinReflectPresent() &amp;&amp; KotlinDetector.isKotlinType(ctor.getDeclaringClass())) &#123; return KotlinDelegate.instantiateClass(ctor, args); &#125; else &#123; Class&lt;?&gt;[] parameterTypes = ctor.getParameterTypes(); Assert.isTrue(args.length &lt;= parameterTypes.length, &quot;Can&#x27;t specify more arguments than constructor parameters&quot;); /*这里的参数数组是为了防止某个类型传过来的值是null，如果是null，就是用默认的值*/ Object[] argsWithDefaultValues = new Object[args.length]; for (int i = 0 ; i &lt; args.length; i++) &#123; if (args[i] == null) &#123; Class&lt;?&gt; parameterType = parameterTypes[i]; /*从这里就能看到是不是选用了默认值*/ /* * DEFAULT_TYPE_VALUES :存各种基本数据类型的默认值。 * */ argsWithDefaultValues[i] = (parameterType.isPrimitive() ? DEFAULT_TYPE_VALUES.get(parameterType) : null); &#125; else &#123; argsWithDefaultValues[i] = args[i]; &#125; &#125; /*通过构造器来进行反射实例化创建bean实例。*/ return ctor.newInstance(argsWithDefaultValues); &#125; &#125; catch (InstantiationException ex) &#123; throw new BeanInstantiationException(ctor, &quot;Is it an abstract class?&quot;, ex); &#125; catch (IllegalAccessException ex) &#123; throw new BeanInstantiationException(ctor, &quot;Is the constructor accessible?&quot;, ex); &#125; catch (IllegalArgumentException ex) &#123; throw new BeanInstantiationException(ctor, &quot;Illegal arguments for constructor&quot;, ex); &#125; catch (InvocationTargetException ex) &#123; throw new BeanInstantiationException(ctor, &quot;Constructor threw exception&quot;, ex.getTargetException()); &#125;&#125; 最终通过单实例bean的构造器反射创建对象。 5.2autowireConstructor()12345protected BeanWrapper autowireConstructor( String beanName, RootBeanDefinition mbd, @Nullable Constructor&lt;?&gt;[] ctors, @Nullable Object[] explicitArgs) &#123; /*通过构造函数解析器的autowireConstructor方法来创建bean实例*/ return new ConstructorResolver(this).autowireConstructor(beanName, mbd, ctors, explicitArgs);&#125; 自动装配构造函数行为。如果指定了显式的构造函数参数值，也可以，将所有剩余参数与来自bean工厂的bean匹配。这块对应于构造函数注入，在这种模式下spring的bean工厂能够承载期望基于构造函数的依赖项解析的组件。**autowireConstructor()** 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209public BeanWrapper autowireConstructor(String beanName, RootBeanDefinition mbd, @Nullable Constructor&lt;?&gt;[] chosenCtors, @Nullable Object[] explicitArgs) &#123; /*创建了一个bean包装器的实例*/ BeanWrapperImpl bw = new BeanWrapperImpl(); /*初始化bean包装器*/ this.beanFactory.initBeanWrapper(bw); /*最终反射调用的构造器*/ Constructor&lt;?&gt; constructorToUse = null; /*holder持有的是：实例化的时候，真正用到的参数*/ ArgumentsHolder argsHolderToUse = null; /*实例化的时候使用到的参数*/ Object[] argsToUse = null; /*如果getbean传的参数不为空，使用到的参数就是指定的参数*/ if (explicitArgs != null) &#123; argsToUse = explicitArgs; &#125; else &#123;/*getBean的时候没有指定参数*/ /*表示构造器参数需要做类型转换，参数引用*/ Object[] argsToResolve = null; synchronized (mbd.constructorArgumentLock) &#123; /*到缓存去看看有没有缓存过构造器*/ constructorToUse = (Constructor&lt;?&gt;) mbd.resolvedConstructorOrFactoryMethod; /*缓存有解析好的构造器并且构造器参数已经被解析过 说明不是第一次通过bd生成实例*/ if (constructorToUse != null &amp;&amp; mbd.constructorArgumentsResolved) &#123; // Found a cached constructor... argsToUse = mbd.resolvedConstructorArguments; if (argsToUse == null) &#123; argsToResolve = mbd.preparedConstructorArguments; &#125; &#125; &#125; /*条件成立说明：resolvedConstructorArguments==null*/ if (argsToResolve != null) &#123; /*解析构造参数*/ argsToUse = resolvePreparedArguments(beanName, mbd, bw, constructorToUse, argsToResolve); &#125; &#125; /*条件成立说明缓存机制失败，或者是第一次创建，需要匹配构造器。*/ if (constructorToUse == null || argsToUse == null) &#123; // Take specified constructors, if any. /*chosenCtors什么时候有值呢？构造器上有@Autowired注解的时候。*/ Constructor&lt;?&gt;[] candidates = chosenCtors; /*条件成立说明：外部程序调用方法的时候并没有指定可选用的构造器，需要通过class拿到构造器信息*/ if (candidates == null) &#123; Class&lt;?&gt; beanClass = mbd.getBeanClass(); try &#123; /*如果非公开的方法也允许访问就获取所有的构造器，否则获取可以访问的构造器。*/ candidates = (mbd.isNonPublicAccessAllowed() ? beanClass.getDeclaredConstructors() : beanClass.getConstructors()); &#125; catch (Throwable ex) &#123; throw new BeanCreationException(mbd.getResourceDescription(), beanName, &quot;Resolution of declared constructors on bean Class [&quot; + beanClass.getName() + &quot;] from ClassLoader [&quot; + beanClass.getClassLoader() + &quot;] failed&quot;, ex); &#125; &#125; /*执行到这里，可选用的构造器列表已经转备好了，但是还没确定具体是哪一个*/ /*如果只有一个构造器并且getBean没有指定构造器参数，并且bean定义信息里面没有参数，那就是用默认的无参构造器*/ if (candidates.length == 1 &amp;&amp; explicitArgs == null &amp;&amp; !mbd.hasConstructorArgumentValues()) &#123; Constructor&lt;?&gt; uniqueCandidate = candidates[0]; if (uniqueCandidate.getParameterCount() == 0) &#123; synchronized (mbd.constructorArgumentLock) &#123; mbd.resolvedConstructorOrFactoryMethod = uniqueCandidate; mbd.constructorArgumentsResolved = true; mbd.resolvedConstructorArguments = EMPTY_ARGS; &#125; bw.setBeanInstance(instantiate(beanName, mbd, uniqueCandidate, EMPTY_ARGS)); return bw; &#125; &#125; // 需要解析到底是用哪一个构造器 boolean autowiring = (chosenCtors != null || mbd.getResolvedAutowireMode() == AutowireCapableBeanFactory.AUTOWIRE_CONSTRUCTOR); /*完成解析后的构造器参数值列表*/ ConstructorArgumentValues resolvedValues = null; /*构造器参数的个数*/ int minNrOfArgs; if (explicitArgs != null) &#123; minNrOfArgs = explicitArgs.length; &#125; else &#123; /*从beanDefinition拿到构造器参数*/ ConstructorArgumentValues cargs = mbd.getConstructorArgumentValues(); resolvedValues = new ConstructorArgumentValues(); /*将此 bean 的构造函数参数解析为 resolveValues 对象。 这可能涉及查找其他 bean。 此方法也用于处理静态工厂方法的调用。，*/ minNrOfArgs = resolveConstructorArguments(beanName, mbd, bw, cargs, resolvedValues); &#125; /*给可选用的构造器数组排序，排序规则： public &gt; no public more args &gt; no args*/ AutowireUtils.sortConstructors(candidates); /*这个值越低，说明当前构造器参数列表类型和构造参数的匹配值越高*/ int minTypeDiffWeight = Integer.MAX_VALUE; /*尚未筛选或者筛选完还未确定的构造器*/ Set&lt;Constructor&lt;?&gt;&gt; ambiguousConstructors = null; Deque&lt;UnsatisfiedDependencyException&gt; causes = null; /*筛选可选的构造方法，找出一个匹配度最高的构造函数*/ for (Constructor&lt;?&gt; candidate : candidates) &#123; /*获取当前处理的构造器参数个数*/ int parameterCount = candidate.getParameterCount(); /*这里判断的指标都是上面循环筛选出来的东西 * 因为candidates是排过序的 排序规则：public &gt; no public &gt; 多参数的 &gt; 参数少的 * 当前筛选出来的构造器优先级一定是优先于后面的构造器的 * */ if (constructorToUse != null &amp;&amp; argsToUse != null &amp;&amp; argsToUse.length &gt; parameterCount) &#123; // Already found greedy constructor that can be satisfied -&gt; // do not look any further, there are only less greedy constructors left. break; &#125;/*表示当前构造器参数 小于 bd中配置了构造器参数个数，说明匹配不上。*/ if (parameterCount &lt; minNrOfArgs) &#123; continue; &#125; /*构造器参数的持有对象*/ ArgumentsHolder argsHolder; /*拿到当前构造器的参数类型数组*/ Class&lt;?&gt;[] paramTypes = candidate.getParameterTypes(); /*已经解析后的构造器参数值不为空，说明bd中是有参数的，需要做匹配逻辑*/ if (resolvedValues != null) &#123; try &#123; /*根据构造器参数注解@ConstructorProperties拿到所有参数的名字*/ String[] paramNames = ConstructorPropertiesChecker.evaluate(candidate, parameterCount); /*如果数组的名字是空,那么就都是默认的名字*/ if (paramNames == null) &#123; ParameterNameDiscoverer pnd = this.beanFactory.getParameterNameDiscoverer(); if (pnd != null) &#123; paramNames = pnd.getParameterNames(candidate); &#125; &#125; argsHolder = createArgumentArray(beanName, mbd, resolvedValues, bw, paramTypes, paramNames, getUserDeclaredConstructor(candidate), autowiring, candidates.length == 1); &#125; catch (UnsatisfiedDependencyException ex) &#123; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Ignoring constructor [&quot; + candidate + &quot;] of bean &#x27;&quot; + beanName + &quot;&#x27;: &quot; + ex); &#125; // Swallow and try next constructor. if (causes == null) &#123; causes = new ArrayDeque&lt;&gt;(1); &#125; causes.add(ex); continue; &#125; &#125; else &#123; // Explicit arguments given -&gt; arguments length must match exactly. if (parameterCount != explicitArgs.length) &#123; continue; &#125; argsHolder = new ArgumentsHolder(explicitArgs); &#125; /*typeDiffWeight数值越高说明构造器与参数的匹配度越低。 * 计算出当前构造器参数类型与当前构造器参数匹配度*/ int typeDiffWeight = (mbd.isLenientConstructorResolution() ? argsHolder.getTypeDifferenceWeight(paramTypes) : argsHolder.getAssignabilityWeight(paramTypes)); // Choose this constructor if it represents the closest match. /*条件成立说明：当前循环处理的构造器比上一次帅选出来的更合适*/ if (typeDiffWeight &lt; minTypeDiffWeight) &#123; constructorToUse = candidate; argsHolderToUse = argsHolder; argsToUse = argsHolder.arguments; minTypeDiffWeight = typeDiffWeight; ambiguousConstructors = null; &#125; /*条件成立说明：当前处理的构造器 他计算出来的typeDiffWeight值与上一次筛选出来的最优先的构造器的值一致，有模棱两可的情况。加入到模棱两可集合。*/ else if (constructorToUse != null &amp;&amp; typeDiffWeight == minTypeDiffWeight) &#123; if (ambiguousConstructors == null) &#123; ambiguousConstructors = new LinkedHashSet&lt;&gt;(); ambiguousConstructors.add(constructorToUse); &#125; ambiguousConstructors.add(candidate); &#125; &#125; /*条件成立：没找到可用构造器，所以直接报错。*/ if (constructorToUse == null) &#123; if (causes != null) &#123; UnsatisfiedDependencyException ex = causes.removeLast(); for (Exception cause : causes) &#123; this.beanFactory.onSuppressedException(cause); &#125; throw ex; &#125; throw new BeanCreationException(mbd.getResourceDescription(), beanName, &quot;Could not resolve matching constructor on bean class [&quot; + mbd.getBeanClassName() + &quot;] &quot; + &quot;(hint: specify index/type/name arguments for simple parameters to avoid type ambiguities)&quot;); &#125; /*条件成立：模棱两可的构造器集合有值，并且beanDefinition里面的构造器指定策略是狭窄策略，这个时候也要报错。*/ else if (ambiguousConstructors != null &amp;&amp; !mbd.isLenientConstructorResolution()) &#123; throw new BeanCreationException(mbd.getResourceDescription(), beanName, &quot;Ambiguous constructor matches found on bean class [&quot; + mbd.getBeanClassName() + &quot;] &quot; + &quot;(hint: specify index/type/name arguments for simple parameters to avoid type ambiguities): &quot; + ambiguousConstructors); &#125; /*条件成立：说明自动匹配成功了，需要进行缓存，方便后来者继续使用mergerdbeanDefinition来创建实例*/ if (explicitArgs == null &amp;&amp; argsHolderToUse != null) &#123; argsHolderToUse.storeCache(mbd, constructorToUse); &#125; &#125; Assert.state(argsToUse != null, &quot;Unresolved constructor arguments&quot;); /*根据上面选择的构造器和解析出来的参数，通过instantiate方法反射创建bean对象实例，最终将实例设置到beanWrapper的beanInstance实例里面。*/ bw.setBeanInstance(instantiate(beanName, mbd, constructorToUse, argsToUse)); return bw;&#125; 5.3 instantiateUsingFactoryMethod()12345protected BeanWrapper instantiateUsingFactoryMethod( String beanName, RootBeanDefinition mbd, @Nullable Object[] explicitArgs) &#123; //工厂模式创建对象的方法 return new ConstructorResolver(this).instantiateUsingFactoryMethod(beanName, mbd, explicitArgs);&#125; **instantiateUsingFactoryMethod()**​ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308public BeanWrapper instantiateUsingFactoryMethod( String beanName, RootBeanDefinition mbd, @Nullable Object[] explicitArgs) &#123; //创建一个空的bean实例的包装器 BeanWrapperImpl bw = new BeanWrapperImpl(); //进行一些参数设置，给包装器赋能 this.beanFactory.initBeanWrapper(bw); //工厂bean对象的引用 Object factoryBean; //工厂bean对象的类型 Class&lt;?&gt; factoryClass; //是不是静态工厂 boolean isStatic; //获取工厂bean的名字 String factoryBeanName = mbd.getFactoryBeanName(); //如果工厂bean的名字不为空 if (factoryBeanName != null) &#123; //如果工厂bean的名字和要创建的bean名字一样，那么说明出现问题了，这个时候要抛异常 if (factoryBeanName.equals(beanName)) &#123; throw new BeanDefinitionStoreException(mbd.getResourceDescription(), beanName, &quot;factory-bean reference points back to the same bean definition&quot;); &#125; //获取工厂bean对象 factoryBean = this.beanFactory.getBean(factoryBeanName); //如果mbd是单实例的 &amp;&amp; 当前一级缓存里面包含这个要创建的单实例bean //这个时候说明不正常，为啥呢？我们要通过工厂模式创建的bean，被以其他方式创建出来了，非正常途径得到的bean if (mbd.isSingleton() &amp;&amp; this.beanFactory.containsSingleton(beanName)) &#123; throw new ImplicitlyAppearedSingletonException(); &#125; //给当前要创建的bean实例对象注册一个依赖bean，这个依赖bean就是他的工厂bean this.beanFactory.registerDependentBean(factoryBeanName, beanName); factoryClass = factoryBean.getClass(); isStatic = false; &#125; //走到这里的情况就是工厂bean没有名字，什么情况会走到这里？通过静态工厂来创建对象 else &#123; // It&#x27;s a static factory method on the bean class. //如果依赖静态工厂来创建bean实例对象的话，那么必须要有工厂bean的类型，否则没法创建 if (!mbd.hasBeanClass()) &#123; throw new BeanDefinitionStoreException(mbd.getResourceDescription(), beanName, &quot;bean definition declares neither a bean class nor a factory-bean reference&quot;); &#125; factoryBean = null; factoryClass = mbd.getBeanClass(); isStatic = true; &#125; //创建bean对象的工厂方法 Method factoryMethodToUse = null; //持有创建对象需要的参数 ArgumentsHolder argsHolderToUse = null; //创建对象需要使用的参数 Object[] argsToUse = null; //getBean()方法传入的参数 if (explicitArgs != null) &#123; argsToUse = explicitArgs; &#125; else &#123;//这里的逻辑就是getBean()的时候没有参数传递进来 Object[] argsToResolve = null; synchronized (mbd.constructorArgumentLock) &#123; //获取已经解析过的工厂方法 factoryMethodToUse = (Method) mbd.resolvedConstructorOrFactoryMethod; //如果工厂方法已经解析过，&amp;&amp; mbd的构造器参数也已经解析过 if (factoryMethodToUse != null &amp;&amp; mbd.constructorArgumentsResolved) &#123; // Found a cached factory method... //这里应该就是判断构造器参数有没有解析过，没有解析过的话，就去设置解析状态为准备解析 argsToUse = mbd.resolvedConstructorArguments; if (argsToUse == null) &#123; argsToResolve = mbd.preparedConstructorArguments; &#125; &#125; &#125; //判断构造器参数尚未解析过，就去解析构造器参数 if (argsToResolve != null) &#123; argsToUse = resolvePreparedArguments(beanName, mbd, bw, factoryMethodToUse, argsToResolve); &#125; &#125; //如果工厂方法为空 || 需要使用的参数为空 if (factoryMethodToUse == null || argsToUse == null) &#123; // Need to determine the factory method... // Try all methods with this name to see if they match the given arguments. //需要一个一个方法尝试，去找到创建bean所需要的工厂方法 factoryClass = ClassUtils.getUserClass(factoryClass); List&lt;Method&gt; candidates = null; //如果工厂方法是唯一的 &amp;&amp; 当前尚未有指定工厂方法 if (mbd.isFactoryMethodUnique) &#123; if (factoryMethodToUse == null) &#123; //获取mbd里面已经解析过的工厂方法 factoryMethodToUse = mbd.getResolvedFactoryMethod(); &#125; //如果mbd里面有已经解析好的工厂方法，把他放到集合 if (factoryMethodToUse != null) &#123; candidates = Collections.singletonList(factoryMethodToUse); &#125; &#125; //如果方法列表为空，有什么情况？ //可能是上面mbd里面没有已经指定的工厂方法 //也可能是工厂方法不是唯一的，一个类有多个工厂方法，这个时候工厂方法的名字都是一样的，只是方法形参不同 if (candidates == null) &#123; candidates = new ArrayList&lt;&gt;(); //拿到工厂里面的所有的方法 Method[] rawCandidates = getCandidateMethods(factoryClass, mbd); //循环遍历，将满足条件的方法加入到集合 //满足什么条件？ 是静态的方法 &amp;&amp; 名字是指定的工厂方法的名字 for (Method candidate : rawCandidates) &#123; if (Modifier.isStatic(candidate.getModifiers()) == isStatic &amp;&amp; mbd.isFactoryMethod(candidate)) &#123; candidates.add(candidate); &#125; &#125; &#125; /*执行到这里，其实可选用的方法列表已经都帅选出来了，但是还没有具体确定是哪一个*/ //如果恰好只匹配到一个方法 &amp;&amp; 参数为空 &amp;&amp; 要创建的单实例bean 的构造参数也为空 ，那这里就是使用无参构造器的情况 if (candidates.size() == 1 &amp;&amp; explicitArgs == null &amp;&amp; !mbd.hasConstructorArgumentValues()) &#123; //将这个方法指定为唯一的工厂方法 Method uniqueCandidate = candidates.get(0); //如果工厂方法没有形参 if (uniqueCandidate.getParameterCount() == 0) &#123; //将mbd的工厂方法指定为这个方法 mbd.factoryMethodToIntrospect = uniqueCandidate; //设置一些参数，给工厂方法创建对象赋能 synchronized (mbd.constructorArgumentLock) &#123; mbd.resolvedConstructorOrFactoryMethod = uniqueCandidate; mbd.constructorArgumentsResolved = true; mbd.resolvedConstructorArguments = EMPTY_ARGS; &#125; //通过这个匹配到的工厂方法 ，利用 无参的工厂方法/构造器创建对象 //其实本质上还是利用反射调用工厂bean的工厂方法 bw.setBeanInstance(instantiate(beanName, mbd, factoryBean, uniqueCandidate, EMPTY_ARGS)); return bw; &#125; &#125; //如果匹配到了多个方法 ，对匹配到的方法进行一个排序 排序规则 ： public &gt; no public &amp;&amp; more args &gt; no args if (candidates.size() &gt; 1) &#123; // explicitly skip immutable singletonList candidates.sort(AutowireUtils.EXECUTABLE_COMPARATOR); &#125; //构造参数 ConstructorArgumentValues resolvedValues = null; //已解析的自动装配代码 == 指示自动装配可以满足的最贪婪的构造函数的常量 //需要解析到底是哪一个方法 boolean autowiring = (mbd.getResolvedAutowireMode() == AutowireCapableBeanFactory.AUTOWIRE_CONSTRUCTOR); //选择权重 值越低，表示匹配度越高 int minTypeDiffWeight = Integer.MAX_VALUE; //模棱两可的构造器集合 Set&lt;Method&gt; ambiguousFactoryMethods = null; //通过getBean()传入的参数的个数/构造器参数的个数 int minNrOfArgs; //如果通过getBean()传入的参数不为空 ，获取参数的个数 if (explicitArgs != null) &#123; minNrOfArgs = explicitArgs.length; &#125; else &#123; // // getBean()没有传入参数，这种情况下，我们就得自己去解析bd里面的构造器 if (mbd.hasConstructorArgumentValues()) &#123; //从bd去拿到构造器 ConstructorArgumentValues cargs = mbd.getConstructorArgumentValues(); resolvedValues = new ConstructorArgumentValues(); //将当前bean的方法参数或构造器参数解析 ，这里可能涉及到查找其他bean minNrOfArgs = resolveConstructorArguments(beanName, mbd, bw, cargs, resolvedValues); &#125; else &#123;//这就是无参构造器的情况 minNrOfArgs = 0; &#125; &#125; Deque&lt;UnsatisfiedDependencyException&gt; causes = null; //遍历匹配到的多个方法 for (Method candidate : candidates) &#123; //获取方法的参数个数 int parameterCount = candidate.getParameterCount(); /*这里判断的指标都是上面循环筛选出来的东西 * 因为candidates是排过序的 排序规则：public &gt; no public &gt; 多参数的 &gt; 参数少的 * 当前筛选出来的构造器优先级一定是优先于后面的构造器的 * */ if (parameterCount &gt;= minNrOfArgs) &#123; ArgumentsHolder argsHolder; //进行类型匹配 Class&lt;?&gt;[] paramTypes = candidate.getParameterTypes(); //如果显式参数不为空 if (explicitArgs != null) &#123; // bd中的参数个数 和 当前给定的参数个数匹配不上 ，直接淘汰当前方法 if (paramTypes.length != explicitArgs.length) &#123; continue; &#125; //将显式参数放到holder里面 argsHolder = new ArgumentsHolder(explicitArgs); &#125; else &#123;//这里是显式参数为空的情况 // 已解析的构造函数参数:需要类型转换和/或自动注入 try &#123; //存放方法的参数名数组 String[] paramNames = null; //参数解析器 ParameterNameDiscoverer pnd = this.beanFactory.getParameterNameDiscoverer(); if (pnd != null) &#123; //解析方法的参数名字 paramNames = pnd.getParameterNames(candidate); &#125; //对参数进行封装 argsHolder = createArgumentArray(beanName, mbd, resolvedValues, bw, paramTypes, paramNames, candidate, autowiring, candidates.size() == 1); &#125; catch (UnsatisfiedDependencyException ex) &#123; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Ignoring factory method [&quot; + candidate + &quot;] of bean &#x27;&quot; + beanName + &quot;&#x27;: &quot; + ex); &#125; // Swallow and try next overloaded factory method. if (causes == null) &#123; causes = new ArrayDeque&lt;&gt;(1); &#125; causes.add(ex); continue; &#125; &#125; //计算当前方法参数和当前方法的匹配度 int typeDiffWeight = (mbd.isLenientConstructorResolution() ? argsHolder.getTypeDifferenceWeight(paramTypes) : argsHolder.getAssignabilityWeight(paramTypes)); // 条件成立，说明本次的匹配度高于上一轮 if (typeDiffWeight &lt; minTypeDiffWeight) &#123; factoryMethodToUse = candidate; argsHolderToUse = argsHolder; argsToUse = argsHolder.arguments; minTypeDiffWeight = typeDiffWeight; ambiguousFactoryMethods = null; &#125; /* * 了解模糊性:对于具有相同参数数量的方法，如果存在相同类型的差异权重，则收集这样的候选项，并最终引发模糊性异常。 * 但是，只在非宽松的构造函数解析模式下执行该检查，并显式忽略重写的方法(具有相同的参数签名)。 * */ /*条件成立说明：当前处理的构造器 他计算出来的typeDiffWeight值与上一次筛选出来的最优先的构造器的值一致，有模棱两可的情况。加入到模棱两可集合*/ else if (factoryMethodToUse != null &amp;&amp; typeDiffWeight == minTypeDiffWeight &amp;&amp; !mbd.isLenientConstructorResolution() &amp;&amp; paramTypes.length == factoryMethodToUse.getParameterCount() &amp;&amp; !Arrays.equals(paramTypes, factoryMethodToUse.getParameterTypes())) &#123; //如果模棱两可的构造方法为空，就创建一个新的集合， 把满足条件的 构造器收集起来 if (ambiguousFactoryMethods == null) &#123; ambiguousFactoryMethods = new LinkedHashSet&lt;&gt;(); ambiguousFactoryMethods.add(factoryMethodToUse); &#125; //保存模棱两可的构造方法 ambiguousFactoryMethods.add(candidate); &#125; &#125; &#125; //如果要使用的工厂方法为空 或 要使用的参数为空，意思就是没找到创建对象的方法，那就得报错了； if (factoryMethodToUse == null || argsToUse == null) &#123; //如果有异常信息，记录异常信息 if (causes != null) &#123; UnsatisfiedDependencyException ex = causes.removeLast(); for (Exception cause : causes) &#123; this.beanFactory.onSuppressedException(cause); &#125; throw ex; &#125; List&lt;String&gt; argTypes = new ArrayList&lt;&gt;(minNrOfArgs); //走到这里就是可用的方法为空，参数不为空 if (explicitArgs != null) &#123; //保存参数类型 for (Object arg : explicitArgs) &#123; argTypes.add(arg != null ? arg.getClass().getSimpleName() : &quot;null&quot;); &#125; &#125; //如果构造器解析出的参数不为空，记录构造器参数的类型 else if (resolvedValues != null) &#123; Set&lt;ValueHolder&gt; valueHolders = new LinkedHashSet&lt;&gt;(resolvedValues.getArgumentCount()); valueHolders.addAll(resolvedValues.getIndexedArgumentValues().values()); valueHolders.addAll(resolvedValues.getGenericArgumentValues()); for (ValueHolder value : valueHolders) &#123; String argType = (value.getType() != null ? ClassUtils.getShortName(value.getType()) : (value.getValue() != null ? value.getValue().getClass().getSimpleName() : &quot;null&quot;)); argTypes.add(argType); &#125; &#125; //否则就要抛出异常了 String argDesc = StringUtils.collectionToCommaDelimitedString(argTypes); throw new BeanCreationException(mbd.getResourceDescription(), beanName, &quot;No matching factory method found on class [&quot; + factoryClass.getName() + &quot;]: &quot; + (mbd.getFactoryBeanName() != null ? &quot;factory bean &#x27;&quot; + mbd.getFactoryBeanName() + &quot;&#x27;; &quot; : &quot;&quot;) + &quot;factory method &#x27;&quot; + mbd.getFactoryMethodName() + &quot;(&quot; + argDesc + &quot;)&#x27;. &quot; + &quot;Check that a method with the specified name &quot; + (minNrOfArgs &gt; 0 ? &quot;and arguments &quot; : &quot;&quot;) + &quot;exists and that it is &quot; + (isStatic ? &quot;static&quot; : &quot;non-static&quot;) + &quot;.&quot;); &#125; //如果工厂方法没有返回值，那指定不对，抛异常 else if (void.class == factoryMethodToUse.getReturnType()) &#123; throw new BeanCreationException(mbd.getResourceDescription(), beanName, &quot;Invalid factory method &#x27;&quot; + mbd.getFactoryMethodName() + &quot;&#x27; on class [&quot; + factoryClass.getName() + &quot;]: needs to have a non-void return type!&quot;); &#125; //如果模棱两可的方法不为空，抛异常 else if (ambiguousFactoryMethods != null) &#123; throw new BeanCreationException(mbd.getResourceDescription(), beanName, &quot;Ambiguous factory method matches found on class [&quot; + factoryClass.getName() + &quot;] &quot; + &quot;(hint: specify index/type/name arguments for simple parameters to avoid type ambiguities): &quot; + ambiguousFactoryMethods); &#125; /*条件成立：说明自动匹配成功了，需要进行缓存，方便后来者继续使用mergerdbeanDefinition来创建实例*/ if (explicitArgs == null &amp;&amp; argsHolderToUse != null) &#123; mbd.factoryMethodToIntrospect = factoryMethodToUse; argsHolderToUse.storeCache(mbd, factoryMethodToUse); &#125; &#125; /*根据上面选择的构造器和解析出来的参数，通过instantiate方法反射创建bean对象实例，最终将实例设置到beanWrapper的beanInstance实例里面。*/ bw.setBeanInstance(instantiate(beanName, mbd, factoryBean, factoryMethodToUse, argsToUse)); return bw;&#125; 6.applyMergedBeanDefinitionPostProcessors()\u0000遍历所有的后置处理器进行方法调用，典型应用：如果开启了自动依赖注入，那么就会将相关的bean加入到集合中。​ 123456789protected void applyMergedBeanDefinitionPostProcessors(RootBeanDefinition mbd, Class&lt;?&gt; beanType, String beanName) &#123; for (MergedBeanDefinitionPostProcessor processor : getBeanPostProcessorCache().mergedDefinition) &#123; /*这里是后置处理器中的方法执行的逻辑 * 做了一件事情：提取出当前beanType类型整个继承体系内的@Autowired @Value @Inject 信息 并且包装成一个InjectionMetadata的一个对象 * 存放到 AutowiredAnnotationBeanPostProcessor 的缓存中，key是beanName。 * */ processor.postProcessMergedBeanDefinition(mbd, beanType, beanName); &#125;&#125; **MergedBeanDefinitionPostProcessor** 7.registerDisposableBeanIfNecessary()判断当前bean是否需要注册一个需要在容器关闭的时候执行的析构函数​ 12345678910111213141516171819202122232425protected void registerDisposableBeanIfNecessary(String beanName, Object bean, RootBeanDefinition mbd) &#123; /*条件一：原型模式的bean不会注册析构函数 * 条件二：判断当前bean是否需要注册析构函数*/ if (!mbd.isPrototype() &amp;&amp; requiresDestruction(bean, mbd)) &#123; /*如果当前bean对象时单例模式*/ if (mbd.isSingleton()) &#123; // Register a DisposableBean implementation that performs all destruction // work for the given bean: DestructionAwareBeanPostProcessors, // DisposableBean interface, custom destroy method. /*给当前单实例bean注册回调适配器。适配器内 根据当前bean实例是继承接口还是通过自定义来决定具体调用哪个方法，完成析构操作。*/ registerDisposableBean(beanName, new DisposableBeanAdapter( bean, beanName, mbd, getBeanPostProcessorCache().destructionAware)); &#125; else &#123; /*这是自定义作用域的逻辑，压根用不到*/ // A bean with a custom scope... Scope scope = this.scopes.get(mbd.getScope()); if (scope == null) &#123; throw new IllegalStateException(&quot;No Scope registered for scope name &#x27;&quot; + mbd.getScope() + &quot;&#x27;&quot;); &#125; scope.registerDestructionCallback(beanName, new DisposableBeanAdapter( bean, beanName, mbd, getBeanPostProcessorCache().destructionAware)); &#125; &#125;&#125; 首先会进行条件过滤，原型模式的bean排除，不需要注册的bean排除，我们只关心单实例bean的注册逻辑，其他作用域的处理逻辑不需要关注，判断如果是单实例bean对象，给当前的bean对象注册回调适配器。在适配器里面根据当前bean实例是继承接口还是通过自定义来决定具体调用哪个方法完成析构操作。​ **DisposableBeanAdapter**\u0000 123456789101112131415161718192021222324252627282930313233343536373839404142@Overridepublic void run() &#123; destroy();&#125;@Overridepublic void destroy() &#123; if (!CollectionUtils.isEmpty(this.beanPostProcessors)) &#123; for (DestructionAwareBeanPostProcessor processor : this.beanPostProcessors) &#123; processor.postProcessBeforeDestruction(this.bean, this.beanName); &#125; &#125; if (this.invokeDisposableBean) &#123; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Invoking destroy() on bean with name &#x27;&quot; + this.beanName + &quot;&#x27;&quot;); &#125; try &#123; ((DisposableBean) this.bean).destroy(); &#125; catch (Throwable ex) &#123; String msg = &quot;Invocation of destroy method failed on bean with name &#x27;&quot; + this.beanName + &quot;&#x27;&quot;; if (logger.isDebugEnabled()) &#123; logger.warn(msg, ex); &#125; else &#123; logger.warn(msg + &quot;: &quot; + ex); &#125; &#125; &#125; if (this.destroyMethod != null) &#123; invokeCustomDestroyMethod(this.destroyMethod); &#125; else if (this.destroyMethodName != null) &#123; Method methodToInvoke = determineDestroyMethod(this.destroyMethodName); if (methodToInvoke != null) &#123; invokeCustomDestroyMethod(ClassUtils.getInterfaceMethodIfPossible(methodToInvoke)); &#125; &#125;&#125; 总结一下，其实就是创建对象的时候分为三种情况：无参构造器，有参构造器，工厂方法，创建完对象之后，在执行对应的后置处理器(**MergedBeanDefinitionPostProcessor**),最终在判断创建的bean实例是不是需要注册一个析构函数，在容器关闭的时候回调，如果需要就创建并保存。","categories":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/tags/Spring/"}]},{"title":"Spring[四]getBean方法内幕","slug":"Spring/Spring[四]getBean方法内幕","date":"2022-01-11T05:59:14.265Z","updated":"2022-01-11T06:06:59.884Z","comments":true,"path":"2022/01/11/Spring/Spring[四]getBean方法内幕/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/Spring/Spring[%E5%9B%9B]getBean%E6%96%B9%E6%B3%95%E5%86%85%E5%B9%95/","excerpt":"","text":"回顾一下前面，在**AbstractApplicationContext**中，**refresh()**完成了整个IOC容器的刷新，上回我们分析到了初始化剩下所有的单实例bean，这里面有几个核心的地方，**getBean(),createBean(),populateBean()**,三级缓存与循环依赖，完成这些后，整个IOC容器的大体流程就分析完了。本篇我们来分析Spring的**getBean()**。 1.初始化所有的单实例bean对象**finishBeanFactoryInitialization()** 12345678910111213141516171819202122232425262728protected void finishBeanFactoryInitialization(ConfigurableListableBeanFactory beanFactory) &#123; // 为此上下文初始化转换服务 if (beanFactory.containsBean(CONVERSION_SERVICE_BEAN_NAME) &amp;&amp; beanFactory.isTypeMatch(CONVERSION_SERVICE_BEAN_NAME, ConversionService.class)) &#123; beanFactory.setConversionService( beanFactory.getBean(CONVERSION_SERVICE_BEAN_NAME, ConversionService.class)); &#125; // 如果容器里面没有字符串转换器，初始化一个字符串转换器放到容器中。 if (!beanFactory.hasEmbeddedValueResolver()) &#123; beanFactory.addEmbeddedValueResolver(strVal -&gt; getEnvironment().resolvePlaceholders(strVal)); &#125; // 尽早初始化LoadTimeWeaverAware beans，以便尽早注册它们的转换器。 String[] weaverAwareNames = beanFactory.getBeanNamesForType(LoadTimeWeaverAware.class, false, false); for (String weaverAwareName : weaverAwareNames) &#123; getBean(weaverAwareName); &#125; // 停止使用临时类加载器进行类型匹配 beanFactory.setTempClassLoader(null); // 允许缓存所有bean定义元数据，不期望进一步的更改，冻结bd信息，冻结之后就无法往bf注册bd了 beanFactory.freezeConfiguration(); // 实例化所有剩余的单实例bean beanFactory.preInstantiateSingletons();&#125; 冻结bd的信息实际上就是通过一个状态位来控制的，这里面最核心的一个方法或者说步骤就是实例化所有剩余的单实例bean对象。**beanFactory.preInstantiateSingletons()**​ 2.preInstantiateSingletons()12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364@Overridepublic void preInstantiateSingletons() throws BeansException &#123; //日志打印 if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Pre-instantiating singletons in &quot; + this); &#125; /*拿过来所有的beanDefinition names 信息*/ List&lt;String&gt; beanNames = new ArrayList&lt;&gt;(this.beanDefinitionNames); // 触发所有的非懒加载的单例bean的初始化 for (String beanName : beanNames) &#123; //获取bean的定义信息 RootBeanDefinition bd = getMergedLocalBeanDefinition(beanName); // bean不是抽象的 是单例的 不是懒加载的 if (!bd.isAbstract() &amp;&amp; bd.isSingleton() &amp;&amp; !bd.isLazyInit()) &#123; //如果是工厂bean if (isFactoryBean(beanName)) &#123; //通过getBean方法获取bean 前缀 &amp; 拿到的是工厂bean Object bean = getBean(FACTORY_BEAN_PREFIX + beanName); //如果拿到的bean确定是工厂bean if (bean instanceof FactoryBean) &#123; FactoryBean&lt;?&gt; factory = (FactoryBean&lt;?&gt;) bean; //判断这个工厂bean是否期望被初始化 /*判断逻辑：SmartFactoryBean里面有一个isEagerInit方法，这个方法为true就表示这个工厂bean是需要现在创建的*/ boolean isEagerInit = (factory instanceof SmartFactoryBean &amp;&amp; ((SmartFactoryBean&lt;?&gt;) factory).isEagerInit()); //如果期望被初始化 if (isEagerInit) &#123; //通过getBean走创建bean的逻辑 getBean(beanName); &#125; &#125; &#125; else &#123;//执行到这里，说明就是普通的单实例bean，不是工厂bean，直接通过getBean创建bean //三级缓存解决循环依赖的入口： /** * 重点 */ /** * eg：man 和 women 产生循环依赖 * 1.第一次来到这里是首先去创建man */ getBean(beanName); &#125; &#125; &#125; // 触发所有的单实例bean的初始化后的回调逻辑 for (String beanName : beanNames) &#123; //从一级缓存获取单实例bean Object singletonInstance = getSingleton(beanName); //类型断言，执行回调 /*SmartInitializingSingleton里面有一个方法 afterSingletonsInstantiated 这个方法需要在创建好单实例bean之后调用一下*/ if (singletonInstance instanceof SmartInitializingSingleton) &#123; StartupStep smartInitialize = this.getApplicationStartup().start(&quot;spring.beans.smart-initialize&quot;) .tag(&quot;beanName&quot;, beanName); SmartInitializingSingleton smartSingleton = (SmartInitializingSingleton) singletonInstance; //这里就是触发初始化后的回调逻辑 smartSingleton.afterSingletonsInstantiated(); smartInitialize.end(); &#125; &#125;&#125; 这里拿到所有的**beanDefinition**的名字，然后循环判断：【不是抽象的，单实例的，非懒加载的】，然后根据拿到的bean定义信息，再分为**FactoryBean**和**Bean**两种情况进行处理。 梳理一下这里的重点逻辑：​ 合并**beanDefinition**信息 **getMergedLocalBeanDefinition(beanName)** 获取bean对象 **getBean() ** **SmartInitializingSingleton**回调**afterSingletonsInstantiated() **触发单实例bean初始化后的回调逻辑 \u0000接下来先来分析，如何合并bean的定义信息。​ 3. getMergedLocalBeanDefinition(beanName)1234567891011protected RootBeanDefinition getMergedLocalBeanDefinition(String beanName) throws BeansException &#123; // Quick check on the concurrent map first, with minimal locking. /*从缓存获取*/ RootBeanDefinition mbd = this.mergedBeanDefinitions.get(beanName); /*缓存有，直接返回*/ if (mbd != null &amp;&amp; !mbd.stale) &#123; return mbd; &#125; /*真正的合并 bd 的逻辑*/ return getMergedBeanDefinition(beanName, getBeanDefinition(beanName));&#125; 首先会先尝试从缓存来获取**beanDefinition**的信息，如果缓存有，就直接返回，否则就要触发合并bd的逻辑。​ 12345protected RootBeanDefinition getMergedBeanDefinition(String beanName, BeanDefinition bd) throws BeanDefinitionStoreException &#123; return getMergedBeanDefinition(beanName, bd, null);&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293protected RootBeanDefinition getMergedBeanDefinition( String beanName, BeanDefinition bd, @Nullable BeanDefinition containingBd) throws BeanDefinitionStoreException &#123; synchronized (this.mergedBeanDefinitions) &#123; /*合并后的bd信息 * */ RootBeanDefinition mbd = null; /*表示当前beanname对应的过期的mbd信息*/ RootBeanDefinition previous = null; // Check with full lock now in order to enforce the same merged instance. /*null==null*/ if (containingBd == null) &#123; /*从缓存拿信息*/ mbd = this.mergedBeanDefinitions.get(beanName); &#125; /*条件成立说明mbd==null或者 过期...*/ if (mbd == null || mbd.stale) &#123; /*表示当前beanname对应的过期的mbd信息*/ previous = mbd; /*没有当前beanName 对应的 bd 没有使用继承 那么就不用处理继承 */ if (bd.getParentName() == null) &#123; // Use copy of given root bean definition. /*如果 mbd 是 root bean*/ if (bd instanceof RootBeanDefinition) &#123; /*克隆一份保存*/ mbd = ((RootBeanDefinition) bd).cloneBeanDefinition(); &#125; else &#123; /*否则直接创建一个root bean*/ mbd = new RootBeanDefinition(bd); &#125; &#125;/*说明当前beanName 对应的 beanDefinition 存在继承关系*/ else &#123; /*表示bd 的父信息*/ BeanDefinition pbd; try &#123; /*处理beanName 拿到处理了别名和&amp;的真是父bd beanName 名称*/ String parentBeanName = transformedBeanName(bd.getParentName()); /*条件成立 说明 子 bd 和父 bd 名称不一样，就是普通情况*/ if (!beanName.equals(parentBeanName)) &#123; /*递归当前方法，最终返回父 bd 信息*/ pbd = getMergedBeanDefinition(parentBeanName); &#125; else &#123;/*特殊：父子bd名称一样*/ BeanFactory parent = getParentBeanFactory(); if (parent instanceof ConfigurableBeanFactory) &#123; pbd = ((ConfigurableBeanFactory) parent).getMergedBeanDefinition(parentBeanName); &#125; else &#123; throw new NoSuchBeanDefinitionException(parentBeanName, &quot;Parent name &#x27;&quot; + parentBeanName + &quot;&#x27; is equal to bean name &#x27;&quot; + beanName + &quot;&#x27;: cannot be resolved without a ConfigurableBeanFactory parent&quot;); &#125; &#125; &#125; catch (NoSuchBeanDefinitionException ex) &#123; throw new BeanDefinitionStoreException(bd.getResourceDescription(), beanName, &quot;Could not resolve parent bean definition &#x27;&quot; + bd.getParentName() + &quot;&#x27;&quot;, ex); &#125; // 按照父bd信息创建 mbd 对象 mbd = new RootBeanDefinition(pbd); // 用子bd 覆盖mbd信息，以子bd 为基准 父 bd为辅 mbd.overrideFrom(bd); &#125; // Set default singleton scope, if not configured before. if (!StringUtils.hasLength(mbd.getScope())) &#123; mbd.setScope(SCOPE_SINGLETON); &#125; // A bean contained in a non-singleton bean cannot be a singleton itself. // Let&#x27;s correct this on the fly here, since this might be the result of // parent-child merging for the outer bean, in which case the original inner bean // definition will not have inherited the merged outer bean&#x27;s singleton status. if (containingBd != null &amp;&amp; !containingBd.isSingleton() &amp;&amp; mbd.isSingleton()) &#123; mbd.setScope(containingBd.getScope()); &#125; // Cache the merged bean definition for the time being // (it might still get re-merged later on in order to pick up metadata changes) if (containingBd == null &amp;&amp; isCacheBeanMetadata()) &#123; /*缓存合并后的 mbd信息*/ this.mergedBeanDefinitions.put(beanName, mbd); &#125; &#125; if (previous != null) &#123; copyRelevantMergedBeanDefinitionCaches(previous, mbd); &#125; return mbd; &#125;&#125; 这里面需要关注的点有两个：​ 如何处理bean的名字 **transformedBeanName()** 如何合并bd信息 **getMergedBeanDefinition()** ​ 4.处理bean的名字12345678910protected String transformedBeanName(String name) &#123; /* * 返回 BeanFactoryUtils.transformedBeanName(name) 处理完的bean name ，这里也可能是别名。 * spring的bean别名是通过aliasMap保存的。 * &#123;C:B,B:A&#125; a有一个别名叫做 b b有一个别名叫做 c * * 通过这个方法canonicalName 去处理 bean的名字 和别名之间的关系 ，最终返回的是 bean 的真实名字。 * */ return canonicalName(BeanFactoryUtils.transformedBeanName(name));&#125; 4.1 canonicalName()处理bean的名字与别名。​ 123456789101112131415161718public String canonicalName(String name) &#123; /*假设传入c*/ String canonicalName = name; // Handle aliasing... String resolvedName; do &#123; /*根据c会从map拿到b*/ resolvedName = this.aliasMap.get(canonicalName); /*如果获取到的结果不是null*/ if (resolvedName != null) &#123; /*赋值再次循环拿*/ canonicalName = resolvedName; &#125; &#125; /*当resolvedName是空的时候，那么此时的canonicalName一定是最终的名字，返回即可。*/ while (resolvedName != null); return canonicalName;&#125; 4.2 BeanFactoryUtils.transformedBeanName(name)1234567891011121314151617181920212223242526public static String transformedBeanName(String name) &#123; /*断言*/ Assert.notNull(name, &quot;&#x27;name&#x27; must not be null&quot;); /*如果当前bean对象不是 &amp; 开头 （说明是正常bean对象实例） 直接返回*/ if (!name.startsWith(BeanFactory.FACTORY_BEAN_PREFIX)) &#123; return name; &#125; /* * 这里是拿工厂bean的逻辑： * transformedBeanNameCache：缓存处理完&amp;开头的beanName，提升性能 * map.computeIfAbsent(k,v) 说明： * 当map中对应的k ==null || v ==null 这次写操作就会成功，并且返回 k v， * 否则就会失败，并且返回原有的k v。 */ return transformedBeanNameCache.computeIfAbsent(name, beanName -&gt; &#123; /** * 假设bean name = &amp;abc * 判断如果name不是以 &amp;开头 跳出循环 最终会返回 abc。 */ do &#123; beanName = beanName.substring(BeanFactory.FACTORY_BEAN_PREFIX.length()); &#125; while (beanName.startsWith(BeanFactory.FACTORY_BEAN_PREFIX)); return beanName; &#125;);&#125; 5.合并bd信息**getMergedBeanDefinition()**​ 1234567891011protected RootBeanDefinition getMergedLocalBeanDefinition(String beanName) throws BeansException &#123; // Quick check on the concurrent map first, with minimal locking. /*从缓存获取*/ RootBeanDefinition mbd = this.mergedBeanDefinitions.get(beanName); /*缓存有，直接返回*/ if (mbd != null &amp;&amp; !mbd.stale) &#123; return mbd; &#125; /*真正的合并 bd 的逻辑*/ return getMergedBeanDefinition(beanName, getBeanDefinition(beanName));&#125; 12345protected RootBeanDefinition getMergedBeanDefinition(String beanName, BeanDefinition bd) throws BeanDefinitionStoreException &#123; return getMergedBeanDefinition(beanName, bd, null);&#125; 再往下其实又回到了上面的**getMergedBeanDefinition()**，递归去处理bd信息。 合并完bd信息之后就是判断是否是工厂bean的逻辑，想要获取工厂bean，在前面提到过，需要在bean的名字前面加一个 &amp; 。​ 默认情况下，我们要看单实例bean的获取过程，此时我们去看 **getBean()**。​ 6.获取单实例bean对象**getBean()**​ 12345@Overridepublic Object getBean(String name) throws BeansException &#123; /*真正加载bean的方法*/ return doGetBean(name, null, null, false);&#125; **doGetBean()** 返回一个指定的bean实例，这个bean可以是共享的，也可以是单例的。​ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246protected &lt;T&gt; T doGetBean( String name, @Nullable Class&lt;T&gt; requiredType, @Nullable Object[] args, boolean typeCheckOnly) throws BeansException &#123; /* * 处理转换bean名字，可能是一个别名，也可能是一个带着&amp; 开头的beanName * 别名：重定向出来真的beanName * &amp;开头：说明要获取的bean实例对象实际上是一个工厂bean * FactoryBean ：如果某个bean的配置特别复杂，使用spring管理不容易...不够灵活，想要使用编码的形式去构建它， * 那么你就可以提供一个构建该bean实例的工厂，这个工厂就是factoryBean接口 * factoryBean接口的实现类还是需要spring来管理的。 * 这里就涉及到两种对象：一种是beanFactory实现类 另一个是FactoryBean内部管理的对象。 * * 如果要获取工厂bean，需要获取 &amp; * 如果要拿factoryBean内部管理的对象，直接传name 不需要带着 &amp; */ String beanName = transformedBeanName(name); //保留返回值的 Object beanInstance; /*到缓存中获取共享单实例 第一次去缓存拿*/ /** * 1.第一次经过这里是创建man的时候，首先去缓存获取，但是这个时候，man对象时第一次创建，所以什么都拿不到 * 2.此时man在第三级缓存，然后去拿women，这个时候women还没创建，所以锤子也拿不到 * 3.第三次经过这里的时候，实际上就是women创建完了，放到三级缓存了，然后发现属性赋值的时候需要man，又通过getBean来拿， * 这个时候，在第三级缓存拿到了早期对象的引用，然后并对man进行一个缓存升级 ，从 三级缓存升级到了二级 */ Object sharedInstance = getSingleton(beanName); /*此时的逻辑应该是从二级缓存拿到了早期暴露的bean实例，并且属性没有填充*/ /*这里如果是第一次创建bean实例，上面从一级缓存实际上是啥也没拿到，所以走到这里，实例是null。*/ if (sharedInstance != null &amp;&amp; args == null) &#123; if (logger.isTraceEnabled()) &#123; if (isSingletonCurrentlyInCreation(beanName)) &#123; logger.trace(&quot;Returning eagerly cached instance of singleton bean &#x27;&quot; + beanName + &quot;&#x27; that is not fully initialized yet - a consequence of a circular reference&quot;); &#125; else &#123; logger.trace(&quot;Returning cached instance of singleton bean &#x27;&quot; + beanName + &quot;&#x27;&quot;); &#125; &#125; /* * 这里为什么又要包装一层呢？ * 从ioc中拿到的对象可能是普通的单实例，也可能是FactoryBean实例 * 如果是FactoryBean实例，还要考虑进行处理 主要看name带不带 &amp; * 带&amp;说明这次getBean想要拿工厂bean，否则想要拿 FactoryBean 内部管理的 bean 实例 */ /** * 1.走到这里是什么时候？创建man，然后对man进行属性赋值，发现需要women，然后递归去创建women，然后对women进行属性赋值， * 然后发现需要man，然后从缓存拿，恰好此时从三级缓存拿到了，早期man对象的引用，这个时候，这里啥也没干，直接return */ beanInstance = getObjectForBeanInstance(sharedInstance, name, beanName, null); &#125; /* * 执行到这里的情况：sharedInstance == null || args != null * 1. 二级缓存未拿到bean实例 * 2. 二级缓存拿到了bean实例，但是属性已经填充完了 ！！！这是不可能的，这样的话就在一级缓存了。 * * 缓存没有想要的数据 * 1.原型循环依赖问题的判定 */ else &#123; /* * 原型循环依赖的判定 * eg： a(prototype) - b b - a (prototype) * 1.会像正在创建中的原型集合添加 a * 2.创建 a 早期对象 二级缓存 * 3.处理 a的依赖 发现 a 依赖 b 类型 * 4.触发 spring getBeab(b.class) 的操作 * 5.根据 b 的构造方法 反射创建 b 的早期实例 * 6.spring 处理 b 对象的依赖发现依赖了 a * 7.所以spring砖头回来再次获取 a getBean(a.class) * 8.程序再次来到这里会判断当前要获取的a对象是不是正在创建中 如果是循环依赖 会返回true ，最终抛出异常 结束了循环依赖注入 */ if (isPrototypeCurrentlyInCreation(beanName)) &#123; throw new BeanCurrentlyInCreationException(beanName); &#125; /*父子容器相关的处理逻辑*/ BeanFactory parentBeanFactory = getParentBeanFactory(); if (parentBeanFactory != null &amp;&amp; !containsBeanDefinition(beanName)) &#123; // Not found -&gt; check parent. String nameToLookup = originalBeanName(name); if (parentBeanFactory instanceof AbstractBeanFactory) &#123; return ((AbstractBeanFactory) parentBeanFactory).doGetBean( nameToLookup, requiredType, args, typeCheckOnly); &#125; else if (args != null) &#123; // Delegation to parent with explicit args. return (T) parentBeanFactory.getBean(nameToLookup, args); &#125; else if (requiredType != null) &#123; // No args -&gt; delegate to standard getBean method. return parentBeanFactory.getBean(nameToLookup, requiredType); &#125; else &#123; return (T) parentBeanFactory.getBean(nameToLookup); &#125; &#125; /*穿的是false就成立*/ if (!typeCheckOnly) &#123; /* * 将指定的 bean 标记为已创建（或即将创建）。 * 允许 bean 工厂优化其缓存以重复创建指定 bean。 */ /** * 1.第一次创建man的时候，经过这里，标记man正在创建中 * 2.第二次经过这里的时候，就是对man进行属性赋值的过程中，发现依赖women，所以去走getBean逻辑。这个时候去标记women也是正在被创建中 */ markBeanAsCreated(beanName); &#125; StartupStep beanCreation = this.applicationStartup.start(&quot;spring.beans.instantiate&quot;) .tag(&quot;beanName&quot;, name); try &#123; if (requiredType != null) &#123; beanCreation.tag(&quot;beanType&quot;, requiredType::toString); &#125; /*获取合并beanDefinition信息 * 为什么需要合并？ * bd 支持继承 子 会 继承 父亲 的所有 信息 * */ RootBeanDefinition mbd = getMergedLocalBeanDefinition(beanName); /*判断当前mbd是不是抽象的，如果是抽象的，需要抛出异常，因为抽象的bd不能创建实例，只能作为模板让子bd继承*/ checkMergedBeanDefinition(mbd, beanName, args); // Guarantee initialization of beans that the current bean depends on. /* * depends-on 属性处理 * &lt;bean name=&quot;A&quot; depends-on=&quot;B&quot; /&gt; * &lt;bean name=&quot;B&quot; /&gt; * * 循环依赖问题 b - a a - b * spring是处理不了这种情况的，需要报错 * spring需要发现这种情况的产生： * 如何发现？依靠两个map * 1.dependentBeanMap 记录依赖当前beanName的其他beanName * 2.dependenciesForBeanMap 记录当前bean依赖的其他bean集合 * */ String[] dependsOn = mbd.getDependsOn(); if (dependsOn != null) &#123; for (String dep : dependsOn) &#123; /*判断循环依赖*/ if (isDependent(beanName, dep)) &#123; throw new BeanCreationException(mbd.getResourceDescription(), beanName, &quot;Circular depends-on relationship between &#x27;&quot; + beanName + &quot;&#x27; and &#x27;&quot; + dep + &quot;&#x27;&quot;); &#125; /* * 注册依赖关系 * 1.记录a依赖了谁 * 2.记录谁依赖了b * */ registerDependentBean(dep, beanName); try &#123; getBean(dep); &#125; catch (NoSuchBeanDefinitionException ex) &#123; throw new BeanCreationException(mbd.getResourceDescription(), beanName, &quot;&#x27;&quot; + beanName + &quot;&#x27; depends on missing bean &#x27;&quot; + dep + &quot;&#x27;&quot;, ex); &#125; &#125; &#125; // 创建单实例 if (mbd.isSingleton()) &#123; /*第二个 getSingthon() 创建实例并返回*/ /** * 1.第一次经过这里是去创建man对象 * 2.创建women对象 */ sharedInstance = getSingleton(beanName, () -&gt; &#123; try &#123; return createBean(beanName, mbd, args); &#125; catch (BeansException ex) &#123; // Explicitly remove instance from singleton cache: It might have been put there // eagerly by the creation process, to allow for circular reference resolution. // Also remove any beans that received a temporary reference to the bean. destroySingleton(beanName); throw ex; &#125; &#125;); /* * 这里为什么不直接返回 ，还调用 这个方法？ * 创建出来的单实例bean也可能是工厂bean对象，所以需要根据名字判断到底返回 * bean对象还是工厂bean * */ beanInstance = getObjectForBeanInstance(sharedInstance, name, beanName, mbd); &#125; /*多例bean的创建*/ else if (mbd.isPrototype()) &#123; // It&#x27;s a prototype -&gt; create a new instance. Object prototypeInstance = null; try &#123; /*记录当前线程相关的正在创建的原型对象beanName*/ beforePrototypeCreation(beanName); /*创建对象*/ prototypeInstance = createBean(beanName, mbd, args); &#125; finally &#123; /*从正在创建中的集合中移除*/ afterPrototypeCreation(beanName); &#125; beanInstance = getObjectForBeanInstance(prototypeInstance, name, beanName, mbd); &#125; /*other作用域： 略 ....*/ else &#123; String scopeName = mbd.getScope(); if (!StringUtils.hasLength(scopeName)) &#123; throw new IllegalStateException(&quot;No scope name defined for bean ´&quot; + beanName + &quot;&#x27;&quot;); &#125; Scope scope = this.scopes.get(scopeName); if (scope == null) &#123; throw new IllegalStateException(&quot;No Scope registered for scope name &#x27;&quot; + scopeName + &quot;&#x27;&quot;); &#125; try &#123; Object scopedInstance = scope.get(beanName, () -&gt; &#123; beforePrototypeCreation(beanName); try &#123; return createBean(beanName, mbd, args); &#125; finally &#123; afterPrototypeCreation(beanName); &#125; &#125;); beanInstance = getObjectForBeanInstance(scopedInstance, name, beanName, mbd); &#125; catch (IllegalStateException ex) &#123; throw new ScopeNotActiveException(beanName, scopeName, ex); &#125; &#125; &#125; catch (BeansException ex) &#123; beanCreation.tag(&quot;exception&quot;, ex.getClass().toString()); beanCreation.tag(&quot;message&quot;, String.valueOf(ex.getMessage())); cleanupAfterBeanCreationFailure(beanName); throw ex; &#125; finally &#123; beanCreation.end(); &#125; &#125; /** * 1.走到这里的逻辑：对man进行属性赋值，递归创建women，对women进行属性赋值，然后从缓存拿到了man */ return adaptBeanInstance(name, beanInstance, requiredType);&#125; 这个方法的逻辑有点多，进行一个简单的梳理：​ 首先上来先处理bean的名字 **transformedBeanName(name)** 然后尝试通过 **getSingleton(beanName) **去一级缓存拿数据，但是这个时候这个bean对象还没有创建过，此时是拿不到数据的。 如果说拿到了bean对象，那么就再通过 **getObjectForBeanInstance() **对bean对象进行一层包装。 这里为什么又要包装一层呢？从IOC中拿到的对象可能是普通单实例bean对象，也可能是一个工厂bean对象。如果是工厂bean对象，还需要考虑是不是需要处理，主要是看名字里面带不带 &amp; 。如果带 &amp; ，就是想要拿工厂本身，如果不带 &amp; ，其实就是想拿工厂bean生产的对象。 正常第一次去获取bean对象的时候，是在缓存拿不到数据的，所以会走else的逻辑。 判断是不是原型模式的循环依赖，如果是的话，抛出异常。 然后就是护理父子容器相关的逻辑 如果是第一次创建bean，就会通过一个状态位来标记bean正在创建中 获取合并后的bd信息 判断当前bean对象是不是抽象的，如果是抽象的，需要跑出异常，因为抽象的bean是模板bean，不能创建实例 判断是不是发生互相依赖，注意是互相依赖，不是循环依赖，如果是发生了互相依赖，则抛出异常。 互相依赖：bean标签或者@Bean 注解里面配置了 **depends-on=&quot;B&quot; a-&gt;b , b-&gt;a**​ 循环依赖：a里面有个属性叫做b，b里面有个属性a\u0000 注册依赖关系，记录当前bean依赖了谁，谁依赖了当前bean 判断如果是单实例bean，这次就会通过**getSingleton()-&gt;createBean()**去创建bean对象 然后再通过**getObjectForBeanInstance() **对bean对象进行一层包装 如果是原型实例的bean对象，就会走多实例bean创建的流程，其实还是通过**createBean()**去创建对象，只是不会被一级缓存所缓存。 对于其他作用域的bean对象，则走他们的创建逻辑，再次不作为重点内容分析。 最终的逻辑**adaptBeanInstance()**其实就是对循环依赖的处理，对 A对象属性赋值的时候，发现需要B对象。然后从一级缓存拿发现没有，就走这里递归去创建B对象，从缓存拿到A对象，对B对象进行属性赋值。 7.getSingleton()看一下这个方法，从缓存来获取bean。这个方法在 **DefaultSingletonRegistry** 中。​ 123456@Override@Nullablepublic Object getSingleton(String beanName) &#123; /*方法重载 allowEarlyReference 是否允许拿到早期引用*/ return getSingleton(beanName, true);&#125; \u0000这里发生了方法重载。这个方法挺叼的，根据名称来返回单实例bean对象，能够解决循环依赖。​ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970@Nullableprotected Object getSingleton(String beanName, boolean allowEarlyReference) &#123; /*通过名字去一级缓存拿单实例bean对象*/ Object singletonObject = this.singletonObjects.get(beanName); /*如果拿到的对象实例是null，有几种情况？ * 1.单实例确实没创建呢 * 2. 当前正在创建中，发生了循环依赖了，这个时候实例其实在二级缓存 * * 循环依赖： * A-&gt;B B-&gt;A * 单实例的循环依赖有几种？ * 1.构造方法 无解 * 2.setter 有解 通过三级缓存 * * 三级缓存实际上如何解决的循环依赖？ * 利用bean的中间状态 ：已经实例化但是还未初始化 * A-B B-&gt;A setter依赖 * 1. 假设spring先实例化A，首先拿到A的构造方法，反射创建A的早期实例对象，这个早期对象被包装了一下， * 变成ObjectFactory对象，放到三级缓存。 * 2. 处理A的依赖数据，检查发现 A依赖B ，所以，spring 根据 B的类型去容器中去getBean(B.class) ,这里就是递归了 * 3. 首先拿到B的构造方法，反射创建B的早期实例对象，把B包装成ObjectFactory对象，放到三级缓存。 * 4. 处理Bde 依赖数据，检查发现，B依赖对象A，所以接下来，spring就会根据A类型去容器去getBean(A.class) 对象，这个时候又递归了 * 5. 程序还会走到当前方法getSingleton * 6. 条件一成立，条件二成立。 * */ if (singletonObject == null &amp;&amp; isSingletonCurrentlyInCreation(beanName)) &#123; /*从二级缓存拿数据*/ singletonObject = this.earlySingletonObjects.get(beanName); /*条件成立说明二级缓存没有 去三级缓存拿*/ if (singletonObject == null &amp;&amp; allowEarlyReference) &#123; synchronized (this.singletonObjects) &#123; /* * spring为什么需要有三级缓存，而不是只有二级缓存？ * AOP 靠什么实现呢？动态代理 jdk cglib * 代理：静态代理：需要手动写代码实现新的JAVA文件，这个JAV类需要和代理对象实现同一个接口，内部维护一个被代理对象 * 代理类在接口调用原生对象前后可以加一些逻辑。 * 代理对象和被代理对象是两个不同的内存地址，一定是不一样的 * 动态代理：... 不需要人为写代码了，而是依靠字节码框架动态生成字节码文件，然后jvm在进行加载，然后也是一样 * 也是去new代理对象，这个代理对象没啥特殊的，也是内部保留了原生对象，然后再调用原生对象前后实现的字节码增强。 * 两者共同点：代理对象和被代理对象实际上都不是同一内存地址 * * 三级缓存在这里有什么意义呢？ * 三级缓存里面保存的是对象工厂，这个对象工厂内部保留着原生对象的引用，ObjectFactory的实现类，getObject方法， * 需要考虑一个问题：到底是返回原生的，还是增强的？ * getObject会判断当前早期实例是否需要被增强，如果是 那么提前完成动态代理增强，返回代理对象，否则，返回原生对象。 * */ /*从一级缓存拿*/ singletonObject = this.singletonObjects.get(beanName); if (singletonObject == null) &#123; /*从二级缓存拿*/ singletonObject = this.earlySingletonObjects.get(beanName); if (singletonObject == null) &#123; /*从三级缓存拿*/ ObjectFactory&lt;?&gt; singletonFactory = this.singletonFactories.get(beanName); /*条件成立：说明第三级缓存有数据。这里就涉及到了缓存的升级 ，很简单 ，从三级挪到二级 ，再反手干掉三级的。*/ if (singletonFactory != null) &#123; singletonObject = singletonFactory.getObject(); this.earlySingletonObjects.put(beanName, singletonObject); this.singletonFactories.remove(beanName); &#125; &#125; &#125; &#125; &#125; &#125; return singletonObject;&#125; 梳理一下这个方法的大体逻辑：(我们现在走的是单实例bean创建的流程，所以假设我们正在创建一个单实例bean，执行到了这里，三级缓存与循环依赖的问题会在后续文章分析。)​ 先根据名字去一级缓存拿bean，此时是拿不到的。 判断当前bean，是不是单实例bean对象，是不是正在创建中？ 走这里的逻辑就是发生了循环依赖： 假设spring先实例化A，首先拿到A的构造方法，反射创建A的早期实例对象，这个早期对象被包装了一下，变成ObjectFactory对象，放到三级缓存。\u0000\u00002. 处理A的依赖数据，检查发现 A依赖B ，所以，spring 根据 B的类型去容器中去getBean(B.class) ,这里就是递归了\u0000 首先拿到B的构造方法，反射创建B的早期实例对象，把B包装成ObjectFactory对象，放到三级缓存。\u0000 处理Bde 依赖数据，检查发现，B依赖对象A，所以接下来，spring就会根据A类型去容器去getBean(A.class) 对象，这个时候又递归了\u0000 程序还会走到当前方法getSingleton\u0000 首先去二级缓存拿数据，这个时候二级缓存是拿不到数据的，所以会继续往下走 再次尝试从一级缓存和二级缓存拿，这个时候其实还是拿不到的，所以从三级缓存来拿 如果三级缓存拿到了数据，那就进行缓存的升级 把三级缓存的对象拿到二级缓存，三级缓存的对象干掉。 ​ 8.思考与沉淀spring为什么需要有三级缓存，而不是只有二级缓存？AOP 靠什么实现呢？动态代理 jdk cglib代理：静态代理：需要手动写代码实现新的JAVA文件，这个JAV类需要和代理对象实现同一个接口，内部维护一个被代理对象 代理类在接口调用原生对象前后可以加一些逻辑。 代理对象和被代理对象是两个不同的内存地址，一定是不一样的 动态代理：… 不需要人为写代码了，而是依靠字节码框架动态生成字节码文件，然后jvm在进行加载，然后也是一样 也是去new代理对象，这个代理对象没啥特殊的，也是内部保留了原生对象，然后再调用原生对象前后实现的字节码增强。两者共同点：代理对象和被代理对象实际上都不是同一内存地址​ 三级缓存在这里有什么意义呢？三级缓存里面保存的是对象工厂，这个对象工厂内部保留着原生对象的引用，ObjectFactory的实现类，getObject方法，需要考虑一个问题：到底是返回原生的，还是增强的？getObject会判断当前早期实例是否需要被增强，如果是 那么提前完成动态代理增强，返回代理对象，否则，返回原生对象。​ 9.getObjectForBeanInstance()这个方法是获取给定的bean实例对象，如果是工厂bean，则根据名字返回工厂本身或者其创建的对象。​ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162/** * 获取给定 bean 实例的对象，如果是 FactoryBean，则是 bean 实例本身或其创建的对象。 * @param beanInstance 共享单实例对象 * @param name 未处理 &amp; 的 name * @param beanName 处理过&amp; 和别名后的name * @param mbd the merged bean definition 合并过后的beanDefinition信息 * @return the object to expose for the bean */protected Object getObjectForBeanInstance( Object beanInstance, String name, String beanName, @Nullable RootBeanDefinition mbd) &#123; /*判断当前的name是不是&amp;开始的，条件成立，说明当前要获取的是工厂bean对象*/ if (BeanFactoryUtils.isFactoryDereference(name)) &#123; if (beanInstance instanceof NullBean) &#123; return beanInstance; &#125; /*条件成立：说明但实例对象不是工厂bean接口的实现类 直接报错*/ if (!(beanInstance instanceof FactoryBean)) &#123; throw new BeanIsNotAFactoryException(beanName, beanInstance.getClass()); &#125; /*打标 给当前bean对应的mbd打标，记录他表达的实例是一个工厂bean*/ if (mbd != null) &#123; mbd.isFactoryBean = true; &#125; return beanInstance; &#125; /* * 执行到这里有几种情况？ * 1.当前bean实例是普通单实例 * 2.当前bean实例是工厂bean接口实现类，但是当前要获取的是工厂内部管理的bean实例 */ if (!(beanInstance instanceof FactoryBean)) &#123; return beanInstance; &#125; /*保存工厂bean实例的getobject值得*/ Object object = null; if (mbd != null) &#123; mbd.isFactoryBean = true; &#125; else &#123; /*尝试到缓存获取工厂bean*/ object = getCachedObjectForFactoryBean(beanName); &#125; /*此时说明缓存没有，需要到工厂bean getObject 获取*/ if (object == null) &#123; // Return bean instance from factory. FactoryBean&lt;?&gt; factory = (FactoryBean&lt;?&gt;) beanInstance; // 条件一几乎恒成立 条件二：判断spring中当前是否有当前beanName的bd信息 if (mbd == null &amp;&amp; containsBeanDefinition(beanName)) &#123; /*拿到合并后的bd信息 * 为什么是合并后的呢？ * 因为bd支持继承的，合并后的bd信息是包含继承回来的bd * */ mbd = getMergedLocalBeanDefinition(beanName); &#125; /*synthetic默认值是false ，表示这是一个用户对象 如果是 true 表示是系统对象*/ boolean synthetic = (mbd != null &amp;&amp; mbd.isSynthetic()); /*到这里说明 说明真正的去执行 getBean() 的逻辑 time 【51：32】*/ object = getObjectFromFactoryBean(factory, beanName, !synthetic); &#125; return object;&#125; ​ 判断当前的name是不是 &amp; 开始的，条件成立，说明当前要获取的是工厂bean对象 如果当前bean独享没有实现工厂bean接口，直接报错 打标：给当前bean对应的mbd打标，记录他表达的实例是一个工厂bean对象 返回bean实例 判断如果当前bean没有实现工厂bean接口，（思考一下，走到这里的逻辑，其实要么是单实例bean，要么是获取工厂bean创建的对象），既然没有实现工厂bean接口，所以这里的逻辑就是处理单实例bean的。 直接返回就行了 走到这里的逻辑就是一种情况：获取工厂bean创建的bean对象。 在mbd打标，表示是一个工厂bean对象 如果缓存里面没有当前bean工厂生产的对象，需要通过工厂bean的**getBean()**去获取。 判断有没有bd信息，如果没有就合并bd信息得到mbd信息 然后再通过**getObjectFromFactoryBean() **根据mbd信息去获取bean对象 \u0000 9.1 getObjectFromFactoryBean()这个方法的逻辑上存在问题，但是不影响分析。​ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556protected Object getObjectFromFactoryBean(FactoryBean&lt;?&gt; factory, String beanName, boolean shouldPostProcess) &#123; /*如果是已经存在的单实例bean对象*/ if (factory.isSingleton() &amp;&amp; containsSingleton(beanName)) &#123; /*加锁 内部逻辑是串行化的，不存在并发*/ synchronized (getSingletonMutex()) &#123; /*先从缓存获取*/ Object object = this.factoryBeanObjectCache.get(beanName); if (object == null) &#123; /*这里已经是空了*/ object = doGetObjectFromFactoryBean(factory, beanName); // Only post-process and store if not put there already during getObject() call above // (e.g. because of circular reference processing triggered by custom getBean calls) Object alreadyThere = this.factoryBeanObjectCache.get(beanName); /*这里一定是空，因为是串行化的方法，所以逻辑有问题*/ if (alreadyThere != null) &#123; object = alreadyThere; &#125; else &#123; if (shouldPostProcess) &#123; /*判断当前实例是否被创建 逻辑有问题 明明上面就是没创建 */ if (isSingletonCurrentlyInCreation(beanName)) &#123; // Temporarily return non-post-processed object, not storing it yet.. return object; &#125; beforeSingletonCreation(beanName); try &#123; /*执行后置处理器的逻辑*/ object = postProcessObjectFromFactoryBean(object, beanName); &#125; catch (Throwable ex) &#123; throw new BeanCreationException(beanName, &quot;Post-processing of FactoryBean&#x27;s singleton object failed&quot;, ex); &#125; finally &#123; afterSingletonCreation(beanName); &#125; &#125; if (containsSingleton(beanName)) &#123; this.factoryBeanObjectCache.put(beanName, object); &#125; &#125; &#125; return object; &#125; /*工厂bean对象内部维护的对象不是单实例，每次都是一个新对象*/ &#125; else &#123; /*直接拿*/ Object object = doGetObjectFromFactoryBean(factory, beanName); if (shouldPostProcess) &#123; try &#123; /*后置处理器的逻辑*/ object = postProcessObjectFromFactoryBean(object, beanName); &#125; catch (Throwable ex) &#123; throw new BeanCreationException(beanName, &quot;Post-processing of FactoryBean&#x27;s object failed&quot;, ex); &#125; &#125; return object; &#125;&#125; if里面的判断是不会成立的，所以直接看else的逻辑。​ 通过委派模式委派给**doGetObjectFromFactoryBean()**去拿bean对象，然后再执行后置处理器的逻辑。​ 继续往下分析：​ 9.2 doGetObjectFromFactoryBean()1234567891011121314151617181920212223private Object doGetObjectFromFactoryBean(FactoryBean&lt;?&gt; factory, String beanName) throws BeanCreationException &#123; Object object; try &#123; object = factory.getObject(); &#125; catch (FactoryBeanNotInitializedException ex) &#123; throw new BeanCurrentlyInCreationException(beanName, ex.toString()); &#125; catch (Throwable ex) &#123; throw new BeanCreationException(beanName, &quot;FactoryBean threw exception on object creation&quot;, ex); &#125; // Do not accept a null value for a FactoryBean that&#x27;s not fully // initialized yet: Many FactoryBeans just return null then. if (object == null) &#123; if (isSingletonCurrentlyInCreation(beanName)) &#123; throw new BeanCurrentlyInCreationException( beanName, &quot;FactoryBean which is currently in creation returned null from getObject&quot;); &#125; object = new NullBean(); &#125; return object;&#125; 这里面的核心代码其实就是只有一句：调用工厂bean的**factory.getObject()** 获取bean对象。​ 剩余的**createBean()** &amp; **adaptBeanInstance()**，**createBean()**创建bean的逻辑会在下一篇中深入分析，**adaptBeanInstance()**循环依赖相关的逻辑会在后续的三级缓存与循环依赖篇中分析，与本次的创建单实例bean对象关系并不大。至此，整个**getBean()**就分析完了。​ 10.触发所有单实例bean初始化后的回调逻辑在**preInstantiateSingletons()**里面通过 getBean() 方法实例化完所有的单实例bean以后，就会触发所有 **SmartInitializingSingleton** 的 **afterSingletonsInstantiated()**。\u0000\u0000这个组件具体的作用，在前面的Spring组件与注解篇里面已经详细介绍过，在此不再赘述。​ 123456789101112131415// 触发所有的单实例bean的初始化后的回调逻辑for (String beanName : beanNames) &#123; //从一级缓存获取单实例bean Object singletonInstance = getSingleton(beanName); //类型断言，执行回调 /*SmartInitializingSingleton里面有一个方法 afterSingletonsInstantiated 这个方法需要在创建好单实例bean之后调用一下*/ if (singletonInstance instanceof SmartInitializingSingleton) &#123; StartupStep smartInitialize = this.getApplicationStartup().start(&quot;spring.beans.smart-initialize&quot;) .tag(&quot;beanName&quot;, beanName); SmartInitializingSingleton smartSingleton = (SmartInitializingSingleton) singletonInstance; //这里就是触发初始化后的回调逻辑 smartSingleton.afterSingletonsInstantiated(); smartInitialize.end(); &#125;&#125; 下一篇，我们将继续分析bean对象的创建逻辑 **createBean()**。","categories":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/tags/Spring/"}]},{"title":"Spring[三]解析配置文件","slug":"Spring/Spring[三]解析配置文件","date":"2022-01-11T05:59:03.018Z","updated":"2022-01-11T06:06:28.495Z","comments":true,"path":"2022/01/11/Spring/Spring[三]解析配置文件/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/Spring/Spring[%E4%B8%89]%E8%A7%A3%E6%9E%90%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/","excerpt":"","text":"本来这一篇是要写如何加载所有的单实例bean对象，但是回顾第二篇，发现对于解析xml文件加载bean定义信息的部分理解表达的并不是很好，所以在此补充一篇Spring解析配置文件，加载bean定义信息的文章。 ​ 1.以refresh()作为抓手**refresh()**作为容器的刷新方法，重要性不必多说，在里面的第二个方法**obtainFreshBeanFactory()**里面，解析了xml配置文件或注解，载入bean定义资源信息，返回了一个全新的bean工厂。接下来来分析**obtainFreshBeanFactory()**。​ 2.obtainFreshBeanFactory()解析xml文件或者注解，加载bean 的定义信息，创建一个全新的bean工厂。​ \u0000 123456protected ConfigurableListableBeanFactory obtainFreshBeanFactory() &#123; //刷新bean工厂 refreshBeanFactory(); /**返回bean工厂*/ return getBeanFactory();&#125; \u0000 3.refreshBeanFactory()这里面主要就是判断当前有没有bean工厂，如果有的话，就把工厂关了，重新造一个，如果没有的话，直接造一个。总之一定要造一个新的工厂。**createBeanFactory()** 12345678910111213141516171819202122232425@Overrideprotected final void refreshBeanFactory() throws BeansException &#123; /*如果已经有了bean工厂，通常情况下并不会，什么情况下会有？通过applicationContext直接调用refresh方法*/ if (hasBeanFactory()) &#123; /*销毁里面的所有bean*/ destroyBeans(); /*关闭bean工厂*/ closeBeanFactory(); &#125; try &#123; //不管上面是否已经有bean工厂存在，最终都会走到这里，去创建一个bean工厂 DefaultListableBeanFactory beanFactory = createBeanFactory(); /*设置序列化id*/ beanFactory.setSerializationId(getId()); /*对工厂进行一些定制化设置*/ customizeBeanFactory(beanFactory); /*加载bean的定义信息*/ loadBeanDefinitions(beanFactory); /*将当前类的bean工厂引用指向创建的bean工厂*/ this.beanFactory = beanFactory; &#125; catch (IOException ex) &#123; throw new ApplicationContextException(&quot;I/O error parsing bean definition source for &quot; + getDisplayName(), ex); &#125;&#125; **createBeanFactory()**返回一个全新的bean工厂。​ 123protected DefaultListableBeanFactory createBeanFactory() &#123; return new DefaultListableBeanFactory(getInternalParentBeanFactory());&#125; 每次容器刷新的时候尝试调用此方法。默认创建一个**DefaultListableBeanFactory**，并将此上下文父级的内部bean工厂作为父bean工厂。​ 可以再子类中覆盖，例如自定义**DefaultListableBeanFactory**的设置。​ 创建完工厂了，就要往工厂里面放东西，** loadBeanDefinitions(beanFactory)**。​ 4.loadBeanDefinitions(beanFactory)载入bean的定义信息 **beanDefinition**。​ 12345678910111213141516@Overrideprotected void loadBeanDefinitions(DefaultListableBeanFactory beanFactory) throws BeansException, IOException &#123; // 创建一个xml 的beanDefinition加载器 这个玩意里面持有一个beanFactory的引用 XmlBeanDefinitionReader beanDefinitionReader = new XmlBeanDefinitionReader(beanFactory); /*给beanDefinition 加载器设置上下文环境 资源加载器 实体解析器*/ /*这玩意我记得都是用的默认的*/ beanDefinitionReader.setEnvironment(this.getEnvironment()); beanDefinitionReader.setResourceLoader(this); beanDefinitionReader.setEntityResolver(new ResourceEntityResolver(this)); /*初始化beanDefinition加载器*/ initBeanDefinitionReader(beanDefinitionReader); /*使用beanDefinition加载器加载beanDefinitions*/ loadBeanDefinitions(beanDefinitionReader);&#125; 首先创建了一个bean定义信息的加载器 **XmlBeanDefinitionReader**，用来解析xml文件。​ 然后给**XmlBeanDefinitionReader**设置一些相关信息。​ 然后初始化 **XmlBeanDefinitionReader**。**initBeanDefinitionReader(beanDefinitionReader)**​ 最后使用**XmlBeanDefinitionReader**加载bean的定义信息。**loadBeanDefinitions(beanDefinitionReader)**\u0000 5.bean定义信息加载器**XmlBeanDefinitionReader**​ 123public XmlBeanDefinitionReader(BeanDefinitionRegistry registry) &#123; super(registry);&#125; 构造器里面传入了一个**BeanDefinitionRegistry**，给父类**AbstractBeanDefinitionReader**的属性赋值。​ ​ 6.Bean定义信息注册器**BeanDefinitionRegistry**​ 持有bean定义的注册表接口。例如_**RootBeanDefinition**__ 和 _**ChildBeanDefinition**_ 实例。_​ 通常由在内部使用_**AbstractBeanDefinition**_层次结构的**beanFactory**实现。​ 这是spring的bean工厂中唯一封装**BeanDefinitionRegistry**的接口。标准的**BeanFactory**接口仅仅涵盖对完全配置的工厂的工厂实例的访问。​ spring的**BeanDefinitionRegistry**期待作用于该接口的实现。spring中已经有的实现是_**DefaultListableBeanFactory**__ &amp; __**GenericApplicationContext**_。 1234567891011121314151617181920212223public interface BeanDefinitionRegistry extends AliasRegistry &#123; /*注册一个bean的定义信息，支持指定bean的名字*/ void registerBeanDefinition(String beanName, BeanDefinition beanDefinition) throws BeanDefinitionStoreException; /*根据bean的名字移除掉已经注册的beanDefinition*/ void removeBeanDefinition(String beanName) throws NoSuchBeanDefinitionException; /*根据名字查找BeanDefinition*/ BeanDefinition getBeanDefinition(String beanName) throws NoSuchBeanDefinitionException; /*判断是否包含给定名字的BeanDefinition信息*/ boolean containsBeanDefinition(String beanName); /*返回已经注册的BeanDefinition名字列表*/ String[] getBeanDefinitionNames(); /*返回已经注册的BeanDefinition数量*/ int getBeanDefinitionCount(); /*确定给定的bean名称是否已经在注册中心使用， 怎么判断是否使用呢？就是是否有别名注册在此名称下，或者已经有注册在这里的bean。*/ boolean isBeanNameInUse(String beanName);&#125; 7.初始化beanDefinition的加载器**initBeanDefinitionReader(beanDefinitionReader)**​ 123protected void initBeanDefinitionReader(XmlBeanDefinitionReader reader) &#123; reader.setValidating(this.validating);&#125; 初始化用于加载次上下文的beanDefinition的beanDefinitionReader。默认实现为空。可以再子类中覆盖，例如关闭xml验证或使用不同的_**XmlBeanDefinitionParser**_实现。​ 8.加载bean的定义信息**loadBeanDefinitions(beanDefinitionReader)**​ 使用给定的_**XmlBeanDefinitionReader**_加载**beanDefinition**。​ **BeanFactory**的生命周期由_**refreshBeanFactory() **_处理，因此此方法仅用于加载和注册**beanDefinition**。 123456789101112131415protected void loadBeanDefinitions(XmlBeanDefinitionReader reader) throws BeansException, IOException &#123; /*这里实际上是一个钩子方法，经典的模板模式，子类根据需要对方法进行重写，实际上加载xml的时候，这里锤子也没拿到*/ Resource[] configResources = getConfigResources(); /*如果资源不为空，走这里的逻辑，但是上面已经分析过，实际上锤子也没拿到，所以走下面的逻辑*/ if (configResources != null) &#123; reader.loadBeanDefinitions(configResources); &#125; /*获取配置文件位置*/ String[] configLocations = getConfigLocations(); /*此时读取到了我们在配置文件指定的配置文件 beans.xml*/ if (configLocations != null) &#123; reader.loadBeanDefinitions(configLocations); &#125;&#125; 在这里我们注意两个方法：​ **getConfigLocations()** 获取到配置文件的位置 **reader.loadBeanDefinitions(configLocations)** 使用bean定义信息的加载器加载**beanDefinition**。 ​ 1234@Nullableprotected String[] getConfigLocations() &#123; return (this.configLocations != null ? this.configLocations : getDefaultConfigLocations());&#125; 返回一个资源位置数组，指的是构建此上下文应用的时候使用的 xml配置文件。还可以包括位置模式，这将通过_**ResourcePatternResolver**_处理。默认实现返回null。子类可以重写这个方法用来提供一组资源位置来加载**beanDefinition**。​ 9.reader.loadBeanDefinitions(configLocations)用bean定义信息的加载器加载**beanDefinition**。\u0000 123456789101112@Overridepublic int loadBeanDefinitions(String... locations) throws BeanDefinitionStoreException &#123; /*断言 判空*/ Assert.notNull(locations, &quot;Location array must not be null&quot;); /*记录beanDefinition的数量*/ int count = 0; /*迭代加载beanDefinition*/ for (String location : locations) &#123; count += loadBeanDefinitions(location); &#125; return count;&#125; 这里是根据配置文件的位置数组进行循环迭代加载并记录**beanDefinition**的数量。​ 来到了**AbstractBeanDefinitionReader**的**loadBeanDefinitions()**。​ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public int loadBeanDefinitions(String location, @Nullable Set&lt;Resource&gt; actualResources) throws BeanDefinitionStoreException &#123; /*获取资源加载器*/ ResourceLoader resourceLoader = getResourceLoader(); /*如果资源加载器为空，抛异常*/ if (resourceLoader == null) &#123; throw new BeanDefinitionStoreException( &quot;Cannot load bean definitions from location [&quot; + location + &quot;]: no ResourceLoader available&quot;); &#125; /*如果资源加载器是资源模式解析器类型的*/ if (resourceLoader instanceof ResourcePatternResolver) &#123; // Resource pattern matching available. try &#123; /* 将xml配置文件加载到resources中，resource其实就是spring底层封装了很多的细节， 抽象出来的资源顶层接口 让开发人员不必专注于底层配置文件的加载细节 */ Resource[] resources = ((ResourcePatternResolver) resourceLoader).getResources(location); /*加载beanDefinition*/ int count = loadBeanDefinitions(resources); /*这玩意不知道是啥，反正是空，没啥锤子用*/ if (actualResources != null) &#123; Collections.addAll(actualResources, resources); &#125; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Loaded &quot; + count + &quot; bean definitions from location pattern [&quot; + location + &quot;]&quot;); &#125; return count; &#125; catch (IOException ex) &#123; throw new BeanDefinitionStoreException( &quot;Could not resolve bean definition resource pattern [&quot; + location + &quot;]&quot;, ex); &#125; &#125;/*走到这里说明资源加载器肯定不是资源模式解析器类型的*/ else &#123; // 只能通过绝对网址加载单个资源 Resource resource = resourceLoader.getResource(location); int count = loadBeanDefinitions(resource); if (actualResources != null) &#123; actualResources.add(resource); &#125; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Loaded &quot; + count + &quot; bean definitions from location [&quot; + location + &quot;]&quot;); &#125; return count; &#125;&#125; 首先获取到资源的加载器，如果加载器是空，那就说明程序无法往下执行了，直接抛异常。​ 如果资源加载器是资源解析器模式的，加载xml文件到**Resource**数组中，**Resource**是什么在上一篇中已经介绍过了，在此不再赘述。​ 此时在根据**Resource**数组去加载**beanDefinition**，最后返回加载的**beanDefinition**的数量。（注意：这里走的是else的逻辑，因为默认我们不是位置模式。）​ 注意这个时候思路已经很明确了，准备了这么多实际上到这里就分为了两步：​ 加载xml配置文件 **getResource(location)** 通过xml配置文件去加载**beanDefinition** **loadBeanDefinitions(resource)** ​ 10.加载xml配置文件来到**DefaultResourceLoader**。​ 12345678910111213141516171819202122232425262728293031@Overridepublic Resource getResource(String location) &#123; Assert.notNull(location, &quot;Location must not be null&quot;); //循环遍历使用解析器解析该location的资源，如果资源不为空，直接返回 for (ProtocolResolver protocolResolver : getProtocolResolvers()) &#123; Resource resource = protocolResolver.resolve(location, this); if (resource != null) &#123; return resource; &#125; &#125; /*以/开头那么根据path去寻找*/ if (location.startsWith(&quot;/&quot;)) &#123; return getResourceByPath(location); &#125; /*以classpath开头，那么抽象为ClassPathResource*/ else if (location.startsWith(CLASSPATH_URL_PREFIX)) &#123; return new ClassPathResource(location.substring(CLASSPATH_URL_PREFIX.length()), getClassLoader()); &#125; else &#123; try &#123; /*其他情况采用urlResource来加载*/ URL url = new URL(location); return (ResourceUtils.isFileURL(url) ? new FileUrlResource(url) : new UrlResource(url)); &#125; catch (MalformedURLException ex) &#123; // No URL -&gt; resolve as resource path. return getResourceByPath(location); &#125; &#125;&#125; 接下来就是加载**beanDefinition**信息。\u0000 11.加载beanDefinition**loadBeanDefinitions(resource)**​ 来到了 **XmlBeanDefinitionReader**。​ 123456@Overridepublic int loadBeanDefinitions(Resource resource) throws BeanDefinitionStoreException &#123; //1.将resource包装成带有编码格式的EncodedResource //2.重载调用loadBeanDefinitions() return loadBeanDefinitions(new EncodedResource(resource));&#125; 我们来到重载的方法。**loadBeanDefinitions(resource)**​ 1234567891011121314151617181920212223242526272829303132333435363738394041public int loadBeanDefinitions(EncodedResource encodedResource) throws BeanDefinitionStoreException &#123; //跳过断言 日志 Assert.notNull(encodedResource, &quot;EncodedResource must not be null&quot;); if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Loading XML bean definitions from &quot; + encodedResource); &#125; //获取引用：当前线程已经加载过的encodingResource资源 Set&lt;EncodedResource&gt; currentResources = this.resourcesCurrentlyBeingLoaded.get(); //将当前的encodingResource加入到threadlocal的set中，加入失败说明当前资源已经加载过了，不能重复加载，需要抛出异常 if (!currentResources.add(encodedResource)) &#123; throw new BeanDefinitionStoreException( &quot;Detected cyclic loading of &quot; + encodedResource + &quot; - check your import definitions!&quot;); &#125; //jdk新版本的语法糖 拿到资源的输入流 try (InputStream inputStream = encodedResource.getResource().getInputStream()) &#123; //因为接下来要使用 sax解析器，解析xml文件 ，所以需要将输入流包装成inputsource， //inputsource是sax中表示资源的对象 InputSource inputSource = new InputSource(inputStream); //设置字符编码 spring源码中判断逻辑特别多 ，稳定化的框架并不相信一切外部的输入 //这也是软件架构原则中的规范之一 稳定性体现 if (encodedResource.getEncoding() != null) &#123; inputSource.setEncoding(encodedResource.getEncoding()); &#125; //真正干活的逻辑 ，加载beanDefinition的入口 return doLoadBeanDefinitions(inputSource, encodedResource.getResource()); &#125; catch (IOException ex) &#123; throw new BeanDefinitionStoreException( &quot;IOException parsing XML document from &quot; + encodedResource.getResource(), ex); &#125; finally &#123; //因为resourcesCurrentlyBeingLoaded表示当前线程正在加载的redource //执行到这里说明资源已经加载完了或者失败了 //所以需要将当前资源移除出去 currentResources.remove(encodedResource); //set没有数据了，说明没啥乱用了，清理一下内存 防止threadlocal内存泄漏 if (currentResources.isEmpty()) &#123; this.resourcesCurrentlyBeingLoaded.remove(); &#125; &#125;&#125; 这里使用了一个委派模式，将真正干活的逻辑交给了**doLoadBeanDefinitions()**。\u0000 1234567891011121314151617181920protected int doLoadBeanDefinitions(InputSource inputSource, Resource resource) throws BeanDefinitionStoreException &#123; try &#123; //把 resource 转换成程序层面可以识别的有层次结构的document对象 Document doc = doLoadDocument(inputSource, resource); //解析文档对象，生成beanDefinition注册到beanFactory中，最终返回新注册到beanFactory的beanDefinition数量 int count = registerBeanDefinitions(doc, resource); //日志打印 if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Loaded &quot; + count + &quot; bean definitions from &quot; + resource); &#125; //返回新注册 bean定义信息的数量 return count; &#125; catch (BeanDefinitionStoreException ex) &#123; throw ex; &#125; //这里省略部分catch的逻辑&#125; 这里再次分成了两步：​ 将**Resource**转换成**Document**。 **doLoadDocument(inputSource, resource)** 解析文档对象，生成**beanDefinition**注册到**BeanFactory**。 **registerBeanDefinitions(doc, resource)** ​ 12.Resource转化成Document**doLoadDocument(inputSource, resource)**​ 1234567891011121314protected Document doLoadDocument(InputSource inputSource, Resource resource) throws Exception &#123; //这个方法就是将inputSource转化成可以识别的文档对象 //通过文档加载器来转化的 //1. getEntityResolver？ 这个实体解析器 //spring官网说明：如果sax应用程序中需要实现自定义处理外部实体，则必须实现此接口并使用setEntityResolver方法向sax驱动器注册一个实例 //也就是说，对于解析一个xml，sax首先读取xml文档上的声明，根据声明去寻找相应的DTD/XSD定义，以便对文档进行一个校验。 //默认的寻找校验规则，即通过网络来下载响应的DTD/XSD声明，在进行校验。并且下载的过程是一个漫长且不可控的过程，当下在失败后，这里还会抛出异常 //那么有什么办法可以避免直接从网络上下载呢？使用EntityResolver //EntityResolver的作用是项目本身可以提供一个如何寻找DTD/XSD声明的方法，即让程序来实现寻找定义声明的过程，比如我们将定义文件 //放到项目的某个地方，在实现时直接将此文件读取并返回给sax即可，这样避免了通过网络来寻找对应的声明。 //2.验证模式是怎么获取的？getValidationModeForResource() return this.documentLoader.loadDocument(inputSource, getEntityResolver(), this.errorHandler, getValidationModeForResource(resource), isNamespaceAware());&#125; **getValidationModeForResource(resource)**​ 123456789101112131415161718protected int getValidationModeForResource(Resource resource) &#123; //获取默认的validationMode int validationModeToUse = getValidationMode(); //条件成立：说明set过默认值，一般情况下，不会走这里，都是使用自动检测 if (validationModeToUse != VALIDATION_AUTO) &#123; return validationModeToUse; &#125; //自动检查xml使用的是哪种验证模式？由这个方法决定 int detectedMode = detectValidationMode(resource); if (detectedMode != VALIDATION_AUTO) &#123; return detectedMode; &#125; // Hmm, we didn&#x27;t get a clear indication... Let&#x27;s assume XSD, // since apparently no DTD declaration has been found up until // detection stopped (before finding the document&#x27;s root tag). return VALIDATION_XSD;&#125; **documentLoader.loadDocument()**​ 1234567891011@Overridepublic Document loadDocument(InputSource inputSource, EntityResolver entityResolver, ErrorHandler errorHandler, int validationMode, boolean namespaceAware) throws Exception &#123; DocumentBuilderFactory factory = createDocumentBuilderFactory(validationMode, namespaceAware); if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Using JAXP provider [&quot; + factory.getClass().getName() + &quot;]&quot;); &#125; DocumentBuilder builder = createDocumentBuilder(factory, entityResolver, errorHandler); return builder.parse(inputSource);&#125; **DocumentBuilder.parse() **就是真正的解析逻辑。​ 13.解析文档对象注册到BeanFactory**registerBeanDefinitions()** 解析文档对象，生成beanDefinition注册到beanFactory中，最终返回新注册到beanFactory的beanDefinition数量。\u0000 123456789101112public int registerBeanDefinitions(Document doc, Resource resource) throws BeanDefinitionStoreException &#123; //创建一个 BeanDefinitionDocumentReader 一对一处理 每个文档对象都会创建一个 BeanDefinitionDocumentReader 对象 BeanDefinitionDocumentReader documentReader = createBeanDefinitionDocumentReader(); //getRegistry 会返回程序创建的beanFactory实例 //countBefore 解析doc之前，bf中已经有的bd数量 int countBefore = getRegistry().getBeanDefinitionCount(); //解析文档，并且注册到bf中 //xmlReaderContext :包含最主要的参数是当前 this -&gt; xmlBeanDefinitionReader -&gt; bf documentReader.registerBeanDefinitions(doc, createReaderContext(resource)); //返回值 返回新注册的bd数量 最新的 - 注册之前的 return getRegistry().getBeanDefinitionCount() - countBefore;&#125; 看重载的方法​ 12345678@Overridepublic void registerBeanDefinitions(Document doc, XmlReaderContext readerContext) &#123; //引用上下文对象 this.readerContext = readerContext; //doc.getDocumentElement() 拿出文档代表的xml的顶层标签 &lt;beans&gt;&lt;/beans&gt; /*真正的解析xml的逻辑*/ doRegisterBeanDefinitions(doc.getDocumentElement());&#125; 又是一个委派模式，终于来到了解析xml的逻辑，这里的逻辑其实没有什么可以学习的点，追到这里主要是为了串起来整个解析xml文件的流程，具体解析xml的过程不做重点说明。​ 1234567891011121314151617181920212223242526272829303132333435protected void doRegisterBeanDefinitions(Element root) &#123; BeanDefinitionParserDelegate parent = this.delegate; //方法返回一个beans标签 解析器对象 this.delegate = createDelegate(getReaderContext(), root, parent); //解析器对象去判断是不是默认的命名空间 一般情况下 条件成立 if (this.delegate.isDefaultNamespace(root)) &#123; //获取 profile 属性， 环境 Dev prod test String profileSpec = root.getAttribute(PROFILE_ATTRIBUTE); //条件成立 说明 beans 标签上 有 profile 属性 if (StringUtils.hasText(profileSpec)) &#123; //将属性值按照,拆分成字符串数组 String[] specifiedProfiles = StringUtils.tokenizeToStringArray( profileSpec, BeanDefinitionParserDelegate.MULTI_VALUE_ATTRIBUTE_DELIMITERS); // We cannot use Profiles.of(...) since profile expressions are not supported // in XML config. See SPR-12458 for details. // Environment.acceptsProfiles(String [] args) 条件成立 ：说明beans 标签可以继续解析 bd //这里取反 ，所以 就是 if里面整个条件成立 ，说明该 beans 标签不在继续解析 ，直接返回 if (!getReaderContext().getEnvironment().acceptsProfiles(specifiedProfiles)) &#123; if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Skipped XML bean definition file due to specified profiles [&quot; + profileSpec + &quot;] not matching: &quot; + getReaderContext().getResource()); &#125; return; &#125; &#125; &#125; //这里是留给子类的扩展点 preProcessXml(root); parseBeanDefinitions(root, this.delegate); //这里也是留给子类扩展 体现的软件设计模式的开闭原则 postProcessXml(root); this.delegate = parent;&#125; 已经走到了这里，大概看一下里面的逻辑。​ **parseBeanDefinitions(root, this.delegate)**​ 12345678910111213141516171819202122232425protected void parseBeanDefinitions(Element root, BeanDefinitionParserDelegate delegate) &#123; /*条件成立说明root是spring缺省的命名空间*/ if (delegate.isDefaultNamespace(root)) &#123; /*这里获取的，大部分情况下，其实都是bean标签*/ NodeList nl = root.getChildNodes(); /*迭代处理每一个子标签*/ for (int i = 0; i &lt; nl.getLength(); i++) &#123; Node node = nl.item(i); if (node instanceof Element) &#123; Element ele = (Element) node; /*说明子标签也是默认的spring标签*/ if (delegate.isDefaultNamespace(ele)) &#123; /*默认标签解析逻辑 step into*/ parseDefaultElement(ele, delegate); &#125;/*自定义标签解析逻辑*/ else &#123; delegate.parseCustomElement(ele); &#125; &#125; &#125; &#125;/*root不是默认的命名空间，解析自定义标签逻辑*/ else &#123; delegate.parseCustomElement(root); &#125;&#125; 因为我们没有自定义标签，所以看默认标签的解析逻辑 **parseDefaultElement(ele, delegate)**。​ 1234567891011121314151617private void parseDefaultElement(Element ele, BeanDefinitionParserDelegate delegate) &#123; /*条件成立，说明此时是import标签*/ if (delegate.nodeNameEquals(ele, IMPORT_ELEMENT)) &#123; importBeanDefinitionResource(ele); &#125;/*条件成立说明是alias别名标签*/ else if (delegate.nodeNameEquals(ele, ALIAS_ELEMENT)) &#123; processAliasRegistration(ele); &#125;/*此时说明是bean标签*/ else if (delegate.nodeNameEquals(ele, BEAN_ELEMENT)) &#123; /*解析bean标签*/ processBeanDefinition(ele, delegate); &#125;/*说明是嵌套beans标签*/ else if (delegate.nodeNameEquals(ele, NESTED_BEANS_ELEMENT)) &#123; // 递归到上层重新来了 doRegisterBeanDefinitions(ele); &#125;&#125; \u0000继续看一下bean标签的解析逻辑**processBeanDefinition(ele, delegate)**。 1234567891011121314151617181920protected void processBeanDefinition(Element ele, BeanDefinitionParserDelegate delegate) &#123; /*用解析器对象解析标签 hodler里面包含三个属性：beanDefinition，beanName，Alias别名信息*/ BeanDefinitionHolder bdHolder = delegate.parseBeanDefinitionElement(ele); if (bdHolder != null) &#123; /*如果当前hodler需要被装饰，执行装饰逻辑 主要是处理自定义属性*/ bdHolder = delegate.decorateBeanDefinitionIfRequired(ele, bdHolder); try &#123; // Register the final decorated instance. /*注册当前bean倒容器中 通过readerContext拿到XMLBeanDefinition拿到beanFactory*/ BeanDefinitionReaderUtils.registerBeanDefinition(bdHolder, getReaderContext().getRegistry()); &#125; catch (BeanDefinitionStoreException ex) &#123; getReaderContext().error(&quot;Failed to register bean definition with name &#x27;&quot; + bdHolder.getBeanName() + &quot;&#x27;&quot;, ele, ex); &#125; // Send registration event. /*发送一个bean注册完成的事件*/ getReaderContext().fireComponentRegistered(new BeanComponentDefinition(bdHolder)); &#125;&#125; **BeanDefinitionReaderUtils.registerBeanDefinition()** 单实例bean的注册逻辑。​ 12345678910111213141516public static void registerBeanDefinition( BeanDefinitionHolder definitionHolder, BeanDefinitionRegistry registry) throws BeanDefinitionStoreException &#123; // Register bean definition under primary name. String beanName = definitionHolder.getBeanName(); registry.registerBeanDefinition(beanName, definitionHolder.getBeanDefinition()); // Register aliases for bean name, if any. String[] aliases = definitionHolder.getAliases(); if (aliases != null) &#123; for (String alias : aliases) &#123; registry.registerAlias(beanName, alias); &#125; &#125;&#125; 最终将**beanDefinition**放到了**beanDefinitionMap**里面，至此整个解析xml配置文件，加载**beanDefinition**并返回全新**BeanFactory**的逻辑结束了。​ ​ ​ ​","categories":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/tags/Spring/"}]},{"title":"Spring[二]IOC容器初始化","slug":"Spring/Spring[二]Ioc容器的初始化","date":"2022-01-11T05:58:52.126Z","updated":"2022-01-11T06:06:02.565Z","comments":true,"path":"2022/01/11/Spring/Spring[二]Ioc容器的初始化/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/Spring/Spring[%E4%BA%8C]Ioc%E5%AE%B9%E5%99%A8%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96/","excerpt":"","text":"从上层视角入手，观测Spring容器的初始化工作。 1.初始准备首先在Spring的源码工程里面创建一段代码，通过简单的运行代码，来追踪Spring ioc 容器的整个启动流程。 12345678910111213141516171819202122232425262728293031public class IocMainTest &#123; private static final String CONFIG_LOCATION = &quot;applicationContext.xml&quot;; public static void main(String[] args) &#123; //自定义 ioc 容器进行扩展 testDiyIoc(); &#125; private static void testDiyIoc() &#123; MyClassPathXmlApplicationContext ioc = new MyClassPathXmlApplicationContext(CONFIG_LOCATION); Person person = ioc.getBean(&quot;person&quot;, Person.class); System.out.println(&quot;person = &quot; + person); &#125; &#125;public class MyClassPathXmlApplicationContext extends ClassPathXmlApplicationContext &#123; public MyClassPathXmlApplicationContext(String...configLocations)&#123; super(configLocations); &#125; @Override protected void initPropertySources()&#123; System.out.println(&quot;这是我自己定义的ioc容器&quot;); //getEnvironment().setRequiredProperties(&quot;ES_HOME&quot;); &#125;&#125; 程序启动之后，首先会进入自定义的ioc容器，然后通过构造器显式调用**super()**方法来到**ClassPathXmlApplicationContext**。 1234567891011121314public ClassPathXmlApplicationContext( String[] configLocations, boolean refresh, @Nullable ApplicationContext parent) throws BeansException &#123; //调用父类的构造方法，AbstractApplicationContext /*模板方法模式 和 钩子方法 易于扩展 开闭原则*/ super(parent); //解析配置文件路径 setConfigLocations(configLocations); /*refresh 是表示一个容器是否刷新过得标识符，如果容器还没有刷新过就进行容器刷新，实际上这里是一个双端检测锁*/ if (refresh) &#123; /*spring ioc 容器刷新方法*/ refresh(); &#125;&#125; 这里面主要是做了两件事：​ 解析配置文件路径 **setConfigLocations()** 判断容器是否刷新过，如果尚未刷新，那么刷新ioc容器 **refresh()** ​ 2.解析配置文件1234567891011121314151617public void setConfigLocations(@Nullable String... locations) &#123; /*如果location！=null 也就是说配置文件不为空，spring扫描到了配置文件，那么我们就可以以xml的形式启动spring容器*/ if (locations != null) &#123; /*断言：配置文件所在位置不能为空*/ Assert.noNullElements(locations, &quot;Config locations must not be null&quot;); /*引用*/ this.configLocations = new String[locations.length]; /*迭代解析路径*/ for (int i = 0; i &lt; locations.length; i++) &#123; /*解析路径*/ this.configLocations[i] = resolvePath(locations[i]).trim(); &#125; &#125; else &#123; this.configLocations = null; &#125;&#125; 如果我们程序是有xml的spring配置文件，那么该文件被扫描到以后，（当然可能不止一个)，回去迭代解析配置文件的路径。 **resolvePath()**​ 123protected String resolvePath(String path) &#123; return getEnvironment().resolveRequiredPlaceholders(path);&#125; 解析给定的路径，如果有必要的话，用相应的环境属性值替换占位符，应用于配置位置。**resolveRequiredPlaceholders()**​ 12345@Overridepublic String resolveRequiredPlaceholders(String text) throws IllegalArgumentException &#123; /*占位符解析器解析并替换占位符*/ return this.propertyResolver.resolveRequiredPlaceholders(text);&#125; 这里其实是将解析的逻辑交给了属性解析器。**resolveRequiredPlaceholders()** 123456789@Overridepublic String resolveRequiredPlaceholders(String text) throws IllegalArgumentException &#123; if (this.strictHelper == null) &#123; /*如果占位符解析器为空，就创建一个占位符解析器*/ this.strictHelper = createPlaceholderHelper(false); &#125; /*真正去解析占位符的操作*/ return doResolvePlaceholders(text, this.strictHelper);&#125; 在这个抽象的解析器里面又通过委派模式委派给了真正的解析占位符操作的方法。 **doResolvePlaceholders()**​ 这里为什么要这么设计？这其实是两个扩展点，开发人员可以自己继承这两个抽象类，重写对应的方法，来实现定制化的解析占位符的逻辑。​ 1234private String doResolvePlaceholders(String text, PropertyPlaceholderHelper helper) &#123; /*使用传入的属性占位符解析器去解析并替换占位符*/ return helper.replacePlaceholders(text, this::getPropertyAsRawString);&#125; 这里交给了真正的属性解析器**PropertyPlaceholderHelper**去解析属性占位符。​ 12345public String replacePlaceholders(String value, PlaceholderResolver placeholderResolver) &#123; Assert.notNull(value, &quot;&#x27;value&#x27; must not be null&quot;); /*解析字符串类型的值*/ return parseStringValue(value, placeholderResolver, null);&#125; 将所有格式的**$&#123;name&#125;**占位符替换为从提供的**PropertyPlaceHolderHelper**，**PlaceHolderResolver**返回的值。​ 再往下就是具体的解析字符串的逻辑，没有什么继续的必要。​ 这里面主要体现的就是利用**抽象类** 和 **委派模式** 提供了默认的属性解析器实现，当然我们也可以提供自己的解析器，只需要继承相应的抽象类，实现相应的抽象方法。​ 3.容器刷新解析完配置文件路径以后，就是判断容器是否已经刷新过，如果尚未刷新，就会进行容器的刷新动作。 **AbstractApplicationContext.refresh()**​ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677@Overridepublic void refresh() throws BeansException, IllegalStateException &#123; //加锁是因为ioc容器只能有一个，防止重复创建 synchronized (this.startupShutdownMonitor) &#123; //准备开始容器刷新 StartupStep contextRefresh = this.applicationStartup.start(&quot;spring.context.refresh&quot;); // 获取当前系统的时间，给容器设置同步锁标识 prepareRefresh(); // 解析xml配置文件或注解 bean定义资源的载入，获取一个全新的bean工厂 ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // 给容器中注册一些组件，添加切面，类加载器，表达式解析器，注解解析器，事件处理器 prepareBeanFactory(beanFactory); try &#123; //为容器的某些子类指定后置处理器，子类可以通过重写这个方法， // 在bean工厂创建并预备完成以后做进一步的设置 /*可以在bd创建出实例之前，对bd信息做进一步的修改*/ postProcessBeanFactory(beanFactory); StartupStep beanPostProcess = this.applicationStartup.start(&quot;spring.context.beans.post-process&quot;); /** * 执行bean工厂的后置处理器，先执行bean定义注册后置处理器，在执行bean工厂的后置处理器。 * 执行顺序：先执行实现了优先级接口的，在执行带有order的，最后执行其他的。 */ invokeBeanFactoryPostProcessors(beanFactory); // 获取所有的后置处理器，先注册带优先级的，在注册带order的， // 在注册剩下的。注册一个ApplicationListenerDetector， // 在bean完成创建后检查是是否是ApplicationListener，如果是，添加到容器。 registerBeanPostProcessors(beanFactory); beanPostProcess.end(); // 做一些国际化相关的操作 initMessageSource(); // 初始化事件派发器：获取beanFactory， // 从beanFactory获取事件派发器，如果没有，创建一个放到容器中。 initApplicationEventMulticaster(); // 留给子容器，子容器可以再容器刷新的时候加入自己的逻辑。 onRefresh(); // 为事件派发器注册事件监听器，派发一些早期事件。 registerListeners(); // 初始化剩下的所有的非懒加载的单实例bean。 finishBeanFactoryInitialization(beanFactory); // 初始化容器的生命周期事件处理器， // 回调onRefresh()，并发布容器的生命周期事件。 finishRefresh(); &#125; catch (BeansException ex) &#123; if (logger.isWarnEnabled()) &#123; logger.warn(&quot;Exception encountered during context initialization - &quot; + &quot;cancelling refresh attempt: &quot; + ex); &#125; // Destroy already created singletons to avoid dangling resources. destroyBeans(); // Reset &#x27;active&#x27; flag. cancelRefresh(ex); // Propagate exception to caller. throw ex; &#125; finally &#123; // Reset common introspection caches in Spring&#x27;s core, since we // might not ever need metadata for singleton beans anymore... resetCommonCaches(); contextRefresh.end(); &#125; &#125;&#125; ​ 这个方法很长，也可以说是ioc容器的核心，这里面涉及到的关于Spring的额外的知识也比较繁多，在此会对相关内容进行一一列举。 4.属性资源设置与校验​ 首先会去获取当前系统的时间，给容器设置同步锁标识。 **prepareRefresh()**​ 12345678910111213141516171819202122232425262728293031323334353637protected void prepareRefresh() &#123; // 获取当前系统的时间 this.startupDate = System.currentTimeMillis(); //关闭状态设置为false this.closed.set(false); //激活状态设置为true this.active.set(true); //日志打印 if (logger.isDebugEnabled()) &#123; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Refreshing &quot; + this); &#125; else &#123; logger.debug(&quot;Refreshing &quot; + getDisplayName()); &#125; &#125; // 留给子类扩展：可以做一些资源初始化 initPropertySources(); // 检验必须有环境变量 ，结合上面的方法可以设置哪些属性是必填的 /*demo：MyClassPathXmlApplicationContext*/ getEnvironment().validateRequiredProperties(); // 获取容器早期的事件监听器 if (this.earlyApplicationListeners == null) &#123; this.earlyApplicationListeners = new LinkedHashSet&lt;&gt;(this.applicationListeners); &#125; else &#123; // 如果已经有了，就先清空 在push this.applicationListeners.clear(); this.applicationListeners.addAll(this.earlyApplicationListeners); &#125; // 允许这些早期事件在容器刷新之后发布出去 this.earlyApplicationEvents = new LinkedHashSet&lt;&gt;();&#125; \u0000这里面可以关注一下几个扩展点：​ 可以再子类做一些资源初始化的方法 **initPropertySources()** 对上一步设置的属性变量进行校验 **validateRequiredProperties()** ​ 随后会获取到容器早期的一些事件监听器，这些早期事件会在容器刷新成功之后发布出去。​ 5.解析配置文件这一步主要的操作就是解析Spring的配置文件或者注解标识的配置类，加载bean的定义信息 **BeanDefinition**，简称bd。最后创建一个**BeanFactory**，简称bf。​ **obtainFreshBeanFactory()**\u0000 123456protected ConfigurableListableBeanFactory obtainFreshBeanFactory() &#123; //刷新bean工厂 refreshBeanFactory(); /**返回bean工厂*/ return getBeanFactory();&#125; 这里面主要的逻辑就在于bean工厂的刷新。 **refreshBeanFactory()**​ 1234567891011121314151617181920212223242526@Overrideprotected final void refreshBeanFactory() throws BeansException &#123; /*如果已经有了bean工厂，通常情况下并不会，什么情况下会有？通过applicationContext直接调用refresh方法*/ if (hasBeanFactory()) &#123; /*销毁里面的所有bean*/ destroyBeans(); /*关闭bean工厂*/ closeBeanFactory(); &#125; try &#123; //不管上面是否已经有bean工厂存在，最终都会走到这里，去创建一个bean工厂 DefaultListableBeanFactory beanFactory = createBeanFactory(); /*设置序列化id*/ beanFactory.setSerializationId(getId()); /*对工厂进行一些定制化设置*/ customizeBeanFactory(beanFactory); /*加载bean的定义信息*/ loadBeanDefinitions(beanFactory); /*将当前类的bean工厂引用指向创建的bean工厂*/ this.beanFactory = beanFactory; &#125; catch (IOException ex) &#123; throw new ApplicationContextException(&quot;I/O error parsing bean definition source for &quot; + getDisplayName(), ex); &#125;&#125; 这里可以关注两点：​ 如何创建一个bean工厂 **createBeanFactory()** 加载bd的信息 **loadBeanDefinitions(beanFactory)** ​ 5.1 创建bean工厂123protected DefaultListableBeanFactory createBeanFactory() &#123; return new DefaultListableBeanFactory(getInternalParentBeanFactory());&#125; 为当前的容器上下文创建了一个内部bean工厂，每次刷新的时候会尝试调用当前方法。默认是创建了一个_**DefaultListableBeanFactory**_，并且将此上下文父级的内部bean作为父 bean工厂。可以再子类中覆盖，例如自定义_**DefaultListableBeanFactory**_的设置。​ 5.2 BeanFactory上面提到了一个**DefaultListableBeanFactory**，那么都有哪些bean工厂呢？为什么有这么多bean工厂呢？​ 来到**BeanFactory**的顶层接口，**BeanFactory** 作为顶层接口，提供了一些列操作工厂内对象的方法，有一点需要注意的。​ 123456/** * * 对FactoryBean的转义定义，因为如果使用bean的名字检索 FactoryBean 得到的对象 * 是工厂生成的对象，如果想要得到工厂本身，需要转义。 */String FACTORY_BEAN_PREFIX = &quot;&amp;&quot;; \u0000这是决定你获取当前工厂还是当前工厂内对象的一个重要标识符号，切记。**&amp;**​ 接下来再去看 **BeanFactory**的继承关系，作为顶层接口，我们自然是从上往下看。​ \u0000这里面列举了几个重要的Bean工厂，同时也可以看到，**DefaultListableBeanFactory**本身其实是bean工厂的重要实现类。​ 从图上的继承关系可以看出：**ListableBeanFactory**、**HierarchicalBeanFactory** 和 **AutowireCapableBeanFactory**。​ 为什么要定义这么多层次的接口呢？​ 主要是为了区分在Spring内部操作过程中对象的传递和转化过程，对对象的数据访问所做的一些限制。这三个接口共同定义了 Bean的集合，Bean之间关系，Bean行为。​ 在 **BeanFactory** 里只对 IOC 容器的基本行为作了定义，根本不关心你的 Bean 是如何定义怎样加载的。正如我们只关心工厂里得到什么的产品对象，至于工厂是怎么生产这些对象的，这个基本的接口不关心。要知道工厂是如何产生对象的，需要看具体的 IOC 容器实现，Spring 提供了许多 IOC 容器的 实 现 。 **ApplicationContext**是Spring提供的一个高级的ioc容器，他除了能够提供ioc容器的基本功能以外，还未用户提供了一些附加的服务：​ 支持信息源，可以实现国际化。 **MessageSource** 访问资源 **ResourcePatternResolver** 支持应用事件 **ApplicationEventPublisher** ​ 容器本身其实也是扩展点，支持我们通过类的继承来做一些定制化实现。​ 5.3加载BeanDefinition**loadBeanDefinitions(beanFactory)**在**AbstractApplicationContext**其实也是一个空方法，交给子类去实现。找到具体的实现**AbstractXmlApplicationContext**。​ 12345678910111213141516@Overrideprotected void loadBeanDefinitions(DefaultListableBeanFactory beanFactory) throws BeansException, IOException &#123; // 创建一个xml 的beanDefinition加载器 这个玩意里面持有一个beanFactory的引用 XmlBeanDefinitionReader beanDefinitionReader = new XmlBeanDefinitionReader(beanFactory); /*给beanDefinition 加载器设置上下文环境 资源加载器 实体解析器*/ /*这玩意我记得都是用的默认的*/ beanDefinitionReader.setEnvironment(this.getEnvironment()); beanDefinitionReader.setResourceLoader(this); beanDefinitionReader.setEntityResolver(new ResourceEntityResolver(this)); /*初始化beanDefinition加载器*/ initBeanDefinitionReader(beanDefinitionReader); /*使用beanDefinition加载器加载beanDefinitions*/ loadBeanDefinitions(beanDefinitionReader);&#125; 这里面需要关注两个方法： 初始化**beanDefinition**加载器 **initBeanDefinitionReader()** 使用**beanDefinition**加载器加载**beanDefinitions** **loadBeanDefinitions()** ​ 不过在分析这两个方法之前，先明确一下**BeanDefinition**。​ 5.4BeanDefinition什么是BeanDefinition？​ **BeanDefinition**作为定义Spring Bean 文件中bean的接口，可以说是bean的抽象的数据结构，它包括属性参数，构造器参数，以及其他具体的参数。​ 继承关系​ **BeanDefinition**继承了**AttributeAccessor**和**BeanMetaDataElement**接口，拥有了对元数据访问的功能。​ 看一下具体的代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107public interface BeanDefinition extends AttributeAccessor, BeanMetadataElement &#123; /*单例的作用域*/ String SCOPE_SINGLETON = ConfigurableBeanFactory.SCOPE_SINGLETON; /*多例作用域*/ String SCOPE_PROTOTYPE = ConfigurableBeanFactory.SCOPE_PROTOTYPE; /*用户*/ int ROLE_APPLICATION = 0; /*某些复杂的配置*/ int ROLE_SUPPORT = 1; /*完全内部使用*/ int ROLE_INFRASTRUCTURE = 2; /*设置父类的名字*/ void setParentName(@Nullable String parentName); /*获取父类的名字*/ @Nullable String getParentName(); /*设置class类型名*/ void setBeanClassName(@Nullable String beanClassName); /*返回当前bean的名字（并不是准确的名字，有些childBeanDefinition是继承自父bean的名字）*/ /*所以不能依靠该名字确定class的类型*/ @Nullable String getBeanClassName(); /*设置作用域*/ void setScope(@Nullable String scope); /*获取作用域*/ @Nullable String getScope(); /*设置懒加载*/ void setLazyInit(boolean lazyInit); /*判断是否懒加载*/ boolean isLazyInit(); /*设置依赖的bean*/ void setDependsOn(@Nullable String... dependsOn); /*依赖的bean*/ @Nullable String[] getDependsOn(); /*设置优先注入其他bean*/ void setAutowireCandidate(boolean autowireCandidate); /*是否优先注入其他bean*/ boolean isAutowireCandidate(); /*设置优先自动装配*/ void setPrimary(boolean primary); /*这个bean是否优先自动装配*/ boolean isPrimary(); /*设置工厂bean的名字*/ void setFactoryBeanName(@Nullable String factoryBeanName); /*返回工厂bean的名字*/ @Nullable String getFactoryBeanName(); /*设置工厂方法名*/ void setFactoryMethodName(@Nullable String factoryMethodName); /*返回工厂方法名*/ @Nullable String getFactoryMethodName(); /*获取构造函数的值*/ ConstructorArgumentValues getConstructorArgumentValues(); /*判断是否有构造器参数*/ default boolean hasConstructorArgumentValues() &#123; return !getConstructorArgumentValues().isEmpty(); &#125; /*获取参数的值*/ MutablePropertyValues getPropertyValues(); /*判断是否有属性参数*/ default boolean hasPropertyValues() &#123; return !getPropertyValues().isEmpty(); &#125; /*设置初始化方法*/ void setInitMethodName(@Nullable String initMethodName); /*获取初始化方法的名字*/ @Nullable String getInitMethodName(); /*设置销毁方法的名字*/ void setDestroyMethodName(@Nullable String destroyMethodName); /*获取销毁方法的名字*/ @Nullable String getDestroyMethodName(); /*设置角色*/ void setRole(int role); /*获取角色*/ int getRole(); /*设置描述信息*/ void setDescription(@Nullable String description); /*获取描述信息*/ @Nullable String getDescription(); // Read-only attributes ResolvableType getResolvableType(); /*是否是单例的*/ boolean isSingleton(); /*是否是多例的*/ boolean isPrototype(); /*是否是抽象bean*/ boolean isAbstract(); /*获取资源描述*/ @Nullable String getResourceDescription(); /*有些beanDeFInition会使用beanDefinitionResource进行包装，将beanDefinition描述为一个资源*/ @Nullable BeanDefinition getOriginatingBeanDefinition();&#125; 具体实现​ **RootBeanDefinition**、**GenericBeanDefinition**、**ChildBeanDefinition** **RootBeanDefinition**是最常用的实现类，它对应一般性的元素标签，**GenericBeanDefinition**是自2.5以后新加入的bean文件配置属性定义类，是一站式服务类。在配置文件中可以定义父和子，父用**RootBeanDefinition**表示，而子用**ChildBeanDefiniton**表示，而没有父的就使用**RootBeanDefinition**表示。 **AnnotatedGenericBeanDefinition** 以**@Configuration**注解标记的会解析为**AnnotatedGenericBeanDefinition**。 **ConfigurationClassBeanDefinition** 以**@Bean**注解标记的会解析为**ConfigurationClassBeanDefinition**。 **ScannedGenericBeanDefinition** 以**@Component**注解标记的会解析为**ScannedGenericBeanDefinition**。​ ​ 总结一下：如果把ioc容器看成是一个飞机场，那么里面的bean对象就是一个个的飞机，bd则是对应的造飞机用的图纸，我们首先要拿到图纸，根据图纸才能造飞机。​ 5.5 初始化BeanDefinition加载器**initBeanDefinitionReader()** 123protected void initBeanDefinitionReader(XmlBeanDefinitionReader reader) &#123; reader.setValidating(this.validating);&#125; 初始化加载此上下文的bean定义信息的bean定义读取器，默认实现为空。可以在子类中覆盖，例如关闭xml验证或者使用不同的_**XmlBeanDefinitionParser**__ 实现。_​ 5.6 加载bean定义信息**loadBeanDefinitions()\u0000** 1234567891011121314protected void loadBeanDefinitions(XmlBeanDefinitionReader reader) throws BeansException, IOException &#123; /*这里实际上是一个钩子方法，经典的模板模式，子类根据需要对方法进行重写，实际上加载xml的时候，这里锤子也没拿到*/ Resource[] configResources = getConfigResources(); /*如果资源不为空，走这里的逻辑，但是上面已经分析过，实际上锤子也没拿到，所以走下面的逻辑*/ if (configResources != null) &#123; reader.loadBeanDefinitions(configResources); &#125; /*获取配置文件位置*/ String[] configLocations = getConfigLocations(); /*此时读取到了我们在配置文件指定的配置文件 beans.xml*/ if (configLocations != null) &#123; reader.loadBeanDefinitions(configLocations); &#125;&#125; 1234567891011121314151617public int loadBeanDefinitions(String... locations) throws BeanDefinitionStoreException &#123; /*断言 判空*/ Assert.notNull(locations, &quot;Location array must not be null&quot;); /*记录beanDefinition的数量*/ int count = 0; /*迭代加载beanDefinition*/ for (String location : locations) &#123; count += loadBeanDefinitions(location); &#125; return count;&#125;@Overridepublic int loadBeanDefinitions(String location) throws BeanDefinitionStoreException &#123; /*方法重载*/ return loadBeanDefinitions(location, null);&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public int loadBeanDefinitions(String location, @Nullable Set&lt;Resource&gt; actualResources) throws BeanDefinitionStoreException &#123; /*获取资源加载器*/ ResourceLoader resourceLoader = getResourceLoader(); /*如果资源加载器为空，抛异常*/ if (resourceLoader == null) &#123; throw new BeanDefinitionStoreException( &quot;Cannot load bean definitions from location [&quot; + location + &quot;]: no ResourceLoader available&quot;); &#125; /*如果资源加载器是资源模式解析器类型的*/ if (resourceLoader instanceof ResourcePatternResolver) &#123; // Resource pattern matching available. try &#123; /* 将xml配置文件加载到resources中，resource其实就是spring底层封装了很多的细节， 抽象出来的资源顶层接口 让开发人员不必专注于底层配置文件的加载细节 */ Resource[] resources = ((ResourcePatternResolver) resourceLoader).getResources(location); /*加载beanDefinition*/ int count = loadBeanDefinitions(resources); /*这玩意不知道是啥，反正是空，没啥锤子用*/ if (actualResources != null) &#123; Collections.addAll(actualResources, resources); &#125; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Loaded &quot; + count + &quot; bean definitions from location pattern [&quot; + location + &quot;]&quot;); &#125; return count; &#125; catch (IOException ex) &#123; throw new BeanDefinitionStoreException( &quot;Could not resolve bean definition resource pattern [&quot; + location + &quot;]&quot;, ex); &#125; &#125;/*走到这里说明资源加载器肯定不是资源模式解析器类型的*/ else &#123; // 只能通过绝对网址加载单个资源 Resource resource = resourceLoader.getResource(location); int count = loadBeanDefinitions(resource); if (actualResources != null) &#123; actualResources.add(resource); &#125; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Loaded &quot; + count + &quot; bean definitions from location [&quot; + location + &quot;]&quot;); &#125; return count; &#125;&#125; 1234567891011@Overridepublic int loadBeanDefinitions(Resource... resources) throws BeanDefinitionStoreException &#123; /*断言*/ Assert.notNull(resources, &quot;Resource array must not be null&quot;); int count = 0; /*迭代遍历加载*/ for (Resource resource : resources) &#123; count += loadBeanDefinitions(resource); &#125; return count;&#125; 最终将加载到的所有**beanDefinition**信息注册到**BeanDefinitionRegistry**。这个**BeanDefinitionRegistry**被**AbstractBeanDefinitionReader**持有。 5.7 Resource这里面涉及到了一个对象，**Resource**。​ Spring把其资源做了一个抽象，底层使用统一的资源访问接口来访问Spring的所有资源。即：不管什么格式的文件，也不管文件在哪里，到Spring底层，都只有一个访问接口，**Resource**。​ 类结构图 类和接口的分析 可以看到有四个比较重要的接口：**InputStreamSource**、**Resource**、**WritableResource**、**ContextResource**。 **InputStreamSource** 12345public interface InputStreamSource &#123; InputStream getInputStream() throws IOException;&#125; **Resource**接口 ​ 接口中定义了对于资源的判断、对资源的获取、对资源描述的获取。通过该接口可以对资源进行有效的操作。但是**Resource**接口注重于对资源的读取。 123456789101112131415161718192021222324252627282930313233343536373839404142/** * 接口中定义了对于资源的判断、对资源的获取、对资源描述的获取。通过该接口可以对资源进行有效的操作。但是Resource接口注重于对资源的读取。 */public interface Resource extends InputStreamSource &#123; /*判断是否存在*/ boolean exists(); /*判断是否可读*/ default boolean isReadable() &#123; return exists(); &#125; /*判断是否可以重复读取，如果为true表示不可以重复读取，在读取完成之后，需要关闭流*/ default boolean isOpen() &#123; return false; &#125; /*判断是否是文件*/ default boolean isFile() &#123; return false; &#125; /*获取URL地址*/ URL getURL() throws IOException; /*获取URL地址*/ URI getURI() throws IOException; /*获取文件*/ File getFile() throws IOException; /*默认通过输入流获取nio的只读字节流管道*/ default ReadableByteChannel readableChannel() throws IOException &#123; return Channels.newChannel(getInputStream()); &#125; /*资源长度*/ long contentLength() throws IOException; /*上次更新时间*/ long lastModified() throws IOException; /*根据资源的当前位置，获取相对位置的其他资源*/ Resource createRelative(String relativePath) throws IOException; /*返回资源名称*/ @Nullable String getFilename(); /*返回资源描述*/ String getDescription();&#125; **WritableResource** 因为Resource接口主要是注重对资源的读取，当我们对资源进行写入的时候，需要获取对应的判断和输出流。WritableResource接口主要定义了对写入的支持。 12345678910111213public interface WritableResource extends Resource &#123; /*返回资源是否可以被写入*/ default boolean isWritable() &#123; return true; &#125; /*获取资源的写入流*/ OutputStream getOutputStream() throws IOException; /*默认提供的支持写的nio字节管道*/ default WritableByteChannel writableChannel() throws IOException &#123; return Channels.newChannel(getOutputStream()); &#125;&#125; **ContextResource** 有些资源是相对于当前容器的，用来获取容器中的资源。 12345public interface ContextResource extends Resource &#123; String getPathWithinContext();&#125; 存在一个**AbstractResource**的抽象类，所有的对于资源获取都继承自**AbstractResource**抽象类。 其余的都是具体的实现类，用来加载指定的资源。 ​ 对资源的加载​ Spring框架为了更方便的获取资源，尽量弱化程序员对各个**Resource**接口的实现类的感知，定义了另一个**ResourceLoader**接口。接口有一个特别重要的方法：**Resource getResource(String location);** 返回**Resource**实例。因此程序猿在使用spring容器的时候，可以不去过于计较比较底层的**Resource**实现，也不需要自己创建**Resource**的实现类，而是直接使用**ResourceLoader**获取到bean容器本身的**Resource**，进而获取到相关的资源信息。​ **ResourceLoader** 只能对**classpath**路径下面的资源进行加载，并且只会加载指定的文件 123456789101112131415161718/** * Spring框架为了更方便的获取资源，尽量弱化程序员对各个Resource接口的实现类的感知，定义了另一个ResourceLoader接口。 * 接口有一个特别重要的方法：Resource getResource(String location)，返回Resource实例。因此程序员在使用Spring容器时， * 可以不去过于计较底层Resource的实现，也不需要自己创建Resource实现类，而是直接使用ReourceLoader，获取到bean容器本身的Resource， * 进而取到相关的资源信息。 */public interface ResourceLoader &#123; /** Pseudo URL prefix for loading from the class path: &quot;classpath:&quot;. */ String CLASSPATH_URL_PREFIX = ResourceUtils.CLASSPATH_URL_PREFIX; /*用来根据location来获取指定的资源*/ Resource getResource(String location); /*获取类加载器*/ @Nullable ClassLoader getClassLoader();&#125; **ResourcePatternResolver** 表示会加载所有路径下面的文件，包括jar包中的文件。同时**locationPattern**可以设置为表达式来加载对应的文件。 123456789101112131415/*表示会加载所有路径下面的文件，包括jar包中的文件。同时locationPattern可以设置为表达式来加载对应的文件。*/public interface ResourcePatternResolver extends ResourceLoader &#123; /** * 表示会加载所有路径下面的文件，包括jar包中 */ String CLASSPATH_ALL_URL_PREFIX = &quot;classpath*:&quot;; /** * 根据 */ Resource[] getResources(String locationPattern) throws IOException;&#125; 区别：​ **classpath**: ：表示从类路径中加载资源，**classpath**:和**classpath**:/是等价的，都是相对于类的根路径。资源文件库标准的在文件系统中，也可以在JAR或ZIP的类包中。​ **classpath**:*：假设多个JAR包或文件系统类路径都有一个相同的配置文件，**classpath**:只会在第一个加载的类路径下查找，而**classpath***:会扫描所有这些JAR包及类路径下出现的同名文件。​ **DefaultResourceLoader** spring实现的默认的加载器，一般其他的加载器会继承该类，并重写**getResourceByPath**方法。 123456789101112131415161718192021222324252627282930@Overridepublic Resource getResource(String location) &#123; Assert.notNull(location, &quot;Location must not be null&quot;); //循环遍历使用解析器解析该location的资源，如果资源不为空，直接返回 for (ProtocolResolver protocolResolver : getProtocolResolvers()) &#123; Resource resource = protocolResolver.resolve(location, this); if (resource != null) &#123; return resource; &#125; &#125; /*以/开头那么根据path去寻找*/ if (location.startsWith(&quot;/&quot;)) &#123; return getResourceByPath(location); &#125; /*以classpath开头，那么抽象为ClassPathResource*/ else if (location.startsWith(CLASSPATH_URL_PREFIX)) &#123; return new ClassPathResource(location.substring(CLASSPATH_URL_PREFIX.length()), getClassLoader()); &#125; else &#123; try &#123; /*其他情况采用urlResource来加载*/ URL url = new URL(location); return (ResourceUtils.isFileURL(url) ? new FileUrlResource(url) : new UrlResource(url)); &#125; catch (MalformedURLException ex) &#123; // No URL -&gt; resolve as resource path. return getResourceByPath(location); &#125; &#125;&#125; **PathMatchingResourcePatternResolver**​ Spring提供了一个**ResourcePatternResolver**实现**PathMatchingResourcePatternResolver**，它是基于模式匹配的，默认使用**AntPathMatcher**进行路径匹配，它除了支持**ResourceLoader**支持的前缀外，还额外支持“**classpath**:”用于加载所有匹配的类路径**Resource**，**ResourceLoader**不支持前缀“**classpath**:”。​ 6.prepareBeanFactory(beanFactory)给容器中注册一些组件，添加切面，类加载器，表达式解析器，注解解析器，事件处理器。 **prepareBeanFactory()**​ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152protected void prepareBeanFactory(ConfigurableListableBeanFactory beanFactory) &#123; // Tell the internal bean factory to use the context&#x27;s class loader etc. /*类加载器*/ beanFactory.setBeanClassLoader(getClassLoader()); if (!shouldIgnoreSpel) &#123; /*表达式解析器*/ beanFactory.setBeanExpressionResolver(new StandardBeanExpressionResolver(beanFactory.getBeanClassLoader())); &#125; /*属性编辑器*/ beanFactory.addPropertyEditorRegistrar(new ResourceEditorRegistrar(this, getEnvironment())); // 给当前工厂设置上下文回调 beanFactory.addBeanPostProcessor(new ApplicationContextAwareProcessor(this)); beanFactory.ignoreDependencyInterface(EnvironmentAware.class); beanFactory.ignoreDependencyInterface(EmbeddedValueResolverAware.class); beanFactory.ignoreDependencyInterface(ResourceLoaderAware.class); beanFactory.ignoreDependencyInterface(ApplicationEventPublisherAware.class); beanFactory.ignoreDependencyInterface(MessageSourceAware.class); beanFactory.ignoreDependencyInterface(ApplicationContextAware.class); beanFactory.ignoreDependencyInterface(ApplicationStartupAware.class); // BeanFactory接口未在普通工厂中注册为可解析类型。 // 注册消息资源的解析器 beanFactory.registerResolvableDependency(BeanFactory.class, beanFactory); beanFactory.registerResolvableDependency(ResourceLoader.class, this); beanFactory.registerResolvableDependency(ApplicationEventPublisher.class, this); beanFactory.registerResolvableDependency(ApplicationContext.class, this); // 将用于检测内部beans的早期后置处理器注册为ApplicationListeners。 beanFactory.addBeanPostProcessor(new ApplicationListenerDetector(this)); // Detect a LoadTimeWeaver and prepare for weaving, if found. if (!NativeDetector.inNativeImage() &amp;&amp; beanFactory.containsBean(LOAD_TIME_WEAVER_BEAN_NAME)) &#123; beanFactory.addBeanPostProcessor(new LoadTimeWeaverAwareProcessor(beanFactory)); // Set a temporary ClassLoader for type matching. beanFactory.setTempClassLoader(new ContextTypeMatchClassLoader(beanFactory.getBeanClassLoader())); &#125; // Register default environment beans. if (!beanFactory.containsLocalBean(ENVIRONMENT_BEAN_NAME)) &#123; beanFactory.registerSingleton(ENVIRONMENT_BEAN_NAME, getEnvironment()); &#125; if (!beanFactory.containsLocalBean(SYSTEM_PROPERTIES_BEAN_NAME)) &#123; beanFactory.registerSingleton(SYSTEM_PROPERTIES_BEAN_NAME, getEnvironment().getSystemProperties()); &#125; if (!beanFactory.containsLocalBean(SYSTEM_ENVIRONMENT_BEAN_NAME)) &#123; beanFactory.registerSingleton(SYSTEM_ENVIRONMENT_BEAN_NAME, getEnvironment().getSystemEnvironment()); &#125; if (!beanFactory.containsLocalBean(APPLICATION_STARTUP_BEAN_NAME)) &#123; beanFactory.registerSingleton(APPLICATION_STARTUP_BEAN_NAME, getApplicationStartup()); &#125;&#125; 7.postProcessBeanFactory(beanFactory)​ 为容器的某些子类指定后置处理器，子类可以通过重写这个方法，在bean工厂创建并预备完成以后做进一步的设置，可以在bd创建出实例对象之前，对bd信息做进一步修改。**postProcessBeanFactory()** 这也是spring提供的一个扩展点。​ 12protected void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) &#123;&#125; 8.执行bean工厂的后置处理器执行bean工厂的后置处理器，先执行bean定义注册后置处理器，在执行bean工厂的后置处理器。执行顺序：先执行实现了优先级接口的，在执行带有order的，最后执行其他的。​ **invokeBeanFactoryPostProcessors(beanFactory)**​ 1234567891011protected void invokeBeanFactoryPostProcessors(ConfigurableListableBeanFactory beanFactory) &#123; //执行bean工厂的后置处理器 PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(beanFactory, getBeanFactoryPostProcessors()); // Detect a LoadTimeWeaver and prepare for weaving, if found in the meantime // (e.g. through an @Bean method registered by ConfigurationClassPostProcessor) if (!NativeDetector.inNativeImage() &amp;&amp; beanFactory.getTempClassLoader() == null &amp;&amp; beanFactory.containsBean(LOAD_TIME_WEAVER_BEAN_NAME)) &#123; beanFactory.addBeanPostProcessor(new LoadTimeWeaverAwareProcessor(beanFactory)); beanFactory.setTempClassLoader(new ContextTypeMatchClassLoader(beanFactory.getBeanClassLoader())); &#125;&#125; **\u0000invokeBeanFactoryPostProcessors()** 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171public static void invokeBeanFactoryPostProcessors( ConfigurableListableBeanFactory beanFactory, List&lt;BeanFactoryPostProcessor&gt; beanFactoryPostProcessors) &#123; //初始化一个set列表，存储已经执行过的beanName Set&lt;String&gt; processedBeans = new HashSet&lt;&gt;(); //类型断言：如果是BeanDefinitionRegistry类型的 //意思就是：当前 beanFactory 是 beanDefinition 的注册中心 ， bd 全部注册到 bf。 if (beanFactory instanceof BeanDefinitionRegistry) &#123; //类型转换 bf -&gt; bd注册中心 BeanDefinitionRegistry registry = (BeanDefinitionRegistry) beanFactory; //存储BeanFactoryPostProcessor List&lt;BeanFactoryPostProcessor&gt; regularPostProcessors = new ArrayList&lt;&gt;(); /*存储BeanDefinitionRegistryPostProcessor*/ List&lt;BeanDefinitionRegistryPostProcessor&gt; registryProcessors = new ArrayList&lt;&gt;(); /** * BeanFactoryPostProcessor 和 BeanDefinitionRegistryPostProcessor 之间的关系 ？ * BeanDefinitionRegistryPostProcessor 是 BeanFactoryPostProcessor 的子类。 * 它里面 搞了一个新的方法 postProcessBeanDefinitionRegistry ，可以往容器中注册更多的bd信息。 * 扩展点： * ①BeanFactoryPostProcessor 对bd信息进行修改 * ②postProcessBeanDefinitionRegistry 添加更多的bd信息 */ //处理ApplicationContext里面硬编码注册的beanFactoryPostProcessor for (BeanFactoryPostProcessor postProcessor : beanFactoryPostProcessors) &#123; /*如果当前后置处理器的类型是BeanDefinitionRegistryPostProcessor*/ if (postProcessor instanceof BeanDefinitionRegistryPostProcessor) &#123; /*将后置处理器转换为BeanDefinitionRegistryPostProcessor类型*/ BeanDefinitionRegistryPostProcessor registryProcessor = (BeanDefinitionRegistryPostProcessor) postProcessor; /*执行后置处理器的方法 这里又是一个后置处理器的调用点*/ registryProcessor.postProcessBeanDefinitionRegistry(registry); /*将执行完的后置处理器加入到集合中*/ registryProcessors.add(registryProcessor); &#125; else &#123;/*此时说明后置处理器是一个BeanFactoryPostProcessor，直接加入到BeanFactoryPostProcessor的集合中。后续统一执行。*/ regularPostProcessors.add(postProcessor); &#125; &#125; // 这里不要初始化FactoryBeans:我们需要保留所有常规bean的未初始化状态， // 让bean工厂的后处理器应用于它们！在实现优先级的bean definition registry后处理器、 // Ordered、Ordered等之间进行分离。 /*当前阶段的registry后置处理器集合*/ List&lt;BeanDefinitionRegistryPostProcessor&gt; currentRegistryProcessors = new ArrayList&lt;&gt;(); // 首先执行实现了主排序接口的后置处理器 PriorityOrdered String[] postProcessorNames = beanFactory.getBeanNamesForType(BeanDefinitionRegistryPostProcessor.class, true, false); for (String ppName : postProcessorNames) &#123; /*判断对应的bean是否实现了主排序接口，如果实现了，就让该后置处理器添加到集合。*/ if (beanFactory.isTypeMatch(ppName, PriorityOrdered.class)) &#123; currentRegistryProcessors.add(beanFactory.getBean(ppName, BeanDefinitionRegistryPostProcessor.class)); /*已经执行的后置处理器名字集合，因为接下来马上要执行这些后置处理器了。*/ processedBeans.add(ppName); &#125; &#125; /*对后置处理器进行排序 根据 getOrder() 返回的值进行升序排序*/ sortPostProcessors(currentRegistryProcessors, beanFactory); /*注册所有的后置处理器*/ registryProcessors.addAll(currentRegistryProcessors); /*调用当前后置处理器的相关接口方法 执行 BeanDefinitionRegistryPostProcessor 的 postProcessBeanDefinitionRegistry()*/ invokeBeanDefinitionRegistryPostProcessors(currentRegistryProcessors, registry, beanFactory.getApplicationStartup()); /*清空当前阶段临时的集合*/ currentRegistryProcessors.clear(); // 接着执行实现了Ordered接口的后置处理器 这里的后置处理器是普通排序后置处理器 postProcessorNames = beanFactory.getBeanNamesForType(BeanDefinitionRegistryPostProcessor.class, true, false); for (String ppName : postProcessorNames) &#123; /*确保每一个后置处理器只执行一次*/ if (!processedBeans.contains(ppName) &amp;&amp; beanFactory.isTypeMatch(ppName, Ordered.class)) &#123; currentRegistryProcessors.add(beanFactory.getBean(ppName, BeanDefinitionRegistryPostProcessor.class)); processedBeans.add(ppName); &#125; &#125; sortPostProcessors(currentRegistryProcessors, beanFactory); registryProcessors.addAll(currentRegistryProcessors); invokeBeanDefinitionRegistryPostProcessors(currentRegistryProcessors, registry, beanFactory.getApplicationStartup()); currentRegistryProcessors.clear(); // 最后将剩下的后置处理器逐个执行 /*这个变量控制是否需要循环*/ boolean reiterate = true; while (reiterate) &#123; reiterate = false; /*从bean工厂拿BeanDefinitionRegistryPostProcessor的后置处理器*/ postProcessorNames = beanFactory.getBeanNamesForType(BeanDefinitionRegistryPostProcessor.class, true, false); /*循环判断执行没执行过得*/ for (String ppName : postProcessorNames) &#123; if (!processedBeans.contains(ppName)) &#123; currentRegistryProcessors.add(beanFactory.getBean(ppName, BeanDefinitionRegistryPostProcessor.class)); processedBeans.add(ppName); /*这里为什么吧变量设置为true？因为新注册的后置处理器，可能也是往容器中注册的 registry 类型的后置处理器*/ reiterate = true; &#125; &#125; sortPostProcessors(currentRegistryProcessors, beanFactory); registryProcessors.addAll(currentRegistryProcessors); invokeBeanDefinitionRegistryPostProcessors(currentRegistryProcessors, registry, beanFactory.getApplicationStartup()); currentRegistryProcessors.clear(); &#125; /*这里是执行BeanDefinitionRegistryPostProcessor 的父类 BeanFactoryPostProcessor 的方法*/ // 上面重复三遍是为了执行 BeanDefinitionRegistryPostProcessor 的方法， // 但是，BeanDefinitionRegistryPostProcessor 还继承了 BeanFactoryPostProcessor // 这里是为了执行所有后置处理器的 // （BeanFactoryPostProcessor）的 postProcessBeanFactory方法。 invokeBeanFactoryPostProcessors(registryProcessors, beanFactory); /*这个方法就是执行BeanFactoryPostProcessor的方法*/ invokeBeanFactoryPostProcessors(regularPostProcessors, beanFactory); &#125; /*处理不是 BeanDefinitionRegistry 类型的后置处理器*/ else &#123; // 调用上下文实例注册的后置处理器 invokeBeanFactoryPostProcessors(beanFactoryPostProcessors, beanFactory); &#125; /** * 上面处理的是硬编码的BeanDefinitionRegistry和BeanFactoryPostProcessor的后置处理器。 * 下面处理器的是普通的后置处理器。 */ // 不要在这里初始化FactoryBeans:我们需要保留所有常规beans未初始化以让bean工厂后处理器应用于它们！ /*获取容器内注册的所有BeanFactoryPostProcessor集合。还是处理主排序，普通排序，为排序的后置处理器集合*/ String[] postProcessorNames = beanFactory.getBeanNamesForType(BeanFactoryPostProcessor.class, true, false); // 在实现优先级有序、有序和其他的BeanFactoryPostProcessors之间进行分离。 List&lt;BeanFactoryPostProcessor&gt; priorityOrderedPostProcessors = new ArrayList&lt;&gt;(); List&lt;String&gt; orderedPostProcessorNames = new ArrayList&lt;&gt;(); List&lt;String&gt; nonOrderedPostProcessorNames = new ArrayList&lt;&gt;(); for (String ppName : postProcessorNames) &#123; /*包含说明已经执行过了，直接啥也不做跳过即可*/ if (processedBeans.contains(ppName)) &#123; // skip - already processed in first phase above &#125; else if (beanFactory.isTypeMatch(ppName, PriorityOrdered.class)) &#123; priorityOrderedPostProcessors.add(beanFactory.getBean(ppName, BeanFactoryPostProcessor.class)); &#125; else if (beanFactory.isTypeMatch(ppName, Ordered.class)) &#123; orderedPostProcessorNames.add(ppName); &#125; else &#123; nonOrderedPostProcessorNames.add(ppName); &#125; &#125; // 首先，执行带有优先级接口的 sortPostProcessors(priorityOrderedPostProcessors, beanFactory); invokeBeanFactoryPostProcessors(priorityOrderedPostProcessors, beanFactory); // 接着执行实现了order接口的 List&lt;BeanFactoryPostProcessor&gt; orderedPostProcessors = new ArrayList&lt;&gt;(orderedPostProcessorNames.size()); for (String postProcessorName : orderedPostProcessorNames) &#123; orderedPostProcessors.add(beanFactory.getBean(postProcessorName, BeanFactoryPostProcessor.class)); &#125; sortPostProcessors(orderedPostProcessors, beanFactory); invokeBeanFactoryPostProcessors(orderedPostProcessors, beanFactory); // 最后执行其他剩余的 List&lt;BeanFactoryPostProcessor&gt; nonOrderedPostProcessors = new ArrayList&lt;&gt;(nonOrderedPostProcessorNames.size()); for (String postProcessorName : nonOrderedPostProcessorNames) &#123; nonOrderedPostProcessors.add(beanFactory.getBean(postProcessorName, BeanFactoryPostProcessor.class)); &#125; invokeBeanFactoryPostProcessors(nonOrderedPostProcessors, beanFactory); // 内存清理，帮助GC beanFactory.clearMetadataCache(); /*思考：为什么spring要把这里代码逻辑写的这么麻烦？为了框架的健壮性，提供更好的扩展性，方便在不同阶段，针对不同的需求进行扩展。*/&#125; 9.注册后置处理器获取所有的后置处理器，先注册实现主排序接口的，在注册实现Order接口的，最后注册剩下的。​ 注册一个**ApplicationListenerDetector**，在bean完成创建后检查是否是**ApplicationListener**，如果是，添加到容器。​ 12345678/** * 实例化并注册所有 BeanPostProcessor bean，如果给出，则遵守显式顺序。 * 必须在应用程序 bean 的任何实例化之前调用。 */protected void registerBeanPostProcessors(ConfigurableListableBeanFactory beanFactory) &#123; //注册所有的后置处理器 PostProcessorRegistrationDelegate.registerBeanPostProcessors(beanFactory, this);&#125; **\u0000registerBeanPostProcessors()** 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374public static void registerBeanPostProcessors( ConfigurableListableBeanFactory beanFactory, AbstractApplicationContext applicationContext) &#123; //获取到所有的BeanPostProcessor后置处理器的beanName数组 String[] postProcessorNames = beanFactory.getBeanNamesForType(BeanPostProcessor.class, true, false); // 注册bean后置处理器检查器，当bean在bean后置处理器实例化期间创建时， // 即当bean不适合被所有bean后置处理器处理时，它会记录一条信息。 /*后置处理器数量，计算方式beanFactory已经有的数量+1+后注册的。1？下面一行手动硬加的*/ int beanProcessorTargetCount = beanFactory.getBeanPostProcessorCount() + 1 + postProcessorNames.length; /*BeanPostProcessorChecker？ step into 检查创建bean实例的时候，后置处理器是否已经全部注册完毕，如果未完毕，日志提示。*/ beanFactory.addBeanPostProcessor(new BeanPostProcessorChecker(beanFactory, beanProcessorTargetCount)); // 分离 实现了主排序接口的后置处理器 List&lt;BeanPostProcessor&gt; priorityOrderedPostProcessors = new ArrayList&lt;&gt;(); /*存放mergedBeanDefinition后置处理器*/ List&lt;BeanPostProcessor&gt; internalPostProcessors = new ArrayList&lt;&gt;(); /*普通排序接口的*/ List&lt;String&gt; orderedPostProcessorNames = new ArrayList&lt;&gt;(); /*没有实现排序接口的*/ List&lt;String&gt; nonOrderedPostProcessorNames = new ArrayList&lt;&gt;(); /*循环将所有的后置处理器进行分离存放*/ for (String ppName : postProcessorNames) &#123; if (beanFactory.isTypeMatch(ppName, PriorityOrdered.class)) &#123; BeanPostProcessor pp = beanFactory.getBean(ppName, BeanPostProcessor.class); priorityOrderedPostProcessors.add(pp); if (pp instanceof MergedBeanDefinitionPostProcessor) &#123; internalPostProcessors.add(pp); &#125; &#125; else if (beanFactory.isTypeMatch(ppName, Ordered.class)) &#123; orderedPostProcessorNames.add(ppName); &#125; else &#123; nonOrderedPostProcessorNames.add(ppName); &#125; &#125; // 首先注册实现了优先级接口的后置处理器 sortPostProcessors(priorityOrderedPostProcessors, beanFactory); registerBeanPostProcessors(beanFactory, priorityOrderedPostProcessors); // 其次，注册实现了order接口的后置处理器 List&lt;BeanPostProcessor&gt; orderedPostProcessors = new ArrayList&lt;&gt;(orderedPostProcessorNames.size()); for (String ppName : orderedPostProcessorNames) &#123; BeanPostProcessor pp = beanFactory.getBean(ppName, BeanPostProcessor.class); orderedPostProcessors.add(pp); if (pp instanceof MergedBeanDefinitionPostProcessor) &#123; internalPostProcessors.add(pp); &#125; &#125; sortPostProcessors(orderedPostProcessors, beanFactory); registerBeanPostProcessors(beanFactory, orderedPostProcessors); // 最终，注册所有剩下的所有后置处理器 List&lt;BeanPostProcessor&gt; nonOrderedPostProcessors = new ArrayList&lt;&gt;(nonOrderedPostProcessorNames.size()); for (String ppName : nonOrderedPostProcessorNames) &#123; BeanPostProcessor pp = beanFactory.getBean(ppName, BeanPostProcessor.class); nonOrderedPostProcessors.add(pp); if (pp instanceof MergedBeanDefinitionPostProcessor) &#123; internalPostProcessors.add(pp); &#125; &#125; //注册后置处理器的逻辑 step into registerBeanPostProcessors(beanFactory, nonOrderedPostProcessors); // 最后，再次处理internalPostProcessors，确保存放mergedBeanDefinition后置处理器在链表的末尾 sortPostProcessors(internalPostProcessors, beanFactory); registerBeanPostProcessors(beanFactory, internalPostProcessors); //重新注册后处理器时，将内部beans检测为ApplicationListeners，将其移动到处理器链的末端(用于获取代理等)。 beanFactory.addBeanPostProcessor(new ApplicationListenerDetector(applicationContext));&#125; 10.initMessageSource()\u0000做一些国际化相关的操作。**initMessageSource()**​ 123456789101112131415161718192021222324252627protected void initMessageSource() &#123; ConfigurableListableBeanFactory beanFactory = getBeanFactory(); if (beanFactory.containsLocalBean(MESSAGE_SOURCE_BEAN_NAME)) &#123; this.messageSource = beanFactory.getBean(MESSAGE_SOURCE_BEAN_NAME, MessageSource.class); // Make MessageSource aware of parent MessageSource. if (this.parent != null &amp;&amp; this.messageSource instanceof HierarchicalMessageSource) &#123; HierarchicalMessageSource hms = (HierarchicalMessageSource) this.messageSource; if (hms.getParentMessageSource() == null) &#123; // Only set parent context as parent MessageSource if no parent MessageSource // registered already. hms.setParentMessageSource(getInternalParentMessageSource()); &#125; &#125; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Using MessageSource [&quot; + this.messageSource + &quot;]&quot;); &#125; &#125; else &#123; // Use empty MessageSource to be able to accept getMessage calls. DelegatingMessageSource dms = new DelegatingMessageSource(); dms.setParentMessageSource(getInternalParentMessageSource()); this.messageSource = dms; beanFactory.registerSingleton(MESSAGE_SOURCE_BEAN_NAME, this.messageSource); if (logger.isTraceEnabled()) &#123; logger.trace(&quot;No &#x27;&quot; + MESSAGE_SOURCE_BEAN_NAME + &quot;&#x27; bean, using [&quot; + this.messageSource + &quot;]&quot;); &#125; &#125;&#125; 11.初始化事件派发器**initApplicationEventMulticaster()**\u0000初始化事件派发器，获取bean工厂，从bean工厂获取事件派发器，如果没有，创建一个放到容器中。​ 1234567891011121314151617181920212223protected void initApplicationEventMulticaster() &#123; //获取bean工厂 ConfigurableListableBeanFactory beanFactory = getBeanFactory(); //判断容器是否有事件派发器 if (beanFactory.containsLocalBean(APPLICATION_EVENT_MULTICASTER_BEAN_NAME)) &#123; //如果有就获取到引用 （用户自定义了一个事件多播器） /*可以实现APPLICATION_EVENT_MULTICASTER接口来自己实现一个事件派发器,通过bean的方式传递给spring*/ this.applicationEventMulticaster = beanFactory.getBean(APPLICATION_EVENT_MULTICASTER_BEAN_NAME, ApplicationEventMulticaster.class); if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Using ApplicationEventMulticaster [&quot; + this.applicationEventMulticaster + &quot;]&quot;); &#125; &#125; else &#123;//走到这里的前提是容器中没有事件派发器,需要使用spring框架默认提供的事件派发器。 //直接造一个事件派发器 this.applicationEventMulticaster = new SimpleApplicationEventMulticaster(beanFactory); //将事件派发器注册到一级缓存，事件派发器是单例的全局通用的。 beanFactory.registerSingleton(APPLICATION_EVENT_MULTICASTER_BEAN_NAME, this.applicationEventMulticaster); if (logger.isTraceEnabled()) &#123; logger.trace(&quot;No &#x27;&quot; + APPLICATION_EVENT_MULTICASTER_BEAN_NAME + &quot;&#x27; bean, using &quot; + &quot;[&quot; + this.applicationEventMulticaster.getClass().getSimpleName() + &quot;]&quot;); &#125; &#125;&#125; 12.onRefresh()\u0000留给子容器，子容器可以在容器刷新的时候加入自己的逻辑。**onRefresh()**​ 123protected void onRefresh() throws BeansException &#123; // For subclasses: do nothing by default.&#125; 13.注册监听器为事件派发器注册事件监听器，派发一些早期事件。**registerListeners()**​ 123456789101112131415161718192021protected void registerListeners() &#123; // 将所有的事件监听器（硬编码的）注册到事件派发器 for (ApplicationListener&lt;?&gt; listener : getApplicationListeners()) &#123; getApplicationEventMulticaster().addApplicationListener(listener); &#125; // 注册通过bean配置提供的监听器 ，用户可以通过 bd 的方式 提供listener String[] listenerBeanNames = getBeanNamesForType(ApplicationListener.class, true, false); for (String listenerBeanName : listenerBeanNames) &#123; getApplicationEventMulticaster().addApplicationListenerBean(listenerBeanName); &#125; // 通过事件派发器派发容器早期应用程序事件 这里可以通过重写onRefresh方法直接往earlyApplicationEvents的set集合里面加。 Set&lt;ApplicationEvent&gt; earlyEventsToProcess = this.earlyApplicationEvents; this.earlyApplicationEvents = null; if (!CollectionUtils.isEmpty(earlyEventsToProcess)) &#123; for (ApplicationEvent earlyEvent : earlyEventsToProcess) &#123; getApplicationEventMulticaster().multicastEvent(earlyEvent); &#125; &#125;&#125; \u0000关注一下：**ApplicationEventMulticaster.multicastEvent(earlyEvent)** 派发事件。​ 123456789101112131415161718192021@Overridepublic void multicastEvent(ApplicationEvent event) &#123; multicastEvent(event, resolveDefaultEventType(event));&#125;@Overridepublic void multicastEvent(final ApplicationEvent event, @Nullable ResolvableType eventType) &#123; /*事件类型为空就是用默认的事件类型解析器*/ ResolvableType type = (eventType != null ? eventType : resolveDefaultEventType(event)); /*获取事件多波器的线程池*/ Executor executor = getTaskExecutor(); /*拿到当前事件的所有监听器，伦轮训所有的监听器，对事件进行派发。*/ for (ApplicationListener&lt;?&gt; listener : getApplicationListeners(event, type)) &#123; if (executor != null) &#123; executor.execute(() -&gt; invokeListener(listener, event)); &#125; else &#123; invokeListener(listener, event); &#125; &#125;&#125; ​ **\u0000invokeListener()**​ 12345678910111213141516protected void invokeListener(ApplicationListener&lt;?&gt; listener, ApplicationEvent event) &#123; /*获取到错误处理器的引用*/ ErrorHandler errorHandler = getErrorHandler(); if (errorHandler != null) &#123; try &#123; /*真正执行派发事件的逻辑*/ doInvokeListener(listener, event); &#125; catch (Throwable err) &#123; errorHandler.handleError(err); &#125; &#125; else &#123; doInvokeListener(listener, event); &#125;&#125; **doInvokeListener()**​ 1234567891011121314151617181920212223242526private void doInvokeListener(ApplicationListener listener, ApplicationEvent event) &#123; try &#123; /*监听器发布事件*/ listener.onApplicationEvent(event); &#125; catch (ClassCastException ex) &#123; String msg = ex.getMessage(); if (msg == null || matchesClassCastMessage(msg, event.getClass()) || (event instanceof PayloadApplicationEvent &amp;&amp; matchesClassCastMessage(msg, ((PayloadApplicationEvent) event).getPayload().getClass()))) &#123; // Possibly a lambda-defined listener which we could not resolve the generic event type for // -&gt; let&#x27;s suppress the exception. Log loggerToUse = this.lazyLogger; if (loggerToUse == null) &#123; loggerToUse = LogFactory.getLog(getClass()); this.lazyLogger = loggerToUse; &#125; if (loggerToUse.isTraceEnabled()) &#123; loggerToUse.trace(&quot;Non-matching event type for listener: &quot; + listener, ex); &#125; &#125; else &#123; throw ex; &#125; &#125;&#125; 13.1 ApplicationListenerApplicationListener &amp;&amp; ApplicationEvent通过自定义不同类型的事件，使用不同的监听器监听不同类型的事件，做到jvm进程内的消息队列，事件驱动，解耦。​ 1234567891011121314151617public class MyEvent extends ApplicationEvent &#123; String message; public MyEvent(Object source) &#123; super(source); &#125; public MyEvent(Object source,String message) &#123; super(source); this.message=message; &#125; public void print()&#123; System.out.println(&quot;发布了一个事件：&quot;+message); &#125;&#125; 12345678public class MyListener implements ApplicationListener&lt;MyEvent&gt; &#123; @Override public void onApplicationEvent(MyEvent event) &#123; event.print(); &#125;&#125; 1234567/** * 测试spring 的 ioc 容器的事件发布 */private static void testPublishEvent() &#123; ApplicationContext ioc = new ClassPathXmlApplicationContext(CONFIG_LOCATION); ioc.publishEvent(new MyEvent(&quot;&quot;, &quot;这是我自定义的一个事件&quot;));&#125; @EventListener对上面写法的一个优化，更加简洁，开发量更少，懒人必备神器。​ 12345678private static void testEventListener() &#123; ioc.publishEvent(new ApplicationEvent(&quot;hello，spring&quot;) &#123; @Override public Object getSource() &#123; return super.getSource(); &#125; &#125;);&#125; 12345678@Componentpublic class MyEventListener &#123; @EventListener(classes = ApplicationEvent.class) public void listener(ApplicationEvent event)&#123; System.out.println(&quot;event = &quot; + event); &#125;&#125; 14.初始化剩下所有的单实例bean**finishBeanFactoryInitialization(beanFactory)**\u0000 12345678910111213141516171819202122232425262728protected void finishBeanFactoryInitialization(ConfigurableListableBeanFactory beanFactory) &#123; // 为此上下文初始化转换服务 if (beanFactory.containsBean(CONVERSION_SERVICE_BEAN_NAME) &amp;&amp; beanFactory.isTypeMatch(CONVERSION_SERVICE_BEAN_NAME, ConversionService.class)) &#123; beanFactory.setConversionService( beanFactory.getBean(CONVERSION_SERVICE_BEAN_NAME, ConversionService.class)); &#125; // 如果容器里面没有字符串转换器，初始化一个字符串转换器放到容器中。 if (!beanFactory.hasEmbeddedValueResolver()) &#123; beanFactory.addEmbeddedValueResolver(strVal -&gt; getEnvironment().resolvePlaceholders(strVal)); &#125; // 尽早初始化LoadTimeWeaverAware beans，以便尽早注册它们的转换器。 String[] weaverAwareNames = beanFactory.getBeanNamesForType(LoadTimeWeaverAware.class, false, false); for (String weaverAwareName : weaverAwareNames) &#123; getBean(weaverAwareName); &#125; // 停止使用临时类加载器进行类型匹配 beanFactory.setTempClassLoader(null); // 允许缓存所有bean定义元数据，不期望进一步的更改，冻结bd信息，冻结之后就无法往bf注册bd了 beanFactory.freezeConfiguration(); // 实例化所有剩余的单实例bean beanFactory.preInstantiateSingletons();&#125; 如何冻结bd信息的？**beanFactory.freezeConfiguration()**​ 123456@Overridepublic void freezeConfiguration() &#123; /*这个字段如果是true，name就不能往容器中添加新的beanDefinition了*/ this.configurationFrozen = true; this.frozenBeanDefinitionNames = StringUtils.toStringArray(this.beanDefinitionNames);&#125; **beanFactory.preInstantiateSingletons()**​ 这个方法内容过多且偏核心内容，留在下一篇分析，跳过这里，加载完所有的单实例bean之后，看还要做什么操作。​ 15.finishRefresh()初始化容器的生命周期事件处理器，回调**onRefresh()**,并发布容器的生命周期事件。**finishRefresh() **\u0000 1234567891011121314151617181920212223protected void finishRefresh() &#123; // 清除上下文级资源缓存(例如来自扫描的ASM元数据)。 clearResourceCaches(); // 为此上下文初始化生命周期处理器。 /*案例演示：lifecycle package * 两者的区别： * smart：正常情况下就会执行 * 普通的：必须显示调用容器.start()/.stop() * */ initLifecycleProcessor(); // 首先将刷新传播到生命周期处理器。回调 onRefresh getLifecycleProcessor().onRefresh(); // 发布容器创建完成事件 publishEvent(new ContextRefreshedEvent(this)); // 注册容器上下文 if (!NativeDetector.inNativeImage()) &#123; LiveBeansView.registerApplicationContext(this); &#125;&#125; 15.1 为此上下文初始化生命周期事件处理器**initLifecycleProcessor()**\u0000 1234567891011121314151617181920212223protected void initLifecycleProcessor() &#123; /*获取bean工厂*/ ConfigurableListableBeanFactory beanFactory = getBeanFactory(); /*判断工厂是否已经有生命周期处理器，有的话直接获取引用*/ if (beanFactory.containsLocalBean(LIFECYCLE_PROCESSOR_BEAN_NAME)) &#123; this.lifecycleProcessor = beanFactory.getBean(LIFECYCLE_PROCESSOR_BEAN_NAME, LifecycleProcessor.class); if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Using LifecycleProcessor [&quot; + this.lifecycleProcessor + &quot;]&quot;); &#125; &#125; else &#123; /*此时就是工厂目前还没有的逻辑，那么就需要自动创建一个*/ DefaultLifecycleProcessor defaultProcessor = new DefaultLifecycleProcessor(); defaultProcessor.setBeanFactory(beanFactory); this.lifecycleProcessor = defaultProcessor; /*注册到一级缓存*/ beanFactory.registerSingleton(LIFECYCLE_PROCESSOR_BEAN_NAME, this.lifecycleProcessor); if (logger.isTraceEnabled()) &#123; logger.trace(&quot;No &#x27;&quot; + LIFECYCLE_PROCESSOR_BEAN_NAME + &quot;&#x27; bean, using &quot; + &quot;[&quot; + this.lifecycleProcessor.getClass().getSimpleName() + &quot;]&quot;); &#125; &#125;&#125; 15.2 生命周期事件处理器**Lifecycle** &amp;&amp; **SmartLifecycle**​ 容器创建完成之后的回调，传递的参数_**autoStartUpOnly**_是干嘛的？​ 表示只启动_**SmartLifeCycle**_生命周期对象，并且启动的对象_**autoStartUpOnly**_必须是true，不会启动普通的生命周期对象，false的时候，会启动全部的生命周期对象。​ 1234567891011121314151617181920public class DemoLifeCycle implements Lifecycle &#123; private boolean running =false; @Override public void start() &#123; this.running=true; System.out.println(&quot;demo one start!&quot;); &#125; @Override public void stop() &#123; this.running=false; System.out.println(&quot;demo one stop!&quot;); &#125; @Override public boolean isRunning() &#123; return running; &#125;&#125; 1234567891011121314151617181920public class DemoSmartLifeCycle implements SmartLifecycle &#123; private boolean running = false; @Override public void start() &#123; this.running=true; System.out.println(&quot;demo two start!&quot;); &#125; @Override public void stop() &#123; this.running=false; System.out.println(&quot;demo two stop!&quot;); &#125; @Override public boolean isRunning() &#123; return running; &#125;&#125; 15.3 回调onRefresh()**onRefresh()** 123456789@Overridepublic void onRefresh() &#123; /*传递的参数autoStartUpOnly是干嘛的？ * 表示只启动SmartLifeCycle生命周期对象，并且启动的对象autoStartUpOnly必须是true， * 不会启动普通的生命周期对象， * false的时候，会启动全部的生命周期对象。*/ startBeans(true); this.running = true;&#125; **startBeans(true)**​ 12345678910111213141516171819202122232425private void startBeans(boolean autoStartupOnly) &#123; /*获取到所有实现了生命周期接口的对象，包装到map内 k：beanName v: 生命周期对象*/ Map&lt;String, Lifecycle&gt; lifecycleBeans = getLifecycleBeans(); /*因为生命周期对象可能依赖其他生命周期对象的执行结果，所以需要执行顺序，依靠 smart生命周期接口实现的另一个接口 ，方法返回值越低，优先级越高 */ Map&lt;Integer, LifecycleGroup&gt; phases = new TreeMap&lt;&gt;(); /*遍历判断满足条件加入到集合 * 条件1：SmartLifecycle * 条件2：自启动 * */ lifecycleBeans.forEach((beanName, bean) -&gt; &#123; if (!autoStartupOnly || (bean instanceof SmartLifecycle &amp;&amp; ((SmartLifecycle) bean).isAutoStartup())) &#123; /*排序值*/ int phase = getPhase(bean); phases.computeIfAbsent( phase, p -&gt; new LifecycleGroup(phase, this.timeoutPerShutdownPhase, lifecycleBeans, autoStartupOnly) ).add(beanName, bean); &#125; &#125;); /*轮训启动所有的生命周期处理器*/ if (!phases.isEmpty()) &#123; phases.values().forEach(LifecycleGroup::start); &#125;&#125; **start()** 123456789101112131415public void start() &#123; /*members？看上面的add方法，相当于判断生命周期事件处理器是不是空，如果是空的话就没必要向下执行了。*/ if (this.members.isEmpty()) &#123; return; &#125; if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Starting beans in phase &quot; + this.phase); &#125; /*对生命周期事件处理器进行一个排序*/ Collections.sort(this.members); for (LifecycleGroupMember member : this.members) &#123; /*真正执行启动的逻辑*/ doStart(this.lifecycleBeans, member.name, this.autoStartupOnly); &#125;&#125; **doStart()** 123456789101112131415161718192021222324252627282930313233343536/** * 将指定的 bean 作为给定的 Lifecycle bean 集的一部分启动，确保首先启动它所依赖的任何 bean。 * @param lifecycleBeans a Map with bean name as key and Lifecycle instance as value * @param beanName the name of the bean to start */private void doStart(Map&lt;String, ? extends Lifecycle&gt; lifecycleBeans, String beanName, boolean autoStartupOnly) &#123; /*确保每一个生命周期处理器智只能被启动一次，在一个分组内被启动，其他分组内就看不到这个生命周期处理器*/ Lifecycle bean = lifecycleBeans.remove(beanName); if (bean != null &amp;&amp; bean != this) &#123; /*获取当前即将要被启动的生命周期处理器锁依赖的其他beanName*/ String[] dependenciesForBean = getBeanFactory().getDependenciesForBean(beanName); /*先启动当前生命周期处理器所依赖的其他lifeCycle*/ for (String dependency : dependenciesForBean) &#123; doStart(lifecycleBeans, dependency, autoStartupOnly); &#125; /* * 条件成立：执行start方法 * */ if (!bean.isRunning() &amp;&amp; (!autoStartupOnly || !(bean instanceof SmartLifecycle) || ((SmartLifecycle) bean).isAutoStartup())) &#123; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Starting bean &#x27;&quot; + beanName + &quot;&#x27; of type [&quot; + bean.getClass().getName() + &quot;]&quot;); &#125; try &#123; /*启动当前lifeCycle*/ bean.start(); &#125; catch (Throwable ex) &#123; throw new ApplicationContextException(&quot;Failed to start bean &#x27;&quot; + beanName + &quot;&#x27;&quot;, ex); &#125; if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Successfully started bean &#x27;&quot; + beanName + &quot;&#x27;&quot;); &#125; &#125; &#125;&#125; 看完了启动，再看一下停止。​ **stop()** 12345678910111213/** * 停止所有注册的bean实现Lifecycle和当前正在运行。 * 任何实现SmartLifecycle bean 都将在其“阶段”内停止， * 并且所有阶段都将从最高值到最低值排序。 * 所有未实现SmartLifecycle将在默认阶段 0 中停止。 * 声明为依赖另一个 bean 的 bean 将在依赖 bean 之前停止，无论声明的阶段如何。 */@Overridepublic void stop() &#123; /*停止所有的bean*/ stopBeans(); this.running = false;&#125; **stopBeans()** 1234567891011121314151617181920212223private void stopBeans() &#123; /*这里其实和启动的时候的逻辑是一样的。*/ Map&lt;String, Lifecycle&gt; lifecycleBeans = getLifecycleBeans(); Map&lt;Integer, LifecycleGroup&gt; phases = new HashMap&lt;&gt;(); lifecycleBeans.forEach((beanName, bean) -&gt; &#123; int shutdownPhase = getPhase(bean); LifecycleGroup group = phases.get(shutdownPhase); if (group == null) &#123; group = new LifecycleGroup(shutdownPhase, this.timeoutPerShutdownPhase, lifecycleBeans, false); phases.put(shutdownPhase, group); &#125; group.add(beanName, bean); &#125;); if (!phases.isEmpty()) &#123; List&lt;Integer&gt; keys = new ArrayList&lt;&gt;(phases.keySet()); /*注意这里是反向排序，启动的时候 1 2 3 4 5 关闭的时候就是 5 4 3 2 1*/ keys.sort(Collections.reverseOrder()); for (Integer key : keys) &#123; /*真正的核心逻辑在这里*/ phases.get(key).stop(); &#125; &#125;&#125; **stop()** 123456789101112131415161718192021222324252627282930313233343536373839404142 public void stop() &#123; /*如果为空，说明没有必要往下走了，直接返回。*/ if (this.members.isEmpty()) &#123; return; &#125; if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Stopping beans in phase &quot; + this.phase); &#125; /*反向排序*/ this.members.sort(Collections.reverseOrder()); /*并发关闭smartLifeCycle*/ CountDownLatch latch = new CountDownLatch(this.smartMemberCount); /*保存当前正处于关闭中的smartLifeCycle*/ Set&lt;String&gt; countDownBeanNames = Collections.synchronizedSet(new LinkedHashSet&lt;&gt;()); /*all 处理器*/ Set&lt;String&gt; lifecycleBeanNames = new HashSet&lt;&gt;(this.lifecycleBeans.keySet()); /*处理本分组内需要关闭的生命周期处理器*/ for (LifecycleGroupMember member : this.members) &#123; if (lifecycleBeanNames.contains(member.name)) &#123; /*真正执行关闭的逻辑*/ doStop(this.lifecycleBeans, member.name, latch, countDownBeanNames); &#125; else if (member.bean instanceof SmartLifecycle) &#123; // Already removed: must have been a dependent bean from another phase latch.countDown(); &#125; &#125; try &#123; /*关闭主线程会在这里等待所有异步关闭的线程关闭完*/ latch.await(this.timeout, TimeUnit.MILLISECONDS); if (latch.getCount() &gt; 0 &amp;&amp; !countDownBeanNames.isEmpty() &amp;&amp; logger.isInfoEnabled()) &#123; logger.info(&quot;Failed to shut down &quot; + countDownBeanNames.size() + &quot; bean&quot; + (countDownBeanNames.size() &gt; 1 ? &quot;s&quot; : &quot;&quot;) + &quot; with phase value &quot; + this.phase + &quot; within timeout of &quot; + this.timeout + &quot;ms: &quot; + countDownBeanNames); &#125; &#125; catch (InterruptedException ex) &#123; Thread.currentThread().interrupt(); &#125; &#125;&#125; **doStop()** 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354private void doStop(Map&lt;String, ? extends Lifecycle&gt; lifecycleBeans, final String beanName, final CountDownLatch latch, final Set&lt;String&gt; countDownBeanNames) &#123; /*吧要关闭的处理器从全局的集合移除掉并返回*/ Lifecycle bean = lifecycleBeans.remove(beanName); if (bean != null) &#123; /*获取依赖当前bean的处理器*/ String[] dependentBeans = getBeanFactory().getDependentBeans(beanName); /*递归关闭依赖的bean*/ for (String dependentBean : dependentBeans) &#123; doStop(lifecycleBeans, dependentBean, latch, countDownBeanNames); &#125; try &#123; if (bean.isRunning()) &#123; if (bean instanceof SmartLifecycle) &#123; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Asking bean &#x27;&quot; + beanName + &quot;&#x27; of type [&quot; + bean.getClass().getName() + &quot;] to stop&quot;); &#125; /*将当前的处理器名添加到countDownBeanNames内，这个集合表示正在关闭的smart bean*/ countDownBeanNames.add(beanName); /*执行smart bean的关闭逻辑，看这个stop可以异步关闭，回调，999*/ ((SmartLifecycle) bean).stop(() -&gt; &#123; latch.countDown(); countDownBeanNames.remove(beanName); if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Bean &#x27;&quot; + beanName + &quot;&#x27; completed its stop procedure&quot;); &#125; &#125;); &#125; else &#123; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Stopping bean &#x27;&quot; + beanName + &quot;&#x27; of type [&quot; + bean.getClass().getName() + &quot;]&quot;); &#125; /*普通生命周期处理器的关闭逻辑*/ bean.stop(); if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Successfully stopped bean &#x27;&quot; + beanName + &quot;&#x27;&quot;); &#125; &#125; &#125; /*如果是已经关闭的smart bean，直接跳过了，相当于...*/ else if (bean instanceof SmartLifecycle) &#123; // Don&#x27;t wait for beans that aren&#x27;t running... latch.countDown(); &#125; &#125; catch (Throwable ex) &#123; if (logger.isWarnEnabled()) &#123; logger.warn(&quot;Failed to stop bean &#x27;&quot; + beanName + &quot;&#x27;&quot;, ex); &#125; &#125; &#125;&#125; 15.4 发布容器创建完成事件**publishEvent(new ContextRefreshedEvent(this))**\u0000 123456789101112131415161718192021222324252627282930protected void publishEvent(Object event, @Nullable ResolvableType eventType) &#123; Assert.notNull(event, &quot;Event must not be null&quot;); // Decorate event as an ApplicationEvent if necessary ApplicationEvent applicationEvent; if (event instanceof ApplicationEvent) &#123; applicationEvent = (ApplicationEvent) event; &#125; else &#123; applicationEvent = new PayloadApplicationEvent&lt;&gt;(this, event); if (eventType == null) &#123; eventType = ((PayloadApplicationEvent&lt;?&gt;) applicationEvent).getResolvableType(); &#125; &#125; // Multicast right now if possible - or lazily once the multicaster is initialized if (this.earlyApplicationEvents != null) &#123; this.earlyApplicationEvents.add(applicationEvent); &#125; else &#123; getApplicationEventMulticaster().multicastEvent(applicationEvent, eventType); &#125; // Publish event via parent context as well... if (this.parent != null) &#123; if (this.parent instanceof AbstractApplicationContext) &#123; ((AbstractApplicationContext) this.parent).publishEvent(event, eventType); &#125; else &#123; this.parent.publishEvent(event); &#125; &#125;&#125; 15.5 注册容器上下文**LiveBeansView.**_**registerApplicationContext**_**(this)**​ 12345678910111213141516171819static void registerApplicationContext(ConfigurableApplicationContext applicationContext) &#123; String mbeanDomain = applicationContext.getEnvironment().getProperty(MBEAN_DOMAIN_PROPERTY_NAME); if (mbeanDomain != null) &#123; synchronized (applicationContexts) &#123; if (applicationContexts.isEmpty()) &#123; try &#123; MBeanServer server = ManagementFactory.getPlatformMBeanServer(); applicationName = applicationContext.getApplicationName(); server.registerMBean(new LiveBeansView(), new ObjectName(mbeanDomain, MBEAN_APPLICATION_KEY, applicationName)); &#125; catch (Throwable ex) &#123; throw new ApplicationContextException(&quot;Failed to register LiveBeansView MBean&quot;, ex); &#125; &#125; applicationContexts.add(applicationContext); &#125; &#125;&#125; \u0000至此，整个**refresh()**的主流程已经完成。对于初始化所有的单实例bean，我将会在下一篇尽量做一个完整的分析。此外针对AOP，事务，三级缓存与循环依赖，加载spring mvc的组件，web环境下一个请求的执行流程，我都会在后续逐行分析源代码，做一些归纳总结。Spring发展至今，已经不单单是一个Java的框架，更是一个生态。","categories":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/tags/Spring/"}]},{"title":"Spring[一]初始化WEB组件","slug":"Spring/Spring[一]初始化Web三大组件","date":"2022-01-11T05:58:37.923Z","updated":"2022-01-11T06:05:07.133Z","comments":true,"path":"2022/01/11/Spring/Spring[一]初始化Web三大组件/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/Spring/Spring[%E4%B8%80]%E5%88%9D%E5%A7%8B%E5%8C%96Web%E4%B8%89%E5%A4%A7%E7%BB%84%E4%BB%B6/","excerpt":"","text":"SpringMVC最后是通过Tomcat来进行部署的。当在Servlet中进行进行应用部署时，主要步骤为:When a web application is deployed into a container, the following steps must be performed, in this order, before the web application begins processing client requests. Instantiate an instance of each event listener identified by a element in the deployment descriptor. For instantiated listener instances that implement ServletContextListener , call the contextInitialized() method. Instantiate an instance of each filter identified by a element in the deployment descriptor and call each filter instance’s init() method. Instantiate an instance of each servlet identified by a element that includes a element in the order defined by the load-on-startup element values, and call each servlet instance’s init() method. 当应用部署到容器时，在应用相应客户的请求之前，需要执行以下步骤： 创建并初始化由元素标记的事件监听器。 对于事件监听器，如果实现了ServletContextListener接口，那么调用其contextInitialized()方法。 创建和初始化由元素标记的过滤器，并调用其init()方法。 根据中定义的顺序创建和初始化由元素标记的servlet，并调用其init()方法。所以在Tomcat下部署的应用，会先初始化listener，然后初始化filter，最后初始化servlet。 现在根据配置文件和Oracle所说的初始化流程分析一下，SpringMVC是如何来一步步启动容器，并加载相关信息的。 配置文件12345678910111213141516171819202122232425262728293031323334353637383940414243444546&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;web-app xmlns=&quot;http://xmlns.jcp.org/xml/ns/javaee&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_3_1.xsd&quot; version=&quot;3.1&quot;&gt; &lt;!--告诉加载器，去这个位置去加载spring的相关配置--&gt; &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:spring-mvc.xml&lt;/param-value&gt; &lt;/context-param&gt; &lt;!--配置前端控制器--&gt; &lt;servlet&gt; &lt;servlet-name&gt;springMvc&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;!--SpringMVC配置文件--&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:spring-mvc.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;0&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;springMvc&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt; &lt;/listener&gt; &lt;!--解决乱码问题的filter--&gt; &lt;filter&gt; &lt;filter-name&gt;CharacterEncodingFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;utf-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;CharacterEncodingFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt;&lt;/web-app&gt; 1.初始化Listener我们在配置文件定义的Listener类是ContextLoaderListener. 继承关系：ContextLoaderListener 类继承了 ContextLoader 类并且实现了 ServletContextListener 接口，按照启动程序，会调用其 contextInitialized() 方法。 123456789/** * 这个方法：初始化应用上下文 * @param event */@Overridepublic void contextInitialized(ServletContextEvent event) &#123; //这里就进入了初始化web容器的核心 initWebApplicationContext(event.getServletContext());&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475/** * 初始化web应用的上下文 * ServletContext官方叫servlet上下文。服务器会为每一个工程创建一个对象，这个对象就是ServletContext对象。 * 这个对象全局唯一，而且工程内部的所有servlet都共享这个对象。所以叫全局应用程序共享对象。 * @param servletContext servlet 上下文 * @return */public WebApplicationContext initWebApplicationContext(ServletContext servletContext) &#123; /* * 首先通过 WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE 这个String类型的静态变量获取一个Root ioc 容器， * 根据 ioc 容器作为全局变量存储在 application 对象中，如果存在则有且只能有一个 * * 如果在初始化 Root WebApplicationContext 即， Root ioc 容器的时候发现已经存在，则直接抛出异常， * 因此web.xml中只允许存在一个ContextLoader类或其子类对象。 * */ if (servletContext.getAttribute(WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE) != null) &#123; throw new IllegalStateException( &quot;Cannot initialize context because there is already a root application context present - &quot; + &quot;check whether you have multiple ContextLoader* definitions in your web.xml!&quot;); &#125; servletContext.log(&quot;Initializing Spring root WebApplicationContext&quot;); Log logger = LogFactory.getLog(ContextLoader.class); if (logger.isInfoEnabled()) &#123; logger.info(&quot;Root WebApplicationContext: initialization started&quot;); &#125; long startTime = System.currentTimeMillis(); try &#123; /*如果context不存在，则直接进行创建*/ if (this.context == null) &#123; /*step into*/ this.context = createWebApplicationContext(servletContext); &#125; if (this.context instanceof ConfigurableWebApplicationContext) &#123; ConfigurableWebApplicationContext cwac = (ConfigurableWebApplicationContext) this.context; if (!cwac.isActive()) &#123; // The context has not yet been refreshed -&gt; provide services such as // setting the parent context, setting the application context id, etc if (cwac.getParent() == null) &#123; // The context instance was injected without an explicit parent -&gt; // determine parent for root web application context, if any. ApplicationContext parent = loadParentContext(servletContext); cwac.setParent(parent); &#125; /*配置并刷新应用的root ioc 容器，这里会进行bean的创建和初始化工作， * 这里最终会调用 AbstractApplicationContext的refresh方法。 * 并且ioc容器中的bean类会被放在application中。*/ configureAndRefreshWebApplicationContext(cwac, servletContext); &#125; &#125; //以属性的配置方式将application配置servletContext中，因为servletContext是整个应用唯一的，所以可以根据key值获取到application，从而能够获取到应用的所有信息 servletContext.setAttribute(WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE, this.context); ClassLoader ccl = Thread.currentThread().getContextClassLoader(); if (ccl == ContextLoader.class.getClassLoader()) &#123; currentContext = this.context; &#125; else if (ccl != null) &#123; currentContextPerThread.put(ccl, this.context); &#125; if (logger.isInfoEnabled()) &#123; long elapsedTime = System.currentTimeMillis() - startTime; logger.info(&quot;Root WebApplicationContext initialized in &quot; + elapsedTime + &quot; ms&quot;); &#125; return this.context; &#125; catch (RuntimeException | Error ex) &#123; logger.error(&quot;Context initialization failed&quot;, ex); servletContext.setAttribute(WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE, ex); throw ex; &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435protected void configureAndRefreshWebApplicationContext(ConfigurableWebApplicationContext wac, ServletContext sc) &#123; if (ObjectUtils.identityToString(wac).equals(wac.getId())) &#123; // The application context id is still set to its original default value // -&gt; assign a more useful id based on available information String idParam = sc.getInitParameter(CONTEXT_ID_PARAM); if (idParam != null) &#123; wac.setId(idParam); &#125; else &#123; // Generate default id... wac.setId(ConfigurableWebApplicationContext.APPLICATION_CONTEXT_ID_PREFIX + ObjectUtils.getDisplayString(sc.getContextPath())); &#125; &#125; //将ServletContext设置到application的属性中 wac.setServletContext(sc); //获取web.xml中配置的contextConfigLocation参数值 String configLocationParam = sc.getInitParameter(CONFIG_LOCATION_PARAM); if (configLocationParam != null) &#123; wac.setConfigLocation(configLocationParam); &#125; // The wac environment&#x27;s #initPropertySources will be called in any case when the context // is refreshed; do it eagerly here to ensure servlet property sources are in place for // use in any post-processing or initialization that occurs below prior to #refresh ConfigurableEnvironment env = wac.getEnvironment(); if (env instanceof ConfigurableWebEnvironment) &#123; ((ConfigurableWebEnvironment) env).initPropertySources(sc, null); &#125; customizeContext(sc, wac); //调用应用的refresh方法，进行IOC容器的装载 wac.refresh();&#125; 思考与沉淀我们在web.xml配置文件定义的Listener类是ContextLoaderListener类.继承关系：ContextLoaderListener 类继承了 ContextLoader 类并且实现了 ServletContextListener 接口，按照启动程序，会调用其 contextInitialized() 方法。这个方法主要是调用了initWebApplicationContext()来初始化web应用的上下文，再通过configureAndRefreshWebApplicationContext()加载web.xml里面指定的配置文件，然后调用ioc容器的刷新方法。 2.初始化Filter在完成了对于 listener 的初始化操作以后，会进行 filter 的创建和初始化操作。我们这里使用的是 CharacterEncodingFilter 。我们先看一下这个类的具体类图信息 ctrl+H。 因为其实现了 Filter 接口，所以会调用其对应的 init(FilterConfig filterConfig) 方法。在其父类 GenericFilterBean 中有该方法的实现。 123456789101112131415161718192021222324252627282930313233343536373839404142/** * filter的初始化方法 * @param filterConfig * @throws ServletException */@Overridepublic final void init(FilterConfig filterConfig) throws ServletException &#123; Assert.notNull(filterConfig, &quot;FilterConfig must not be null&quot;); this.filterConfig = filterConfig; // 将设置的初始化参数信息设置到pvs中 PropertyValues pvs = new FilterConfigPropertyValues(filterConfig, this.requiredProperties); if (!pvs.isEmpty()) &#123; try &#123; //将具体的filter类进行包装 BeanWrapper bw = PropertyAccessorFactory.forBeanPropertyAccess(this); //创建对应的资源加载器 ResourceLoader resourceLoader = new ServletContextResourceLoader(filterConfig.getServletContext()); Environment env = this.environment; if (env == null) &#123; env = new StandardServletEnvironment(); &#125; bw.registerCustomEditor(Resource.class, new ResourceEditor(resourceLoader, env)); initBeanWrapper(bw); bw.setPropertyValues(pvs, true); &#125; catch (BeansException ex) &#123; String msg = &quot;Failed to set bean properties on filter &#x27;&quot; + filterConfig.getFilterName() + &quot;&#x27;: &quot; + ex.getMessage(); logger.error(msg, ex); throw new NestedServletException(msg, ex); &#125; &#125; // 交给子类实现 initFilterBean(); if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Filter &#x27;&quot; + filterConfig.getFilterName() + &quot;&#x27; configured for use&quot;); &#125;&#125; 123protected void initFilterBean() throws ServletException &#123; //这里并未进行任何的初始化操作。其实Filter的主要作用还是在有请求过来时，进行的 doFilter() 中的处理，在启动阶段，处理比较少。&#125; 思考与沉淀在完成了对于 listener 的初始化操作以后，会进行 filter 的创建和初始化操作。通常在web.xml文件配置的是：CharacterEncodingFilter 。通过这个类的继承关系，再结合实际源码分析：因为其实现了 Filter 接口，所以会调用其对应的 init(FilterConfig filterConfig) 方法。在其父类 GenericFilterBean 中有该方法的实现。然后又交给了子类，但是实际上啥也没做，其实Filter的主要作用还是在有请求过来时，进行的 doFilter() 中的处理，在启动阶段，处理比较少。 3.初始化Servletweb应用启动的最后一个步骤就是创建和初始化 Servlet ，我们就从我们使用的 DispatcherServlet 这个类来进行分析，这个类是前端控制器，主要用于分发用户请求到具体的实现类，并返回具体的响应信息。 这里面有一个看源码的技巧，当分析到某一个类的时候，不知道如何再往下分析了，也就是缺少相应的抓手，怎么办？ 1.看类的继承关系，从类的继承关系触发，寻找相应的方法 2.看初始化这种的方法 ctrl+H 查看类的继承关系，DispatcherServlet 实现了Servlet 接口，所以按照加载过程，最终会调用其 init(ServletConfig config)方法。从DispatcherServlet中寻找init方法发现没有，这个时候怎么办？按照类的继承关系，逐步向上寻找。最终定位到 GenericServlet 中，但是这里什么也没有做，只是交给了子类去实现，那就继续寻找，最终定位到抓手：HttpServletBean。 123456789101112131415161718192021222324252627282930313233/** * 这个方法几乎是web-ioc容器的核心入口 * 当前类是DispatcherServlet的爷爷类 * 加载DispatcherServlet的时候，他的爷爷类会先加载的并且 init方法会执行 * @throws ServletException */@Overridepublic final void init() throws ServletException &#123; // 设置属性信息 PropertyValues pvs = new ServletConfigPropertyValues(getServletConfig(), this.requiredProperties); if (!pvs.isEmpty()) &#123; try &#123; //使用装饰器模式，对具体的实现类进行一个包装 BeanWrapper bw = PropertyAccessorFactory.forBeanPropertyAccess(this); ResourceLoader resourceLoader = new ServletContextResourceLoader(getServletContext()); bw.registerCustomEditor(Resource.class, new ResourceEditor(resourceLoader, getEnvironment())); //将web.xml里面的属性信息设置到 bw 里面。 initBeanWrapper(bw); bw.setPropertyValues(pvs, true); &#125; catch (BeansException ex) &#123; if (logger.isErrorEnabled()) &#123; logger.error(&quot;Failed to set bean properties on servlet &#x27;&quot; + getServletName() + &quot;&#x27;&quot;, ex); &#125; throw ex; &#125; &#125; // Let subclasses do whatever initialization they like. //交给子类来实现 initServletBean();&#125; 来到子类：FrameworkServlet。 12345678910111213141516171819202122232425262728293031@Overrideprotected final void initServletBean() throws ServletException &#123; getServletContext().log(&quot;Initializing Spring &quot; + getClass().getSimpleName() + &quot; &#x27;&quot; + getServletName() + &quot;&#x27;&quot;); if (logger.isInfoEnabled()) &#123; logger.info(&quot;Initializing Servlet &#x27;&quot; + getServletName() + &quot;&#x27;&quot;); &#125; long startTime = System.currentTimeMillis(); try &#123; //初始化web应用容器 this.webApplicationContext = initWebApplicationContext(); //初始化框架servlet initFrameworkServlet(); &#125; catch (ServletException | RuntimeException ex) &#123; logger.error(&quot;Context initialization failed&quot;, ex); throw ex; &#125; if (logger.isDebugEnabled()) &#123; String value = this.enableLoggingRequestDetails ? &quot;shown which may lead to unsafe logging of potentially sensitive data&quot; : &quot;masked to prevent unsafe logging of potentially sensitive data&quot;; logger.debug(&quot;enableLoggingRequestDetails=&#x27;&quot; + this.enableLoggingRequestDetails + &quot;&#x27;: request parameters and headers will be &quot; + value); &#125; if (logger.isInfoEnabled()) &#123; logger.info(&quot;Completed initialization in &quot; + (System.currentTimeMillis() - startTime) + &quot; ms&quot;); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061/** * 这里主要是进行了web应用容器上下文的创建，并进行了初始化工作。 * 跟踪一下初始化的具体流程 * @return */protected WebApplicationContext initWebApplicationContext() &#123; //获取到 root ioc 容器 WebApplicationContext rootContext = WebApplicationContextUtils.getWebApplicationContext(getServletContext()); WebApplicationContext wac = null; if (this.webApplicationContext != null) &#123; // A context instance was injected at construction time -&gt; use it wac = this.webApplicationContext; if (wac instanceof ConfigurableWebApplicationContext) &#123; ConfigurableWebApplicationContext cwac = (ConfigurableWebApplicationContext) wac; if (!cwac.isActive()) &#123; // The context has not yet been refreshed -&gt; provide services such as // setting the parent context, setting the application context id, etc if (cwac.getParent() == null) &#123; //将 root ioc 容器设置成 servlet 的ioc容器的父类 //如果当前servlet存在一个 WebApplicationContext 即：子IOC容器 //并且上下文获取的root ioc 容器存在，则将root ioc 容器作为子 ioc 容器的父容器 cwac.setParent(rootContext); &#125; //配置并刷新子容器，加载子容器中对应的bean实体类 configureAndRefreshWebApplicationContext(cwac); &#125; &#125; &#125; //如果当前servlet中不存在子ioc容器，则去查找 if (wac == null) &#123; // No context instance was injected at construction time -&gt; see if one // has been registered in the servlet context. If one exists, it is assumed // that the parent context (if any) has already been set and that the // user has performed any initialization such as setting the context id wac = findWebApplicationContext(); &#125; if (wac == null) &#123; // No context instance is defined for this servlet -&gt; create a local one //如果查找不到，就去创建一个 wac = createWebApplicationContext(rootContext); &#125; if (!this.refreshEventReceived) &#123; // Either the context is not a ConfigurableApplicationContext with refresh // support or the context injected at construction time had already been // refreshed -&gt; trigger initial onRefresh manually here. synchronized (this.onRefreshMonitor) &#123; onRefresh(wac); &#125; &#125; if (this.publishContext) &#123; // Publish the context as a servlet context attribute. String attrName = getServletContextAttributeName(); getServletContext().setAttribute(attrName, wac); &#125; return wac;&#125; 12345678910111213141516171819202122232425protected WebApplicationContext createWebApplicationContext(@Nullable ApplicationContext parent) &#123; Class&lt;?&gt; contextClass = getContextClass(); if (!ConfigurableWebApplicationContext.class.isAssignableFrom(contextClass)) &#123; throw new ApplicationContextException( &quot;Fatal initialization error in servlet with name &#x27;&quot; + getServletName() + &quot;&#x27;: custom WebApplicationContext class [&quot; + contextClass.getName() + &quot;] is not of type ConfigurableWebApplicationContext&quot;); &#125; //根据类信息初始化一个ConfigurableWebApplicationContext对象 ConfigurableWebApplicationContext wac = (ConfigurableWebApplicationContext) BeanUtils.instantiateClass(contextClass); //设置web上下文环境信息 wac.setEnvironment(getEnvironment()); //设置其父类为root ioc 容器，root ioc 容器是整个应用唯一的。 wac.setParent(parent); //设置其具体的配置信息的位置，这里是 classpath:spring-mvc.xml String configLocation = getContextConfigLocation(); if (configLocation != null) &#123; wac.setConfigLocation(configLocation); &#125; //配置并刷新web应用的ioc容器 configureAndRefreshWebApplicationContext(wac); return wac;&#125; 123456789101112131415161718192021222324252627282930313233343536protected void configureAndRefreshWebApplicationContext(ConfigurableWebApplicationContext wac) &#123; if (ObjectUtils.identityToString(wac).equals(wac.getId())) &#123; // The application context id is still set to its original default value // -&gt; assign a more useful id based on available information if (this.contextId != null) &#123; wac.setId(this.contextId); &#125; else &#123; // Generate default id... wac.setId(ConfigurableWebApplicationContext.APPLICATION_CONTEXT_ID_PREFIX + ObjectUtils.getDisplayString(getServletContext().getContextPath()) + &#x27;/&#x27; + getServletName()); &#125; &#125; //配置容器的相关信息 wac.setServletContext(getServletContext()); wac.setServletConfig(getServletConfig()); wac.setNamespace(getNamespace()); //配置容器应用加载的监听器 wac.addApplicationListener(new SourceFilteringListener(wac, new ContextRefreshListener())); // The wac environment&#x27;s #initPropertySources will be called in any case when the context // is refreshed; do it eagerly here to ensure servlet property sources are in place for // use in any post-processing or initialization that occurs below prior to #refresh ConfigurableEnvironment env = wac.getEnvironment(); if (env instanceof ConfigurableWebEnvironment) &#123; ((ConfigurableWebEnvironment) env).initPropertySources(getServletContext(), getServletConfig()); &#125; postProcessWebApplicationContext(wac); //应用初始化信息 applyInitializers(wac); //刷新加载里面的bean实体类 wac.refresh();&#125; 这里其实可以看到，在这个其实主要是根据配置文件信息进行类加载的工作，并且配置了一个容器加载信息的监听器 SourceFilteringListener。在最后通过 refresh 方法进行了容器中实体类的加载过程。这个refresh方法和我们在listener中实现类的初始化过程使用的是同一个方法。到此为止，在我们应用中配置的所有的类都能够扫描到，并且配置了我们的ioc容器中。因为我们配置了相关的容器加载的监听器，在refresh方法中调用了 finishRefresh 方法时，发送对应的容器加载完成广播信息，从而能够调用我们所注册的监听器 SourceFilteringListener。看一下里面的逻辑~ 12345678protected void onApplicationEventInternal(ApplicationEvent event) &#123; if (this.delegate == null) &#123; throw new IllegalStateException( &quot;Must specify a delegate object or override the onApplicationEventInternal method&quot;); &#125; //这里的delegate，是传入的具体的代理类，所以在此回到了我们的FrameworkServlet this.delegate.onApplicationEvent(event);&#125; 1234567891011public void onApplicationEvent(ContextRefreshedEvent event) &#123; this.refreshEventReceived = true; synchronized (this.onRefreshMonitor) &#123; //最终调用了FrameworkServlet的onApplicationEvent方法 onRefresh(event.getApplicationContext()); &#125;&#125;protected void onRefresh(ApplicationContext context) &#123; // 又是一个扩展点，交给子类去实现&#125; 终于来到了我们的DispatcherServlet。 12345@Overrideprotected void onRefresh(ApplicationContext context) &#123; //通过重写父类的扩展点来到这里 initStrategies(context);&#125; 123456789101112131415161718192021222324252627282930/** * 初始化servlet使用的策略信息，子类可以通过覆写该方法类增加更多的呃策略方法 * Initialize the strategy objects that this servlet uses. * &lt;p&gt;May be overridden in subclasses in order to initialize further strategy objects. */protected void initStrategies(ApplicationContext context) &#123; //初始化MultipartResolver,可以支持文件的上传 initMultipartResolver(context); //初始化本地解析器 initLocaleResolver(context); //初始化主题解析器 initThemeResolver(context); //处理器映射器，将请求和方法进行映射关联 initHandlerMappings(context); //处理器适配器 initHandlerAdapters(context); //处理器异常解析器 initHandlerExceptionResolvers(context); //从请求到视图名的转换器 initRequestToViewNameTranslator(context); //视图解析器 initViewResolvers(context); //FlashMap管理器 initFlashMapManager(context); /* * 可以看到里面主要是初始化了我们的所使用到的一些解析器和处理器等。 * 当接收到请求后，就可以根据这些解析器来进行请求的解析处理、方法的调用、异常的处理等等。 * 到此为止，Servlet的初始化工作就整个完成了。想当的复杂，主要是将很多的方法实现在父类中进行了处理。层级比较复杂，需要一点点跟踪分析。 */&#125; 思考与沉淀web应用启动的最后一个步骤就是创建和初始化 Servlet ，我们就从我们使用的 DispatcherServlet 这个类来进行分析，这个类是前端控制器，主要用于分发用户请求到具体的实现类，并返回具体的响应信息。看这个类的继承关系：在HttpServlet（他的爷爷类）中，实现了servlet的init方法（其实再往里追源码，实际是从源码里面实现的，算是一个扩展点吧），这里主要是委派给了子类FrameworkServlet（dispatcherServlet的父类）的initServletBean()，这个方法主要是初始化web应用。通过initWebApplicationContext()配置并刷新子容器，这里其实可以看到，在这个其实主要是根据配置文件信息进行类加载的工作，并且配置了一个容器加载信息的监听器 SourceFilteringListener。在最后通过 refresh 方法进行了容器中实体类的加载过程。这个refresh方法和我们在listener中实现类的初始化过程使用的是同一个方法。到此为止，在我们应用中配置的所有的类都能够扫描到，并且配置了我们的ioc容器中。因为我们配置了相关的容器加载的监听器，在refresh方法中调用了 finishRefresh 方法时，发送对应的容器加载完成广播信息，从而能够调用我们所注册的监听器 SourceFilteringListener。看一下里面的逻辑~这里面实际上又调用了传入的代理类的onRefresh()。这个代理类是谁？其实就是DispatcherServlet。他的onRefresh()实际上就是调用了initStrategies()，可以看到里面主要是初始化了我们的所使用到的一些解析器和处理器等。当接收到请求后，就可以根据这些解析器来进行请求的解析处理、方法的调用、异常的处理等等。到此为止，Servlet的初始化工作就整个完成了。","categories":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/tags/Spring/"}]},{"title":"MySQL[十四]redo日志","slug":"MySQL/MySQL[十四]redo日志","date":"2022-01-11T03:17:34.284Z","updated":"2022-01-11T03:25:35.265Z","comments":true,"path":"2022/01/11/MySQL/MySQL[十四]redo日志/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/MySQL/MySQL[%E5%8D%81%E5%9B%9B]redo%E6%97%A5%E5%BF%97/","excerpt":"","text":"一，什么是redo日志InnoDB存储引擎是以页为单位来管理存储空间的，我们的CRUD操作其实都是在访问页面。在真正访问页面之前，需要把磁盘中的页加载到内存的BufferPool，之后才能访问，但是因为事务要保持持久性，如果我们仅仅在内存的缓冲池修改了页面，假设事务提交后突然发生故障，导致内存的数据都消失了，那么这个已经提交的事务在数据库做的更改就丢失了。 如何保证持久性呢？可以在事务提交完成之前，把事务修改的所有页面都刷新到磁盘。不过这样做存在一些问题： 刷新一个完整的数据页过于浪费 随机IO效率比较低 事实上仅仅是为了保证事务的持久性，没有必要每次提交事务的时候就把该事务在内存修改过的全部页面刷新到磁盘，只需要把修改的内容记录一下就好，这样在事务提交的时候，就会把这个记录刷新到磁盘。即使系统因为崩溃而重启只需要按照记录的内容重新更新数据页即可恢复数据，上述记录修改的内容就叫做重做日志（redo log）。 相比于在事务提交的时候将所有修改过的内存中的页面刷新到磁盘，重做日志有以下好处： redo日志占用空间小：在存储表空间ID，页号，偏移量以及需要更新的值时，需要的存储空间很小。 redo日志是顺序写入磁盘的：在执行事务过程中，每执行一条语句，就可能产生若干条redo日志，这些日志是按照产生的顺序写入磁盘的，也就是使用顺序IO。 二，redo日志格式重做日志本质上仅仅是记录了一下事务对数据库进行了哪些修改。针对事务对数据库的不同修改场景MySQL定义了很多种重做日志，但是大部分类型的重做日志都有以下的通用结构。 type 重做日志的类型 space ID 表空间ID page number 页号 Data 日志的具体内容 1. 简单的redo日志类型行格式里面有一个隐藏列叫做row_id。为row_id进行赋值的方式如下： 服务器会在内存中维护一个全局变量，每当像某个包含row_id隐藏列的表插入一条记录的时候，就会把这个全局变量的值当做新记录row_id的值，并且把这个全局变量自增1。 每当这个全局变量的值是256的整数倍的时候，就会把这个变量的值刷新到系统表空间页号为7的页面中一个叫做Max Row ID的属性中。 当系统启动的时候，会将Max Row ID属性加载到内存，并把这个值加上256之后赋值给前面提到的全局变量。 这个Max Row ID占用的存储空间是8字节。当某个事务向某个包含row_id的表插入一条记录并且该记录分配的row_id值为256的整数倍的时候，就会像系统表空间页号为7的页面的相应偏移量处写入8字节的值。但是这个写入操作实际上是在内存缓冲区完成的，我们需要把这次修改以redo日志的形式记录下来，这样在事务提交之后，即使系统崩溃，也可以将该页面恢复成崩溃前的状态。在这种对页面的修改特别简单的时候，重做日志仅仅需要记录一下在某个页面的某个偏移量处修改了几个字节的值，具体修改后的内容是什么就可以了。这也叫做物理日志。 offset表示页面中的偏移量。如果写入的是字节序列类型的重做日志，还需要有一个len属性记录实际写入的长度。 2.复杂的redo日志类型有时候执行一条语句会修改非常多的页面，包括系统数据页面和用户数据页面（用户数据指的就是聚簇索引和二级索引对应的B+树）。 这时我们如果使用简单的物理redo日志来记录这些修改时，可以有两种解决方案： 方案一：在每个修改的地方都记录一条redo日志。也就是有多少个修改的记录，就写多少条物理redo日志。这样子记录redo日志的缺点是显而易见的，因为被修改的地方是在太多了，可能记录的redo日志占用的空间都比整个页面占用的空间都多。 方案二：将整个页面的第一个被修改的字节到最后一个修改的字节之间所有的数据当成是一条物理redo日志中的具体数据。第一个被修改的字节到最后一个修改的字节之间仍然有许多没有修改过的数据，我们把这些没有修改的数据也加入到redo日志中太浪费了。 正因为上述两种使用物理redo日志的方式来记录某个页面中做了哪些修改比较浪费，InnoDB提出了一些新的redo日志类型。 这些类型的redo日志既包含物理层面的意思，也包含逻辑层面的意思，具体指： 物理层面看，这些日志都指明了对哪个表空间的哪个页进行了修改。 逻辑层面看，在系统崩溃重启时，并不能直接根据这些日志里的记载，将页面内的某个偏移量处恢复成某个数据，而是需要调用一些事先准备好的函数，执行完这些函数后才可以将页面恢复成系统崩溃前的样子。 这个类型为MLOG_COMP_REC_INSERT的redo日志并没有记录PAGE_N_DIR_SLOTS的值修改为了什么，PAGE_HEAP_TOP的值修改为了什么，PAGE_N_HEAP的值修改为了什么等等这些信息，而只是把在本页面中插入一条记录所有必备的要素记了下来，之后系统崩溃重启时，服务器会调用相关向某个页面插入一条记录的那个函数，而redo日志中的那些数据就可以被当成是调用这个函数所需的参数，在调用完该函数后，页面中的PAGE_N_DIR_SLOTS、PAGE_HEAP_TOP、PAGE_N_HEAP等等的值也就都被恢复到系统崩溃前的样子了。这就是所谓的逻辑日志的意思。 日志格式说了一堆核心其实就是：重做日志会把事务执行过程中对数据库所做的所有修改都记录下来，在之后系统因为崩溃而重启后可以把事务所做的任何修改都恢复过来。 为了节省重做日志占用的空间大小，InnoDB还对重做日志中的某些数据进行了压缩处理，比如表空间ID&amp;page number 一般占用4字节来存储，但是经过压缩之后占用的空间就更小了。 三，Mini-Transcation1.以组的形式写入redo日志语句在执行过程中可能修改若干个页面。比如我们前边说的一条INSERT语句可能修改系统表空间页号为7的页面的Max Row ID属性（当然也可能更新别的系统页面，只不过我们没有都列举出来而已），还会更新聚簇索引和二级索引对应B+树中的页面。由于对这些页面的更改都发生在Buffer Pool中，所以在修改完页面之后，需要记录一下相应的redo日志。在执行语句的过程中产生的redo日志被InnoDB人为的划分成了若干个不可分割的组，比如： 更新Max Row ID属性时产生的redo日志是不可分割的。 向聚簇索引对应B+树的页面中插入一条记录时产生的redo日志是不可分割的。 向某个二级索引对应B+树的页面中插入一条记录时产生的redo日志是不可分割的。 还有其他的一些对页面的访问操作时产生的redo日志是不可分割的。。。 怎么理解这个不可分割的意思呢？我们以向某个索引对应的B+树插入一条记录为例，在向B+树中插入这条记录之前，需要先定位到这条记录应该被插入到哪个叶子节点代表的数据页中，定位到具体的数据页之后，有两种可能的情况： 情况一：该数据页的剩余的空闲空间充足，足够容纳这一条待插入记录，那么事情很简单，直接把记录插入到这个数据页中，记录一条类型为MLOG_COMP_REC_INSERT的redo日志就好了，我们把这种情况称之为乐观插入。假如某个索引对应的B+树长这样：现在我们要插入一条键值为10的记录，很显然需要被插入到页b中，由于页b现在有足够的空间容纳一条记录，所以直接将该记录插入到页b中就好了，就像这样： 情况二：该数据页剩余的空闲空间不足，那么事情就悲剧了，我们前边说过，遇到这种情况要进行所谓的页分裂操作，也就是新建一个叶子节点，然后把原先数据页中的一部分记录复制到这个新的数据页中，然后再把记录插入进去，把这个叶子节点插入到叶子节点链表中，最后还要在内节点中添加一条目录项记录指向这个新创建的页面。很显然，这个过程要对多个页面进行修改，也就意味着会产生多条redo日志，我们把这种情况称之为悲观插入。假如某个索引对应的B+树长这样：现在我们要插入一条键值为10的记录，很显然需要被插入到页b中，但是从图中也可以看出来，此时页b已经塞满了记录，没有更多的空闲空间来容纳这条新记录了，所以我们需要进行页面的分裂操作，就像这样：如果作为内节点的页a的剩余空闲空间也不足以容纳增加一条目录项记录，那需要继续做内节点页a的分裂操作，也就意味着会修改更多的页面，从而产生更多的redo日志。另外，对于悲观插入来说，由于需要新申请数据页，还需要改动一些系统页面，比方说要修改各种段、区的统计信息信息，各种链表的统计信息（比如什么FREE链表、FSP_FREE_FRAG链表等，我们在介绍表空间那一篇中介绍过的各种东西），反正总共需要记录的redo日志有二、三十条。 其实不光是悲观插入一条记录会生成许多条redo日志，InnoDB为了其他的一些功能，在乐观插入时也可能产生多条redo日志。 InnoDB认为向某个索引对应的B+树中插入一条记录的这个过程必须是原子的，不能说插了一半之后就停止了。比方说在悲观插入过程中，新的页面已经分配好了，数据也复制过去了，新的记录也插入到页面中了，可是没有向内节点中插入一条目录项记录，这个插入过程就是不完整的，这样会形成一棵不正确的B+树。我们知道redo日志是为了在系统崩溃重启时恢复崩溃前的状态，如果在悲观插入的过程中只记录了一部分redo日志，那么在系统崩溃重启时会将索引对应的B+树恢复成一种不正确的状态，这是InnoDB所不能忍受的。所以他们规定在执行这些需要保证原子性的操作时必须以组的形式来记录的redo日志，在进行系统崩溃重启恢复时，针对某个组中的redo日志，要么把全部的日志都恢复掉，要么一条也不恢复。怎么做到的呢？这得分情况讨论： 有的需要保证原子性的操作会生成多条redo日志，比如向某个索引对应的B+树中进行一次悲观插入就需要生成许多条redo日志。如何把这些redo日志划分到一个组里边儿呢？InnoDB做了一个很简单的操作，就是在该组中的最后一条redo日志后边加上一条特殊类型的redo日志，该类型名称为MLOG_MULTI_REC_END，type字段对应的十进制数字为31，该类型的redo日志结构很简单，只有一个type字段：所以某个需要保证原子性的操作产生的一系列redo日志必须要以一个类型为MLOG_MULTI_REC_END结尾，就像这样：这样在系统崩溃重启进行恢复时，只有当解析到类型为MLOG_MULTI_REC_END的redo日志，才认为解析到了一组完整的redo日志，才会进行恢复。否则的话直接放弃前边解析到的redo日志。 有的需要保证原子性的操作只生成一条redo日志，比如更新Max Row ID属性的操作就只会生成一条redo日志。其实在一条日志后边跟一个类型为MLOG_MULTI_REC_END的redo日志也是可以的，InnoDB不想浪费一个比特位。虽然redo日志的类型比较多，但撑死了也就是几十种，是小于127这个数字的，也就是说我们用7个比特位就足以包括所有的redo日志类型，而type字段其实是占用1个字节的，也就是说我们可以省出来一个比特位用来表示该需要保证原子性的操作只产生单一的一条redo日志，示意图如下：如果type字段的第一个比特位为1，代表该需要保证原子性的操作只产生了单一的一条redo日志，否则表示该需要保证原子性的操作产生了一系列的redo日志。 2.Mini-TransactionMySQL把对底层页面中的一次原子访问的过程称之为一个Mini-Transaction，简称mtr，比如上边所说的修改一次Max Row ID的值算是一个Mini-Transaction，向某个索引对应的B+树中插入一条记录的过程也算是一个Mini-Transaction。一个所谓的mtr可以包含一组redo日志，在进行崩溃恢复时这一组redo日志作为一个不可分割的整体。 一个事务可以包含若干条语句，每一条语句其实是由若干个mtr组成，每一个mtr又可以包含若干条redo日志，画个图表示它们的关系就是这样： 四，redo日志的写入过程1.redo log blockInnoDB为了更好的进行系统崩溃恢复，他们把通过mtr生成的redo日志都放在了大小为512字节的页中。为了和表空间中的页做区别，我们这里把用来存储redo日志的页称为block。一个redo log block的示意图如下： 真正的redo日志都是存储到占用496字节大小的log block body中，图中的log block header和log block trailer存储的是一些管理信息。 其中log block header的几个属性的意思分别如下： LOG_BLOCK_HDR_NO：每一个block都有一个大于0的唯一标号，本属性就表示该标号值。 LOG_BLOCK_HDR_DATA_LEN：表示block中已经使用了多少字节，初始值为12（因为log block body从第12个字节处开始）。随着往block中写入的redo日志越来也多，本属性值也跟着增长。如果log block body已经被全部写满，那么本属性的值被设置为512。 LOG_BLOCK_FIRST_REC_GROUP：一条redo日志也可以称之为一条redo日志记录（redo log record），一个mtr会生产多条redo日志记录，这些redo日志记录被称之为一个redo日志记录组（redo log record group）。LOG_BLOCK_FIRST_REC_GROUP就代表该block中第一个mtr生成的redo日志记录组的偏移量（其实也就是这个block里第一个mtr生成的第一条redo日志的偏移量）。 LOG_BLOCK_CHECKPOINT_NO：表示所谓的checkpoint的序号，checkpoint是我们后续内容的重点，现在先不用清楚它的意思，稍安勿躁。 log block trailer中属性的意思如下： LOG_BLOCK_CHECKSUM：表示block的校验值，用于正确性校验，我们暂时不关心它。 2.redo 日志缓冲区InnoDB为了解决磁盘速度过慢的问题而引入了Buffer Pool。同理，写入redo日志时也不能直接直接写到磁盘上，实际上在服务器启动时就向操作系统申请了一大片称之为redo log buffer的连续内存空间，翻译成中文就是redo日志缓冲区，也可以简称为log buffer。这片内存空间被划分成若干个连续的redo log block，就像这样： 我们可以通过启动参数innodb_log_buffer_size来指定log buffer的大小，在MySQL 5.7.21这个版本中，该启动参数的默认值为16MB。 3.redo log 日志写入log buffer向log buffer中写入redo日志的过程是顺序的，也就是先往前边的block中写，当该block的空闲空间用完之后再往下一个block中写。当我们想往log buffer中写入redo日志时，第一个遇到的问题就是应该写在哪个block的哪个偏移量处，所以InnoDB特意提供了一个称之为buf_free的全局变量，该变量指明后续写入的redo日志应该写入到log buffer中的哪个位置，如图所示： 一个mtr执行过程中可能产生若干条redo日志，这些redo日志是一个不可分割的组，所以其实并不是每生成一条redo日志，就将其插入到log buffer中，而是每个mtr运行过程中产生的日志先暂时存到一个地方，当该mtr结束的时候，将过程中产生的一组redo日志再全部复制到log buffer中。我们现在假设有两个名为T1、T2的事务，每个事务都包含2个mtr，我们给这几个mtr命名一下： 事务T1的两个mtr分别称为mtr_T1_1和mtr_T1_2。 事务T2的两个mtr分别称为mtr_T2_1和mtr_T2_2。 每个mtr都会产生一组redo日志，不同的事务可能是并发执行的，所以T1、T2之间的mtr可能是交替执行的。每当一个mtr执行完成时，伴随该mtr生成的一组redo日志就需要被复制到log buffer中，也就是说不同事务的mtr可能是交替写入log buffer的，我们画个示意图（为了美观，我们把一个mtr中产生的所有的redo日志当作一个整体来画）： 从示意图中我们可以看出来，不同的mtr产生的一组redo日志占用的存储空间可能不一样，有的mtr产生的redo日志量很少，比如mtr_t1_1、mtr_t2_1就被放到同一个block中存储，有的mtr产生的redo日志量非常大，比如mtr_t1_2产生的redo日志甚至占用了3个block来存储。 五，redo 日志文件1.redo日志刷盘时机mtr运行过程中产生的一组redo日志在mtr结束时会被复制到log buffer中，在一些情况下它们会被刷新到磁盘里，比如： log buffer空间不足时log buffer的大小是有限的（通过系统变量innodb_log_buffer_size指定），如果不停的往这个有限大小的log buffer里塞入日志，很快它就会被填满。InnoDB认为如果当前写入log buffer的redo日志量已经占满了log buffer总容量的大约一半左右，就需要把这些日志刷新到磁盘上。 事务提交时之所以使用redo日志主要是因为它占用的空间少，还是顺序写，在事务提交时可以不把修改过的Buffer Pool页面刷新到磁盘，但是为了保证持久性，必须要把修改这些页面对应的redo日志刷新到磁盘。 将某个脏页刷新到磁盘前，会保证先将该脏页对应的 redo 日志刷新到磁盘中（再一次 强调，redo 日志是顺序刷新的，所以在将某个脏页对应的 redo 日志从 redo log buffer 刷新到磁盘时，也会保证将在其之前产生的 redo 日志也刷新到磁盘）。 后台线程不停的刷后台有一个线程，大约每秒都会刷新一次log buffer中的redo日志到磁盘。 正常关闭服务器时 做所谓的checkpoint时 其他的一些情况… 2.redo日志文件组MySQL的数据目录（使用SHOW VARIABLES LIKE &#39;datadir&#39;查看）下默认有两个名为ib_logfile0和ib_logfile1的文件，log buffer中的日志默认情况下就是刷新到这两个磁盘文件中。如果我们对默认的redo日志文件不满意，可以通过下边几个启动参数来调节： innodb_log_group_home_dir该参数指定了redo日志文件所在的目录，默认值就是当前的数据目录。 innodb_log_file_size该参数指定了每个redo日志文件的大小，在MySQL 5.7.21这个版本中的默认值为48MB， innodb_log_files_in_group该参数指定redo日志文件的个数，默认值为2，最大值为100。 磁盘上的redo日志文件不只一个，而是以一个日志文件组的形式出现的。这些文件以ib_logfile[数字]（数字可以是0、1、2…）的形式进行命名。在将redo日志写入日志文件组时，是从ib_logfile0开始写，如果ib_logfile0写满了，就接着ib_logfile1写，同理，ib_logfile1写满了就去写ib_logfile2，依此类推。如果写到最后一个文件该咋办？那就重新转到ib_logfile0继续写，所以整个过程如下图所示： 总共的redo日志文件大小其实就是：innodb_log_file_size × innodb_log_files_in_group。 如果采用循环使用的方式向redo日志文件组里写数据的话，那岂不是要追尾，也就是后写入的redo日志覆盖掉前边写的redo日志？当然可能了！所以InnoDB提出了checkpoint的概念。 3.redo日志文件格式log buffer本质上是一片连续的内存空间，被划分成了若干个512字节大小的block。将log buffer中的redo日志刷新到磁盘的本质就是把block的镜像写入日志文件中，所以redo日志文件其实也是由若干个512字节大小的block组成。 redo日志文件组中的每个文件大小都一样，格式也一样，都是由两部分组成： 前2048个字节，也就是前4个block是用来存储一些管理信息的。 从第2048字节往后是用来存储log buffer中的block镜像的。 所以我们前边所说的循环使用redo日志文件，其实是从每个日志文件的第2048个字节开始算，画个示意图就是这样： 普通block的格式我们在了解log buffer的时候都说过了，就是log block header、log block body、log block trialer这三个部分。这里需要介绍一下每个redo日志文件前2048个字节，也就是前4个特殊block的格式都是什么作用。 从图中可以看出来，这4个block分别是： log file header：描述该redo日志文件的一些整体属性各个属性的具体释义如下： 属性名 长度（单位：字节） 描述 LOG_HEADER_FORMAT 4 redo日志的版本，在MySQL 5.7.21中该值永远为1 LOG_HEADER_PAD1 4 做字节填充用的，没什么实际意义，忽略～ LOG_HEADER_START_LSN 8 标记本redo日志文件开始的LSN值，也就是文件偏移量为2048字节初对应的LSN值。 LOG_HEADER_CREATOR 32 一个字符串，标记本redo日志文件的创建者是谁。正常运行时该值为MySQL的版本号，比如：&quot;MySQL 5.7.21&quot;，使用mysqlbackup命令创建的redo日志文件的该值为&quot;ibbackup&quot;和创建时间。 LOG_BLOCK_CHECKSUM 4 本block的校验值，所有block都有，我们不关心 checkpoint1：记录关于checkpoint的一些属性，看一下它的结构：各个属性的具体释义如下： 属性名 长度（单位：字节） 描述 LOG_CHECKPOINT_NO 8 服务器做checkpoint的编号，每做一次checkpoint，该值就加1。 LOG_CHECKPOINT_LSN 8 服务器做checkpoint结束时对应的LSN值，系统崩溃恢复时将从该值开始。 LOG_CHECKPOINT_OFFSET 8 上个属性中的LSN值在redo日志文件组中的偏移量 LOG_CHECKPOINT_LOG_BUF_SIZE 8 服务器在做checkpoint操作时对应的log buffer的大小 LOG_BLOCK_CHECKSUM 4 本block的校验值，所有block都有，我们不关心 第三个block未使用，忽略 checkpoint2：结构和checkpoint1一样。 六，Log Sequence Number自系统开始运行，就不断的在修改页面，也就意味着会不断的生成redo日志。redo日志的量在不断的递增。InnoDB为记录已经写入的redo日志量，设计了一个称之为Log Sequence Number的全局变量，翻译过来就是：日志序列号，简称lsn。InnoDB规定初始的lsn值为8704（也就是一条redo日志也没写入时，lsn的值为8704）。 在向log buffer中写入redo日志时不是一条一条写入的，而是以一个mtr生成的一组redo日志为单位进行写入的。而且实际上是把日志内容写在了log block body处。但是在统计lsn的增长量时，是按照实际写入的日志量加上占用的log block header和log block trailer来计算的。我们来看一个例子： 系统第一次启动后初始化log buffer时，buf_free（就是标记下一条redo日志应该写入到log buffer的位置的变量）就会指向第一个block的偏移量为12字节（log block header的大小）的地方，那么lsn值也会跟着增加12： 如果某个mtr产生的一组redo日志占用的存储空间比较小，也就是待插入的block剩余空闲空间能容纳这个mtr提交的日志时，lsn增长的量就是该mtr生成的redo日志占用的字节数，就像这样：我们假设上图中mtr_1产生的redo日志量为200字节，那么lsn就要在8716的基础上增加200，变为8916。 如果某个mtr产生的一组redo日志占用的存储空间比较大，也就是待插入的block剩余空闲空间不足以容纳这个mtr提交的日志时，lsn增长的量就是该mtr生成的redo日志占用的字节数加上额外占用的log block header和log block trailer的字节数，就像这样：我们假设上图中mtr_2产生的redo日志量为1000字节，为了将mtr_2产生的redo日志写入log buffer，我们不得不额外多分配两个block，所以lsn的值需要在8916的基础上增加1000 + 12×2 + 4 × 2 = 1032。 从上边的描述中可以看出来，每一组由mtr生成的redo日志都有一个唯一的LSN值与其对应，LSN值越小，说明redo日志产生的越早。 1.flushed_to_disk_lsnredo日志是首先写到log buffer中，之后才会被刷新到磁盘上的redo日志文件。所以InnoDB提出了一个称之为buf_next_to_write的全局变量，标记当前log buffer中已经有哪些日志被刷新到磁盘中了。画个图表示就是这样： lsn是表示当前系统中写入的redo日志量，这包括了写到log buffer而没有刷新到磁盘的日志，相应的，InnoDB提出了一个表示刷新到磁盘中的redo日志量的全局变量，称之为flushed_to_disk_lsn。系统第一次启动时，该变量的值和初始的lsn值是相同的，都是8704。随着系统的运行，redo日志被不断写入log buffer，但是并不会立即刷新到磁盘，lsn的值就和flushed_to_disk_lsn的值拉开了差距。我们推理一下： 系统第一次启动后，向log buffer中写入了mtr_1、mtr_2、mtr_3这三个mtr产生的redo日志，假设这三个mtr开始和结束时对应的lsn值分别是： mtr_1：8716 ～ 8916 mtr_2：8916 ～ 9948 mtr_3：9948 ～ 10000 此时的lsn已经增长到了10000，但是由于没有刷新操作，所以此时flushed_to_disk_lsn的值仍为8704，如图： 随后进行将log buffer中的block刷新到redo日志文件的操作，假设将mtr_1和mtr_2的日志刷新到磁盘，那么flushed_to_disk_lsn就应该增长mtr_1和mtr_2写入的日志量，所以flushed_to_disk_lsn的值增长到了9948，如图： 综上所述，当有新的redo日志写入到log buffer时，首先lsn的值会增长，但flushed_to_disk_lsn不变，随后随着不断有log buffer中的日志被刷新到磁盘上，flushed_to_disk_lsn的值也跟着增长。如果两者的值相同时，说明log buffer中的所有redo日志都已经刷新到磁盘中了。 应用程序向磁盘写入文件时其实是先写到操作系统的缓冲区中去，如果某个写入操作要等到操作系统确认已经写到磁盘时才返回，那需要调用一下操作系统提供的fsync函数。其实只有当系统执行了fsync函数后，flushed_to_disk_lsn的值才会跟着增长，当仅仅把log buffer中的日志写入到操作系统缓冲区却没有显式的刷新到磁盘时，另外的一个称之为write_lsn的值跟着增长。 2.lsn值和redo日志文件偏移量的对应关系因为lsn的值是代表系统写入的redo日志量的一个总和，一个mtr中产生多少日志，lsn的值就增加多少（当然有时候要加上log block header和log block trailer的大小），这样mtr产生的日志写到磁盘中时，很容易计算某一个lsn值在redo日志文件组中的偏移量，如图： 初始时的LSN值是8704，对应文件偏移量2048，之后每个mtr向磁盘中写入多少字节日志，lsn的值就增长多少。 3.flush链表中的LSN一个mtr代表一次对底层页面的原子访问，在访问过程中可能会产生一组不可分割的redo日志，在mtr结束时，会把这一组redo日志写入到log buffer中。除此之外，在mtr结束时还有一件非常重要的事情要做，就是把在mtr执行过程中可能修改过的页面加入到Buffer Pool的flush链表。 当第一次修改某个缓存在Buffer Pool中的页面时，就会把这个页面对应的控制块插入到flush链表的头部，之后再修改该页面时由于它已经在flush链表中了，就不再次插入了。也就是说flush链表中的脏页是按照页面的第一次修改时间从大到小进行排序的。在这个过程中会在缓存页对应的控制块中记录两个关于页面何时修改的属性： oldest_modification：如果某个页面被加载到Buffer Pool后进行第一次修改，那么就将修改该页面的mtr开始时对应的lsn值写入这个属性。 newest_modification：每修改一次页面，都会将修改该页面的mtr结束时对应的lsn值写入这个属性。也就是说该属性表示页面最近一次修改后对应的系统lsn值。 接着上边flushed_to_disk_lsn的例子看一下： 假设mtr_1执行过程中修改了页a，那么在mtr_1执行结束时，就会将页a对应的控制块加入到flush链表的头部。并且将mtr_1开始时对应的lsn，也就是8716写入页a对应的控制块的oldest_modification属性中，把mtr_1结束时对应的lsn，也就是8916写入页a对应的控制块的newest_modification属性中。画个图表示一下（oldest_modification缩写成了o_m，newest_modification缩写成了n_m）： 接着假设mtr_2执行过程中又修改了页b和页c两个页面，那么在mtr_2执行结束时，就会将页b和页c对应的控制块都加入到flush链表的头部。并且将mtr_2开始时对应的lsn，也就是8916写入页b和页c对应的控制块的oldest_modification属性中，把mtr_2结束时对应的lsn，也就是9948写入页b和页c对应的控制块的newest_modification属性中。画个图表示一下：从图中可以看出来，每次新插入到flush链表中的节点都是被放在了头部，也就是说flush链表中前边的脏页修改的时间比较晚，后边的脏页修改时间比较早。 接着假设mtr_3执行过程中修改了页b和页d，不过页b之前已经被修改过了，所以它对应的控制块已经被插入到了flush链表，所以在mtr_3执行结束时，只需要将页d对应的控制块都加入到flush链表的头部即可。所以需要将mtr_3开始时对应的lsn，也就是9948写入页d对应的控制块的oldest_modification属性中，把mtr_3结束时对应的lsn，也就是10000写入页d对应的控制块的newest_modification属性中。另外，由于页b在mtr_3执行过程中又发生了一次修改，所以需要更新页b对应的控制块中newest_modification的值为10000。画个图表示一下： 总结一下上边说的，就是：flush链表中的脏页按照修改发生的时间顺序进行排序，也就是按照oldest_modification代表的LSN值进行排序，被多次更新的页面不会重复插入到flush链表中，但是会更新newest_modification属性的值。 七，checkpointredo日志文件组容量是有限的，我们不得不选择循环使用redo日志文件组中的文件，但是这会造成最后写的redo日志与最开始写的redo日志追尾，这时应该想到：redo日志只是为了系统崩溃后恢复脏页用的，如果对应的脏页已经刷新到了磁盘，也就是说即使现在系统崩溃，那么在重启后也用不着使用redo日志恢复该页面了，所以该redo日志也就没有存在的必要了，那么它占用的磁盘空间就可以被后续的redo日志所重用。也就是说：判断某些redo日志占用的磁盘空间是否可以覆盖的依据就是它对应的脏页是否已经刷新到磁盘里。我们看一下前边的那个例子： 如图，虽然mtr_1和mtr_2生成的redo日志都已经被写到了磁盘上，但是它们修改的脏页仍然留在Buffer Pool中，所以它们生成的redo日志在磁盘上的空间是不可以被覆盖的。之后随着系统的运行，如果页a被刷新到了磁盘，那么它对应的控制块就会从flush链表中移除，就像这样子： 这样mtr_1生成的redo日志就没有用了，它们占用的磁盘空间就可以被覆盖掉了。InnoDB提出了一个全局变量checkpoint_lsn来代表当前系统中可以被覆盖的redo日志总量是多少，这个变量初始值也是8704。 比方说现在页a被刷新到了磁盘，mtr_1生成的redo日志就可以被覆盖了，所以我们可以进行一个增加checkpoint_lsn的操作，我们把这个过程称之为做一次checkpoint。做一次checkpoint其实可以分为两个步骤： 步骤一：计算一下当前系统中可以被覆盖的redo日志对应的lsn值最大是多少。redo日志可以被覆盖，意味着它对应的脏页被刷到了磁盘，只要我们计算出当前系统中被最早修改的脏页对应的oldest_modification值，那凡是在系统lsn值小于该节点的oldest_modification值时产生的redo日志都是可以被覆盖掉的，我们就把该脏页的oldest_modification赋值给checkpoint_lsn。比方说当前系统中页a已经被刷新到磁盘，那么flush链表的尾节点就是页c，该节点就是当前系统中最早修改的脏页了，它的oldest_modification值为8916，我们就把8916赋值给checkpoint_lsn（也就是说在redo日志对应的lsn值小于8916时就可以被覆盖掉）。 步骤二：将checkpoint_lsn和对应的redo日志文件组偏移量以及此次checkpint的编号写到日志文件的管理信息（就是checkpoint1或者checkpoint2）中。InnoDB维护了一个目前系统做了多少次checkpoint的变量checkpoint_no，每做一次checkpoint，该变量的值就加1。我们前边说过计算一个lsn值对应的redo日志文件组偏移量是很容易的，所以可以计算得到该checkpoint_lsn在redo日志文件组中对应的偏移量checkpoint_offset，然后把这三个值都写到redo日志文件组的管理信息中。我们说过，每一个redo日志文件都有2048个字节的管理信息，但是上述关于checkpoint的信息只会被写到日志文件组的第一个日志文件的管理信息中。不过我们是存储到checkpoint1中还是checkpoint2中呢？InnoDB规定，当checkpoint_no的值是偶数时，就写到checkpoint1中，是奇数时，就写到checkpoint2中。 记录完checkpoint的信息之后，redo日志文件组中各个lsn值的关系就像这样： 1.批量从flush链表中刷出脏页一般情况下都是后台的线程在对LRU链表和flush链表进行刷脏操作，这主要因为刷脏操作比较慢，不想影响用户线程处理请求。但是如果当前系统修改页面的操作十分频繁，这样就导致写日志操作十分频繁，系统lsn值增长过快。如果后台的刷脏操作不能将脏页刷出，那么系统无法及时做checkpoint，可能就需要用户线程同步的从flush链表中把那些最早修改的脏页（oldest_modification最小的脏页）刷新到磁盘，这样这些脏页对应的redo日志就没用了，然后就可以去做checkpoint了。 2.查看系统中的各种LSN值我们可以使用SHOW ENGINE INNODB STATUS命令查看当前InnoDB存储引擎中的各种LSN值的情况，比如： 12345678910111213mysql&gt; SHOW ENGINE INNODB STATUS\\G(...省略前边的许多状态)LOG---Log sequence number 124476971Log flushed up to 124099769Pages flushed up to 124052503Last checkpoint at 1240524940 pending log flushes, 0 pending chkp writes24 log i/o&#x27;s done, 2.00 log i/o&#x27;s/second----------------------(...省略后边的许多状态) 其中： Log sequence number：代表系统中的lsn值，也就是当前系统已经写入的redo日志量，包括写入log buffer中的日志。 Log flushed up to：代表flushed_to_disk_lsn的值，也就是当前系统已经写入磁盘的redo日志量。 Pages flushed up to：代表flush链表中被最早修改的那个页面对应的oldest_modification属性值。 Last checkpoint at：当前系统的checkpoint_lsn值。 3.innodb_flush_log_at_trx_commit的用法为了保证事务的持久性，用户线程在事务提交时需要将该事务执行过程中产生的所有redo日志都刷新到磁盘上。这一条要求太狠了，会很明显的降低数据库性能。如果对事务的持久性要求不是那么强烈的话，可以选择修改一个称为innodb_flush_log_at_trx_commit的系统变量的值，该变量有3个可选的值： 0：当该系统变量值为0时，表示在事务提交时不立即向磁盘中同步redo日志，这个任务是交给后台线程做的。这样很明显会加快请求处理速度，但是如果事务提交后服务器挂了，后台线程没有及时将redo日志刷新到磁盘，那么该事务对页面的修改会丢失。 1：当该系统变量值为1时，表示在事务提交时需要将redo日志同步到磁盘，可以保证事务的持久性。1也是innodb_flush_log_at_trx_commit的默认值。 2：当该系统变量值为2时，表示在事务提交时需要将redo日志写到操作系统的缓冲区中，但并不需要保证将日志真正的刷新到磁盘。这种情况下如果数据库挂了，操作系统没挂的话，事务的持久性还是可以保证的，但是操作系统也挂了的话，那就不能保证持久性了。 八，崩溃恢复在服务器不挂的情况下，redo日志不仅没用，反而让性能变得更差。但是万一数据库挂了，我们就可以在重启时根据redo日志中的记录就可以将页面恢复到系统崩溃前的状态。我们接下来大致看一下恢复过程。 1.确定恢复的起点checkpoint_lsn之前的redo日志都可以被覆盖，也就是说这些redo日志对应的脏页都已经被刷新到磁盘中了，既然它们已经被刷盘，我们就没必要恢复它们了。对于checkpoint_lsn之后的redo日志，它们对应的脏页可能没被刷盘，也可能被刷盘了，我们不能确定，所以需要从checkpoint_lsn开始读取redo日志来恢复页面。 当然，redo日志文件组的第一个文件的管理信息中有两个block都存储了checkpoint_lsn的信息，我们当然是要选取最近发生的那次checkpoint的信息。衡量checkpoint发生时间早晚的信息就是所谓的checkpoint_no，只要把checkpoint1和checkpoint2这两个block中的checkpoint_no值读出来比一下大小，哪个的checkpoint_no值更大，说明哪个block存储的就是最近的一次checkpoint信息。这样我们就能拿到最近发生的checkpoint对应的checkpoint_lsn值以及它在redo日志文件组中的偏移量checkpoint_offset。 2.确定恢复的终点redo日志恢复的起点确定了，那终点是哪个呢？这个还得从block的结构说起。在写redo日志的时候都是顺序写的，写满了一个block之后会再往下一个block中写。 普通block的log block header部分有一个称之为LOG_BLOCK_HDR_DATA_LEN的属性，该属性值记录了当前block里使用了多少字节的空间。对于被填满的block来说，该值永远为512。如果该属性的值不为512，那么就是它了，它就是此次崩溃恢复中需要扫描的最后一个block。 3.怎么恢复确定了需要扫描哪些redo日志进行崩溃恢复之后，接下来就是怎么进行恢复了。假设现在的redo日志文件中有5条redo日志，如图： 由于redo 0在checkpoint_lsn后前边，恢复时可以不管它。现在可以按照redo日志的顺序依次扫描checkpoint_lsn之后的各条redo日志，按照日志中记载的内容将对应的页面恢复出来。这样没什么问题，不过InnoDB还是想了一些办法加快这个恢复的过程： 使用哈希表根据redo日志的space ID和page number属性计算出散列值，把space ID和page number相同的redo日志放到哈希表的同一个槽里，如果有多个space ID和page number都相同的redo日志，那么它们之间使用链表连接起来，按照生成的先后顺序链接起来的，如图所示：之后就可以遍历哈希表，因为对同一个页面进行修改的redo日志都放在了一个槽里，所以可以一次性将一个页面修复好（避免了很多读取页面的随机IO），这样可以加快恢复速度。另外需要注意一点的是，同一个页面的redo日志是按照生成时间顺序进行排序的，所以恢复的时候也是按照这个顺序进行恢复，如果不按照生成时间顺序进行排序的话，那么可能出现错误。比如原先的修改操作是先插入一条记录，再删除该条记录，如果恢复时不按照这个顺序来，就可能变成先删除一条记录，再插入一条记录，这显然是错误的。 跳过已经刷新到磁盘的页面checkpoint_lsn之前的redo日志对应的脏页确定都已经刷到磁盘了，但是checkpoint_lsn之后的redo日志我们不能确定是否已经刷到磁盘，主要是因为在最近做的一次checkpoint后，可能后台线程又不断的从LRU链表和flush链表中将一些脏页刷出Buffer Pool。这些在checkpoint_lsn之后的redo日志，如果它们对应的脏页在崩溃发生时已经刷新到磁盘，那在恢复时也就没有必要根据redo日志的内容修改该页面了。那在恢复时怎么知道某个redo日志对应的脏页是否在崩溃发生时已经刷新到磁盘了呢？这还得从页面的结构说起，每个页面都有一个称之为File Header的部分，在File Header里有一个称之为FIL_PAGE_LSN的属性，该属性记载了最近一次修改页面时对应的lsn值（其实就是页面控制块中的newest_modification值）。如果在做了某次checkpoint之后有脏页被刷新到磁盘中，那么该页对应的FIL_PAGE_LSN代表的lsn值肯定大于checkpoint_lsn的值，凡是符合这种情况的页面就不需要重复执行lsn值小于FIL_PAGE_LSN的redo日志了，所以更进一步提升了崩溃恢复的速度。 九，LOG_BLOCK_HDR_NO是如何计算的对于实际存储redo日志的普通的log block来说，在log block header处有一个称之为LOG_BLOCK_HDR_NO的属性，我们说这个属性代表一个唯一的标号。这个属性是初次使用该block时分配的，跟当时的系统lsn值有关。使用下边的公式计算该block的LOG_BLOCK_HDR_NO值： 1((lsn / 512) &amp; 0x3FFFFFFFUL) + 1 从图中可以看出，0x3FFFFFFFUL对应的二进制数的前2位为0，后30位的值都为1。一个二进制位与0做与运算（&amp;）的结果肯定是0，一个二进制位与1做与运算（&amp;）的结果就是原值。让一个数和0x3FFFFFFFUL做与运算的意思就是要将该值的前2个比特位的值置为0，这样该值就肯定小于或等于0x3FFFFFFFUL了。这也就说明了，不论lsn多大，((lsn / 512) &amp; 0x3FFFFFFFUL)的值肯定在0``~~0x3FFFFFFFUL~~之间，再加1的话肯定在~~1~~``0x40000000UL之间。而0x40000000UL这个值就代表着1GB。也就是说系统最多能产生不重复的LOG_BLOCK_HDR_NO值只有1GB个。InnoDB规定redo日志文件组中包含的所有文件大小总和不得超过512GB，一个block大小是512字节，也就是说redo日志文件组中包含的block块最多为1GB个，所以有1GB个不重复的编号值也就够用了。 另外，LOG_BLOCK_HDR_NO值的第一个比特位比较特殊，称之为flush bit，如果该值为1，代表着本block是在某次将log buffer中的block刷新到磁盘的操作中的第一个被刷入的block。 十，double write​ 1.脏页刷盘风险​ 关于IO的最小单位： 数据库IO的最小单位是16K（MySQL默认，oracle是8K） 文件系统IO的最小单位是4K（也有1K的） 磁盘IO的最小单位是512字节 因此，存在IO写入导致page损坏的风险：​ ​​ 2.doublewrite：两次写提高innodb的可靠性，用来解决部分写失败(partial page write页断裂)。​ 2.1 Double write解决了什么问题​ 一个数据页的大小是16K，假设在把内存中的脏页写到数据库的时候，写了2K突然掉电，也就是说前2K数据是新的，后14K是旧的，那么磁盘数据库这个数据页就是不完整的，是一个坏掉的数据页。redo只能加上旧、校检完整的数据页恢复一个脏块，不能修复坏掉的数据页，所以这个数据就丢失了，可能会造成数据不一致，所以需要double write。​ 2.2使用情景​ 当数据库正在从内存想磁盘写一个数据页是，数据库宕机，从而导致这个页只写了部分数据，这就是部分写失效，它会导致数据丢失。这时是无法通过重做日志恢复的，因为重做日志记录的是对页的物理修改，如果页本身已经损坏，重做日志也无能为力。​ 2.3 double write工作流程 doublewrite由两部分组成，一部分为内存中的doublewrite buffer，其大小为2MB，另一部分是磁盘上共享表空间(ibdata x)中连续的128个页，即2个区(extent)，大小也是2M。 当一系列机制触发数据缓冲池中的脏页刷新时，并不直接写入磁盘数据文件中，而是先拷贝至内存中的doublewrite buffer中； 接着从两次写缓冲区分两次写入磁盘共享表空间中(连续存储，顺序写，性能很高)，每次写1MB； 待第二步完成后，再将doublewrite buffer中的脏页数据写入实际的各个表空间文件(离散写)；(脏页数据固化后，即进行标记对应doublewrite数据可覆盖) 2.4 doublewrite的崩溃恢复​ 如果操作系统在将页写入磁盘的过程中发生崩溃，在恢复过程中，innodb存储引擎可以从共享表空间的doublewrite中找到该页的一个最近的副本，将其复制到表空间文件，再应用redo log，就完成了恢复过程。因为有副本所以也不担心表空间中数据页是否损坏。 Q：为什么_log write_不需要_doublewrite_的支持？A：因为_redolog_写入的单位就是512字节，也就是磁盘IO的最小单位，所以无所谓数据损坏。 ​ 3.doublewrite的副作用3.1 double write带来的写负载 double write是一个buffer, 但其实它是开在物理文件上的一个buffer, 其实也就是file, 所以它会导致系统有更多的fsync操作, 而硬盘的fsync性能是很慢的, 所以它会降低mysql的整体性能。 但是，doublewrite buffer写入磁盘共享表空间这个过程是连续存储，是顺序写，性能非常高，(约占写的10%)，牺牲一点写性能来保证数据页的完整还是很有必要的。3.2 监控double write工作负载 12345678mysql&gt; show global status like &#x27;%dblwr%&#x27;;+----------------------------+-------+| Variable_name | Value |+----------------------------+-------+| Innodb_dblwr_pages_written | 7 || Innodb_dblwr_writes | 3 |+----------------------------+-------+2 rows in set (0.00 sec) 关注点：Innodb_dblwr_pages_written / Innodb_dblwr_writes​ 开启doublewrite后，每次脏页刷新必须要先写doublewrite，而doublewrite存在于磁盘上的是两个连续的区，每个区由连续的页组成，一般情况下一个区最多有64个页，所以一次IO写入应该可以最多写64个页。​ 而根据以上系统Innodb_dblwr_pages_written与Innodb_dblwr_writes的比例来看，大概在3左右，远远还没到64(如果约等于64，那么说明系统的写压力非常大，有大量的脏页要往磁盘上写)，所以从这个角度也可以看出，系统写入压力并不高。​ 3.3 关闭double write适合的场景 海量DML 不惧怕数据损坏和丢失 系统写负载成为主要负载1234567mysql&gt; show variables like &#x27;%double%&#x27;;+--------------------+-------+| Variable_name | Value |+--------------------+-------+| innodb_doublewrite | ON |+--------------------+-------+1 row in set (0.04 sec) 作为InnoDB的一个关键特性，doublewrite功能默认是开启的，但是在上述特殊的一些场景也可以视情况关闭，来提高数据库写性能。静态参数，配置文件修改，重启数据库。3.4 为什么没有把double write里面的数据写到data page里面呢？ double write里面的数据是连续的，如果直接写到data page里面，而data page的页又是离散的，写入会很慢。 double write里面的数据没有办法被及时的覆盖掉，导致double write的压力很大；短时间内可能会出现double write溢出的情况。十一，总结 redo日志记录了事务执行过程中都修改了哪些内容。 事务提交时只将执行过程中产生的redo日志刷新到磁盘，而不是将所有修改过的页面都刷新到磁盘。这样做有两个好处： redo日志占用的空间非常小 redo日志是顺序写入磁盘的 一条redo日志由下面几部分组成。 type：这条redo日志的类型 space ID:表空间ID page number :页号 data：这条redo日志的具体内容 redo日志的类型有简单和复杂之分。简单类型的redo日志是纯粹的物理日志，复杂类型的redo日志兼有物理日志和逻辑日志的特性。 一个MTR可以包含一组redo日志。在进行崩溃恢复时，这一组redo日志作为一个不可分割的整体来处理。 redo日志存放在大小为512字节的block中。每一个block被分为3部分： log block header log block body log block trailer redo日志缓冲区是一片连续的内存空间，由若干个block组成；可以通过启动选项innodb_log_buffer_size 来调整他的大小。 redo日志文件组由若干个日志文件组成，这些redo日志文件是被循环使用的。redo日志文件组中每个文件的大小都一样，格式也一样，都是由两部分组成的： 前2048字节用来存储一些管理信息 从第2048字节往后的字节用来存储log buffer中的block镜像 lsn指已经写入的redo日志量，flushed_to_disk_lsn指刷新到磁盘中的redo日志量，flush链表中的脏页按照修改发生的时间顺序进行排序，也就是按照oldest_modification代表的lsn值进行排序。被多次更新的页面不会重复插入到flush链表，但是会更新newest_modification属性的值。checkpoint_lsn表示当前系统中可以被覆盖的redo日志总量是多少。 redo日志占用的磁盘空间在他对应的脏页已经被刷新到磁盘后即可被覆盖。执行一次checkpoint的意思就是增加checkpoint_lsn的值，然后把相关信息放到日志文件的管理信息中。 innodb_flush_log_at_trx_commit系统变量控制着在事务提交时是否将该事务运行过程中产生的redo刷新到磁盘。 在崩溃恢复过程中，从redo日志文件组第一个文件的管理信息中取出最近发生的那次checkpoint信息，然后从checkpoint_lsn在日志文件组中对应的偏移量开始，一直扫描日志文件中的block，直到某个block的LOG_BLOCK_HDR_DATA_LEN值不等于512为止。再恢复过程中，使用hash表可加快恢复过程，并且会跳过已经刷新到磁盘的页面。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/tags/MySQL/"}]},{"title":"MySQL[十三]事务","slug":"MySQL/MySQL[十三]事务","date":"2022-01-11T03:17:20.893Z","updated":"2022-01-11T03:25:11.029Z","comments":true,"path":"2022/01/11/MySQL/MySQL[十三]事务/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/MySQL/MySQL[%E5%8D%81%E4%B8%89]%E4%BA%8B%E5%8A%A1/","excerpt":"","text":"一，事务的起源1.原子性（Atomicity）现实世界中转账操作是一个不可分割的操作，也就是说要么压根儿就没转，要么转账成功，不能存在中间的状态，也就是转了一半的这种情况。设计数据库的大叔们把这种要么全做，要么全不做的规则称之为原子性。但是在现实世界中的一个不可分割的操作却可能对应着数据库世界若干条不同的操作，数据库中的一条操作也可能被分解成若干个步骤（比如先修改缓存页，之后再刷新到磁盘等），最要命的是在任何一个可能的时间都可能发生意想不到的错误（可能是数据库本身的错误，或者是操作系统错误，甚至是直接断电之类的）而使操作执行不下去。为了保证在数据库世界中某些操作的原子性，MySQL需要保证如果在执行操作的过程中发生了错误，把已经做了的操作恢复成没执行之前的样子。 2.隔离性（Isolation）现实世界中的两次状态转换应该是互不影响的，所以对于现实世界中状态转换对应的某些数据库操作来说，不仅要保证这些操作以原子性的方式执行完成，而且要保证其它的状态转换不会影响到本次状态转换，这个规则被称之为隔离性。这时MySQL就需要采取一些措施来让访问相同数据的不同状态转换对应的数据库操作的执行顺序有一定规律。 3.一致性（Consistency）我们生活的这个世界存在着形形色色的约束，比如身份证号不能重复，性别只能是男或者女，高考的分数只能在0～750之间，人民币面值最大只能是100（现在是2019年），红绿灯只有3种颜色，房价不能为负的。 只有符合这些约束的数据才是有效的。数据库世界只是现实世界的一个映射，现实世界中存在的约束当然也要在数据库世界中有所体现。如果数据库中的数据全部符合现实世界中的约束（all defined rules），我们说这些数据就是一致的，或者说符合一致性的。 如何保证数据库中数据的一致性（就是符合所有现实世界的约束）呢？这其实靠两方面的努力： 数据库本身能为我们保证一部分一致性需求（就是数据库自身可以保证一部分现实世界的约束永远有效）。我们知道MySQL数据库可以为表建立主键、唯一索引、外键、声明某个列为NOT NULL来拒绝NULL值的插入。比如说当我们对某个列建立唯一索引时，如果插入某条记录时该列的值重复了，那么MySQL就会报错并且拒绝插入。除了这些我们已经非常熟悉的保证一致性的功能，MySQL还支持CHECK语法来自定义约束，比如这样：上述例子中的CHECK语句本意是想规定balance列不能存储小于0的数字，对应的现实世界的意思就是银行账户余额不能小于0。但是很遗憾，MySQL仅仅支持CHECK语法，但实际上并没有一点卵用，也就是说即使我们使用上述带有CHECK子句的建表语句来创建account表，那么在后续插入或更新记录时，MySQL并不会去检查CHECK子句中的约束是否成立。虽然CHECK子句对一致性检查没什么卵用，但是我们还是可以通过定义触发器的方式来自定义一些约束条件以保证数据库中数据的一致性。 1234567CREATE TABLE account ( id INT NOT NULL AUTO_INCREMENT COMMENT &#x27;自增id&#x27;, name VARCHAR(100) COMMENT &#x27;客户名称&#x27;, balance INT COMMENT &#x27;余额&#x27;, PRIMARY KEY (id), CHECK (balance &gt;= 0) ); 其它的一些数据库，比如SQL Server或者Oracle支持的CHECK语法是有实实在在的作用的，每次进行插入或更新记录之前都会检查一下数据是否符合CHECK子句中指定的约束条件是否成立，如果不成立的话就会拒绝插入或更新。 更多的一致性需求需要靠写业务代码的程序员自己保证。为建立现实世界和数据库世界的对应关系，理论上应该把现实世界中的所有约束都反应到数据库世界中，但是在更改数据库数据时进行一致性检查是一个耗费性能的工作，比方说我们为account表建立了一个触发器，每当插入或者更新记录时都会校验一下balance列的值是不是大于0，这就会影响到插入或更新的速度。仅仅是校验一行记录符不符合一致性需求倒也不是什么大问题，有的一致性需求简直变态，比方说银行会建立一张代表账单的表，里边儿记录了每个账户的每笔交易，每一笔交易完成后，都需要保证整个系统的余额等于所有账户的收入减去所有账户的支出。如果在数据库层面实现这个一致性需求的话，每次发生交易时，都需要将所有的收入加起来减去所有的支出，再将所有的账户余额加起来，看看两个值相不相等。如果账单表里有几亿条记录，光是这个校验的过程可能就要跑好几个小时，这样的性能代价是完全承受不起的。现实生活中复杂的一致性需求比比皆是，而由于性能问题把一致性需求交给数据库去解决这是不现实的，所以这个锅就甩给了业务端程序员。比方说我们的account表，我们也可以不建立触发器，只要编写业务的程序员在自己的业务代码里判断一下，当某个操作会将balance列的值更新为小于0的值时，就不执行该操作就好了！ 原子性和隔离性都会对一致性产生影响，比如我们现实世界中转账操作完成后，有一个一致性需求就是参与转账的账户的总的余额是不变的。如果数据库不遵循原子性要求，也就是转了一半就不转了，那最后就是不符合一致性需求的；类似的，如果数据库不遵循隔离性要求，也就是说可能不符合一致性需求了。所以说，数据库某些操作的原子性和隔离性都是保证一致性的一种手段，在操作执行完成后保证符合所有既定的约束则是一种结果。那满足原子性和隔离性的操作一定就满足一致性么？那倒也不一定，那不满足原子性和隔离性的操作就一定不满足一致性么？这也不一定，只要最后的结果符合所有现实世界中的约束，那么就是符合一致性的。 4.持久性（Durability）当现实世界的一个状态转换完成后，这个转换的结果将永久的保留，这个规则被MySQL称为持久性。当把现实世界的状态转换映射到数据库世界时，持久性意味着该转换对应的数据库操作所修改的数据都应该在磁盘上保留下来，不论之后发生了什么事故，本次转换造成的影响都不应该被丢失掉。 二，事务的概念我们把原子性（Atomicity）、隔离性（Isolation）、一致性（Consistency）和持久性（Durability）这四个词对应的英文单词首字母提取出来就是A、I、C、D，稍微变换一下顺序可以组成一个完整的英文单词：ACID。MySQL叔为了方便起见，把需要保证原子性、隔离性、一致性和持久性的一个或多个数据库操作称之为一个事务（英文名是：transaction）。 我们现在知道事务是一个抽象的概念，它其实对应着一个或多个数据库操作，MySQL根据这些操作所执行的不同阶段把事务大致上划分成了这么几个状态： 活动的（active）事务对应的数据库操作正在执行过程中时，我们就说该事务处在活动的状态。 部分提交的（partially committed）当事务中的最后一个操作执行完成，但由于操作都在内存中执行，所造成的影响并没有刷新到磁盘时，我们就说该事务处在部分提交的状态。 失败的（failed）当事务处在活动的或者部分提交的状态时，可能遇到了某些错误（数据库自身的错误、操作系统错误或者直接断电等）而无法继续执行，或者人为的停止当前事务的执行，我们就说该事务处在失败的状态。 中止的（aborted）如果事务执行了半截而变为失败的状态，那么就需要把已经修改的数据调整为未修改之前的数据，换句话说，就是要撤销失败事务对当前数据库造成的影响。书面一点的话，我们把这个撤销的过程称之为回滚。当回滚操作执行完毕时，也就是数据库恢复到了执行事务之前的状态，我们就说该事务处在了中止的状态。 提交的（committed）当一个处在部分提交的状态的事务将修改过的数据都同步到磁盘上之后，我们就可以说该事务处在了提交的状态。 随着事务对应的数据库操作执行到不同阶段，事务的状态也在不断变化，一个基本的状态转换图如下所示： 从图中大家也可以看出了，只有当事务处于提交的或者中止的状态时，一个事务的生命周期才算是结束了。对于已经提交的事务来说，该事务对数据库所做的修改将永久生效，对于处于中止状态的事务，该事务对数据库所做的所有修改都会被回滚到没执行该事务之前的状态。 三，MySQL中事务的语法我们说事务的本质其实只是一系列数据库操作，只不过这些数据库操作符合ACID特性而已，那么MySQL中如何将某些操作放到一个事务里去执行的呢？ 1.开启事务我们可以使用下边两种语句之一来开启一个事务： BEGIN [WORK];BEGIN语句代表开启一个事务，后边的单词WORK可有可无。开启事务后，就可以继续写若干条语句，这些语句都属于刚刚开启的这个事务。 1234mysql&gt; BEGIN;Query OK, 0 rows affected (0.00 sec)mysql&gt; 加入事务的语句... START TRANSACTION;START TRANSACTION语句和BEGIN语句有着相同的功效，都标志着开启一个事务，比如这样：不过比BEGIN语句牛逼一点儿的是，可以在START TRANSACTION语句后边跟随几个修饰符，就是它们几个： 1234mysql&gt; START TRANSACTION;Query OK, 0 rows affected (0.00 sec)mysql&gt; 加入事务的语句... READ ONLY：标识当前事务是一个只读事务，也就是属于该事务的数据库操作只能读取数据，而不能修改数据。 其实只读事务中只是不允许修改那些其他事务也能访问到的表中的数据，对于临时表来说（我们使用CREATE TMEPORARY TABLE创建的表），由于它们只能在当前会话中可见，所以只读事务其实也是可以对临时表进行增、删、改操作的。 READ WRITE：标识当前事务是一个读写事务，也就是属于该事务的数据库操作既可以读取数据，也可以修改数据。 WITH CONSISTENT SNAPSHOT：启动一致性读。 比如我们想开启一个只读事务的话，直接把READ ONLY这个修饰符加在START TRANSACTION语句后边就好，比如这样： 1START TRANSACTION READ ONLY; 如果我们想在START TRANSACTION后边跟随多个修饰符的话，可以使用逗号将修饰符分开，比如开启一个只读事务和一致性读，就可以这样写： 1START TRANSACTION READ ONLY, WITH CONSISTENT SNAPSHOT; 或者开启一个读写事务和一致性读，就可以这样写： 1START TRANSACTION READ WRITE, WITH CONSISTENT SNAPSHOT 不过这里需要注意的一点是，READ ONLY和READ WRITE是用来设置所谓的事务访问模式的，就是以只读还是读写的方式来访问数据库中的数据，一个事务的访问模式不能同时既设置为只读的也设置为读写的，所以我们不能同时把READ ONLY和READ WRITE放到START TRANSACTION语句后边。另外，如果我们不显式指定事务的访问模式，那么该事务的访问模式就是读写模式。 2.提交事务开启事务之后就可以继续写需要放到该事务中的语句了，当最后一条语句写完了之后，我们就可以提交该事务了，提交的语句也很简单： 1COMMIT [WORK] COMMIT语句就代表提交一个事务，后边的WORK可有可无。转账举例： 12345678910111213mysql&gt; BEGIN;Query OK, 0 rows affected (0.00 sec)mysql&gt; UPDATE account SET balance = balance - 10 WHERE id = 1;Query OK, 1 row affected (0.02 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; UPDATE account SET balance = balance + 10 WHERE id = 2;Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; COMMIT;Query OK, 0 rows affected (0.00 sec) 3.手动中止事务如果我们写了几条语句之后发现上边的某条语句写错了，我们可以手动的使用下边这个语句来将数据库恢复到事务执行之前的样子： 1ROLLBACK [WORK] ROLLBACK语句就代表中止并回滚一个事务，后边的WORK可有可无类似的。转账举例： 12345678910111213mysql&gt; BEGIN;Query OK, 0 rows affected (0.00 sec)mysql&gt; UPDATE account SET balance = balance - 10 WHERE id = 1;Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; UPDATE account SET balance = balance + 1 WHERE id = 2;Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; ROLLBACK;Query OK, 0 rows affected (0.00 sec) 这里需要强调一下，ROLLBACK语句是我们程序员手动的去回滚事务时才去使用的，如果事务在执行过程中遇到了某些错误而无法继续执行的话，事务自身会自动的回滚。 我们这里所说的开启、提交、中止事务的语法只是针对使用黑框框时通过mysql客户端程序与服务器进行交互时控制事务的语法，如果大家使用的是别的客户端程序，比如JDBC之类的，那需要参考相应的文档来看看如何控制事务。 4.支持事务的存储引擎MySQL中并不是所有存储引擎都支持事务的功能，目前只有InnoDB和NDB存储引擎支持（NDB存储引擎不是我们的重点），如果某个事务中包含了修改使用不支持事务的存储引擎的表，那么对该使用不支持事务的存储引擎的表所做的修改将无法进行回滚。比方说我们有两个表，tbl1使用支持事务的存储引擎InnoDB，tbl2使用不支持事务的存储引擎MyISAM，它们的建表语句如下所示： 1234567CREATE TABLE tbl1 ( i int) engine=InnoDB;CREATE TABLE tbl2 ( i int) ENGINE=MyISAM; 我们看看先开启一个事务，写一条插入语句后再回滚该事务，tbl1和tbl2的表现有什么不同： 1234567891011121314mysql&gt; SELECT * FROM tbl1;Empty set (0.00 sec)mysql&gt; BEGIN;Query OK, 0 rows affected (0.00 sec)mysql&gt; INSERT INTO tbl1 VALUES(1);Query OK, 1 row affected (0.00 sec)mysql&gt; ROLLBACK;Query OK, 0 rows affected (0.00 sec)mysql&gt; SELECT * FROM tbl1;Empty set (0.00 sec) 可以看到，对于使用支持事务的存储引擎的tbl1表来说，我们在插入一条记录再回滚后，tbl1就恢复到没有插入记录时的状态了。再看看tbl2表的表现： 12345678910111213141516171819mysql&gt; SELECT * FROM tbl2;Empty set (0.00 sec)mysql&gt; BEGIN;Query OK, 0 rows affected (0.00 sec)mysql&gt; INSERT INTO tbl2 VALUES(1);Query OK, 1 row affected (0.00 sec)mysql&gt; ROLLBACK;Query OK, 0 rows affected, 1 warning (0.01 sec)mysql&gt; SELECT * FROM tbl2;+------+| i |+------+| 1 |+------+1 row in set (0.00 sec) 可以看到，虽然我们使用了ROLLBACK语句来回滚事务，但是插入的那条记录还是留在了tbl2表中。 5.自动提交MySQL中有一个系统变量autocommit： 1234567mysql&gt; SHOW VARIABLES LIKE &#x27;autocommit&#x27;;+---------------+-------+| Variable_name | Value |+---------------+-------+| autocommit | ON |+---------------+-------+1 row in set (0.01 sec) 可以看到它的默认值为ON，也就是说默认情况下，如果我们不显式的使用START TRANSACTION或者BEGIN语句开启一个事务，那么每一条语句都算是一个独立的事务，这种特性称之为事务的自动提交。假如我们在转账时不以START TRANSACTION或者BEGIN语句显式的开启一个事务，那么下边这两条语句就相当于放到两个独立的事务中去执行： 12UPDATE account SET balance = balance - 10 WHERE id = 1;UPDATE account SET balance = balance + 10 WHERE id = 2; 当然，如果我们想关闭这种自动提交的功能，可以使用下边两种方法之一： 显式的的使用START TRANSACTION或者BEGIN语句开启一个事务。这样在本次事务提交或者回滚前会暂时关闭掉自动提交的功能。 把系统变量autocommit的值设置为OFF，就像这样：这样的话，我们写入的多条语句就算是属于同一个事务了，直到我们显式的写出COMMIT语句来把这个事务提交掉，或者显式的写出ROLLBACK语句来把这个事务回滚掉。 1SET autocommit = OFF; 6.隐式提交当我们使用START TRANSACTION或者BEGIN语句开启了一个事务，或者把系统变量autocommit的值设置为OFF时，事务就不会进行自动提交，但是如果我们输入了某些语句之后就会悄悄的提交掉，就像我们输入了COMMIT语句了一样，这种因为某些特殊的语句而导致事务提交的情况称为隐式提交，这些会导致事务隐式提交的语句包括： 定义或修改数据库对象的数据定义语言（Data definition language，缩写为：DDL）。所谓的数据库对象，指的就是数据库、表、视图、存储过程等等这些东西。当我们使用CREATE、ALTER、DROP等语句去修改这些所谓的数据库对象时，就会隐式的提交前边语句所属于的事务，就像这样： 1234567BEGIN;SELECT ... # 事务中的一条语句UPDATE ... # 事务中的一条语句... # 事务中的其它语句CREATE TABLE ... # 此语句会隐式的提交前边语句所属于的事务 隐式使用或修改mysql数据库中的表当我们使用ALTER USER、CREATE USER、DROP USER、GRANT、RENAME USER、REVOKE、SET PASSWORD等语句时也会隐式的提交前边语句所属于的事务。 事务控制或关于锁定的语句当我们在一个事务还没提交或者回滚时就又使用START TRANSACTION或者BEGIN语句开启了另一个事务时，会隐式的提交上一个事务，比如这样：或者当前的autocommit系统变量的值为OFF，我们手动把它调为ON时，也会隐式的提交前边语句所属的事务。或者使用LOCK TABLES、UNLOCK TABLES等关于锁定的语句也会隐式的提交前边语句所属的事务。 1234567BEGIN;SELECT ... # 事务中的一条语句UPDATE ... # 事务中的一条语句... # 事务中的其它语句BEGIN; # 此语句会隐式的提交前边语句所属于的事务 加载数据的语句比如我们使用LOAD DATA语句来批量往数据库中导入数据时，也会隐式的提交前边语句所属的事务。 关于MySQL复制的一些语句使用START SLAVE、STOP SLAVE、RESET SLAVE、CHANGE MASTER TO等语句时也会隐式的提交前边语句所属的事务。 其它的一些语句使用ANALYZE TABLE、CACHE INDEX、CHECK TABLE、FLUSH、 LOAD INDEX INTO CACHE、OPTIMIZE TABLE、REPAIR TABLE、RESET等语句也会隐式的提交前边语句所属的事务。 7.保存点如果你开启了一个事务，并且已经敲了很多语句，忽然发现上一条语句有点问题，你只好使用ROLLBACK语句来让数据库状态恢复到事务执行之前的样子，然后一切从头再来，总有一种一夜回到解放前的感觉。所以MySQL提出了一个保存点（英文：savepoint）的概念，就是在事务对应的数据库语句中打几个点，我们在调用ROLLBACK语句时可以指定会滚到哪个点，而不是回到最初的原点。定义保存点的语法如下： 1SAVEPOINT 保存点名称; 当我们想回滚到某个保存点时，可以使用下边这个语句（下边语句中的单词WORK和SAVEPOINT是可有可无的）： 1ROLLBACK [WORK] TO [SAVEPOINT] 保存点名称; 不过如果ROLLBACK语句后边不跟随保存点名称的话，会直接回滚到事务执行之前的状态。 如果我们想删除某个保存点，可以使用这个语句： 1RELEASE SAVEPOINT 保存点名称; 下边还是以转账的例子展示一下保存点的用法，在执行完扣除第一个账户的钱10元的语句之后打一个保存点： 12345678910111213141516171819202122232425262728293031323334353637383940414243mysql&gt; SELECT * FROM account;+----+--------+---------+| id | name | balance |+----+--------+---------+| 1 | 狗哥 | 11 || 2 | 猫爷 | 2 |+----+--------+---------+2 rows in set (0.00 sec)mysql&gt; BEGIN;Query OK, 0 rows affected (0.00 sec)mysql&gt; UPDATE account SET balance = balance - 10 WHERE id = 1;Query OK, 1 row affected (0.01 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; SAVEPOINT s1; # 一个保存点Query OK, 0 rows affected (0.00 sec)mysql&gt; SELECT * FROM account;+----+--------+---------+| id | name | balance |+----+--------+---------+| 1 | 狗哥 | 1 || 2 | 猫爷 | 2 |+----+--------+---------+2 rows in set (0.00 sec)mysql&gt; UPDATE account SET balance = balance + 1 WHERE id = 2; # 更新错了Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; ROLLBACK TO s1; # 回滚到保存点s1处Query OK, 0 rows affected (0.00 sec)mysql&gt; SELECT * FROM account;+----+--------+---------+| id | name | balance |+----+--------+---------+| 1 | 狗哥 | 1 || 2 | 猫爷 | 2 |+----+--------+---------+2 rows in set (0.00 sec)","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/tags/MySQL/"}]},{"title":"MySQL[十二]InnoDB之BufferPool","slug":"MySQL/MySQL[十二]InnoDB之BufferPool","date":"2022-01-11T03:17:09.017Z","updated":"2022-01-11T03:24:53.735Z","comments":true,"path":"2022/01/11/MySQL/MySQL[十二]InnoDB之BufferPool/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/MySQL/MySQL[%E5%8D%81%E4%BA%8C]InnoDB%E4%B9%8BBufferPool/","excerpt":"","text":"一，缓存的重要性对于使用InnoDB作为存储引擎的表来说，不管是用于存储用户数据的索引（包括聚簇索引和二级索引），还是各种系统数据，都是以页的形式存放在表空间中的，而所谓的表空间只不过是InnoDB对文件系统上一个或几个实际文件的抽象，也就是说我们的数据说到底还是存储在磁盘上的。但是磁盘太慢了，所以InnoDB存储引擎在处理客户端的请求时，当需要访问某个页的数据时，就会把完整的页的数据全部加载到内存中，也就是说即使我们只需要访问一个页的一条记录，那也需要先把整个页的数据加载到内存中。将整个页加载到内存中后就可以进行读写访问了，在进行完读写访问之后并不着急把该页对应的内存空间释放掉，而是将其缓存起来，这样将来有请求再次访问该页面时，就可以省去磁盘IO的开销了。 二，InnoDB的Buffer Pool 1.啥是个Buffer Pool为了缓存磁盘中的页，在MySQL服务器启动的时候就向操作系统申请了一片连续的内存，叫做Buffer Pool（中文名是缓冲池）。那它有多大呢？这个其实看我们机器的配置，默认情况下Buffer Pool只有128M大小，但是可以在启动服务器的时候配置innodb_buffer_pool_size参数的值，它表示Buffer Pool的大小，就像这样： 12[server]innodb_buffer_pool_size = 268435456 其中，268435456的单位是字节，也就是我指定Buffer Pool的大小为256M。需要注意的是，Buffer Pool也不能太小，最小值为5M(当小于该值时会自动设置成5M)。 2.Buffer Pool内部组成Buffer Pool中默认的缓存页大小和在磁盘上默认的页大小是一样的，都是16KB。为了更好的管理这些在Buffer Pool中的缓存页，InnoDB为每一个缓存页都创建了一些所谓的控制信息，这些控制信息包括该页所属的表空间编号、页号、缓存页在Buffer Pool中的地址、链表节点信息、一些锁信息以及LSN信息（锁和LSN先忽略），当然还有一些别的控制信息，暂时省略。 每个缓存页对应的控制信息占用的内存大小是相同的，我们就把每个页对应的控制信息占用的一块内存称为一个控制块，控制块和缓存页是一一对应的，它们都被存放到 Buffer Pool 中，其中控制块被存放到 Buffer Pool 的前边，缓存页被存放到 Buffer Pool 后边。 每一个控制块都对应一个缓存页，那在分配足够多的控制块和缓存页后，可能剩余的那点儿空间不够一对控制块和缓存页的大小，这个用不到的那点儿内存空间就被称为碎片。当然，如果把Buffer Pool的大小设置的刚刚好的话，也可能不会产生碎片。 每个控制块大约占用缓存页大小的5%，在MySQL5.7.21这个版本中，每个控制块占用的大小是808字节。而我们设置的innodb_buffer_pool_size并不包含这部分控制块占用的内存空间大小，也就是说InnoDB在为Buffer Pool向操作系统申请连续的内存空间时，这片连续的内存空间一般会比innodb_buffer_pool_size的值大5%左右。 3.free链表的管理当我们最初启动MySQL服务器的时候，需要完成对Buffer Pool的初始化过程，就是先向操作系统申请Buffer Pool的内存空间，然后把它划分成若干对控制块和缓存页。但是此时并没有真实的磁盘页被缓存到Buffer Pool中（因为还没有用到），之后随着程序的运行，会不断的有磁盘上的页被缓存到Buffer Pool中。那么问题来了，从磁盘上读取一个页到Buffer Pool中的时候该放到哪个缓存页的位置呢？或者说怎么区分Buffer Pool中哪些缓存页是空闲的，哪些已经被使用了呢？我们最好在某个地方记录一下Buffer Pool中哪些缓存页是可用的，我们可以把所有空闲的缓存页对应的控制块作为一个节点放到一个链表中，这个链表也可以被称作free链表（或者说空闲链表）。刚刚完成初始化的Buffer Pool中所有的缓存页都是空闲的，所以每一个缓存页对应的控制块都会被加入到free链表中。 为了管理好这个free链表，特意为这个链表定义了一个基节点，里边儿包含着链表的头节点地址，尾节点地址，以及当前链表中节点的数量等信息。这里需要注意的是，链表的基节点占用的内存空间并不包含在为Buffer Pool申请的一大片连续内存空间之内，而是单独申请的一块内存空间。 链表基节点占用的内存空间并不大，在MySQL5.7.21这个版本里，每个基节点只占用40字节大小。后边我们即将介绍许多不同的链表，它们的基节点和free链表的基节点的内存分配方式是一样的，都是单独申请的一块40字节大小的内存空间，并不包含在为Buffer Pool申请的一大片连续内存空间之内。 有了这个free链表之后，每当需要从磁盘中加载一个页到Buffer Pool中时，就从free链表中取一个空闲的缓存页，并且把该缓存页对应的控制块的信息填上（就是该页所在的表空间、页号之类的信息），然后把该缓存页对应的free链表节点从链表中移除，表示该缓存页已经被使用了。 4.缓存页的哈希处理当我们需要访问某个页中的数据时，就会把该页从磁盘加载到Buffer Pool中，如果该页已经在Buffer Pool中的话直接使用就可以了。那么问题也就来了，我们怎么知道该页在不在Buffer Pool中呢？难不成需要依次遍历Buffer Pool中各个缓存页么？ 我们其实是根据表空间号 + 页号来定位一个页的，也就相当于表空间号 + 页号是一个key，缓存页就是对应的value，怎么通过一个key来快速找着一个value呢？那肯定是哈希表。 所以我们可以用表空间号 + 页号作为key，缓存页作为value创建一个哈希表，在需要访问某个页的数据时，先从哈希表中根据表空间号 + 页号看看有没有对应的缓存页，如果有，直接使用该缓存页就好，如果没有，那就从free链表中选一个空闲的缓存页，然后把磁盘中对应的页加载到该缓存页的位置。 5.flush链表的管理如果我们修改了Buffer Pool中某个缓存页的数据，那它就和磁盘上的页不一致了，这样的缓存页也被称为脏页（英文名：dirty page）。最简单的做法就是每发生一次修改就立即同步到磁盘上对应的页上，但是频繁的往磁盘中写数据会严重的影响程序的性能。所以每次修改缓存页后，我们一般一般异步同步磁盘。 但是如果不立即同步到磁盘的话，那之后再同步的时候我们怎么知道Buffer Pool中哪些页是脏页，哪些页从来没被修改过呢？创建一个存储脏页的链表，凡是修改过的缓存页对应的控制块都会作为一个节点加入到一个链表中，因为这个链表节点对应的缓存页都是需要被刷新到磁盘上的，所以也叫flush链表。链表的构造和free链表差不多。 6.LRU链表的管理6.1 缓存不够的窘境Buffer Pool对应的内存大小毕竟是有限的，如果需要缓存的页占用的内存大小超过了Buffer Pool大小，也就是free链表中已经没有多余的空闲缓存页咋办？当然是把某些旧的缓存页从Buffer Pool中移除，然后再把新的页放进来， 那么移除哪些缓存页呢？ 我们设立Buffer Pool的初衷就是想减少和磁盘的IO交互，最好每次在访问某个页的时候它都已经被缓存到Buffer Pool中了。假设我们一共访问了n次页，那么被访问的页已经在缓存中的次数除以n就是所谓的缓存命中率，我们的期望就是让缓存命中率越高越好。所以是留下最近很频繁使用的。 6.2简单的LRU链表管理Buffer Pool的缓存页其实也是这个道理，当Buffer Pool中不再有空闲的缓存页时，就需要淘汰掉部分最近很少使用的缓存页。怎么知道哪些缓存页最近频繁使用，哪些最近很少使用呢？我们可以再创建一个链表，由于这个链表是为了按照最近最少使用的原则去淘汰缓存页的，所以这个链表可以被称为LRU链表（LRU的英文全称：Least Recently Used）。当我们需要访问某个页时，可以这样处理LRU链表： 如果该页不在Buffer Pool中，在把该页从磁盘加载到Buffer Pool中的缓存页时，就把该缓存页对应的控制块作为节点塞到链表的头部。 如果该页已经缓存在Buffer Pool中，则直接把该页对应的控制块移动到LRU链表的头部。 也就是说：只要我们使用到某个缓存页，就把该缓存页调整到LRU链表的头部，这样LRU链表尾部就是最近最少使用的缓存页了。 所以当Buffer Pool中的空闲缓存页使用完时，到LRU链表的尾部找些缓存页淘汰。 6.3划分区域的LRU链表上边的这个简单的LRU链表有问题，因为存在这两种比较尴尬的情况： 情况一：InnoDB提供了一个服务——预读（英文名：read ahead）。所谓预读，就是InnoDB认为执行当前的请求可能之后会读取某些页面，就预先把它们加载到Buffer Pool中。根据触发方式的不同，预读又可以细分为下边两种： 线性预读InnoDB提供了一个系统变量innodb_read_ahead_threshold，如果顺序访问了某个区（extent）的页面超过这个系统变量的值，就会触发一次异步读取下一个区中全部的页面到Buffer Pool的请求，异步读取意味着从磁盘中加载这些被预读的页面并不会影响到当前工作线程的正常执行。这个innodb_read_ahead_threshold系统变量的值默认是56，我们可以在服务器启动时通过启动参数或者服务器运行过程中直接调整该系统变量的值，不过它是一个全局变量，注意使用SET GLOBAL命令来修改。 InnoDB是怎么实现异步读取的呢？在Windows或者Linux平台上，可能是直接调用操作系统内核提供的AIO接口，在其它类Unix操作系统中，使用了一种模拟AIO接口的方式来实现异步读取，其实就是让别的线程去读取需要预读的页面。 随机预读如果Buffer Pool中已经缓存了某个区的13个连续的页面，不论这些页面是不是顺序读取的，都会触发一次异步读取本区中所有其的页面到Buffer Pool的请求。InnoDB同时提供了innodb_random_read_ahead系统变量，它的默认值为OFF，也就意味着InnoDB并不会默认开启随机预读的功能，如果我们想开启该功能，可以通过修改启动参数或者直接使用SET GLOBAL命令把该变量的值设置为ON。 如果预读到Buffer Pool中的页成功的被使用到，那就可以极大的提高语句执行的效率。可是如果用不到呢？这些预读的页都会放到LRU链表的头部，但是如果此时Buffer Pool的容量不太大而且很多预读的页面都没有用到的话，这就会导致处在LRU链表尾部的一些缓存页会很快的被淘汰掉，也就是所谓的劣币驱逐良币，会大大降低缓存命中率。 情况二：有一些扫描全表的查询语句（比如没有建立合适的索引或者压根儿没有WHERE子句的查询）。扫描全表意味着将访问到该表所在的所有页！假设这个表中记录非常多的话，那该表会占用特别多的页，当需要访问这些页时，会把它们统统都加载到Buffer Pool中，这也就意味着Buffer Pool中的所有页都被换了一次，其他查询语句在执行时又得执行一次从磁盘加载到Buffer Pool的操作。而这种全表扫描的语句执行的频率也不高，每次执行都要把Buffer Pool中的缓存页换一次，这严重的影响到其他查询对 Buffer Pool的使用，从而大大降低了缓存命中率。 总结一下上边说的可能降低Buffer Pool的两种情况： 加载到Buffer Pool中的页不一定被用到。 如果非常多的使用频率偏低的页被同时加载到Buffer Pool时，可能会把那些使用频率非常高的页从Buffer Pool中淘汰掉。 因为有这两种情况的存在，所以InnoDB把这个LRU链表按照一定比例分成两截，分别是： 一部分存储使用频率非常高的缓存页，所以这一部分链表也叫做热数据，或者称young区域。 另一部分存储使用频率不是很高的缓存页，所以这一部分链表也叫做冷数据，或者称old区域。 我们是按照某个比例将LRU链表分成两半的，不是某些节点固定是young区域的，某些节点固定是old区域的，随着程序的运行，某个节点所属的区域也可能发生变化。那这个划分成两截的比例怎么确定呢？对于InnoDB存储引擎来说，我们可以通过查看系统变量innodb_old_blocks_pct的值来确定old区域在LRU链表中所占的比例，比方说这样： 1234567mysql&gt; SHOW VARIABLES LIKE &#x27;innodb_old_blocks_pct&#x27;;+-----------------------+-------+| Variable_name | Value |+-----------------------+-------+| innodb_old_blocks_pct | 37 |+-----------------------+-------+1 row in set (0.01 sec) 从结果可以看出来，默认情况下，old区域在LRU链表中所占的比例是37%，也就是说old区域大约占LRU链表的3/8。这个比例我们是可以设置的，我们可以在启动时修改innodb_old_blocks_pct参数来控制old区域在LRU链表中所占的比例，比方说这样修改配置文件： 12[server]innodb_old_blocks_pct = 40 这样我们在启动服务器后，old区域占LRU链表的比例就是40%。当然，如果在服务器运行期间，我们也可以修改这个系统变量的值，不过需要注意的是，这个系统变量属于全局变量，一经修改，会对所有客户端生效，所以我们只能这样修改： 1SET GLOBAL innodb_old_blocks_pct = 40; 有了这个被划分成young和old区域的LRU链表之后，InnoDB就可以针对我们上边提到的两种可能降低缓存命中率的情况进行优化： 针对预读的页面可能不进行后续访问情况的优化InnoDB规定，当磁盘上的某个页面在初次加载到Buffer Pool中的某个缓存页时，该缓存页对应的控制块会被放到old区域的头部。这样针对预读到Buffer Pool却不进行后续访问的页面就会被逐渐从old区域逐出，而不会影响young区域中被使用比较频繁的缓存页。 针对全表扫描时，短时间内访问大量使用频率非常低的页面情况的优化在进行全表扫描时，虽然首次被加载到Buffer Pool的页被放到了old区域的头部，但是后续会被马上访问到，每次进行访问的时候又会把该页放到young区域的头部，这样仍然会把那些使用频率比较高的页面给顶下去。全表扫描有一个特点，那就是它的执行频率非常低，而且在执行全表扫描的过程中，即使某个页面中有很多条记录，也就是去多次访问这个页面所花费的时间也是非常少的。所以我们只需要规定，在对某个处在old区域的缓存页进行第一次访问时就在它对应的控制块中记录下来这个访问时间，如果后续的访问时间与第一次访问的时间在某个时间间隔内，那么该页面就不会被从old区域移动到young区域的头部，否则将它移动到young区域的头部。上述的这个间隔时间是由系统变量innodb_old_blocks_time控制的： 1234567mysql&gt; SHOW VARIABLES LIKE &#x27;innodb_old_blocks_time&#x27;;+------------------------+-------+| Variable_name | Value |+------------------------+-------+| innodb_old_blocks_time | 1000 |+------------------------+-------+1 row in set (0.01 sec) 这个innodb_old_blocks_time的默认值是1000，它的单位是毫秒，也就意味着对于从磁盘上被加载到LRU链表的old区域的某个页来说，如果第一次和最后一次访问该页面的时间间隔小于1s（很明显在一次全表扫描的过程中，多次访问一个页面中的时间不会超过1s），那么该页是不会被加入到young区域的。 当然，像innodb_old_blocks_pct一样，我们也可以在服务器启动或运行时设置innodb_old_blocks_time的值。 这里需要注意的是，如果我们把innodb_old_blocks_time的值设置为0，那么每次我们访问一个页面时就会把该页面放到young区域的头部。 综上所述，正是因为将LRU链表划分为young和old区域这两个部分，又添加了innodb_old_blocks_time这个系统变量，才使得预读机制和全表扫描造成的缓存命中率降低的问题得到了遏制，因为用不到的预读页面以及全表扫描的页面都只会被放到old区域，而不影响young区域中的缓存页。 6.4 更进一步优化LRU链表对于young区域的缓存页来说，我们每次访问一个缓存页就要把它移动到LRU链表的头部，这样开销太大了，毕竟在young区域的缓存页都是热点数据，也就是可能被经常访问的，这样频繁的对LRU链表进行节点移动操作不太好，为了解决这个问题其实我们还可以提出一些优化策略，比如只有被访问的缓存页位于young区域的1/4的后边，才会被移动到LRU链表头部，这样就可以降低调整LRU链表的频率，从而提升性能（也就是说如果某个缓存页对应的节点在young区域的1/4中，再次访问该缓存页时也不会将其移动到LRU链表头部）。 介绍随机预读的时候曾说，如果Buffer Pool中有某个区的13个连续页面就会触发随机预读，这其实是不严谨的（但是MySQL文档就是这么说的），其实还要求这13个页面是非常热的页面，所谓的非常热，指的是这些页面在整个young区域的头1/4处。 还有针对LRU链表的优化措施，核心就是尽量高效的提高 Buffer Pool 的缓存命中率。 7.其他的一些链表为了更好的管理Buffer Pool中的缓存页，除了我们上边提到的一些措施，InnoDB还引进了其他的一些链表，比如unzip LRU链表用于管理解压页，zip clean链表用于管理没有被解压的压缩页，zip free数组中每一个元素都代表一个链表，它们组成所谓的伙伴系统来为压缩页提供内存空间等等，为了更好的管理这个Buffer Pool引入了各种链表或其他数据结构。 8.刷新脏页到磁盘后台有专门的线程每隔一段时间负责把脏页刷新到磁盘，这样可以不影响用户线程处理正常的请求。主要有两种刷新路径： 从LRU链表的冷数据中刷新一部分页面到磁盘。后台线程会定时从LRU链表尾部开始扫描一些页面，扫描的页面数量可以通过系统变量innodb_lru_scan_depth来指定，如果从里边儿发现脏页，会把它们刷新到磁盘。这种刷新页面的方式被称之为BUF_FLUSH_LRU。 从flush链表中刷新一部分页面到磁盘。后台线程也会定时从flush链表中刷新一部分页面到磁盘，刷新的速率取决于当时系统是不是很繁忙。这种刷新页面的方式被称之为BUF_FLUSH_LIST。 有时候后台线程刷新脏页的进度比较慢，导致用户线程在准备加载一个磁盘页到Buffer Pool时没有可用的缓存页，这时就会尝试看看LRU链表尾部有没有可以直接释放掉的未修改页面，如果没有的话会不得不将LRU链表尾部的一个脏页同步刷新到磁盘（和磁盘交互是很慢的，这会降低处理用户请求的速度）。这种刷新单个页面到磁盘中的刷新方式被称之为BUF_FLUSH_SINGLE_PAGE。 当然，有时候系统特别繁忙时，也可能出现用户线程批量的从flush链表中刷新脏页的情况，很显然在处理用户请求过程中去刷新脏页是一种严重降低处理速度的行为，这属于一种迫不得已的情况。 9.多个Buffer Pool实例Buffer Pool本质是InnoDB向操作系统申请的一块连续的内存空间，在多线程环境下，访问Buffer Pool中的各种链表都需要加锁处理，在Buffer Pool特别大而且多线程并发访问特别高的情况下，单一的Buffer Pool可能会影响请求的处理速度。所以在Buffer Pool特别大的时候，我们可以把它们拆分成若干个小的Buffer Pool，每个Buffer Pool都称为一个实例，它们都是独立的，独立的去申请内存空间，独立的管理各种链表，所以在多线程并发访问时并不会相互影响，从而提高并发处理能力。我们可以在服务器启动的时候通过设置innodb_buffer_pool_instances的值来修改Buffer Pool实例的个数，比方说这样： 12[server]innodb_buffer_pool_instances = 2 这样就表明我们要创建2个Buffer Pool实例。 每个Buffer Pool实例实际占多少内存空间呢？其实使用这个公式算出来的： 1innodb_buffer_pool_size/innodb_buffer_pool_instances 也就是总共的大小除以实例的个数，结果就是每个Buffer Pool实例占用的大小。 不过也不是说Buffer Pool实例创建的越多越好，分别管理各个Buffer Pool也是需要性能开销的，InnoDB规定：当innodb_buffer_pool_size的值小于1G的时候设置多个实例是无效的，InnoDB会默认把innodb_buffer_pool_instances 的值修改为1。而MySQL希望在Buffer Pool大于或等于1G的时候设置多个Buffer Pool实例。 10.innodb_buffer_pool_chunk_size在MySQL 5.7.5之前，Buffer Pool的大小只能在服务器启动时通过配置innodb_buffer_pool_size启动参数来调整大小，在服务器运行过程中是不允许调整该值的。不过MySQL在5.7.5以及之后的版本中支持了在服务器运行过程中调整Buffer Pool大小的功能，但是有一个问题，就是每次当我们要重新调整Buffer Pool大小时，都需要重新向操作系统申请一块连续的内存空间，然后将旧的Buffer Pool中的内容复制到这一块新空间，这是极其耗时的。所以MySQL决定不再一次性为某个Buffer Pool实例向操作系统申请一大片连续的内存空间，而是以一个所谓的chunk为单位向操作系统申请空间。也就是说一个Buffer Pool实例其实是由若干个chunk组成的，一个chunk就代表一片连续的内存空间，里边儿包含了若干缓存页与其对应的控制块。 正是因为发明了这个chunk的概念，我们在服务器运行期间调整Buffer Pool的大小时就是以chunk为单位增加或者删除内存空间，而不需要重新向操作系统申请一片大的内存，然后进行缓存页的复制。这个所谓的chunk的大小是我们在启动操作MySQL服务器时通过innodb_buffer_pool_chunk_size启动参数指定的，它的默认值是134217728，也就是128M。不过需要注意的是，innodb_buffer_pool_chunk_size的值只能在服务器启动时指定，在服务器运行过程中是不可以修改的。 为什么不允许在服务器运行过程中修改innodb_buffer_pool_chunk_size的值？因为innodb_buffer_pool_chunk_size的值代表InnoDB向操作系统申请的一片连续的内存空间的大小，如果你在服务器运行过程中修改了该值，就意味着要重新向操作系统申请连续的内存空间并且将原先的缓存页和它们对应的控制块复制到这个新的内存空间中，这是十分耗时的操作！ 另外，这个innodb_buffer_pool_chunk_size的值并不包含缓存页对应的控制块的内存空间大小，所以实际上InnoDB向操作系统申请连续内存空间时，每个chunk的大小要比innodb_buffer_pool_chunk_size的值大一些，约5%。 11.配置Buffer Pool时的注意事项 innodb_buffer_pool_size必须是innodb_buffer_pool_chunk_size × innodb_buffer_pool_instances的倍数（这主要是想保证每一个Buffer Pool实例中包含的chunk数量相同）。假设我们指定的innodb_buffer_pool_chunk_size的值是128M，innodb_buffer_pool_instances的值是16，那么这两个值的乘积就是2G，也就是说innodb_buffer_pool_size的值必须是2G或者2G的整数倍。比方说我们在启动MySQL服务器是这样指定启动参数的：默认的innodb_buffer_pool_chunk_size值是128M，指定的innodb_buffer_pool_instances的值是16，所以innodb_buffer_pool_size的值必须是2G或者2G的整数倍，上边例子中指定的innodb_buffer_pool_size的值是8G，符合规定，所以在服务器启动完成之后我们查看一下该变量的值就是我们指定的8G（8589934592字节）：如果我们指定的innodb_buffer_pool_size大于2G并且不是2G的整数倍，那么服务器会自动的把innodb_buffer_pool_size的值调整为2G的整数倍，比方说我们在启动服务器时指定的innodb_buffer_pool_size的值是9G：那么服务器会自动把innodb_buffer_pool_size的值调整为10G，10737418240字节。 1mysqld --innodb-buffer-pool-size=8G --innodb-buffer-pool-instances=16 1234567mysql&gt; show variables like &#x27;innodb_buffer_pool_size&#x27;;+-------------------------+------------+| Variable_name | Value |+-------------------------+------------+| innodb_buffer_pool_size | 8589934592 |+-------------------------+------------+1 row in set (0.00 sec) 1mysqld --innodb-buffer-pool-size=9G --innodb-buffer-pool-instances=16 1234567mysql&gt; show variables like &#x27;innodb_buffer_pool_size&#x27;;+-------------------------+-------------+| Variable_name | Value |+-------------------------+-------------+| innodb_buffer_pool_size | 10737418240 |+-------------------------+-------------+1 row in set (0.01 sec) 如果在服务器启动时，innodb_buffer_pool_chunk_size × innodb_buffer_pool_instances的值已经大于innodb_buffer_pool_size的值，那么innodb_buffer_pool_chunk_size的值会被服务器自动设置为innodb_buffer_pool_size/innodb_buffer_pool_instances的值。比方说我们在启动服务器时指定的innodb_buffer_pool_size的值为2G，innodb_buffer_pool_instances的值为16，innodb_buffer_pool_chunk_size的值为256M：由于256M × 16 = 4G，而4G &gt; 2G，所以innodb_buffer_pool_chunk_size值会被服务器改写为innodb_buffer_pool_size/innodb_buffer_pool_instances的值，也就是：2G/16 = 128M（134217728字节）。 1mysqld --innodb-buffer-pool-size=2G --innodb-buffer-pool-instances=16 --innodb-buffer-pool-chunk-size=256M 123456789101112131415mysql&gt; show variables like &#x27;innodb_buffer_pool_size&#x27;;+-------------------------+------------+| Variable_name | Value |+-------------------------+------------+| innodb_buffer_pool_size | 2147483648 |+-------------------------+------------+1 row in set (0.01 sec)mysql&gt; show variables like &#x27;innodb_buffer_pool_chunk_size&#x27;;+-------------------------------+-----------+| Variable_name | Value |+-------------------------------+-----------+| innodb_buffer_pool_chunk_size | 134217728 |+-------------------------------+-----------+1 row in set (0.00 sec) 12.Buffer Pool中存储的其它信息Buffer Pool的缓存页除了用来缓存磁盘上的页面以外，还可以存储锁信息、自适应哈希索引等信息。 13.查看Buffer Pool的状态信息MySQL给我们提供了SHOW ENGINE INNODB STATUS语句来查看关于InnoDB存储引擎运行过程中的一些状态信息，其中就包括Buffer Pool的一些信息，我们看一下（为了突出重点，我们只把输出中关于Buffer Pool的部分提取了出来）： 123456789101112131415161718192021222324252627mysql&gt; SHOW ENGINE INNODB STATUS\\G(...省略前边的许多状态)----------------------BUFFER POOL AND MEMORY----------------------Total memory allocated 13218349056;Dictionary memory allocated 4014231Buffer pool size 786432Free buffers 8174Database pages 710576Old database pages 262143Modified db pages 124941Pending reads 0Pending writes: LRU 0, flush list 0, single page 0Pages made young 6195930012, not young 78247510485108.18 youngs/s, 226.15 non-youngs/sPages read 2748866728, created 29217873, written 4845680877160.77 reads/s, 3.80 creates/s, 190.16 writes/sBuffer pool hit rate 956 / 1000, young-making rate 30 / 1000 not 605 / 1000Pages read ahead 0.00/s, evicted without access 0.00/s, Random read ahead 0.00/sLRU len: 710576, unzip_LRU len: 118I/O sum[134264]:cur[144], unzip sum[16]:cur[0]--------------(...省略后边的许多状态)mysql&gt; 我们来详细看一下这里边的每个值都代表什么意思： Total memory allocated：代表Buffer Pool向操作系统申请的连续内存空间大小，包括全部控制块、缓存页、以及碎片的大小。 Dictionary memory allocated：为数据字典信息分配的内存空间大小，注意这个内存空间和Buffer Pool没啥关系，不包括在Total memory allocated中。 Buffer pool size：代表该Buffer Pool可以容纳多少缓存页，注意，单位是页！ Free buffers：代表当前Buffer Pool还有多少空闲缓存页，也就是free链表中还有多少个节点。 Database pages：代表LRU链表中的页的数量，包含young和old两个区域的节点数量。 Old database pages：代表LRU链表old区域的节点数量。 Modified db pages：代表脏页数量，也就是flush链表中节点的数量。 Pending reads：正在等待从磁盘上加载到Buffer Pool中的页面数量。当准备从磁盘中加载某个页面时，会先为这个页面在Buffer Pool中分配一个缓存页以及它对应的控制块，然后把这个控制块添加到LRU的old区域的头部，但是这个时候真正的磁盘页并没有被加载进来，Pending reads的值会跟着加1。 Pending writes LRU：即将从LRU链表中刷新到磁盘中的页面数量。 Pending writes flush list：即将从flush链表中刷新到磁盘中的页面数量。 Pending writes single page：即将以单个页面的形式刷新到磁盘中的页面数量。 Pages made young：代表LRU链表中曾经从old区域移动到young区域头部的节点数量。这里需要注意，一个节点每次只有从old区域移动到young区域头部时才会将Pages made young的值加1，也就是说如果该节点本来就在young区域，由于它符合在young区域1/4后边的要求，下一次访问这个页面时也会将它移动到young区域头部，但这个过程并不会导致Pages made young的值加1。 Page made not young：在将innodb_old_blocks_time设置的值大于0时，首次访问或者后续访问某个处在old区域的节点时由于不符合时间间隔的限制而不能将其移动到young区域头部时，Page made not young的值会加1。这里需要注意，对于处在young区域的节点，如果由于它在young区域的1/4处而导致它没有被移动到young区域头部，这样的访问并不会将Page made not young的值加1。 youngs/s：代表每秒从old区域被移动到young区域头部的节点数量。 non-youngs/s：代表每秒由于不满足时间限制而不能从old区域移动到young区域头部的节点数量。 Pages read、created、written：代表读取，创建，写入了多少页。后边跟着读取、创建、写入的速率。 Buffer pool hit rate：表示在过去某段时间，平均访问1000次页面，有多少次该页面已经被缓存到Buffer Pool了。 young-making rate：表示在过去某段时间，平均访问1000次页面，有多少次访问使页面移动到young区域的头部了。 需要注意的一点是，这里统计的将页面移动到**young**区域的头部次数不仅仅包含从**old**区域移动到**young**区域头部的次数，还包括从**young**区域移动到**young**区域头部的次数（访问某个**young**区域的节点，只要该节点在**young**区域的1/4处往后，就会把它移动到**young**区域的头部）。 not (young-making rate)：表示在过去某段时间，平均访问1000次页面，有多少次访问没有使页面移动到young区域的头部。 需要注意的一点是，这里统计的没有将页面移动到**young**区域的头部次数不仅仅包含因为设置了**innodb_old_blocks_time**系统变量而导致访问了**old**区域中的节点但没把它们移动到**young**区域的次数，还包含因为该节点在**young**区域的前1/4处而没有被移动到**young**区域头部的次数。 LRU len：代表LRU链表中节点的数量。 unzip_LRU：代表unzip_LRU链表中节点的数量。 I/O sum：最近50s读取磁盘页的总数。 I/O cur：现在正在读取的磁盘页数量。 I/O unzip sum：最近50s解压的页面数量。 I/O unzip cur：正在解压的页面数量。 三，总结 磁盘太慢，用内存作为缓存很有必要。 Buffer Pool本质上是InnoDB向操作系统申请的一段连续的内存空间，可以通过innodb_buffer_pool_size来调整它的大小。 Buffer Pool向操作系统申请的连续内存由控制块和缓存页组成，每个控制块和缓存页都是一一对应的，在填充足够多的控制块和缓存页的组合后，Buffer Pool剩余的空间可能产生不够填充一组控制块和缓存页，这部分空间不能被使用，也被称为碎片。 InnoDB使用了许多链表来管理Buffer Pool。 free链表中每一个节点都代表一个空闲的缓存页，在将磁盘中的页加载到Buffer Pool时，会从free链表中寻找空闲的缓存页。 为了快速定位某个页是否被加载到Buffer Pool，使用表空间号 + 页号作为key，缓存页作为value，建立哈希表。 在Buffer Pool中被修改的页称为脏页，脏页并不是立即刷新，而是被加入到flush链表中，待之后的某个时刻同步到磁盘上。 LRU链表分为young和old两个区域，可以通过innodb_old_blocks_pct来调节old区域所占的比例。首次从磁盘上加载到Buffer Pool的页会被放到old区域的头部，在innodb_old_blocks_time间隔时间内访问该页不会把它移动到young区域头部。在Buffer Pool没有可用的空闲缓存页时，会首先淘汰掉old区域的一些页。 我们可以通过指定innodb_buffer_pool_instances来控制Buffer Pool实例的个数，每个Buffer Pool实例中都有各自独立的链表，互不干扰。 自MySQL 5.7.5版本之后，可以在服务器运行过程中调整Buffer Pool大小。每个Buffer Pool实例由若干个chunk组成，每个chunk的大小可以在服务器启动时通过启动参数调整。 可以用下边的命令查看Buffer Pool的状态信息： 1SHOW ENGINE INNODB STATUS\\G","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/tags/MySQL/"}]},{"title":"MySQL[十一]高性能MySQL调优实战","slug":"MySQL/MySQL[十一]高性能MySQL调优实战","date":"2022-01-11T03:16:57.298Z","updated":"2022-01-11T03:24:28.380Z","comments":true,"path":"2022/01/11/MySQL/MySQL[十一]高性能MySQL调优实战/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/MySQL/MySQL[%E5%8D%81%E4%B8%80]%E9%AB%98%E6%80%A7%E8%83%BDMySQL%E8%B0%83%E4%BC%98%E5%AE%9E%E6%88%98/","excerpt":"","text":"一，数据库应该如何优化数据库优化有很多层面。 1.SQL与索引因为 SQL 语句是在我们的应用端编写的，所以第一步，我们可以在程序中对 SQL 语句进行优化，最终的目标是用到索引。这个是容易的也是最常用的优化手段。 2.表与存储引擎数据是存放在表里面的，表又是以不同的格式存放在存储引擎中的，所以我们可以选用特定的存储引擎，或者对表进行分区，对表结构进行拆分或者冗余处理，或者对表结构比如字段的定义进行优化。 3.架构对于数据库的服务，我们可以对它的架构进行优化。如果只有一台数据库的服务器，我们可以运行多个实例，做集群的方案，做负载均衡。或者基于主从复制实现读写分离，让写的服务都访问 master 服务器，读的请求都访问从服务器，slave 服务器自动 master 主服务器同步数据。或者在数据库前面加一层缓存，达到减少数据库的压力，提升访问速度的目的。为了分散数据库服务的存储压力和访问压力，我们也可以把不同的数据分布到不同的服务节点，这个就是分库分表（scale out）。 注意主从（replicate）和分片（shard）的区别： 主从通过数据冗余实现高可用，和实现读写分离。 分片通过拆分数据分散存储和访问压力。 4.配置数据库配置的优化，比如连接数，缓冲区大小等等，优化配置的目的都是为了更高效地利用硬件。 5.操作系统与硬件从上往下，成本收益比慢慢地在增加。所以肯定不是查询一慢就堆硬件，堆硬件叫做向上的扩展（scale up）。 二，慢日志查询1.概述MySQL的慢查询日志是MySQL提供的一种日志记录，它用来记录在MySQL中响应时间超过阀值的语句，具体指运行时间超过long_query_time值的SQL，则会被记录到慢查询日志中。long_query_time的默认值为10，意思是运行10秒以上的语句。由他来查看哪些SQL超出了我们的最大忍耐时间值，比如一条sql执行超过5秒钟，我们就算慢SQL，希望能收集超过5秒的sql，结合explain进行全面分析。 2.实操默认情况下，MySQL数据库没有开启慢查询日志，需要我们手动来设置这个参数。 当然，如果不是调优需要的话，一般不建议启动该参数，因为开启慢查询日志会或多或少带来一定的性能影响。慢查询日志支持将日志记录写入文件。 2.1查看及开启①日志1SHOW VARIABLES LIKE &#x27;%slow_query_log%&#x27;; 默认情况下slow_query_log的值为OFF，表示慢查询日志是禁用的。 1set global slow_query_log=1; 只对窗口生效，重启服务失效。 ②时间1SHOW VARIABLES LIKE &#x27;%long_query_time%&#x27;; 1SET GLOBAL long_query_time=0.1; 全局变量设置，对所有客户端有效。但必须是设置后进行登录的客户端。 1SET SESSION long_query_time=0.1; #session可省略 对当前会话连接立即生效，对其他客户端无效。 假如运行时间正好等于long_query_time的情况，并不会被记录下来。也就是说，在mysql源码里是判断大于long_query_time，而非大于等于。 ③永久生效修改配置文件my.cnf（其它系统变量也是如此） [mysqld]下增加或修改参数 slow_query_log 和slow_query_log_file后，然后重启MySQL服务器。也即将如下两行配置进my.cnf文件 [1] 1234slow_query_log =1slow_query_log_file=/var/lib/mysql/yhd-slow.log long_query_time=3log_output=FILE 2.2Case记录慢SQL并后续分析 查询当前系统中有多少条慢查询记录 1SHOW GLOBAL STATUS LIKE &#x27;%Slow_queries%&#x27;; 3.日志分析工具-mysqldumpslow在生产环境中，如果要手工分析日志，查找、分析SQL，显然是个体力活，MySQL提供了日志分析工具mysqldumpslow。 查看mysqldumpslow的帮助信息（windows下需要安装perl环境） 1mysqldumpslow --help -a: 不将数字抽象成N，字符串抽象成S-s: 是表示按照何种方式排序；c: 访问次数l: 锁定时间r: 返回记录t: 查询时间al:平均锁定时间ar:平均返回记录数at:平均查询时间-t: 即为返回前面多少条的数据；-g: 后边搭配一个正则匹配模式，大小写不敏感的； 3.1常用SQL12345678得到返回记录集最多的10个SQLmysqldumpslow -s r -t 10 /var/lib/mysql/yhd-slow.log得到访问次数最多的10个SQLmysqldumpslow -s c -t 10 /var/lib/mysql/yhd-slow.log得到按照时间排序的前10条里面含有左连接的查询语句mysqldumpslow -s t -t 10 -g &quot;left join&quot; /var/lib/mysql/yhd-slow.log另外建议在使用这些命令时结合 | 和more 使用 ，否则有可能出现爆屏情况mysqldumpslow -s r -t 10 /var/lib/mysql/yhd-slow.log | more 4.SHOW PROCESSLIST作用：查询所有用户正在干什么。 如果出现不顺眼的：kill [id] 三，EXPLAIN调优实战1.准备数据员工表插入500w数据，部门表插入10w数据。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586CREATE TABLE `dept`( `id` INT(11) NOT NULL AUTO_INCREMENT, `deptName` VARCHAR(30) DEFAULT NULL, `address` VARCHAR(40) DEFAULT NULL, `ceo` INT NULL, PRIMARY KEY (`id`)) ENGINE = INNODB AUTO_INCREMENT = 1 DEFAULT CHARSET = utf8;CREATE TABLE `emp`( `id` INT(11) NOT NULL AUTO_INCREMENT, `empno` INT NOT NULL, `name` VARCHAR(20) DEFAULT NULL, `age` INT(3) DEFAULT NULL, `deptId` INT(11) DEFAULT NULL, PRIMARY KEY (`id`) #CONSTRAINT `fk_dept_id` FOREIGN KEY (`deptId`) REFERENCES `t_dept` (`id`)) ENGINE = INNODB AUTO_INCREMENT = 1 DEFAULT CHARSET = utf8;#生成随机字符串DELIMITER $$CREATE FUNCTION rand_string(n INT) RETURNS VARCHAR(255)BEGIN DECLARE chars_str VARCHAR(100) DEFAULT &#x27;abcdefghijklmnopqrstuvwxyzABCDEFJHIJKLMNOPQRSTUVWXYZ&#x27;; DECLARE return_str VARCHAR(255) DEFAULT &#x27;&#x27;; DECLARE i INT DEFAULT 0; WHILE i &lt; n DO SET return_str = CONCAT(return_str, SUBSTRING(chars_str, FLOOR(1 + RAND() * 52), 1)); SET i = i + 1; END WHILE; RETURN return_str;END $$#用于随机产生多少到多少的编号DELIMITER $$CREATE FUNCTION rand_num(from_num INT, to_num INT) RETURNS INT(11)BEGIN DECLARE i INT DEFAULT 0; SET i = FLOOR(from_num + RAND() * (to_num - from_num + 1)); RETURN i;END$$#创建往emp表中插入数据的存储过程DELIMITER $$CREATE PROCEDURE insert_emp(START INT, max_num INT)BEGIN DECLARE i INT DEFAULT 0; SET autocommit = 0; #设置手动提交事务 REPEAT #循环 SET i = i + 1; #赋值 INSERT INTO emp (empno, NAME, age, deptid) VALUES ((START + i), rand_string(6), rand_num(30, 50), rand_num(1, 10000)); UNTIL i = max_num END REPEAT; COMMIT; #提交事务END$$#创建往dept表中插入数据的存储过程DELIMITER $$CREATE PROCEDURE `insert_dept`(max_num INT)BEGIN DECLARE i INT DEFAULT 0; SET autocommit = 0; REPEAT SET i = i + 1; INSERT INTO dept (deptname, address, ceo) VALUES (rand_string(8), rand_string(10), rand_num(1, 500000)); UNTIL i = max_num END REPEAT; COMMIT;END$$#执行存储过程，往dept表添加10万条数据CALL insert_dept(100000);#执行存储过程，往emp表添加500万条数据CALL insert_emp(100000, 5000000); 2.批量删除索引建立好的索引在哪里？ 12SHOW INDEX FROM t_emp ; -- 只能查看索引，但不能删除。information_schema.STATISTICS -- 存储索引的表（元数据库，统计表），我们可以对表数据进行删除操作。 知识点 删除某一个索引 1DROP INDEX idx_xxx ON emp 查出该表有哪些索引，索引名–&gt;集合 12345678SHOW INDEX FROM t_emp-- 元数据：meta DATA 描述数据的数据SELECT index_nameFROM information_schema.STATISTICSWHERE table_name = &#x27;t_emp&#x27; AND table_schema = &#x27;mydb&#x27; AND index_name &lt;&gt; &#x27;PRIMARY&#x27; AND seq_in_index = 1 3.单表使用索引建立索引 12CREATE INDEX idx_age_deptid_name ON emp(age,deptid,NAME);CREATE INDEX idx_name ON emp(NAME); 3.1 全值匹配1234567891011121314151617181920# 单表查询-全值匹配EXPLAINSELECT SQL_NO_CACHE *FROM empWHERE emp.age = 30;EXPLAINSELECT SQL_NO_CACHE *FROM empWHERE emp.age = 30 and deptid = 4;EXPLAINSELECT SQL_NO_CACHE *FROM empWHERE emp.age = 30 and deptid = 4 AND emp.name = &#x27;abcd&#x27;; 3.2 最左前缀法则12345678# 单表查询-左前缀法则EXPLAIN SELECT * FROM emp WHERE age=1 AND deptid=1 AND NAME=&#x27;aaa&#x27;;EXPLAIN SELECT * FROM emp WHERE age=1 AND deptid=1;EXPLAIN SELECT * FROM emp WHERE age=1 AND NAME=&#x27;aaa&#x27; AND deptid=1;EXPLAIN SELECT * FROM emp WHERE deptid=1 AND NAME =&#x27;aaa&#x27;; 过滤条件要使用索引必须按照索引建立时的顺序，依次满足，一旦跳过某个字段，索引后面的字段都无法被使用。 3.3 索引列上计算/函数导致索引失效1234# 单表查询-操作索引列导致索引失效EXPLAIN SELECT SQL_NO_CACHE * FROM emp WHERE emp.name LIKE &#x27;abc%&#x27;;EXPLAIN SELECT SQL_NO_CACHE * FROM emp WHERE LEFT(emp.name,3) = &#x27;abc&#x27;; 3.4 范围查询导致的索引失效12345678EXPLAIN SELECT SQL_NO_CACHE * FROM emp WHERE emp.name = &#x27;abc&#x27; AND emp.deptId &gt; 20 AND emp.age = 30 ; 应用开发中范围查询，例如： 金额查询，日期查询往往都是范围查询。应将查询条件放置where语句最后。 3.5 不等于(!= 或者&lt;&gt;)索引失效1EXPLAIN SELECT SQL_NO_CACHE * FROM emp WHERE emp.name &lt;&gt; &#x27;abc&#x27; ; 3.6 is not null无法使用索引，is null可使用索引1234EXPLAIN SELECT SQL_NO_CACHE * FROM emp WHERE age IS NULL;#用到索引 EXPLAIN SELECT SQL_NO_CACHE * FROM emp WHERE age IS NOT NULL;#未用到索引 3.7 like以%开头索引失效1EXPLAIN SELECT SQL_NO_CACHE * FROM emp WHERE NAME LIKE &#x27;%aaa&#x27;; 3.8 类型转换导致索引失效1EXPLAIN SELECT SQL_NO_CACHE * FROM emp WHERE NAME=123; 设计实体类属性时，一定要与数据库字段类型相对应，否则会出现类型转换的情况，导致索引失效。 4. 关联查询优化4.1 左外连接1explain select * from emp left join dept on emp.deptId=dept.id; 这种情况下，驱动表无法避免全表扫描，但是因为被驱动表的主键存在索引并且是两张表关联查询的关联条件，所以可以避免被驱动表的全表扫描。 4.2 内连接(TODO)内连接MySQL会自动为我们选择驱动表。 123456explain select * from dept straight_join emp on emp.deptId=dept.id;## 1. dept 全表扫描 10w## 2. emp deptid refexplain select * from dept join emp on emp.deptId=dept.id;## 1. emp 500w## 2. dept id ref 保证被驱动表的join字段被索引 left join 时，选择小表作为驱动表，大表作为被驱动表 inner join 时，mysql会自动将小结果集的表选为驱动表。选择相信mysql优化策略。 子查询尽量不要放在被驱动表，衍生表建不了索引。 能够直接多表关联的尽量直接关联，不用子查询。 两张表的连接查询，比方说 left join right、inner join 等，他们的连表方式是什么？ 连表查询一共三种算法：nlj bnl bka 算法 。 right join 底层，会给你转化为left join。 4.3 子查询优化1234567891011121314151617181920212223#①不推荐explain SELECT *FROM empWHERE emp.id NOT IN -- not in 导致无法对in进行优化，用不了exists (SELECT dept.ceo FROM dept WHERE dept.ceo IS NOT NULL) ; -- is not null 导致索引失效#②推荐explain SELECT emp.*FROM emp LEFT JOIN dept ON emp.id = dept.ceo -- 如果ceo没有索引，两张表都是全表扫描，如果ceo有索引，被驱动表就是ref级别WHERE dept.id IS NULL ;# 尝试在ceo创建索引后，确实是 create index idx_ceo on dept(ceo); 尽量不要使用not in 或者 not exists，会使索引失效。 MySQL自动做出的子查询优化，物化子查询，转为半连接 物化子查询：把子查询的结果查出来后，建立一个临时表，“物化”-&gt;变成一张内存临时表 半连接：把子查询转化为类似连接查询的方式，但又不是真正的连接查询，所以叫 半 连接优化 5.排序分组优化5.1 无过滤，不索引1234EXPLAIN SELECT SQL_NO_CACHE * FROM emp ORDER BY age,deptid; #没用上索引，Using filesort EXPLAIN SELECT SQL_NO_CACHE * FROM emp ORDER BY age,deptid LIMIT 10; #使用上索引 null 因为order by的字段顺序和索引的顺序一样，所以此时会先尝试内存排序，但是因为上面的sql没有limit，导致内存放不下，使用了文件排序（文件系统级别，相当于在磁盘做排序），所以第一条sql效率更低。 order后面的字段想要使用索引，必须要有过滤条件，limit也行。 5.2顺序错，必排序1234567891011121314EXPLAIN SELECT * FROM emp WHERE age=45 ORDER BY deptid;# Using index conditionEXPLAIN SELECT * FROM emp WHERE age=45 ORDER BY deptid,NAME;# Using index conditionEXPLAIN SELECT * FROM emp WHERE age=45 ORDER BY deptid,empno;# Using index condition; Using filesortEXPLAIN SELECT * FROM emp WHERE age=45 ORDER BY NAME,deptid;# Using index condition; Using filesortEXPLAIN SELECT * FROM emp WHERE deptid=45 ORDER BY age;# Using where; Using filesort 在SQL语句中的顺序一定要和定义索引中的字段顺序完全一致。 5.3 方向反，必排序1234EXPLAIN SELECT * FROM emp WHERE age=45 ORDER BY deptid DESC, NAME DESC ;#Using whereEXPLAIN SELECT * FROM emp WHERE age=45 ORDER BY deptid ASC, NAME DESC ;#Using index condition; Using filesort ORDER BY子句，尽量使用Index方式排序，避免使用FileSort方式排序 要么全升序、要么全降序。有升有降无法使用索引。 5.4 索引的选择 两个索引同时存在，mysql自动选择最优的方案，但是，随着数据量的变化，选择的索引也会随之变化的。 所有的排序都是在条件过滤之后才执行的，所以，如果条件过滤掉大部分数据的话，剩下几百几千条数据进行排序其实并不是很消耗性能，即使索引优化了排序，但实际提升性能很有限。 当【范围条件】和【group by 或者 order by】的字段出现二选一时，优先观察条件字段的过滤数量，如果过滤的数据足够多，而需要排序的数据并不多时，优先把索引放在范围（过滤条件）字段上。反之，亦然。 扫描行数的多少，就是explain里的rows，可以说明一个需要扫描的行数多，一个扫描行数少，扫描行数多，代表成本高，扫描行数少代表成本少。优化器最终是对比成本值的大小来选取索引的。准确的说，是MySQL基于成本，优化器是在server层。 有时候优化器会选择错索引为什么？ 主要是出在优化器预估行数上，这个涉及到了一条sql的执行流程，语法分析，词法分析之后，进入优化阶段，由优化器进行优化，在优化阶段，会尽可能的生成全部的执行计划，然后对比一下哪一个成本值最低，就选它，所以优化器有一个选择索引，选择表的连接顺序的过程，索引不同，成本不同，读表顺序不同，成本不同，索引的选取，需要存储引擎提供统计信息，innodb中，统计信息是随机采样，随机选取8个索引页，取平均值，当做该索引的全部情况，也就是部分代表整体，也就是最终导致rows那里是个预估值，而不是准确的。所以有时候MySQL选错了索引，有一定概率，是由于这个随机采样造成的。而随机采样的不准确，是由于数据不断添加导致索引页的分裂，导致有些页内数据较少。 解决方案： 执行一下alter table +表名 就可以使统计信息稍微准确点，他会重新构建索引，使索引页保持紧凑，这个就是B+树的分裂。 调整参数，加大InnoDB采样的页数，页数越大越精确，但性能消耗更高。一般不建议这么干。 在优化阶段，会对表中所有索引进行对比，优化器基于成本的原因，选择成本最低的索引，所以会错过最佳索引。带来的问题便是，执行速度很慢。 解决方案： 通过explain查看执行计划，结合sql条件查看可以利用哪些索引。 使用 force index(indexName)强制走指定索引。弊端就是后期若索引名发生改变，或索引被删除，该sql语句需要调整。 5.5 双路排序&amp;单路排序如果不在索引列上，filesort有两种算法： mysql就要启动双路排序和单路排序。 双路排序 MySQL 4.1之前是使用双路排序，字面意思就是两次扫描磁盘，最终得到数据， 读取行指针和order by列，对他们进行排序，然后扫描已经排序好的列表，按照列表中的值重新从列表中读取对应的数据输出 从磁盘取排序字段，在buffer进行排序，再从磁盘取其他字段。 取一批数据，要对磁盘进行两次扫描，众所周知，I\\O是很耗时的，所以在mysql4.1之后，出现了第二种改进的算法，就是单路排序。 单路排序 从磁盘读取查询需要的所有列，按照order by列在buffer对它们进行排序，然后扫描排序后的列表进行输出， 它的效率更快一些，避免了第二次读取数据。并且把随机IO变成了顺序IO，但是它会使用更多的空间， 因为它把每一行都保存在内存中了。 结论 由于单路是后出的，总体而言好过双路。 但是用单路有问题： 在sort_buffer中，单路比多路要多占用很多空间，因为单路是把所有字段都取出, 所以有可能取出的数据的总大小超出了sort_buffer的容量，导致每次只能取sort_buffer容量大小的数据，进行排序（创建tmp文件，多路合并），排完再取sort_buffer容量大小，再排……从而多次I/O。 单路本来想省一次I/O操作，反而导致了大量的I/O操作，反而得不偿失。 优化策略 增大sort_buffer_size参数的设置 增大max_length_for_sort_data参数的设置 减少select 后面的查询的字段。 提高order by的速度 Order by时select * 是一个大忌。只Query需要的字段， 这点非常重要。 当Query的字段大小总和小于max_length_for_sort_data 而且排序字段不是 TEXT|BLOB 类型时，会用改进后的算法——单路排序， 否则用老算法——多路排序。 两种算法的数据都有可能超出sort_buffer的容量，超出之后，会创建tmp文件进行合并排序，导致多次I/O，但是用单路排序算法的风险会更大一些，所以要提高sort_buffer_size。 尝试提高 sort_buffer_size 不管用哪种算法，提高这个参数都会提高效率，当然，要根据系统的能力去提高，因为这个参数是针对每个进程（connection）的 1M-8M之间调整。 MySQL5.7，InnoDB存储引擎默认值是1048576字节，1MB。 1SHOW VARIABLES LIKE &#x27;%sort_buffer_size%&#x27;; 尝试提高 max_length_for_sort_data 提高这个参数， 会增加用改进算法的概率。 但是如果设的太高，数据总容量超出sort_buffer_size的概率就增大，明显症状是高的磁盘I/O活动和低的处理器使用率。如果需要返回的列的总长度大于max_length_for_sort_data，使用双路算法，否则使用单路算法。1024-8192字节之间调整。 1SHOW VARIABLES LIKE &#x27;%max_length_for_sort_data%&#x27;; #默认1024字节 5.6 分组优化group by 使用索引的原则几乎跟order by一致 ，唯一区别： group by 先排序再分组，遵照索引建的最佳左前缀法则 当无法使用索引列，增大max_length_for_sort_data和sort_buffer_size参数的设置 where高于having,能写在where限定的条件就不要写在having中了 group by没有过滤条件，也可以用上索引。Order By 必须有过滤条件才能使用上索引。 6. 覆盖索引禁止使用select *，禁止查询与业务无关字段，尽量使用覆盖索引，防止回表。 覆盖索引减少了 IO 次数，减少了数据的访问量，可以大大地提升查询效率。 四，追踪优化器前面的原理篇详细分析过，在此不再赘述。 五， 分库分表从维度来说分成两种，一种是垂直，一种是水平。 垂直切分：基于表或字段划分，表结构不同。我们有单库的分表，也有多库的分库。 水平切分：基于数据划分，表结构相同，数据不同，也有同库的水平切分和多库的切分。 1.垂直切分垂直分表有两种，一种是单库的，一种是多库的。 1.1 单库垂直分表单库分表，比如：商户信息表，拆分成基本信息表，联系方式表，结算信息表，附件表等等。 可以考虑根据冷热点字段拆分，是否经常发生修改操作拆分，根据字段功能拆分。 1.2 多库垂直分表多库垂直分表就是把原来存储在一个库的不同的表，拆分到不同的数据库。 比如电商平台的消费系统：一开始，商品表，商品详情表，订单表，用户表，支付记录表，库存表，风控表都在一个库里面，随着数据的增长和业务的扩张，可以考虑将商品和商品详情表单独放到一个库，订单表单独放到一个库，支付记录单独放到一个库，库存表单独放到一个库，风控表单独放到一个库。 当我们对原来的一张表做了分库的处理，如果某些业务系统的数据还是有一个非常快的增长速度，比如说订单数据库的订单表，数据量达到了几个亿，这个时候硬件限制导致的性能问题还是会出现，所以从这个角度来说垂直切分并没有从根本上解决单库单表数据量过大的问题。在这个时候，我们还需要对我们的数据做一个水平的切分。 2.水平拆分当我们的客户表数量已经到达数千万甚至上亿的时候，单表的存储容量和查询效率都会出现问题，我们需要进一步对单张表的数据进行水平切分。水平切分的每个数据库的表结构都是一样的，只是存储的数据不一样，比如每个库存储 1000 万的数据。 水平切分也可以分成两种，一种是单库的，一种是多库的。 2.1 单库水平分表银行的交易流水表，所有进出的交易都需要登记这张表，因为绝大部分时候客户都是查询当天的交易和一个月以内的交易数据，所以我们根据使用频率把这张表拆分成三张表： 当天表：只存储当天的数据。 当月表：在夜间运行一个定时任务，前一天的数据，全部迁移到当月表。用的是 insert into select，然后 delete。 历史表：同样是通过定时任务，把登记时间超过 30 天的数据，迁移到 history历史表（历史表的数据非常大，我们按照月度，每个月建立分区）。 跟分区一样，这种方式虽然可以一定程度解决单表查询性能的问题，但是并不能解决单机存储瓶颈的问题。 2.2 多库水平分表比如客户表，我们拆分到多个库存储，表结构是完全一样的。 一般我们说的分库分表都是跨库的分表。 3. 分库分表带来的问题3.1 跨库关联查询比如查询合同信息的时候要关联客户数据，由于是合同数据和客户数据是在不同的数据库，那么我们肯定不能直接使用 join 的这种方式去做关联查询。 解决方案 ①字段冗余比如我们查询合同库的合同表的时候需要关联客户库的客户表，我们可以直接把一些经常关联查询的客户字段放到合同表，通过这种方式避免跨库关联查询的问题。 ②数据同步比如商户系统要查询产品系统的产品表，我们干脆在商户系统创建一张产品表，通过 ETL 或者其他方式定时同步产品数据。 ③全局表（广播表）比如行名行号信息被很多业务系统用到，如果我们放在核心系统，每个系统都要去关联查询，这个时候我们可以在所有的数据库都存储相同的基础数据。 ④ER表我们有些表的数据是存在逻辑的主外键关系的，比如订单表 order_info，存的是汇总的商品数，商品金额；订单明细表 order_detail，是每个商品的价格，个数等等。或者叫做从属关系，父表和子表的关系。他们之间会经常有关联查询的操作，如果父表的数据和子表的数据分别存储在不同的数据库，跨库关联查询也比较麻烦。所以我们能不能把父表的数据和从属于父表的数据落到一个节点上呢？ 比如 order_id=1001 的数据在 node1，它所有的明细数据也放到 node1；order_id=1002 的数据在 node2，它所有的明细数据都放到 node2，这样在关联查询的时候依然是在一个数据库。 上面的思路都是通过合理的数据分布避免跨库关联查询，实际上在我们的业务中，也是尽量不要用跨库关联查询，如果出现了这种情况，就要分析一下业务或者数据拆分是不是合理。如果还是出现了需要跨库关联的情况，那我们就只能用最后一种办法。 ⑤系统层组装在不同的数据库节点把符合条件数据的数据查询出来，然后重新组装，返回给客户端。 3.2 分布式事务具体分布式事务会单独写一篇文章 3.3 排序，翻页，函数计算问题跨节点多库进行查询时，会出现 limit 分页，order by 排序的问题。比如有两个节点，节点 1 存的是奇数 id=1,3,5,7,9……；节点 2 存的是偶数 id=2,4,6,8,10…… 执行 select * from user_info order by id limit 0,10 需要在两个节点上各取出 10 条，然后合并数据，重新排序。 max、min、sum、count 之类的函数在进行计算的时候，也需要先在每个分片上执行相应的函数，然后将各个分片的结果集进行汇总和再次计算，最终将结果返回。 3.4 全局主键避重MySQL 的数据库里面字段有一个自增的属性，Oracle 也有 Sequence 序列。如果是一个数据库，那么可以保证 ID 是不重复的，但是水平分表以后，每个表都按照自己的规律自增，肯定会出现 ID 重复的问题，这个时候我们就不能用本地自增的方式了。 解决方案 ①UUIDUUID 标准形式包含 32 个 16 进制数字，分为 5 段，形式为 8-4-4-4-12 的 36 个字符，例如：c4e7956c-03e7-472c-8909-d733803e79a9。 UUID 是主键是最简单的方案，本地生成，性能高，没有网络耗时。但缺点也很明显，由于 UUID 非常长，会占用大量的存储空间；另外，作为主键建立索引和基于索引进行查询时都会存在性能问题，在 InnoDB 中，UUID 的无序性会引起数据位置频繁变动，导致分页。 ②数据库把序号维护在数据库的一张表中。这张表记录了全局主键的类型、位数、起始值，当前值。当其他应用需要获得全局 ID 时，先 for update 锁行，取到值+1 后并且更新后返回。并发性比较差。 ③redis基于 Redis 的 INT 自增的特性，使用批量的方式降低数据库的写压力，每次获取一段区间的 ID 号段，用完之后再去数据库获取，可以大大减轻数据库的压力。 ④雪花算法优点：毫秒数在高位，生成的 ID 整体上按时间趋势递增；不依赖第三方系统，稳定性和效率较高，理论上 QPS 约为 409.6w/s(1000*2^12)，并且整个分布式系统内不会产生 ID 碰撞；可根据自身业务灵活分配 bit 位。 不足就在于：强依赖机器时钟，如果时钟回拨，则可能导致生成 ID 重复。 4. 多数据源/读写数据源的解决方案分析一下 SQL 执行经过的流程： DAO——Mapper（ORM）——JDBC——代理——数据库服务 4.1 客户端DAO 层在我们连接到某一个数据源之前，我们先根据配置的分片规则，判断需要连接到哪些节点，再建立连接。 Spring 中提供了一个抽象类 AbstractRoutingDataSource，可以实现数据源的动态切换。 123456789101）aplication.properties 定义多个数据源2）创建@TargetDataSource 注解3）创建 DynamicDataSource 继承 AbstractRoutingDataSource4）多数据源配置类 DynamicDataSourceConfig5）创建切面类 DataSourceAspect，对添加了@TargetDataSource 注解的类进行拦截设置数据源。6）在 启 动 类 上 自 动 装 配 数 据 源 配 置@Import(&#123;DynamicDataSourceConfig.class&#125;)7）在 实 现 类 上 加 上 注 解 ， 如 @TargetDataSource(name =DataSourceNames.SECOND)，调用 在 DAO 层实现的优势：不需要依赖 ORM 框架，即使替换了 ORM 框架也不受影响。实现简单（不需要解析 SQL 和路由规则），可以灵活地定制。 缺点：不能复用，不能跨语言。 4.2 ORM框架层比如我们用 MyBatis 连接数据库，也可以指定数据源。我们可以基于 MyBatis 插件的拦截机制（拦截 query 和 update 方法），实现数据源的选择。 4.3 驱动层不管是MyBatis还是Hibernate，还是Spring的JdbcTemplate，本质上都是对JDBC的封装，所以第三层就是驱动层。比如 Sharding-JDBC，就是对 JDBC 的对象进行了封装。JDBC 的核心对象： DataSource：数据源 Connection：数据库连接 Statement：语句对象 ResultSet：结果集 那我们只要对这几个对象进行封装或者拦截或者代理，就可以实现分片的操作。 4.4 代理层前面三种都是在客户端实现的，也就是说不同的项目都要做同样的改动，不同的编程语言也有不同的实现，所以我们能不能把这种选择数据源和实现路由的逻辑提取出来，做成一个公共的服务给所有的客户端使用呢？ 这个就是第四层，代理层。比如 Mycat 和 Sharding-Proxy，都是属于这一层。 4.5 数据库服务某些特定的数据库或者数据库的特定版本可以实现这个功能。 六，主从复制1. 基本原理 MySQL复制过程分成三步： master将改变记录到二进制日志（binary log）。这些记录过程叫做二进制日志事件，binary log events； slave将master的binary log events拷贝到它的中继日志（relay log）； slave重做中继日志中的事件，将改变应用到自己的数据库中。 MySQL复制是异步的且串行化的，slave会从master读取binlog来进行数据同步。 2.与Redis主从复制的差别 redis主从复制是将主机的所有数据都拷贝给从机，并且是近乎实时的。 mysql主从复制不会将建立连接以前的数据发送给从机，并且是异步，且串行化的。 3.复制的基本原则每个slave只有一个master 每个slave只能有一个唯一的服务器ID 每个master可以有多个salve 4.复制的最大问题延时 全同步可以避免，但性能会极差，正常情况下半同步，且容忍一部分数据不一致。如果不容忍数据不一致，只有强制读主。 5.一主一从常见配置 MySQL版本一致且后台以服务运行 主从配置都在【mysqld】节点下，且全部小写 主机修改my.ini文件 主服务器唯一ID server-id=1 启用二进制日志 设置不要复制的数据库 设置需要复制的数据库 设置logbin格式 log-bin=自己的本地路径/data/mysqlbin binlog-ignore-db=mysql binlog-do-db=需要复制的主数据库名字 binlog_fromat=STATEMENT(默认) 七，硬件层面的配置1.选择合适的CPU数据库分为两大类，在线事务处理和在线分析处理。 InnoDB储存引擎一般应用于OLTP的数据库应用，这种应用的特点如下： 用户操作的并发量大 事务处理时间一般比较短 查询的语句较为简单，一般都走索引 复杂查询比较少 在当前的MySQL数据库版本中，一条SQL语句只能在一个CPU工作，并不支持多CPU。若cpu支持多核，innodb版本应该选择1.1或者更高。另外如果是多核cpu，可以通过修改参数innodb_read_io_threads和innodb_write_io_threads来增大IO的线程，这样也可以更充分的利用cpu的多核性能。 2.内存的重要性内存大小直接反映数据库的性能。Innodb存储引擎既缓存数据，又缓存索引，并且将它们缓存于一个很大的缓冲池中，即InnoDB Buffer Pool。因此，内存的大小直接影像数据库的性能。 3.磁盘对数据库性能的影响4.合理设置RAID类型5.操作系统的选择6.文件系统的选择","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/tags/MySQL/"}]},{"title":"MySQL[十]Explain&optimizer trace","slug":"MySQL/MySQL[十]Explain&optimizer trace","date":"2022-01-11T03:16:42.428Z","updated":"2022-01-11T03:24:05.355Z","comments":true,"path":"2022/01/11/MySQL/MySQL[十]Explain&optimizer trace/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/MySQL/MySQL[%E5%8D%81]Explain&optimizer%20trace/","excerpt":"","text":"一，Explain一条查询语句在经过MySQL查询优化器的各种基于成本和规则的优化会后生成一个所谓的执行计划，这个执行计划展示了接下来具体执行查询的方式，比如多表连接的顺序是什么，对于每个表采用什么访问方法来具体执行查询等等。MySQL为我们提供了EXPLAIN语句来帮助我们查看某个查询语句的具体执行计划，如果我们想看看某个查询的执行计划的话，可以在具体的查询语句前边加一个EXPLAIN，就像这样： 其实除了以SELECT开头的查询语句，其余的DELETE、INSERT、REPLACE以及UPDATE语句前边都可以加上EXPLAIN这个词儿，用来查看这些语句的执行计划，不过我们这里对SELECT语句更感兴趣，所以后边只会以SELECT语句为例来描述EXPLAIN语句的用法。我们先把EXPLAIN语句输出的各个列的作用先大致罗列一下： 列名 描述 id 在一个大的查询语句中每个SELECT关键字都对应一个唯一的id select_type SELECT关键字对应的那个查询的类型 table 表名 partitions 匹配的分区信息 type 针对单表的访问方法 possible_keys 可能用到的索引 key 实际上使用的索引 key_len 实际使用到的索引长度 ref 当使用索引列等值查询时，与索引列进行等值匹配的对象信息 rows 预估的需要读取的记录条数 filtered 某个表经过搜索条件过滤后剩余记录条数的百分比 Extra 一些额外的信息 我们前面创建过一张single_table表： 123456789101112131415CREATE TABLE single_table ( id INT NOT NULL AUTO_INCREMENT, key1 VARCHAR(100), key2 INT, key3 VARCHAR(100), key_part1 VARCHAR(100), key_part2 VARCHAR(100), key_part3 VARCHAR(100), common_field VARCHAR(100), PRIMARY KEY (id), KEY idx_key1 (key1), UNIQUE KEY idx_key2 (key2), KEY idx_key3 (key3), KEY idx_key_part(key_part1, key_part2, key_part3)) Engine=InnoDB CHARSET=utf8; 1.执行计划输出中各列详解table不论我们的查询语句有多复杂，里边儿包含了多少个表，到最后也是需要对每个表进行单表访问的，所以MySQL规定EXPLAIN语句输出的每条记录都对应着某个单表的访问方法，该条记录的table列代表着该表的表名。所以我们看一条比较简单的查询语句： 这个查询语句只涉及对s1表的单表查询，所以EXPLAIN输出中只有一条记录，其中的table列的值是s1，表明这条记录是用来说明对s1表的单表访问方法的。 下边我们看一下一个连接查询的执行计划： 可以看到这个连接查询的执行计划中有两条记录，这两条记录的table列分别是s1和s2，这两条记录用来分别说明对s1表和s2表的访问方法是什么。 id我们知道我们写的查询语句一般都以SELECT关键字开头，比较简单的查询语句里只有一个SELECT关键字，比如下边这个查询语句： 1SELECT * FROM s1 WHERE key1 = &#x27;a&#x27;; 稍微复杂一点的连接查询中也只有一个SELECT关键字，比如： 123SELECT * FROM s1 INNER JOIN s2 ON s1.key1 = s2.key1 WHERE s1.common_field = &#x27;a&#x27;; 但是下边两种情况下在一条查询语句中会出现多个SELECT关键字： 查询中包含子查询的情况比如下边这个查询语句中就包含2个SELECT关键字： 12SELECT * FROM s1 WHERE key1 IN (SELECT key3 FROM s2); 查询中包含UNION语句的情况比如下边这个查询语句中也包含2个SELECT关键字： 1SELECT * FROM s1 UNION SELECT * FROM s2; 查询语句中每出现一个SELECT关键字，MySQL就会为它分配一个唯一的id值。这个id值就是EXPLAIN语句的第一个列，比如下边这个查询中只有一个SELECT关键字，所以EXPLAIN的结果中也就只有一条id列为1的记录： 对于连接查询来说，一个SELECT关键字后边的FROM子句中可以跟随多个表，所以在连接查询的执行计划中，每个表都会对应一条记录，但是这些记录的id值都是相同的，比如： 可以看到，上述连接查询中参与连接的s1和s2表分别对应一条记录，但是这两条记录对应的id值都是1。在连接查询的执行计划中，每个表都会对应一条记录，这些记录的id列的值是相同的，出现在前边的表表示驱动表，出现在后边的表表示被驱动表。所以从上边的EXPLAIN输出中我们可以看出，查询优化器准备让s1表作为驱动表，让s2表作为被驱动表来执行查询。 对于包含子查询的查询语句来说，就可能涉及多个SELECT关键字，所以在包含子查询的查询语句的执行计划中，每个SELECT关键字都会对应一个唯一的id值，比如这样： 从输出结果中我们可以看到，s1表在外层查询中，外层查询有一个独立的SELECT关键字，所以第一条记录的id值就是1，s2表在子查询中，子查询有一个独立的SELECT关键字，所以第二条记录的id值就是2。 查询优化器可能对涉及子查询的查询语句进行重写，从而转换为连接查询。所以如果我们想知道查询优化器对某个包含子查询的语句是否进行了重写，直接查看执行计划就好了，比如说： 虽然我们的查询语句是一个子查询，但是执行计划中s1和s2表对应的记录的id值全部是1，这就表明了查询优化器将子查询转换为了连接查询。 对于包含UNION子句的查询语句来说，每个SELECT关键字对应一个id值也是没错的，不过还是有点儿特别，比方说下边这个查询： 这个语句的执行计划的第三条记录是个什么？为什么id值是NULL，而且table名也不大对？UNION子句会把多个查询的结果集合并起来并对结果集中的记录进行去重，怎么去重呢？MySQL使用的是内部的临时表。正如上边的查询计划中所示，UNION子句是为了把id为1的查询和id为2的查询的结果集合并起来并去重，所以在内部创建了一个临时表（就是执行计划第三条记录的table列的名称），id为NULL表明这个临时表是为了合并两个查询的结果集而创建的。 跟UNION对比起来，UNION ALL就不需要为最终的结果集进行去重，它只是单纯的把多个查询的结果集中的记录合并成一个并返回给用户，所以也就不需要使用临时表。所以在包含UNION ALL子句的查询的执行计划中，就没有那个id为NULL的记录，如下所示： select_type通过上边的内容我们知道，一条大的查询语句里边可以包含若干个SELECT关键字，每个SELECT关键字代表着一个小的查询语句，而每个SELECT关键字的FROM子句中都可以包含若干张表（这些表用来做连接查询），每一张表都对应着执行计划输出中的一条记录，对于在同一个SELECT关键字中的表来说，它们的id值是相同的。 MySQL为每一个SELECT关键字代表的小查询都定义了一个称之为select_type的属性，意思是我们只要知道了某个小查询的select_type属性，就知道了这个小查询在整个大查询中有什么作用。 名称 描述 SIMPLE Simple SELECT (not using UNION or subqueries) PRIMARY Outermost SELECT UNION Second or later SELECT statement in a UNION UNION RESULT Result of a UNION SUBQUERY First SELECT in subquery DEPENDENT SUBQUERY First SELECT in subquery, dependent on outer query DEPENDENT UNION Second or later SELECT statement in a UNION, dependent on outer query DERIVED Derived table MATERIALIZED Materialized subquery UNCACHEABLE SUBQUERY A subquery for which the result cannot be cached and must be re-evaluated for each row of the outer query UNCACHEABLE UNION The second or later select in a UNION that belongs to an uncacheable subquery (see UNCACHEABLE SUBQUERY) SIMPLE查询语句中不包含UNION或者子查询的查询都算作是SIMPLE类型，比方说下边这个单表查询的select_type的值就是SIMPLE：当然，连接查询也算是SIMPLE类型，比如： PRIMARY对于包含UNION、UNION ALL或者子查询的大查询来说，它是由几个小查询组成的，其中最左边的那个查询的select_type值就是PRIMARY，比方说：从结果中可以看到，最左边的小查询SELECT * FROM s1对应的是执行计划中的第一条记录，它的select_type值就是PRIMARY。 UNION对于包含UNION或者UNION ALL的大查询来说，它是由几个小查询组成的，其中除了最左边的那个小查询以外，其余的小查询的select_type值就是UNION，可以对比上一个例子的效果，这就不多举例子了。 UNION RESULTMySQL选择使用临时表来完成UNION查询的去重工作，针对该临时表的查询的select_type就是UNION RESULT，例子上边有，就不赘述了。 SUBQUERY如果包含子查询的查询语句不能够转为对应的semi-join的形式，并且该子查询是不相关子查询，并且查询优化器决定采用将该子查询物化的方案来执行该子查询时，该子查询的第一个SELECT关键字代表的那个查询的select_type就是SUBQUERY，比如下边这个查询：可以看到，外层查询的select_type就是PRIMARY，子查询的select_type就是SUBQUERY。由于select_type为SUBQUERY的子查询会被物化，所以只需要执行一遍。 DEPENDENT SUBQUERY如果包含子查询的查询语句不能够转为对应的semi-join的形式，并且该子查询是相关子查询，则该子查询的第一个SELECT关键字代表的那个查询的select_type就是DEPENDENT SUBQUERY，比如下边这个查询： select_type为DEPENDENT SUBQUERY的查询可能会被执行多次。 DEPENDENT UNION在包含UNION或者UNION ALL的大查询中，如果各个小查询都依赖于外层查询的话，那除了最左边的那个小查询之外，其余的小查询的select_type的值就是DEPENDENT UNION。说的有些绕哈，比方说下边这个查询：这个查询比较复杂，大查询里包含了一个子查询，子查询里又是由UNION连起来的两个小查询。从执行计划中可以看出来，SELECT key1 FROM s2 WHERE key1 = &#39;a&#39;这个小查询由于是子查询中第一个查询，所以它的select_type是DEPENDENT SUBQUERY，而SELECT key1 FROM s1 WHERE key1 = &#39;b&#39;这个查询的select_type就是DEPENDENT UNION。 DERIVED对于采用物化的方式执行的包含派生表的查询，该派生表对应的子查询的select_type就是DERIVED，比方说下边这个查询：从执行计划中可以看出，id为2的记录就代表子查询的执行方式，它的select_type是DERIVED，说明该子查询是以物化的方式执行的。id为1的记录代表外层查询，它的table列显示的是``，表示该查询是针对将派生表物化之后的表进行查询的。 如果派生表可以通过和外层查询合并的方式执行的话，执行计划又是另一番景象。 MATERIALIZED当查询优化器在执行包含子查询的语句时，选择将子查询物化之后与外层查询进行连接查询时，该子查询对应的select_type属性就是MATERIALIZED，比如下边这个查询：执行计划的第三条记录的id值为2，说明该条记录对应的是一个单表查询，从它的select_type值为MATERIALIZED可以看出，查询优化器是要把子查询先转换成物化表。然后看执行计划的前两条记录的id值都为1，说明这两条记录对应的表进行连接查询，需要注意的是第二条记录的table列的值是``，说明该表其实就是id为2对应的子查询执行之后产生的物化表，然后将s1和该物化表进行连接查询。 UNCACHEABLE SUBQUERY不常用 UNCACHEABLE UNION不常用 partitions一般情况下我们的查询语句的执行计划的partitions列的值都是NULL。 type执行计划的一条记录就代表着MySQL对某个表的执行查询时的访问方法，其中的type列就表明了这个访问方法是什么，比方说下边这个查询： 可以看到type列的值是ref，表明MySQL即将使用ref访问方法来执行对s1表的查询。但是我们之前只分析过对使用InnoDB存储引擎的表进行单表访问的一些访问方法，完整的访问方法如下：system，const，eq_ref，ref，fulltext，ref_or_null，index_merge，unique_subquery，index_subquery，range，index，ALL。接下来我们详细看一下： system当表中只有一条记录并且该表使用的存储引擎的统计数据是精确的，比如MyISAM、Memory，那么对该表的访问方法就是system。比方说我们新建一个MyISAM表，并为其插入一条记录：然后我们看一下查询这个表的执行计划：可以看到type列的值就是system了。 12345mysql&gt; CREATE TABLE t(i int) Engine=MyISAM;Query OK, 0 rows affected (0.05 sec)mysql&gt; INSERT INTO t VALUES(1);Query OK, 1 row affected (0.01 sec) 把表改成使用InnoDB存储引擎，执行计划的type列是ALL。 const当我们根据主键或者唯一二级索引列与常数进行等值匹配时，对单表的访问方法就是const，比如： eq_ref在连接查询时，如果被驱动表是通过主键或者唯一二级索引列等值匹配的方式进行访问的（如果该主键或者唯一二级索引是联合索引的话，所有的索引列都必须进行等值比较），则对该被驱动表的访问方法就是eq_ref，比方说：从执行计划的结果中可以看出，MySQL打算将s1作为驱动表，s2作为被驱动表，重点关注s2的访问方法是eq_ref，表明在访问s2表的时候可以通过主键的等值匹配来进行访问。 ref当通过普通的二级索引列与常量进行等值匹配时来查询某个表，那么对该表的访问方法就可能是ref。 fulltext全文索引,意义不大。 ref_or_null当对普通二级索引进行等值匹配查询，该索引列的值也可以是NULL值时，那么对该表的访问方法就可能是ref_or_null，比如说： index_merge在某些场景下可以使用Intersection、Union、Sort-Union这三种索引合并的方式来执行查询，我们看一下执行计划中是怎么体现MySQL使用索引合并的方式来对某个表执行查询的：从执行计划的type列的值是index_merge就可以看出，MySQL打算使用索引合并的方式来执行对s1表的查询。 unique_subquery类似于两表连接中被驱动表的eq_ref访问方法，unique_subquery是针对在一些包含IN子查询的查询语句中，如果查询优化器决定将IN子查询转换为EXISTS子查询，而且子查询可以使用到主键进行等值匹配的话，那么该子查询执行计划的type列的值就是unique_subquery，比如下边的这个查询语句：可以看到执行计划的第二条记录的type值就是unique_subquery，说明在执行子查询时会使用到id列的索引。 index_subqueryindex_subquery与unique_subquery类似，只不过访问子查询中的表时使用的是普通的索引，比如这样： range如果使用索引获取某些范围区间的记录，那么就可能使用到range访问方法，比如下边的这个查询：或者： index当我们可以使用索引覆盖，但需要扫描全部的索引记录时，该表的访问方法就是index，比如这样：上述查询中的搜索列表中只有key_part2一个列，而且搜索条件中也只有key_part3一个列，这两个列又恰好包含在idx_key_part这个索引中，可是搜索条件key_part3不能直接使用该索引进行ref或者range方式的访问，只能扫描整个idx_key_part索引的记录，所以查询计划的type列的值就是index。 对于使用InnoDB存储引擎的表来说，二级索引的记录只包含索引列和主键列的值，而聚簇索引中包含用户定义的全部列以及一些隐藏列，所以扫描二级索引的代价比直接全表扫描，也就是扫描聚簇索引的代价更低一些。 ALL最熟悉的全表扫描直接看例子： 一般来说，这些访问方法按照我们介绍它们的顺序性能依次变差。其中除了All这个访问方法外，其余的访问方法都能用到索引，除了index_merge访问方法外，其余的访问方法都最多只能用到一个索引。 possible_keys和key在EXPLAIN语句输出的执行计划中，possible_keys列表示在某个查询语句中，对某个表执行单表查询时可能用到的索引有哪些，key列表示实际用到的索引有哪些，比方说下边这个查询： 上述执行计划的possible_keys列的值是idx_key1,idx_key3，表示该查询可能使用到idx_key1,idx_key3两个索引，然后key列的值是idx_key3，表示经过查询优化器计算使用不同索引的成本后，最后决定使用idx_key3来执行查询比较划算。 不过有一点比较特别，就是在使用index访问方法来查询某个表时，possible_keys列是空的，而key列展示的是实际使用到的索引，比如这样： 另外需要注意的一点是，possible_keys列中的值并不是越多越好，可能使用的索引越多，查询优化器计算查询成本时就得花费更长时间，所以如果可以的话，尽量删除那些用不到的索引。 key_lenkey_len列表示当优化器决定使用某个索引执行查询时，该索引记录的最大长度，它是由这三个部分构成的： 对于使用固定长度类型的索引列来说，它实际占用的存储空间的最大长度就是该固定值，对于指定字符集的变长类型的索引列来说，比如某个索引列的类型是VARCHAR(100)，使用的字符集是utf8，那么该列实际占用的最大存储空间就是100 × 3 = 300个字节。 如果该索引列可以存储NULL值，则key_len比不可以存储NULL值时多1个字节。 对于变长字段来说，都会有2个字节的空间来存储该变长列的实际长度。 比如下边这个查询： 由于id列的类型是INT，并且不可以存储NULL值，所以在使用该列的索引时key_len大小就是4。当索引列可以存储NULL值时，比如： 可以看到key_len列就变成了5，比使用id列的索引时多了1。 对于可变长度的索引列来说，比如下边这个查询： 由于key1列的类型是VARCHAR(100)，所以该列实际最多占用的存储空间就是300字节，又因为该列允许存储NULL值，所以key_len需要加1，又因为该列是可变长度列，所以key_len需要加2，所以最后ken_len的值就是303。 这里需要强调的一点是，执行计划的生成是在MySQL server层中的功能，并不是针对具体某个存储引擎的功能，MySQL在执行计划中输出key_len列主要是为了让我们区分某个使用联合索引的查询具体用了几个索引列，而不是为了准确的说明针对某个具体存储引擎存储变长字段的实际长度占用的空间到底是占用1个字节还是2个字节。比方说下边这个使用到联合索引idx_key_part的查询： 我们可以从执行计划的key_len列中看到值是303，这意味着MySQL在执行上述查询中只能用到idx_key_part索引的一个索引列，而下边这个查询： 这个查询的执行计划的ken_len列的值是606，说明执行这个查询的时候可以用到联合索引idx_key_part的两个索引列。 ref当使用索引列等值匹配的条件去执行查询时，也就是在访问方法是const、eq_ref、ref、ref_or_null、unique_subquery、index_subquery其中之一时，ref列展示的就是与索引列作等值匹配的东东是个啥，比如只是一个常数或者是某个列。 可以看到ref列的值是const，表明在使用idx_key1索引执行查询时，与key1列作等值匹配的对象是一个常数，当然有时候更复杂一点： 可以看到对被驱动表s2的访问方法是eq_ref，而对应的ref列的值是yhd.s1.id，这说明在对被驱动表进行访问时会用到PRIMARY索引，也就是聚簇索引与一个列进行等值匹配的条件，于s2表的id作等值匹配的对象就是yhd.s1.id列（注意这里把数据库名也写出来了）。 有的时候与索引列进行等值匹配的对象是一个函数，比方说下边这个查询： 我们看执行计划的第二条记录，可以看到对s2表采用ref访问方法执行查询，然后在查询计划的ref列里输出的是func，说明与s2表的key1列进行等值匹配的对象是一个函数。 rows如果查询优化器决定使用全表扫描的方式对某个表执行查询时，执行计划的rows列就代表预计需要扫描的行数，如果使用索引来执行查询时，执行计划的rows列就代表预计扫描的索引记录行数。比如下边这个查询： 我们看到执行计划的rows列的值是1，这意味着查询优化器在经过分析使用idx_key1进行查询的成本之后，觉得满足key1 &gt; &#39;z&#39;这个条件的记录只有1条。 filtered之前在分析连接查询的成本时提出过一个condition filtering的概念，就是MySQL在计算驱动表扇出时采用的一个策略： 如果使用的是全表扫描的方式执行的单表查询，那么计算驱动表扇出时需要估计出满足搜索条件的记录到底有多少条。 如果使用的是索引执行的单表扫描，那么计算驱动表扇出的时候需要估计出满足除使用到对应索引的搜索条件外的其他搜索条件的记录有多少条。 比方说下边这个查询： 从执行计划的key列中可以看出来，该查询使用idx_key1索引来执行查询，从rows列可以看出满足key1 &gt; &#39;z&#39;的记录有1条。执行计划的filtered列就代表查询优化器预测在这1条记录中，有多少条记录满足其余的搜索条件，也就是common_field = &#39;a&#39;这个条件的百分比。此处filtered列的值是10.00，说明查询优化器预测在1条记录中有10.00%的记录满足common_field = &#39;a&#39;这个条件。 对于单表查询来说，这个filtered列的值没什么意义，我们更关注在连接查询中驱动表对应的执行计划记录的filtered值，比方说下边这个查询： 从执行计划中可以看出来，查询优化器打算把s1当作驱动表，s2当作被驱动表。我们可以看到驱动表s1表的执行计划的rows列为997219， filtered列为10.00，这意味着驱动表s1的扇出值就是997219 × 10.00% = 99721.9，这说明还要对被驱动表执行大约99721.9次查询。 Extra顾名思义，Extra列是用来说明一些额外信息的，我们可以通过这些额外信息来更准确的理解MySQL到底将如何执行给定的查询语句。 No tables used当查询语句的没有FROM子句时将会提示该额外信息，比如： Impossible WHERE查询语句的WHERE子句永远为FALSE时将会提示该额外信息，比方说： No matching min/max row当查询列表处有MIN或者MAX聚集函数，但是并没有符合WHERE子句中的搜索条件的记录时，将会提示该额外信息，比方说： Using index当我们的查询列表以及搜索条件中只包含属于某个索引的列，也就是在可以使用索引覆盖的情况下，在Extra列将会提示该额外信息。比方说下边这个查询中只需要用到idx_key1而不需要回表操作： Using index condition有些搜索条件中虽然出现了索引列，但却不能使用到索引，比如下边这个查询：其中的key1 &gt; &#39;z&#39;可以使用到索引，但是key1 LIKE &#39;%a&#39;却无法使用到索引，在以前版本的MySQL中，是按照下边步骤来执行这个查询的： 1SELECT * FROM s1 WHERE key1 &gt; &#x27;z&#x27; AND key1 LIKE &#x27;%a&#x27;; 先根据key1 &gt; &#39;z&#39;这个条件，从二级索引idx_key1中获取到对应的二级索引记录。 根据上一步骤得到的二级索引记录中的主键值进行回表，找到完整的用户记录再检测该记录是否符合key1 LIKE &#39;%a&#39;这个条件，将符合条件的记录加入到最后的结果集。 但是虽然key1 LIKE &#39;%a&#39;不能组成范围区间参与range访问方法的执行，但这个条件毕竟只涉及到了key1列，所以MySQL把上边的步骤改进了一下： 先根据key1 &gt; &#39;z&#39;这个条件，定位到二级索引idx_key1中对应的二级索引记录。 对于指定的二级索引记录，先不着急回表，而是先检测一下该记录是否满足key1 LIKE &#39;%a&#39;这个条件，如果这个条件不满足，则该二级索引记录压根儿就没必要回表。 对于满足key1 LIKE &#39;%a&#39;这个条件的二级索引记录执行回表操作。 我们说回表操作其实是一个随机IO，比较耗时，所以上述修改虽然只改进了一点点，但是可以省去好多回表操作的成本。MySQL把他们的这个改进称之为索引条件下推（英文名：Index Condition Pushdown）。如果在查询语句的执行过程中将要使用索引条件下推这个特性，在Extra列中将会显示Using index condition，比如这样： Using where当我们使用全表扫描来执行对某个表的查询，并且该语句的WHERE子句中有针对该表的搜索条件时，在Extra列中会提示上述额外信息。比如下边这个查询：当使用索引访问来执行对某个表的查询，并且该语句的WHERE子句中有除了该索引包含的列之外的其他搜索条件时，在Extra列中也会提示上述额外信息。比如下边这个查询虽然使用idx_key1索引执行查询，但是搜索条件中除了包含key1的搜索条件key1 = &#39;a&#39;，还有包含common_field的搜索条件，所以Extra列会显示Using where的提示： Using join buffer (Block Nested Loop)在连接查询执行过程中，当被驱动表不能有效的利用索引加快访问速度，MySQL一般会为其分配一块名叫join buffer的内存块来加快查询速度，也就是我们所讲的基于块的嵌套循环算法，比如下边这个查询语句：可以在对s2表的执行计划的Extra列显示了两个提示： Using join buffer (Block Nested Loop)：这是因为对表s2的访问不能有效利用索引，只好退而求其次，使用join buffer来减少对s2表的访问次数，从而提高性能。 Using where：可以看到查询语句中有一个s1.common_field = s2.common_field条件，因为s1是驱动表，s2是被驱动表，所以在访问s2表时，s1.common_field的值已经确定下来了，所以实际上查询s2表的条件就是s2.common_field = 一个常数，所以提示了Using where额外信息。 Not exists当我们使用左（外）连接时，如果WHERE子句中包含要求被驱动表的某个列等于NULL值的搜索条件，而且那个列又是不允许存储NULL值的，那么在该表的执行计划的Extra列就会提示Not exists额外信息，比如这样：上述查询中s1表是驱动表，s2表是被驱动表，s2.id列是不允许存储NULL值的，而WHERE子句中又包含s2.id IS NULL的搜索条件，这意味着必定是驱动表的记录在被驱动表中找不到匹配ON子句条件的记录才会把该驱动表的记录加入到最终的结果集，所以对于某条驱动表中的记录来说，如果能在被驱动表中找到1条符合ON子句条件的记录，那么该驱动表的记录就不会被加入到最终的结果集，也就是说我们没有必要到被驱动表中找到全部符合ON子句条件的记录，这样可以稍微节省一点性能。 右（外）连接可以被转换为左（外）连接，所以就不提右（外）连接的情况了。 Using intersect(...)、Using union(...)和Using sort_union(...)如果执行计划的Extra列出现了Using intersect(...)提示，说明准备使用Intersect索引合并的方式执行查询，括号中的...表示需要进行索引合并的索引名称；如果出现了Using union(...)提示，说明准备使用Union索引合并的方式执行查询；出现了Using sort_union(...)提示，说明准备使用Sort-Union索引合并的方式执行查询。比如这个查询的执行计划：其中Extra列就显示了Using intersect(idx_key3,idx_key1)，表明MySQL即将使用idx_key3和idx_key1这两个索引进行Intersect索引合并的方式执行查询。 Zero limit当我们的LIMIT子句的参数为0时，表示不打算从表中读出任何记录，将会提示该额外信息，比如这样： Using filesort有一些情况下对结果集中的记录进行排序是可以使用到索引的，比如下边这个查询：这个查询语句可以利用idx_key1索引直接取出key1列的10条记录，然后再进行回表操作就好了。但是很多情况下排序操作无法使用到索引，只能在内存中（记录较少的时候）或者磁盘中（记录较多的时候）进行排序，MySQL把这种在内存中或者磁盘上进行排序的方式统称为文件排序（英文名：filesort）。如果某个查询需要使用文件排序的方式执行查询，就会在执行计划的Extra列中显示Using filesort提示，比如这样：需要注意的是，如果查询中需要使用filesort的方式进行排序的记录非常多，那么这个过程是很耗费性能的，我们最好想办法将使用文件排序的执行方式改为使用索引进行排序。 Using temporary在许多查询的执行过程中，MySQL可能会借助临时表来完成一些功能，比如去重、排序之类的，比如我们在执行许多包含DISTINCT、GROUP BY、UNION等子句的查询过程中，如果不能有效利用索引来完成查询，MySQL很有可能寻求通过建立内部的临时表来执行查询。如果查询中使用到了内部的临时表，在执行计划的Extra列将会显示Using temporary提示，比方说这样：再比如：不知道大家注意到没有，上述执行计划的Extra列不仅仅包含Using temporary提示，还包含Using filesort提示，可是我们的查询语句中明明没有写ORDER BY子句呀？这是因为MySQL会在包含GROUP BY子句的查询中默认添加上ORDER BY子句，也就是说上述查询其实和下边这个查询等价： 1EXPLAIN SELECT common_field, COUNT(*) AS amount FROM s1 GROUP BY common_field ORDER BY common_field; 如果我们并不想为包含GROUP BY子句的查询进行排序，需要我们显式的写上ORDER BY NULL，就像这样：​ ​ 这回执行计划中就没有Using filesort的提示了，也就意味着执行查询时可以省去对记录进行文件排序的成本了。​ 另外，执行计划中出现Using temporary并不是一个好的征兆，因为建立与维护临时表要付出很大成本的，所以我们最好能使用索引来替代掉使用临时表，比方说下边这个包含GROUP BY子句的查询就不需要使用临时表：​ ​ 从Extra的Using index的提示里我们可以看出，上述查询只需要扫描idx_key1索引就可以搞定了，不再需要临时表了。 Start temporary, End temporary查询优化器会优先尝试将IN子查询转换成semi-join，而semi-join又有好多种执行策略，当执行策略为DuplicateWeedout时，也就是通过建立临时表来实现为外层查询中的记录进行去重操作时，驱动表查询执行计划的Extra列将显示Start temporary提示，被驱动表查询执行计划的Extra列将显示End temporary提示，就是这样： LooseScan在将In子查询转为semi-join时，如果采用的是LooseScan执行策略，则在驱动表执行计划的Extra列就是显示LooseScan提示，比如这样： FirstMatch(tbl_name)在将In子查询转为semi-join时，如果采用的是FirstMatch执行策略，则在被驱动表执行计划的Extra列就是显示FirstMatch(tbl_name)提示，比如这样： 2.Json格式的执行计划我们上边介绍的EXPLAIN语句输出中缺少了一个衡量执行计划好坏的重要属性 —— 成本。不过MySQL贴心的为我们提供了一种查看某个执行计划花费的成本的方式： 在EXPLAIN单词和真正的查询语句中间加上FORMAT=JSON。 这样我们就可以得到一个json格式的执行计划，里边儿包含该计划花费的成本，比如这样： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586mysql&gt; EXPLAIN FORMAT=JSON SELECT * FROM s1 INNER JOIN s2 ON s1.key1 = s2.key2 WHERE s1.common_field = &#x27;a&#x27;\\G*************************** 1. row ***************************EXPLAIN: &#123; &quot;query_block&quot;: &#123; &quot;select_id&quot;: 1, # 整个查询语句只有1个SELECT关键字，该关键字对应的id号为1 &quot;cost_info&quot;: &#123; &quot;query_cost&quot;: &quot;3197.16&quot; # 整个查询的执行成本预计为3197.16 &#125;, &quot;nested_loop&quot;: [ # 几个表之间采用嵌套循环连接算法执行 # 以下是参与嵌套循环连接算法的各个表的信息 &#123; &quot;table&quot;: &#123; &quot;table_name&quot;: &quot;s1&quot;, # s1表是驱动表 &quot;access_type&quot;: &quot;ALL&quot;, # 访问方法为ALL，意味着使用全表扫描访问 &quot;possible_keys&quot;: [ # 可能使用的索引 &quot;idx_key1&quot; ], &quot;rows_examined_per_scan&quot;: 9688, # 查询一次s1表大致需要扫描9688条记录 &quot;rows_produced_per_join&quot;: 968, # 驱动表s1的扇出是968 &quot;filtered&quot;: &quot;10.00&quot;, # condition filtering代表的百分比 &quot;cost_info&quot;: &#123; &quot;read_cost&quot;: &quot;1840.84&quot;, # 稍后解释 &quot;eval_cost&quot;: &quot;193.76&quot;, # 稍后解释 &quot;prefix_cost&quot;: &quot;2034.60&quot;, # 单次查询s1表总共的成本 &quot;data_read_per_join&quot;: &quot;1M&quot; # 读取的数据量 &#125;, &quot;used_columns&quot;: [ # 执行查询中涉及到的列 &quot;id&quot;, &quot;key1&quot;, &quot;key2&quot;, &quot;key3&quot;, &quot;key_part1&quot;, &quot;key_part2&quot;, &quot;key_part3&quot;, &quot;common_field&quot; ], # 对s1表访问时针对单表查询的条件 &quot;attached_condition&quot;: &quot;((`xiaohaizi`.`s1`.`common_field` = &#x27;a&#x27;) and (`xiaohaizi`.`s1`.`key1` is not null))&quot; &#125; &#125;, &#123; &quot;table&quot;: &#123; &quot;table_name&quot;: &quot;s2&quot;, # s2表是被驱动表 &quot;access_type&quot;: &quot;ref&quot;, # 访问方法为ref，意味着使用索引等值匹配的方式访问 &quot;possible_keys&quot;: [ # 可能使用的索引 &quot;idx_key2&quot; ], &quot;key&quot;: &quot;idx_key2&quot;, # 实际使用的索引 &quot;used_key_parts&quot;: [ # 使用到的索引列 &quot;key2&quot; ], &quot;key_length&quot;: &quot;5&quot;, # key_len &quot;ref&quot;: [ # 与key2列进行等值匹配的对象 &quot;xiaohaizi.s1.key1&quot; ], &quot;rows_examined_per_scan&quot;: 1, # 查询一次s2表大致需要扫描1条记录 &quot;rows_produced_per_join&quot;: 968, # 被驱动表s2的扇出是968（由于后边没有多余的表进行连接，所以这个值也没啥用） &quot;filtered&quot;: &quot;100.00&quot;, # condition filtering代表的百分比 # s2表使用索引进行查询的搜索条件 &quot;index_condition&quot;: &quot;(`xiaohaizi`.`s1`.`key1` = `xiaohaizi`.`s2`.`key2`)&quot;, &quot;cost_info&quot;: &#123; &quot;read_cost&quot;: &quot;968.80&quot;, # 稍后解释 &quot;eval_cost&quot;: &quot;193.76&quot;, # 稍后解释 &quot;prefix_cost&quot;: &quot;3197.16&quot;, # 单次查询s1、多次查询s2表总共的成本 &quot;data_read_per_join&quot;: &quot;1M&quot; # 读取的数据量 &#125;, &quot;used_columns&quot;: [ # 执行查询中涉及到的列 &quot;id&quot;, &quot;key1&quot;, &quot;key2&quot;, &quot;key3&quot;, &quot;key_part1&quot;, &quot;key_part2&quot;, &quot;key_part3&quot;, &quot;common_field&quot; ] &#125; &#125; ] &#125;&#125;1 row in set, 2 warnings (0.00 sec) &quot;cost_info&quot;里边的成本是怎么计算出来的？先看s1表的&quot;cost_info&quot;部分： 123456&quot;cost_info&quot;: &#123; &quot;read_cost&quot;: &quot;1840.84&quot;, &quot;eval_cost&quot;: &quot;193.76&quot;, &quot;prefix_cost&quot;: &quot;2034.60&quot;, &quot;data_read_per_join&quot;: &quot;1M&quot;&#125; read_cost是由下边这两部分组成的： IO成本 检测rows × (1 - filter)条记录的CPU成本 rows和filter都是我们前边介绍执行计划的输出列，在JSON格式的执行计划中，rows相当于rows_examined_per_scan，filtered名称不变。 eval_cost是这样计算的：检测 rows × filter条记录的成本。 prefix_cost就是单独查询s1表的成本，也就是：read_cost + eval_cost data_read_per_join表示在此次查询中需要读取的数据量，我们就不多唠叨这个了。 其实没必要关注MySQL为啥使用这么古怪的方式计算出read_cost和eval_cost，关注prefix_cost是查询s1表的成本就好了。 对于s2表的&quot;cost_info&quot;部分是这样的： 123456&quot;cost_info&quot;: &#123; &quot;read_cost&quot;: &quot;968.80&quot;, &quot;eval_cost&quot;: &quot;193.76&quot;, &quot;prefix_cost&quot;: &quot;3197.16&quot;, &quot;data_read_per_join&quot;: &quot;1M&quot;&#125; 由于s2表是被驱动表，所以可能被读取多次，这里的read_cost和eval_cost是访问多次s2表后累加起来的值，主要关注里边儿的prefix_cost的值代表的是整个连接查询预计的成本，也就是单次查询s1表和多次查询s2表后的成本的和，也就是： 1968.80 + 193.76 + 2034.60 = 3197.16 3.Extented EXPLAIN在我们使用EXPLAIN语句查看了某个查询的执行计划后，紧接着还可以使用SHOW WARNINGS语句查看与这个查询的执行计划有关的一些扩展信息，比如这样： 可以看到SHOW WARNINGS展示出来的信息有三个字段，分别是Level、Code、Message。我们最常见的就是Code为1003的信息，当Code值为1003时，Message字段展示的信息类似于查询优化器将我们的查询语句重写后的语句。比如我们上边的查询本来是一个左（外）连接查询，但是有一个s2.common_field IS NOT NULL的条件，着就会导致查询优化器把左（外）连接查询优化为内连接查询，从SHOW WARNINGS的Message字段也可以看出来，原本的LEFT JOIN已经变成了JOIN。 我们说Message字段展示的信息类似于查询优化器将我们的查询语句重写后的语句，并不是等价于，也就是说Message字段展示的信息并不是标准的查询语句，在很多情况下并不能直接运行，它只能作为帮助我们理解查MySQL将如何执行查询语句的一个参考依据而已。 二，optimizer trace对于MySQL 5.6以及之前的版本来说，查询优化器就像是一个黑盒子一样，只能通过EXPLAIN语句查看到最后优化器决定使用的执行计划，却无法知道它为什么做这个决策。 在MySQL 5.6以及之后的版本中，MySQL提出了一个optimizer trace的功能，这个功能可以让我们方便的查看优化器生成执行计划的整个过程，这个功能的开启与关闭由系统变量optimizer_trace决定，我们看一下： 1234567mysql&gt; SHOW VARIABLES LIKE &#x27;optimizer_trace&#x27;;+-----------------+--------------------------+| Variable_name | Value |+-----------------+--------------------------+| optimizer_trace | enabled=off,one_line=off |+-----------------+--------------------------+1 row in set (0.02 sec) 可以看到enabled值为off，表明这个功能默认是关闭的。 one_line的值是控制输出格式的，如果为on那么所有输出都将在一行中展示，不适合人阅读，所以我们就保持其默认值为off吧。 如果想打开这个功能，必须首先把enabled的值改为on，就像这样： 12mysql&gt; SET optimizer_trace=&quot;enabled=on&quot;;Query OK, 0 rows affected (0.00 sec) 然后我们就可以输入我们想要查看优化过程的查询语句，当该查询语句执行完成后，就可以到information_schema数据库下的OPTIMIZER_TRACE表中查看完整的优化过程。这个OPTIMIZER_TRACE表有4个列，分别是： QUERY：表示我们的查询语句。 TRACE：表示优化过程的JSON格式文本。 MISSING_BYTES_BEYOND_MAX_MEM_SIZE：由于优化过程可能会输出很多，如果超过某个限制时，多余的文本将不会被显示，这个字段展示了被忽略的文本字节数。 INSUFFICIENT_PRIVILEGES：表示是否没有权限查看优化过程，默认值是0，只有某些特殊情况下才会是1，我们暂时不关心这个字段的值。 完整的使用optimizer trace功能的步骤总结如下： 1234567891011121314# 1. 打开optimizer trace功能 (默认情况下它是关闭的):SET optimizer_trace=&quot;enabled=on&quot;;# 2. 这里输入查询语句SELECT ...; # 3. 从OPTIMIZER_TRACE表中查看上一个查询的优化过程SELECT * FROM information_schema.OPTIMIZER_TRACE;# 4. 可能还要观察其他语句执行的优化过程，重复上边的第2、3步...# 5. 当停止查看语句的优化过程时，把optimizer trace功能关闭SET optimizer_trace=&quot;enabled=off&quot;; 现在我们有一个搜索条件比较多的查询语句，它的执行计划如下： 可以看到该查询可能使用到的索引有3个，那么为什么优化器最终选择了idx_key2而不选择其他的索引或者直接全表扫描呢？这时候就可以通过otpimzer trace功能来查看优化器的具体工作过程： 123456789SET optimizer_trace=&quot;enabled=on&quot;;SELECT * FROM s1 WHERE key1 &gt; &#x27;z&#x27; AND key2 &lt; 1000000 AND key3 IN (&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;) AND common_field = &#x27;abc&#x27;; SELECT * FROM information_schema.OPTIMIZER_TRACE\\G 直接看一下通过查询OPTIMIZER_TRACE表得到的输出： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281*************************** 1. row ***************************# 分析的查询语句是什么QUERY: SELECT * FROM s1 WHERE key1 &gt; &#x27;z&#x27; AND key2 &lt; 1000000 AND key3 IN (&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;) AND common_field = &#x27;abc&#x27;# 优化的具体过程TRACE: &#123; &quot;steps&quot;: [ &#123; &quot;join_preparation&quot;: &#123; # prepare阶段 &quot;select#&quot;: 1, &quot;steps&quot;: [ &#123; &quot;IN_uses_bisection&quot;: true &#125;, &#123; &quot;expanded_query&quot;: &quot;/* select#1 */ select `s1`.`id` AS `id`,`s1`.`key1` AS `key1`,`s1`.`key2` AS `key2`,`s1`.`key3` AS `key3`,`s1`.`key_part1` AS `key_part1`,`s1`.`key_part2` AS `key_part2`,`s1`.`key_part3` AS `key_part3`,`s1`.`common_field` AS `common_field` from `s1` where ((`s1`.`key1` &gt; &#x27;z&#x27;) and (`s1`.`key2` &lt; 1000000) and (`s1`.`key3` in (&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;)) and (`s1`.`common_field` = &#x27;abc&#x27;))&quot; &#125; ] /* steps */ &#125; /* join_preparation */ &#125;, &#123; &quot;join_optimization&quot;: &#123; # optimize阶段 &quot;select#&quot;: 1, &quot;steps&quot;: [ &#123; &quot;condition_processing&quot;: &#123; # 处理搜索条件 &quot;condition&quot;: &quot;WHERE&quot;, # 原始搜索条件 &quot;original_condition&quot;: &quot;((`s1`.`key1` &gt; &#x27;z&#x27;) and (`s1`.`key2` &lt; 1000000) and (`s1`.`key3` in (&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;)) and (`s1`.`common_field` = &#x27;abc&#x27;))&quot;, &quot;steps&quot;: [ &#123; # 等值传递转换 &quot;transformation&quot;: &quot;equality_propagation&quot;, &quot;resulting_condition&quot;: &quot;((`s1`.`key1` &gt; &#x27;z&#x27;) and (`s1`.`key2` &lt; 1000000) and (`s1`.`key3` in (&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;)) and (`s1`.`common_field` = &#x27;abc&#x27;))&quot; &#125;, &#123; # 常量传递转换 &quot;transformation&quot;: &quot;constant_propagation&quot;, &quot;resulting_condition&quot;: &quot;((`s1`.`key1` &gt; &#x27;z&#x27;) and (`s1`.`key2` &lt; 1000000) and (`s1`.`key3` in (&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;)) and (`s1`.`common_field` = &#x27;abc&#x27;))&quot; &#125;, &#123; # 去除没用的条件 &quot;transformation&quot;: &quot;trivial_condition_removal&quot;, &quot;resulting_condition&quot;: &quot;((`s1`.`key1` &gt; &#x27;z&#x27;) and (`s1`.`key2` &lt; 1000000) and (`s1`.`key3` in (&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;)) and (`s1`.`common_field` = &#x27;abc&#x27;))&quot; &#125; ] /* steps */ &#125; /* condition_processing */ &#125;, &#123; # 替换虚拟生成列 &quot;substitute_generated_columns&quot;: &#123; &#125; /* substitute_generated_columns */ &#125;, &#123; # 表的依赖信息 &quot;table_dependencies&quot;: [ &#123; &quot;table&quot;: &quot;`s1`&quot;, &quot;row_may_be_null&quot;: false, &quot;map_bit&quot;: 0, &quot;depends_on_map_bits&quot;: [ ] /* depends_on_map_bits */ &#125; ] /* table_dependencies */ &#125;, &#123; &quot;ref_optimizer_key_uses&quot;: [ ] /* ref_optimizer_key_uses */ &#125;, &#123; # 预估不同单表访问方法的访问成本 &quot;rows_estimation&quot;: [ &#123; &quot;table&quot;: &quot;`s1`&quot;, &quot;range_analysis&quot;: &#123; &quot;table_scan&quot;: &#123; # 全表扫描的行数以及成本 &quot;rows&quot;: 9688, &quot;cost&quot;: 2036.7 &#125; /* table_scan */, # 分析可能使用的索引 &quot;potential_range_indexes&quot;: [ &#123; &quot;index&quot;: &quot;PRIMARY&quot;, # 主键不可用 &quot;usable&quot;: false, &quot;cause&quot;: &quot;not_applicable&quot; &#125;, &#123; &quot;index&quot;: &quot;idx_key2&quot;, # idx_key2可能被使用 &quot;usable&quot;: true, &quot;key_parts&quot;: [ &quot;key2&quot; ] /* key_parts */ &#125;, &#123; &quot;index&quot;: &quot;idx_key1&quot;, # idx_key1可能被使用 &quot;usable&quot;: true, &quot;key_parts&quot;: [ &quot;key1&quot;, &quot;id&quot; ] /* key_parts */ &#125;, &#123; &quot;index&quot;: &quot;idx_key3&quot;, # idx_key3可能被使用 &quot;usable&quot;: true, &quot;key_parts&quot;: [ &quot;key3&quot;, &quot;id&quot; ] /* key_parts */ &#125;, &#123; &quot;index&quot;: &quot;idx_key_part&quot;, # idx_keypart不可用 &quot;usable&quot;: false, &quot;cause&quot;: &quot;not_applicable&quot; &#125; ] /* potential_range_indexes */, &quot;setup_range_conditions&quot;: [ ] /* setup_range_conditions */, &quot;group_index_range&quot;: &#123; &quot;chosen&quot;: false, &quot;cause&quot;: &quot;not_group_by_or_distinct&quot; &#125; /* group_index_range */, # 分析各种可能使用的索引的成本 &quot;analyzing_range_alternatives&quot;: &#123; &quot;range_scan_alternatives&quot;: [ &#123; # 使用idx_key2的成本分析 &quot;index&quot;: &quot;idx_key2&quot;, # 使用idx_key2的范围区间 &quot;ranges&quot;: [ &quot;NULL &lt; key2 &lt; 1000000&quot; ] /* ranges */, &quot;index_dives_for_eq_ranges&quot;: true, # 是否使用index dive &quot;rowid_ordered&quot;: false, # 使用该索引获取的记录是否按照主键排序 &quot;using_mrr&quot;: false, # 是否使用mrr &quot;index_only&quot;: false, # 是否是索引覆盖访问 &quot;rows&quot;: 12, # 使用该索引获取的记录条数 &quot;cost&quot;: 15.41, # 使用该索引的成本 &quot;chosen&quot;: true # 是否选择该索引 &#125;, &#123; # 使用idx_key1的成本分析 &quot;index&quot;: &quot;idx_key1&quot;, # 使用idx_key1的范围区间 &quot;ranges&quot;: [ &quot;z &lt; key1&quot; ] /* ranges */, &quot;index_dives_for_eq_ranges&quot;: true, # 同上 &quot;rowid_ordered&quot;: false, # 同上 &quot;using_mrr&quot;: false, # 同上 &quot;index_only&quot;: false, # 同上 &quot;rows&quot;: 266, # 同上 &quot;cost&quot;: 320.21, # 同上 &quot;chosen&quot;: false, # 同上 &quot;cause&quot;: &quot;cost&quot; # 因为成本太大所以不选择该索引 &#125;, &#123; # 使用idx_key3的成本分析 &quot;index&quot;: &quot;idx_key3&quot;, # 使用idx_key3的范围区间 &quot;ranges&quot;: [ &quot;a &lt;= key3 &lt;= a&quot;, &quot;b &lt;= key3 &lt;= b&quot;, &quot;c &lt;= key3 &lt;= c&quot; ] /* ranges */, &quot;index_dives_for_eq_ranges&quot;: true, # 同上 &quot;rowid_ordered&quot;: false, # 同上 &quot;using_mrr&quot;: false, # 同上 &quot;index_only&quot;: false, # 同上 &quot;rows&quot;: 21, # 同上 &quot;cost&quot;: 28.21, # 同上 &quot;chosen&quot;: false, # 同上 &quot;cause&quot;: &quot;cost&quot; # 同上 &#125; ] /* range_scan_alternatives */, # 分析使用索引合并的成本 &quot;analyzing_roworder_intersect&quot;: &#123; &quot;usable&quot;: false, &quot;cause&quot;: &quot;too_few_roworder_scans&quot; &#125; /* analyzing_roworder_intersect */ &#125; /* analyzing_range_alternatives */, # 对于上述单表查询s1最优的访问方法 &quot;chosen_range_access_summary&quot;: &#123; &quot;range_access_plan&quot;: &#123; &quot;type&quot;: &quot;range_scan&quot;, &quot;index&quot;: &quot;idx_key2&quot;, &quot;rows&quot;: 12, &quot;ranges&quot;: [ &quot;NULL &lt; key2 &lt; 1000000&quot; ] /* ranges */ &#125; /* range_access_plan */, &quot;rows_for_plan&quot;: 12, &quot;cost_for_plan&quot;: 15.41, &quot;chosen&quot;: true &#125; /* chosen_range_access_summary */ &#125; /* range_analysis */ &#125; ] /* rows_estimation */ &#125;, &#123; # 分析各种可能的执行计划 #（对多表查询这可能有很多种不同的方案，单表查询的方案上边已经分析过了，直接选取idx_key2就好） &quot;considered_execution_plans&quot;: [ &#123; &quot;plan_prefix&quot;: [ ] /* plan_prefix */, &quot;table&quot;: &quot;`s1`&quot;, &quot;best_access_path&quot;: &#123; &quot;considered_access_paths&quot;: [ &#123; &quot;rows_to_scan&quot;: 12, &quot;access_type&quot;: &quot;range&quot;, &quot;range_details&quot;: &#123; &quot;used_index&quot;: &quot;idx_key2&quot; &#125; /* range_details */, &quot;resulting_rows&quot;: 12, &quot;cost&quot;: 17.81, &quot;chosen&quot;: true &#125; ] /* considered_access_paths */ &#125; /* best_access_path */, &quot;condition_filtering_pct&quot;: 100, &quot;rows_for_plan&quot;: 12, &quot;cost_for_plan&quot;: 17.81, &quot;chosen&quot;: true &#125; ] /* considered_execution_plans */ &#125;, &#123; # 尝试给查询添加一些其他的查询条件 &quot;attaching_conditions_to_tables&quot;: &#123; &quot;original_condition&quot;: &quot;((`s1`.`key1` &gt; &#x27;z&#x27;) and (`s1`.`key2` &lt; 1000000) and (`s1`.`key3` in (&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;)) and (`s1`.`common_field` = &#x27;abc&#x27;))&quot;, &quot;attached_conditions_computation&quot;: [ ] /* attached_conditions_computation */, &quot;attached_conditions_summary&quot;: [ &#123; &quot;table&quot;: &quot;`s1`&quot;, &quot;attached&quot;: &quot;((`s1`.`key1` &gt; &#x27;z&#x27;) and (`s1`.`key2` &lt; 1000000) and (`s1`.`key3` in (&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;)) and (`s1`.`common_field` = &#x27;abc&#x27;))&quot; &#125; ] /* attached_conditions_summary */ &#125; /* attaching_conditions_to_tables */ &#125;, &#123; # 再稍稍的改进一下执行计划 &quot;refine_plan&quot;: [ &#123; &quot;table&quot;: &quot;`s1`&quot;, &quot;pushed_index_condition&quot;: &quot;(`s1`.`key2` &lt; 1000000)&quot;, &quot;table_condition_attached&quot;: &quot;((`s1`.`key1` &gt; &#x27;z&#x27;) and (`s1`.`key3` in (&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;)) and (`s1`.`common_field` = &#x27;abc&#x27;))&quot; &#125; ] /* refine_plan */ &#125; ] /* steps */ &#125; /* join_optimization */ &#125;, &#123; &quot;join_execution&quot;: &#123; # execute阶段 &quot;select#&quot;: 1, &quot;steps&quot;: [ ] /* steps */ &#125; /* join_execution */ &#125; ] /* steps */&#125;# 因优化过程文本太多而丢弃的文本字节大小，值为0时表示并没有丢弃MISSING_BYTES_BEYOND_MAX_MEM_SIZE: 0# 权限字段INSUFFICIENT_PRIVILEGES: 01 row in set (0.00 sec) 这只是优化器执行过程中的一小部分，MySQL可能会在之后的版本中添加更多的优化过程信息。不过杂乱之中其实还是蛮有规律的，优化过程大致分为了三个阶段： prepare阶段 optimize阶段 execute阶段 我们所说的基于成本的优化主要集中在optimize阶段，对于单表查询来说，我们主要关注optimize阶段的&quot;rows_estimation&quot;这个过程，这个过程深入分析了对单表查询的各种执行方案的成本；对于多表连接查询来说，我们更多需要关注&quot;considered_execution_plans&quot;这个过程，这个过程里会写明各种不同的连接方式所对应的成本。反正优化器最终会选择成本最低的那种方案来作为最终的执行计划，也就是我们使用EXPLAIN语句所展现出的那种方案。 如果对使用EXPLAIN语句展示出的对某个查询的执行计划很不理解，可以尝试使用optimizer trace功能来详细了解每一种执行方案对应的成本。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/tags/MySQL/"}]},{"title":"MySQL[九]基于规则的优化&子查询优化","slug":"MySQL/MySQL[九]基于规则的优化&子查询优化","date":"2022-01-11T03:16:30.409Z","updated":"2022-01-11T03:23:03.085Z","comments":true,"path":"2022/01/11/MySQL/MySQL[九]基于规则的优化&子查询优化/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/MySQL/MySQL[%E4%B9%9D]%E5%9F%BA%E4%BA%8E%E8%A7%84%E5%88%99%E7%9A%84%E4%BC%98%E5%8C%96&%E5%AD%90%E6%9F%A5%E8%AF%A2%E4%BC%98%E5%8C%96/","excerpt":"","text":"MySQL依据一些规则，把语句转换成某种可以比较高效执行的形式，这个过程也可以被称作查询重写。 1.条件化简我们编写的查询语句的搜索条件本质上是一个表达式，这些表达式可能比较繁杂，或者不能高效的执行，MySQL的查询优化器会为我们简化这些表达式。 1.1移除不必要的括号有时候表达式里有许多无用的括号，比如这样： 1((a = 5 AND b = c) OR ((a &gt; c) AND (c &lt; 5))) 优化器会把那些用不到的括号给干掉，就是这样： 1(a = 5 and b = c) OR (a &gt; c AND c &lt; 5) 1.2常量传递有时候某个表达式是某个列和某个常量做等值匹配，比如这样： 1a = 5 当这个表达式和其他涉及列a的表达式使用AND连接起来时，可以将其他表达式中的a的值替换为5，比如这样： 1a = 5 AND b &gt; a 就可以被转换为： 1a = 5 AND b &gt; 5 用OR的表达式不能进行能量传递是因为OR两边的条件是取并集的，或者说互不相关。 1.3 等值传递有时候多个列之间存在等值匹配的关系，比如这样： 1a = b and b = c and c = 5 这个表达式可以被简化为： 1a = 5 and b = 5 and c = 5 1.4移除没用的条件对于一些明显永远为TRUE或者FALSE的表达式，优化器会移除掉它们，比如这个表达式： 1(a &lt; 1 and b = b) OR (a = 6 OR 5 != 5) 很明显，b = b这个表达式永远为TRUE，5 != 5这个表达式永远为FALSE，所以简化后的表达式就是这样的： 1(a &lt; 1 and TRUE) OR (a = 6 OR FALSE) 可以继续被简化为 1a &lt; 1 OR a = 6 1.5表达式计算在查询开始执行之前，如果表达式中只包含常量的话，它的值会被先计算出来，比如这个： 1a = 5 + 1 因为5 + 1这个表达式只包含常量，所以就会被化简成： 1a = 6 但是这里需要注意的是，如果某个列并不是以单独的形式作为表达式的操作数时，比如出现在函数中，出现在某个更复杂表达式中，就像这样： 1ABS(a) &gt; 5 或者： 1-a &lt; -8 优化器是不会尝试对这些表达式进行化简的。我们前边说过只有搜索条件中索引列和常数使用某些运算符连接起来才可能使用到索引，所以如果可以的话，最好让索引列以单独的形式出现在表达式中。 1.6HAVING&amp;WHERE子句的合并如果查询语句中没有出现诸如SUM、MAX等等的聚集函数以及GROUP BY子句，优化器就把HAVING子句和WHERE子句合并起来。 1.7常量表检测MySQL觉得下边这两种查询运行的特别快： 查询的表中一条记录没有，或者只有一条记录。 使用主键等值匹配或者唯一二级索引列等值匹配作为搜索条件来查询某个表。 MySQL觉得这两种查询花费的时间特别少，少到可以忽略，所以也把通过这两种方式查询的表称之为常量表（英文名：constant tables）。优化器在分析一个查询语句时，先首先执行常量表查询，然后把查询中涉及到该表的条件全部替换成常数，最后再分析其余表的查询成本，比方说这个查询语句： 123SELECT * FROM table1 INNER JOIN table2 ON table1.column1 = table2.column2 WHERE table1.primary_key = 1; 很明显，这个查询可以使用主键和常量值的等值匹配来查询table1表，也就是在这个查询中table1表相当于常量表，在分析对table2表的查询成本之前，就会执行对table1表的查询，并把查询中涉及table1表的条件都替换掉，也就是上边的语句会被转换成这样： 12SELECT table1表记录的各个字段的常量值, table2.* FROM table1 INNER JOIN table2 ON table1表column1列的常量值 = table2.column2; 2.外连接消除内连接的驱动表和被驱动表的位置可以相互转换，而左（外）连接和右（外）连接的驱动表和被驱动表是固定的。这就导致内连接可能通过优化表的连接顺序来降低整体的查询成本，而外连接却无法优化表的连接顺序。我在之前的文章创建了两个表： 123456789CREATE TABLE t1 ( m1 int, n1 char(1)) Engine=InnoDB, CHARSET=utf8;CREATE TABLE t2 ( m2 int, n2 char(1)) Engine=InnoDB, CHARSET=utf8; 再看一下表的数据 12345678910111213141516171819mysql&gt; SELECT * FROM t1;+------+------+| m1 | n1 |+------+------+| 1 | a || 2 | b || 3 | c |+------+------+3 rows in set (0.00 sec)mysql&gt; SELECT * FROM t2;+------+------+| m2 | n2 |+------+------+| 2 | b || 3 | c || 4 | d |+------+------+3 rows in set (0.00 sec) 外连接和内连接的本质区别就是：对于外连接的驱动表的记录来说，如果无法在被驱动表中找到匹配ON子句中的过滤条件的记录，那么该记录仍然会被加入到结果集中，对应的被驱动表记录的各个字段使用NULL值填充；而内连接的驱动表的记录如果无法在被驱动表中找到匹配ON子句中的过滤条件的记录，那么该记录会被舍弃。查询效果就是这样： 123456789101112131415161718mysql&gt; SELECT * FROM t1 INNER JOIN t2 ON t1.m1 = t2.m2;+------+------+------+------+| m1 | n1 | m2 | n2 |+------+------+------+------+| 2 | b | 2 | b || 3 | c | 3 | c |+------+------+------+------+2 rows in set (0.00 sec)mysql&gt; SELECT * FROM t1 LEFT JOIN t2 ON t1.m1 = t2.m2;+------+------+------+------+| m1 | n1 | m2 | n2 |+------+------+------+------+| 2 | b | 2 | b || 3 | c | 3 | c || 1 | a | NULL | NULL |+------+------+------+------+3 rows in set (0.00 sec) 对于上边例子中的（左）外连接来说，由于驱动表t1中m1=1, n1=&#39;a&#39;的记录无法在被驱动表t2中找到符合ON子句条件t1.m1 = t2.m2的记录，所以就直接把这条记录加入到结果集，对应的t2表的m2和n2列的值都设置为NULL。 右（外）连接和左（外）连接其实只在驱动表的选取方式上是不同的，其余方面都是一样的，所以优化器会首先把右（外）连接查询转换成左（外）连接查询。 凡是不符合WHERE子句中条件的记录都不会参与连接。只要我们在搜索条件中指定关于被驱动表相关列的值不为NULL，那么外连接中在被驱动表中找不到符合ON子句条件的驱动表记录也就被排除出最后的结果集了，也就是说：在这种情况下：外连接和内连接也就没有什么区别了！比方说这个查询： 12345678mysql&gt; SELECT * FROM t1 LEFT JOIN t2 ON t1.m1 = t2.m2 WHERE t2.n2 IS NOT NULL;+------+------+------+------+| m1 | n1 | m2 | n2 |+------+------+------+------+| 2 | b | 2 | b || 3 | c | 3 | c |+------+------+------+------+2 rows in set (0.01 sec) 由于指定了被驱动表t2的n2列不允许为NULL，所以上边的t1和t2表的左（外）连接查询和内连接查询是一样一样的。当然，我们也可以不用显式的指定被驱动表的某个列IS NOT NULL，只要隐含的有这个意思就行了，比方说这样： 1234567mysql&gt; SELECT * FROM t1 LEFT JOIN t2 ON t1.m1 = t2.m2 WHERE t2.m2 = 2;+------+------+------+------+| m1 | n1 | m2 | n2 |+------+------+------+------+| 2 | b | 2 | b |+------+------+------+------+1 row in set (0.00 sec) 在这个例子中，我们在WHERE子句中指定了被驱动表t2的m2列等于2，也就相当于间接的指定了m2列不为NULL值，所以上边的这个左（外）连接查询其实和下边这个内连接查询是等价的： 1234567mysql&gt; SELECT * FROM t1 INNER JOIN t2 ON t1.m1 = t2.m2 WHERE t2.m2 = 2;+------+------+------+------+| m1 | n1 | m2 | n2 |+------+------+------+------+| 2 | b | 2 | b |+------+------+------+------+1 row in set (0.00 sec) 我们把这种在外连接查询中，指定的WHERE子句中包含被驱动表中的列不为NULL值的条件称之为空值拒绝（英文名：reject-NULL）。在被驱动表的WHERE子句符合空值拒绝的条件后，外连接和内连接可以相互转换。这种转换带来的好处就是查询优化器可以通过评估表的不同连接顺序的成本，选出成本最低的那种连接顺序来执行查询。 3.子查询优化3.1子查询语法3.1.1按返回的结果集区分子查询因为子查询本身也算是一个查询，所以可以按照它们返回的不同结果集类型而把这些子查询分为不同的类型： 标量子查询那些只返回一个单一值的子查询称之为标量子查询，比如这样：或者这样：这两个查询语句中的子查询都返回一个单一的值，也就是一个标量。这些标量子查询可以作为一个单一值或者表达式的一部分出现在查询语句的各个地方。 1SELECT (SELECT m1 FROM t1 LIMIT 1); 1SELECT * FROM t1 WHERE m1 = (SELECT MIN(m2) FROM t2); 行子查询顾名思义，就是返回一条记录的子查询，不过这条记录需要包含多个列（只包含一个列就成了标量子查询了）。比如这样：其中的(SELECT m2, n2 FROM t2 LIMIT 1)就是一个行子查询，整条语句的含义就是要从t1表中找一些记录，这些记录的m1和n1列分别等于子查询结果中的m2和n2列。 1SELECT * FROM t1 WHERE (m1, n1) = (SELECT m2, n2 FROM t2 LIMIT 1); 列子查询列子查询自然就是查询出一个列的数据喽，不过这个列的数据需要包含多条记录（只包含一条记录就成了标量子查询了）。比如这样：其中的(SELECT m2 FROM t2)就是一个列子查询，表明查询出t2表的m2列的值作为外层查询IN语句的参数。 1SELECT * FROM t1 WHERE m1 IN (SELECT m2 FROM t2); 表子查询顾名思义，就是子查询的结果既包含很多条记录，又包含很多个列，比如这样：其中的(SELECT m2, n2 FROM t2)就是一个表子查询，这里需要和行子查询对比一下，行子查询中我们用了LIMIT 1来保证子查询的结果只有一条记录，表子查询中不需要这个限制。 1SELECT * FROM t1 WHERE (m1, n1) IN (SELECT m2, n2 FROM t2); 3.1.2按与外层查询关系来区分子查询 不相关子查询如果子查询可以单独运行出结果，而不依赖于外层查询的值，我们就可以把这个子查询称之为不相关子查询。我们前边介绍的那些子查询全部都可以看作不相关子查询，所以也就不举例子了哈。 相关子查询如果子查询的执行需要依赖于外层查询的值，我们就可以把这个子查询称之为相关子查询。比如：例子中的子查询是(SELECT m2 FROM t2 WHERE n1 = n2)，可是这个查询中有一个搜索条件是n1 = n2，别忘了n1是表t1的列，也就是外层查询的列，也就是说子查询的执行需要依赖于外层查询的值，所以这个子查询就是一个相关子查询。 1SELECT * FROM t1 WHERE m1 IN (SELECT m2 FROM t2 WHERE n1 = n2); 3.1.3 子查询语法注意事项 子查询必须用小括号扩起来。 在SELECT子句中的子查询必须是标量子查询。 在想要得到标量子查询或者行子查询，但又不能保证子查询的结果集只有一条记录时，应该使用LIMIT 1语句来限制记录数量。 对于[NOT] IN/ANY/SOME/ALL子查询来说，子查询中不允许有LIMIT语句。比如这样是非法的：因为[NOT] IN/ANY/SOME/ALL子查询不支持LIMIT语句，所以子查询中的这些语句也就是多余的了： 1mysql&gt; SELECT * FROM t1 WHERE m1 IN (SELECT * FROM t2 LIMIT 2); ORDER BY子句：子查询的结果其实就相当于一个集合，集合里的值排不排序一点儿都不重要。 DISTINCT语句：集合里的值去不去重也没啥意义。 没有聚集函数以及HAVING子句的GROUP BY子句。 对于这些冗余的语句，查询优化器在一开始就把它们给干掉了。 不允许在一条语句中增删改某个表的记录时同时还对该表进行子查询。 1mysql&gt; DELETE FROM t1 WHERE m1 &lt; (SELECT MAX(m1) FROM t1); 3.2 子查询的执行还是复用前面的表： 123456789101112131415CREATE TABLE single_table ( id INT NOT NULL AUTO_INCREMENT, key1 VARCHAR(100), key2 INT, key3 VARCHAR(100), key_part1 VARCHAR(100), key_part2 VARCHAR(100), key_part3 VARCHAR(100), common_field VARCHAR(100), PRIMARY KEY (id), KEY idx_key1 (key1), UNIQUE KEY idx_key2 (key2), KEY idx_key3 (key3), KEY idx_key_part(key_part1, key_part2, key_part3)) Engine=InnoDB CHARSET=utf8; 我们假设有两个表s1、s2与这个single_table表的构造是相同的，而且这两个表里边儿有10000条记录。 3.2.1 标量/行子查询的执行方式我们经常在下边两个场景中使用到标量子查询或者行子查询： SELECT子句中，我们前边说过的在查询列表中的子查询必须是标量子查询。 子查询使用=、&gt;、&lt;、&gt;=、&lt;=、&lt;&gt;、!=、&lt;=&gt;等操作符和某个操作数组成一个布尔表达式，这样的子查询必须是标量子查询或者行子查询。 对于上述两种场景中的不相关标量子查询或者行子查询来说，它们的执行方式是简单的，比方说下边这个查询语句： 12SELECT * FROM s1 WHERE key1 = (SELECT common_field FROM s2 WHERE key3 = &#x27;a&#x27; LIMIT 1); 先单独执行(SELECT common_field FROM s2 WHERE key3 = &#39;a&#39; LIMIT 1)这个子查询。 然后在将上一步子查询得到的结果当作外层查询的参数再执行外层查询SELECT * FROM s1 WHERE key1 = ...。 也就是说，对于包含不相关的标量子查询或者行子查询的查询语句来说，MySQL会分别独立的执行外层查询和子查询，就当作两个单表查询就好了。 对于相关的标量子查询或者行子查询来说，比如下边这个查询： 12SELECT * FROM s1 WHERE key1 = (SELECT common_field FROM s2 WHERE s1.key3 = s2.key3 LIMIT 1); 3.2.2 IN子查询优化① 物化表的提出12SELECT * FROM s1 WHERE key1 IN (SELECT common_field FROM s2 WHERE key3 = &#x27;a&#x27;); 对于不相关的IN子查询来说，如果子查询的结果集中的记录条数很少，那么把子查询和外层查询分别看成两个单独的单表查询效率还行，但是如果单独执行子查询后的结果集太多的话，就会导致这些问题： 结果集太多，可能内存中都放不下。 对于外层查询来说，如果子查询的结果集太多，那就意味着IN子句中的参数特别多，这就导致： 无法有效的使用索引，只能对外层查询进行全表扫描。 在对外层查询执行全表扫描时，由于IN子句中的参数太多，这会导致检测一条记录是否符合和IN子句中的参数匹配花费的时间太长。 所以MySQL并不直接将不相关子查询的结果集当作外层查询的参数，而是将该结果集写入一个临时表里。 该临时表的列就是子查询结果集中的列。 写入临时表的记录会被去重。我们说IN语句是判断某个操作数在不在某个集合中，集合中的值重不重复对整个IN语句的结果并没有影响，所以我们在将结果集写入临时表时对记录进行去重可以让临时表变得更小。 临时表如何对记录进行去重？只要为表中记录的所有列建立主键或者唯一索引就好了嘛～ 一般情况下子查询结果集不会大的离谱，所以会为它建立基于内存的使用Memory存储引擎的临时表，而且会为该表建立哈希索引。如果子查询的结果集非常大，超过了系统变量tmp_table_size或者max_heap_table_size，临时表会转而使用基于磁盘的存储引擎来保存结果集中的记录，索引类型也对应转变为B+树索引。 IN语句的本质就是判断某个操作数在不在某个集合里，如果集合中的数据建立了哈希索引，那么这个匹配的过程就是超级快的。 MySQL把这个将子查询结果集中的记录保存到临时表的过程称之为物化。为了方便起见，我们就把那个存储子查询结果集的临时表称之为物化表。正因为物化表中的记录都建立了索引（基于内存的物化表有哈希索引，基于磁盘的有B+树索引），通过索引执行IN语句判断某个操作数在不在子查询结果集中变得非常快，从而提升了子查询语句的性能。 ② 物化表转连接再看一下最开始的那个查询语句： 12SELECT * FROM s1 WHERE key1 IN (SELECT common_field FROM s2 WHERE key3 = &#x27;a&#x27;); 当我们把子查询进行物化之后，假设子查询物化表的名称为materialized_table，该物化表存储的子查询结果集的列为m_val，那么这个查询其实可以从下边两种角度来看待： 从表s1的角度来看待，整个查询的意思其实是：对于s1表中的每条记录来说，如果该记录的key1列的值在子查询对应的物化表中，则该记录会被加入最终的结果集。画个图表示一下就是这样： 从子查询物化表的角度来看待，整个查询的意思其实是：对于子查询物化表的每个值来说，如果能在s1表中找到对应的key1列的值与该值相等的记录，那么就把这些记录加入到最终的结果集。画个图表示一下就是这样： 也就是说其实上边的查询就相当于表s1和子查询物化表materialized_table进行内连接： 1SELECT s1.* FROM s1 INNER JOIN materialized_table ON key1 = m_val; 转化成内连接之后就有意思了，查询优化器可以评估不同连接顺序需要的成本是多少，选取成本最低的那种查询方式执行查询。我们分析一下上述查询中使用外层查询的表s1和物化表materialized_table进行内连接的成本都是由哪几部分组成的： 如果使用s1表作为驱动表的话，总查询成本由下边几个部分组成： 物化子查询时需要的成本 扫描s1表时的成本 s1表中的记录数量 × 通过m_val = xxx对materialized_table表进行单表访问的成本（我们前边说过物化表中的记录是不重复的，并且为物化表中的列建立了索引，所以这个步骤显然是非常快的）。 如果使用materialized_table表作为驱动表的话，总查询成本由下边几个部分组成： 物化子查询时需要的成本 扫描物化表时的成本 物化表中的记录数量 × 通过key1 = xxx对s1表进行单表访问的成本（非常庆幸key1列上建立了索引，所以这个步骤是非常快的）。 MySQL查询优化器会通过运算来选择上述成本更低的方案来执行查询。 ③ 将子查询转换为semi-join虽然将子查询进行物化之后再执行查询都会有建立临时表的成本，但是不管怎么说，我们见识到了将子查询转换为连接的强大作用，能不能不进行物化操作直接把子查询转换为连接呢？ 12SELECT * FROM s1 WHERE key1 IN (SELECT common_field FROM s2 WHERE key3 = &#x27;a&#x27;); 我们可以把这个查询理解成：对于s1表中的某条记录，如果我们能在s2表（准确的说是执行完WHERE s2.key3 = &#39;a&#39;之后的结果集）中找到一条或多条记录，这些记录的common_field的值等于s1表记录的key1列的值，那么该条s1表的记录就会被加入到最终的结果集。这个过程其实和把s1和s2两个表连接起来的效果很像： 123SELECT s1.* FROM s1 INNER JOIN s2 ON s1.key1 = s2.common_field WHERE s2.key3 = &#x27;a&#x27;; 只不过我们不能保证对于s1表的某条记录来说，在s2表（准确的说是执行完WHERE s2.key3 = &#39;a&#39;之后的结果集）中有多少条记录满足s1.key1 = s2.common_field这个条件，不过我们可以分三种情况讨论： 情况一：对于s1表的某条记录来说，s2表中没有任何记录满足s1.key1 = s2.common_field这个条件，那么该记录自然也不会加入到最后的结果集。 情况二：对于s1表的某条记录来说，s2表中有且只有1条记录满足s1.key1 = s2.common_field这个条件，那么该记录会被加入最终的结果集。 情况三：对于s1表的某条记录来说，s2表中至少有2条记录满足s1.key1 = s2.common_field这个条件，那么该记录会被多次加入最终的结果集。 对于s1表的某条记录来说，由于我们只关心s2表中是否存在记录满足s1.key1 = s2.common_field这个条件，而不关心具体有多少条记录与之匹配，又因为有情况三的存在，我们上边所说的IN子查询和两表连接之间并不完全等价。但是将子查询转换为连接又真的可以充分发挥优化器的作用，所以MySQL在这里提出了一个新概念 — 半连接。将s1表和s2表进行半连接的意思就是：对于s1表的某条记录来说，我们只关心在s2表中是否存在与之匹配的记录，而不关心具体有多少条记录与之匹配，最终的结果集中只保留s1表的记录。我们假设MySQL内部是这么改写上边的子查询的： 123SELECT s1.* FROM s1 SEMI JOIN s2 ON s1.key1 = s2.common_field WHERE key3 = &#x27;a&#x27;; semi-join只是在MySQL内部采用的一种执行子查询的方式，MySQL并没有提供面向用户的semi-join语法，所以我们不需要，也不能尝试把上边这个语句放到mysql客户端执行。 怎么实现这种所谓的半连接呢？ Table pullout （子查询中的表上拉）当子查询的查询列表处只有主键或者唯一索引列时，可以直接把子查询中的表上拉到外层查询的FROM子句中，并把子查询中的搜索条件合并到外层查询的搜索条件中，比如这个由于key2列是s2表的唯一二级索引列，所以我们可以直接把s2表上拉到外层查询的FROM子句中，并且把子查询中的搜索条件合并到外层查询的搜索条件中，上拉之后的查询就是这样的：为什么当子查询的查询列表处只有主键或者唯一索引列时，就可以直接将子查询转换为连接查询呢？主键或者唯一索引列中的数据本身就是不重复的，所以对于同一条s1表中的记录，不可能找到两条以上的符合s1.key2 = s2.key2的记录。 12SELECT * FROM s1 WHERE key2 IN (SELECT key2 FROM s2 WHERE key3 = &#x27;a&#x27;); 123SELECT s1.* FROM s1 INNER JOIN s2 ON s1.key2 = s2.key2 WHERE s2.key3 = &#x27;a&#x27;; DuplicateWeedout execution strategy （重复值消除）对于这个查询来说：转换为半连接查询后，s1表中的某条记录可能在s2表中有多条匹配的记录，所以该条记录可能多次被添加到最后的结果集中，为了消除重复，我们可以建立一个临时表，比方说这个临时表长这样：这样在执行连接查询的过程中，每当某条s1表中的记录要加入结果集时，就首先把这条记录的id值加入到这个临时表里，如果添加成功，说明之前这条s1表中的记录并没有加入最终的结果集，现在把该记录添加到最终的结果集；如果添加失败，说明之前这条s1表中的记录已经加入过最终的结果集，这里直接把它丢弃就好了，这种使用临时表消除semi-join结果集中的重复值的方式称之为DuplicateWeedout。 12SELECT * FROM s1 WHERE key1 IN (SELECT common_field FROM s2 WHERE key3 = &#x27;a&#x27;); 123CREATE TABLE tmp ( id PRIMARY KEY); LooseScan execution strategy （松散扫描）大家看这个查询：在子查询中，对于s2表的访问可以使用到key1列的索引，而恰好子查询的查询列表处就是key1列，这样在将该查询转换为半连接查询后，如果将s2作为驱动表执行查询的话，那么执行过程就是这样：如图所示，在s2表的idx_key1索引中，值为&#39;aa&#39;的二级索引记录一共有3条，那么只需要取第一条的值到s1表中查找s1.key3 = &#39;aa&#39;的记录，如果能在s1表中找到对应的记录，那么就把对应的记录加入到结果集。依此类推，其他值相同的二级索引记录，也只需要取第一条记录的值到s1表中找匹配的记录，这种虽然是扫描索引，但只取值相同的记录的第一条去做匹配操作的方式称之为松散扫描。 12SELECT * FROM s1 WHERE key3 IN (SELECT key1 FROM s2 WHERE key1 &gt; &#x27;a&#x27; AND key1 &lt; &#x27;b&#x27;); Semi-join Materialization execution strategy我们之前介绍的先把外层查询的IN子句中的不相关子查询进行物化，然后再进行外层查询的表和物化表的连接本质上也算是一种semi-join，只不过由于物化表中没有重复的记录，所以可以直接将子查询转为连接查询。 FirstMatch execution strategy （首次匹配）FirstMatch是一种最原始的半连接执行方式，先取一条外层查询的中的记录，然后到子查询的表中寻找符合匹配条件的记录，如果能找到一条，则将该外层查询的记录放入最终的结果集并且停止查找更多匹配的记录，如果找不到则把该外层查询的记录丢弃掉；然后再开始取下一条外层查询中的记录，重复上边这个过程。 对于某些使用IN语句的相关子查询，比方这个查询： 12SELECT * FROM s1 WHERE key1 IN (SELECT common_field FROM s2 WHERE s1.key3 = s2.key3); 它也可以很方便的转为半连接，转换后的语句类似这样： 12SELECT s1.* FROM s1 SEMI JOIN s2 ON s1.key1 = s2.common_field AND s1.key3 = s2.key3; 然后就可以使用我们上边介绍过的DuplicateWeedout、LooseScan、FirstMatch等半连接执行策略来执行查询，当然，如果子查询的查询列表处只有主键或者唯一二级索引列，还可以直接使用table pullout的策略来执行查询，但是，由于相关子查询并不是一个独立的查询，所以不能转换为物化表来执行查询。 ④ semi-join的适用条件当然，并不是所有包含IN子查询的查询语句都可以转换为semi-join，只有形如这样的查询才可以被转换为semi-join： 12SELECT ... FROM outer_tables WHERE expr IN (SELECT ... FROM inner_tables ...) AND ... 或者这样的形式也可以： 12SELECT ... FROM outer_tables WHERE (oe1, oe2, ...) IN (SELECT ie1, ie2, ... FROM inner_tables ...) AND ... 用文字总结一下，只有符合下边这些条件的子查询才可以被转换为semi-join： 该子查询必须是和IN语句组成的布尔表达式，并且在外层查询的WHERE或者ON子句中出现。 外层查询也可以有其他的搜索条件，只不过和IN子查询的搜索条件必须使用AND连接起来。 该子查询必须是一个单一的查询，不能是由若干查询由UNION连接起来的形式。 该子查询不能包含GROUP BY或者HAVING语句或者聚集函数。 … 还有一些条件比较少见…. ⑤ 不适用于semi-join的情况对于一些不能将子查询转位semi-join的情况，典型的比如下边这几种： 外层查询的WHERE条件中有其他搜索条件与IN子查询组成的布尔表达式使用OR连接起来 123SELECT * FROM s1 WHERE key1 IN (SELECT common_field FROM s2 WHERE key3 = &#x27;a&#x27;) OR key2 &gt; 100; 使用NOT IN而不是IN的情况 12SELECT * FROM s1 WHERE key1 NOT IN (SELECT common_field FROM s2 WHERE key3 = &#x27;a&#x27;) 在SELECT子句中的IN子查询的情况 1SELECT key1 IN (SELECT common_field FROM s2 WHERE key3 = &#x27;a&#x27;) FROM s1 ; 子查询中包含GROUP BY、HAVING或者聚集函数的情况 12SELECT * FROM s1 WHERE key2 IN (SELECT COUNT(*) FROM s2 GROUP BY key1); 子查询中包含UNION的情况 12345SELECT * FROM s1 WHERE key1 IN ( SELECT common_field FROM s2 WHERE key3 = &#x27;a&#x27; UNION SELECT common_field FROM s2 WHERE key3 = &#x27;b&#x27;); MySQL仍然会尝试优化不能转为semi-join查询的子查询，那就是： 对于不相关子查询来说，可以尝试把它们物化之后再参与查询比如我们上边提到的这个查询：先将子查询物化，然后再判断key1是否在物化表的结果集中可以加快查询执行的速度。 12SELECT * FROM s1 WHERE key1 NOT IN (SELECT common_field FROM s2 WHERE key3 = &#x27;a&#x27;) 这里将子查询物化之后不能转为和外层查询的表的连接，只能是先扫描s1表，然后对s1表的某条记录来说，判断该记录的key1值在不在物化表中。 不管子查询是相关的还是不相关的，都可以把IN子查询尝试转为EXISTS子查询其实对于任意一个IN子查询来说，都可以被转为EXISTS子查询，通用的例子如下：可以被转换为：当然这个过程中有一些特殊情况，比如在outer_expr或者inner_expr值为NULL的情况下就比较特殊。因为有NULL值作为操作数的表达式结果往往是NULL，比方说：而EXISTS子查询的结果肯定是TRUE或者FASLE：但是，我们大部分使用IN子查询的场景是把它放在WHERE或者ON子句中，而WHERE或者ON子句是不区分NULL和FALSE的，比方说：所以只要我们的IN子查询是放在WHERE或者ON子句中的，那么IN -&gt; EXISTS的转换就是没问题的。说了这么多，为啥要转换呢？这是因为不转换的话可能用不到索引，比方说下边这个查询：这个查询中的子查询是一个相关子查询，而且子查询执行的时候不能使用到索引，但是将它转为EXISTS子查询后却可以使用到索引：转为EXISTS子查询时便可以使用到s2表的idx_key3索引了。需要注意的是，如果IN子查询不满足转换为semi-join的条件，又不能转换为物化表或者转换为物化表的成本太大，那么它就会被转换为EXISTS查询。 1outer_expr IN (SELECT inner_expr FROM ... WHERE subquery_where) 1EXISTS (SELECT inner_expr FROM ... WHERE subquery_where AND outer_expr=inner_expr) 1234567891011121314151617181920212223mysql&gt; SELECT NULL IN (1, 2, 3);+-------------------+| NULL IN (1, 2, 3) |+-------------------+| NULL |+-------------------+1 row in set (0.00 sec)mysql&gt; SELECT 1 IN (1, 2, 3);+----------------+| 1 IN (1, 2, 3) |+----------------+| 1 |+----------------+1 row in set (0.00 sec)mysql&gt; SELECT NULL IN (NULL);+----------------+| NULL IN (NULL) |+----------------+| NULL |+----------------+1 row in set (0.00 sec) 1234567891011121314151617181920212223mysql&gt; SELECT EXISTS (SELECT 1 FROM s1 WHERE NULL = 1);+------------------------------------------+| EXISTS (SELECT 1 FROM s1 WHERE NULL = 1) |+------------------------------------------+| 0 |+------------------------------------------+1 row in set (0.01 sec)mysql&gt; SELECT EXISTS (SELECT 1 FROM s1 WHERE 1 = NULL);+------------------------------------------+| EXISTS (SELECT 1 FROM s1 WHERE 1 = NULL) |+------------------------------------------+| 0 |+------------------------------------------+1 row in set (0.00 sec)mysql&gt; SELECT EXISTS (SELECT 1 FROM s1 WHERE NULL = NULL);+---------------------------------------------+| EXISTS (SELECT 1 FROM s1 WHERE NULL = NULL) |+---------------------------------------------+| 0 |+---------------------------------------------+1 row in set (0.00 sec) 12345mysql&gt; SELECT 1 FROM s1 WHERE NULL;Empty set (0.00 sec)mysql&gt; SELECT 1 FROM s1 WHERE FALSE;Empty set (0.00 sec) 123SELECT * FROM s1 WHERE key1 IN (SELECT key3 FROM s2 where s1.common_field = s2.common_field) OR key2 &gt; 1000; 123SELECT * FROM s1 WHERE EXISTS (SELECT 1 FROM s2 where s1.common_field = s2.common_field AND s2.key3 = s1.key1) OR key2 &gt; 1000; 在MySQL5.5以及之前的版本没有引进semi-join和物化的方式优化子查询时，优化器都会把IN子查询转换为EXISTS子查询。 ⑥ 阶段梳理 如果IN子查询符合转换为semi-join的条件，查询优化器会优先把该子查询转换为semi-join，然后再考虑下边5种执行半连接的策略中哪个成本最低： Table pullout DuplicateWeedout LooseScan Materialization FirstMatch 选择成本最低的那种执行策略来执行子查询。 如果IN子查询不符合转换为semi-join的条件，那么查询优化器会从下边两种策略中找出一种成本更低的方式执行子查询： 先将子查询物化之后再执行查询 执行IN to EXISTS转换。 3.2.3 ANY/ALL子查询优化如果ANY/ALL子查询是不相关子查询的话，它们在很多场合都能转换成我们熟悉的方式去执行，比方说： 原始表达式 转换为 &lt; ANY (SELECT inner_expr …) &lt; (SELECT MAX(inner_expr) …) &gt; ANY (SELECT inner_expr …) &gt; (SELECT MIN(inner_expr) …) &lt; ALL (SELECT inner_expr …) &lt; (SELECT MIN(inner_expr) …) &gt; ALL (SELECT inner_expr …) &gt; (SELECT MAX(inner_expr) …) 3.2.4 [NOT] EXISTS子查询的执行如果[NOT] EXISTS子查询是不相关子查询，可以先执行子查询，得出该[NOT] EXISTS子查询的结果是TRUE还是FALSE，并重写原先的查询语句，比如对这个查询来说： 123SELECT * FROM s1 WHERE EXISTS (SELECT 1 FROM s2 WHERE key1 = &#x27;a&#x27;) OR key2 &gt; 100; 因为这个语句里的子查询是不相关子查询，所以优化器会首先执行该子查询，假设该EXISTS子查询的结果为TRUE，那么接着优化器会重写查询为： 12SELECT * FROM s1 WHERE TRUE OR key2 &gt; 100; 进一步简化后就变成了： 12SELECT * FROM s1 WHERE TRUE; 对于相关的[NOT] EXISTS子查询来说，比如这个查询： 12SELECT * FROM s1 WHERE EXISTS (SELECT 1 FROM s2 WHERE s1.common_field = s2.common_field); 这个查询只能按照普通的那种执行相关子查询的方式来执行。不过如果[NOT] EXISTS子查询中如果可以使用索引的话，那查询速度也会加快不少，比如： 12SELECT * FROM s1 WHERE EXISTS (SELECT 1 FROM s2 WHERE s1.common_field = s2.key1); 上边这个EXISTS子查询中可以使用idx_key1来加快查询速度。 3.2.5 对于派生表的优化我们前边说过把子查询放在外层查询的FROM子句后，那么这个子查询的结果相当于一个派生表，比如下边这个查询： 123SELECT * FROM ( SELECT id AS d_id, key3 AS d_key3 FROM s2 WHERE key1 = &#x27;a&#x27; ) AS derived_s1 WHERE d_key3 = &#x27;a&#x27;; 子查询( SELECT id AS d_id, key3 AS d_key3 FROM s2 WHERE key1 = &#39;a&#39;)的结果就相当于一个派生表，这个表的名称是derived_s1，该表有两个列，分别是d_id和d_key3。 对于含有派生表的查询，MySQL提供了两种执行策略： 把派生表物化我们可以将派生表的结果集写到一个内部的临时表中，然后就把这个物化表当作普通表一样参与查询。当然，在对派生表进行物化时，MySQL使用了一种称为延迟物化的策略，也就是在查询中真正使用到派生表时才会去尝试物化派生表，而不是还没开始执行查询就把派生表物化掉。比方说对于下边这个含有派生表的查询来说：如果采用物化派生表的方式来执行这个查询的话，那么执行时首先会到s2表中找出满足s2.key2 = 1的记录，如果找不到，说明参与连接的s2表记录就是空的，所以整个查询的结果集就是空的，所以也就没有必要去物化查询中的派生表了。 12345SELECT * FROM ( SELECT * FROM s1 WHERE key1 = &#x27;a&#x27; ) AS derived_s1 INNER JOIN s2 ON derived_s1.key1 = s2.key1 WHERE s2.key2 = 1; 将派生表和外层的表合并，也就是将查询重写为没有派生表的形式我们来看这个包含派生表的查询：这个查询本质上就是想查看s1表中满足key1 = &#39;a&#39;条件的的全部记录，所以和下边这个语句是等价的：对于一些稍微复杂的包含派生表的语句，比如我们上边提到的那个：我们可以将派生表与外层查询的表合并，然后将派生表中的搜索条件放到外层查询的搜索条件中，就像这样：这样通过将外层查询和派生表合并的方式成功的消除了派生表，也就意味着我们没必要再付出创建和访问临时表的成本了。可是并不是所有带有派生表的查询都能被成功的和外层查询合并，当派生表中有这些语句就不可以和外层查询合并： 1SELECT * FROM (SELECT * FROM s1 WHERE key1 = &#x27;a&#x27;) AS derived_s1; 1SELECT * FROM s1 WHERE key1 = &#x27;a&#x27;; 12345SELECT * FROM ( SELECT * FROM s1 WHERE key1 = &#x27;a&#x27; ) AS derived_s1 INNER JOIN s2 ON derived_s1.key1 = s2.key1 WHERE s2.key2 = 1; 123SELECT * FROM s1 INNER JOIN s2 ON s1.key1 = s2.key1 WHERE s1.key1 = &#x27;a&#x27; AND s2.key2 = 1; 聚集函数，比如MAX()、MIN()、SUM()… DISTINCT GROUP BY HAVING LIMIT UNION 或者 UNION ALL 派生表对应的子查询的SELECT子句中含有另一个子查询 … 还有些不常用的情况… 所以MySQL在执行带有派生表的时候，优先尝试把派生表和外层查询合并掉，如果不行的话，再把派生表物化掉执行查询。 4.总结MySQL会对用户编写的SQL语句进行重写操作，比如： 移除不必要的括号 常量传递 移除没用的条件 表达式计算 HAVING&amp;WHERE子句的合并 常量表检测 在被驱动表的WHERE子句符合空值拒绝条件的时候，外连接&amp;内连接可以相互转换。 子查询可以按照不同维度进行不同分类，比如按照子查询返回的结果集分类： 标量子查询 行子查询 列子查询 表子查询 按照与外层查询的关系来分类： 不相关子查询 相关子查询 MySQL对in查询进行了很多优化。如果in子查询符合转换为半连接的条件，查询优化器会优先把该子查询转换为半连接，然后再考虑下面五种执行半连接查询的策略中哪个成本最低，最后选择成本最低的执行策略来执行子查询。 table pullout duplicate weedout looseScan Semj-join Materialization FirstMatch 如果IN子查询不符合转换为半连接的条件，查询优化器会从下面的两种策略里面找出一种成本更低的方式去执行子查询： 先将子查询物化，在执行子查询 执行in到exists的转换 MySQL在处理带有派生表的语句的时候，优先尝试把派生表和外层查询进行合并；如果不行，再把派生表物化掉，然后执行查询。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/tags/MySQL/"}]},{"title":"MySQL[八]InnoDB统计数据收集原理","slug":"MySQL/MySQL[八]InnoDB统计数据收集原理","date":"2022-01-11T03:16:18.849Z","updated":"2022-01-11T03:22:22.716Z","comments":true,"path":"2022/01/11/MySQL/MySQL[八]InnoDB统计数据收集原理/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/MySQL/MySQL[%E5%85%AB]InnoDB%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%E5%8E%9F%E7%90%86/","excerpt":"","text":"本篇我们来分析下InnoDB存储引擎的统计数据收集策略。 1.统计数据的存储方式InnoDB提供了两种存储统计数据的方式： 永久性的统计数据这种统计数据存储在磁盘上，也就是服务器重启之后这些统计数据还在。 非永久性的统计数据这种统计数据存储在内存中，当服务器关闭时这些这些统计数据就都被清除掉了，等到服务器重启之后，在某些适当的场景下才会重新收集这些统计数据。 MySQL提供了系统变量innodb_stats_persistent来控制到底采用哪种方式去存储统计数据。在MySQL 5.6.6之前，innodb_stats_persistent的值默认是OFF，也就是说InnoDB的统计数据默认是存储到内存的，之后的版本中innodb_stats_persistent的值默认是ON，也就是统计数据默认被存储到磁盘中。 不过InnoDB默认是以表为单位来收集和存储统计数据的，也就是说我们可以把某些表的统计数据（以及该表的索引统计数据）存储在磁盘上，把另一些表的统计数据存储在内存中。 我们可以在创建和修改表的时候通过指定STATS_PERSISTENT属性来指明该表的统计数据存储方式： 123CREATE TABLE 表名 (...) Engine=InnoDB, STATS_PERSISTENT = (1|0);ALTER TABLE 表名 Engine=InnoDB, STATS_PERSISTENT = (1|0); 当STATS_PERSISTENT=1时，表明我们想把该表的统计数据永久的存储到磁盘上，当STATS_PERSISTENT=0时，表明我们想把该表的统计数据临时的存储到内存中。如果我们在创建表时未指定STATS_PERSISTENT属性，那默认采用系统变量innodb_stats_persistent的值作为该属性的值。 2.永久性统计数据当我们选择把某个表以及该表索引的统计数据存放到磁盘上时，实际上是把这些统计数据存储到了两个表里： 12345678mysql&gt; SHOW TABLES FROM mysql LIKE &#x27;innodb%&#x27;;+---------------------------+| Tables_in_mysql (innodb%) |+---------------------------+| innodb_index_stats || innodb_table_stats |+---------------------------+2 rows in set (0.01 sec) 可以看到，这两个表都位于mysql系统数据库下边，其中： innodb_table_stats存储了关于表的统计数据，每一条记录对应着一个表的统计数据。 innodb_index_stats存储了关于索引的统计数据，每一条记录对应着一个索引的一个统计项的统计数据。 看一下这两个表里边都有什么以及表里的数据是如何生成的。 2.1 innodb_table_stats直接看一下这个innodb_table_stats表中的各个列都是干嘛的： 字段名 描述 database_name 数据库名 table_name 表名 last_update 本条记录最后更新时间 n_rows 表中记录的条数 clustered_index_size 表的聚簇索引占用的页面数量 sum_of_other_index_sizes 表的其他索引占用的页面数量 注意这个表的主键是(database_name,table_name)，也就是innodb_table_stats表的每条记录代表着一个表的统计信息。 12345678910mysql&gt; SELECT * FROM mysql.innodb_table_stats;+---------------+---------------+---------------------+--------+----------------------+--------------------------+| database_name | table_name | last_update | n_rows | clustered_index_size | sum_of_other_index_sizes |+---------------+---------------+---------------------+--------+----------------------+--------------------------+| mysql | gtid_executed | 2021-12-14 00:00:08 | 0 | 1 | 0 || sys | sys_config | 2021-12-14 00:00:09 | 6 | 1 | 0 || yhd | person_info | 2021-12-19 00:27:39 | 0 | 1 | 1 || yhd | single_table | 2021-12-19 02:13:19 | 910545 | 3109 | 5836 |+---------------+---------------+---------------------+--------+----------------------+--------------------------+4 rows in set (0.00 sec) 可以看到single_table表的统计信息就对应着mysql.innodb_table_stats的第三条记录。几个重要统计信息项的值如下： n_rows的值是9693，表明single_table表中大约有9693条记录，注意这个数据是估计值。 clustered_index_size的值是97，表明single_table表的聚簇索引占用97个页面，这个值是也是一个估计值。 sum_of_other_index_sizes的值是175，表明single_table表的其他索引一共占用175个页面，这个值是也是一个估计值。 2.1.1 n_rows统计项的收集为啥n_rows这个统计项的值是估计值呢？InnoDB`统计一个表中有多少行记录的套路是这样的： 按照一定算法（并不是纯粹随机的）选取几个叶子节点页面，计算每个页面中主键值记录数量，然后计算平均一个页面中主键值的记录数量乘以全部叶子节点的数量就算是该表的n_rows值。 真实的计算过程比这个稍微复杂一些，不过大致上就是这样。 可以看出来这个n_rows值精确与否取决于统计时采样的页面数量，MySQL为我们准备了一个名为innodb_stats_persistent_sample_pages的系统变量来控制使用永久性的统计数据时，计算统计数据时采样的页面数量。该值设置的越大，统计出的n_rows值越精确，但是统计耗时也就最久；该值设置的越小，统计出的n_rows值越不精确，但是统计耗时特别少。所以在实际使用是需要我们去权衡利弊，该系统变量的默认值是20。 我们前边说过，不过InnoDB默认是以表为单位来收集和存储统计数据的，我们也可以单独设置某个表的采样页面的数量，设置方式就是在创建或修改表的时候通过指定STATS_SAMPLE_PAGES属性来指明该表的统计数据存储方式： 123CREATE TABLE 表名 (...) Engine=InnoDB, STATS_SAMPLE_PAGES = 具体的采样页面数量;ALTER TABLE 表名 Engine=InnoDB, STATS_SAMPLE_PAGES = 具体的采样页面数量; 如果我们在创建表的语句中并没有指定STATS_SAMPLE_PAGES属性的话，将默认使用系统变量innodb_stats_persistent_sample_pages的值作为该属性的值。 2.1.2 clustered_index_size和sum_of_other_index_sizes统计项的收集这两个统计项的收集过程如下： 从数据字典里找到表的各个索引对应的根页面位置。系统表SYS_INDEXES里存储了各个索引对应的根页面信息。 从根页面的Page Header里找到叶子节点段和非叶子节点段对应的Segment Header。在每个索引的根页面的Page Header部分都有两个字段： PAGE_BTR_SEG_LEAF：表示B+树叶子段的Segment Header信息。 PAGE_BTR_SEG_TOP：表示B+树非叶子段的Segment Header信息。 从叶子节点段和非叶子节点段的Segment Header中找到这两个段对应的INODE Entry结构。这个是Segment Header结构： 从对应的INODE Entry结构中可以找到该段对应所有零散的页面地址以及FREE、NOT_FULL、FULL链表的基节点。这个是INODE Entry结构： 直接统计零散的页面有多少个，然后从那三个链表的List Length字段中读出该段占用的区的大小，每个区占用64个页，所以就可以统计出整个段占用的页面。这个是链表基节点的示意图： 分别计算聚簇索引的叶子结点段和非叶子节点段占用的页面数，它们的和就是clustered_index_size的值，按照同样的套路把其余索引占用的页面数都算出来，加起来之后就是sum_of_other_index_sizes的值。 注意，我们说一个段的数据在非常多时（超过32个页面），会以区为单位来申请空间，这里头的问题是以区为单位申请空间中有一些页可能并没有使用，但是在统计clustered_index_size和sum_of_other_index_sizes时都把它们算进去了，所以说聚簇索引和其他的索引占用的页面数可能比这两个值要小一些。 2.2 innodb_index_stats直接看一下这个innodb_index_stats表中的各个列： 字段名 描述 database_name 数据库名 table_name 表名 index_name 索引名 last_update 本条记录最后更新时间 stat_name 统计项的名称 stat_value 对应的统计项的值 sample_size 为生成统计数据而采样的页面数量 stat_description 对应的统计项的描述 注意这个表的主键是(database_name,table_name,index_name,stat_name)，其中的stat_name是指统计项的名称，也就是说innodb_index_stats表的每条记录代表着一个索引的一个统计项。我们直接看一下关于single_table表的索引统计数据都有些什么： 1234567891011121314151617181920212223242526mysql&gt; SELECT * FROM mysql.innodb_index_stats WHERE table_name = &#x27;single_table&#x27;;+---------------+--------------+--------------+---------------------+--------------+------------+-------------+-----------------------------------+| database_name | table_name | index_name | last_update | stat_name | stat_value | sample_size | stat_description |+---------------+--------------+--------------+---------------------+--------------+------------+-------------+-----------------------------------+| yhd | single_table | PRIMARY | 2021-12-19 02:13:19 | n_diff_pfx01 | 910518 | 20 | id || yhd | single_table | PRIMARY | 2021-12-19 02:13:19 | n_leaf_pages | 3097 | NULL | Number of leaf pages in the index || yhd | single_table | PRIMARY | 2021-12-19 02:13:19 | size | 3109 | NULL | Number of pages in the index || yhd | single_table | idx_key1 | 2021-12-19 02:13:19 | n_diff_pfx01 | 8 | 10 | key1 || yhd | single_table | idx_key1 | 2021-12-19 02:13:19 | n_diff_pfx02 | 882192 | 20 | key1,id || yhd | single_table | idx_key1 | 2021-12-19 02:13:19 | n_leaf_pages | 828 | NULL | Number of leaf pages in the index || yhd | single_table | idx_key1 | 2021-12-19 02:13:19 | size | 993 | NULL | Number of pages in the index || yhd | single_table | idx_key3 | 2021-12-19 02:13:19 | n_diff_pfx01 | 8 | 10 | key3 || yhd | single_table | idx_key3 | 2021-12-19 02:13:19 | n_diff_pfx02 | 910072 | 20 | key3,id || yhd | single_table | idx_key3 | 2021-12-19 02:13:19 | n_leaf_pages | 827 | NULL | Number of leaf pages in the index || yhd | single_table | idx_key3 | 2021-12-19 02:13:19 | size | 993 | NULL | Number of pages in the index || yhd | single_table | idx_key_part | 2021-12-19 02:13:19 | n_diff_pfx01 | 8 | 10 | key_part1 || yhd | single_table | idx_key_part | 2021-12-19 02:13:19 | n_diff_pfx02 | 82 | 20 | key_part1,key_part2 || yhd | single_table | idx_key_part | 2021-12-19 02:13:19 | n_diff_pfx03 | 730 | 20 | key_part1,key_part2,key_part3 || yhd | single_table | idx_key_part | 2021-12-19 02:13:19 | n_diff_pfx04 | 1028534 | 20 | key_part1,key_part2,key_part3,id || yhd | single_table | idx_key_part | 2021-12-19 02:13:19 | n_leaf_pages | 2556 | NULL | Number of leaf pages in the index || yhd | single_table | idx_key_part | 2021-12-19 02:13:19 | size | 2985 | NULL | Number of pages in the index || yhd | single_table | uk_key2 | 2021-12-19 02:13:19 | n_diff_pfx01 | 913104 | 20 | key2 || yhd | single_table | uk_key2 | 2021-12-19 02:13:19 | n_leaf_pages | 816 | NULL | Number of leaf pages in the index || yhd | single_table | uk_key2 | 2021-12-19 02:13:19 | size | 865 | NULL | Number of pages in the index |+---------------+--------------+--------------+---------------------+--------------+------------+-------------+-----------------------------------+20 rows in set (0.01 sec) 这个结果有点儿多，正确查看这个结果的方式是这样的： 先查看index_name列，这个列说明该记录是哪个索引的统计信息，从结果中我们可以看出来，PRIMARY索引（也就是主键）占了3条记录，idx_key_part索引占了6条记录。 针对index_name列相同的记录，stat_name表示针对该索引的统计项名称，stat_value展示的是该索引在该统计项上的值，stat_description指的是来描述该统计项的含义的。我们来具体看一下一个索引都有哪些统计项： n_leaf_pages：表示该索引的叶子节点占用多少页面。 size：表示该索引共占用多少页面。 n_diff_pfx**NN**：表示对应的索引列不重复的值有多少。其实NN可以被替换为01、02、03… 这样的数字。比如对于idx_key_part来说： 1. n_diff_pfx01表示的是统计key_part1这单单一个列不重复的值有多少。 1. n_diff_pfx02表示的是统计key_part1、key_part2这两个列组合起来不重复的值有多少。 1. n_diff_pfx03表示的是统计key_part1、key_part2、key_part3这三个列组合起来不重复的值有多少。 1. n_diff_pfx04表示的是统计key_part1、key_part2、key_part3、id这四个列组合起来不重复的值有多少。 注意：对于普通的二级索引，并不能保证它的索引列值是唯一的，比如对于idx_key1来说，key1列就可能有很多值重复的记录。此时只有在索引列上加上主键值才可以区分两条索引列值都一样的二级索引记录。对于主键和唯一二级索引则没有这个问题，它们本身就可以保证索引列值的不重复，所以也不需要再统计一遍在索引列后加上主键值的不重复值有多少。比如上边的idx_key1有n_diff_pfx01、n_diff_pfx02两个统计项，而idx_key2却只有n_diff_pfx01一个统计项。 在计算某些索引列中包含多少不重复值时，需要对一些叶子节点页面进行采样，sample_size列就表明了采样的页面数量是多少。 注意：对于有多个列的联合索引来说，采样的页面数量是：innodb_stats_persistent_sample_pages × 索引列的个数。当需要采样的页面数量大于该索引的叶子节点数量的话，就直接采用全表扫描来统计索引列的不重复值数量了。所以在查询结果中看到不同索引对应的size列的值可能是不同的。 2.3定期更新统计数据随着我们不断的对表进行增删改操作，表中的数据也一直在变化，innodb_table_stats和innodb_index_stats表里的统计数据是要变的，不变的话MySQL查询优化器计算的成本就会有问题。MySQL提供了如下两种更新统计数据的方式： 开启innodb_stats_auto_recalc。系统变量innodb_stats_auto_recalc决定着服务器是否自动重新计算统计数据，它的默认值是ON，也就是该功能默认是开启的。每个表都维护了一个变量，该变量记录着对该表进行增删改的记录条数，如果发生变动的记录数量超过了表大小的10%，并且自动重新计算统计数据的功能是打开的，那么服务器会重新进行一次统计数据的计算，并且更新innodb_table_stats和innodb_index_stats表。不过自动重新计算统计数据的过程是异步发生的，也就是即使表中变动的记录数超过了10%，自动重新计算统计数据也不会立即发生，可能会延迟几秒才会进行计算。InnoDB默认是以表为单位来收集和存储统计数据的，我们也可以单独为某个表设置是否自动重新计算统计数的属性，设置方式就是在创建或修改表的时候通过指定STATS_AUTO_RECALC属性来指明该表的统计数据存储方式：当STATS_AUTO_RECALC=1时，表明我们想让该表自动重新计算统计数据，当STATS_AUTO_RECALC=0时，表明不想让该表自动重新计算统计数据。如果我们在创建表时未指定STATS_AUTO_RECALC属性，那默认采用系统变量innodb_stats_auto_recalc的值作为该属性的值。 123CREATE TABLE 表名 (...) Engine=InnoDB, STATS_AUTO_RECALC = (1|0);ALTER TABLE 表名 Engine=InnoDB, STATS_AUTO_RECALC = (1|0); 手动调用ANALYZE TABLE语句来更新统计信息如果innodb_stats_auto_recalc系统变量的值为OFF的话，我们也可以手动调用ANALYZE TABLE语句来重新计算统计数据，比如我们可以这样更新关于single_table表的统计数据：需要注意的是，ANALYZE TABLE语句会立即重新计算统计数据，也就是这个过程是同步的，在表中索引多或者采样页面特别多时这个过程可能会特别慢，请不要没事儿就运行一下ANALYZE TABLE语句，最好在业务不是很繁忙的时候再运行。 1234567mysql&gt; ANALYZE TABLE single_table;+------------------------+---------+----------+----------+| Table | Op | Msg_type | Msg_text |+------------------------+---------+----------+----------+| yhd.single_table | analyze | status | OK |+------------------------+---------+----------+----------+1 row in set (0.08 sec) 2.4手动更新统计数据其实innodb_table_stats和innodb_index_stats表就相当于一个普通的表一样，我们能对它们做增删改查操作。这也就意味着我们可以手动更新某个表或者索引的统计数据。比如说我们想把single_table表关于行数的统计数据更改一下可以这么做： 步骤一：更新innodb_table_stats表。 123UPDATE innodb_table_stats SET n_rows = 1 WHERE table_name = &#x27;single_table&#x27;; 步骤二：让MySQL查询优化器重新加载我们更改过的数据。更新完innodb_table_stats只是单纯的修改了一个表的数据，需要让MySQL查询优化器重新加载我们更改过的数据，运行下边的命令就可以了： 1FLUSH TABLE single_table; 之后我们使用SHOW TABLE STATUS语句查看表的统计数据时就看到Rows行变为了1。 3.非永久性统计数据当我们把系统变量innodb_stats_persistent的值设置为OFF时，之后创建的表的统计数据默认就都是非永久性的了，或者我们直接在创建表或修改表时设置STATS_PERSISTENT属性的值为0，那么该表的统计数据就是非永久性的了。 与永久性的统计数据不同，非永久性的统计数据采样的页面数量是由innodb_stats_transient_sample_pages控制的，这个系统变量的默认值是8。 另外，由于非永久性的统计数据经常更新，所以导致MySQL查询优化器计算查询成本的时候依赖的是经常变化的统计数据，也就会生成经常变化的执行计划。 4.innodb_stats_method索引列不重复的值的数量这个统计数据对于MySQL查询优化器十分重要，因为通过它可以计算出在索引列中平均一个值重复多少行，它的应用场景主要有两个： 单表查询中单点区间太多，比方说这样：当IN里的参数数量过多时，采用index dive的方式直接访问B+树索引去统计每个单点区间对应的记录的数量就太耗费性能了，所以直接依赖统计数据中的平均一个值重复多少行来计算单点区间对应的记录数量。 1SELECT * FROM tbl_name WHERE key IN (&#x27;xx1&#x27;, &#x27;xx2&#x27;, ..., &#x27;xxn&#x27;); 连接查询时，如果有涉及两个表的等值匹配连接条件，该连接条件对应的被驱动表中的列又拥有索引时，则可以使用ref访问方法来对被驱动表进行查询，比方说这样：在真正执行对t2表的查询前，t1.comumn的值是不确定的，所以我们也不能通过index dive的方式直接访问B+树索引去统计每个单点区间对应的记录的数量，所以也只能依赖统计数据中的平均一个值重复多少行来计算单点区间对应的记录数量。 1SELECT * FROM t1 JOIN t2 ON t1.column = t2.key WHERE ...; 在统计索引列不重复的值的数量时，有一个比较烦的问题就是索引列中出现NULL值怎么办，比方说某个索引列的内容是这样： 12345678+------+| col |+------+| 1 || 2 || NULL || NULL |+------+ 此时计算这个col列中不重复的值的数量就有下边的分歧： 有的人认为NULL值代表一个未确定的值，所以MySQL认为任何和NULL值做比较的表达式的值都为NULL，就是这样：所以每一个NULL值都是独一无二的，也就是说统计索引列不重复的值的数量时，应该把NULL值当作一个独立的值，所以col列的不重复的值的数量就是：4（分别是1、2、NULL、NULL这四个值）。 12345678910111213141516171819202122232425262728293031mysql&gt; SELECT 1 = NULL;+----------+| 1 = NULL |+----------+| NULL |+----------+1 row in set (0.00 sec)mysql&gt; SELECT 1 != NULL;+-----------+| 1 != NULL |+-----------+| NULL |+-----------+1 row in set (0.00 sec)mysql&gt; SELECT NULL = NULL;+-------------+| NULL = NULL |+-------------+| NULL |+-------------+1 row in set (0.00 sec)mysql&gt; SELECT NULL != NULL;+--------------+| NULL != NULL |+--------------+| NULL |+--------------+1 row in set (0.00 sec) 有的人认为其实NULL值在业务上就是代表没有，所有的NULL值代表的意义是一样的，所以col列不重复的值的数量就是：3（分别是1、2、NULL这三个值）。 有的人认为这NULL完全没有意义嘛，所以在统计索引列不重复的值的数量时压根儿不能把它们算进来，所以col列不重复的值的数量就是：2（分别是1、2这两个值）。 MySQL提供了一个名为innodb_stats_method的系统变量，我们可以自己来设置，这个系统变量有三个候选值： nulls_equal：认为所有NULL值都是相等的。这个值也是innodb_stats_method的默认值。如果某个索引列中NULL值特别多的话，这种统计方式会让优化器认为某个列中平均一个值重复次数特别多，所以倾向于不使用索引进行访问。 nulls_unequal：认为所有NULL值都是不相等的。如果某个索引列中NULL值特别多的话，这种统计方式会让优化器认为某个列中平均一个值重复次数特别少，所以倾向于使用索引进行访问。 nulls_ignored：直接把NULL值忽略掉。 5.总结 InnoDB以表为单位来收集统计数据，这些统计数据可以是基于磁盘的永久性统计数据，也可以是基于内存的非永久性统计数据。 innodb_stats_persistent控制着使用永久性统计数据还是非永久性统计数据；innodb_stats_persistent_sample_pages控制着永久性统计数据的采样页面数量；innodb_stats_transient_sample_pages控制着非永久性统计数据的采样页面数量；innodb_stats_auto_recalc控制着是否自动重新计算统计数据。 我们可以针对某个具体的表，在创建和修改表时通过指定STATS_PERSISTENT、STATS_AUTO_RECALC、STATS_SAMPLE_PAGES的值来控制相关统计数据属性。 innodb_stats_method决定着在统计某个索引列不重复值的数量时如何对待NULL值。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/tags/MySQL/"}]},{"title":"MySQL[七]基于成本的优化","slug":"MySQL/MySQL[七]基于成本的优化","date":"2022-01-11T03:16:05.985Z","updated":"2022-01-11T03:21:45.620Z","comments":true,"path":"2022/01/11/MySQL/MySQL[七]基于成本的优化/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/MySQL/MySQL[%E4%B8%83]%E5%9F%BA%E4%BA%8E%E6%88%90%E6%9C%AC%E7%9A%84%E4%BC%98%E5%8C%96/","excerpt":"","text":"1.什么是成本？在MySQL中一条查询语句的执行成本是由下边这两个方面组成的： I/O成本我们的表经常使用的MyISAM、InnoDB存储引擎都是将数据和索引都存储到磁盘上的，当我们想查询表中的记录时，需要先把数据或者索引加载到内存中然后再操作。这个从磁盘到内存这个加载的过程损耗的时间称之为I/O成本。 CPU成本读取以及检测记录是否满足对应的搜索条件、对结果集进行排序等这些操作损耗的时间称之为CPU成本。 对于InnoDB存储引擎来说，页是磁盘和内存之间交互的基本单位，MySQL规定读取一个页面花费的成本默认是1.0，读取以及检测一条记录是否符合搜索条件的成本默认是0.2。1.0、0.2这些数字称之为成本常数。 不管读取记录时需不需要检测是否满足搜索条件，其成本都算是0.2。 2.单表查询的成本2.1 准备工作把之前用到的single_table表搬来。 123456789101112131415CREATE TABLE single_table ( id INT NOT NULL AUTO_INCREMENT, key1 VARCHAR(100), key2 INT, key3 VARCHAR(100), key_part1 VARCHAR(100), key_part2 VARCHAR(100), key_part3 VARCHAR(100), common_field VARCHAR(100), PRIMARY KEY (id), KEY idx_key1 (key1), UNIQUE KEY idx_key2 (key2), KEY idx_key3 (key3), KEY idx_key_part(key_part1, key_part2, key_part3)) Engine=InnoDB CHARSET=utf8; 2.2 基于成本的优化步骤在一条单表查询语句真正执行之前，MySQL的查询优化器会找出执行该语句所有可能使用的方案，对比之后找出成本最低的方案，这个成本最低的方案就是所谓的执行计划，之后才会调用存储引擎提供的接口真正的执行查询，这个过程总结一下就是这样： 根据搜索条件，找出所有可能使用的索引 计算全表扫描的代价 计算使用不同索引执行查询的代价 对比各种执行方案的代价，找出成本最低的那一个 下边我们就以一个实例来分析一下这些步骤，单表查询语句如下： 123456SELECT * FROM single_table WHERE key1 IN (&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;) AND key2 &gt; 10 AND key2 &lt; 1000 AND key3 &gt; key2 AND key_part1 LIKE &#x27;%hello%&#x27; AND common_field = &#x27;123&#x27;; 2.2.1 根据搜索条件，找出所有可能使用的索引对于B+树索引来说，只要索引列和常数使用=、&lt;=&gt;、IN、NOT IN、IS NULL、IS NOT NULL、&gt;、&lt;、&gt;=、&lt;=、BETWEEN、!=（不等于也可以写成&lt;&gt;）或者LIKE操作符连接起来，就可以产生一个所谓的范围区间（LIKE匹配字符串前缀也行），也就是说这些搜索条件都可能使用到索引，MySQL把一个查询中可能使用到的索引称之为possible keys。 我们分析一下上边查询中涉及到的几个搜索条件： key1 IN (&#39;a&#39;, &#39;b&#39;, &#39;c&#39;)，这个搜索条件可以使用二级索引idx_key1。 key2 &gt; 10 AND key2 &lt; 1000，这个搜索条件可以使用二级索引idx_key2。 key3 &gt; key2，这个搜索条件的索引列由于没有和常数比较，所以并不能使用到索引。 key_part1 LIKE &#39;%hello%&#39;，key_part1通过LIKE操作符和以通配符开头的字符串做比较，不可以适用索引。 common_field = &#39;123&#39;，由于该列上压根儿没有索引，所以不会用到索引。 综上所述，上边的查询语句可能用到的索引，也就是possible keys只有idx_key1和idx_key2。 2.2.2 计算全表扫描的代价对于InnoDB存储引擎来说，全表扫描的意思就是把聚簇索引中的记录都依次和给定的搜索条件做一下比较，把符合搜索条件的记录加入到结果集，所以需要将聚簇索引对应的页面加载到内存中，然后再检测记录是否符合搜索条件。由于查询成本=I/O成本+CPU成本，所以计算全表扫描的代价需要两个信息： 聚簇索引占用的页面数 该表中的记录数 这两个信息从哪来呢？MySQL为每个表维护了一系列的统计信息，关于这些统计信息是如何收集起来的后面再谈，现在看看怎么查看这些统计信息。MySQL给我们提供了SHOW TABLE STATUS语句来查看表的统计信息，如果要看指定的某个表的统计信息，在该语句后加对应的LIKE语句就好了，比方说我们要查看single_table这个表的统计信息可以这么写： 1SHOW TABLE STATUS LIKE &#x27;single_table&#x27; 虽然出现了很多统计选项，但我们目前只关心两个： Rows本选项表示表中的记录条数。对于使用MyISAM存储引擎的表来说，该值是准确的，对于使用InnoDB存储引擎的表来说，该值是一个估计值。从查询结果我们也可以看出来，由于我们的single_table表是使用InnoDB存储引擎的，所以虽然实际上表中有10000条记录，但是SHOW TABLE STATUS显示的Rows值只有9693条记录。 Data_length本选项表示表占用的存储空间字节数。使用MyISAM存储引擎的表来说，该值就是数据文件的大小，对于使用InnoDB存储引擎的表来说，该值就相当于聚簇索引占用的存储空间大小，也就是说可以这样计算该值的大小：我们的single_table使用默认16KB的页面大小，而上边查询结果显示Data_length的值是1589248，所以我们可以反向来推导出聚簇索引的页面数量： 1Data_length = 聚簇索引的页面数量 x 每个页面的大小 1聚簇索引的页面数量 = 1589248 ÷ 16 ÷ 1024 = 97 我们现在已经得到了聚簇索引占用的页面数量以及该表记录数的估计值，所以就可以计算全表扫描成本了，但是MySQL在真实计算成本时会进行一些微调，这些微调的值是直接硬编码到代码里的，这些微调的值十分的小，并不影响我们分析。现在可以看一下全表扫描成本的计算过程： I/O成本97指的是聚簇索引占用的页面数，1.0指的是加载一个页面的成本常数，后边的1.1是一个微调值，我们不用在意。 197 x 1.0 + 1.1 = 98.1 CPU成本：9693指的是统计数据中表的记录数，对于InnoDB存储引擎来说是一个估计值，0.2指的是访问一条记录所需的成本常数，后边的1.0是一个微调值，我们不用在意。 19693 x 0.2 + 1.0 = 1939.6 总成本： 198.1 + 1939.6 = 2037.7 综上所述，对于single_table的全表扫描所需的总成本就是2037.7。 表中的记录其实都存储在聚簇索引对应B+树的叶子节点中，所以只要我们通过根节点获得了最左边的叶子节点，就可以沿着叶子节点组成的双向链表把所有记录都查看一遍。也就是说全表扫描这个过程其实有的B+树内节点是不需要访问的，但是MySQL在计算全表扫描成本时直接使用聚簇索引占用的页面数作为计算I/O成本的依据，是不区分内节点和叶子节点的。 2.2.3 计算使用不同索引执行查询的代价从第1步分析我们得到，上述查询可能使用到idx_key1和idx_key2这两个索引，我们需要分别分析单独使用这些索引执行查询的成本，最后还要分析是否可能使用到索引合并。这里需要提一点的是，MySQL查询优化器先分析使用唯一二级索引的成本，再分析使用普通索引的成本，所以我们也先分析idx_key2的成本，然后再看使用idx_key1的成本。 ①使用idx_key2执行查询的成本分析idx_key2对应的搜索条件是：key2 &gt; 10 AND key2 &lt; 1000，也就是说对应的范围区间就是：(10, 1000)，使用idx_key2搜索的示意图就是这样子： 对于使用二级索引 + 回表方式的查询，MySQL计算这种查询的成本依赖两个方面的数据： 范围区间数量不论某个范围区间的二级索引到底占用了多少页面，查询优化器粗暴的认为读取索引的一个范围区间的I/O成本和读取一个页面是相同的。本例中使用idx_key2的范围区间只有一个：(10, 1000)，所以相当于访问这个范围区间的二级索引付出的I/O成本就是： 11 x 1.0 = 1.0 需要回表的记录数优化器需要计算二级索引的某个范围区间到底包含多少条记录，对于本例来说就是要计算idx_key2在(10, 1000)这个范围区间中包含多少二级索引记录，计算过程是这样的： 步骤1：先根据key2 &gt; 10这个条件访问一下idx_key2对应的B+树索引，找到满足key2 &gt; 10这个条件的第一条记录，我们把这条记录称之为区间最左记录。在B+数树中定位一条记录的过程是贼快的，是常数级别的，所以这个过程的性能消耗是可以忽略不计的。 步骤2：然后再根据key2 &lt; 1000这个条件继续从idx_key2对应的B+树索引中找出最后一条满足这个条件的记录，我们把这条记录称之为区间最右记录，这个过程的性能消耗也可以忽略不计的。 步骤3：如果区间最左记录和区间最右记录相隔不太远（在MySQL 5.7.21这个版本里，只要相隔不大于10个页面即可），那就可以精确统计出满足key2 &gt; 10 AND key2 &lt; 1000条件的二级索引记录条数。否则只沿着区间最左记录向右读10个页面，计算平均每个页面中包含多少记录，然后用这个平均值乘以区间最左记录和区间最右记录之间的页面数量就可以了。那么问题又来了，怎么估计区间最左记录和区间最右记录之间有多少个页面呢？解决这个问题还得回到B+树索引的结构中来： 如图，我们假设区间最左记录在页b中，区间最右记录在页c中，那么我们想计算区间最左记录和区间最右记录之间的页面数量就相当于计算页b和页c之间有多少页面，而每一条目录项记录都对应一个数据页，所以计算页b和页c之间有多少页面就相当于计算它们父节点（也就是页a）中对应的目录项记录之间隔着几条记录。在一个页面中统计两条记录之间有几条记录的成本就贼小了。如果页b和页c之间的页面实在太多，以至于页b和页c对应的目录项记录都不在一个页面中，继续递归，也就是再统计页b和页c对应的目录项记录所在页之间有多少个页面。过一个B+树有4层高已经很了不得了，所以这个统计过程也不是很耗费性能。知道了如何统计二级索引某个范围区间的记录数之后，就需要回到现实问题中来，根据上述算法测得idx_key2在区间(10, 1000)之间大约有95条记录。读取这95条二级索引记录需要付出的CPU成本就是： 195 x 0.2 + 0.01 = 19.01 其中95是需要读取的二级索引记录条数，0.2是读取一条记录成本常数，0.01是微调。在通过二级索引获取到记录之后，还需要干两件事儿： 根据这些记录里的主键值到聚簇索引中做回表操作MySQL认为每次回表操作都相当于访问一个页面，也就是说二级索引范围区间有多少记录，就需要进行多少次回表操作，也就是需要进行多少次页面I/O。我们上边统计了使用idx_key2二级索引执行查询时，预计有95条二级索引记录需要进行回表操作，所以回表操作带来的I/O成本就是：其中95是预计的二级索引记录数，1.0是一个页面的I/O成本常数。 195 x 1.0 = 95.0 回表操作后得到的完整用户记录，然后再检测其他搜索条件是否成立回表操作的本质就是通过二级索引记录的主键值到聚簇索引中找到完整的用户记录，然后再检测除key2 &gt; 10 AND key2 &lt; 1000这个搜索条件以外的搜索条件是否成立。因为我们通过范围区间获取到二级索引记录共95条，也就对应着聚簇索引中95条完整的用户记录，读取并检测这些完整的用户记录是否符合其余的搜索条件的CPU成本如下：MySQL只计算这个查找过程所需的I/O成本，也就是我们上一步骤中得到的95.0，在内存中的定位完整用户记录的过程的成本是忽略不计的。在定位到这些完整的用户记录后，需要检测除key2 &gt; 10 AND key2 &lt; 1000这个搜索条件以外的搜索条件是否成立，这个比较过程花费的CPU成本就是：其中95是待检测记录的条数，0.2是检测一条记录是否符合给定的搜索条件的成本常数。 195 x 0.2 = 19.0 所以本例中使用idx_key2执行查询的成本就如下所示： I/O成本： 11.0 + 95 x 1.0 = 96.0 (范围区间的数量 + 预估的二级索引记录条数) CPU成本： 195 x 0.2 + 0.01 + 95 x 0.2 = 38.01 （读取二级索引记录的成本 + 读取并检测回表后聚簇索引记录的成本） 综上所述，使用idx_key2执行查询的总成本就是： 196.0 + 38.01 = 134.01 ②使用idx_key1执行查询的成本分析idx_key1对应的搜索条件是：key1 IN (&#39;a&#39;, &#39;b&#39;, &#39;c&#39;)，也就是说相当于3个单点区间： [&#39;a&#39;, &#39;a&#39;] [&#39;b&#39;, &#39;b&#39;] [&#39;c&#39;, &#39;c&#39;] 使用idx_key1搜索的示意图就是这样子： 与使用idx_key2的情况类似，我们也需要计算使用idx_key1时需要访问的范围区间数量以及需要回表的记录数： 范围区间数量使用idx_key1执行查询时很显然有3个单点区间，所以访问这3个范围区间的二级索引付出的I/O成本就是： 13 x 1.0 = 3.0 需要回表的记录数由于使用idx_key1时有3个单点区间，所以每个单点区间都需要查找一遍对应的二级索引记录数： 查找单点区间[&#39;a&#39;, &#39;a&#39;]对应的二级索引记录数计算单点区间对应的二级索引记录数和计算连续范围区间对应的二级索引记录数是一样的，都是先计算区间最左记录和区间最右记录，然后再计算它们之间的记录数，最后计算得到单点区间[&#39;a&#39;, &#39;a&#39;]对应的二级索引记录数是：35。 查找单点区间[&#39;b&#39;, &#39;b&#39;]对应的二级索引记录数与上同理，计算得到本单点区间对应的记录数是：44。 查找单点区间[&#39;c&#39;, &#39;c&#39;]对应的二级索引记录数与上同理，计算得到本单点区间对应的记录数是：39。 所以，这三个单点区间总共需要回表的记录数就是： 135 + 44 + 39 = 118 读取这些二级索引记录的CPU成本就是： 1118 x 0.2 + 0.01 = 23.61 得到总共需要回表的记录数之后，就要考虑： 根据这些记录里的主键值到聚簇索引中做回表操作所需的I/O成本就是： 1118 x 1.0 = 118.0 回表操作后得到的完整用户记录，然后再比较其他搜索条件是否成立此步骤对应的CPU成本就是： 1118 x 0.2 = 23.6 所以本例中使用idx_key1执行查询的成本就如下所示： I/O成本： 13.0 + 118 x 1.0 = 121.0 (范围区间的数量 + 预估的二级索引记录条数) CPU成本： 1118 x 0.2 + 0.01 + 118 x 0.2 = 47.21 （读取二级索引记录的成本 + 读取并检测回表后聚簇索引记录的成本） 综上所述，使用idx_key1执行查询的总成本就是： 1121.0 + 47.21 = 168.21 ③是否有可能使用索引合并（Index Merge）本例中有关key1和key2的搜索条件是使用AND连接起来的，而对于idx_key1和idx_key2都是范围查询，也就是说查找到的二级索引记录并不是按照主键值进行排序的，并不满足使用Intersection索引合并的条件，所以并不会使用索引合并。 2.2.4 对比各种执行方案的代价，找出成本最低的那一个下边把执行本例中的查询的各种可执行方案以及它们对应的成本列出来： 全表扫描的成本：2037.7 使用idx_key2的成本：134.01 使用idx_key1的成本：168.21 很显然，使用idx_key2的成本最低，所以选择idx_key2来执行查询。 2.3 基于索引统计数据的成本计算有时候使用索引执行查询时会有许多单点区间，比如使用IN语句就很容易产生非常多的单点区间，比如下边这个查询（下边查询语句中的...表示还有很多参数）： 1SELECT * FROM single_table WHERE key1 IN (&#x27;aa1&#x27;, &#x27;aa2&#x27;, &#x27;aa3&#x27;, ... , &#x27;zzz&#x27;); 很显然，这个查询可能使用到的索引就是idx_key1，由于这个索引并不是唯一二级索引，所以并不能确定一个单点区间对应的二级索引记录的条数有多少，需要我们去计算。计算方式就是先获取索引对应的B+树的区间最左记录和区间最右记录，然后再计算这两条记录之间有多少记录（记录条数少的时候可以做到精确计算，多的时候只能估算）。MySQL把这种通过直接访问索引对应的B+树来计算某个范围区间对应的索引记录条数的方式称之为index dive。 index dive就是直接利用索引对应的B+树来计算某个范围区间对应的记录条数。 有几个单点区间的话，使用index dive的方式去计算这些单点区间对应的记录数也不是什么问题，可是如果很多的话，这就意味着MySQL的查询优化器为了计算这些单点区间对应的索引记录条数，要进行20000次index dive操作，这性能损耗可就大了，搞不好计算这些单点区间对应的索引记录条数的成本比直接全表扫描的成本都大了。MySQL提供了一个系统变量eq_range_index_dive_limit，我们看一下在MySQL 5.7.21中这个系统变量的默认值： 1234567mysql&gt; SHOW VARIABLES LIKE &#x27;%dive%&#x27;;+---------------------------+-------+| Variable_name | Value |+---------------------------+-------+| eq_range_index_dive_limit | 200 |+---------------------------+-------+1 row in set (0.08 sec) 也就是说如果我们的IN语句中的参数个数小于200个的话，将使用index dive的方式计算各个单点区间对应的记录条数，如果大于或等于200个的话，可就不能使用index dive了，要使用所谓的索引统计数据来进行估算。 像会为每个表维护一份统计数据一样，MySQL也会为表中的每一个索引维护一份统计数据，查看某个表中索引的统计数据可以使用SHOW INDEX FROM 表名的语法，比如我们查看一下single_table的各个索引的统计数据可以这么写： 12345678910111213mysql&gt; SHOW INDEX FROM single_table;+--------------+------------+--------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+| Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation | Cardinality | Sub_part | Packed | Null | Index_type | Comment | Index_comment |+--------------+------------+--------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+| single_table | 0 | PRIMARY | 1 | id | A | 9693 | NULL | NULL | | BTREE | | || single_table | 0 | idx_key2 | 1 | key2 | A | 9693 | NULL | NULL | YES | BTREE | | || single_table | 1 | idx_key1 | 1 | key1 | A | 968 | NULL | NULL | YES | BTREE | | || single_table | 1 | idx_key3 | 1 | key3 | A | 799 | NULL | NULL | YES | BTREE | | || single_table | 1 | idx_key_part | 1 | key_part1 | A | 9673 | NULL | NULL | YES | BTREE | | || single_table | 1 | idx_key_part | 2 | key_part2 | A | 9999 | NULL | NULL | YES | BTREE | | || single_table | 1 | idx_key_part | 3 | key_part3 | A | 10000 | NULL | NULL | YES | BTREE | | |+--------------+------------+--------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+7 rows in set (0.01 sec) 属性名 描述 Table 索引所属表的名称。 Non_unique 索引列的值是否是唯一的，聚簇索引和唯一二级索引的该列值为0 ，普通二级索引该列值为1 。 Key_name 索引的名称。 Seq_in_index 索引列在索引中的位置，从1开始计数。比如对于联合索引idx_key_part ，来说，key_part1 、key_part2 和key_part3 对应的位置分别是1、2、3。 Column_name 索引列的名称。 Collation 索引列中的值是按照何种排序方式存放的，值为A 时代表升序存放，为NULL 时代表降序存放。 Cardinality 索引列中不重复值的数量。后边我们会重点看这个属性的。 Sub_part 对于存储字符串或者字节串的列来说，有时候我们只想对这些串的前n 个字符或字节建立索引，这个属性表示的就是那个n 值。如果对完整的列建立索引的话，该属性的值就是NULL 。 Packed 索引列如何被压缩，NULL 值表示未被压缩。这个属性我们暂时不了解，可以先忽略掉。 Null 该索引列是否允许存储NULL 值。 Index_type 使用索引的类型，我们最常见的就是BTREE ，其实也就是B+ 树索引。 Comment 索引列注释信息。 Index_comment 索引注释信息。 上述属性其实我们现在最在意的是Cardinality属性，Cardinality直译过来就是基数的意思，表示索引列中不重复值的个数。比如对于一个一万行记录的表来说，某个索引列的Cardinality属性是10000，那意味着该列中没有重复的值，如果Cardinality属性是1的话，就意味着该列的值全部是重复的。不过需要注意的是，对于InnoDB存储引擎来说，使用SHOW INDEX语句展示出来的某个索引列的Cardinality属性是一个估计值，并不是精确的。 前边说道，当IN语句中的参数个数大于或等于系统变量eq_range_index_dive_limit的值的话，就不会使用index dive的方式计算各个单点区间对应的索引记录条数，而是使用索引统计数据，这里所指的索引统计数据指的是这两个值： 使用SHOW TABLE STATUS展示出的Rows值，也就是一个表中有多少条记录。 使用SHOW INDEX语句展示出的Cardinality属性。结合上一个Rows统计数据，我们可以针对索引列，计算出平均一个值重复多少次。 1一个值的重复次数 ≈ Rows ÷ Cardinality 以single_table表的idx_key1索引为例，它的Rows值是9693，它对应索引列key1的Cardinality值是968，所以我们可以计算key1列平均单个值的重复次数就是： 19693 ÷ 968 ≈ 10（条） 此时再看上边那条查询语句： 1SELECT * FROM single_table WHERE key1 IN (&#x27;aa1&#x27;, &#x27;aa2&#x27;, &#x27;aa3&#x27;, ... , &#x27;zzz&#x27;); 假设IN语句中有20000个参数的话，就直接使用统计数据来估算这些参数需要单点区间对应的记录条数了，每个参数大约对应10条记录，所以总共需要回表的记录数就是： 120000 x 10 = 200000 使用统计数据来计算单点区间对应的索引记录条数可比index dive的方式简单多了，但是不精确！。使用统计数据算出来的查询成本与实际所需的成本可能相差非常大。 在MySQL 5.7.3以及之前的版本中，eq_range_index_dive_limit的默认值为10，之后的版本默认值为200。所以如果采用的是5.7.3以及之前的版本的话，很容易采用索引统计数据而不是index dive的方式来计算查询成本。当查询中使用到了IN查询，但是却实际没有用到索引，就应该考虑一下是不是由于 eq_range_index_dive_limit 值太小导致的。 3. 连接查询的成本我们直接构造一个和single_table表一模一样的single_table2表。为了简便起见，我们把single_table表称为s1表，把single_table2表称为s2表。 3.1 条件过滤我们前边说过，MySQL中连接查询采用的是嵌套循环连接算法，驱动表会被访问一次，被驱动表可能会被访问多次，所以对于两表连接查询来说，它的查询成本由下边两个部分构成： 单次查询驱动表的成本 多次查询被驱动表的成本（具体查询多少次取决于对驱动表查询的结果集中有多少条记录） 我们把对驱动表进行查询后得到的记录条数称之为驱动表的扇出（英文名：fanout）。很显然驱动表的扇出值越小，对被驱动表的查询次数也就越少，连接查询的总成本也就越低。当查询优化器想计算整个连接查询所使用的成本时，就需要计算出驱动表的扇出值，有的时候扇出值的计算是很容易的，比如下边这两个查询： 查询一：假设使用s1表作为驱动表，很显然对驱动表的单表查询只能使用全表扫描的方式执行，驱动表的扇出值也很明确，那就是驱动表中有多少记录，扇出值就是多少。我们前边说过，统计数据中s1表的记录行数是9693，也就是说优化器就直接会把9693当作在s1表的扇出值。 1SELECT * FROM single_table AS s1 INNER JOIN single_table2 AS s2; 查询二：仍然假设s1表是驱动表的话，很显然对驱动表的单表查询可以使用idx_key2索引执行查询。此时idx_key2的范围区间(10, 1000)中有多少条记录，那么扇出值就是多少。我们前边计算过，满足idx_key2的范围区间(10, 1000)的记录数是95条，也就是说本查询中优化器会把95当作驱动表s1的扇出值。 12SELECT * FROM single_table AS s1 INNER JOIN single_table2 AS s2 WHERE s1.key2 &gt;10 AND s1.key2 &lt; 1000; 有的时候扇出值的计算就变得很棘手，比方说下边几个查询： 查询三：本查询和查询一类似，只不过对于驱动表s1多了一个common_field &gt; &#39;xyz&#39;的搜索条件。查询优化器又不会真正的去执行查询，所以它只能猜这9693记录里有多少条记录满足common_field &gt; &#39;xyz&#39;条件。 12SELECT * FROM single_table AS s1 INNER JOIN single_table2 AS s2 WHERE s1.common_field &gt; &#x27;xyz&#x27;; 查询四：本查询和查询二类似，只不过对于驱动表s1也多了一个common_field &gt; &#39;xyz&#39;的搜索条件。不过因为本查询可以使用idx_key2索引，所以只需要从符合二级索引范围区间的记录中猜有多少条记录符合common_field &gt; &#39;xyz&#39;条件，也就是只需要猜在95条记录中有多少符合common_field &gt; &#39;xyz&#39;条件。 123SELECT * FROM single_table AS s1 INNER JOIN single_table2 AS s2 WHERE s1.key2 &gt; 10 AND s1.key2 &lt; 1000 AND s1.common_field &gt; &#x27;xyz&#x27;; 查询五：本查询和查询二类似，不过在驱动表s1选取idx_key2索引执行查询后，优化器需要从符合二级索引范围区间的记录中猜有多少条记录符合下边两个条件： 1234SELECT * FROM single_table AS s1 INNER JOIN single_table2 AS s2 WHERE s1.key2 &gt; 10 AND s1.key2 &lt; 1000 AND s1.key1 IN (&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;) AND s1.common_field &gt; &#x27;xyz&#x27;; key1 IN (&#39;a&#39;, &#39;b&#39;, &#39;c&#39;) common_field &gt; &#39;xyz&#39; 也就是优化器需要猜在95条记录中有多少符合上述两个条件的。 说了这么多，其实就是想表达在这两种情况下计算驱动表扇出值时需要靠猜： 如果使用的是全表扫描的方式执行的单表查询，那么计算驱动表扇出时需要猜满足搜索条件的记录到底有多少条。 如果使用的是索引执行的单表扫描，那么计算驱动表扇出的时候需要猜满足除使用到对应索引的搜索条件外的其他搜索条件的记录有多少条。 MySQL把这个猜的过程称之为condition filtering。当然，这个过程可能会使用到索引，也可能使用到统计数据，也可能就是MySQL单纯的瞎猜。 在MySQL 5.7之前的版本中，查询优化器在计算驱动表扇出时，如果是使用全表扫描的话，就直接使用表中记录的数量作为扇出值，如果使用索引的话，就直接使用满足范围条件的索引记录条数作为扇出值。在MySQL 5.7中，MySQL引入了这个condition filtering的功能，就是还要猜一猜剩余的那些搜索条件能把驱动表中的记录再过滤多少条，其实本质上就是为了让成本估算更精确。 MySQL称之为启发式规则（heuristic）。 3.2 两表连接成本分析连接查询的成本计算公式是这样的： 1连接查询总成本 = 单次访问驱动表的成本 + 驱动表扇出数 x 单次访问被驱动表的成本 对于左（外）连接和右（外）连接查询来说，它们的驱动表是固定的，所以想要得到最优的查询方案只需要： 分别为驱动表和被驱动表选择成本最低的访问方法。 可是对于内连接来说，驱动表和被驱动表的位置是可以互换的，所以需要考虑两个方面的问题： 不同的表作为驱动表最终的查询成本可能是不同的，也就是需要考虑最优的表连接顺序。 然后分别为驱动表和被驱动表选择成本最低的访问方法。 很显然，计算内连接查询成本的方式更麻烦一些，下边我们就以内连接为例来看看如何计算出最优的连接查询方案。 左（外）连接和右（外）连接查询在某些特殊情况下可以被优化为内连接查询。 比如对于下边这个查询来说： 1234SELECT * FROM single_table AS s1 INNER JOIN single_table2 AS s2 ON s1.key1 = s2.common_field WHERE s1.key2 &gt; 10 AND s1.key2 &lt; 1000 AND s2.key2 &gt; 1000 AND s2.key2 &lt; 2000; 可以选择的连接顺序有两种： s1连接s2，也就是s1作为驱动表，s2作为被驱动表。 s2连接s1，也就是s2作为驱动表，s1作为被驱动表。 查询优化器需要分别考虑这两种情况下的最优查询成本，然后选取那个成本更低的连接顺序以及该连接顺序下各个表的最优访问方法作为最终的查询计划。我们分别来看一下（定性的分析一下，不像分析单表查询那样定量的分析了）： 使用s1作为驱动表的情况 分析对于驱动表的成本最低的执行方案首先看一下涉及s1表单表的搜索条件有哪些： s1.key2 &gt; 10 AND s1.key2 &lt; 1000 所以这个查询可能使用到idx_key2索引，从全表扫描和使用idx_key2这两个方案中选出成本最低的那个，很显然使用idx_key2执行查询的成本更低些。 然后分析对于被驱动表的成本最低的执行方案此时涉及被驱动表s2的搜索条件就是： s2.common_field = 常数（这是因为对驱动表s1结果集中的每一条记录，都需要进行一次被驱动表s2的访问，此时那些涉及两表的条件现在相当于只涉及被驱动表s2了。） s2.key2 &gt; 1000 AND s2.key2 &lt; 2000 很显然，第一个条件由于common_field没有用到索引，此时访问s2表时可用的方案也是全表扫描和使用idx_key2两种，假设使用idx_key2的成本更小。所以此时使用s1作为驱动表时的总成本就是（暂时不考虑使用join buffer对成本的影响）： 1使用idx_key2访问s1的成本 + s1的扇出 × 使用idx_key2访问s2的成本 使用s2作为驱动表的情况 分析对于驱动表的成本最低的执行方案首先看一下涉及s2表单表的搜索条件有哪些： s2.key2 &gt; 1000 AND s2.key2 &lt; 2000 所以这个查询可能使用到idx_key2索引，从全表扫描和使用idx_key2这两个方案中选出成本最低的那个，假设使用idx_key2执行查询的成本更低些。 然后分析对于被驱动表的成本最低的执行方案此时涉及被驱动表s1的搜索条件就是： s1.key1 = 常数 s1.key2 &gt; 10 AND s1.key2 &lt; 2000 这时就很有趣了，使用idx_key1可以进行ref方式的访问，使用idx_key2可以使用range方式的访问。这时优化器需要从全表扫描、使用idx_key1、使用idx_key2这几个方案里选出一个成本最低的方案。这里有个问题，因为idx_key2的范围区间是确定的：(10, 1000)，怎么计算使用idx_key2的成本我们上边已经说过了，可是在没有真正执行查询前，s1.key1 = 常数中的常数值我们并不知道，怎么衡量使用idx_key1执行查询的成本呢？其实很简单，直接使用索引统计数据就好了（就是索引列平均一个值重复多少次）。一般情况下，ref的访问方式要比range成本更低，这里假设使用idx_key1进行对s1的访问。所以此时使用s2作为驱动表时的总成本就是： 1使用idx_key2访问s2的成本 + s2的扇出 × 使用idx_key1访问s1的成本 最后优化器会比较这两种方式的最优访问成本，选取那个成本更低的连接顺序去真正的执行查询。从上边的计算过程也可以看出来，连接查询成本占大头的其实是驱动表扇出数 x 单次访问被驱动表的成本，所以我们的优化重点其实是下边这两个部分： 尽量减少驱动表的扇出 对被驱动表的访问成本尽量低这一点对于我们实际书写连接查询语句时十分有用，我们需要尽量在被驱动表的连接列上建立索引，这样就可以使用ref访问方法来降低访问被驱动表的成本了。如果可以，被驱动表的连接列最好是该表的主键或者唯一二级索引列，这样就可以把访问被驱动表的成本降到更低了。 3.3 多表连接的成本分析首先要考虑一下多表连接时可能产生出多少种连接顺序： 对于两表连接，比如表A和表B连接只有 AB、BA这两种连接顺序。其实相当于2 × 1 = 2种连接顺序。 对于三表连接，比如表A、表B、表C进行连接有ABC、ACB、BAC、BCA、CAB、CBA这么6种连接顺序。其实相当于3 × 2 × 1 = 6种连接顺序。 对于四表连接的话，则会有4 × 3 × 2 × 1 = 24种连接顺序。 对于n表连接的话，则有 n × (n-1) × (n-2) × ··· × 1种连接顺序，就是n的阶乘种连接顺序，也就是n!。 有n个表进行连接，MySQL查询优化器要每一种连接顺序的成本都计算一遍，不过MySQL想了很多办法减少计算非常多种连接顺序的成本的方法： 提前结束某种顺序的成本评估MySQL在计算各种链接顺序的成本之前，会维护一个全局的变量，这个变量表示当前最小的连接查询成本。如果在分析某个连接顺序的成本时，该成本已经超过当前最小的连接查询成本，那就不对该连接顺序继续往下分析了。比方说A、B、C三个表进行连接，已经得到连接顺序ABC是当前的最小连接成本，比方说10.0，在计算连接顺序BCA时，发现B和C的连接成本就已经大于10.0时，就不再继续往后分析BCA这个连接顺序的成本了。 系统变量optimizer_search_depth为了防止无穷无尽的分析各种连接顺序的成本，MySQL提出了optimizer_search_depth系统变量，如果连接表的个数小于该值，那么就继续穷举分析每一种连接顺序的成本，否则只对与optimizer_search_depth值相同数量的表进行穷举分析。很显然，该值越大，成本分析的越精确，越容易得到好的执行计划，但是消耗的时间也就越长，否则得到不是很好的执行计划，但可以省掉很多分析连接成本的时间。 根据某些规则压根儿就不考虑某些连接顺序即使是有上边两条规则的限制，但是分析多个表不同连接顺序成本花费的时间还是会很长，所以MySQL干脆提出了一些所谓的启发式规则（就是根据以往经验指定的一些规则），凡是不满足这些规则的连接顺序压根儿就不分析，这样可以极大的减少需要分析的连接顺序的数量，但是也可能造成错失最优的执行计划。他们提供了一个系统变量optimizer_prune_level来控制到底是不是用这些启发式规则。 4. 调节成本常数我们前边已经介绍了两个成本常数： 读取一个页面花费的成本默认是1.0 检测一条记录是否符合搜索条件的成本默认是0.2 其实除了这两个成本常数，MySQL还支持好多，它们被存储到了mysql数据库（这是一个系统数据库）的两个表中： 12345678mysql&gt; SHOW TABLES FROM mysql LIKE &#x27;%cost%&#x27;;+--------------------------+| Tables_in_mysql (%cost%) |+--------------------------+| engine_cost || server_cost |+--------------------------+2 rows in set (0.00 sec) 一条语句的执行其实是分为两层的： server层 存储引擎层 在server层进行连接管理、查询缓存、语法解析、查询优化等操作，在存储引擎层执行具体的数据存取操作。也就是说一条语句在server层中执行的成本是和它操作的表使用的存储引擎是没关系的，所以关于这些操作对应的成本常数就存储在了server_cost表中，而依赖于存储引擎的一些操作对应的成本常数就存储在了engine_cost表中。 4.1mysql.server_cost表server_cost表中在server层进行的一些操作对应的成本常数，具体内容如下： 123456789101112mysql&gt; SELECT * FROM mysql.server_cost;+------------------------------+------------+---------------------+---------+| cost_name | cost_value | last_update | comment |+------------------------------+------------+---------------------+---------+| disk_temptable_create_cost | NULL | 2018-01-20 12:03:21 | NULL || disk_temptable_row_cost | NULL | 2018-01-20 12:03:21 | NULL || key_compare_cost | NULL | 2018-01-20 12:03:21 | NULL || memory_temptable_create_cost | NULL | 2018-01-20 12:03:21 | NULL || memory_temptable_row_cost | NULL | 2018-01-20 12:03:21 | NULL || row_evaluate_cost | NULL | 2018-01-20 12:03:21 | NULL |+------------------------------+------------+---------------------+---------+6 rows in set (0.05 sec) 我们先看一下server_cost各个列都分别是什么意思： cost_name表示成本常数的名称。 cost_value表示成本常数对应的值。如果该列的值为NULL的话，意味着对应的成本常数会采用默认值。 last_update表示最后更新记录的时间。 comment注释。 从server_cost中的内容可以看出来，目前在server层的一些操作对应的成本常数有以下几种： 成本常数名称 默认值 描述 disk_temptable_create_cost 40.0 创建基于磁盘的临时表的成本，如果增大这个值的话会让优化器尽量少的创建基于磁盘的临时表。 disk_temptable_row_cost 1.0 向基于磁盘的临时表写入或读取一条记录的成本，如果增大这个值的话会让优化器尽量少的创建基于磁盘的临时表。 key_compare_cost 0.1 两条记录做比较操作的成本，多用在排序操作上，如果增大这个值的话会提升filesort 的成本，让优化器可能更倾向于使用索引完成排序而不是filesort 。 memory_temptable_create_cost 2.0 创建基于内存的临时表的成本，如果增大这个值的话会让优化器尽量少的创建基于内存的临时表。 memory_temptable_row_cost 0.2 向基于内存的临时表写入或读取一条记录的成本，如果增大这个值的话会让优化器尽量少的创建基于内存的临时表。 row_evaluate_cost 0.2 这个就是我们之前一直使用的检测一条记录是否符合搜索条件的成本，增大这个值可能让优化器更倾向于使用索引而不是直接全表扫描。 MySQL在执行诸如DISTINCT查询、分组查询、Union查询以及某些特殊条件下的排序查询都可能在内部先创建一个临时表，使用这个临时表来辅助完成查询（比如对于DISTINCT查询可以建一个带有UNIQUE索引的临时表，直接把需要去重的记录插入到这个临时表中，插入完成之后的记录就是结果集了）。在数据量大的情况下可能创建基于磁盘的临时表，也就是为该临时表使用MyISAM、InnoDB等存储引擎，在数据量不大时可能创建基于内存的临时表，也就是使用Memory存储引擎。创建临时表和对这个临时表进行写入和读取的操作代价还是很高的。 这些成本常数在server_cost中的初始值都是NULL，意味着优化器会使用它们的默认值来计算某个操作的成本，如果我们想修改某个成本常数的值的话，需要做两个步骤： 对我们感兴趣的成本常数做更新操作比方说我们想把检测一条记录是否符合搜索条件的成本增大到0.4，那么就可以这样写更新语句： 123UPDATE mysql.server_cost SET cost_value = 0.4 WHERE cost_name = &#x27;row_evaluate_cost&#x27;; 让系统重新加载这个表的值。使用下边语句即可： 1FLUSH OPTIMIZER_COSTS; 当然，在你修改完某个成本常数后想把它们再改回默认值的话，可以直接把cost_value的值设置为NULL，再使用FLUSH OPTIMIZER_COSTS语句让系统重新加载它就好了。 4.2mysql.engine_cost表engine_cost表表中在存储引擎层进行的一些操作对应的成本常数，具体内容如下： 12345678mysql&gt; SELECT * FROM mysql.engine_cost;+-------------+-------------+------------------------+------------+---------------------+---------+| engine_name | device_type | cost_name | cost_value | last_update | comment |+-------------+-------------+------------------------+------------+---------------------+---------+| default | 0 | io_block_read_cost | NULL | 2018-01-20 12:03:21 | NULL || default | 0 | memory_block_read_cost | NULL | 2018-01-20 12:03:21 | NULL |+-------------+-------------+------------------------+------------+---------------------+---------+2 rows in set (0.05 sec) 与server_cost相比，engine_cost多了两个列： engine_name列指成本常数适用的存储引擎名称。如果该值为default，意味着对应的成本常数适用于所有的存储引擎。 device_type列指存储引擎使用的设备类型，这主要是为了区分常规的机械硬盘和固态硬盘，不过在MySQL 5.7.21这个版本中并没有对机械硬盘的成本和固态硬盘的成本作区分，所以该值默认是0。 我们从engine_cost表中的内容可以看出来，目前支持的存储引擎成本常数只有两个： 成本常数名称 默认值 描述 io_block_read_cost 1.0 从磁盘上读取一个块对应的成本。请注意我使用的是块 ，而不是页 这个词儿。对于InnoDB 存储引擎来说，一个页 就是一个块，不过对于MyISAM 存储引擎来说，默认是以4096 字节作为一个块的。增大这个值会加重I/O 成本，可能让优化器更倾向于选择使用索引执行查询而不是执行全表扫描。 memory_block_read_cost 1.0 与上一个参数类似，只不过衡量的是从内存中读取一个块对应的成本。 怎么从内存中和从磁盘上读取一个块的默认成本是一样的？这主要是因为在MySQL目前的实现中，并不能准确预测某个查询需要访问的块中有哪些块已经加载到内存中，有哪些块还停留在磁盘上，所以MySQL认为不管这个块有没有加载到内存中，使用的成本都是1.0，不过随着MySQL的发展，等到可以准确预测哪些块在磁盘上，那些块在内存中的那一天，这两个成本常数的默认值可能会改。 与更新server_cost表中的记录一样，我们也可以通过更新engine_cost表中的记录来更改关于存储引擎的成本常数，我们也可以通过为engine_cost表插入新记录的方式来添加只针对某种存储引擎的成本常数： 插入针对某个存储引擎的成本常数比如我们想增大InnoDB存储引擎页面I/O的成本，书写正常的插入语句即可： 123INSERT INTO mysql.engine_cost VALUES (&#x27;InnoDB&#x27;, 0, &#x27;io_block_read_cost&#x27;, 2.0, CURRENT_TIMESTAMP, &#x27;increase Innodb I/O cost&#x27;); 让系统重新加载这个表的值。使用下边语句即可： 1FLUSH OPTIMIZER_COSTS; 5. 总结在MySQL中，一个查询的执行成本是由IO成本和CPU成本组成的。对于InnoDB存储引擎来说，读取一个页面的默认IO成本是1.0，读取以及检测一条记录是否符合搜索条件的成本默认是0.2。 在单表查询中，优化器生成执行计划的步骤如下： 根据搜索条件，找出所有可能使用的索引 计算全表扫描的代价 计算使用不同索引执行查询的代价 对比各种执行方案的代价，找出成本最低的那个方案 在优化器生成执行计划过程中，需要依赖一些数据。这些数据可能是使用下面两种方式得到的： index dive：通过直接访问索引对应的B+树来获取数据 索引统计数据：直接依赖对表或者索引的统计数据 为了更准确的计算连接查询的成本，MySQL提出了条件过滤的概念，也就是采用了某些规则来预测驱动表的扇出值。 对于内连接来说，为了生成成本最低的执行计划，需要考虑两方面的事情： 选择最优的表连接顺序 为驱动表和被驱动表选择成本最低的访问方法 我们可以通过手动修改MySQL数据库下engine_cost &amp; server_cost表中的某些成本常数，更精确的控制在生成执行计划时的成本计算过程。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/tags/MySQL/"}]},{"title":"MySQL[六]单表查询&连接查询原理","slug":"MySQL/MySQL[六]单表查询&连接查询原理","date":"2022-01-11T03:15:54.543Z","updated":"2022-01-11T03:21:05.026Z","comments":true,"path":"2022/01/11/MySQL/MySQL[六]单表查询&连接查询原理/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/MySQL/MySQL[%E5%85%AD]%E5%8D%95%E8%A1%A8%E6%9F%A5%E8%AF%A2&%E8%BF%9E%E6%8E%A5%E6%9F%A5%E8%AF%A2%E5%8E%9F%E7%90%86/","excerpt":"","text":"一，单表查询不会走之前不要跑，在学SQL优化之前，我们先来分析下SQL是怎么执行的。 前面说过，MySQL Server有一个称为查询优化器的模块，一条查询语句进行语法解析之后就会被交给查询优化器来进行优化，优化的结果就是生成一个所谓的执行计划，这个执行计划表明了应该使用哪些索引进行查询，表之间的连接顺序是啥样的，最后会按照执行计划中的步骤调用存储引擎提供的方法来真正的执行查询，并将查询结果返回给用户。 如果觉得我这篇博客讲的看不懂，回头看看我前面的几篇，MySQL是一个很复杂的东西，尽量不要跳着学，要静下心系统的来学习，之前我都是四处看帖子看博客，一直觉得自己MySQL迷迷糊糊，甚至成了痛点，所以决心写个MySQL专栏，系统的学习下。 我们前面创建过一张表，现在拿来复用下。 123456789101112131415CREATE TABLE single_table ( id INT NOT NULL AUTO_INCREMENT, key1 VARCHAR(100), key2 INT, key3 VARCHAR(100), key_part1 VARCHAR(100), key_part2 VARCHAR(100), key_part3 VARCHAR(100), common_field VARCHAR(100), PRIMARY KEY (id), KEY idx_key1 (key1), UNIQUE KEY idx_key2 (key2), KEY idx_key3 (key3), KEY idx_key_part(key_part1, key_part2, key_part3)) Engine=InnoDB CHARSET=utf8; 我们为这个single_table表建立了1个聚簇索引和4个二级索引，分别是： 为id列建立的聚簇索引。 为key1列建立的idx_key1二级索引。 为key2列建立的idx_key2二级索引，而且该索引是唯一二级索引。 为key3列建立的idx_key3二级索引。 为key_part1、key_part2、key_part3列建立的idx_key_part二级索引，这也是一个联合索引。 这张表我插入了一百万数据，用来做实验。 1.访问方法对于单个表的查询来说，MySQL把查询的执行方式大致分为下边两种： 使用全表扫描进行查询这种执行方式很好理解，就是把表的每一行记录都扫一遍嘛，把符合搜索条件的记录加入到结果集就完了。不管是啥查询都可以使用这种方式执行，当然，这种也是最笨的执行方式。 使用索引进行查询因为直接使用全表扫描的方式执行查询要遍历好多记录，所以代价可能太大了。如果查询语句中的搜索条件可以使用到某个索引，那直接使用索引来执行查询可能会加快查询执行的时间。使用索引来执行查询的方式五花八门，又可以细分为许多种类： 针对主键或唯一二级索引的等值查询 针对普通二级索引的等值查询 针对索引列的范围查询 直接扫描整个索引 MySQL把MySQL执行查询语句的方式称之为访问方法或者访问类型。同一个查询语句可能可以使用多种不同的访问方法来执行，虽然最后的查询结果都是一样的，但是执行的时间可能相差很多。 2.const1SELECT * FROM single_table WHERE id = 1438; MySQL会直接利用主键值在聚簇索引中定位对应的用户记录。 **B+**树叶子节点中的记录是按照索引列排序的，对于的聚簇索引来说，它对应的**B+**树叶子节点中的记录就是按照**id**列排序的。所以这样根据主键值定位一条记录的速度贼快。类似的，我们根据唯一二级索引列来定位一条记录的速度也是贼快的，比如下边这个查询： 1SELECT * FROM single_table WHERE key2 = 3841; 这个查询的执行过程的示意图就是这样： 这个查询的执行分两步： 先从idx_key2对应的B+树索引中根据key2列与常数的等值比较条件定位到一条二级索引记录 再根据该记录的id值到聚簇索引中获取到完整的用户记录 MySQL认为通过主键或者唯一二级索引列与常数的等值比较来定位一条记录非常快，所以他们把这种通过主键或者唯一二级索引列来定位一条记录的访问方法定义为：const，意思是常数级别的，代价是可以忽略不计的。不过这种const访问方法只能在主键列或者唯一二级索引列和一个常数进行等值比较时才有效，如果主键或者唯一二级索引是由多个列构成的话，索引中的每一个列都需要与常数进行等值比较，这个const访问方法才有效（这是因为只有该索引中全部列都采用等值比较才可以定位唯一的一条记录）。 对于唯一二级索引来说，查询该列为NULL值的情况比较特殊，比如这样： 1SELECT * FROM single_table WHERE key2 IS NULL; 因为唯一二级索引列并不限制 NULL 值的数量，所以上述语句可能访问到多条记录，也就是说 上边这个语句不可以使用const访问方法来执行。 3.ref有时候我们对某个普通的二级索引列与常数进行等值比较，比如这样： 1SELECT * FROM single_table WHERE key1 = &#x27;abc&#x27;; 对于这个查询，我们当然可以选择全表扫描来逐一对比搜索条件是否满足要求，我们也可以先使用二级索引找到对应记录的id值，然后再回表到聚簇索引中查找完整的用户记录。由于普通二级索引并不限制索引列值的唯一性，所以可能找到多条对应的记录，也就是说使用二级索引来执行查询的代价取决于等值匹配到的二级索引记录条数。如果匹配的记录较少，则回表的代价还是比较低的，所以MySQL可能选择使用索引而不是全表扫描的方式来执行查询。MySQL把这种搜索条件为二级索引列与常数等值比较，采用二级索引来执行查询的访问方法称为：ref。我们看一下采用ref访问方法执行查询的图示： 对于普通的二级索引来说，通过索引列进行等值比较后可能匹配到多条连续的记录，而不是像主键或者唯一二级索引那样最多只能匹配1条记录，所以这种ref访问方法比const差了那么一点，但是在二级索引等值比较时匹配的记录数较少时的效率还是很高的（如果匹配的二级索引记录太多那么回表的成本就太大了）。 有两种特殊情况： 二级索引列值为NULL的情况不论是普通的二级索引，还是唯一二级索引，它们的索引列对包含NULL值的数量并不限制，所以我们采用key IS NULL这种形式的搜索条件最多只能使用ref的访问方法，而不是const的访问方法。 对于某个包含多个索引列的二级索引来说，只要是最左边的连续索引列是与常数的等值比较就可能采用ref的访问方法，比方说下边这几个查询： 12345SELECT * FROM single_table WHERE key_part1 = &#x27;god like&#x27;;SELECT * FROM single_table WHERE key_part1 = &#x27;god like&#x27; AND key_part2 = &#x27;legendary&#x27;;SELECT * FROM single_table WHERE key_part1 = &#x27;god like&#x27; AND key_part2 = &#x27;legendary&#x27; AND key_part3 = &#x27;penta kill&#x27;; 但是如果最左边的连续索引列并不全部是等值比较的话，它的访问方法就不能称为ref了，比方说这样： 1SELECT * FROM single_table WHERE key_part1 = &#x27;god like&#x27; AND key_part2 &gt; &#x27;legendary&#x27;; 4.ref_or_null有时候我们不仅想找出某个二级索引列的值等于某个常数的记录，还想把该列的值为NULL的记录也找出来，就像下边这个查询： 1SELECT * FROM single_table WHERE key1 = &#x27;abc&#x27; OR key1 IS NULL; 当使用二级索引而不是全表扫描的方式执行该查询时，这种类型的查询使用的访问方法就称为ref_or_null，这个ref_or_null访问方法的执行过程如下： 上边的查询相当于先分别从idx_key1索引对应的B+树中找出key1 IS NULL和key1 = &#39;abc&#39;的两个连续的记录范围，然后根据这些二级索引记录中的id值再回表查找完整的用户记录。 5.range1SELECT * FROM single_table WHERE key2 IN (1438, 6328) OR (key2 &gt;= 38 AND key2 &lt;= 79); 如果采用二级索引 + 回表的方式来执行的话，那么此时的搜索条件就不只是要求索引列与常数的等值匹配了，而是索引列需要匹配某个或某些范围的值，在本查询中key2列的值只要匹配下列3个范围中的任何一个就算是匹配成功了： key2的值是1438 key2的值是6328 key2的值在38和79之间。 MySQL把这种利用索引进行范围匹配的访问方法称之为：range。 此处所说的使用索引进行范围匹配中的 索引 可以是聚簇索引，也可以是二级索引。 我们可以把那种索引列等值匹配的情况称之为单点区间，上边所说的范围1和范围2都可以被称为单点区间，像范围3这种的我们可以称为连续范围区间。 6.index1SELECT key_part1, key_part2, key_part3 FROM single_table WHERE key_part2 = &#x27;abc&#x27;; 由于key_part2并不是联合索引idx_key_part最左索引列，所以我们无法使用ref或者range访问方法来执行这个语句。但是这个查询符合下边这两个条件： 它的查询列表只有3个列：key_part1, key_part2, key_part3，而索引idx_key_part又包含这三个列。 搜索条件中只有key_part2列。这个列也包含在索引idx_key_part中。 也就是说我们可以直接通过遍历idx_key_part索引的叶子节点的记录来比较key_part2 = &#39;abc&#39;这个条件是否成立，把匹配成功的二级索引记录的key_part1, key_part2, key_part3列的值直接加到结果集中就行了。由于二级索引记录比聚簇索记录小的多（聚簇索引记录要存储所有用户定义的列以及所谓的隐藏列，而二级索引记录只需要存放索引列和主键），而且这个过程也不用进行回表操作，所以直接遍历二级索引比直接遍历聚簇索引的成本要小很多，MySQL就把这种采用遍历二级索引记录的执行方式称之为：index。 7.all全表扫描 8.注意8.1 二级索引 + 回表一般情况下只能利用单个二级索引执行查询，比方说下边的这个查询： 1SELECT * FROM single_table WHERE key1 = &#x27;abc&#x27; AND key2 &gt; 1000; 查询优化器会识别到这个查询中的两个搜索条件： key1 = &#39;abc&#39; key2 &gt; 1000 优化器一般会根据single_table表的统计数据来判断到底使用哪个条件到对应的二级索引中查询扫描的行数会更少，选择那个扫描行数较少的条件到对应的二级索引中查询。然后将从该二级索引中查询到的结果经过回表得到完整的用户记录后再根据其余的WHERE条件过滤记录。一般来说，等值查找比范围查找需要扫描的行数更少（也就是ref的访问方法一般比range好，但这也不总是一定的，也可能采用ref访问方法的那个索引列的值为特定值的行数特别多），所以这里假设优化器决定使用idx_key1索引进行查询，那么整个查询过程可以分为两个步骤： 使用二级索引定位记录的阶段，也就是根据条件key1 = &#39;abc&#39;从idx_key1索引代表的B+树中找到对应的二级索引记录。 回表阶段，也就是根据上一步骤中找到的记录的主键值进行回表操作，也就是到聚簇索引中找到对应的完整的用户记录，再根据条件key2 &gt; 1000到完整的用户记录继续过滤。将最终符合过滤条件的记录返回给用户。 注意，因为二级索引的节点中的记录只包含索引列和主键，所以在步骤1中使用idx_key1索引进行查询时只会用到与key1列有关的搜索条件，其余条件，比如key2 &gt; 1000这个条件在步骤1中是用不到的，只有在步骤2完成回表操作后才能继续针对完整的用户记录中继续过滤。 一般情况下执行一个查询只会用到单个二级索引，不过还是有特殊情况的。 从上文可以看出，每次从二级索引中读取到一条记录后，就会根据该记录的主键值执行回表操作。而在某个扫描区间中的二级索引记录的主键值是无序的，也就是说这些二级索引记录对应的聚簇索引记录所在的页面的页号是无序的。每次执行回表操作时都相当于要随机读取一个聚簇索引页面，而这些随机I/O带来的性能开销比较大。于是MySQL提出了一个名为Disk-S weep Multi-Range Read(MRR，多范围读取)的优化措施，即先读取一部分二级索引记录，将它们的主键值排好序之后再统一执行回表操作。相对于每读取一条二级索引记录 就立即执行回表操作，这样会节省一些I/0开销。当然使用这个MRR优化措施的条件比较苛刻，我们之前的讨论中没有涉及MRR 之后的讨论中也将忽略这项优化措施，直接认为每读取一条二级索引记录就立即执行回表操作。 8.2 range访问方法使用的范围区间其实对于B+树索引来说，只要索引列和常数使用=、&lt;=&gt;、IN、NOT IN、IS NULL、IS NOT NULL、&gt;、&lt;、&gt;=、&lt;=、BETWEEN、!=（不等于也可以写成&lt;&gt;）或者LIKE操作符连接起来，就可以产生一个所谓的区间。 LIKE操作符比较特殊，只有在匹配完整字符串或者匹配字符串前缀时才可以利用索引。 IN操作符的效果和若干个等值匹配操作符=之间用OR连接起来是一样的，也就是说会产生多个单点区间，比如下边这两个语句的效果是一样的： 123SELECT * FROM single_table WHERE key2 IN (1438, 6328); SELECT * FROM single_table WHERE key2 = 1438 OR key2 = 6328; 在日常的工作中，一个查询的WHERE子句可能有很多个小的搜索条件，这些搜索条件需要使用AND或者OR操作符连接起来。当我们想使用range访问方法来执行一个查询语句时，重点就是找出该查询可用的索引以及这些索引对应的范围区间。 9.索引合并MySQL在一般情况下执行一个查询时最多只会用到单个二级索引，但是还有特殊情况，在这些特殊情况下也可能在一个查询中使用到多个二级索引，MySQL把这种使用到多个索引来完成一次查询的执行方法称之为：index merge，具体的索引合并算法有下边三种。 9.1 Intersection合并Intersection翻译过来的意思是交集。这里是说某个查询可以使用多个二级索引，将从多个二级索引中查询到的结果取交集，比方说下边这个查询： 1SELECT * FROM single_table WHERE key1 = &#x27;a&#x27; AND key3 = &#x27;b&#x27;; 假设这个查询使用Intersection合并的方式执行的话，那这个过程就是这样的： 从idx_key1二级索引对应的B+树中取出key1 = &#39;a&#39;的相关记录。 从idx_key3二级索引对应的B+树中取出key3 = &#39;b&#39;的相关记录。 二级索引的记录都是由索引列 + 主键构成的，所以我们可以计算出这两个结果集中id值的交集。 按照上一步生成的id值列表进行回表操作，也就是从聚簇索引中把指定id值的完整用户记录取出来，返回给用户。 为啥不直接使用idx_key1或者idx_key3只根据某个搜索条件去读取一个二级索引，然后回表后再过滤另外一个搜索条件呢？这里要分析一下两种查询执行方式之间需要的成本代价。 只读取一个二级索引的成本： 按照某个搜索条件读取一个二级索引 根据从该二级索引得到的主键值进行回表操作，然后再过滤其他的搜索条件 读取多个二级索引之后取交集成本： 按照不同的搜索条件分别读取不同的二级索引 将从多个二级索引得到的主键值取交集，然后进行回表操作 虽然读取多个二级索引比读取一个二级索引消耗性能，但是读取二级索引的操作是顺序I/O，而回表操作是随机I/O，所以如果只读取一个二级索引时需要回表的记录数特别多，而读取多个二级索引之后取交集的记录数非常少，当节省的因为回表而造成的性能损耗比访问多个二级索引带来的性能损耗更高时，读取多个二级索引后取交集比只读取一个二级索引的成本更低。 MySQL在某些特定的情况下才可能会使用到Intersection索引合并： 情况一：二级索引列是等值匹配的情况，对于联合索引来说，在联合索引中的每个列都必须等值匹配，不能出现只匹配部分列的情况。比方说下边这个查询可能用到idx_key1和idx_key_part这两个二级索引进行Intersection索引合并的操作： 1SELECT * FROM single_table WHERE key1 = &#x27;a&#x27; AND key_part1 = &#x27;a&#x27; AND key_part2 = &#x27;b&#x27; AND key_part3 = &#x27;c&#x27;; 而下边这两个查询就不能进行Intersection索引合并： 123SELECT * FROM single_table WHERE key1 &gt; &#x27;a&#x27; AND key_part1 = &#x27;a&#x27; AND key_part2 = &#x27;b&#x27; AND key_part3 = &#x27;c&#x27;;SELECT * FROM single_table WHERE key1 = &#x27;a&#x27; AND key_part1 = &#x27;a&#x27;; 第一个查询是因为对key1进行了范围匹配，第二个查询是因为联合索引idx_key_part中的key_part2和key_part3列并没有出现在搜索条件中，所以这两个查询不能进行Intersection索引合并。 情况二：主键列可以是范围匹配比方说下边这个查询可能用到主键和idx_key1进行Intersection索引合并的操作： 1SELECT * FROM single_table WHERE id &gt; 100 AND key1 = &#x27;a&#x27;; 对于InnoDB的二级索引来说，记录先是按照索引列进行排序，如果该二级索引是一个联合索引，那么会按照联合索引中的各个列依次排序。而二级索引的用户记录是由索引列 + 主键构成的，二级索引列的值相同的记录可能会有好多条，这些索引列的值相同的记录又是按照主键的值进行排序的。所以在二级索引列都是等值匹配的情况下才可能使用Intersection索引合并，是因为只有在这种情况下根据二级索引查询出的结果集是按照主键值排序的。 根据二级索引查询出的结果集是按照主键值排序的对使用**Intersection**索引合并的好处？Intersection索引合并会把从多个二级索引中查询出的主键值求交集，如果从各个二级索引中查询的到的结果集本身就是已经按照主键排好序的，那么求交集的过程就很简单。假设某个查询使用Intersection索引合并的方式从idx_key1和idx_key2这两个二级索引中获取到的主键值分别是： 从idx_key1中获取到已经排好序的主键值：1、3、5 从idx_key2中获取到已经排好序的主键值：2、3、4 那么求交集的过程就是这样：逐个取出这两个结果集中最小的主键值，如果两个值相等，则加入最后的交集结果中，否则丢弃当前较小的主键值，再取该丢弃的主键值所在结果集的后一个主键值来比较，直到某个结果集中的主键值用完了： 先取出这两个结果集中较小的主键值做比较，因为1 &lt; 2，所以把idx_key1的结果集的主键值1丢弃，取出后边的3来比较。 因为3 &gt; 2，所以把idx_key2的结果集的主键值2丢弃，取出后边的3来比较。 因为3 = 3，所以把3加入到最后的交集结果中，继续两个结果集后边的主键值来比较。 后边的主键值也不相等，所以最后的交集结果中只包含主键值3。 这个过程其实很快，时间复杂度是O(n)，但是如果从各个二级索引中查询出的结果集并不是按照主键排序的话，那就要先把结果集中的主键值排序完再来做上边的那个过程，就比较耗时了。 按照有序的主键值去回表取记录有个专有名词儿，叫：Rowid Ordered Retrieval，简称ROR。 另外，不仅是多个二级索引之间可以采用Intersection索引合并，索引合并也可以有聚簇索引参加，也就是我们上边写的情况二：在搜索条件中有主键的范围匹配的情况下也可以使用Intersection索引合并索引合并。 1SELECT * FROM single_table WHERE key1 = &#x27;a&#x27; AND id &gt; 100; 假设这个查询可以采用Intersection索引合并，我们理所当然的以为这个查询会分别按照id &gt; 100这个条件从聚簇索引中获取一些记录，在通过key1 = &#39;a&#39;这个条件从idx_key1二级索引中获取一些记录，然后再求交集，其实这样就把问题复杂化了，没必要从聚簇索引中获取一次记录。二级索引的记录中都带有主键值的，所以可以在从idx_key1中获取到的主键值上直接运用条件id &gt; 100过滤就行了。所以涉及主键的搜索条件只不过是为了从别的二级索引得到的结果集中过滤记录罢了，是不是等值匹配不重要。 当然，上边说的情况一和情况二只是发生Intersection索引合并的必要条件，不是充分条件。也就是说即使情况一、情况二成立，也不一定发生Intersection索引合并，这得看优化器的心情。优化器只有在单独根据搜索条件从某个二级索引中获取的记录数太多，导致回表开销太大，而通过Intersection索引合并后需要回表的记录数大大减少时才会使用Intersection索引合并。 9.2 Union合并有时候OR关系的不同搜索条件会使用到不同的索引。 1SELECT * FROM single_table WHERE key1 = &#x27;a&#x27; OR key3 = &#x27;b&#x27; Intersection是交集的意思，这适用于使用不同索引的搜索条件之间使用AND连接起来的情况；Union是并集的意思，适用于使用不同索引的搜索条件之间使用OR连接起来的情况。与Intersection索引合并类似，MySQL在某些特定的情况下才可能会使用到Union索引合并： 情况一：二级索引列是等值匹配的情况，对于联合索引来说，在联合索引中的每个列都必须等值匹配，不能出现只出现匹配部分列的情况。比方说下边这个查询可能用到idx_key1和idx_key_part这两个二级索引进行Union索引合并的操作： 1SELECT * FROM single_table WHERE key1 = &#x27;a&#x27; OR ( key_part1 = &#x27;a&#x27; AND key_part2 = &#x27;b&#x27; AND key_part3 = &#x27;c&#x27;); 而下边这两个查询就不能进行Union索引合并： 123SELECT * FROM single_table WHERE key1 &gt; &#x27;a&#x27; OR (key_part1 = &#x27;a&#x27; AND key_part2 = &#x27;b&#x27; AND key_part3 = &#x27;c&#x27;);SELECT * FROM single_table WHERE key1 = &#x27;a&#x27; OR key_part1 = &#x27;a&#x27;; 第一个查询是因为对key1进行了范围匹配，第二个查询是因为联合索引idx_key_part中的key_part2和key_part3列并没有出现在搜索条件中，所以这两个查询不能进行Union索引合并。 情况二：主键列可以是范围匹配 情况三：使用Intersection索引合并的搜索条件 这种情况其实就是搜索条件的某些部分使用Intersection索引合并的方式得到的主键集合和其他方式得到的主键集合取交集，比方说这个查询： 1SELECT * FROM single_table WHERE key_part1 = &#x27;a&#x27; AND key_part2 = &#x27;b&#x27; AND key_part3 = &#x27;c&#x27; OR (key1 = &#x27;a&#x27; AND key3 = &#x27;b&#x27;); 优化器可能采用这样的方式来执行这个查询： 先按照搜索条件key1 = &#39;a&#39; AND key3 = &#39;b&#39;从索引idx_key1和idx_key3中使用Intersection索引合并的方式得到一个主键集合。 再按照搜索条件key_part1 = &#39;a&#39; AND key_part2 = &#39;b&#39; AND key_part3 = &#39;c&#39;从联合索引idx_key_part中得到另一个主键集合。 采用Union索引合并的方式把上述两个主键集合取并集，然后进行回表操作，将结果返回给用户。 当然，查询条件符合了这些情况也不一定就会采用Union索引合并，也得看优化器的心情。优化器只有在单独根据搜索条件从某个二级索引中获取的记录数比较少，通过Union索引合并后进行访问的代价比全表扫描更小时才会使用Union索引合并。 9.3 Sort-Union合并Union索引合并的使用条件太苛刻，必须保证各个二级索引列在进行等值匹配的条件下才可能被用到，比方说下边这个查询就无法使用到Union索引合并： 1SELECT * FROM single_table WHERE key1 &lt; &#x27;a&#x27; OR key3 &gt; &#x27;z&#x27; 这是因为根据key1 &lt; &#39;a&#39;从idx_key1索引中获取的二级索引记录的主键值不是排好序的，根据key3 &gt; &#39;z&#39;从idx_key3索引中获取的二级索引记录的主键值也不是排好序的，但是key1 &lt; &#39;a&#39;和key3 &gt; &#39;z&#39;这两个条件又特别让我们动心，所以我们可以这样： 先根据key1 &lt; &#39;a&#39;条件从idx_key1二级索引中获取记录，并按照记录的主键值进行排序 再根据key3 &gt; &#39;z&#39;条件从idx_key3二级索引中获取记录，并按照记录的主键值进行排序 因为上述的两个二级索引主键值都是排好序的，剩下的操作和Union索引合并方式就一样了。 我们把上述这种先按照二级索引记录的主键值进行排序，之后按照Union索引合并方式执行的方式称之为Sort-Union索引合并，很显然，这种Sort-Union索引合并比单纯的Union索引合并多了一步对二级索引记录的主键值排序的过程。 为啥有Sort-Union索引合并，就没有Sort-Intersection索引合并么？是的，的确没有Sort-Intersection索引合并这么一说， Sort-Union的适用场景是单独根据搜索条件从某个二级索引中获取的记录数比较少，这样即使对这些二级索引记录按照主键值进行排序的成本也不会太高 而Intersection索引合并的适用场景是单独根据搜索条件从某个二级索引中获取的记录数太多，导致回表开销太大，合并后可以明显降低回表开销，但是如果加入Sort-Intersection后，就需要为大量的二级索引记录按照主键值进行排序，这个成本可能比回表查询都高了，所以也就没有引入Sort-Intersection。 9.4 索引合并注意事项联合索引替代Intersection索引合并1SELECT * FROM single_table WHERE key1 = &#x27;a&#x27; AND key3 = &#x27;b&#x27;; 这个查询之所以可能使用Intersection索引合并的方式执行，还不是因为idx_key1和idx_key3是两个单独的B+树索引，要是把这两个列搞一个联合索引，那直接使用这个联合索引就可以了： 1ALTER TABLE single_table drop index idx_key1, idx_key3, add index idx_key1_key3(key1, key3); 这样我们把没用的idx_key1、idx_key3都干掉，再添加一个联合索引idx_key1_key3，使用这个联合索引进行查询效果更好，既不用多读一棵B+树，也不用合并结果。 不过如果有单独对key3列进行查询的业务场景，这样子不得不再把key3列的单独索引给加上。具体还得以业务为准。 二，连接查询原理1. 连接简介1.1 连接的本质我们先建立两个简单的表并给它们填充一点数据： 1234567CREATE TABLE t1 (m1 int, n1 char(1));CREATE TABLE t2 (m2 int, n2 char(1));INSERT INTO t1 VALUES(1, &#x27;a&#x27;), (2, &#x27;b&#x27;), (3, &#x27;c&#x27;);INSERT INTO t2 VALUES(2, &#x27;b&#x27;), (3, &#x27;c&#x27;), (4, &#x27;d&#x27;); 我们成功建立了t1、t2两个表，这两个表都有两个列，一个是INT类型的，一个是CHAR(1)类型的。 12345678910111213141516171819mysql&gt; SELECT * FROM t1;+------+------+| m1 | n1 |+------+------+| 1 | a || 2 | b || 3 | c |+------+------+3 rows in set (0.00 sec)mysql&gt; SELECT * FROM t2;+------+------+| m2 | n2 |+------+------+| 2 | b || 3 | c || 4 | d |+------+------+3 rows in set (0.00 sec) 连接的本质就是把各个连接表中的记录都取出来依次匹配的组合加入结果集并返回给用户。所以我们把t1和t2两个表连接起来的过程如下图所示： 这个过程看起来就是把t1表的记录和t2的记录连起来组成新的更大的记录，所以这个查询过程称之为连接查询。连接查询的结果集中包含一个表中的每一条记录与另一个表中的每一条记录相互匹配的组合，像这样的结果集就可以称之为笛卡尔积。因为表t1中有3条记录，表t2中也有3条记录，所以这两个表连接之后的笛卡尔积就有3×3=9行记录。在MySQL中，连接查询的语法很简单，只要在FROM语句后边跟多个表名就好了，比如我们把t1表和t2表连接起来的查询语句可以写成这样： 1SELECT * FROM t1, t2; 1.2 连接过程简介在连接查询中的过滤条件可以分成两种： 涉及单表的条件这种只涉及单表的过滤条件我们之前都提到过一万遍了，我们之前也一直称为搜索条件，比如t1.m1 &gt; 1是只针对t1表的过滤条件，t2.n2 &lt; &#39;d&#39;是只针对t2表的过滤条件。 涉及两表的条件这种过滤条件我们之前没见过，比如t1.m1 = t2.m2、t1.n1 &gt; t2.n2等，这些条件中涉及到了两个表。 我们看一下携带过滤条件的连接查询的大致执行过程： 1SELECT * FROM t1, t2 WHERE t1.m1 &gt; 1 AND t1.m1 = t2.m2 AND t2.n2 &lt; &#x27;d&#x27;; 在这个查询中我们指明了这三个过滤条件： t1.m1 &gt; 1 t1.m1 = t2.m2 t2.n2 &lt; &#39;d&#39; 这个连接查询的大致执行过程如下： 首先确定第一个需要查询的表，这个表称之为驱动表。此处假设使用t1作为驱动表，那么就需要到t1表中找满足t1.m1 &gt; 1的记录，因为表中的数据太少，我们也没在表上建立二级索引，所以此处查询t1表的访问方法就设定为all吧，也就是采用全表扫描的方式执行单表查询，所以查询过程就如下图所示：我们可以看到，t1表中符合t1.m1 &gt; 1的记录有两条。 针对上一步骤中从驱动表产生的结果集中的每一条记录，分别需要到t2表中查找匹配的记录，所谓匹配的记录，指的是符合过滤条件的记录。因为是根据t1表中的记录去找t2表中的记录，所以t2表也可以被称之为被驱动表。上一步骤从驱动表中得到了2条记录，所以需要查询2次t2表。此时涉及两个表的列的过滤条件t1.m1 = t2.m2就派上用场了： 当t1.m1 = 2时，过滤条件t1.m1 = t2.m2就相当于t2.m2 = 2，所以此时t2表相当于有了t2.m2 = 2、t2.n2 &lt; &#39;d&#39;这两个过滤条件，然后到t2表中执行单表查询。 当t1.m1 = 3时，过滤条件t1.m1 = t2.m2就相当于t2.m2 = 3，所以此时t2表相当于有了t2.m2 = 3、t2.n2 &lt; &#39;d&#39;这两个过滤条件，然后到t2表中执行单表查询。 所以整个连接查询的执行过程就如下图所示： 也就是说整个连接查询最后的结果只有两条符合过滤条件的记录： 123456+------+------+------+------+| m1 | n1 | m2 | n2 |+------+------+------+------+| 2 | b | 2 | b || 3 | c | 3 | c |+------+------+------+------+ 这个两表连接查询共需要查询1次t1表，2次t2表。当然这是在特定的过滤条件下的结果，如果我们把t1.m1 &gt; 1这个条件去掉，那么从t1表中查出的记录就有3条，就需要查询3次t2表了。也就是说在两表连接查询中，驱动表只需要访问一次，被驱动表可能被访问多次。 1.3 内连接和外连接我们先创建两个有现实意义的表。 12345678910111213CREATE TABLE student ( number INT NOT NULL AUTO_INCREMENT COMMENT &#x27;学号&#x27;, name VARCHAR(5) COMMENT &#x27;姓名&#x27;, major VARCHAR(30) COMMENT &#x27;专业&#x27;, PRIMARY KEY (number)) Engine=InnoDB CHARSET=utf8 COMMENT &#x27;学生信息表&#x27;;CREATE TABLE score ( number INT COMMENT &#x27;学号&#x27;, subject VARCHAR(30) COMMENT &#x27;科目&#x27;, score TINYINT COMMENT &#x27;成绩&#x27;, PRIMARY KEY (number, subject)) Engine=InnoDB CHARSET=utf8 COMMENT &#x27;学生成绩表&#x27;; 我们新建了一个学生信息表，一个学生成绩表，然后我们向上述两个表中插入一些数据： 1234567891011121314151617181920mysql&gt; SELECT * FROM student;+----------+-----------+--------------------------+| number | name | major |+----------+-----------+--------------------------+| 20180101 | 杜子腾 | 软件学院 || 20180102 | 范统 | 计算机科学与工程 || 20180103 | 史珍香 | 计算机科学与工程 |+----------+-----------+--------------------------+3 rows in set (0.00 sec)mysql&gt; SELECT * FROM score;+----------+-----------------------------+-------+| number | subject | score |+----------+-----------------------------+-------+| 20180101 | 母猪的产后护理 | 78 || 20180101 | 论萨达姆的战争准备 | 88 || 20180102 | 论萨达姆的战争准备 | 98 || 20180102 | 母猪的产后护理 | 100 |+----------+-----------------------------+-------+4 rows in set (0.00 sec) 现在我们想把每个学生的考试成绩都查询出来就需要进行两表连接了（因为score中没有姓名信息，所以不能单纯只查询score表）。连接过程就是从student表中取出记录，在score表中查找number相同的成绩记录，所以过滤条件就是student.number = socre.number，整个查询语句就是这样： 1SELECT * FROM student, score WHERE student.number = score.number; 从上述查询结果中我们可以看到，各个同学对应的各科成绩就都被查出来了，可是有个问题，史珍香同学，也就是学号为20180103的同学因为某些原因没有参加考试，所以在score表中没有对应的成绩记录。那如果老师想查看所有同学的考试成绩，即使是缺考的同学也应该展示出来，但是到目前为止我们介绍的连接查询是无法完成这样的需求的。 这个需求的本质是：驱动表中的记录即使在被驱动表中没有匹配的记录，也仍然需要加入到结果集。为了解决这个问题，就有了内连接和外连接的概念。 对于内连接的两个表，驱动表中的记录在被驱动表中找不到匹配的记录，该记录不会加入到最后的结果集，我们上边提到的连接都是所谓的内连接。 对于外连接的两个表，驱动表中的记录即使在被驱动表中没有匹配的记录，也仍然需要加入到结果集。在MySQL中，根据选取驱动表的不同，外连接仍然可以细分为2种： 左外连接：选取左侧的表为驱动表。 右外连接：选取右侧的表为驱动表。 对于外连接来说，有时候我们也并不想把驱动表的全部记录都加入到最后的结果集。把过滤条件分为两种就可以了，所以放在不同地方的过滤条件是有不同语义的： WHERE子句中的过滤条件WHERE子句中的过滤条件就是我们平时见的那种，不论是内连接还是外连接，凡是不符合WHERE子句中的过滤条件的记录都不会被加入最后的结果集。 ON子句中的过滤条件对于外连接的驱动表的记录来说，如果无法在被驱动表中找到匹配ON子句中的过滤条件的记录，那么该记录仍然会被加入到结果集中，对应的被驱动表记录的各个字段使用NULL值填充。 需要注意的是，这个ON子句是专门为外连接驱动表中的记录在被驱动表找不到匹配记录时应不应该把该记录加入结果集这个场景下提出的，所以如果把ON子句放到内连接中，MySQL会把它和WHERE子句一样对待，也就是说：内连接中的WHERE子句和ON子句是等价的。 一般情况下，我们都把只涉及单表的过滤条件放到WHERE子句中，把涉及两表的过滤条件都放到ON子句中，我们也一般把放到ON子句中的过滤条件也称之为连接条件。 1.3.1 左（外）连接的语法比如我们要把t1表和t2表进行左外连接查询可以这么写： 1SELECT * FROM t1 LEFT [OUTER] JOIN t2 ON 连接条件 [WHERE 普通过滤条件]; 其中中括号里的OUTER单词是可以省略的。对于LEFT JOIN类型的连接来说，我们把放在左边的表称之为外表或者驱动表，右边的表称之为内表或者被驱动表。所以上述例子中t1就是外表或者驱动表，t2就是内表或者被驱动表。需要注意的是，对于左（外）连接和右（外）连接来说，必须使用ON子句来指出连接条件。 回到我们上边那个现实问题中来，看看怎样写查询语句才能把所有的学生的成绩信息都查询出来，即使是缺考的考生也应该被放到结果集中： 1SELECT s1.number, s1.name, s2.subject, s2.score FROM student AS s1 LEFT JOIN score AS s2 ON s1.number = s2.number; 从结果集中可以看出来，虽然史珍香并没有对应的成绩记录，但是由于采用的是连接类型为左（外）连接，所以仍然把她放到了结果集中，只不过在对应的成绩记录的各列使用NULL值填充而已。 1.3.2 右（外）连接的语法右（外）连接和左（外）连接的原理是一样一样的，语法也只是把LEFT换成RIGHT而已： 1SELECT * FROM t1 RIGHT [OUTER] JOIN t2 ON 连接条件 [WHERE 普通过滤条件]; 只不过驱动表是右边的表，被驱动表是左边的表。 1.3.3 内连接的语法内连接和外连接的根本区别就是在驱动表中的记录不符合ON子句中的连接条件时不会把该记录加入到最后的结果集。 一种最简单的内连接语法，就是直接把需要连接的多个表都放到FROM子句后边。其实针对内连接，MySQL提供了好多不同的语法，我们以t1和t2表为例： 1SELECT * FROM t1 [INNER | CROSS] JOIN t2 [ON 连接条件] [WHERE 普通过滤条件]; 也就是说在MySQL中，下边这几种内连接的写法都是等价的： SELECT * FROM t1 JOIN t2; SELECT * FROM t1 INNER JOIN t2; SELECT * FROM t1 CROSS JOIN t2; 上边的这些写法和直接把需要连接的表名放到FROM语句之后，用逗号,分隔开的写法是等价的： 1SELECT * FROM t1, t2; 在内连接中ON子句和WHERE子句是等价的，所以内连接中不要求强制写明ON子句。 前边说过，连接的本质就是把各个连接表中的记录都取出来依次匹配的组合加入结果集并返回给用户。不论哪个表作为驱动表，两表连接产生的笛卡尔积肯定是一样的。而对于内连接来说，由于凡是不符合ON子句或WHERE子句中的条件的记录都会被过滤掉，其实也就相当于从两表连接的笛卡尔积中把不符合过滤条件的记录给踢出去，所以对于内连接来说，驱动表和被驱动表是可以互换的，并不会影响最后的查询结果。但是对于外连接来说，由于驱动表中的记录即使在被驱动表中找不到符合ON子句条件的记录时也要将其加入到结果集，所以此时驱动表和被驱动表的关系就很重要了，也就是说左外连接和右外连接的驱动表和被驱动表不能轻易互换。 2.连接的原理接下来看一下MySQL采用了什么样的算法来进行表与表之间的连接。 2.1 嵌套循环连接对于两表连接来说，驱动表只会被访问一遍，但被驱动表却要被访问到好多遍，具体访问几遍取决于对驱动表执行单表查询后的结果集中的记录条数。对于内连接来说，选取哪个表为驱动表都没关系，而外连接的驱动表是固定的，也就是说左（外）连接的驱动表就是左边的那个表，右（外）连接的驱动表就是右边的那个表。 再来看一下t1表和t2表执行内连接查询的大致过程： 步骤1：选取驱动表，使用与驱动表相关的过滤条件，选取代价最低的单表访问方法来执行对驱动表的单表查询。 步骤2：对上一步骤中查询驱动表得到的结果集中每一条记录，都分别到被驱动表中查找匹配的记录。 通用的两表连接过程如下图所示： 如果有3个表进行连接的话，那么步骤2中得到的结果集就像是新的驱动表，然后第三个表就成为了被驱动表，重复上边过程，也就是步骤2中得到的结果集中的每一条记录都需要到t3表中找一找有没有匹配的记录，用伪代码表示一下这个过程就是这样： 123456789for each row in t1 &#123; #此处表示遍历满足对t1单表查询结果集中的每一条记录 for each row in t2 &#123; #此处表示对于某条t1表的记录来说，遍历满足对t2单表查询结果集中的每一条记录 for each row in t3 &#123; #此处表示对于某条t1和t2表的记录组合来说，对t3表进行单表查询 if row satisfies join conditions, send to client &#125; &#125;&#125; 这个过程就像是一个嵌套的循环，所以这种驱动表只访问一次，但被驱动表却可能被多次访问，访问次数取决于对驱动表执行单表查询后的结果集中的记录条数的连接执行方式称之为嵌套循环连接（Nested-Loop Join），这是最简单，也是最笨拙的一种连接查询算法。 2.2 使用索引加快连接速度在嵌套循环连接的步骤2中可能需要访问多次被驱动表，如果访问被驱动表的方式都是全表扫描的话，要查很多次。但是查询t2表其实就相当于一次单表扫描，我们可以利用索引来加快查询速度。回到最开始的t1表和t2表进行内连接的例子： 1SELECT * FROM t1, t2 WHERE t1.m1 &gt; 1 AND t1.m1 = t2.m2 AND t2.n2 &lt; &#x27;d&#x27;; 查询驱动表t1后的结果集中有两条记录，嵌套循环连接算法需要对被驱动表查询2次： 当t1.m1 = 2时，去查询一遍t2表，对t2表的查询语句相当于： 1SELECT * FROM t2 WHERE t2.m2 = 2 AND t2.n2 &lt; &#x27;d&#x27;; 当t1.m1 = 3时，再去查询一遍t2表，此时对t2表的查询语句相当于： 1SELECT * FROM t2 WHERE t2.m2 = 3 AND t2.n2 &lt; &#x27;d&#x27;; 可以看到，原来的t1.m1 = t2.m2这个涉及两个表的过滤条件在针对t2表做查询时关于t1表的条件就已经确定了，所以我们只需要单单优化对t2表的查询了，上述两个对t2表的查询语句中利用到的列是m2和n2列，我们可以： 在m2列上建立索引，因为对m2列的条件是等值查找，比如t2.m2 = 2、t2.m2 = 3等，所以可能使用到ref的访问方法，假设使用ref的访问方法去执行对t2表的查询的话，需要回表之后再判断t2.n2 &lt; d这个条件是否成立。这里有一个比较特殊的情况，就是假设m2列是t2表的主键或者唯一二级索引列，那么使用t2.m2 = 常数值这样的条件从t2表中查找记录的过程的代价就是常数级别的。我们知道在单表中使用主键值或者唯一二级索引列的值进行等值查找的方式称之为const，而MySQL把在连接查询中对被驱动表使用主键值或者唯一二级索引列的值进行等值查找的查询执行方式称之为：eq_ref。 在n2列上建立索引，涉及到的条件是t2.n2 &lt; &#39;d&#39;，可能用到range的访问方法，假设使用range的访问方法对t2表的查询的话，需要回表之后再判断在m2列上的条件是否成立。 假设m2和n2列上都存在索引的话，那么就需要从这两个里边儿挑一个代价更低的去执行对t2表的查询。当然，建立了索引不一定使用索引，只有在二级索引 + 回表的代价比全表扫描的代价更低时才会使用索引。 另外，有时候连接查询的查询列表和过滤条件中可能只涉及被驱动表的部分列，而这些列都是某个索引的一部分，这种情况下即使不能使用eq_ref、ref、ref_or_null或者range这些访问方法执行对被驱动表的查询的话，也可以使用索引扫描，也就是index的访问方法来查询被驱动表。所以我们建议在真实工作中最好不要使用*作为查询列表，最好把真实用到的列作为查询列表。 2.3 基于块的嵌套循环连接扫描一个表的过程其实是先把这个表从磁盘上加载到内存中，然后从内存中比较匹配条件是否满足。现实生活中的表可不像t1、t2这种只有3条记录，成千上万条记录都是少的，几百万、几千万甚至几亿条记录的表到处都是。内存里可能并不能完全存放的下表中所有的记录，所以在扫描表前边记录的时候后边的记录可能还在磁盘上，等扫描到后边记录的时候可能内存不足，所以需要把前边的记录从内存中释放掉。我们前边又说过，采用嵌套循环连接算法的两表连接过程中，被驱动表可是要被访问好多次的，如果这个被驱动表中的数据特别多而且不能使用索引进行访问，那就相当于要从磁盘上读好几次这个表，这个I/O代价就非常大了，所以我们得想办法：尽量减少访问被驱动表的次数。 当被驱动表中的数据非常多时，每次访问被驱动表，被驱动表的记录会被加载到内存中，在内存中的每一条记录只会和驱动表结果集的一条记录做匹配，之后就会被从内存中清除掉。然后再从驱动表结果集中拿出另一条记录，再一次把被驱动表的记录加载到内存中一遍，周而复始，驱动表结果集中有多少条记录，就得把被驱动表从磁盘上加载到内存中多少次。 如果在把被驱动表的记录加载到内存的时候，一次性和多条驱动表中的记录做匹配，这样就可以大大减少重复从磁盘上加载被驱动表的代价。 MySQL提出了一个join buffer的概念，join buffer就是执行连接查询前申请的一块固定大小的内存，先把若干条驱动表结果集中的记录装在这个join buffer中，然后开始扫描被驱动表，每一条被驱动表的记录一次性和join buffer中的多条驱动表记录做匹配，因为匹配的过程都是在内存中完成的，所以这样可以显著减少被驱动表的I/O代价。使用join buffer的过程如下图所示： 最好的情况是join buffer足够大，能容纳驱动表结果集中的所有记录，这样只需要访问一次被驱动表就可以完成连接操作了。MySQL把这种加入了join buffer的嵌套循环连接算法称之为基于块的嵌套连接（Block Nested-Loop Join）算法。 这个join buffer的大小是可以通过启动参数或者系统变量join_buffer_size进行配置，默认大小为262144字节（也就是256KB），最小可以设置为128字节。当然，对于优化被驱动表的查询来说，最好是为被驱动表加上效率高的索引，如果实在不能使用索引，并且自己的机器的内存也比较大可以尝试调大join_buffer_size的值来对连接查询进行优化。 另外需要注意的是，驱动表的记录并不是所有列都会被放到join buffer中，只有查询列表中的列和过滤条件中的列才会被放到join buffer中，所以再次提醒我们，最好不要把*作为查询列表，只需要把我们关心的列放到查询列表就好了，这样还可以在join buffer中放置更多的记录。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/tags/MySQL/"}]},{"title":"MySQL[五]InnoDb表空间","slug":"MySQL/MySQL[五]InnoDb表空间","date":"2022-01-11T03:15:42.796Z","updated":"2022-01-11T03:20:19.117Z","comments":true,"path":"2022/01/11/MySQL/MySQL[五]InnoDb表空间/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/MySQL/MySQL[%E4%BA%94]InnoDb%E8%A1%A8%E7%A9%BA%E9%97%B4/","excerpt":"","text":"在前面的两篇文章已经对InnoDB索引的结构，页存储结构，行格式做了十分细致的分析，也详细阐述了为什么你的SQL会慢，索引命中的原理，接下来我要继续深入学习MySQL。在此之前还要先来补充一下MySQL的一些基础知识。 一，MySQL的数据目录1. 数据库和文件系统的关系InnoDB,MyISAM这样的存储引擎都是把表存储在磁盘上，而操作系统是使用文件系统来管理磁盘的。【像InnoDB,MyISAM这样的存储引擎都是把数据存储在文件系统上的。】当我们想读取数据的时候，这些存储引擎会从文件系统中把数据读出来返回给我们，当我们想写入数据的时候，这些存储引擎又会把数据写回到文件系统。 本小节主要就是分析下InnoDB,MyISAM两个存储引擎的数据是如何在文件系统中存储的。 我的MySQL版本是5.7.28，所以接下来的操作和分析都是基于这个小版本的。其他版本可能会有细微的差异。 2.MySQL数据目录MySQL服务器程序在启动时，会到文件系统的某个目录下加载一些数据，之后再运行过程中产生的数据也会存储到这个目录下的某些文件中。这个目录就是数据目录。 2.1 数据目录和安装目录的区别MySQL的安装目录是在安装MySQL的时候指定的安装位置，下面有个很重要的bin目录，里面存储着控制客户端程序与服务器程序的命令。 MySQL的数据目录是用来存储MySQL在运行过程中产生的数据。 2.2 MySQL的数据目录在哪里数据目录对应着一个系统变量datadir，在使用客户端与服务器建立连接以后，查看这个系统变量的值就知道了： 1show variables like &#x27;datadir&#x27;; 结果如下： 1234567mysql&gt; show variables like &#x27;datadir&#x27;;+---------------+--------------------+| Variable_name | Value |+---------------+--------------------+| datadir | C:\\yhd\\mysql\\Data\\ |+---------------+--------------------+1 row in set, 1 warning (0.00 sec) 3.数据目录的结构3.1 数据库在文件系统中的表示每个数据库都对应着数据目录下的一个子目录，或者说对应着一个文件夹。当我们创建一个新的数据库的时候，MySQL会帮助我们做两件事： 在数据目录下创建一个与数据库同名的文件目录 在该子目录下创建一个db.opt文件。这个文件中包含了数据库的一些属性，比如该数据库的字符集和比较规则。 下面来看一下我的MySQL中的数据库： 1234567891011mysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || mysql || performance_schema || sys || yhd |+--------------------+5 rows in set (0.00 sec) 再从数据目录里看一下： 123456789101112131415161718192021222324252627282930313233C:\\yhd\\mysql\\Data&gt;dir 驱动器 C 中的卷没有标签。 卷的序列号是 CA5F-90F5 C:\\yhd\\mysql\\Data 的目录2021/12/19 00:27 &lt;DIR&gt; .2021/12/19 00:27 &lt;DIR&gt; ..2021/12/14 00:00 56 auto.cnf2021/12/14 00:00 1,703 ca-key.pem2021/12/14 00:00 1,131 ca.pem2021/12/14 00:00 1,131 client-cert.pem2021/12/14 00:00 1,707 client-key.pem2021/12/14 00:12 696 DESKTOP-NJIMTJP-slow.log2021/12/18 01:15 25,345 DESKTOP-NJIMTJP.err2021/12/18 01:15 5 DESKTOP-NJIMTJP.pid2021/12/19 02:18 79,691,776 ibdata12021/12/18 01:15 12,582,912 ibtmp12021/12/18 01:15 356 ib_buffer_pool2021/12/19 02:18 50,331,648 ib_logfile02021/12/19 02:18 50,331,648 ib_logfile12021/12/14 00:00 &lt;DIR&gt; mysql2021/12/14 00:00 &lt;DIR&gt; performance_schema2021/12/14 00:00 1,707 private_key.pem2021/12/14 00:00 461 public_key.pem2021/12/14 00:00 1,131 server-cert.pem2021/12/14 00:00 1,707 server-key.pem2021/12/14 00:00 &lt;DIR&gt; sys2021/12/19 00:56 &lt;DIR&gt; yhd 17 个文件 192,975,120 字节 6 个目录 189,156,126,720 可用字节C:\\yhd\\mysql\\Data&gt; 仔细看会发现，除了information_schema这个数据库以外，其他的数据库都对应一个文件目录，这个数据库有点特殊，后面在具体分析。 3.2 表在文件系统中的表示我们的数据其实是以记录的形式插入到表中的。每个表的信息其实可以分为两种。 表结构信息 表数据信息 为了保存表结构信息，InnoDB,MyISAM这两种存储引擎都会在数据目录下对应的数据库子目录中创建一个专门用于描述表结构的文件，文件名是表名.frm。这个文件是二进制格式的，直接打开会乱码。 我们知道不同的存储引擎对于表中的数据存储是不一样的，接下来我们分别来看一下InnoDB,MyISAM是如何存储表中的数据的。 InnoDb是如何存储数据的我们再来回顾下上一篇的知识： innoDB其实是使用页来作为基本单位管理存储空间的，默认大小16KB。 对于InnoDB存储引擎来说，每个索引都对应一颗B+树，该B+树的每个结点都是一个数据页。数据页之间没有必要是物理连续的，因为数据页之间有双向链表来维护这些页的顺序。 InnoDB的聚簇索引的叶子结点存储了完整的用户记录，也就是所谓的索引即数据，数据即索引。 为了更好的管理这些页，InnoDB提出了表空间或者文件空间的概念。这个表空间是一个抽象的概念，他可以对应文件系统上一个或者多个真实文件（不同表空间对应的文件数量可能不同）。每一个表空间可以被划分为很多个页，表数据被存放在某个表空间下的某些页中。InnoDB将表空间划分几种不同的类型，我们一个个分析一下子。 系统表空间 这个系统表空间可以对应文件系统上一个或者多个实际的文件。在默认情况下，InnoDB会在数据目录下创建一个名为ibdata1，大小为12MB的文件，这个文件就是对应的系统表空间在文件系统上的表示。怎么才12MB？这是因为这个文件是自扩展文件，也就是当不够用的时候会自己增加文件大小。 当然，如果想让系统表空间对应文件系统上的多个实际文件，或者仅仅觉得原来的ibdata1这个文件名难听，那么可以在MySQL服务启动的时候，配置对应的文件路径以及他们的大小。比如像下面这样修改配置文件： 12[server]innodb_data_file_path=data1:512M;data2:512M:autoextend 这样，在MySQL启动之后会创建data1和data2这两个各自512MB大小的文件作为系统表空间。其中的autoextend表明，如果这两个文件不够用，则会自动扩展data2文件的大小。 我们也可以把系统表空间对应的文件路径不配置到数据目录下，甚至可以配置到单独的磁盘分区上，涉及到的启动参数就是innodb_data_file_path和innodb_data_home_dir。 需要注意的一点是，在一个MySQL服务器中，系统表空间只有一份。从MySQL5.5.7到MySQL5.6.6之间的各个版本中，我们表中的数据都会被默认存储到这个 **系统表空间**。 独立表空间 在MySQL5.6.6以及以后的版本中，InnoDB不在默认把各个表的数据存储到系统表空间，而是为每一个表建立一个独立的表空间，也就是说，创建多少张表就会对应多少个表空间。在使用独立表空间来存储表数据的时候，会在该表所属的数据库对应的子目录下创建一个表示该独立表空间的文件，其文件名和表名相同，只不过添加了一个.ibd扩展名。所以完整的文件名称：表名.ibd。 假如我们使用独立表空间来存储yhd数据库下的person_info表，那么在该数据库所对应的yhd文件目录下会为person_info表创建下面两个文件：person_info.frm,person_info.ibd。 其中ibd文件用来存储表中的数据。当然也可以自己指定是使用系统表空间还是独立表空间来存储数据。 其他类型表空间 除了上述两种表空间之外，还有一些不同类型的表空间，比如通用表空间，undo表空间，临时表空间。 MyISAM是如何存储数据的索引和数据在InnoDB是一回事，但是MyISAM中的索引相当于全部都是二级索引，该存储引擎的数据和索引是分开存放的。所以在文件系统中也是使用不同的文件来存储数据文件和索引文件，而且与InnoDB不同的是，MyISAM并没有什么表空间一说，表的数据和索引都存放到对应的数据库子目录下。 假设我们person_info表使用的是MyISAM存储引擎，那么在它所在数据库对应的yhd文件目录下会为person_info创建三个文件：person_info.frm,person_info.MYD,person_info.MYI。 其中person_info.MYD表示表的数据文件，也就是插入的用户记录，person_info.MYI表示表的索引文件，我们为该表创建的索引都会放到这个文件中。 3.3 其他的文件除了我们上边说的这些用户自己存储的数据以外，数据目录下还包括为了更好运行程序的一些额外文件，主要包括这几种类型的文件： 服务器进程文件。我们知道每运行一个MySQL服务器程序，都意味着启动一个进程。MySQL服务器会把自己的进程ID写入到一个文件中。 服务器日志文件。在服务器运行过程中，会产生各种各样的日志，比如常规的查询日志、错误日志、二进制日志、redo日志等各种日志，这些日志各有各的用途，现在先了解一下就可以了。 默认/自动生成的SSL和RSA证书和密钥文件。主要是为了客户端和服务器安全通信而创建的一些文件 4.文件系统对数据库的影响因为MySQL的数据都是存在文件系统中的，就不得不受到文件系统的一些制约，这在数据库和表的命名、表的大小和性能方面体现的比较明显，比如下边这些方面： 数据库名称和表名称不得超过文件系统所允许的最大长度。每个数据库都对应数据目录的一个子目录，数据库名称就是这个子目录的名称；每个表都会在数据库子目录下产生一个和表名同名的.frm文件，如果是InnoDB的独立表空间或者使用MyISAM引擎还会有别的文件的名称与表名一致。这些目录或文件名的长度都受限于文件系统所允许的长度～ 特殊字符的问题为了避免因为数据库名和表名出现某些特殊字符而造成文件系统不支持的情况，MySQL会把数据库名和表名中所有除数字和拉丁字母以外的所有字符在文件名里都映射成 @+编码值的形式作为文件名。比方说我们创建的表的名称为&#39;test?&#39;，由于?不属于数字或者拉丁字母，所以会被映射成编码值，所以这个表对应的.frm文件的名称就变成了test@003f.frm。 文件长度受文件系统最大长度限制对于InnoDB的独立表空间来说，每个表的数据都会被存储到一个与表名同名的.ibd文件中；对于MyISAM存储引擎来说，数据和索引会分别存放到与表同名的.MYD和.MYI文件中。这些文件会随着表中记录的增加而增大，它们的大小受限于文件系统支持的最大文件大小。 5.MySQL系统数据库简介我们前边提到了MySQL的几个系统数据库，这几个数据库包含了MySQL服务器运行过程中所需的一些信息以及一些运行状态信息，我们现在稍微了解一下。 mysql这个数据库贼核心，它存储了MySQL的用户账户和权限信息，一些存储过程、事件的定义信息，一些运行过程中产生的日志信息，一些帮助信息以及时区信息等。 information_schema这个数据库保存着MySQL服务器维护的所有其他数据库的信息，比如有哪些表、哪些视图、哪些触发器、哪些列、哪些索引等等。这些信息并不是真实的用户数据，而是一些描述性信息，有时候也称之为元数据。 performance_schema这个数据库里主要保存MySQL服务器运行过程中的一些状态信息，算是对MySQL服务器的一个性能监控。包括统计最近执行了哪些语句，在执行过程的每个阶段都花费了多长时间，内存的使用情况等等信息。 sys这个数据库主要是通过视图的形式把information_schema和performance_schema结合起来，让程序员可以更方便的了解MySQL服务器的一些性能信息。 二，回顾前面数据页（也就是Index类型的页）由7部分组成，其中有两个部分是所有类型的页面都通用的。 所有类型的页都会包含下面两个部分。 File Header：记录页面的一些通用信息 File Trailer: 校验页是否完整，保证页面在从内存刷新到磁盘后内容是相同的 名称 占用空间大小（字节） 描述 FIL_PAGE_SPACE_OR_CHKSUM 4 页的校验和（checksum值） FIL_PAGE_OFFSET 4 页号 FIL_PAGE_PREV 4 上一个页的页号 FIL_PAGE_NEXT 4 下一个页的页号 FIL_PAGE_LSN 8 页面被最后修改时对应的日志序列位置 FIL_PAGE_TYPE 2 该页的类型 FIL_PAGE_FILE_FLUSH_LSN 8 仅仅在系统表空间的一个页中定义，代表文件至少被刷新到了对应的LSN值 FIL_PAGE_ARCH_LOG_NO_OR_SPACE_ID 4 页属于哪个表空间 表空间中的每一个页都对应着一个页号，也就是FIL_PAGE_OFFSET，这个页号由4个字节组成，也就是32个比特位，所以一个表空间最多可以拥有2³²个页，如果按照页的默认大小16KB来算，一个表空间最多支持64TB的数据。表空间的第一个页的页号为0，之后的页号分别是1，2，3…依此类推 某些类型的页可以组成链表，链表中的页可以不按照物理顺序存储，而是根据FIL_PAGE_PREV和FIL_PAGE_NEXT来存储上一个页和下一个页的页号。需要注意的是，这两个字段主要是为了INDEX类型的页，也就是我们之前一直说的数据页建立B+树后，为每层节点建立双向链表用的，一般类型的页是不使用这两个字段的。 每个页的类型由FIL_PAGE_TYPE表示，比如像数据页的该字段的值就是0x45BF，不同类型的页在该字段上的值是不同的。 InnoDB支持许多种类型的表空间，我们暂时重点关注系统表空间和独立表空间的结构。他们结构比较相似，但是由于系统表空间中额外包含了一些关于整个系统的信息，所以我们先分析独立表空间，再说系统表空间。 三，独立表空间1.区的概念为了更好的管理表中的页，InnoDB提出了区的概念。对于16KB的页来说，连续的64个页就是一个区。也就是说一个区默认占用1M空间。不论是系统表空间还是独立表空间，都可以看成是由若干区组成的，每256个区划分为一个组。 为什么要有区的概念？ 从理论上来讲，不引入区的概念只使用页的概念对存储引擎的运行并没有任何影响，但是我们来分析下： 我们每向表中插入一条记录，本质上就是向该表的聚簇索引以及所有二级索引代表的B+树的节点中插入数据。而B+树的每一层中的页都会形成一个双向链表，如果是以页为单位来分配存储空间的话，双向链表相邻的两个页之间的物理位置可能离得非常远。 B+树的范围查询只需要定位到最左边的记录和最右边的记录，然后沿着双向链表一直扫描就可以了，而如果链表中相邻的两个页物理位置离得非常远，就是所谓的随机I/O。磁盘的速度和内存的速度差了好几个数量级，随机I/O是非常慢的，所以我们应该尽量让链表中相邻的页的物理位置也相邻，这样进行范围查询的时候才可以使用所谓的顺序I/O。 所以才引入了区（extent）的概念，一个区就是在物理位置上连续的64个页。在表中数据量大的时候，为某个索引分配空间的时候就不再按照页为单位分配了，而是按照区为单位分配，甚至在表中的数据十分非常特别多的时候，可以一次性分配多个连续的区。虽然可能造成一点点空间的浪费（数据不足填充满整个区），但是从性能角度看，可以消除很多的随机I/O。 2.段的概念范围查询，其实是对B+树叶子节点中的记录进行顺序扫描，而如果不区分叶子节点和非叶子节点，统统把节点代表的页面放到申请到的区中的话，进行范围扫描的效果就大打折扣了。InnoDB对B+树的叶子节点和非叶子节点进行了区别对待：叶子节点有自己独有的区，非叶子节点也有自己独有的区。存放叶子节点的区的集合就算是一个段（segment），存放非叶子节点的区的集合也算是一个段。也就是说一个索引会生成2个段，一个叶子节点段，一个非叶子节点段。 默认情况下一个使用InnoDB存储引擎的表只有一个聚簇索引，一个索引会生成2个段，而段是以区为单位申请存储空间的，一个区默认占用1M存储空间，所以默认情况下一个只存了几条记录的小表也需要2M的存储空间么？以后每次添加一个索引都要多申请2M的存储空间么？ 为了考虑以完整的区为单位分配给某个段对于数据量较小的表太浪费存储空间的这种情况，InnoDB提出了碎片区的概念，也就是在一个碎片区中，并不是所有的页都是为了存储同一个段的数据而存在的，而是碎片区中的页可以用于不同的目的，比如有些页用于段A，有些页用于段B，有些页甚至哪个段都不属于。碎片区直属于表空间，并不属于任何一个段。所以此后为某个段分配存储空间的策略是这样的： 在刚开始向表中插入数据的时候，段是从某个碎片区以单个页面为单位来分配存储空间的 当某个段已经占用了32个碎片区页面之后，就会以完整的区为单位来分配存储空间 所以现在段不能仅定义为是某些区的集合，更精确的应该是某些零散的页面以及一些完整的区的集合。除了索引的叶子节点段和非叶子节点段之外，InnoDB中还有为存储一些特殊的数据而定义的段，比如回滚段，当然我们现在并不关心别的类型的段，现在只需要知道段是一些零散的页面以及一些完整的区的集合就好了。 有的时候处于不同的阶段，对于某个概念的定义或者理解是不同的，随着知识水平的提升后续再来逐渐完善，就像小学的时候老师会告诉你最小的数是0，中学又告诉你最小的数是负无穷一样。 3.区的分类每个区都对应一个XDES Entry结构，这个结构中存储了一些与这个区有关的属性。这些区可以被分为下面四种类型。 空闲的区：现在还没有用到这个区中的任何页面，这些区会被加入到FREE链表。 有剩余空间的碎片区：表示碎片区中还有可用的页面，这些区会被加入到FREE_FRAG链表。 没有剩余空间的碎片区：表示碎片区中的所有页面都被使用，没有空闲页面；这些区会被加入到FULL_FRAG链表。 附属于某个段的区。每一个索引都可以分为叶子节点段和非叶子节点段，除此之外InnoDB还会另外定义一些特殊作用的段，在这些段中的数据量很大时将使用区来作为基本的分配单位；每个段所属的区又会被组织成下面几种链表。 FREE链表：在同一个段中，所有页面都是空闲页面的区对应的XDES Entry结构会被加入到这个链表。 NOT_FULL链表：在同一个段中，仍有空闲页面的区对应的XDES Entry结构会被加入到这个链表。 FULL链表：在同一个段中，已经没有空闲页面的区对应的XDES Entry结构会被加入到这个链表。 这四种类型的区也被叫做区的四种状态。 状态名 含义 FREE 空闲的区 FREE_FRAG 有剩余空间的碎片区 FULL_FRAG 没有剩余空间的碎片区 FSEG 附属于某个段的区 处于**FREE**、**FREE_FRAG**以及**FULL_FRAG**这三种状态的区都是独立的，算是直属于表空间；而处于**FSEG**状态的区是附属于某个段的。 每个段都会对应一个INODE Entry结构，该结构中存储了一些与这个段有关的属性。 表空间中第一个页面的类型为FSP_HDR，它存储了表空间的一些整体属性以及第一个组内256个区对应的XDES Entry结构。 除了表空间的第一个组以外，其余组的第一个页面的类型为XDES，这种页面的结构和FSP_HDR类型的页面对比，除了少了File Space header（记录表空间整体属性的部分）部分之外，其余部分是一样的。 每个组的第二个页面类型为IBUF_BITMAP，存储了一些关于Change Buffer的信息。 表空间中第一个组的第三个页面的类型是INODE，他是为了存储INODE Entry结构而设计的，这种类型的页面会组织成下面两个链表。 SEG_INODES_FULL链表：在该链表中，INODE类型的页面中已经没有空闲空间来存储额外的INODE Entry结构。 SEG_INODES_FREE链表：在该链表中，INODE类型的页面中还有空闲空间来存储额外的INODE Entry结构。 4. Segment Header一个索引会产生两个段，分别是叶子节点段和非叶子节点段，而每个段都会对应一个INODE Entry结构，那我们怎么知道某个段对应哪个**INODE Entry**结构呢？所以得找个地方记下来这个对应关系。INDEX类型的页时有一个Page Header部分，其中的PAGE_BTR_SEG_LEAF和PAGE_BTR_SEG_TOP都占用10个字节，它们其实对应一个叫Segment Header的结构，该结构图示如下： 各个部分的具体释义如下： 名称 占用字节数 描述 Space ID of the INODE Entry 4 INODE Entry结构所在的表空间ID Page Number of the INODE Entry 4 INODE Entry结构所在的页面页号 Byte Offset of the INODE Ent 2 INODE Entry结构在该页面中的偏移量 PAGE_BTR_SEG_LEAF记录着叶子节点段对应的INODE Entry结构的地址是哪个表空间的哪个页面的哪个偏移量，PAGE_BTR_SEG_TOP记录着非叶子节点段对应的INODE Entry结构的地址是哪个表空间的哪个页面的哪个偏移量。这样子索引和其对应的段的关系就建立起来了。不过需要注意的一点是，因为一个索引只对应两个段，所以只需要在索引的根页面中记录这两个结构即可。 其实Segment Header的作用就是记录哪个段对应哪个INODE Entry结构的。 5. 真实表空间对应的文件大小一个新建的表对应的.ibd文件只占用了96KB，才6个页的大小。刚开始的时候，表空间占用空间自然很小，因为表里面没有数据。不过，ibd文件是自扩展文件，随着数据的增多文件也在逐渐增大。 四，系统表空间系统表空间的结构和独立表空间基本类似，只不过由于整个MySQL进程只有一个系统表空间，在系统表空间中会额外记录一些有关整个系统信息的页面，所以会比独立表空间多出一些记录这些信息的页面。因为这个系统表空间相当于是表空间之首，所以它的表空间 ID（Space ID）是0。 1.系统表空间的整体结构系统表空间与独立表空间的一个非常明显的不同之处就是在表空间开头有许多记录整个系统属性的页面。 可以看到，系统表空间和独立表空间的前三个页面（页号分别为0、1、2，类型分别是FSP_HDR、IBUF_BITMAP、INODE）的类型是一致的，只是页号为3～7的页面是系统表空间特有的，我们来看一下这些多出来的页面都是干啥使的： 页号 页面类型 英文描述 描述 3 SYS Insert Buffer Header 存储Insert Buffer的头部信息 4 INDEX Insert Buffer Root 存储Insert Buffer的根页面 5 TRX_SYS Transaction System 事务系统的相关信息 6 SYS First Rollback Segment 第一个回滚段的页面 7 SYS Data Dictionary Header 数据字典头部信息 除了这几个记录系统属性的页面之外，系统表空间的extent 1和extent 2这两个区，也就是页号从64~`191这128个页面被称为Doublewrite buffer`，也就是双写缓冲区。不过上述的大部分知识都涉及到了事务和多版本控制的问题，现在我们只分析有关InnoDB数据字典的知识，其余的概念在后边再看。 1.1 InnoDB数据字典每当我们向一个表中插入一条记录的时候，MySQL先要校验一下插入语句对应的表存不存在，插入的列和表中的列是否符合，如果语法没有问题的话，还需要知道该表的聚簇索引和所有二级索引对应的根页面是哪个表空间的哪个页面，然后把记录插入对应索引的B+树中。所以说，MySQL除了保存着我们插入的用户数据之外，还需要保存许多额外的信息，比方说： 某个表属于哪个表空间，表里边有多少列 表对应的每一个列的类型是什么 该表有多少索引，每个索引对应哪几个字段，该索引对应的根页面在哪个表空间的哪个页面 该表有哪些外键，外键对应哪个表的哪些列 某个表空间对应文件系统上文件路径是什么 上述这些数据并不是我们使用INSERT语句插入的用户数据，实际上是为了更好的管理我们这些用户数据而不得已引入的一些额外数据，这些数据也称为元数据。InnoDB存储引擎特意定义了一些列的内部系统表（internal system table）来记录这些这些元数据： 表名 描述 SYS_TABLES 整个InnoDB存储引擎中所有的表的信息 SYS_COLUMNS 整个InnoDB存储引擎中所有的列的信息 SYS_INDEXES 整个InnoDB存储引擎中所有的索引的信息 SYS_FIELDS 整个InnoDB存储引擎中所有的索引对应的列的信息 SYS_FOREIGN 整个InnoDB存储引擎中所有的外键的信息 SYS_FOREIGN_COLS 整个InnoDB存储引擎中所有的外键对应列的信息 SYS_TABLESPACES 整个InnoDB存储引擎中所有的表空间信息 SYS_DATAFILES 整个InnoDB存储引擎中所有的表空间对应文件系统的文件路径信息 SYS_VIRTUAL 整个InnoDB存储引擎中所有的虚拟生成列的信息 这些系统表也被称为数据字典，它们都是以B+树的形式保存在系统表空间的某些页面中，其中SYS_TABLES、SYS_COLUMNS、SYS_INDEXES、SYS_FIELDS这四个表尤其重要，称之为基本系统表（basic system tables），我们先看看这4个表的结构： 1.2 SYS_TABLES表 列名 描述 name 表的名称 id InnoDB存储引擎每一张表都有一个唯一的ID n_cols 该表拥有的列的个数 type 表的类型，记录了一些文件格式，行格式，压缩等信息 Mix_id 已经过时，忽略 Mix_len 表的一些额外属性 Cluster_id 未使用，忽略 Space 该表所属空间的ID 这个SYS_TABLES表有两个索引： 以NAME列为主键的聚簇索引 以ID列建立的二级索引 1.3 SYS_COLUMNS表 列名 描述 TABLE_ID 该列所属表对应的ID POS 该列在表中是第几列 NAME 该列的名称 MTYPE main data type，主数据类型，就是那堆INT、CHAR、VARCHAR、FLOAT、DOUBLE等 PRTYPE precise type，精确数据类型，就是修饰主数据类型的那堆东东，比如是否允许NULL值，是否允许负数啥的 LEN 该列最多占用存储空间的字节数 PREC 该列的精度，不过这列貌似都没有使用，默认值都是0 这个SYS_COLUMNS表只有一个聚集索引： 以(TABLE_ID, POS)列为主键的聚簇索引 1.4 SYS_INDEXES表 列名 描述 TABLE_ID 该索引所属表对应的ID ID InnoDB存储引擎中每个索引都有一个唯一的ID NAME 该索引的名称 N_FIELDS 该索引包含列的个数 TYPE 该索引的类型，比如聚簇索引、唯一索引、更改缓冲区的索引、全文索引、普通的二级索引等等各种类型 SPACE 该索引根页面所在的表空间ID PAGE_NO 该索引根页面所在的页面号 MERGE_THRESHOLD 如果页面中的记录被删除到某个比例，就把该页面和相邻页面合并，这个值就是这个比例 这个SYS_INDEXES表只有一个聚集索引： 以(TABLE_ID, ID)列为主键的聚簇索引 1.5 SYS_FIELDS表 列名 描述 INDEX_ID 该索引列所属的索引的ID POS 该索引列在某个索引中是第几列 COL_NAME 该索引列的名称 这个SYS_FIELDS表只有一个聚集索引： 以(INDEX_ID, POS)列为主键的聚簇索引 1.6 Data Dictionary Header页面只要有了上述4个基本系统表，也就意味着可以获取其他系统表以及用户定义的表的所有元数据。比方说我们想看看SYS_TABLESPACES这个系统表里存储了哪些表空间以及表空间对应的属性，那就可以： 到SYS_TABLES表中根据表名定位到具体的记录，就可以获取到SYS_TABLESPACES表的TABLE_ID 使用这个TABLE_ID到SYS_COLUMNS表中就可以获取到属于该表的所有列的信息。 使用这个TABLE_ID还可以到SYS_INDEXES表中获取所有的索引的信息，索引的信息中包括对应的INDEX_ID，还记录着该索引对应的B+数根页面是哪个表空间的哪个页面。 使用INDEX_ID就可以到SYS_FIELDS表中获取所有索引列的信息。 这4个表的元数据去哪里获取呢？这4个表的元数据，就是它们有哪些列、哪些索引等信息是硬编码到代码中的，InnoDB用一个固定的页面来记录这4个表的聚簇索引和二级索引对应的B+树位置，这个页面就是页号为7的页面，类型为SYS，记录了Data Dictionary Header，也就是数据字典的头部信息。除了这4个表的5个索引的根页面信息外，这个页号为7的页面还记录了整个InnoDB存储引擎的一些全局属性。 这个页面由下边几个部分组成： 名称 中文名 占用空间（字节） 简单描述 File Header 文件头部 38 页的一些通用信息 Data Dictionary Header 数据字典头部信息 56 记录一些基本系统表的根页面位置以及InnoDB存储引擎的一些全局信息 Segment Header 段头部信息 10 记录本页面所在段对应的INODE Entry位置信息 Empty Space 尚未使用空间 16272 用于页结构的填充，没啥实际意义 File Trailer 文件尾部 8 校验页是否完整 这个页面里有Segment Header部分，意味着InnoDB把这些有关数据字典的信息当成一个段来分配存储空间，我们称之为数据字典段。由于目前我们需要记录的数据字典信息非常少（可以看到Data Dictionary Header部分仅占用了56字节），所以该段只有一个碎片页，也就是页号为7的这个页。 接下来我们需要看一下Data Dictionary Header部分的各个字段： Max Row ID：如果我们不显式的为表定义主键，而且表中也没有UNIQUE索引，那么InnoDB存储引擎会默认为我们生成一个名为row_id的列作为主键。因为它是主键，所以每条记录的row_id列的值不能重复。原则上只要一个表中的row_id列不重复就可以了，也就是说表a和表b拥有一样的row_id列也没啥关系，不过InnoDB只提供了这个Max Row ID字段，不论哪个拥有row_id列的表插入一条记录时，该记录的row_id列的值就是Max Row ID对应的值，然后再把Max Row ID对应的值加1，也就是说这个Max Row ID是全局共享的。 Max Table ID：InnoDB存储引擎中的所有的表都对应一个唯一的ID，每次新建一个表时，就会把本字段的值作为该表的ID，然后自增本字段的值。 Max Index ID：InnoDB存储引擎中的所有的索引都对应一个唯一的ID，每次新建一个索引时，就会把本字段的值作为该索引的ID，然后自增本字段的值。 Max Space ID：InnoDB存储引擎中的所有的表空间都对应一个唯一的ID，每次新建一个表空间时，就会把本字段的值作为该表空间的ID，然后自增本字段的值。 Mix ID Low(Unused)：这个字段没啥用，跳过。 Root of SYS_TABLES clust index：本字段代表SYS_TABLES表聚簇索引的根页面的页号。 Root of SYS_TABLE_IDS sec index：本字段代表SYS_TABLES表为ID列建立的二级索引的根页面的页号。 Root of SYS_COLUMNS clust index：本字段代表SYS_COLUMNS表聚簇索引的根页面的页号。 Root of SYS_INDEXES clust index本字段代表SYS_INDEXES表聚簇索引的根页面的页号。 Root of SYS_FIELDS clust index：本字段代表SYS_FIELDS表聚簇索引的根页面的页号。 Unused：这4个字节没用，跳过。 以上就是页号为7的页面的全部内容。 1.7 information_schema系统数据库用户是不能直接访问InnoDB的这些内部系统表的，除非你直接去解析系统表空间对应文件系统上的文件。不过InnoDB考虑到查看这些表的内容可能有助于大家分析问题，所以在系统数据库information_schema中提供了一些以innodb_sys开头的表： 12345678910111213141516171819mysql&gt; USE information_schema;Database changedmysql&gt; SHOW TABLES LIKE &#x27;innodb_sys%&#x27;;+--------------------------------------------+| Tables_in_information_schema (innodb_sys%) |+--------------------------------------------+| INNODB_SYS_DATAFILES || INNODB_SYS_VIRTUAL || INNODB_SYS_INDEXES || INNODB_SYS_TABLES || INNODB_SYS_FIELDS || INNODB_SYS_TABLESPACES || INNODB_SYS_FOREIGN_COLS || INNODB_SYS_COLUMNS || INNODB_SYS_FOREIGN || INNODB_SYS_TABLESTATS |+--------------------------------------------+10 rows in set (0.00 sec) 在information_schema数据库中的这些以INNODB_SYS开头的表并不是真正的内部系统表，而是在存储引擎启动时读取这些以SYS开头的系统表，然后填充到这些以INNODB_SYS开头的表中。以INNODB_SYS开头的表和以SYS开头的表中的字段并不完全一样。​ 补充一张表空间完整结构图","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/tags/MySQL/"}]},{"title":"MySQL[四]索引命中原理","slug":"MySQL/MySQL[四]索引命中原理","date":"2022-01-11T03:15:31.769Z","updated":"2022-01-11T03:19:35.568Z","comments":true,"path":"2022/01/11/MySQL/MySQL[四]索引命中原理/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/MySQL/MySQL[%E5%9B%9B]%E7%B4%A2%E5%BC%95%E5%91%BD%E4%B8%AD%E5%8E%9F%E7%90%86/","excerpt":"","text":"上一篇分析了InnoDB存储引擎的B+树索引，现在来进行一个简单的回顾。 每个索引都对应一颗B+树，B+树分为好多层，最下边一层是叶子结点，其余的是内结点。所有的用户记录都存储在B+树的叶子结点，所有目录项记录都存储在内节点。 InnoDB存储引擎会自动为主键建立聚簇索引，聚簇索引的叶子结点包含完整的用户记录。 我们可以为指定的列建立二级索引，二级索引的叶子结点包含的用户记录由索引列+主键组成，所以如果项通过二级索引来查找完整的用户记录的话，需要通过回表操作，也就是在通过二级索引找到主键值之后再到聚簇索引中查找完整的用户记录。 B+树中每层结点都是按照索引列值从大到小的顺序排序而组成了双向链表，而且每个页内的记录(不论是用户记录还是目录项记录)都是按照索引列的值从小到大的顺序而形成了一个单链表。如果是联合索引的话，则页面和记录先按照联合索引前边的列排序，如果该列值相同，在按照联合索引后边的列排序。 通过索引查找记录是从B+树的根节点开始，一层一层乡下搜索。由于每个页面都按照索引列的值建立了页目录，所以在这些页面中查找非常快。 一，做一些前置的准备为了这篇文章的演示，需要先建立一张简单的表，用来演示索引执行过程中出现的一些情况。 123456789101112131415161718192021create table single_table( # 主键索引 id int primary key auto_increment, key1 varchar(100), key2 int, key3 varchar(100), key_part1 varchar(100), key_part2 varchar(100), key_part3 varchar(100), common_field varchar(100), # 索引列key1 key idx_key1 (key1), # 唯一索引：索引列key2 unique key uk_key2 (key2), # 索引列key3 key idx_key3 (key3), # 联合索引：索引列：key_part1, key_part2, key_part3 key idx_key_part (key_part1, key_part2, key_part3)) engine = innodb charset = utf8; 再往表中插入100w条数据，具体的插入过程不在演示。 二，索引的代价凡事都是有利有弊的，索引可以加快查询的速度，但是同样的，他也有相应的缺点。 空间上 一个索引对应一个B+树，每一个B+树的每一个节点都是一个数据页。一个数据页大小默认是16kb，所以一张表的索引越多，占用的空间其实越大，特别是在数据量大的时候，所以一般我们建立索引，默认每张表不要超过5个。 时间上 在对表进行增删改操作的时候，要对所有索引对应的B+树进行修改。而且上一篇分析过，B+树的每一层节点都按照索引列的值从小到大的顺序排序组成了双向链表。页中的记录都按照索引列的值从小到大的顺序形成了一个单向链表。而增删改操作可能会对结点和记录的排序造成破坏，所以存储引擎需要额外的时间进行页面分裂，页回收等操作，好维护结点和记录的排序。索引越多，维护的时间成本越高。 还有一点就是执行查询语句之前，会先生成执行计划。一般情况下一条语句再一次执行过程中只会使用一个二级索引(有特殊的，后面会分析)，在生成执行计划的时候需要计算使用不同索引执行查询时所需成本，最后选取成本最低的索引执行查询。如果索引太多，分析成本就会很高，耗时严重，从而影响查询语句的执行性能。 为了合理的建立索引，一方面加快我们的查询速度，一方面又不会过分的占用我们的时间和空间，我们需要了解索引在查询执行期间到底是如何发挥作用的。 三，使用B+树索引1.扫描区间和边界条件先说什么是全表扫描？就是从头到尾依次遍历所有结点，再依次遍历结点中的所有记录。全表扫描虽然效率很低，但是却是一种万能的解决方案，所有查询都可以使用这种方案兜底。 我们可以利用B+树查找索引列值等于某个值的记录，这样可以明显减少需要扫描的记录数量。由于B+树叶子节点中的记录是按照索引列值从小到大的顺序排序的，所以只扫描某个区间或某些区间中的记录也是很快的，比如下面这个查询语句： 1explain select * from single_table where id&gt;=2 and id&lt;=100; 这个时候其实是走了主键索引，这个语句其实是想查找id值在区间【2，100】内的所有聚簇索引记录，我们可以通过主键索引先定位到id=2的记录，然后顺着这条记录的单向链表往后找就行了。 与全表扫描的100w数据相比，扫描这个区间的成本简直太小了，所以提升了查询效率。我们把这个案例中待扫描的id值所在区间称为扫描区间，把形成这个扫描区间的搜索条件称为形成这个扫描区间的边界条件。 其实对于全表扫描来讲，就是在【-∞，+∞】的区间进行扫描而已。 再来看一条查询语句： 12345explainselect *from single_tablewhere key2 in (1438, 6328) or (key2 &gt;= 38 and key2 &lt;= 79); 这个查询的搜索条件涉及到key2列，我们正好在key2列上建立了唯一索引。如果使用唯一索引执行这个查询，实际上相当于从三个区间获取二级索引的记录。 【1438，1438】 【6328，6328】 【38，79】 类似前面两个区间这种，只有一个值的区间，我们称为单点扫描区间，把类似第三个区间这样存在多个值的叫做范围扫描区间，另外，由于我们的查询列是*，导致从上述的区间每次获取到一条二级索引记录，就需要根据二级索引记录的id列的值取回表一次。 当然，并不是所有的条件都可以称为边界条件，比如下面的查询语句： 123456explainselect *from single_tablewhere key1 &gt; &#x27;aaa&#x27; and key3 &lt; &#x27;zzz&#x27; and common_field = &#x27;aaa&#x27;; 如果使用idx_key1执行查询，那么相应的扫描区间就变成了【’aaa’,+∞】，后面的条件就是普通搜索条件，这些普通的搜索条件需要在获取到idx_key1的二级索引记录后，在执行回表操作，在获取到完整的用户记录后才能去判断他们是否成立。 如果使用idx_key3执行查询，那么扫描区间就是【-∞,’zzz’】,其余的条件就是普通搜索条件，这些普通的搜索条件需要在获取到idx_key3的二级索引记录后，在执行回表操作，在获取到完整的用户记录后才能去判断他们是否成立。 在使用某个索引执行查询的时候，关键的问题就是通过搜索条件找出合适的区间，然后再去对应的B+树中扫描索引列值在这些扫描区间的记录，对于每一个区间来说，只需要定位到第一条，就可以沿着单链表一直往后扫符合条件的记录。 其实对于B+树索引来说： = &lt;=&gt; in not in is null is not null &gt; &lt; &gt;= &lt;= between != like 都会进行区间扫描，只不过区间扫描大小不同导致效率不同。 不过也有一些需要注意的点： in和多个 = 用or连接起来的效果其实是一样的，都会产生多个单点扫描区间 不等于 产生的区间比较操蛋： 1select * from single_table where key1 != &#x27;aaa&#x27; 这个时候idx_key1 执行查询的时候对应的扫描区间就是【-∞,’aaa’】和【’aaa’,+∞】。 like操作比较特殊，只有在匹配完整的字符串或者匹配字符串前缀的时候才会产生合适的扫描区间 比较字符串的大小其实就是逐个比较每个字符的大小。字符串的比较过程如下： 先比较字符串的第一个字符，第一个字符串小的字符就比较小 如果第一个字符一样的话就按照上面的规则比较第二个，以此类推。 对于某个索引列来说，字符串前缀相同的记录在由记录组成的单向链表中肯定是相邻的。比如我们有一个搜索条件是key1 like &#39;a%&#39;，对于二级索引idx_key1来说，所有字符串前缀为a的二级索引记录肯定是相邻的。这也就意味着我们只要定位到key1值得字符串前缀为a的第一条记录，就可以依次往后扫描，直到某条二级索引记录的字符串不是a为止。 很显然，key1 like &#39;a%&#39;形成的扫描区间相当于【’a’,’b’】。 在执行一个查询语句的时候，首先需要找出所有可用的索引以及使用他们时对应的扫描区间。接下来我们分析下怎么从包含若干个and或者or的复杂搜索条件中提取出正确的扫描区间。 1.1 所有搜索条件都可以生成合适的扫描区间的情况在使用某个索引执行查询的时候，有时每个小的搜索条件都可以生成一个合适的扫描区间来减少需要扫描的记录数量。 12345explainselect *from single_tablewhere key2 &gt; 100 and key2 &gt; 200; 在使用唯一索引进行查询的时候，这两个条件都可以形成一个扫描区间【100，+∞】，【200，+∞】。因为这两个条件是用and连接的，所以最终就是两个区间取交集【200，+∞】。 我们把sql改一改： 12345explainselect *from single_tablewhere key2 &gt; 100 or key2 &gt; 200; 这个时候因为是使用or进行两个条件的连接，所以两个条件的区间应该取并集：【100，+∞】。 1.2 有的搜索条件不能生成合适的扫描区间的情况在使用某个索引进行查询的时候，有些小的搜索条件并不能生成合适的扫描区间来减少需要扫描的行数。 12345explainselect *from single_tablewhere key2 &gt; 100 and common_field =&#x27;abc&#x27;; 在使用唯一索引进行查询的时候，第一个条件会定位出区间【100，+∞】，但是第二个条件是一个普通条件，相当于【-∞，+∞】，因为两个条件使用and连接的，所以最终取交集之后的区间就是【100，+∞】。 其实在使用唯一索引进行查询的时候，在寻找对应的扫描区间的过程中，搜索条件common_field =&#39;abc&#39;没有起到任何作用，我们可以直接把这个条件进行一个等价替换【TRUE】(true对应的扫描区间也是【-∞，+∞】)。 12345explainselect *from single_tablewhere key2 &gt; 100 and true; 在进行化简之后就变成： 1234explainselect *from single_tablewhere key2 &gt; 100 也就是说上面的查询语句在使用唯一索引进行查询的时候对应的扫描区间就是【100，+∞】。 再来看一下使用OR的情况： 12345explainselect *from single_tablewhere key2 &gt; 100 or common_field =&#x27;abc&#x27;; 同样进行化简 12345explainselect *from single_tablewhere key2 &gt; 100 or true; 继续化简 1234explainselect *from single_tablewhere true 可见如果此时强制使用唯一索引进行查询，对应的扫描区间就是【-∞，+∞】，再加上这是二级索引，每次匹配到一条都要进行回表，所以这个查询的代价甚至比全表扫描还大，这个时候再使用唯一索引就没意义了。 1.3从复杂的搜索条件中找出扫描区间来一个复杂点的条件： 12345select *from single_tablewhere (key1 &gt; &#x27;aaa&#x27; and key2 &gt; 748) or (key1 &lt; &#x27;eee&#x27; and key1 &gt; &#x27;ccc&#x27;) or (key1 like &#x27;%f&#x27; and key1 &gt; &#x27;aaa&#x27; and (key2 &lt; 8000 or common_field = &#x27;aaa&#x27;)); 这无语的SQL怎么搞？ 先看where子句里面都涉及到了哪些列，以及我们为哪些列建立了索引 对于可以用到的索引，我们来分析索引的扫描区间 1.3.1 使用idx_key1查询先把不能形成合适扫描区间的搜索条件干掉，怎么干掉？直接把他们替换成TRUE。 替换之后的效果： 12345select *from single_tablewhere (key1 &gt; &#x27;aaa&#x27; and TRUE) or (key1 &lt; &#x27;eee&#x27; and key1 &gt; &#x27;ccc&#x27;) or (TRUE and key1 &gt; &#x27;aaa&#x27; and (TRUE or TRUE)); 化简之后的结果： 1234select *from single_tablewhere (key1 &gt; &#x27;aaa&#x27; ) -- 【aaa,+∞】 or (key1 &lt; &#x27;eee&#x27; and key1 &gt; &#x27;ccc&#x27;) -- 【&#x27;ccc&#x27;,&#x27;eee&#x27;】 因为这两个条件之间是用OR连接起来的，所以我们应该取并集，最终：【aaa,+∞】。 也就是需要把所有key1在这个区间内的所有二级索引记录都取出来，针对获取到的每一条二级索引记录进行一次回表，在得到完整的用户记录之后在使用其他的搜索条件进行过滤。 1.3.2 使用唯一二级索引查询我们还是进行化简 12345select *from single_tablewhere (TRUE and key2 = 748) or (TRUE and TRUE) or (TRUE and TRUE and (key2 &lt; 8000 or common_field = &#x27;aaa&#x27;)); 再继续化简 1234select *from single_tablewhere key2 = 748 or TRUE 因为两个条件使用OR连接的，所以最终的结果就是【-∞，+∞】。 也就是需要把所有key2所有二级索引记录都取出来，针对获取到的每一条二级索引记录进行一次回表，在得到完整的用户记录之后在使用其他的搜索条件进行过滤，比全表扫描还耗时，所以这个时候我们是不会走唯一二级索引的。 在使用idx_key1执行上述查询的时候，搜索条件 key1 like &#39;%f&#39; 比较特殊。虽然他不能作为形成扫描区间的边界条件，但是idx_key1的二级索引记录是包含key1列的。因此我们可以先判断获取到的二级索引记录是否符合这个条件。如果符合在执行回表操作，如果不符合就不用回表了。这样就可以较少因为回表带来的性能损耗，这就是索引下推。 1.4使用联合索引执行查询时对应的扫描区间联合索引的索引列包含多个列，B+树中的每一层页面以及每一个页中的记录采用的排序规则比较复杂。以上面的表为例，idx_key_part (key_part1, key_part2, key_part3) 采用的排序规则如下： 先按照key_part1进行排序 key_part1相同按照key_part2进行排序，以此类推 1.4.1全值匹配原理对于下面这条查询语句来讲： 123select *from single_tablewhere key_part1 =&#x27;a&#x27;; 因为二级索引记录先按照key_part1进行值排序的，所以符合条件的所有记录肯定是相邻的。我们可以先定位到符合条件的第一条记录，沿着链表顺序往下扫描知道不符合条件为止（当然，对于获取到的每一条二级索引记录都需要进行回表）。此时的扫描区间【’a’,’a’】。 在看一条查询语句： 1234select *from single_tablewhere key_part1 = &#x27;a&#x27; and key_part2 = &#x27;b&#x27;; 按照联合索引的排序规则，最终的扫描区间其实就是【(‘a’,’b’),(‘a’,’b’)】。 在看一条SQL： 123select *from single_tablewhere key_part1 &lt;&#x27;a&#x27;; 因为二级索引记录先按照key_part1进行值排序的，所以符合条件的所有记录肯定是相邻的。我们可以先定位到符合条件的第一条记录，然后顺着单向链表继续往后扫描，直到遇到不符合规则的记录就停止。【-∞,’a’】 1.4.2最佳左前缀匹配原理在看一条SQL： 123select *from single_tablewhere key_part2 = &#x27;a&#x27;; 由于二级索引记录不是直接按照key_part2列的值进行排序的，所以符合条件的二级索引记录可能并不相邻，也就意味着我们不能通过搜索条件来减少需要扫描的行数，这种情况下，我们是不会使用这个索引的。 在看一条SQL： 1234select *from single_tablewhere key_part1 = &#x27;a&#x27; and key_part3 = &#x27;c&#x27;; 这个时候，其实是可以按照key_part1进行过滤的，但是因为接下来是按照key_part2进行排序的，所以满足搜索条件 key_part3 = &#39;c&#39;的二级索引值记录可能并不相邻，这个时候扫描区间其实就是【’a’,’a’】。因为第二个条件走不了索引。 针对获取到的每一条二级索引记录，如果没有开启索引条件下推的特性，则必须先回表获取完整的记录在来判断 key_part3 = &#39;c&#39;条件是否成立，如果开启了索引下推特性，可以判断完 key_part3 = &#39;c&#39;是否成立后在进行回表操作，索引下推是在MySQL5.6引入的，默认开启。 在看一条SQL： 1234select *from single_tablewhere key_part1 &lt; &#x27;a&#x27; and key_part2 = &#x27;c&#x27;; 因为二级索引记录先按照key_part1进行值排序的，所以符合条件的所有记录肯定是相邻的。但是对于key_part1 &lt; &#39;a&#39;条件的二级索引记录来说，并不是直接按照key_part2进行排序的，也就是说我们不能根据key_part2 = &#39;c&#39;来进一步减少扫描的行数。那么，如果使用当前索引执行查询，可以定位到符合key_part1 &lt; &#39;a&#39;的第一条记录，然后沿着单链表往后扫描，一直到不符合key_part1 &lt; &#39;a&#39;为止。 所以在使用当前索引执行SQL的时候，对应的扫描区间其实就是【-∞,’a’】。 在看一条SQL： 1234select *from single_tablewhere key_part1 &lt;= &#x27;a&#x27; and key_part2 = &#x27;c&#x27;; 这条SQL和上一条SQL很像，唯一的区别就是从小于变成了小于等于。很显然符合key_part1 &lt;= &#39;a&#39;的索引值记录是连续的，但是对于符合key_part1 &lt;= &#39;a&#39;条件的二级索引记录来说，并不是直接按照key_part2列排序的。但是，对于符合key_part1 = &#39;a&#39;的二级索引记录来说，是按照key_part2的值进行排序的。那么再确定需要扫描的二级索引记录的范围时，当二级索引记录的key_part1 = &#39;a&#39;时，也可以通过key_part2 = &#39;c&#39;来减少扫描行数，也就是说，当扫描到不符合key_part1 &lt;= &#39;a&#39; and key_part2 = &#39;c&#39;的第一条记录的时候，就可以结束扫描，而不需要将所有的key_part1 = &#39;a&#39;的记录全部扫描完。 2. 索引用于排序我们在写查询语句的时候经常需要对查询出来的记录通过ORDER BY子句按照某种规则进行排序。一般情况下，我们只能把记录都加载到内存中，再用一些排序算法，比如快速排序、归并排序等等在内存中对这些记录进行排序，有的时候可能查询的结果集太大以至于不能在内存中进行排序的话，还可能暂时借助磁盘的空间来存放中间结果，排序操作完成后再把排好序的结果集返回到客户端。在MySQL中，把这种在内存中或者磁盘上进行排序的方式统称为文件排序（英文名：filesort），跟文件这个词儿一沾边儿，就显得这些排序操作非常慢了（磁盘和内存的速度比起来，就像是飞机和蜗牛的对比）。但是如果ORDER BY子句里使用到了我们的索引列，就有可能省去在内存或文件中排序的步骤，比如下边这个简单的查询语句： 1SELECT * FROM single_table ORDER BY key_part1, key_part2, key_part3 LIMIT 10; 这个查询的结果集需要先按照key_part1值排序，如果记录的key_part1值相同，则需要按照key_part2来排序，如果key_part2的值相同，则需要按照key_part3排序。因为这个B+树索引本身就是按照上述规则排好序的，所以直接从索引中提取数据，然后进行回表操作取出该索引中不包含的列就好了。 2.1使用联合索引进行排序注意事项对于联合索引有个问题需要注意，ORDER BY的子句后边的列的顺序也必须按照索引列的顺序给出，如果给出ORDER BY key_part1, key_part3, key_part2的顺序，那也是用不了B+树索引，这种颠倒顺序就不能使用索引的原因我们上边详细说过了，这就不赘述了。 同理，ORDER BY key_part1、ORDER BY key_part1, key_part2这种匹配索引左边的列的形式可以使用部分的B+树索引。当联合索引左边列的值为常量，也可以使用后边的列进行排序，比如这样： 1SELECT * FROM single_table WHERE key_part1 = &#x27;A&#x27; ORDER BY key_part2, key_part3 LIMIT 10; 这个查询能使用联合索引进行排序是因为key_part1列的值相同的记录是按照key_part2, key_part3排序的。 2.2不可以使用索引进行排序的几种情况2.2.1ASC、DESC混用对于使用联合索引进行排序的场景，我们要求各个排序列的排序顺序是一致的，也就是要么各个列都是ASC规则排序，要么都是DESC规则排序。 ORDER BY子句后的列如果不加ASC或者DESC默认是按照ASC排序规则排序的，也就是升序排序的。 为啥会有这种规定呢？这个还得回头想想这个idx_key_part联合索引中记录的结构： 先按照记录的key_part1列的值进行升序排列。 如果记录的key_part1列的值相同，再按照key_part2列的值进行升序排列。 如果记录的key_part2列的值相同，再按照key_part3列的值进行升序排列。 如果查询中的各个排序列的排序顺序是一致的，比方说下边这两种情况： ORDER BY key_part1, key_part2 LIMIT 10这种情况直接从索引的最左边开始往右读10行记录就可以了。 ORDER BY key_part1 DESC, key_part2 DESC LIMIT 10，这种情况直接从索引的最右边开始往左读10行记录就可以了。 但是如果我们查询的需求是先按照key_part1列进行升序排列，再按照key_part2列进行降序排列的话，比如说这样的查询语句： 1SELECT * FROM single_table ORDER BY key_part1, key_part2 DESC LIMIT 10; 这样如果使用索引排序的话过程就是这样的： 先从索引的最左边确定key_part1列最小的值，然后找到key_part1列等于该值的所有记录，然后从name列等于该值的最右边的那条记录开始往左找10条记录。 如果key_part1列等于最小的值的记录不足10条，再继续往右找key_part1值第二小的记录，重复上边那个过程，直到找到10条记录为止。 这样不能高效使用索引，而要采取更复杂的算法去从索引中取数据，所以就规定使用联合索引的各个排序列的排序顺序必须是一致的。 2.2.2排序列包含非同一个索引的列有时候用来排序的多个列不是一个索引里的，这种情况也不能使用索引进行排序，比方说： 1SELECT * FROM single_table ORDER BY key_part1, common_field LIMIT 10; key_part1和common_field并不属于一个联合索引中的列，所以无法使用索引进行排序。 2.2.3排序列使用了复杂的表达式要想使用索引进行排序操作，必须保证索引列是以单独列的形式出现，而不是修饰过的形式，比方说这样： 1SELECT * FROM single_table ORDER BY UPPER(key_part1) LIMIT 10; 使用了UPPER函数修饰过的列就不是单独的列了，这样就无法使用索引进行排序了。 3. 索引用于分组有时候我们为了方便统计表中的一些信息，会把表中的记录按照某些列进行分组。比如下边这个分组查询： 1SELECT key_part1, key_part2, key_part3, COUNT(*) FROM single_table GROUP BY key_part1, key_part2, key_part3 这个查询语句相当于做了3次分组操作： 先把记录按照key_part1值进行分组，所有key_part1值相同的记录划分为一组。 将每个key_part1值相同的分组里的记录再按照key_part2的值进行分组，将key_part3值相同的记录放到一个小分组里，所以看起来就像在一个大分组里又化分了好多小分组。 再将上一步中产生的小分组按照key_part3的值分成更小的分组，所以整体上看起来就像是先把记录分成一个大分组，然后把大分组分成若干个小分组，然后把若干个小分组再细分成更多的小小分组。 然后针对那些小小分组进行统计，比如在我们这个查询语句中就是统计每个小小分组包含的记录条数。如果没有索引的话，这个分组过程全部需要在内存里实现，而如果有了索引的话，恰巧这个分组顺序又和我们的B+树中的索引列的顺序是一致的，而我们的B+树索引又是按照索引列排好序的，这不正好么，所以可以直接使用B+树索引进行分组。 和使用B+树索引进行排序是一个道理，分组列的顺序也需要和索引列的顺序一致，也可以只使用索引列中左边的列进行分组。 四，回表的代价看下边这个查询： 1SELECT * FROM single_table WHERE key_part1 &gt; &#x27;aaa&#x27; AND key_part1 &lt; &#x27;zzz&#x27;; 在使用idx_key_part索引进行查询时大致可以分为这两个步骤： 从索引idx_key_part对应的B+树中取出key_part1值在aaa～zzz之间的用户记录。 由于索引idx_key_part对应的B+树用户记录中只包含key_part1、key_part2、key_part3、id这4个字段，而查询列表是*，意味着要查询表中所有字段，也就是还要包括其他字段。这时需要把从上一步中获取到的每一条记录的id字段都到聚簇索引对应的B+树中找到完整的用户记录，也就是我们通常所说的回表，然后把完整的用户记录返回给查询用户。 由于索引idx_key_part对应的B+树中的记录首先会按照key_part1列的值进行排序，所以值在aaa～zzz之间的记录在磁盘中的存储是相连的，集中分布在一个或几个数据页中，我们可以很快的把这些连着的记录从磁盘中读出来，这种读取方式我们也可以称为顺序I/O。根据第1步中获取到的记录的id字段的值可能并不相连，而在聚簇索引中记录是根据id（也就是主键）的顺序排列的，所以根据这些并不连续的id值到聚簇索引中访问完整的用户记录可能分布在不同的数据页中，这样读取完整的用户记录可能要访问更多的数据页，这种读取方式我们也可以称为随机I/O。一般情况下，顺序I/O比随机I/O的性能高很多，所以步骤1的执行可能很快，而步骤2就慢一些。所以这个使用索引idx_key_part的查询有这么两个特点： 会使用到两个B+树索引，一个二级索引，一个聚簇索引。 访问二级索引使用顺序I/O，访问聚簇索引使用随机I/O。 需要回表的记录越多，使用二级索引的性能就越低，甚至让某些查询宁愿使用全表扫描也不使用二级索引。比方说key_part1值在aaa～zzz之间的用户记录数量占全部记录数量90%以上，那么如果使用idx_key_part索引的话，有90%多的id值需要回表，还不如直接去扫描聚簇索引（也就是全表扫描）。 那什么时候采用全表扫描的方式，什么时候使用采用二级索引 + 回表的方式去执行查询呢？这个就是查询优化器做的工作，查询优化器会事先对表中的记录计算一些统计数据，然后再利用这些统计数据根据查询的条件来计算一下需要回表的记录数，需要回表的记录数越多，就越倾向于使用全表扫描，反之倾向于使用二级索引 + 回表的方式。当然优化器做的分析工作不仅仅是这么简单，但是大致上是这个过程。一般情况下，限制查询获取较少的记录数会让优化器更倾向于选择使用二级索引 + 回表的方式进行查询，因为回表的记录越少，性能提升就越高，比方说上边的查询可以改写成这样： 1SELECT * FROM single_table WHERE key_part1 &gt; &#x27;aaa&#x27; AND key_part1 &lt; &#x27;zzz&#x27; LIMIT 10; 添加了LIMIT 10的查询更容易让优化器采用二级索引 + 回表的方式进行查询。 对于有排序需求的查询，上边讨论的采用全表扫描还是二级索引 + 回表的方式进行查询的条件也是成立的，比方说下边这个查询： 1SELECT * FROM single_table ORDER BY key_part1, key_part2, key_part3; 由于查询列表是*，所以如果使用二级索引进行排序的话，需要把排序完的二级索引记录全部进行回表操作，这样操作的成本还不如直接遍历聚簇索引然后再进行文件排序（filesort）低，所以优化器会倾向于使用全表扫描的方式执行查询。如果我们加了LIMIT子句，比如这样： 1SELECT * FROM single_table ORDER BY key_part1, key_part2, key_part3 LIMIT 10; 这样需要回表的记录特别少，优化器就会倾向于使用二级索引 + 回表的方式执行查询。 五，更好的创建和使用索引1. 只为了用于搜索，排序&amp;分组的列创建索引也就是说，只为出现在WHERE子句中的列、连接子句中的连接列，或者出现在ORDER BY或GROUP BY子句中的列创建索引。而出现在查询列表中的列就没必要建立索引了： 1SELECT key_part1, key_part2 FROM single_table WHERE key_part3 = &#x27;Ashburn&#x27;; 像查询列表中的key_part1、key_part2这两个列就不需要建立索引，我们只需要为出现在WHERE子句中的key_part3列创建索引就可以了。 2. 考虑列的基数列的基数指的是某一列中不重复数据的个数，比方说某个列包含值2, 5, 8, 2, 5, 8, 2, 5, 8，虽然有9条记录，但该列的基数却是3。也就是说，在记录行数一定的情况下，列的基数越大，该列中的值越分散，列的基数越小，该列中的值越集中。这个列的基数指标非常重要，直接影响我们是否能有效的利用索引。假设某个列的基数为1，也就是所有记录在该列中的值都一样，那为该列建立索引是没有用的，因为所有值都一样就无法排序，无法进行快速查找了～ 而且如果某个建立了二级索引的列的重复值特别多，那么使用这个二级索引查出的记录还可能要做回表操作，这样性能损耗就更大了。所以结论就是：最好为那些列的基数大的列建立索引，为基数太小列的建立索引效果可能不好。 3. 索引列的类型尽量小我们在定义表结构的时候要显式的指定列的类型，以整数类型为例，有TINYINT、MEDIUMINT、INT、BIGINT这么几种，它们占用的存储空间依次递增，我们这里所说的类型大小指的就是该类型表示的数据范围的大小。能表示的整数范围当然也是依次递增，如果我们想要对某个整数列建立索引的话，在表示的整数范围允许的情况下，尽量让索引列使用较小的类型，比如我们能使用INT就不要使用BIGINT，能使用MEDIUMINT就不要使用INT～ 这是因为： 数据类型越小，在查询时进行的比较操作越快（这是CPU层次的东西） 数据类型越小，索引占用的存储空间就越少，在一个数据页内就可以放下更多的记录，从而减少磁盘I/O带来的性能损耗，也就意味着可以把更多的数据页缓存在内存中，从而加快读写效率。 这个建议对于表的主键来说更加适用，因为不仅是聚簇索引中会存储主键值，其他所有的二级索引的节点处都会存储一份记录的主键值，如果主键适用更小的数据类型，也就意味着节省更多的存储空间和更高效的I/O。 4. 为列前缀建立索引一个字符串其实是由若干个字符组成，如果我们在MySQL中使用utf8字符集去存储字符串的话，编码一个字符需要占用1~3个字节。假设我们的字符串很长，那存储一个字符串就需要占用很大的存储空间。在我们需要为这个字符串列建立索引时，那就意味着在对应的B+树中有这么两个问题： B+树索引中的记录需要把该列的完整字符串存储起来，而且字符串越长，在索引中占用的存储空间越大。 如果B+树索引中索引列存储的字符串很长，那在做字符串比较时会占用更多的时间。 索引列的字符串前缀其实也是排好序的，所以索引的设计者提出了个方案 — 只对字符串的前几个字符进行索引也就是说在二级索引的记录中只保留字符串前几个字符。这样在查找记录时虽然不能精确的定位到记录的位置，但是能定位到相应前缀所在的位置，然后根据前缀相同的记录的主键值回表查询完整的字符串值，再对比就好了。这样只在B+树中存储字符串的前几个字符的编码，既节约空间，又减少了字符串的比较时间，还大概能解决排序的问题。 5. 覆盖索引为了彻底告别回表操作带来的性能损耗，建议：最好在查询列表里只包含索引列，比如这样： 1SELECT key_part1, key_part2 FROM single_table WHERE key_part3 = &#x27;Ashburn&#x27;; 因为我们只查询key_part1, key_part2, 这2个索引列的值，所以在通过idx_key_part索引得到结果后就不必到聚簇索引中再查找记录的剩余列，这样就省去了回表操作带来的性能损耗。我们把这种只需要用到索引的查询方式称为索引覆盖。排序操作也优先使用覆盖索引的方式进行查询，比方说这个查询： 1SELECT key_part1, key_part2, key_part3 FROM person_info ORDER BYkey_part1, key_part2, key_part3; 虽然这个查询中没有LIMIT子句，但是采用了覆盖索引，所以查询优化器就会直接使用idx_key_part索引进行排序而不需要回表操作了。 当然，如果业务需要查询出索引以外的列，那还是以保证业务需求为重。但是尽量不要用用*号作为查询列表，最好把需要查询的列依次标明。 6.不要乱动列名假设表中有一个整数列my_col，我们为这个列建立了索引。下边的两个WHERE子句虽然语义是一致的，但是在效率上却有差别： WHERE my_col * 2 &lt; 4 WHERE my_col &lt; 4/2 第1个WHERE子句中my_col列并不是以单独列的形式出现的，而是以my_col * 2这样的表达式的形式出现的，存储引擎会依次遍历所有的记录，计算这个表达式的值是不是小于4，所以这种情况下是使用不到为my_col列建立的B+树索引的。而第2个WHERE子句中my_col列并是以单独列的形式出现的，这样的情况可以直接使用B+树索引。 所以结论就是：如果索引列在比较表达式中不是以单独列的形式出现，而是以某个表达式，或者函数调用形式出现的话，是用不到索引的。 7. 尽量维持有序插入对于一个使用InnoDB存储引擎的表来说，在我们没有显式的创建索引时，表中的数据实际上都是存储在聚簇索引的叶子节点的。而记录又是存储在数据页中的，数据页和记录又是按照记录主键值从小到大的顺序进行排序，所以如果我们插入的记录的主键值是依次增大的话，那我们每插满一个数据页就换到下一个数据页继续插，而如果我们插入的主键值忽大忽小的话，这就比较麻烦了，假设某个数据页存储的记录已经满了，它存储的主键值在1~100之间： 如果此时再插入一条主键值为9的记录，那它插入的位置就如下图： 可这个数据页已经满了，再插进来咋办呢？我们需要把当前页面分裂成两个页面，把本页中的一些记录移动到新创建的这个页中。页面分裂和记录移位意味着什么？意味着：性能损耗！所以如果我们想尽量避免这样无谓的性能损耗，最好让插入的记录的主键值依次递增，这样就不会发生这样的性能损耗了。所以我们建议：让主键具有**AUTO_INCREMENT**，让存储引擎自己为表生成主键，而不是我们手动插入 。 8.冗余和重复索引我们知道，通过idx_key_part索引就可以对key_part1列进行快速搜索，再创建一个专门针对key_part1列的索引就算是一个冗余索引，维护这个索引只会增加维护的成本，并不会对搜索有什么好处。 至此，索引命中的原理和我们在建立索引的时候应该注意什么就分析完了，好家伙，又是一个通宵。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/tags/MySQL/"}]},{"title":"MySQL[三]InnoDB索引结构","slug":"MySQL/MySQL[三]InnoDB索引结构","date":"2022-01-11T03:08:18.570Z","updated":"2022-01-11T03:14:40.784Z","comments":true,"path":"2022/01/11/MySQL/MySQL[三]InnoDB索引结构/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/MySQL/MySQL[%E4%B8%89]InnoDB%E7%B4%A2%E5%BC%95%E7%BB%93%E6%9E%84/","excerpt":"","text":"索引其实就是对数据按照某种格式进行存储的文件。就InnoDB来讲，索引文件里面会有很多的基本单元【页】。​ 为什么有页的概念？​ 查询数据的时候直接交互磁盘，效率显然又会很慢，所以真正处理数据的过程其实是在内存中，这样就需要把磁盘的数据加载到内存，如果是写操作，可能还要将内存的数据再次刷新到磁盘。如果内存与磁盘的数据交互过程是基于一条条记录来进行的，显然又会很慢，所以InnoDB采取的方式是将数据划分为若干个页，以页来作为内存和磁盘交互的基本单位，默认大小为16KB。 ​ 数据或者叫记录，其实是以【行】的格式存储在页里面的，可以简单的理解成页里面的一行对应一条记录。​ 当然索引文件里面肯定不光只有页，还会有其余的东西，页里面也不光只有行格式，也会有额外的信息，这个下面我们会详细分析，至此我们仅仅需要明确一下索引的概念和层级关系。 明确了这个层级关系之后，接下来我们来从最基础的行格式来进行分析。 一，行格式我们平时都是以记录为单位向表中插入数据的，这些记录在磁盘上的存储形式被称为行格式或者记录格式，截至目前，一共有4种行格式。分别是 compact redundant dynamic compressed，MySQL5.7默认的行格式为dynamic。 1. 如何指定行格式123CREATE TABLE 表名 (列的信息) ROW_FORMAT=行格式名称 ALTER TABLE 表名 ROW_FORMAT=行格式名称 比如我们创建一张表来指定行格式： 12345678create table record_format( c1 varchar(10), c2 varchar(10) not null, c3 char(10), c4 varchar(10))charset=ascii row_format=compact;INSERT INTO record_format_demo(c1, c2, c3, c4) VALUES(&#x27;aaaa&#x27;, &#x27;bbb&#x27;, &#x27;cc&#x27;, &#x27;d&#x27;), (&#x27;eeee&#x27;, &#x27;fff&#x27;, NULL, NULL); 2.compact 行格式首先我们来看Compact行格式。 一条完整的行格式可以被分为两个部分：记录额外信息的部分&amp;记录真实数据的部分。 2.1 额外的信息额外的信息实包含三部分：变长字段的长度列表，NULL值列表和记录头信息。 2.1.1 变长字段长度列表MySQL支持很多的变长字段，我们就以最经典的varchar来进行举例，变长字段的数据存储多少字节其实是不固定的，所以在存储真实的数据的时候，要记录一下真实数据的字节数，这样的话，一个变长字段列实际上就占用了两部分的空间来存储：【真实数据】&amp;【真实数据占用字节数】。 注意：对于一个列varchar(100)，我们实际上存储一个10字节的数据，当在内存中为这个列的数据分配内存空间的时候，实际上会分配100字节，但是这个列的数据在磁盘上，实际上只会分配10字节。 在Compact行格式中，会把所有的变长字段占用的真实长度全部逆序存储在记录的开头位置，形成一个变长字段长度列表。 比如我们刚才创建的那张表，我们来分析一下： c1,c2,c4三个列都是变长字段，所以这三个列的值的长度其实都需要保存到变长字段长度列表，因为这张表的字符集的ASCII，所以每个字符实际只占用1字节来进行编码： 列名 储存内容 内容长度(十进制表示) 内容长度(十六进制表示) C1 ‘aaaa’ 4 0x04 C2 ‘bbb’ 3 0x03 C4 ‘d’ 1 0x01 因为这些长度是按照逆序来存放的，所以最终变长字段长度列表的字节串用十六进制表示的效果就是【010304】。 因为我们演示的这条记录中，c1,c2,c4列中的字符串都比较短，所以真实的数据占用的字节数就比较小，真实数据的长度用一个字节就可以表示，但是如果变长列的内容占用字节数比较多，可能就需要用2个字节来表示。对此InnoDB的规定是： 【W】：某个字符集中表示一个字符最多需要使用的字节数 【M】：当前列类型最多能存储的字符数(比如varchar(100),M=100),如果换算成字节数就是W*M 【L】：真实占用的字节数 如果M*W&lt;=255,那么使用1字节来表示字符串实际用到的字节数。 InnoDB在读记录的变长字段长度列表的时候会先去查看表结构，判断用几个字节去存储的。 如果M*W&gt;=255,这个时候再次分为两种情况： 如果L&lt;=127，那就用1个字节表示 否则就用2个字节表示 如果某个变长字段允许存储的最大字节数大于255的时候，怎么区分他正在读取的字节是一个单独的字段长度还是半个字段长度呢？ InnoDB用该字节的第一个二进制为作为标志位，0：单独的字段长度，1：半个字段长度。 对于一些占用字节数特别多的字段，单个页都无法存储的时候，InnoDB会把一部分数据放到所谓的溢出页，在变长字段长度列表中只会记录当前页的字段长度，所以用两个字节也可以存的下。 此外，变长字段的长度列表中只存储真实数据值为非NULL的列占用的长度，真实数据为NULL的列的长度是不存储的。 也并不是所有的记录都会有变长字段长度列表，假如表中的列要是没有变长字段，或者记录中的变长字段值都是NULL，那就没有变长字段长度列表了。 2.1.2 NULL值列表如果一条记录有多个字段的真实值为NULL，不统一管理的话就会比较占用空间，所以抽取出来了NULL值列表。 当然如果这个表的所有字段都是NOT NULL约束的，就不会有NULL值列表。 看一下处理过程： 首先统计出表中允许存储NULL的字段 如果表中没有NULL字段的列，那就没必要再往下了，否则将每个允许存储NULL的列对应的一个二进制位按照列的顺序逆序排列。1：NULL，0：不是NULL。 MySQL规定NULL值必须用整数个字节的位表示，如果使用的二进制位个数不是整数个字节，则在字节的高位补0。 以此类推，如果一个表中有9个字段允许为NULL，那么这个记录的NULL值列表部分就需要2个字节来表示。 这个时候再来看我们上面创建的表中的记录。 2.1.3 记录头信息由五个固定的字节组成，换算成二进制就是40位，每一部分代表不同的信息。 名称 大小(bit) 描述 预留位1 1 没有使用 预留位2 1 没有使用 delete_mask 1 标记该记录是否被删除 min_rec_mask 1 B+树的每层非叶子节点中的最小记录都会添加该标记 n_owned 4 表示当前记录拥有的记录数 heap_no 13 表示当前记录在记录堆的位置信息 record_type 3 表示当前记录的类型 0 ：普通记录，1：B+树非页节点记录，2：最小记录，3：最大记录 next_record 16 下一条记录的相对位置 接下来来看记录的真实数据。 2.2 真实数据除了表中显式定义的列，MySQL会往我们的表中放一些隐藏列。 列名 是否必须 占用空间 描述 row_id 否 6字节 行ID，唯一标识一条记录 transaction_id 是 6字节 事务ID roll_pointer 是 7字节 回滚指针 【row_id】：这个玩意，跟主键的选择有关，如果我们显式定义了表的主键，就不会有它，如果我们没显式定义主键，那么会去选择一个unique的列作为主键，如果unique的列也没有，那么就会生成一个row_id列作为隐藏的主键。 【transaction_id】&amp;【roll_pointer】和一致性非锁读(MVCC)有关,后面遇到的时候我会在分析介绍。 在完善下我们开头创建的那张表的记录形象。 至此，其实就剩下我们显式插入数据库的真实记录了，但是还有一个特殊的类型需要说明一下。 2.2.1 CHAR 也是变长的？在Compact行格式下只会把变长类型的列的长度逆序记录到变长字段长度列表，但是这其实和我们的字符集有关系，上面我们创建的表显式指定为ASCII字符集，这个时候一个字符只会用一个字节表示，但是假如我们指定的是其它字符集，比如utf8，这个时候一个字符用几个字节表示就不确定了，所以CHAR列的真实字节长度也会被记录到变长字段长度列表。 另外，变长字符集的CHAR(M)类型的列要求至少占用M个字节，而VARCHAR(M)就没有这个要求。 对于使用utf8字符集的CHAR(10)的列来说，该列存储的数据字节长度的范围是10～30个字节。即使我们向该列中存储一个空字符串也会占用10个字节，这是怕将来更新该列的值的字节长度大于原有值的字节长度而小于10个字节时，可以在该记录处直接更新，而不是在存储空间中重新分配一个新的记录空间，导致原有的记录空间成为所谓的碎片。 3. 行溢出上面提到了，如果一条记录的真实字节数太大，就会导致行溢出，把超出的一部分数据存储到其他行或者页。 3.1 varchar(M)最多能存储的数据varchar(M)的列最多可以占用65535个字节。其中M代表该类型最多存储的字符数量。 实际上，MySQL对一条记录占用的最大存储空间是有限制的，除了BLOB，TEXT类型的列之外，其他所有的列(不包含隐藏列和记录头信息)占用的字节长度加起来不能超过65535个字节。这个65535个字节除了列本身的数据之外，还包括一些其他的数据，比如说我们为了存储一个varchar列，其实还需要占用3部分空间。 真实数据 真实数据占用的字节长度 NULL值标识，如果该列有NOT_NULL属性则可以没有这部分存储空间 如果该varchar类型的列没有NOT NULL属性那最多只能存储65532个字节的数据，因为真实数据的长度可能占用2个字节，NULL值标识需要占用1个字节。 如果VARCHAR类型的列有NOT NULL属性，那最多只能存储65533个字节的数据，因为真实数据的长度可能占用2个字节，不需要NULL值标识。 如果VARCHAR(M)类型的列使用的不是ascii字符集，那会怎么样呢？ 如果VARCHAR(M)类型的列使用的不是ascii字符集，那M的最大取值取决于该字符集表示一个字符最多需要的字节数。在列的值允许为NULL的情况下，gbk字符集表示一个字符最多需要2个字节，那在该字符集下，M的最大取值就是32766（也就是：65532/2），也就是说最多能存储32766个字符；utf8字符集表示一个字符最多需要3个字节，那在该字符集下，M的最大取值就是21844，就是说最多能存储21844（也就是：65532/3）个字符。 上述所言在列的值允许为NULL的情况下，gbk字符集下M的最大取值就是32766，utf8字符集下M的最大取值就是21844，这都是在表中只有一个字段的情况下说的，一定要记住一个行中的所有列（不包括隐藏列和记录头信息）占用的字节长度加起来不能超过65535个字节！ 3.2 记录中的数据太多产生溢出MySQL中磁盘和内存交互的基本单位是页，也就是说MySQL是以页为基本单位来管理存储空间的，我们的记录都会被分配到某个页中存储。而一个页的大小一般是16KB，也就是16384字节，而一个VARCHAR(M)类型的列就最多可以存储65532个字节，这样就可能造成一个页存放不了一条记录的尴尬情况。 在Compact和Redundant行格式中，对于占用存储空间非常大的列，在记录的真实数据处只会存储该列的一部分数据，把剩余的数据分散存储在几个其他的页中，然后记录的真实数据处用20个字节存储指向这些页的地址（当然这20个字节中还包括这些分散在其他页面中的数据的占用的字节数），从而可以找到剩余数据所在的页。 从图中可以看出来，对于Compact和Redundant行格式来说，如果某一列中的数据非常多的话，在本记录的真实数据处只会存储该列的前768个字节的数据和一个指向其他页的地址，然后把剩下的数据存放到其他页中，这个过程也叫做行溢出，存储超出768字节的那些页面也被称为溢出页。画一个简图就是这样： 不只是 VARCHAR(M)类型的列，其他的 TEXT、BLOB 类型的列在存储数据非常多的时候也会发生行溢出。 3.3 行溢出的临界点发生行溢出的临界点是什么呢？也就是说在列存储多少字节的数据时就会发生行溢出？ MySQL中规定一个页中至少存放两行记录，至于为什么这么规定我们之后再说，现在看一下这个规定造成的影响。我们往表中插入亮条记录，每条记录最少插入多少字节的数据才会行溢出呢？ 分析一下页空间是如何利用的 每个页除了存放我们的记录以外，也需要存储一些额外的信息，乱七八糟的额外信息加起来需要132个字节的空间（现在只要知道这个数字就好了），其他的空间都可以被用来存储记录。 每个记录需要的额外信息是27字节。这27个字节包括下边这些部分： 内容 大小(字节) 真实数据的长度 2 列是否是NULL值 1 头信息 5 row_id 6 transaction_id 6 roll_pointer 7 因为表中具体有多少列不确定，所以没法确定具体的临界点，只需要知道插入的字段数据长度很大就会导致行溢出的现象。 4.Dynamic &amp; Compressed 行格式这俩行格式和Compact行格式挺像，只不过在处理行溢出数据时有点儿分歧，它们不会在记录的真实数据处存储字段真实数据的前768个字节，而是把所有的字节都存储到其他页面中，只在记录的真实数据处存储其他页面的地址，就像这样： Compressed行格式和Dynamic不同的一点是，Compressed行格式会采用压缩算法对页面进行压缩，以节省空间。 至此，行格式就分析的差不多了，接下来我们来看页的存储结构。 二，页的存储结构InnoDB为了不同的目的设计了许多种页，比如存放表空间头部信息的页，存放 Insert Buffer信息的页，存放Innode信息的页，存放undo日志信息的页等等。 本节分析存放表中记录的页，官方成为索引页，为了分析方便，我们暂且叫做数据页。 系统变量innodb_page_size表明了InnoDB存储引擎中的页大小，默认值是16384字节，也就是16kb。 该变量只能在第一次初始化MySQL数据目录时指定，之后就再也不能更改了。 数据页代表的这块16kb的存储空间被划分为多个部分，不同部分有不同的功能。 从图中可以看出，一个InnoDB数据页的存储空间大致被划分为了7个部分，有的部分占用的字节数是确定的，有的占用的字节数不是确定的。 名称 中文名 占用空间大小（字节） 简单描述 File Header 文件头部 38 页的一些通用信息 Page Header 页面头部 56 数据页专有的一些信息 Infifmum + Supremum 最小记录和最大记录 26 两个虚拟的行记录 User Records 用户记录 不确定 实际存储的行记录内容 Free Space 空闲空间 不确定 页中尚未使用的空间 Page Directory 页面目录 不确定 页中某些记录的相对位置 File Trailer 文件尾部 8 校验页是否完整 1. 记录在页中的存储我们先来创建一张表 1234567mysql&gt; create table page_demo( -&gt; c1 int , -&gt; c2 int , -&gt; c3 varchar(10000), -&gt; primary key(c1) -&gt; ) charset=ascii row_format=Compact;Query OK, 0 rows affected (0.03 sec) 因为我们指定了主键，所以存储实际数据的列里面不会有隐藏的row_id,我们来看一下他的行格式。 再次回顾下记录头中5个字节表示的数据。 名称 大小(bit) 描述 预留位1 1 没有使用 预留位2 1 没有使用 delete_mask 1 标记该记录是否被删除 min_rec_mask 1 B+树的每层非叶子节点中的最小记录都会添加该标记 n_owned 4 表示当前记录拥有的记录数 heap_no 13 表示当前记录在记录堆的位置信息 record_type 3 表示当前记录的类型 0 ：普通记录，1：B+树非页节点记录，2：最小记录，3：最大记录 next_record 16 下一条记录的相对位置 针对当前这个表的行格式简化图： 接下来我们往表中插入几条数据： 1INSERT INTO page_demo VALUES(1, 100, &#x27;aaaa&#x27;), (2, 200, &#x27;bbbb&#x27;), (3, 300, &#x27;cccc&#x27;), (4, 400, &#x27;dddd&#x27;); 为了分析这些记录在页的User Records 部分中是怎么表示的，把记录头信息和实际的列数据都用十进制表示出来了（其实是一堆二进制位），所以这些记录的示意图就是： 分析一下头信息中的每个属性是什么意思。 1.1 delete_mask标记当前记录是否被删除，占用1个二进制位，0：未删除，1：删除。 被删除的记录不会立即从磁盘上删除，因为删除他们之后吧其他的记录在磁盘上重新排列需要性能消耗，所以只是打一个删除标记，所有被删掉的数据会组成一个垃圾链表，在这个链表中的记录占用的空间成为可重用空间，之后如果有新的记录插入到表中，可能会把这些删除的记录覆盖掉。 将delete_mask 设置为1 和 将被删除的记录加入到垃圾链表中其实是两个阶段。 1.2 min_rec_maskB+树的每层非叶子节点中的最小记录都会添加该标记，如果这个字段的值是0，意味着不是B+树的非叶子节点中的最小记录。 1.3 n_owned1.4 heap_no这个属性表示当前记录在本页中的位置，我们插入的四条记录在本页中的位置分别是 2，3，4 ，5 。为什么不见 0 和 1 的记录呢？ 这是因为InnoDB自动给每个页里边加了两个记录，由于这两个记录并不是我们自己插入的，所以有时候也称为虚拟记录。这两个伪记录一个代表最小记录，一个代表最大记录。 记录是如何比较大小的？对于一条完整的记录来说，比较记录大小就是比较主键的大小。比方说我们插入的4行记录的主键值分别为1，2，3，4，这也就意味着这四条记录的大小从大到小递增。 但是不管我们往页中插入了多少自己的记录，InnoDB都规定他们定义的两条伪记录分别为最小记录和最大记录。这两条记录的构造十分简单，都是由5字节大小的记录头信息和8字节大小的一个固定的部分组成的。 由于这两条记录不是我们自己定义的记录，所以他们并不存放在页的User Records部分，他们被单独放在一个称为Infimum+Supremum的部分。 从图中我们可以看出来，最小记录和最大记录的heap_no值分别是0 和 1 ， 也就是说他们的位置最靠前。 1.5 record_type这个属性表示当前记录的类型。0：普通记录，1：B+树非叶子节点记录，2：最小记录，3：最大记录。 我们自己插入的记录是普通记录 0 ， 而最大记录和最小记录record_type 分别为 2 和 3。 1.6 next_record表示从当前记录的真实数据到下一条记录的真实数据的地址偏移量。这其实是一条链表，可以通过一条记录找到他的下一条记录，但是下一条记录指的并不是按照我们插入顺序的下一条记录，而是按照主键值由小到大的顺序的下一条记录。而且规定 infimum记录 的下一条记录就是本页主键值最小的用户记录，而本页中主键最大的用户记录的下一条记录就是supremum记录。 如果从中删除一条记录，这个链表也是会跟着变化的，假如现在删除第二条记录 1delete from page_demo where c1 =2 ; 删除第二条记录以后： 发生的变化： 第二条记录并没有从存储空间中移除，而是把该记录的delete_mask设置为1 第二条记录的next_records值变成了0，意味着该记录没有下一条记录了 第一条记录的next record指向了第三条记录 最大记录的 n_owned 值从5 变成了4 所以，不论我们怎么对页中的记录做增删改查操作，InnoDB始终会维护一条记录的单链表，链表中各个节点是按照主键值由小到大的顺序连接起来的。 next_records 为啥要指向记录头信息和真实数据之间的位置呢？为啥不干脆指向整条记录的开头位置，也就是记录的额外信息开头的位置呢？ 因为这个位置刚刚好，向左读取就是记录头信息，向右读取就是真实数据。我们前边还说过变长字段长度列表，null值列表中的信息都是逆序存放的，这样可以使记录中位置靠前的字段和他们对应的字段长度信息在内存中的距离更近，可能会提高高速缓存的命中率。 因为主键值为2的记录已经被我们删除了，但是存储空间并没有回收，如果再次把这条记录插入到表中，会发生什么？ 1INSERT INTO page_demo VALUES(2, 200, &#x27;bbbb&#x27;); 从图中可以看到，InnoDB并没有因为新记录的插入而为他申请新的存储空间，而是直接复用了原来删除的记录的存储空间。 2. Page Directory（页目录）如果我们想根据主键值查找页中某条记录该咋办？ 1select * from page_demo where c1 = 3; 将所有正常的记录(包括两条隐藏记录但是不包括已经标记为删除的记录)划分为几组 每个组的最后一条记录（也就是组内最大的那条记录）的头信息中的n_owned属性表示该组拥有多少条记录 将每个组的最后一条记录的地址偏移量单独提取出来按照顺序存储到靠近页的尾部的地方，这个地方就是所谓的【Page Directory】,也就是页目录。页目录中的这些地址偏移量被称为槽，所以页目录就是由槽组成的 比方说刚才创建的表中正常的记录由6条，InnoDB会把他们分成两组，第一组中只有一条最小记录，第二组中是剩余的5条记录。 现在页目录部分中有两个槽，也就意味着我们的记录被分成了两个组，槽1中的值为112，代表最大记录的地址偏移量；槽0的值为99，代表最小记录的地址偏移量。 注意最大和最小记录的头信息的n_owned属性： 最小记录中的n_owned值为1，这就代表着以最小记录结尾的这个分组中只有1条记录，也就是最小记录本身 最大记录中的n_owned值为5，这就代表着以最大记录结尾的这个分组中只有5条记录，包括最大记录本身还有我们自己插入的4条记录 【99】&amp;【112】这样的地址偏移量很不直观，我们用箭头指向的方式替代数字。 InnoDB对每个分组中的记录条数是有规定的：对于最小记录所在的分组只能有 1 条记录，最大记录所在的分组拥有的记录条数只能在 1~8 条之间，剩下的分组中记录的条数范围只能在是 4~8 条之间。所以分组是按照下边的步骤进行的： 初始情况下一个数据页里只有最小记录和最大记录两条记录，它们分属于两个分组。 之后每插入一条记录，都会从页目录中找到主键值比本记录的主键值大并且差值最小的槽，然后把该槽对应的记录的n_owned值加1，表示本组内又添加了一条记录，直到该组中的记录数等于8个。 在一个组中的记录数等于8个后再插入一条记录时，会将组中的记录拆分成两个组，一个组中4条记录，另一个5条记录。这个过程会在页目录中新增一个槽来记录这个新增分组中最大的那条记录的偏移量。 由于现在page_demo表中的记录太少，无法演示添加了页目录之后加快查找速度的过程，所以再往page_demo表中添加一些记录： 1INSERT INTO page_demo VALUES(5, 500, &#x27;eeee&#x27;), (6, 600, &#x27;ffff&#x27;), (7, 700, &#x27;gggg&#x27;), (8, 800, &#x27;hhhh&#x27;), (9, 900, &#x27;iiii&#x27;), (10, 1000, &#x27;jjjj&#x27;), (11, 1100, &#x27;kkkk&#x27;), (12, 1200, &#x27;llll&#x27;), (13, 1300, &#x27;mmmm&#x27;), (14, 1400, &#x27;nnnn&#x27;), (15, 1500, &#x27;oooo&#x27;), (16, 1600, &#x27;pppp&#x27;); 现在看怎么从这个页目录中查找记录。因为各个槽代表的记录的主键值都是从小到大排序的，所以我们可以使用所谓的二分法来进行快速查找。5个槽的编号分别是：0、1、2、3、4，所以初始情况下最低的槽就是low=0，最高的槽就是high=4。比方说我们想找主键值为6的记录，过程是这样的： 计算中间槽的位置：(0+4)/2=2，所以查看槽2对应记录的主键值为8，又因为8 &gt; 6，所以设置high=2，low保持不变。 重新计算中间槽的位置：(0+2)/2=1，所以查看槽1对应的主键值为4，又因为4 &lt; 6，所以设置low=1，high保持不变。 因为high - low的值为1，所以确定主键值为6的记录在槽2对应的组中。此刻我们需要找到槽2中主键值最小的那条记录，然后沿着单向链表遍历槽2中的记录。但是我们前边又说过，每个槽对应的记录都是该组中主键值最大的记录，这里槽2对应的记录是主键值为8的记录，怎么定位一个组中最小的记录呢？别忘了各个槽都是挨着的，我们可以很轻易的拿到槽1对应的记录（主键值为4），该条记录的下一条记录就是槽2中主键值最小的记录，该记录的主键值为5。所以我们可以从这条主键值为5的记录出发，遍历槽2中的各条记录，直到找到主键值为6的那条记录即可。由于一个组中包含的记录条数只能是1~8条，所以遍历一个组中的记录的代价是很小的。 所以在一个数据页中查找指定主键值的记录的过程分为两步： 通过二分法确定该记录所在的槽，并找到该槽所在分组中主键值最小的那条记录。 通过记录的next_record属性遍历该槽所在的组中的各个记录。 3.Page Header（页面头部）为了能得到一个数据页中存储的记录的状态信息，比如本页中已经存储了多少条记录，第一条记录的地址是什么，页目录中存储了多少个槽等等，特意在页中定义了一个叫Page Header的部分，它是页结构的第二部分，这个部分占用固定的56个字节，专门存储各种状态信息。 名称 占用空间大小（字节） 描述 PAGE_N_DIR_SLOTS 2 在页目录中的槽数量 PAGE_HEAP_TOP 2 还未使用的空间最小地址，也就是说从该地址之后就是Free Space PAGE_N_HEAP 2 本页中的记录的数量（包括最小和最大记录以及标记为删除的记录） PAGE_FREE 2 第一个已经标记为删除的记录地址（各个已删除的记录通过next_record也会组成一个单链表，这个单链表中的记录可以被重新利用） PAGE_GARBAGE 2 已删除记录占用的字节数 PAGE_LAST_INSERT 2 最后插入记录的位置 PAGE_DIRECTION 2 记录插入的方向 PAGE_N_DIRECTION 2 一个方向连续插入的记录数量 PAGE_N_RECS 2 该页中记录的数量（不包括最小和最大记录以及被标记为删除的记录） PAGE_MAX_TRX_ID 8 修改当前页的最大事务ID，该值仅在二级索引中定义 PAGE_LEVEL 2 当前页在B+树中所处的层级 PAGE_INDEX_ID 8 索引ID，表示当前页属于哪个索引 PAGE_BTR_SEG_LEAF 10 B+树叶子段的头部信息，仅在B+树的Root页定义 PAGE_BTR_SEG_TOP 10 B+树非叶子段的头部信息，仅在B+树的Root页定义 4.File Header（文件头部）File Header针对各种类型的页都通用，也就是说不同类型的页都会以File Header作为第一个组成部分，它描述了一些针对各种页都通用的一些信息，比方说这个页的编号是多少，它的上一个页、下一个页是谁，这个部分占用固定的38个字节。 名称 占用空间大小（字节） 描述 FIL_PAGE_SPACE_OR_CHKSUM 4 页的校验和（checksum值） FIL_PAGE_OFFSET 4 页号 FIL_PAGE_PREV 4 上一个页的页号 FIL_PAGE_NEXT 4 下一个页的页号 FIL_PAGE_LSN 8 页面被最后修改时对应的日志序列位置（英文名是：Log Sequence Number） FIL_PAGE_TYPE 2 该页的类型 FIL_PAGE_FILE_FLUSH_LSN 8 仅在系统表空间的一个页中定义，代表文件至少被刷新到了对应的LSN值 FIL_PAGE_ARCH_LOG_NO_OR_SPACE_ID 4 页属于哪个表空间 InnoDB为了不同的目的而把页分为不同的类型，我们上边介绍的其实都是存储记录的数据页，其实还有很多别的类型的页，具体如下表： 类型名称 十六进制 描述 FIL_PAGE_TYPE_ALLOCATED 0x0000 最新分配，还没使用 FIL_PAGE_UNDO_LOG 0x0002 Undo日志页 FIL_PAGE_INODE 0x0003 段信息节点 FIL_PAGE_IBUF_FREE_LIST 0x0004 Insert Buffer空闲列表 FIL_PAGE_IBUF_BITMAP 0x0005 Insert Buffer位图 FIL_PAGE_TYPE_SYS 0x0006 系统页 FIL_PAGE_TYPE_TRX_SYS 0x0007 事务系统数据 FIL_PAGE_TYPE_FSP_HDR 0x0008 表空间头部信息 FIL_PAGE_TYPE_XDES 0x0009 扩展描述页 FIL_PAGE_TYPE_BLOB 0x000A 溢出页 FIL_PAGE_INDEX 0x45BF 索引页，也就是我们所说的数据页 我们存放记录的数据页的类型其实是FIL_PAGE_INDEX，也就是所谓的索引页。 有时候我们存放某种类型的数据占用的空间非常大（比方说一张表中可以有成千上万条记录），InnoDB可能不可以一次性为这么多数据分配一个非常大的存储空间，如果分散到多个不连续的页中存储的话需要把这些页关联起来，FIL_PAGE_PREV和FIL_PAGE_NEXT就分别代表本页的上一个和下一个页的页号。这样通过建立一个双向链表把许许多多的页就都串联起来了，而无需这些页在物理上真正连着。需要注意的是，并不是所有类型的页都有上一个和下一个页的属性，不过我们现在分析的数据页（也就是类型为FIL_PAGE_INDEX的页）是有这两个属性的，所以所有的数据页其实是一个双链表。 5.File Trailer(文件尾部)如果页中的数据在内存中被修改了，那么在修改后的某个时间需要把数据同步到磁盘中。但是在同步了一半的时候中断电了咋办？ 为了检测一个页是否完整，在每个页的尾部都加了一个File Trailer部分，这个部分由8个字节组成，可以分成2个小部分： 前四个字节代表校验和 后四个字节代表页面被最后修改时对应的日志序列位置 这个File Trailer &amp; File Header 类似，都是所有类型的页通用的。 至此，整个数据页的结构我们也基本上分析完了，现在在回头看一下开头我们那张恐怖的图，是不是感觉清晰很多了呢？接下来，我们来分析索引的结构。 三，索引1.假如没有索引我们先来看看没有索引的情况下，我们进行数据的查找(毕竟没有对比就没有伤害)。 1.1 在一个页中查找假设表中的记录很少，所有的记录仅仅用一个页就存放下了，这个时候按照不同的搜索条件其实可以分为两种情况讨论： 【以主键为搜索的条件】：可以再页目录中根据二分查找快速定位到槽，在根据槽定位到该组的最小索引记录，然后进行遍历匹配查找。 【以其他列作为搜索条件】：在数据页中并没有为非主键列建立所谓的页目录，所以无法通过二分法快速定位相应的槽。在这种情况下，只能从最小记录开始依次往后遍历单链表中的每条记录，然后对比每条记录是否符合搜索条件，显然，效率很低。 1.2 在很多页中查找很多时候，表的记录一个页都是存储不下的，这个时候的查找其实分为两个步骤： 【定位到记录所在的页】 【从所在的页内查找相应的记录】 因为我们不能快速的定位到所在的页，所以只能从第一页开始沿着双链表往后遍历定位页，定位到页以后在根据在一个页中的查找方式进行匹配查找，显而易见，这个时候效率低的可怕。 有了痛点，就会有大牛去思考整个生命周期，完善逻辑和资源倾斜，形成一套自己的方法论，想办法为快速查找赋能。 2. 索引我们先创建一张表： 1234567mysql&gt; CREATE TABLE index_demo( -&gt; c1 INT, -&gt; c2 INT, -&gt; c3 CHAR(1), -&gt; PRIMARY KEY(c1) -&gt; ) ROW_FORMAT = Compact;Query OK, 0 rows affected (0.03 sec) 2.1 一个简单的索引方案我们在根据某个搜索条件查找一些记录时为什么要遍历所有的数据页呢？ 因为各个页中的记录并没有规律，我们并不知道我们的搜索条件匹配哪些页中的记录，所以 不得不 依次遍历所有的数据页。 如果我们想快速的定位到需要查找的记录在哪些数据页中该咋办？ 对比根据主键值快速定位一条记录从而在页中的位置建立页目录，我们也可以想办法为快速定位记录所在的数据页而建立一个别的目录。 【下一个数据页中用户记录的主键值必须大于上一个页中用户记录的主键值。】 假设我们现在每一页只能放三条记录，现在已经放了主键为1,3,5的三条记录。这个时候我们再添加一条主键为4的记录，我们不得不为他分配一个新的页。 注意：新分配的数据页编号可能和原来并不是连续的，也就是说我们使用的这些页在存储空间里可能并不挨着。他们只是通过维护着上一页和下一页的编号而建立了链表关系。 原来页中主键最大的值为5，现在我们新插入一条记录，如果直接放在新页里面，那就会有问题，这不符合下一个数据页中用户记录的主键值必须大于上一个页中用户记录的主键值得要求，所以在插入主键值为4 的记录的时候需要伴随一次记录的移动，也就是把主键值为5 的记录移动到新分配的页中，然后把主键值为4 的记录插入到原来的页中。 这个过程表明了在对页中的记录进行增删改操作的过程中，我们必须通过一些诸如记录移动的操作来始终保证这个状态一直成立：下一个数据页中用户记录的主键值必须大于上一个页中用户记录的主键值。这个过程我们也可以称为页分裂。 【给所有的页建立一个目录项。】 由于数据页的编号可能并不是连续的，所以在向index_demo表中插入许多条记录后，可能是这样的效果： 因为这些16KB的页在物理存储上可能并不挨着，所以如果想从这么多页中根据主键值快速定位某些记录所在的页，我们需要给它们做个目录，每个页对应一个目录项，每个目录项包括下边两个部分： 页的用户记录中最小的主键值，我们用key来表示。 页号，我们用page_no表示。 以页28为例，它对应目录项2，这个目录项中包含着该页的页号28以及该页中用户记录的最小主键值5。我们只需要把几个目录项在物理存储器上连续存储，比如把他们放到一个数组里，就可以实现根据主键值快速查找某条记录的功能了。比方说我们想找主键值为20的记录，具体查找过程分两步： 先从目录项中根据二分法快速确定出主键值为20的记录在目录项3中（因为 12 &lt; 20 &lt; 209），它对应的页是页9。 再根据前边说的在页中查找记录的方式去页9中定位具体的记录。 至此，针对数据页做的简易目录就搞定了。这个目录其实就是【索引】。 2.2 InnoDB中的索引方案上面的方案存在什么样的问题？ InnoDB是使用页来作为管理存储空间的基本单位，也就是最多能保证16KB的连续存储空间，而随着表中记录数量的增多，需要非常大的连续的存储空间才能把所有的目录项都放下，这对记录数量非常多的表是不现实的。 我们时常会对记录进行增删，假设我们把页28中的记录都删除了，页28也就没有存在的必要了，那意味着目录项2也就没有存在的必要了，这就需要把目录项2后的目录项都向前移动一下。 InnoDB复用了之前存储用户记录的数据页来存储目录项，为了和用户记录做一下区分，我们把这些用来表示目录项的记录称为目录项记录。 那InnoDB怎么区分一条记录是普通的用户记录还是目录项记录呢？通过记录头信息里的record_type属性，它的各个取值代表的意思如下： 0：普通的用户记录 **1**：目录项记录 2：最小记录 3：最大记录 把前边使用到的目录项放到数据页中的样子就是这样： 从图中可以看出来，我们新分配了一个编号为30的页来专门存储目录项记录。这里再次强调一遍目录项记录和普通的用户记录的不同点： 目录项记录的record_type值是1，而普通用户记录的record_type值是0。 目录项记录只有主键值和页的编号两个列，而普通的用户记录的列是用户自己定义的，可能包含很多列，另外还有InnoDB自己添加的隐藏列。 头信息里面有一个叫min_rec_mask的属性，只有在存储目录项记录的页中的主键值最小的目录项记录的min_rec_mask值为1，其他别的记录的min_rec_mask值都是0。 除此之外，两者就没有区别了，页的组成结构也是一样一样的（就是我们前边介绍过的7个部分），都会为主键值生成Page Directory（页目录），从而在按照主键值进行查找时可以使用二分法来加快查询速度。 现在以查找主键为20的记录为例，根据某个主键值去查找记录的步骤就可以大致拆分成下边两步： 先到存储目录项记录的页，也就是页30中通过二分法快速定位到对应目录项，因为12 &lt; 20 &lt; 209，所以定位到对应的记录所在的页就是页9。 再到存储用户记录的页9中根据二分法快速定位到主键值为20的用户记录。 虽然说目录项记录中只存储主键值和对应的页号，比用户记录需要的存储空间小多了，但是不论怎么说一个页只有16KB大小，能存放的目录项记录也是有限的，那如果表中的数据太多，以至于一个数据页不足以存放所有的目录项记录，该咋办呢？ 当然是再多整一个存储**目录项记录**的页。 从图中可以看出，我们插入了一条主键值为320的用户记录之后需要两个新的数据页： 为存储该用户记录而新生成了页31。 因为原先存储目录项记录的页30的容量已满（我们前边假设只能存储4条目录项记录），所以不得不需要一个新的页32来存放页31对应的目录项。 现在因为存储目录项记录的页不止一个，所以如果我们想根据主键值查找一条用户记录大致需要3个步骤，以查找主键值为20的记录为例： 确定目录项记录页 我们现在的存储目录项记录的页有两个，即页30和页32，又因为页30表示的目录项的主键值的范围是[1, 320)，页32表示的目录项的主键值不小于320，所以主键值为20的记录对应的目录项记录在页30中。 通过目录项记录页确定用户记录真实所在的页。 在真实存储用户记录的页中定位到具体的记录。 那么问题来了，在这个查询步骤的第1步中我们需要定位存储目录项记录的页，但是这些页在存储空间中也可能不挨着，如果我们表中的数据非常多则会产生很多存储目录项记录的页，那我们怎么根据主键值快速定位一个存储目录项记录的页呢？ 为这些存储目录项记录的页再生成一个更高级的目录，就像是一个多级目录一样，大目录里嵌套小目录，小目录里才是实际的数据，所以现在各个页的示意图就是这样子： 随着表中记录的增加，这个目录的层级会继续增加，如果简化一下，那么我们可以用下边这个图来描述它： 其实这是一种组织数据的形式，或者说是一种数据结构，它的名称是B+树。 不论是存放用户记录的数据页，还是存放目录项记录的数据页，我们都把它们存放到B+树这个数据结构中了，所以我们也称这些数据页为节点。从图中可以看出来，我们的实际用户记录其实都存放在B+树的最底层的节点上，这些节点也被称为叶子节点或叶节点，其余用来存放目录项的节点称为非叶子节点或者内节点，其中B+树最上边的那个节点也称为根节点。 从图中可以看出来，一个B+树的节点其实可以分成好多层，InnoDB规定最下边的那层，也就是存放我们用户记录的那层为第0层，之后依次往上加。之前的分析我们做了一个非常极端的假设：存放用户记录的页最多存放3条记录，存放目录项记录的页最多存放4条记录。其实真实环境中一个页存放的记录数量是非常大的，假设所有存放用户记录的叶子节点代表的数据页可以存放100条用户记录，所有存放目录项记录的内节点代表的数据页可以存放1000条目录项记录，那么： 如果B+树只有1层，也就是只有1个用于存放用户记录的节点，最多能存放100条记录。 如果B+树有2层，最多能存放1000×100=100000条记录。 如果B+树有3层，最多能存放1000×1000×100=100000000条记录。 如果B+树有4层，最多能存放1000×1000×1000×100=100000000000条记录。 一般情况下，我们用到的B+树都不会超过4层，那我们通过主键值去查找某条记录最多只需要做4个页面内的查找（查找3个目录项页和一个用户记录页），又因为在每个页面内有所谓的Page Directory（页目录），所以在页面内也可以通过二分法实现快速定位记录。 2.3 聚簇索引上边介绍的B+树本身就是一个目录，或者说本身就是一个索引。它有两个特点： 使用记录主键值的大小进行记录和页的排序，这包括三个方面的含义： 页内的记录是按照主键的大小顺序排成一个单向链表。 各个存放用户记录的页也是根据页中用户记录的主键大小顺序排成一个双向链表。 存放目录项记录的页分为不同的层次，在同一层次中的页也是根据页中目录项记录的主键大小顺序排成一个双向链表。 B+树的叶子节点存储的是完整的用户记录。所谓完整的用户记录，就是指这个记录中存储了所有列的值（包括隐藏列）。 我们把具有这两种特性的B+树称为聚簇索引，所有完整的用户记录都存放在这个聚簇索引的叶子节点处。这种聚簇索引并不需要我们在MySQL语句中显式的使用INDEX语句去创建，InnoDB存储引擎会自动的为我们创建聚簇索引。另外，在InnoDB存储引擎中，聚簇索引就是数据的存储方式（所有的用户记录都存储在了叶子节点），也就是所谓的索引即数据，数据即索引。 2.4 二级索引聚簇索引只能在搜索条件是主键值时才能发挥作用，因为B+树中的数据都是按照主键进行排序的。那如果我们想以别的列作为搜索条件怎么办？ 我们可以多建几棵B+树，不同的B+树中的数据采用不同的排序规则。比方说我们用c2列的大小作为数据页、页中记录的排序规则，再建一棵B+树，效果如下图所示： 这个B+树与上边介绍的聚簇索引有几处不同： 使用记录c2列的大小进行记录和页的排序，这包括三个方面的含义： 页内的记录是按照c2列的大小顺序排成一个单向链表。 各个存放用户记录的页也是根据页中记录的c2列大小顺序排成一个双向链表。 存放目录项记录的页分为不同的层次，在同一层次中的页也是根据页中目录项记录的c2列大小顺序排成一个双向链表。 B+树的叶子节点存储的并不是完整的用户记录，而只是c2列+主键这两个列的值。 目录项记录中不再是主键+页号的搭配，而变成了c2列+页号的搭配。 所以如果我们现在想通过c2列的值查找某些记录的话就可以使用我们刚刚建好的这个B+树了。以查找c2列的值为4的记录为例，查找过程如下： 确定目录项记录页 根据根页面，也就是页44，可以快速定位到目录项记录所在的页为页42（因为2 &lt; 4 &lt; 9）。 通过目录项记录页确定用户记录真实所在的页。 在页42中可以快速定位到实际存储用户记录的页，但是由于c2列并没有唯一性约束，所以c2列值为4的记录可能分布在多个数据页中，又因为2 &lt; 4 ≤ 4，所以确定实际存储用户记录的页在页34和页35中。 在真实存储用户记录的页中定位到具体的记录. 到页34和页35中定位到具体的记录。 但是这个B+树的叶子节点中的记录只存储了c2和c1（也就是主键）两个列，所以我们必须再根据主键值去聚簇索引中再查找一遍完整的用户记录。 我们根据这个以**c2**列大小排序的**B+**树只能确定我们要查找记录的主键值，所以如果我们想根据**c2**列的值查找到完整的用户记录的话，仍然需要到**聚簇索引**中再查一遍，这个过程也被称为**回表**。也就是根据c2列的值查询一条完整的用户记录需要使用到2棵B+树！！！ 为什么我们还需要一次回表操作呢？直接把完整的用户记录放到叶子节点不就好了么？ 如果把完整的用户记录放到叶子节点是可以不用回表，相当于每建立一棵B+树都需要把所有的用户记录再都拷贝一遍，这就有点太浪费存储空间了。因为这种按照非主键列建立的B+树需要一次回表操作才可以定位到完整的用户记录，所以这种B+树也被称为二级索引（英文名secondary index），或者辅助索引。由于我们使用的是c2列的大小作为B+树的排序规则，所以我们也称这个B+树为为c2列建立的索引。 假设我们的查询结果是十条，那就是要进行10次回表，那这样的话，效率不是又慢了？ 在MySQL5.6对这种情况进行了优化，如果发现查询结果会导致多次回表，那么就会进行IO合并，拿到所有的主键再去进行回表。 2.5 联合索引我们也可以同时以多个列的大小作为排序规则，也就是同时为多个列建立索引，比方说我们想让B+树按照c2和c3列的大小进行排序，这个包含两层含义： 先把各个记录和页按照c2列进行排序。 在记录的c2列相同的情况下，采用c3列进行排序 为c2和c3列建立的索引的示意图如下： 3. InnoDB的B+树索引的注意事项3.1 跟页面永远固定不动前边介绍B+树索引的时候，为了理解上的方便，先把存储用户记录的叶子节点都画出来，然后接着画存储目录项记录的内节点，实际上B+树的形成过程是这样的： 每当为某个表创建一个B+树索引（聚簇索引不是人为创建的，默认就有）的时候，都会为这个索引创建一个根节点页面。最开始表中没有数据的时候，每个B+树索引对应的根节点中既没有用户记录，也没有目录项记录。 随后向表中插入用户记录时，先把用户记录存储到这个根节点中。 当根节点中的可用空间用完时继续插入记录，此时会将根节点中的所有记录复制到一个新分配的页，比如页a中，然后对这个新页进行页分裂的操作，得到另一个新页，比如页b。这时新插入的记录根据键值（也就是聚簇索引中的主键值，二级索引中对应的索引列的值）的大小就会被分配到页a或者页b中，而根节点便升级为存储目录项记录的页。 这个过程需要特别注意的是：一个B+树索引的根节点自诞生之日起，便不会再移动。这样只要我们对某个表建立一个索引，那么它的根节点的页号便会被记录到某个地方，然后凡是InnoDB存储引擎需要用到这个索引的时候，都会从那个固定的地方取出根节点的页号，从而来访问这个索引。 3.2 内节点中目录项记录的唯一性我们知道B+树索引的内节点中目录项记录的内容是索引列 + 页号的搭配，但是这个搭配对于二级索引来说有点儿不严谨。假设表中的数据是这样的： c1 c2 c3 1 1 ‘u’ 3 1 ‘d’ 5 1 ‘y’ 7 1 ‘a’ 如果二级索引中目录项记录的内容只是索引列 + 页号的搭配的话，那么为c2列建立索引后的B+树应该长这样： 如果我们想新插入一行记录，其中c1、c2、c3的值分别是：9、1、&#39;c&#39;，那么在修改这个为c2列建立的二级索引对应的B+树时便碰到了个大问题：由于页3中存储的目录项记录是由c2列 + 页号的值构成的，页3中的两条目录项记录对应的c2列的值都是1，而我们新插入的这条记录的c2列的值也是1，那我们这条新插入的记录到底应该放到页4中，还是应该放到页5中? 为了让新插入记录能找到自己在那个页里，我们需要保证在B+树的同一层内节点的目录项记录除页号这个字段以外是唯一的。所以对于二级索引的内节点的目录项记录的内容实际上是由三个部分构成的： 索引列的值 主键值 页号 也就是我们把主键值也添加到二级索引内节点中的目录项记录了，这样就能保证B+树每一层节点中各条目录项记录除页号这个字段外是唯一的。 这样我们再插入记录(9, 1, &#39;c&#39;)时，由于页3中存储的目录项记录是由c2列 + 主键 + 页号的值构成的，可以先把新记录的c2列的值和页3中各目录项记录的c2列的值作比较，如果c2列的值相同的话，可以接着比较主键值，因为B+树同一层中不同目录项记录的c2列 + 主键的值肯定是不一样的，所以最后肯定能定位唯一的一条目录项记录，在本例中最后确定新记录应该被插入到页5中。 3.3 一个页面最少存储2条记录B+树只需要很少的层级就可以轻松存储数亿条记录，这是因为B+树本质上就是一个大的多层级目录，每经过一个目录时都会过滤掉许多无效的子目录，直到最后访问到存储真实数据的目录。那如果一个大的目录中只存放一个子目录会怎么样？那就是目录层级非常多，而且最后的那个存放真实数据的目录中只能存放一条记录，会导致效率很低。 其实让B+数的叶子结点值存储一条记录，让内节点存储多条记录，也还是可以发挥B+数的作用的。但是InnoDB为了避免数的层级过高，要求所有的数据页都至少可以容纳两条记录。 4. MyISAM中的索引方案简单介绍MyISAM的索引方案虽然也使用树形结构，但是却将索引和数据分开存储： 将表中的记录按照记录的插入顺序单独存储在一个文件中，称之为数据文件。这个文件并不划分为若干个数据页，有多少记录就往这个文件中塞多少记录就成了。我们可以通过行号而快速访问到一条记录。 使用MyISAM存储引擎的表会把索引信息另外存储到一个称为索引文件的另一个文件中。MyISAM会单独为表的主键创建一个索引，只不过在索引的叶子节点中存储的不是完整的用户记录，而是主键值 + 行号的组合。也就是先通过索引找到对应的行号，再通过行号去找对应的记录！这一点和InnoDB是完全不相同的，在InnoDB存储引擎中，我们只需要根据主键值对聚簇索引进行一次查找就能找到对应的记录，而在MyISAM中却需要进行一次回表操作，意味着MyISAM中建立的索引相当于全部都是二级索引！ 如果有需要的话，我们也可以对其它的列分别建立索引或者建立联合索引，原理和InnoDB中的索引差不多，不过在叶子节点处存储的是相应的列 + 行号。这些索引也全部都是二级索引。 由于在插入数据的时候并没有刻意按照主键大小排序，所以我们并不能在MyIsaM数据上使用二分法进行查找。 5. 创建和删除索引的语句InnoDB和MyISAM会自动为主键或者声明为UNIQUE的列去自动建立B+树索引，但是如果我们想为其他的列建立索引就需要我们显式的去指明。 我们可以在创建表的时候指定需要建立索引的单个列或者建立联合索引的多个列： 1234CREATE TALBE 表名 ( 各种列的信息 ··· , [KEY|INDEX] 索引名 (需要被索引的单个列或多个列)) 我们也可以在修改表结构的时候添加索引： 1ALTER TABLE 表名 ADD [INDEX|KEY] 索引名 (需要被索引的单个列或多个列); 也可以在修改表结构的时候删除索引： 1ALTER TABLE 表名 DROP [INDEX|KEY] 索引名; 至此，整个索引相关的结构我们就都分析完了。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/tags/MySQL/"}]},{"title":"MySQL[一]入门","slug":"MySQL/MySQL[一]入门","date":"2022-01-11T02:49:27.263Z","updated":"2022-01-11T03:14:44.975Z","comments":true,"path":"2022/01/11/MySQL/MySQL[一]入门/","link":"","permalink":"https://yinhuidong.github.io/2022/01/11/MySQL/MySQL[%E4%B8%80]%E5%85%A5%E9%97%A8/","excerpt":"","text":"一，MYSQL入门1.数据库相关概念123DB：数据库：存储数据的仓库，保存了一系列有组织的数据。DBMS：数据库管理系统：数据库是通过DBMS创建和操作的容器。SQL：结构化查询语言：专门用来与数据库通信的语言。 2.数据库的好处121.可以持久化数据到本地2.可以实现结构化查询，方便管理 3.数据库存储数据特点12345671.将数据放到表中，表放到库中。2.一个数据库有多张表，每个表都有一个名字，用来标识自己。表名具有唯一性。3.表具有一些特性，这些特性定义了数据在表中如何存储，类似Java中类的设计。4.表有列组成，我们也称为字段。所有表都是由一个列或多个列组成的，每一列类似Java中的属性。5.表中的数据按照行来存储，每一行类似于Java中的对象。 4.mysql的安装与使用参照mysql安装文档 5.Mysql常用命令12345678910111213141516171819显示数据库-----&gt;show Databases;使用数据库-----&gt;use 数据库名；显示表----&gt;show tables;显式指定数据库的表----&gt;show tables from 数据库名；查看位于那个数据库----&gt;select database();显示表结构---&gt;desc 表名；查看数据库版本：---&gt;select version();查看数据库版本2:-----&gt;Dos:mysql --version;查看数据库信息-----&gt;show CREATE DATABASE mydb1;查看服务器中的数据库，并把mydb1的字符集修改为utf-8-----&gt;ALTER DATABASE mydb1character set utf8;删除数据库-----&gt;drop database mydb1;表中增加一栏信息-----&gt;alter table student add image blob;删除表-----&gt;drop table student;修改地址-----&gt;alter table student modify address varchar(100);删除一个属性-----&gt; alter table student drop image;修改表名-----&gt;rename table student to students;查看表的创建细节-----&gt;show create table students;修改表的字符集为 gbk-----&gt;alter table students character set gbk;列名name修改为studentname-----&gt;alter table students change name studentname varchar(100); 6.mysql语法规范12345671.不区分大小写，建议关键字大写，表名列名小写。2.每条命令最好用分号结尾。3.每条语句可以缩进，换行。4.注释单行注释：#注释文字 -- 注释文字多行注释：/* */ 二，DQL查询语言1.基础查询12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758**语法： select 查询列表 from 表名****查询列表：表中的字段，常量，表达式，函数****查询的结果是张虚拟的表格**1.查询表中的单个字段select last_name from employee;2.查询表中的多个字段select last_name,salary,email from employee;3.查询表中的所有字段select * from employee;4.查询常量值select 100;select &#x27;john&#x27;;5.查询表达式select 100*98;6.查询函数select version();7.起别名select last_name as name from employee;select last_name name from employee;8.去重查询员工表中涉及到的所有的部门编号select distinct department_id from employee;9.+的作用#运算符：两个操作数都为数值型，则做加法运算；#其中一方为字符型，试图将字符型数值转换成数值型，#如果转换成功，继续做加法运算；否则，将字符型数值#转换为0；10.使用concat实现连接#案例：查询员工名和性连接成一个字段SELECT CONCAT(username,PASSWORD) FROM USER;#任何数与null做运算结果都为null 2.条件查询语法： select 查询列表 from 表名 where 筛选条件 分类： ①按照条件表达式筛选条件运算符：&gt;,&lt;,=,!=,&gt;=,&lt;= 123456查询员工工资&gt;1w2的员工信息select * from employee where salary &gt;12000;查询部门编号！=90号的员工名和部门编号select name, dep_id from employee where dep_id 1=90； ②按照逻辑表达式筛选逻辑运算符：&amp;&amp;,||,!,AND,OR,NOT 1234查询工资在一万到两万之见的员工名，工资以及奖金。select name,salary ,jiangjin where salary between 10000 and 20000;查询部门编号不在90-110之间，或者工资高于15000的员工信息。select * from employee where department&lt;90||department&gt;110 ||salary &gt;15000; ③模糊查询like：一般和通配符搭配使用通配符：%任意多个字符，包含0个字符_任意单个字符BETWEEN AND:包含临界值IN:判断某个字段的值是否属于in列表中的某一项IS NULL,IS NOT NULL:=或者！=不能用来判断null安全等于&lt;=&gt;可以判断null 123456789101112131415查询员工名中包含a的员工信息select * from emp where name like %a%;查询员工名中第三个字符为e第五个字符为a的员工名和工资select name ,salary from emp where name like %__e_a%;员工名中第二个字符为_的员工名select name from emp where name like %_\\_%;查询员工编号在100到120之间的所有员工信息select * from emp where id between 100 and 120;查询员工的工种编号是IT_PRIG,AD_PRES,AD_VP中的一个员工名和工种编号；select name , id from emp where id in(IT_PRIG,AD_PRES,AD_VP);查询没有奖金的员工名和奖金率select salary , jjl from emp where salary is Null;查询有奖金的员工名和奖金率select salary ,jjl from emp where salary is not null; ④IF null的使用：123查询员工号为176的员工的姓名和部门号和年薪SELECT last_name ,department_id , salary*(1+IFNULL(commission_pct,0))*12 &#x27;年薪&#x27;FROM employees WHERE employee_id =176; 3.排序查询语法： select 查询列表 from 表 where 筛选条件 order by 排序列表 asc 或desc （升序或者降序，默认为升序） 12345678910查询员工信息，要求工资从高到低排序select * from emp order by salary desc;查询部门编号大于等于90的员工信息，按照入职时间先后排序select * from emp where dep_id &gt;=90 order by createtime asc;按照员工年薪的高低显示员工的信息和年薪select * ,年薪 from emp order by salary*(1+if null(jjl,0))*12 as 年薪 desc;按姓名长度显示员工的姓名和工资select name ,salary from emp order by length(name) asc;查询员工信息，先按照工资排序，再按照员工编号排序select * from emp order by salary asc,id asc; 4.常见函数功能：类似Java中的方法分类：单行函数分组函数 1.单行函数1.字符函数12345678910111213141516171819202122232425262728293031323334353637381.length 获取参数值的字节个数select * from emp order by length(name);2.concat 拼接字符串select concat(last_name,first_name) as 姓名 from emp;3.upper，lower 大小写转换函数案例：将姓变大写，名字变小写，然后拼接SELECT CONCAT(UPPER(last_name),LOWER(first_name))FROM employees;4.substr,SUBSTRING 截取字符串SELECT SUBSTR(&#x27;李莫愁&#x27;,2);SELECT SUBSTR(&#x27;李莫愁&#x27;,2,3);案例：姓名中首字符大写，其他的小写然后用_拼接显示出来SELECT CONCAT( UPPER(SUBSTR(last_name, 1, 1)), &#x27;_&#x27;, LOWER(SUBSTR(last_name, 2)) ) output FROM employees ;5.instr:返回字串第一次出现的索引，如果找不到返回0SELECT INSTR(&#x27;风急天高猿啸哀&#x27;,&#x27;天&#x27;) AS out_put;6.trim :去掉前后空格或前后指定字符SELECT LENGTH(TRIM(&#x27; 张三丰 &#x27;)) AS out_put;SELECT TRIM(&#x27;a&#x27; FROM &#x27;aaaa1aa2aaa3aaa&#x27;) AS out_put;7.lpad :用指定字符填满指定长度（左填充）SELECT LPAD(&#x27;苍老师&#x27;,10,&#x27;*&#x27;);8.rpad:用指定字符填满指定长度（右填充）SELECT RPAD(&#x27;苍老师&#x27;,10,&#x27;*&#x27;);9.replace 替换SELECT REPLACE(&#x27;千锋培训机构&#x27;,&#x27;千锋&#x27;,&#x27;尚硅谷&#x27;); 2.数学函数12345678910111.round:四舍五入SELECT ROUND(1.666);SELECT ROUND(1.567,2);2.ceil 向上取整SELECT CEIL(1.52);3.floor 向下取整SELECT FLOOR(1.52);4.truncate:截断（小数点后保留几位）SELECT TRUNCATE(1.65,2);5.mod:取余SELECT MOD(10,3); 3.日期函数123456789101112131415161718192021222324252627281.now:返回当前系统日期时间SELECT NOW();2.curdate:返回当前系统日期SELECT CURDATE();3.curtime:返回当前时间SELECT CURTIME();4.获取指定部分的年月日时分秒SELECT YEAR(NOW());SELECT YEAR(hiredate) FROM employees;5.str_to_date将字符通过指定的格式转化成日期SELECT STR_TO_DATE(&#x27;1998-3-2&#x27;,&#x27;%Y-%c-%d&#x27;) AS out_put;案例：查询入职时间为1992-4-3的员工信息SELECT * FROM employeesWHERE hiredate=STR_TO_DATE(&#x27;2016-3-3&#x27;,&#x27;%Y-%c-%d&#x27;);6.date_format 将日期转换成字符SELECT DATE_FORMAT(NOW(),&#x27;%y年%m月%d日&#x27;) AS 日期;案例：查询有奖金的员工名和入职日期（xx月/xx日 xx年）SELECT last_name, DATE_FORMAT(hiredate, &#x27;%c月/%d日 %y&#x27;) FROM employees WHERE commission_pct IS NOT NULL ; 4.其他函数123SELECT VERSION();SELECT DATABASE();SELECT USER(); 5.流程控制函数1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465661.if:IF else效果SELECT IF(10&gt;5,&#x27;true&#x27;,&#x27;false&#x27;);案例：查询如果有奖金就备注有，没有就备注没有。SELECT last_name, commission_pct, IF( commission_pct IS NULL, &#x27;没奖金&#x27;, &#x27;有奖金&#x27; ) AS 备注 FROM employees ;2.case函数1)switch-CASE语法:CASE 要判断的字段或者表达式WHEN 常量1 THEN 要显示的值1或者语句1WHEN 常量2 THEN 要显示的值2或者语句2...ELSE 要显示的值n或者语句n；案例：查询员工的工资，要求部门号==30，显示的工资为1.1倍，部门号==40，显示的工资为1.2倍，部门号==50，显示的工资为1.3倍，其他部门，显示原有工资。SELECT salary AS 原始工资, department_id , CASE department_id WHEN 30 THEN salary * 1.1 WHEN 40 THEN salary * 1.2 WHEN 50 THEN salary * 1.3 ELSE salary END AS 新工资 FROM employees ;2)CASE 使用2：语法：CASE WHEN 条件1 THEN 要显示的值1或语句1WHEN 条件2 THEN 要显示的值2或语句2...ELSE 要显示的值n或语句nEND案例：查询员工的工资情况如果&gt;2w，显示A如果&gt;1.5w，显示B如果&gt;1w，显示C否则，显示DSELECT salary, CASE WHEN salary &gt; 20000 THEN &#x27;A&#x27; WHEN salary &gt; 15000 THEN &#x27;B&#x27; WHEN salary &gt; 10000 THEN &#x27;C&#x27; ELSE &#x27;D&#x27; END AS 工资等级 FROM employees ; 2.分组函数功能：用作统计使用 123456789101112131415161718192021222324251.sum :求和SELECT SUM(salary) FROM employees;2.avg：平均值SELECT AVG(salary) FROM employees;3.max：最大值SELECT MAX(salary) FROM employees;4.min：最小值SELECT MIN(salary) FROM employees;5.count：计算个数SELECT COUNT(salary) FROM employees;总结①.sum,avg一般用于处理数值类型②.max，min，count用来处理任何类型③.以上分组函数都忽略null值④.可以和distinct搭配SELECT SUM(DISTINCT salary) 纯净,SUM(salary) FROM employees;6.count的详细介绍①select COUNT(*) FROM employees;②select COUNT(1) FROM employees;③和分组函数一同查询的字段要求是group by后的字段。 5.分组查询GROUP BY 和分组函数对应分组查询中分组条件分为两类 数据源 位置 关键字 分组前筛选 原始表 GROUP BY 子句的前面 WHERE 分组后筛选 分组后的结果集 GROUP BY 子句的后面 HAVING 分组函数做条件肯定是放在having子句中。group BY 子句支持单个字段分组，多个字段分组（多个字段之间用逗号隔开没有顺序要求），表达式或函数。也可以添加排序，放在整个分组查询的最后。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869案例：查询每个工种的最高工资SELECT MAX(salary), job_id FROM employees GROUP BY job_id ORDER BY MAX(salary) ASC ;案例：查询邮箱中包含a字符的，每个部门的平均工资SELECT AVG(salary), department_id FROM employees WHERE email LIKE &#x27;%a%&#x27; GROUP BY department_id ;#select Avg(salary),dep_id from employee where email like %a% group by dep_id ;案例：查询有奖金的每个领导手下员工的最高工资SELECT MAX(salary), manager_id FROM employees WHERE commission_pct IS NOT NULL GROUP BY manager_id ;#select max(salary) ,manage_id from employees where commission_pct is not null group by manager_id;案例：哪个部门的员工个数大于二？SELECT COUNT(*), department_id FROM employees GROUP BY department_id HAVING COUNT(*) &gt; 2 ;#select dep_id from emp group by dep_id having count(*)&gt;2;案例：查询每个工种有奖金的员工的最高工资&gt;12000的工种编号和最高工资SELECT MAX(salary), job_id FROM employees WHERE commission_pct IS NOT NULL GROUP BY job_id HAVING MAX(salary) &gt; 12000 ;#select job_id ,max(salary) from emp where commission_pct IS NOT NULL group by job_id having max(salary)&gt;12000;案例：查询领导编号&gt;102的每个领导手下的最低工资&gt;5000的领导编号是哪个？SELECT manager_id ,MIN(salary)FROM employeesWHERE manager_id&gt;102GROUP BY manager_idHAVING MIN(salary)&gt;5000;#select manager_id from emp where manager_id&gt;102 group by manager_id having min(salary)&gt;5000;#按照员工姓名的长度分组，查询每一组的员工个数，筛选员工个数&gt;5的有哪些？SELECT COUNT(*) AS cFROM employeesGROUP BY LENGTH(last_name) HAVING c&gt;5;# select count(*) from emp group by length(name) having count(*)&gt;5;#查询每个部门每个工种的员工的平均工资SELECT AVG(salary),job_idFROM employeesGROUP BY department_id,job_id;#select avg(salary) from emp group by dep_id,job_id;#查询每个部门每个工种的员工的平均工资并且按照平均工资的高低显示SELECT AVG(salary),job_idFROM employeesGROUP BY department_id,job_idORDER BY AVG(salary) ASC;#select avg(salary) from emp group by dep_id,job_id order by avg(salary) asc; 6.连接查询又称为多表查询，当查询的字段来自多个表时，就会用到连接查询。**笛卡尔乘积现象：表1有m行，表2有n行，结果：m_n行_发生原因：没有有效的连接条件 分类 ①按年代分类sql92:仅仅支持内连接sql99：不支持全外连接 ②按功能分类 内连接 外连接 交叉连接 等值连接 左外连接 非等值连接 右外连接 自连接 全外连接 1.等值连接①多表等值连接的结果为多表的交集部分②n表连接，至少需要n-1个连接条件③多表的顺序没有要求④一般需要为表起别名⑤可以搭配前面介绍的所有子句使用，比如排序，分组，筛选 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192#案例一：查询女优名对应的男优名SELECT NAME, boyName FROM beauty, boys WHERE beauty.boyfriend_id = boys.`id` ;#select name, boyname from girl ,boy where girl.boyfriend_id=boy.id;#案例：查询员工名和对应的部门名SELECT last_name, department_name FROM employees, departments WHERE employees.`department_id` = departments.`department_id` ;#select name ,dep_name from emp e,dep d where e.dep.id= d.id;#案例：查询员工名，工种号，工种名。SELECT last_name, emp.`job_id`, job_title FROM employees emp, jobs job WHERE emp.`job_id` = job.`job_id` ;#select name , e.job_id,job_title from emp e,job j where e.job_id=j.id;#案例：查询有奖金的员工名和部门名SELECT last_name, department_name FROM employees emp, departments dep WHERE commission_pct IS NOT NULL &amp;&amp; emp.`department_id` = dep.`department_id` ;#select name ,dep_name from emp e ,dep d where e.dep_id =d.id &amp;&amp;e.salary_pct is not null;#案例：查询城市名第二个字符为o的部门SELECT department_name FROM locations l, departments d WHERE l.`location_id` = d.`location_id` AND l.`city` LIKE &#x27;_o%&#x27; ;#select dep_name from location l , dep d where l.city like %_o% &amp;&amp; l.id =d.location_id;#案例：查询每个城市的部门个数SELECT COUNT(*), city FROM locations l, departments d WHERE l.`location_id` = d.`location_id` GROUP BY l.`city` ;#select count(*),city from loca l,dep d where l.loc_id=d.loc_id group by count(*) asc;#案例：查询有奖金的每个部门的部门名和部门的领导编号和该部门的最低工资SELECT d.`department_name`, d.manager_id, MIN(salary) FROM employees e, departments d WHERE e.`department_id` = d.`department_id` AND e.`commission_pct` IS NOT NULL GROUP BY d.`department_id`, d.`department_name` ;#select dep_name ,d.manager_id ,min(salary) from emp e ,dep d where e.`department_id` = d.`department_id` AND e.`commission_pct` IS NOT NULL GROUP BY d.`department_id`,d.`department_name` ;#案例：查询每个工种的工种名和员工的个数，并且按照员工个数降序排序SELECT j.job_title, COUNT(*) FROM jobs j, employees e WHERE j.`job_id` = e.`job_id` GROUP BY e.`job_id`, j.`job_title` ORDER BY COUNT(*) DESC ;#案例：查询员工名，部门名和所在城市SELECT last_name, department_name, city FROM employees e, departments d, locations l WHERE e.`department_id` = d.`department_id` AND d.`location_id` = l.`location_id` ; 2.非等值连接123456789#案例：查询员工的工资和工资级别SELECT DISTINCT salary, grade_level FROM employees e, job_grades j WHERE e.salary &gt;= j.lowest_sal &amp;&amp; e.salary &lt;= j.highest_sal ORDER BY salary ASC ; 3.自连接12345678#案例：查询员工名和上级的名称SELECT e.last_name, m.last_name FROM employees e, employees m WHERE e.manager_id = m.employee_id ; 4.内连接INNER 可以省略 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475#查询员工名，部门名SELECT last_name, department_name FROM employees e INNER JOIN departments d ON e.department_id = d.department_id ;#查询名字中包含e的员工名和工种名SELECT last_name, job_title FROM employees e INNER JOIN jobs j ON e.job_id = j.job_id WHERE last_name LIKE &#x27;%e%&#x27; ;#查询部门个数&gt;3的城市名和部门个数SELECT city, COUNT(*) FROM departments d INNER JOIN locations l ON d.`location_id` = l.`location_id` GROUP BY cityHAVING COUNT(*) &gt; 3 ;#查询哪个部门的部门员工个数&gt;3的部门名和员工个数，并按照个数降序排序SELECT department_name, COUNT(*) FROM employees e INNER JOIN departments d ON e.`department_id` = d.`department_id` GROUP BY e.department_id HAVING COUNT(*) &gt; 3 ORDER BY COUNT(*) DESC ;#查询员工名，部门名，工种名，并按照部门名降序排序SELECT last_name, department_name, job_title FROM employees e INNER JOIN departments d ON e.`department_id` = d.`department_id` INNER JOIN jobs j ON e.`job_id` = j.`job_id` ORDER BY department_name DESC ;#查询员工工资级别SELECT grade_level, salaryFROM job_grades j INNER JOIN employees e ON e.`salary` BETWEEN j.`lowest_sal` AND j.`highest_sal` ;#查询每个工资级别的个数，并且降序排序SELECT grade_level,COUNT(*)FROM employees eINNER JOIN job_grades jON e.`salary` BETWEEN j.`lowest_sal` AND j.`highest_sal` GROUP BY grade_levelORDER BY COUNT(*) DESC;#查询员工的名字和上级的名字SELECT e1.last_name, e2.last_nameFROM employees e1INNER JOIN employees e2ON e1.`employee_id`=e2.`manager_id`; 5.左外连接语法：SELECT 查询列表FROM 表1 【连接类型】JOIN 表2ON 连接条件WHERE 筛选条件GROUP BY 分组HAVING 筛选条件ORDER BY 排序条件连接类型：内连接：inner左外连接：left右外连接：right全外连接：full交叉连接：cross外连接用于查询一个表中有，另一个表中没有的数据左外连接，left左边是主表右外连接，right右边是主表Mysql不支持全外连接 1234567#没有男朋友的女生SELECT g.`name`,b.`boyName`FROM beauty gLEFT JOIN boys bON g.`boyfriend_id`=b.`id`WHERE b.`boyName` IS NULL; 6.交叉连接笛卡尔乘积 7.子查询出现在其它语句中的select语句，称为子查询或内查询外部的查询语句，称为主查询或外查询分类： ①按照子查询出现的位置： select后面 from后面 where或having后面 exists后面 仅仅支持标量子查询 支持表子查询 标量子查询，列子查询 表子查询 ②按照结果集的行列数不同： 标量子查询 列子查询 行子查询 表子查询 结果只有一行一列 结果一列多行 一行多列 多行多列 1）where或having后面特点：子查询一般放在小括号内子查询一般放在条件的右边标量子查询，一般搭配着单行操作符列子查询：一般搭配多行操作符使用 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211.标量子查询#谁的工资比Abel高SELECT last_name FROM employees WHERE salary &gt; (SELECT salary FROM employees WHERE last_name = &#x27;Abel&#x27;) ;#返回job_id于141号员工相同，salary比143号员工多的员工 姓名，job_id和工资SELECT last_name, job_id, salary FROM employees WHERE job_id = (SELECT job_id FROM employees WHERE employee_id = 141) AND salary &gt; (SELECT salary FROM employees WHERE employee_id = 143)#返回公司工资工资最少的员工的姓名，job_id,salarySELECT last_name, job_id, salary FROM employees WHERE salary = (SELECT MIN(salary) FROM employees);#查询最低工资大于50号部门最低工资的部门id和其最低工资SELECT department_id, MIN(salary) FROM employees GROUP BY department_id HAVING MIN(salary) &gt; (SELECT MIN(salary) FROM employees WHERE department_id = 50) ;2.列子查询多行操作符：IN / NOT in：等于列表中的任意一个ANY / SOME ：和子查询返回的某一个值比较ALL ：和子查询返回的所有值比较#返回location_id是1400或者1700的部门中的所有员工姓名SELECT last_name FROM employees WHERE department_id IN (SELECT DISTINCT department_id FROM departments WHERE location_id IN (1400, 1700)) ;#返回其他工种中比job_id为IT_PROG部门任意工资低的员工#工号，姓名，job_id以及salarySELECT employee_id, last_name, job_id, salary FROM employees WHERE salary &lt; (SELECT MAX(salary) FROM employees WHERE job_id = &#x27;IT_PROG&#x27;) AND job_id !=&#x27;IT_PROG&#x27;;#返回其他工种中比job_id为IT_PROG部门所有工资低的员工#工号，姓名，job_id以及salarySELECT employee_id, last_name, job_id, salary FROM employees WHERE salary &lt; (SELECT MIN(salary) FROM employees WHERE job_id = &#x27;IT_PROG&#x27;) AND job_id !=&#x27;IT_PROG&#x27;;*********************************3.行子查询#查询员工编号最小并且工资最高的员工信息SELECT * FROM employees WHERE employee_id = (SELECT MIN(employee_id) FROM employees) AND salary = (SELECT MAX(salary) FROM employees) 2）SELECT 后面123456789101112131415161718192021#查询每个部门的员工个数SELECT d.*, (SELECT COUNT(*) FROM employees e WHERE e.department_id = d.department_id) FROM departments d ;#查询员工号等于102的部门名SELECT department_name FROM departments WHERE department_id = (SELECT department_id FROM employees WHERE employee_id = 102) ; 3）FROM 后面1234567891011121314#查询每个部门平均工资的工资等级SELECT grade_level ,aa.department_idFROM (SELECT AVG(salary) ag, department_id FROM employees GROUP BY department_id) aa INNER JOIN job_grades j ON aa.ag BETWEEN lowest_sal AND highest_sal ; 4）exists后面（相关子查询）12345678#查询有员工的部门名SELECT department_name FROM departments dWHERE EXISTS(SELECT * FROM employees e WHERE d.department_id=e.department_id);#查询没有女朋友的男生信息SELECT bo.* FROM boys bo WHEREbo.`id` NOT IN(SELECT boyfriend_id FROM beauty); 5）子查询经典案例祥讲1234567891011121314151617181920212223242526272829303132331.查询工资最低的员工信息：last_name,salarySELECT last_name,salary FROM employeesWHERE salary=(SELECT MIN(salary) FROM employees);2.查询平均工资最低的部门信息SELECT * FROM departments WHERE department_id=(SELECT department_id FROM employees GROUP BY department_id ORDER BY AVG(salary)LIMIT 1)3.查询平均工资最低的部门信息和该部门的平均工资SELECT d.*,a1.ag FROM departments d JOIN (SELECT AVG(salary) ag,department_id FROM employees GROUP BY department_id ORDER BY AVG(salary)LIMIT 1) a1ON d.department_id=a1.department_id4.查询平均工资最高的job信息SELECT j.* FROM jobs j WHERE j.job_id=(SELECT job_id FROM employeesGROUP BY job_id ORDER BY AVG(salary) DESC LIMIT 1)5.查询平均工资高于公司平均工资的部门有哪些SELECT department_id FROM (SELECT department_id ,AVG(salary) AS avg1 FROM employees GROUP BY department_id) e1WHERE e1.avg1&gt;(SELECT AVG(salary) AS avg2 FROM employees) 6.查询出公司中所有manager的详细信息SELECT * FROM employeesWHERE employee_id IN(SELECT DISTINCT manager_id FROM employees);7.各个部门中，最高工资中最低的那个部门的最低工资是多少SELECT MIN(salary) FROM employees GROUP BY department_idHAVING department_id=(SELECT department_id FROM employees GROUP BY department_id ORDER BY MAX(salary)LIMIT 1)8.查询平均工资最高的部门的manager的详细信息：last_name,department_id,email,salarySELECT last_name,department_id,email,salary FROM employees WHERE employee_id=(SELECT manager_id FROM departments WHERE department_id=(SELECT department_id FROM employees GROUP BY department_id ORDER BY AVG(salary)DESC LIMIT 1)) 8.分页查询**语法：limit(currentPage-1)size,size 123456789#查询前五条员工信息SELECT * FROM employees LIMIT 0,5;#查询第11-25条员工信息SELECT * FROM employees LIMIT 10,15;#查询有奖金的员工，并且工资最高的前十名显示出来SELECT * FROM employeesWHERE commission_pct IS NOT NULLORDER BY salary DESC LIMIT 0 ,10; 9.联合查询要查询的结果来自于多个表，且多个表没有直接的连接关系，单查询的信息一致时特点：1.要求多条查询语句的查询列数是一致的2.要求多条查询语句的查询的每一列的类型和顺序最好一致3.union关键字默认去重，如果使用union all 可以不去除重复项 1234案例：查询员工部门编号大于90或邮箱包含a的员工信息SELECT * FROM employees WHERE department_id&gt;90UNIONSELECT * FROM employees WHERE email LIKE &#x27;%a%&#x27;; 三，DML数据操作语言插入insert 1234567891011121314151617一：插入语句#插入beauty一行数据INSERT INTO beauty(NAME,sex,borndate,phone,photo,boyfriend_id)VALUES(&#x27;波多野吉依&#x27;,&#x27;女&#x27;,&#x27;1998-11-11&#x27;,&#x27;13342969497&#x27;,NULL,10)#可以为null的列如何不插入值直接写null，或列名少写一列INSERT INTO beauty(NAME,sex,borndate,phone,photo,boyfriend_id)VALUES(&#x27;小泽玛利亚&#x27;,&#x27;女&#x27;,&#x27;1999-11-11&#x27;,&#x27;13342456497&#x27;,NULL,11)INSERT INTO beauty VALUES(15,&#x27;马蓉&#x27;,&#x27;女&#x27;,&#x27;1989-11-11&#x27;,&#x27;13342456123&#x27;,NULL,12);INSERT INTO beauty SET id=16,NAME=&#x27;刘亦菲&#x27;, sex=&#x27;女&#x27;,borndate=&#x27;1989-10-01&#x27;,phone=&#x27;15945231056&#x27;,boyfriend_id=16;#insert 嵌套子查询，将一个表的数据插入另一张表INSERT INTO beauty (NAME,sex,borndate,phone,boyfriend_id)SELECT &#x27;妲己&#x27;,&#x27;女&#x27;,&#x27;1111-11-11&#x27;,&#x27;13146587954&#x27;,0; 修改update 1234567891011121314151617181920二，修改 UPDATE beauty SET phone=&#x27;110&#x27; WHERE id=16;多表修改：sql99UPDATE 表1 别名INNER|LEFT|RIGHT JOIN 表2 别名ON 连接条件SET 列=值WHERE 筛选条件#修改张无忌的女朋友手机号为114UPDATE beauty gINNER JOIN boys bON g.boyfriend_id=b.idSET g.phone=&#x27;114&#x27;WHERE b.boyName=&#x27;张无忌&#x27;;#修改没有男朋友的女生的男朋友编号都为4号UPDATE beauty gLEFT JOIN boys bON g.`boyfriend_id`=b.idSET g.`boyfriend_id`=4WHERE b.id=NULL; 删除delete 1234567891011121314151617181920212223三，删除DELETE 和 TRUNCATE 的区别：1.delete可以加where条件，truncate不行2.truncate删除效率高3.加入要删除的表中有自增列，用delete删除整个表后在插入数据，从断点处开始插入用truncate删除后在插入数据，从1开始。4.truncate删除没有返回值，delete有返回值5.truncate删除不能回滚，delete删除可以回滚DELETE FROM beauty WHERE id=17;语法：truncate TABLE 表名;#删除张无忌的女朋友的信息DELETE g FROM beauty gINNER JOIN boys bON g.boyfriend_id=b.idWHERE b.id=1;#删除黄晓明以及他女朋友的信息DELETE b,g FROM beauty gINNER JOIN boys bON b.`id`=g.`boyfriend_id`WHERE b.`boyName`=&#x27;黄晓明&#x27;;多表删除 :TRUNCATETRUNCATE TABLE boys 四，DDL数据定义语言1.库和表的管理12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849一，库的管理创建 CREATECREATE DATABASE IF NOT EXISTS mydb1 ;修改 ALTER1.更改字符集ALTER DATABASE mydb1 CHARACTER SET utf8;删除 DROPDROP DATABASE IF EXISTS school;二，表的管理创建 CREATECREATE TABLE book(id INT PRIMARY KEY,b_name VARCHAR(30),price DOUBLE,author_id INT ,publishDate DATE);DESC book ;CREATE TABLE author(id INT PRIMARY KEY ,au_name VARCHAR(20),nation VARCHAR(10));DESC author;修改 ALTER1.修改列名ALTER TABLE book CHANGE COLUMN publishDate pub_date DATETIME;2.修改列的类型或约束ALTER TABLE book MODIFY COLUMN pub_date DATE;3.添加新列ALTER TABLE author ADD COLUMN annual DOUBLE;4.删除新列ALTER TABLE author DROP COLUMN annual;5.修改表名ALTER TABLE author RENAME TO book_author;删除 DROPDROP TABLE IF EXISTS my_employee;SHOW TABLES;复制1.仅仅复制表的结构CREATE TABLE copy LIKE book_author;2.复制表的结构加数据CREATE TABLE copy2SELECT * FROM book_author;3.复制部分结构CREATE TABLE copy3 SELECT id,au_nameFROM book_authorWHERE id=0; 2.数据类型数值型1.整型 TINYINT SMALLINT MEDIUMINT INT/INTEGER BIGINT 1 2 3 4 8 12345如何设置无符号和有符号(默认有符号)DROP TABLE tab_int;CREATE TABLE tab_int(t1 INT,t2 INT UNSIGNED);INSERT INTO tab_int(t1,t2) VALUES(-1,1);DESC tab_int; 1）如果插入的数值超出了整形的范围，会报out of range异常，并且插入临界值。2）如果不设置长度，会有默认的长度。长度代表了显示的最大宽度，如果不够会用0在左边填充，但必须搭配zerofill使用。2.小数①定点数dec（M,D）②浮点数float（4） ，double（8）M，D的意思：M指定一共多少位，D指定小数几位，超出会四舍五入。MD都可以省略，如果是dec，则M默认为10，D默认为0如果是浮点数，则会根据插入数值的精度改变精度定点型精度相对较高。3.字符型①较短的文本CHAR(M)默认为1,VARCHAR(M)M:字符数char：固定长度字符，比较耗费空间，但是效率高。varchar：可变长度字符 123456789ENUM 枚举类CREATE TABLE tab_char( t1 ENUM(&#x27;a&#x27;,&#x27;c&#x27;,&#x27;b&#x27;));SET 集合CREATE TABLE tab_set(s1 SET(&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;,&#x27;d&#x27;));INSERT INTO tab_set(s1) VALUES(&#x27;a,b&#x27;); BINARY:保存较短的二进制。②较长的文本text（文本）,BLOB(较大的二进制)4.日期型DATE:日期DATETIME:日期加时间，8字节timestamp：跟时区有关系，建议使用，4字节time：时间year：年 123456789CREATE TABLE tab_date(t1 DATETIME,t2 TIMESTAMP);INSERT INTO tab_date(t1,t2)VALUES(NOW(),NOW());SELECT * FROM tab_date;SET time_zone=&#x27;+9:00&#x27;;#设置时区为东9区 3.常见约束含义：一种限制，用于限制表中的数据，保证数据的一致性。 NOT NULL DEFAULT PRIMARY KEY 唯一，且不为空 UNIQUE 唯一，可以为空 CHECK Mysql不支持 FOREIGN KEY 外键约束，用于限制两个表的关系，用于保证该字段的值必须来自于主表的关联列的值。约束的分类：列级约束：除外键约束表级约束：除了非空，默认。CREATE TABLE 表名(字段1 字段类型 列级约束,字段2 字段类型 列级约束,表级约束); 1234567891011121314151617181920212223242526#创建表时添加列级约束DROP TABLE tab_test;CREATE TABLE tab_test(id INT PRIMARY KEY,stu_name VARCHAR(20) NOT NULL,gender CHAR DEFAULT &#x27;男&#x27;,seat_id INT UNIQUE, major_id INT REFERENCES tab_major(id) );CREATE TABLE tab_major(id INT PRIMARY KEY ,major_name VARCHAR(20) NOT NULL);DESC tab_test;SHOW INDEX FROM tab_test;#查看索引信息#添加表级约束CREATE TABLE tab_test(id INT PRIMARY KEY AUTO_INCREMENT,stu_name VARCHAR(20) NOT NULL,gender CHAR DEFAULT &#x27;男&#x27;,seat_id INT UNIQUE, major_id INT ,CONSTRAINT m_id FOREIGN KEY(major_id) REFERENCES tab_major(id) );CONSTRAINT m_id 可以省略 面试题：主键约束和唯一约束的区别：都可以保证唯一性，主键不能为空 ，unique 能为空，但是只能有一个null。主键只能有1个，unique可以有多个。都允许两个列组合成一个约束。面试题：外键：要求在从表设置外键关系从表的外键列类型和主表的关联列类型一致，名称无要求要求主表的关联列必须是主键或者唯一键插入数据应该先插入主表再插入从表删除数据应该先删除从表，在删除主表二，修改表时添加约束 1234567891011CREATE TABLE tab_test2(id INT ,stu_name VARCHAR(20) ,gender CHAR ,seat_id INT , major_id INT );ALTER TABLE tab_test2 MODIFY COLUMN stu_name VARCHAR(20) NOT NULL ;ALTER TABLE tab_test2 MODIFY COLUMN id INT PRIMARY KEY AUTO_INCREMENT;#添加外键ALTER TABLE tab_test2 ADD FOREIGN KEY(major_id) REFERENCES tab_major(id); 4.标识列自增长列 AUTO_INCREMENT特点：1.表示必须和一个key搭配2.一个表最多一个标识列3.标识列类型只能是数值型4.标识列可以通过set auto_increment_increment=3;设置步长 1234CREATE tab_auto(id INT PRIMARY KEY AUTO_INCREMENT,NAME VARCHAR(20) NOT NULL); 五，TCL语言：事务控制语言事务：一个或一组sql语句组成的执行单元， 要么全部执行,要么都不执行。存储引擎:在MySQL中的数据用各种不同的技术存储在文件中。通过show ENGINES;来查看mysql支持的存储引擎。innodb引擎支持事务。事务的ACID属性：1.原子性:事务是一个不可分割的工作单位，要么都发生，要么都不发生。2.一致性：事务必须使数据库从一个一致性状态变为另一个一致性状态。3.隔离性：一个事务的执行不能被另一个事务干扰。4.持久性：事务一旦被提交，对数据库事务的改变就是永久性的。 DELETE 和 TRUNCATE 在事务中的区别： 1234567891011演示deleteSET autocommit=0;START TRANSACTION;DELETE FROM tab_teacher;ROLLBACK;演示 TRUNCATESET autocommit=0;START TRANSACTION;TRUNCATE TABLE tab_teacher;ROLLBACK;DELETE 是直接删除表中数据，truncate是江表删除，创建一张与原来一样的空表。 六，视图含义：虚拟表，和普通表格一样使用通过表动态生成的数据 1.创建视图语法：CREATE VIEW 视图名AS查询语句 ; 12345678910111213141516171819202122# 案例：查询姓名中包含a字符的员工名，部门名和工种信息create view view1 as select e.last_name,d.department_name ,j.job_title from employees einner join departments d on e.department_id = d.department_id inner join jobs j on e.job_id = j.job_idwhere e.last_name like &#x27;%a%&#x27;;select * from view1;# 案例：查询各个部门的平均工资级别create view view2 asselect j.grade_level ,aa.department_id from job_grades jinner join (select avg(salary) avg_s,department_id from employees group by department_id) aa on aa.avg_s between j.lowest_sal and j.highest_sal;select * from view2;# 案例：查询平均工资最低的部门信息create view view3 asselect avg(salary) avg_s ,department_idfrom employeesgroup by department_idorder by avg_s asclimit 1;select * from view3; 2.视图修改①create OR REPLACE VIEW 视图名 AS 查询语句;②alter VIEW 视图名 AS 查询语句; 3.删除视图DROP VIEW v1,v2; 4.查看视图DESC v1; 12345678910#创建视图emp_v1，要求查询电话号码以011开头的员工姓名和工资，邮箱CREATE VIEW emp_v1 ASSELECT last_name ,salary,email FROM employees WHEREphone_number LIKE &#x27;%011&#x27;;#创建视图emp_v2,要求查询部门的最高工资高于12000的部门信息CREATE VIEW v4 ASSELECT department_id FROM employees GROUP BY department_idHAVING MAX(salary)&gt; 12000;CREATE VIEW emp_v2 ASSELECT * FROM departments WHERE department_id IN(SELECT * FROM v4); 5.视图的更新视图的可更新性和视图中查询的定义有关，以下类型的视图是不能更新的。1.包含以下关键字的sql语句：分组函数，distinct，group by，having，union2.常量视图3.select中包含子查询的4.join5.from 一个不能更新的视图6.where子句的子查询引用了from子句的表 6.视图和表的对比： 创建语法的关键字 是否实际占用物理空间 使用 视图 CREATE VIEW 只是保存了sql逻辑 增删改查，一般不能增删改 表 CREATE TABLE 占用 增删改查 七，变量系统变量 ：变量由系统提供，不是用户自定义，属于服务器层面。查看系统所有变量：show GLOBAL VARIABLES;查看满足条件的部分系统变量： SHOW GLOBAL VARIABLES LIKE ‘%char%’;查看指定的某个系统变量的值： SELECT @@global.autocommit;为某个系统变量赋值：set @@global.系统变量名=值;全局变量:GLOBAL作用域：服务器每次启动将为所有的全局变量赋初始值，针对于所有的会话有效，但不能跨重启。会话变量:SESSION作用域：针对当前的会话有效。用户自定义变量用户变量声明： SET/SELECT @用户变量名 :=值;赋值：通过 SELECT 字段 INTO 变量名;或 SET/SELECT @用户变量名 :=值;使用：select @用户变量名;应用在任何地方。作用域：针对当前会话和连接有效。局部变量作用域：作用在定义它的begin END 块中。声明： DECLARE 变量名 类型 （default 值）;赋值：通过 SELECT 字段 INTO 变量名;或 SET/SELECT @变量名 :=值;使用：select @变量名;只能放在begin END 中的第一句话 八，存储过程和函数存储过程：一组预先定义好的sql语句集合，理解成批处理语句。1.提高代码的重用性2.简化操作3.减少了编译次数并且减少了和数据库服务器的连接次数，提高了效率。 1.创建语法：CREATE PROCEDURE 存储过程名（参数列表）BEGIN一组合法的sql语句;END参数列表：参数模式 参数名 参数类型 参数模式：in：该参数可以作为输入，也就是该参数需要调用方传入值OUT ：该参数可以作为输出，也就是该参数可以作为返回值inout：该参数既可以作为输入又可以作为输出 如果存储过程只有一句话，begin END 可以省略 存储过程体中的每条sql语句的结尾需要必须加分号，存储过程的结尾可以使用 DELIMITER 重新设置。 2.调用CALL 存储过程名（实参列表）; 3.案例1234567891011121314151617181920212223242526272829303132333435363738394041424344#插入到admin表中五条记录DELIMITER $CREATE PROCEDURE my_a()BEGININSERT INTO admin(username,PASSWORD) VALUES(&#x27;yin&#x27;,&#x27;666&#x27;);INSERT INTO admin(username,PASSWORD) VALUES(&#x27;aa&#x27;,&#x27;123&#x27;);INSERT INTO admin(username,PASSWORD) VALUES(&#x27;bb&#x27;,&#x27;666&#x27;);INSERT INTO admin(username,PASSWORD) VALUES(&#x27;cc&#x27;,&#x27;123&#x27;);INSERT INTO admin(username,PASSWORD) VALUES(&#x27;dd&#x27;,&#x27;666&#x27;);END $#创建存储过程实现 根据女生名查询对应的男生信息DELIMITER $CREATE PROCEDURE my_b(IN beauty_name VARCHAR(20))BEGIN SELECT bo.* FROM boys bo RIGHT JOIN beauty b ON bo.id=b.boyfriend_id WHERE b.name=beauty_name;END $CALL my_b(&#x27;热巴&#x27;);#根据女生名返回他的男朋友名DELIMITER $CREATE PROCEDURE my_d(IN beautyName VARCHAR(20),OUT boyName VARCHAR(20))BEGIN SELECT bo.boyName INTO boyName FROM boys bo INNER JOIN beauty b ON bo.id=b.boyfriend_id WHERE b.name=beautyName;END $CALL my_d(&#x27;小昭&#x27;,@b_name);SELECT @b_name;#传入两个值a，b，最终翻倍返回a和bDELIMITER $CREATE PROCEDURE my_e(INOUT a INT ,INOUT b INT )BEGIN SET a=a*2; SET b=b*2;END $SET @m=10;SET @n=20;CALL my_e(@m,@n);SELECT @m,@n; 4.删除存储过程12DROP PROCEDURE 存储过程名DROP PROCEDURE my_a; 5.查看存储过程的信息1SHOW CREATE PROCEDURE my_b; 函数存储过程可以有0/n个返回值：适合批量增删改函数有且仅有一个返回值：适合查询 1.创建12345DELIMITER $CREATE FUNCTION 函数名(参数列表) RETURNS 返回类型BEGINEND 注意：参数列表：参数名，参数类型一定会有return语句 2.使用SELECT 函数名(参数列表) 12345678910111213141516171819#返回公司员工个数DELIMITER $CREATE FUNCTION my_f1() RETURNS INTBEGINDECLARE c INT DEFAULT 0 ; SELECT COUNT(*) INTO c FROM employees; RETURN c;END $SELECT my_f1();#根据员工名返回他的工资DELIMITER $CREATE FUNCTION my_f2(NAME VARCHAR(20)) RETURNS DOUBLEBEGIN DECLARE c DOUBLE; SELECT salary INTO c FROM employees WHERE last_name=NAME; RETURN c;END $SET @a=&#x27;Hunold&#x27;;SELECT my_f2(@a); 3.查看1SHOW CREATE FUNCTION my_f2; 4.删除1DROP FUNCTION my_f2; 九，流程控制分支结构1.if （表达式1，表达式2，表达式3）如果表达式1成立，就返回表达式2的值，否则返回表达式3的值。应用在任何地方 2.case1)switch-CASE语法:CASE 要判断的字段或者表达式WHEN 常量1 THEN 要显示的值1或者语句1WHEN 常量2 THEN 要显示的值2或者语句2…ELSE 要显示的值n或者语句n； 1234567891011121314151617181920案例：查询员工的工资，要求部门号==30，显示的工资为1.1倍，部门号==40，显示的工资为1.2倍，部门号==50，显示的工资为1.3倍，其他部门，显示原有工资。SELECT salary AS 原始工资, department_id , CASE department_id WHEN 30 THEN salary * 1.1 WHEN 40 THEN salary * 1.2 WHEN 50 THEN salary * 1.3 ELSE salary END AS 新工资 FROM employees ; 2)CASE 使用2：语法：CASEWHEN 条件1 THEN 要显示的值1或语句1WHEN 条件2 THEN 要显示的值2或语句2…ELSE 要显示的值n或语句nEND 12345678910111213141516171819202122232425262728293031323334案例：查询员工的工资情况如果&gt;2w，显示A如果&gt;1.5w，显示B如果&gt;1w，显示C否则，显示DSELECT salary, CASE WHEN salary &gt; 20000 THEN &#x27;A&#x27; WHEN salary &gt; 15000 THEN &#x27;B&#x27; WHEN salary &gt; 10000 THEN &#x27;C&#x27; ELSE &#x27;D&#x27; END AS 工资等级 FROM employees 可以放在任何地方 #创建存储过程，根据传入的成绩，显示等级，90A,80B，70C，60D ，F DELIMITER $ CREATE PROCEDURE my_1(IN score INT) BEGIN CASE WHEN score BETWEEN 90 AND 100 THEN SELECT &#x27;A&#x27;; WHEN score BETWEEN 80 AND 90 THEN SELECT &#x27;B&#x27;; WHEN score BETWEEN 70 AND 80 THEN SELECT &#x27;C&#x27;; WHEN score BETWEEN 70 AND 60 THEN SELECT &#x27;D&#x27;; ELSE SELECT &#x27;E&#x27;; END CASE; END $CALL my_1(95); 3.if语法：IF 条件1 THEN 语句1;ELSEIF 条件2 THEN 语句2;…ELSE 语句n;END IF;只能用在begin end中 12345678910111213#创建存储过程，根据传入的成绩，返回等级，90A,80B，70C，60D ，FDELIMITER $ CREATE FUNCTION my_2( score INT) RETURNS CHAR BEGIN IF score &gt;=90 THEN RETURN&#x27;A&#x27;; ELSEIF score &gt;=80 THEN RETURN&#x27;B&#x27;; ELSEIF score &gt;=70 THEN RETURN&#x27;C&#x27;; ELSEIF score &gt;=60 THEN RETURN&#x27;D&#x27;; ELSE RETURN&#x27;E&#x27;; END IF; END $ SELECT my_2(85); 循环结构在存储过程或函数里面使用 1.while语法：标签:WHILE 循环条件 DO循环体;END WHILE 标签;循环控制和标签搭配使用 2.loop语法：标签： LOOP循环体;END LOOP 标签; 3.repeat语法：标签： REPEAT循环体;UNTIL 结束循环的条件END REPEAT 标签; 循环控制ITERATE 类似continueLEAVE 类似break left join==left outer join a left join b 就是取a和b的交集加a剩下的部分 inner join a inner join b就是取交集","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/tags/MySQL/"}]},{"title":"MySQL[二]概述","slug":"MySQL/MySQL[二]概述","date":"2022-01-10T13:58:17.078Z","updated":"2022-01-11T03:14:31.883Z","comments":true,"path":"2022/01/10/MySQL/MySQL[二]概述/","link":"","permalink":"https://yinhuidong.github.io/2022/01/10/MySQL/MySQL[%E4%BA%8C]%E6%A6%82%E8%BF%B0/","excerpt":"","text":"一，一条SQL的查询流程 去连接池获取连接 查询缓存，命中返回，否则继续向下 词法解析&amp;预处理 词法解析拆分SQL，语法分析检查SQL的正确性生成一颗解析树，预处理检查表名，列名，生成一颗解析树。 优化器优化，优化计划，查询计划 执行引擎生成执行计划 存储引擎查询SQL，加入缓存，返回结果。 1.获取连接MySQL支持多种通信协议，可以使用同步/异步的方式，支持长连接，短连接。​ 1.1 通信类型​ 一般来说，连接数据库都是同步连接。​ 同步连接：依赖于被调用方，受限制于被调用方的性能；一般只能一对一。 异步连接：避免阻塞，但不能节省SQL的执行时间，并发情况下，每个SQL的执行都要单独建立连接，占用大量CPU资源；异步连接必须使用连接池减少线程创建销毁的开销。 1.2 连接方式MySQL长短连接都支持，一般我们会在连接池中使用长连接。保持长连接会消耗内存，长时间不活动的连接，MySQL服务器会断开。​ 12show global variables like &#x27;wait_timeout&#x27;; -- 非交互式超时时间，如 JDBC 程序show global variables like &#x27;interactive_timeout&#x27;; -- 交互式超时时间，如数据库工具 默认长连接断开时间是8小时。 可以使用 show status;查看当前MySQL有多少个连接。 1show global status like &#x27;Thread%&#x27;; Threads_cached 缓存中的线程连接数 Threads_connected 当前打开的连接数 Threads_created 为处理连接创建的线程数 Threads_running 非睡眠状态的连接数，通常指并发连接数 每产生一个连接或者会话，服务端就会创建一个线程来处理。杀死会话本质就是kill 线程。​ 可以使用SHOW PROCESSLIST; （root 用户）查看 SQL 的执行状态。​ +—-+——+———–+——+———+——+———-+| Id | User | Host | db | Command | Time | State | Info |+—-+——+———–+——+———+——+———-+| 11 | root | localhost | NULL | Query | 0 | starting | show processlist |+—-+——+———–+——+———+——+———-+ 状态 含义 Sleep 线程正在等待客户端，以向它发送一个新语句 Query 线程正在执行查询或往客户端发送数据 Locked 该查询被其它查询锁定 Copying to tmp table on disk 临时结果集合大于 tmp_table_size。线程把临时表从存储器内部格式改变为磁盘模式，以节约存储器 Sending data 线程正在为 SELECT 语句处理行，同时正在向客户端发送数据 Sorting for group 线程正在进行分类，以满足 GROUP BY 要求 Sorting for order 线程正在进行分类，以满足 ORDER BY 要求 在5.7版本，MySQL的默认连接数是151个，我们最大可以修改为16384个 （214）。​ 123show variables like &#x27;max_connections&#x27;;set [global | session] max_connections =10000; 1.3 通信协议​ 编程语言的连接模块都是用 TCP 协议连接到 MySQL 服务器的，比如mysql-connector-java-x.x.xx.jar。 类unix系统上，支持 Socket套接字文件进行进程间通信。/tmp/mysql.sock windows系统上还支持命名管道和共享内存。 ​ 1.4 通信方式MySQL使用了半双工通信，所以客户端发送SQL语句给服务端的时候，不管SQL有多大，都是一次发过去的。​ 比如我们用MyBatis动态SQL生成了一个批量插入的语句，插入10万条数据，values后面跟了一长串的内容，或者 where 条件 in 里面的值太多，会出现问题。这个时候我们必须要调整 MySQL 服务器配置 max_allowed_packet 参数的值（默认是 4M），把它调大，否则就会报错。 对于服务端来说，也是一次性发送所有的数据，不能因为你已经取到了想要的数据就中断操作，这个时候会对网络和内存产生大量消耗。在程序里面避免不带 limit 的这种操作，比如一次把所有满足条件的数据全部查出来，一定要先 count 一下。如果数据量的话，可以分批查询。 2.查询缓存MySQL 的缓存默认是关闭的。​ 1show variables like &#x27;query_cache%&#x27;; MySQL不推荐使用自带的缓存，命中条件过于苛刻。且表里数据发生变化，整张表的缓存全部失效，MySQL8移除掉了缓存。 3.语法解析&amp;预处理3.1 词法解析词法分析就是把一个完整的 SQL 语句打碎成一个个的单词。​ 1select name from user where id =1; 它会打碎成 8 个符号，每个符号是什么类型，从哪里开始到哪里结束。​ 3.2 语法解析语法分析会对 SQL 做一些语法检查，比如单引号有没有闭合，然后根据 MySQL 定义的语法规则，根据 SQL 语句生成一个数据结构。这个数据结构我们把它叫做解析树（select_lex）。​ 任何数据库的中间件，比如 Mycat，Sharding-JDBC（用到了 Druid Parser），都必须要有词法和语法分析功能。 3.3 预处理​ 如果写了一个词法和语法都正确的 SQL，但是表名或者字段不存在，会在哪里报错？是在数据库的执行层还是解析器？​ 实际上还是在解析的时候报错，解析 SQL 的环节里面有个预处理器。它会检查生成的解析树，解决解析器无法解析的语义。比如，它会检查表和列名是否存在，检查名字和别名，保证没有歧义。预处理之后得到一个新的解析树。​ 4.查询优化&amp;查询执行计划一条SQL语句的执行方式有很多种，但是最终返回的结果都是相同的。查询优化器的目的就是根据解析树生成不同的执行计划（Execution Plan），然后选择一种最优的执行计划，MySQL 里面使用的是基于开销（cost）的优化器，那种执行计划开销最小，就用哪种。 12# 查看查询的开销show status like &#x27;Last_query_cost&#x27;; 4.1 优化器的作用 多表联查，以哪张表为基准表 用不用索引，用哪个索引 。。。。 ​ 4.2 优化器是怎么得到执行计划的 首先我们要启用优化器的追踪（默认是关闭的）。 开启这开关是会消耗性能的，因为它要把优化分析的结果写到表里面，所以不要轻易开启，或者查看完之后关闭它（改成 off）。 接着执行一个 SQL 语句，优化器会生成执行计划： 这个时候优化器分析的过程已经记录到系统表里面了，我们可以查询： 它是一个 JSON 类型的数据，主要分成三部分，准备阶段、优化阶段和执行阶段。 expanded_query 是优化后的 SQL 语句。 considered_execution_plans 里面列出了所有的执行计划。 分析完记得关掉它 通过追踪优化器，可以看到优化器对sql的初始优化，表的读取顺序，为什么采用了这种读取顺序。为什么采用了某个索引或者采用了全表查询。 4.3 优化器得到的结果​ 优化器最终会把解析树变成一个查询执行计划，查询执行计划是一个数据结构。 当然，这个执行计划是不是一定是最优的执行计划呢？不一定，因为 MySQL 也有可能覆盖不到所有的执行计划。​ MySQL 提供了一个执行计划的工具。我们在 SQL 语句前面加上 EXPLAIN，就可以看到执行计划的信息。​ Explain 的结果也不一定最终执行的方式。​ 4.4 选错索引这里错误决定分两类，第一，彻底错误。第二，基于成本最低，但执行速度不是最快。 由于InnoDB的 MVCC 功能和随机采样方式，默认随机采取几个数据页，当做总体数据。以部分代表整体，本来就有错误的风险。加上数据不断地添加过程中，索引树可能会分裂，结果更加不准确。 执行 ANALYZE TABLE ,可以重新构建索引，使索引树不过于分裂。 调整参数，加大InnoDB采样的页数，页数越大越精确，但性能消耗更高。一般不建议这么干。 在优化阶段，会对表中所有索引进行对比，优化器基于成本的原因，选择成本最低的索引，所以会错过最佳索引。带来的问题便是，执行速度很慢。 通过explain查看执行计划，结合sql条件查看可以利用哪些索引。 使用 force index(indexName)强制走指定索引。弊端就是后期若索引名发生改变，或索引被删除，该sql语句需要调整。 5. 存储引擎得到执行计划以后，SQL 语句是不是终于可以执行了？ 从逻辑的角度来说，我们的数据是放在哪里的，或者说放在一个什么结构里面？ 执行计划在哪里执行？是谁去执行？ 表在存储数据的同时，还要组织数据的存储结构，这个存储结构就是由存储引擎决定的，所以也可以把存储引擎叫做表类型。 在 MySQL 里面，支持多种存储引擎，他们是可以替换的，所以叫做插件式的存储引擎。​ 5.1 查看存储引擎我们数据库里面已经存在的表，我们怎么查看它们的存储引擎呢？​ 1show table status from `数据库名`; 或者通过 DDL 建表语句来查看。​ 在 MySQL 里面，我们创建的每一张表都可以指定它的存储引擎，而不是一个数据库只能使用一个存储引擎。存储引擎的使用是以表为单位的。而且，创建表之后还可以修改存储引擎。​ 一张表使用的存储引擎决定存储数据的结构，那在服务器上它们是怎么存储的呢？先要找到数据库存放数据的路径：默认情况下，每个数据库有一个自己文件夹，以 yhd数据库为例。任何一个存储引擎都有一个 frm 文件，这个是表结构定义文件。不同的存储引擎存放数据的方式不一样，产生的文件也不一样，innodb 是 1 个，memory 没有，myisam 是两个。 5.2 存储引擎比较①常见存储引擎MyISAM 和 InnoDB 是我们用得最多的两个存储引擎，在 MySQL 5.5 版本之前，默认的存储引擎是 MyISAM，它是 MySQL 自带的。 5.5 版本之后默认的存储引擎改成了 InnoDB，最主要的原因还是 InnoDB 支持事务，支持行级别的锁，对于业务一致性要求高的场景来说更适合。 ②数据库支持的存储引擎可以用这个命令查看数据库对存储引擎的支持情况： 1show engines ; 其中有存储引擎的描述和对事务、XA 协议和 Savepoints 的支持。 XA 协议用来实现分布式事务（分为本地资源管理器，事务管理器）。 Savepoints 用来实现子事务（嵌套事务）。创建了一个 Savepoints 之后，事务就可以回滚到这个点，不会影响到创建 Savepoints 之前的操作。 ③MyISAM（3 个文件）应用范围比较小。表级锁定限制了读/写的性能，因此在 Web 和数据仓库配置中，它通常用于只读或以读为主的工作。 特点 支持表级别的锁（插入和更新会锁表）。不支持事务。 拥有较高的插入（insert）和查询（select）速度。 存储了表的行数（count 速度更快）。 适合：只读之类的数据分析的项目。 ④InnoDB（2个文件）mysql 5.7 中的默认存储引擎。InnoDB 是一个事务安全（与 ACID 兼容）的 MySQL存储引擎，它具有提交、回滚和崩溃恢复功能来保护用户数据。InnoDB 行级锁（不升级为更粗粒度的锁）和 Oracle 风格的一致非锁读提高了多用户并发性和性能。InnoDB 将用户数据存储在聚集索引中，以减少基于主键的常见查询的 I/O。为了保持数据完整性，InnoDB 还支持外键引用完整性约束。 特点 支持事务，支持外键，因此数据的完整性、一致性更高。 支持行级别的锁和表级别的锁。 支持读写并发，写不阻塞读（MVCC）。 特殊的索引存放方式，可以减少 IO，提升查询效率。 适合：经常更新的表，存在并发读写或者有事务处理的业务系统。 ⑤Memory(1个文件)基于内存的存储引擎。 特征： 基于内存的表，服务器重启后，表结构会被保留，但表中的数据会被清空。 不需要进行磁盘IO，比 MYISAM 快了一个数量级。 表级锁，故并发插入性能较低。 每一行是固定的，VARCHAR 列在 memory 存储引擎中会变成 CHAR，可能导致内存浪费。 不支持 BLOB 或 TEXT 列，如果sql返回的结果列中包含 BLOB 或 TEXT，就直接采用 MYISAM 存储引擎，在磁盘上建临时表 支持哈希索引，B+树索引 MEMORY 存储引擎在很多地方可以发挥很好的作用： 用于查找或映射表，例如邮编和州名的映射表 用于缓存周期性聚合数据的结果 用于保存数据分析中产生的中间结果。即SQL执行过程中用到的临时表 监控MySQL内存中的执行情况，例如：information_schema 库下的表基本都是 memory 存储引擎，监控InnoDB缓冲池中page(INNODB_BUFFER_PAGE表)，InnoDB缓冲池状态(INNODB_BUFFER_POOL_STATS表)、InnoDB缓存页淘汰记录(INNODB_BUFFER_PAGE_LRU表)、InnoDB锁等待(INNODB_LOCK_WAITS表)、InnoDB锁信息(INNODB_LOCKS表)、InnoDB中正在执行的事务(INNODB_TRX表)等。 MEMORY 存储引擎默认 hash 索引，故等值查询特别快。同时也支持B+树索引。虽然查询速度特别快，但依旧无法取代传统的磁盘建表。 ⑥CSV(3个文件)它的表实际上是带有逗号分隔值的文本文件。csv表允许以csv格式导入或转储数据，以便与读写相同格式的脚本和应用程序交换数据。因为 csv 表没有索引，所以通常在正常操作期间将数据保存在 innodb 表中，并且只在导入或导出阶段使用 csv 表。 特点 不允许空行，不支持索引。格式通用，可以直接编辑，适合在不同数据库之间导入导出。​ 5.3 如何选择存储引擎 如果对数据一致性要求比较高，需要事务支持，可以选择 InnoDB。 如果数据查询多更新少，对查询性能要求比较高，可以选择 MyISAM。 如果需要一个用于查询的临时表，可以选择 Memory。 ​ 6.执行引擎执行引擎，它利用存储引擎提供的相应的 API 来完成操作。 为什么我们修改了表的存储引擎，操作方式不需要做任何改变？因为不同功能的存储引擎实现的 API 是相同的。 最后把数据返回给客户端，即使没有结果也要返回。​ 二，一条SQL的更新流程更新和查询很多地方并没有区别，仅仅在于拿到数据之后的操作。 1.内存结构InnnoDB 的数据都是放在磁盘上的，InnoDB 操作数据有一个最小的逻辑单位，叫做页（索引页和数据页）。我们对于数据的操作，不是每次都直接操作磁盘，因为磁盘的速度太慢了。InnoDB 使用了一种缓冲池的技术，也就是把磁盘读到的页放到一块内存区域里面。这个内存区域就叫 Buffer Pool。​ 下一次读取相同的页，先判断是不是在缓冲池里面，如果是，就直接读取，不用再次访问磁盘。 修改数据的时候，先修改缓冲池里面的页。内存的数据页和磁盘数据不一致的时候，我们把它叫做脏页。InnoDB 里面有专门的后台线程把 Buffer Pool 的数据写入到磁盘，每隔一段时间就一次性地把多个修改写入磁盘，这个动作就叫做刷脏。 Buffer Pool 是 InnoDB 里面非常重要的一个结构，主要分为 3 个部分： Buffer Pool、Change Buffer、Adaptive HashIndex，另外还有一个（redo）log buffer。​ 1.1 buffer poolBuffer Pool 缓存的是页信息，包括数据页、索引页，默认大小是 128M（134217728 字节），可以调整。 查看服务器状态，里面有很多跟 Buffer Pool 相关的信息： 1SHOW STATUS LIKE &#x27;%innodb_buffer_pool%&#x27;; 查看参数（系统变量）： 1SHOW VARIABLES like &#x27;%innodb_buffer_pool%&#x27;; 内存的缓冲池写满了怎么办？（Redis 设置的内存满了怎么办？）InnoDB 用 LRU算法来管理缓冲池（链表实现，不是传统的 LRU，分成了 young 和 old），经过淘汰的数据就是热点数据。 内存缓冲区对于提升读写性能有很大的作用。当需要更新一个数据页时，如果数据页在 Buffer Pool 中存在，那么就直接更新好了。否则的话就需要从磁盘加载到内存，再对内存的数据页进行操作。也就是说，如果没有命中缓冲池，至少要产生一次磁盘 IO。​ 1.2 ChangeBuffer写缓冲如果这个数据页不是唯一索引，不存在数据重复的情况，也就不需要从磁盘加载索引页判断数据是不是重复（唯一性检查）。这种情况下可以先把修改记录在内存的缓冲池中，从而提升更新语句（Insert、Delete、Update）的执行速度。 这一块区域就是 Change Buffer。5.5 之前叫 Insert Buffer 插入缓冲，现在也能支持 delete 和 update。 最后把 Change Buffer 记录到数据页的操作叫做 merge。什么时候发生 merge？有几种情况：在访问这个数据页的时候，或者通过后台线程、或者数据库 shut down、redo log 写满时触发。 如果数据库大部分索引都是非唯一索引，并且业务是写多读少，不会在写数据后立刻读取，就可以使用 Change Buffer（写缓冲）。写多读少的业务，调大这个值： 1SHOW VARIABLES LIKE &#x27;innodb_change_buffer_max_size&#x27;; 代表 Change Buffer 占 Buffer Pool 的比例，默认 25%。​ 1.3 Adaptive Hash Index当我们需要访问某个页中的数据时，就会把该页从磁盘加载到Buffer Pool中，如果该页已经在Buffer Pool中的话直接使用就可以了。那么问题也就来了，我们怎么知道该页在不在Buffer Pool中呢？ 我们其实是根据表空间号 + 页号来定位一个页的，也就相当于表空间号 + 页号是一个key，缓存页就是对应的value，怎么通过一个key来快速找着一个value呢？那肯定是哈希表。 所以我们可以用表空间号 + 页号作为key，缓存页作为value创建一个哈希表，在需要访问某个页的数据时，先从哈希表中根据表空间号 + 页号看看有没有对应的缓存页，如果有，直接使用该缓存页就好，如果没有，那就从free链表中选一个空闲的缓存页，然后把磁盘中对应的页加载到该缓存页的位置。​ 1.4 （redo）Log Buffer​ 如果 Buffer Pool 里面的脏页还没有刷入磁盘时，数据库宕机或者重启，这些数据丢失。如果写操作写到一半，甚至可能会破坏数据文件导致数据库不可用。为了避免这个问题，InnoDB 把所有对页面的修改操作专门写入一个日志文件，并且在数据库启动时从这个文件进行恢复操作（实现 crash-safe）——用它来实现事务的持久性。​ 这个文件就是磁盘的 redo log（叫做重做日志），对应于/var/lib/mysql/目录下的ib_logfile0 和 ib_logfile1，每个 48M。这 种 日 志 和 磁 盘 配 合 的 整 个 过 程 ， 其 实 就 是 MySQL 里 的 WAL 技 术（Write-Ahead Logging），它的关键点就是先写日志，再写磁盘。 1show variables like &#x27;innodb_log%&#x27;; 值 含义 innodb_log_file_size 指定每个文件的大小，默认 48M innodb_log_files_in_group 指定文件的数量，默认为 2 innodb_log_group_home_dir 指定文件所在路径，相对或绝对。如果不指定，则为datadir 路径。 同样是写磁盘，为什么不直接写到 db file 里面去？为什么先写日志再写磁盘？ 磁盘的最小组成单元是扇区，通常是 512 个字节。操作系统和内存打交道，最小的单位是页 Page。操作系统和磁盘打交道，读写磁盘，最小的单位是块 Block。​ 如果我们所需要的数据是随机分散在不同页的不同扇区中，那么找到相应的数据需要等到磁臂旋转到指定的页，然后盘片寻找到对应的扇区，才能找到我们所需要的一块数据，依次进行此过程直到找完所有数据，这个就是随机 IO，读取数据速度较慢。 假设我们已经找到了第一块数据，并且其他所需的数据就在这一块数据后边，那么就不需要重新寻址，可以依次拿到我们所需的数据，这个就叫顺序 IO。 刷盘是随机 I/O，而记录日志是顺序 I/O，顺序 I/O 效率更高。因此先把修改写入日志，可以延迟刷盘时机，进而提升系统吞吐。 当然 redo log 也不是每一次都直接写入磁盘，在 Buffer Pool 里面有一块内存区域（Log Buffer）专门用来保存即将要写入日志文件的数据，默认 16M，它一样可以节省磁盘 IO。​ 1SHOW VARIABLES LIKE &#x27;innodb_log_buffer_size&#x27;; redo log 的内容主要是用于崩溃恢复。磁盘的数据文件，数据来自 buffer pool。redo log 写入磁盘，不是写入数据文件。 那么，Log Buffer 什么时候写入 log file？ 在我们写入数据到磁盘的时候，操作系统本身是有缓存的。flush 就是把操作系统缓冲区写入到磁盘。 log buffer 写入磁盘的时机，由一个参数控制，默认是 1。 1SHOW VARIABLES LIKE &#x27;innodb_flush_log_at_trx_commit&#x27;; 值 含义 0（延迟写） log buffer 将每秒一次地写入 log file 中，并且 log file 的 flush 操作同时进行。该模式下，在事务提交的时候，不会主动触发写入磁盘的操作。 1（默认，实时写，实时刷） 每次事务提交时 MySQL 都会把 log buffer 的数据写入 log file，并且刷到磁盘中去。 2（实时写，延迟刷） 每次事务提交时 MySQL 都会把 log buffer 的数据写入 log file。但是 flush 操作并不会同时进行。该模式下，MySQL 会每秒执行一次 flush 操作。 redo log，它又分成内存和磁盘两部分。redo log 有什么特点？ redo log 是 InnoDB 存储引擎实现的，并不是所有存储引擎都有。 不是记录数据页更新之后的状态，而是记录这个页做了什么改动，属于物理日志。（redo log 记录的是执行的结果） redo log 的大小是固定的，前面的内容会被覆盖。 check point 是当前要覆盖的位置。如果 write pos 跟 check point 重叠，说明 redolog 已经写满，这时候需要同步 redo log 到磁盘中。 这是 MySQL 的内存结构，总结一下，分为：Buffer pool、change buffer、Adaptive Hash Index、 log buffer。 磁盘结构里面主要是各种各样的表空间，叫做 Table space。 1.5 缓存的疑问缓存（cache）是在读取硬盘中的数据时，把最常用的数据保存在内存的缓存区中，再次读取该数据时，就不去硬盘中读取了，而在缓存中读取。缓冲（buffer）是在向硬盘写入数据时，先把数据放入缓冲区,然后再一起向硬盘写入，把分散的写操作集中进行，减少磁盘碎片和硬盘的反复寻道，从而提高系统性能。 然后，InnoDB架构中，有非常重要的一个部分——缓冲池。该缓冲池需要占用服务器内存，且专用于MySQL的服务器，建议把80%的内存交给MySQL。 缓冲池有一个缓存的功能。这个缓存，是InnoDB自带的，而且经常会用到。该缓存功能并不是MySQL架构中的缓存组件。这是两者最大的区别。 MySQL组件中的缓存 所处位置：MySQL架构中的缓存组件 缓存内容：缓存的是SQL 和 该SQL的查询结果。如果SQL的大小写，格式，注释不一致，则被认为是不同的SQL，重新查询数据库，并缓存一份数据。 可否关闭：是可以手动关闭，并卸载该组件的。 InnoDB中的缓存 所处位置：InnoDB架构中的缓冲池 缓存内容：缓存的是所有需要查找的数据，所在的数据页。 可否关闭：是InnoDB缓冲池自带的功能，无法关闭，无法卸载。如果InnoDB的缓冲池被关闭或卸载，则InnoDB直接瘫痪。所以说缓冲池是InnoDB的最重要的一部分。 不建议使用MySQL的缓存是指，不建议使用MySQL架构中的缓存组件，并不是同时否定了InnoDB中的缓存功能。​ 2.磁盘结构表空间可以看做是 InnoDB 存储引擎逻辑结构的最高层，所有的数据都存放在表空间中。InnoDB 的表空间分为 5 大类。​ 2.1 系统表空间在默认情况下 InnoDB 存储引擎有一个共享表空间（对应文件/var/lib/mysql/ibdata1），也叫系统表空间。 InnoDB 系统表空间包含 InnoDB 数据字典和双写缓冲区，（Change Buffer 和 UndoLogs），如果没有指定 file-per-table，也包含用户创建的表和索引数据。 undo 在后面介绍，因为有独立的表空间。 数据字典：由内部系统表组成，存储表和索引的元数据（定义信息）。 双写缓冲（InnoDB 的一大特性） InnoDB 的页和操作系统的页大小不一致，InnoDB 页大小一般为 16K，操作系统页大小为 4K，InnoDB 的页写入到磁盘时，一个页需要分 4 次写。 如果存储引擎正在写入页的数据到磁盘时发生了宕机，可能出现页只写了一部分的情况，比如只写了 4K，就宕机了，这种情况叫做部分写失效（partial page write），可能会导致数据丢失。 1show variables like &#x27;innodb_doublewrite&#x27;; 如果这个页本身已经损坏了，用它来做崩溃恢复是没有意义的。所以在对于应用 redo log 之前，需要一个页的副本。如果出现了写入失效，就用页的副本来还原这个页，然后再应用 redo log。这个页的副本就是 double write，InnoDB 的双写技术。通过它实现了数据页的可靠性。 跟 redo log 一样，double write 由两部分组成，一部分是内存的 double write，一个部分是磁盘上的 double write。因为 double write 是顺序写入的，不会带来很大的开销。 在MySQL5.7之前，所有的表共享一个系统表空间，这个文件会越来越大，而且它的空间不会收缩。​ 2.2 独占表空间我们可以让每张表独占一个表空间。这个开关通过 innodb_file_per_table 设置，默认开启。 1SHOW VARIABLES LIKE &#x27;innodb_file_per_table&#x27;; 开启后，则每张表会开辟一个表空间，这个文件就是数据目录下的 ibd 文件，存放表的索引和数据。但是其他类的数据，如回滚（undo）信息，插入缓冲索引页、系统事务信息，二次写缓冲（Double write buffer）等还是存放在原来的共享表空间内。​ 2.3 通用表空间通用表空间也是一种共享的表空间，跟 ibdata1 类似。 可以创建一个通用的表空间，用来存储不同数据库的表，数据路径和文件可以自定义。语法： 1create tablespace ts2673 add datafile &#x27;/var/lib/mysql/ts2673.ibd&#x27; file_block_size=16K engine=innodb; 在创建表的时候可以指定表空间，用 ALTER 修改表空间可以转移表空间。 1create table t2673(id integer) tablespace ts2673; 不同表空间的数据是可以移动的。删除表空间需要先删除里面的所有表： 12drop table t2673;drop tablespace ts2673; 2.4 临时表空间存储临时表的数据，包括用户创建的临时表，和磁盘的内部临时表。对应数据目录下的 ibtmp1 文件。当数据服务器正常关闭时，该表空间被删除，下次重新产生。 memory向template的过渡，还有磁盘上简历临时表用的什么存储引擎？ 8.0之前，内存临时表用Memory引擎创建，但假如字段中有BLOB或TEXT,或结果太大，就会转用MYISM在磁盘上建表，8.0之后内存临时表由MEMORY引擎更改为TempTable引擎，相比于前者，后者支持以变长方式存储VARCHAR，VARBINARY等变长字段。从MySQL 8.0.13开始，TempTable引擎支持BLOB字段。如果超过内存表大小，则用InnoDB建表。 2.5 redo log2.6 undo log 表空间undo log（撤销日志或回滚日志）记录了事务发生之前的数据状态（不包括 select）。 如果修改数据时出现异常，可以用 undo log 来实现回滚操作（保持原子性）。 在执行 undo 的时候，仅仅是将数据从逻辑上恢复至事务之前的状态，而不是从物理页面上操作实现的，属于逻辑格式的日志(记录操作)。 redo Log 和 undo Log 与事务密切相关，统称为事务日志。 undo Log 的数据默认在系统表空间 ibdata1 文件中，因为共享表空间不会自动收缩，也可以单独创建一个 undo 表空间。 1show global variables like &#x27;%undo%&#x27;; 2.7 一条SQL的更新流程12# id =1 的记录原 name = &#x27;yhd&#x27;update user set name = &#x27;二十&#x27; where id=1; 事务开始，从内存或者磁盘取到这条数据，返回给server的执行器 执行器修改这一行数据的值为二十 记录name =yhd 到undo log 记录name = 二十 到redo log 调用存储引擎接口，在buffer pool 中修改 name =二十 事务提交 ​ 内存和磁盘之间，工作着很多后台线程。 3.后台线程后台线程的主要作用是负责刷新内存池中的数据和把修改的数据页刷新到磁盘。后台线程分为：master线程，IO 线程，purge 线程，page cleaner 线程。​ 3.1 Master 线程Master Thread是InnoDB存储引擎非常核心的一个后台线程，主要负责将缓冲池中的数据异步刷新到磁盘，保证数据的一致性，包括脏页的刷新、合并插入缓冲、UNDO页的回收等。​ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546void master_thread()&#123; loop: for(int i = 0; i &lt; 10; ++i)&#123; thread_sleep(1); // sleep 1秒 do log buffer flush to disk; if(last_one_second_ios &lt; 5) do merge at most 5 insert buffer; if(buf_get_modified_ratio_pct &gt; innodb_max_dirty_pages_pct) // 如果缓冲池中的脏页比例大于innodb_max_dirty_pages_pct(默认是75时) do buffer pool flush 100 dirty page; // 刷新100脏页到磁盘 if(no user activity) goto backgroud loop; &#125; if(last_ten_second_ios &lt; 200) // 如果过去10内磁盘IO次数小于设置的innodb_io_capacity的值（默认是200） do buffer pool flush 100 dirty page; do merge at most 5 insert buffer; // 合并插入缓冲是innodb_io_capacity的5%（10）（总是） do log buffer flush to disk; do full purge; if(buf_get_modified_ratio_pct &gt; 70%) do buffer pool flush 100 dirty page; else buffer pool flush 10 dirty page; backgroud loop： // 后台循环 do full purge // 删除无用的undo页 （总是） do merge 20 insert buffer; // 合并插入缓冲是innodb_io_capacity的5%（10）（总是） if not idle // 如果不空闲，就跳回主循环，如果空闲就跳入flush loop goto loop: // 跳到主循环 else goto flush loop flush loop: // 刷新循环 do buffer pool flush 100 dirty page; if(buf_get_modified_ratio_pct &gt; innodb_max_dirty_pages_pct) // 如果缓冲池中的脏页比例大于innodb_max_dirty_pages_pct的值（默认75%） goto flush loop; // 跳到刷新循环，不断刷新脏页，直到符合条件 goto suspend loop; // 完成刷新脏页的任务后，跳入suspend loop suspend loop: suspend_thread(); //master线程挂起，等待事件发生 waiting event; goto loop;&#125; Master Thread具有最高的线程优先级别。内部由多个循环组成：主循环（loop）、后台循环（backgroup loop）、刷新循环（flush loop）、暂停循环（suspend loop）。Master Thread会根据数据库运行的状态在loop、backgroup loop、flush loop和suspend loop中进行切换。loop是主循环，大多数的操作都在这个循环中，主要有两大部分的操作——每秒钟的操作和每10秒钟的操作。​ ①每秒钟的操作​ ​日志缓冲刷新到磁盘，即使这个事务还没有提交（总是）；即使某个事务还没有提交，InnoDB存储引擎仍然每秒会将重做日志缓冲中的内容刷新到重做日志文件。这也解释了为什么再大的事务提交的时间也是很短的。 合并插入缓冲（可能）；合并插入缓冲并不是每秒都会发生的。InnoDB存储引擎会判断当前一秒内发生的IO次数是否小于5次，如果小于5次，InnoDB存储引擎认为当前的IO压力很小，可以执行合并插入缓冲的操作； 至多刷新100个InnoDB的缓冲池中的脏页到磁盘（可能）； 刷新100个脏页也不是每秒都会发生的，InnoDB存储引擎通过判断当前缓冲池中脏页的比例(buf_get_modified_ratio_pct)是否超过了配置文件中 innodb_max_dirty_pages_pct这个参数（默认是75，代表75%），如果超过了这个值，InnoDB存储引擎则认为需要做磁盘同步的操作，将100个脏页写入磁盘中。 如果当前没有用户活动，则切换到background loop(可能)。 ​ ②每十秒的操作 刷新100个脏页到磁盘（可能） InnoDB存储引擎会先判断过去10秒之内磁盘的IO操作是否小于200次，如果是，InnoDB存储引擎认为当前有足够的磁盘IO能力，因此将100个脏页刷新到磁盘。 合并至多5个插入缓冲（总是） 将日志缓冲刷新到磁盘（总是） 删除无用的Undo页（总是） 刷新100个或者10个脏页到磁盘（总是） InnoDB存储引擎会执行full purge操作，即删除无用的Undo页。对表进行update，delete这类的操作时，原先的行被标记为删除，但是因为一致性读的关系，需要保留这些行版本的信息。但是在full purge过程中，InnoDB存储引擎会判断当前事务系统中已被删除的行是否可以删除，比如有时候可能还有查询操作需要读取之前版本的undo信息，如果可以删除，InnoDB存储引擎会立即将其删除。从源代码中可以看出，InnoDB存储引擎在执行full purge 操作时，每次最多尝试回收20个undo页。然后，InnoDB存储引擎会判断缓冲池中脏页的比例（buf_get_modified_ratio_pct）,如果有超过70%的脏页，则刷新100个脏页到磁盘，如果脏页的比例小于70%,则只需刷新10%的脏页到磁盘。 ​ 如果当前没有用户活动（数据库空闲）或者数据库关系，就会切换到backgroud loop这个循环。 backgroud loop会执行以下操作： 删除无用的Undo页（总是） 合并20个插入缓冲（总是） 跳回到主循环（总是） 不断刷新100个页直到符合条件（可能，需要跳转到flush loop中完成） 如果flush loop中也没有什么事情可以做了，InnoDB存储引擎会切换到suspend_loop，将Master Thread挂起，等待事件的发生。若用户启用了InnoDB存储引擎，却没有使用任何InnoDB存储引擎的表，那么Master Thread总是处于挂起的状态。​ 1.0.x版本中，InnoDB存储引擎最多只会刷新100个脏页到磁盘，合并20个插入缓冲。如果是在写入密集的应用程序中，每秒可能会产生大于100个的脏页，如果是产生大于20个插入缓冲的情况，那么可能会来不及刷新所有的脏页以及合并插入缓冲。后来，InnoDB存储引擎提供了参数innodb_io_capacity，用来表示磁盘IO的吞吐量，默认值为200。​ 对于刷新到磁盘的页的数量，会按照innodb_io_capacity的百分比来进行控制。规则如下： 在合并插入缓冲时，合并插入缓冲的数量为innodb_io_capacity值的5%; 在从缓冲区刷新脏页时，刷新脏页的数量为innodb_io_capacity; 如果用户使用的是SSD类的磁盘，可以将innodb_io_capacity的值调高，直到符合磁盘IO的吞吐量为止； 另一个问题是参数innodb_max_dirty_pages_pct的默认值，在1.0.x版本之前，该值的默认值是90，意味着脏页占缓冲池的90%。InnoDB存储引擎在每秒刷新缓冲池和flush loop时会判断这个值，如果该值大于innodb_max_dirty_pages_pct,才会刷新100个脏页，如果有很大的内存，或者数据库服务器的压力很大，这时刷新脏页的速度反而会降低。 后来将innodb_max_dirty_pages_pct的默认值改为了75。这样既可以加快刷新脏页的频率，又能够保证磁盘IO的负载。​ 还有一个新的参数是innodb_adaptive_flushing(自适应地刷新)，该值影响每秒刷新脏页的数量。原来的刷新规则是：脏页在缓冲池所占的比例小于innodb_max_dirty_pages_pct时，不刷新脏页；大于innodb_max_dirty_pages_pct时，刷新100个脏页。随着innodb_adaptive_flushing参数的引入，InnoDB通过一个名为buf_flush_get_desired_flush_rate的函数来判断需要刷新脏页最合适的数量。buf_flush_get_desired_flush_rate函数通过判断产生重做日志的速率来决定最合适的刷新脏页数量。 之前每次进行full purge 操作时，最多回收20个Undo页，从InnoDB 1.0.x版本开始引入了参数innodb_purge_batch_size,该参数可以控制每次full purge回收的Undo页的数量。该参数的默认值为20，并可以动态地对其进行修改。​ 1.2.x版本中再次对Master Thread进行了优化，对于刷新脏页的操作，从Master Thread线程分离到一个单独的Page Cleaner Thread，从而减轻了Master Thread的工作，同时进一步提高了系统的并发性。​ 3.2 IO 线程InnoDB中大量使用AIO (Async IO) 来处理IO请求。IO Thread的作用，是负责这些 IO 请求的回调（call back）。​ 3.3 Purge 线程事务被提交后，其所使用的undo log可能不在需要。因此，需要purge thread来回收已经使用并分配的undo页。以前Master Thread来完成释放undo log，InnoDB1.1独立出来，分担主线程压力。​ 3.4 Page Cleaner 线程​ 负责将脏页刷新到磁盘。以前Master Thread来刷新脏页，InnoDB1.2独立出来，分担主线程压力。​ 除了 InnoDB 架构中的日志文件，MySQL 的 Server 层也有一个日志文件，叫做binlog，它可以被所有的存储引擎使用。 4.binlogbinlog 以事件的形式记录了所有的DDL 和DML 语句（因为它记录的是操作而不是数据值，属于逻辑日志），可以用来做主从复制和数据恢复。跟redo log不一样，它的文件内容是可以追加的，没有固定大小限制。在开启了 binlog 功能的情况下，我们可以把 binlog 导出成 SQL 语句，把所有的操作重放一遍，来实现数据的恢复。binlog 的另一个功能就是用来实现主从复制，它的原理就是从服务器读取主服务器的 binlog，然后执行一遍。​ 有了这两个日志之后，来看一下一条更新语句是怎么执行的：​ 12# id =1 的记录原 name = &#x27;yhd&#x27;update user set name = &#x27;二十&#x27; where id=1; ​ 事务开始，从内存或者磁盘取到这条数据所在的数据页，返回给server的执行器 执行器修改这一行数据的值为二十 记录name =yhd 到undo log 在buffer pool 中修改 name =二十，此时该页变成脏页 记录name = 二十 到redo log buffer，redo log buffer每秒刷盘。 redo log 进入prepare状态，然后告诉执行器，执行完成了，可以随时提交 写入binlog 事务提交，并回写最终状态到redo log里，代表该事务已经提交 ​ 事务开始之后就产生redo log，redo log的落盘并不是随着事务的提交才写入的，而是在事务的执行过程中，便不断写入redo log文件中。一般情况下，每次事务commit时，必须调用 fsync 操作，将redo日志缓冲同步写到磁盘。另外，每次事务提交时同步写到磁盘bin log中。 那么就有了一个谁先谁后的问题：redo log 先，bin log 后。 两阶段提交的内容：**事务提交时，redo log处于 pre状态 -&gt; 写入bin log -&gt; 事务真正提交。 ** 当发生崩溃恢复时，查看的是bin log是否完整，如果bin log完整，则代表事务已经提交。 如果在两阶段提交过程中，bin log写入失败，则事务无法终止提交，崩溃恢复时就不需要重做。如果bin log写完的一瞬间，服务器宕机了，事务都来不及提交，此时bin log并不是完整的，缺少了最终的commit标记。因此也是提交失败。 简单说，redo log和bin log都可以用于表示事务的提交状态，而两阶段提交就是让这两个状态保持逻辑上的一致。 三，MySQL中支持的字符集和排序规则1.MySQL中的utf8和utf8mb4utf8字符集表示一个字符需要使用1～4个字节，但是我们常用的一些字符使用1～3个字节就可以表示了。而在MySQL中字符集表示一个字符所用最大字节长度在某些方面会影响系统的存储和性能，所以设计MySQL的大叔偷偷的定义了两个概念： utf8mb3：阉割过的utf8字符集，只使用1～3个字节表示字符。 utf8mb4：正宗的utf8字符集，使用1～4个字节表示字符。 在MySQL中utf8是utf8mb3的别名，所以之后在MySQL中提到utf8就意味着使用1~3个字节来表示一个字符，如果有使用4字节编码一个字符的情况，比如存储一些emoji表情啥的，那请使用utf8mb4。 查看字符集：SHOW (CHARACTER SET|CHARSET)。 2.字符集&amp;比较规则的应用2.1 各级别的字符集和比较规则MySQL有4个级别的字符集和比较规则，分别是： 服务器级别 数据库级别 表级别 列级别 接下来仔细看一下怎么设置和查看这几个级别的字符集和比较规则。 服务器级别123456789101112131415mysql&gt; SHOW VARIABLES LIKE &#x27;character_set_server&#x27;;+----------------------+-------+| Variable_name | Value |+----------------------+-------+| character_set_server | utf8 |+----------------------+-------+1 row in set (0.00 sec)mysql&gt; SHOW VARIABLES LIKE &#x27;collation_server&#x27;;+------------------+-----------------+| Variable_name | Value |+------------------+-----------------+| collation_server | utf8_general_ci |+------------------+-----------------+1 row in set (0.00 sec) 可以在启动服务器程序时通过启动选项或者在服务器程序运行过程中使用SET语句修改这两个变量的值。比如我们可以在配置文件中这样写： 123[server]character_set_server=gbkcollation_server=gbk_chinese_ci 当服务器启动的时候读取这个配置文件后这两个系统变量的值便修改了。 数据库级别我们在创建和修改数据库的时候可以指定该数据库的字符集和比较规则，具体语法如下： 1234567CREATE DATABASE 数据库名 [[DEFAULT] CHARACTER SET 字符集名称] [[DEFAULT] COLLATE 比较规则名称];ALTER DATABASE 数据库名 [[DEFAULT] CHARACTER SET 字符集名称] [[DEFAULT] COLLATE 比较规则名称]; 其中的DEFAULT可以省略，并不影响语句的语义。比方说我们新创建一个名叫charset_demo_db的数据库，在创建的时候指定它使用的字符集为gb2312，比较规则为gb2312_chinese_ci： 1234mysql&gt; CREATE DATABASE charset_demo_db -&gt; CHARACTER SET gb2312 -&gt; COLLATE gb2312_chinese_ci;Query OK, 1 row affected (0.01 sec) 查看 1234567891011121314151617181920mysql&gt; USE charset_demo_db;Database changedmysql&gt; SHOW VARIABLES LIKE &#x27;character_set_database&#x27;;+------------------------+--------+| Variable_name | Value |+------------------------+--------+| character_set_database | gb2312 |+------------------------+--------+1 row in set (0.00 sec)mysql&gt; SHOW VARIABLES LIKE &#x27;collation_database&#x27;;+--------------------+-------------------+| Variable_name | Value |+--------------------+-------------------+| collation_database | gb2312_chinese_ci |+--------------------+-------------------+1 row in set (0.00 sec)mysql&gt; 可以看到这个charset_demo_db数据库的字符集和比较规则就是我们在创建语句中指定的。需要注意的一点是： character_set_database 和 _collation_database_ 这两个系统变量是只读的，我们不能通过修改这两个变量的值而改变当前数据库的字符集和比较规则。 表级别我们也可以在创建和修改表的时候指定表的字符集和比较规则，语法如下： 1234567CREATE TABLE 表名 (列的信息) [[DEFAULT] CHARACTER SET 字符集名称] [COLLATE 比较规则名称]]ALTER TABLE 表名 [[DEFAULT] CHARACTER SET 字符集名称] [COLLATE 比较规则名称] 比方说我们在刚刚创建的charset_demo_db数据库中创建一个名为t的表，并指定这个表的字符集和比较规则： 1234mysql&gt; CREATE TABLE t( -&gt; col VARCHAR(10) -&gt; ) CHARACTER SET utf8 COLLATE utf8_general_ci;Query OK, 0 rows affected (0.03 sec) 如果创建和修改表的语句中没有指明字符集和比较规则，将使用该表所在数据库的字符集和比较规则作为该表的字符集和比较规则。 列级别需要注意的是，对于存储字符串的列，同一个表中的不同的列也可以有不同的字符集和比较规则。我们在创建和修改列定义的时候可以指定该列的字符集和比较规则，语法如下： 123456CREATE TABLE 表名( 列名 字符串类型 [CHARACTER SET 字符集名称] [COLLATE 比较规则名称], 其他列...);ALTER TABLE 表名 MODIFY 列名 字符串类型 [CHARACTER SET 字符集名称] [COLLATE 比较规则名称]; 比如我们修改一下表t中列col的字符集和比较规则可以这么写： 12345mysql&gt; ALTER TABLE t MODIFY col VARCHAR(10) CHARACTER SET gbk COLLATE gbk_chinese_ci;Query OK, 0 rows affected (0.04 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; 对于某个列来说，如果在创建和修改的语句中没有指明字符集和比较规则，将使用该列所在表的字符集和比较规则作为该列的字符集和比较规则。 在转换列的字符集时需要注意，如果转换前列中存储的数据不能用转换后的字符集进行表示会发生错误。比方说原先列使用的字符集是utf8，列中存储了一些汉字，现在把列的字符集转换为ascii的话就会出错，因为ascii字符集并不能表示汉字字符。 2.2 客户端和服务器通信中的字符集编码和解码使用的字符集不一致的后果如果对于同一个字符串编码和解码使用的字符集不一样，会产生意想不到的结果，作为人类的我们看上去就像是产生了乱码一样。 从发送请求到接收结果过程中发生的字符集转换 客户端使用操作系统的字符集编码请求字符串，向服务器发送的是经过编码的一个字节串。 服务器将客户端发送来的字节串采用character_set_client代表的字符集进行解码，将解码后的字符串再按照character_set_connection代表的字符集进行编码。 如果character_set_connection代表的字符集和具体操作的列使用的字符集一致，则直接进行相应操作，否则的话需要将请求中的字符串从character_set_connection代表的字符集转换为具体操作的列使用的字符集之后再进行操作。 将从某个列获取到的字节串从该列使用的字符集转换为character_set_results代表的字符集后发送到客户端。 客户端使用操作系统的字符集解析收到的结果集字节串。 在这个过程中各个系统变量的含义如下： 系统变量 描述 character_set_client 服务器解码请求时使用的字符集 character_set_connection 服务器处理请求时会把请求字符串从character_set_client转为character_set_connection character_set_results 服务器向客户端返回数据时使用的字符集 一般情况下要使用保持这三个变量的值和客户端使用的字符集相同。 比较规则的作用通常体现比较字符串大小的表达式以及对某个字符串列进行排序中。​","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/tags/MySQL/"}]},{"title":"ConcurrentHashMap源码解读","slug":"1.基础知识/ConcurrentHashMap","date":"2022-01-05T12:23:06.000Z","updated":"2022-01-05T12:23:06.000Z","comments":true,"path":"2022/01/05/1.基础知识/ConcurrentHashMap/","link":"","permalink":"https://yinhuidong.github.io/2022/01/05/1.%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/ConcurrentHashMap/","excerpt":"","text":"#1.成员变量 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147//散列表数组的最大限制private static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;//散列表默认值private static final int DEFAULT_CAPACITY = 16;static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8;//并发级别：jdk7历史遗留问题，仅仅在初始化的时候使用到，并不是真正的代表并发级别private static final int DEFAULT_CONCURRENCY_LEVEL = 16;//负载因子，JDK1.8中 ConcurrentHashMap 是固定值private static final float LOAD_FACTOR = 0.75f;//树化阈值，指定桶位 链表长度达到8的话，有可能发生树化操作。static final int TREEIFY_THRESHOLD = 8;//红黑树转化为链表的阈值static final int UNTREEIFY_THRESHOLD = 6;//联合TREEIFY_THRESHOLD控制桶位是否树化，只有当table数组长度达到64且 某个桶位 中的链表长度达到8，才会真正树化static final int MIN_TREEIFY_CAPACITY = 64;//线程迁移数据最小步长，控制线程迁移任务最小区间一个值private static final int MIN_TRANSFER_STRIDE = 16;//计算扩容时候生成的一个 标识戳private static int RESIZE_STAMP_BITS = 16;//结果是65535 表示并发扩容最多线程数private static final int MAX_RESIZERS = (1 &lt;&lt; (32 - RESIZE_STAMP_BITS)) - 1;//扩容相关private static final int RESIZE_STAMP_SHIFT = 32 - RESIZE_STAMP_BITS;//当node节点hash=-1 表示当前节点已经被迁移了 ，fwd节点static final int MOVED = -1; // hash for forwarding nodes//node hash=-2 表示当前节点已经树化 且 当前节点为treebin对象 ，代理操作红黑树static final int TREEBIN = -2; // hash for roots of treesstatic final int RESERVED = -3; // hash for transient reservations//转化成二进制实际上是 31个 1 可以将一个负数通过位移运算得到一个正数static final int HASH_BITS = 0x7fffffff; // usable bits of normal node hash//当前系统的cpu数量static final int NCPU = Runtime.getRuntime().availableProcessors();//为了兼容7版本的chp保存的，核心代码并没有使用到private static final ObjectStreamField[] serialPersistentFields = &#123; new ObjectStreamField(&quot;segments&quot;, Segment[].class), new ObjectStreamField(&quot;segmentMask&quot;, Integer.TYPE), new ObjectStreamField(&quot;segmentShift&quot;, Integer.TYPE) &#125;;//散列表，长度一定是2次方数transient volatile Node&lt;K,V&gt;[] table;//扩容过程中，会将扩容中的新table 赋值给nextTable 保持引用，扩容结束之后，这里会被设置为Nullprivate transient volatile Node&lt;K,V&gt;[] nextTable;//LongAdder 中的 baseCount 未发生竞争时 或者 当前LongAdder处于加锁状态时，增量累到到baseCount中private transient volatile long baseCount;/** * sizeCtl &lt; 0 * 1. -1 表示当前table正在初始化（有线程在创建table数组），当前线程需要自旋等待.. * 2.表示当前table数组正在进行扩容 ,高16位表示：扩容的标识戳 低16位表示：（1 + nThread） 当前参与并发扩容的线程数量 * * sizeCtl = 0，表示创建table数组时 使用DEFAULT_CAPACITY为大小 * * sizeCtl &gt; 0 * * 1. 如果table未初始化，表示初始化大小 * 2. 如果table已经初始化，表示下次扩容时的 触发条件（阈值） */private transient volatile int sizeCtl;/** * * 扩容过程中，记录当前进度。所有线程都需要从transferIndex中分配区间任务，去执行自己的任务。 */private transient volatile int transferIndex;/** * LongAdder中的cellsBuzy 0表示当前LongAdder对象无锁状态，1表示当前LongAdder对象加锁状态 */private transient volatile int cellsBusy;/** * LongAdder中的cells数组，当baseCount发生竞争后，会创建cells数组， * 线程会通过计算hash值 取到 自己的cell ，将增量累加到指定cell中 * 总数 = sum(cells) + baseCount */private transient volatile CounterCell[] counterCells;// Unsafe mechanicsprivate static final sun.misc.Unsafe U;/**表示sizeCtl属性在ConcurrentHashMap中内存偏移地址*/private static final long SIZECTL;/**表示transferIndex属性在ConcurrentHashMap中内存偏移地址*/private static final long TRANSFERINDEX;/**表示baseCount属性在ConcurrentHashMap中内存偏移地址*/private static final long BASECOUNT;/**表示cellsBusy属性在ConcurrentHashMap中内存偏移地址*/private static final long CELLSBUSY;/**表示cellValue属性在CounterCell中内存偏移地址*/private static final long CELLVALUE;/**表示数组第一个元素的偏移地址*/private static final long ABASE;private static final int ASHIFT;static &#123; try &#123; U = sun.misc.Unsafe.getUnsafe(); Class&lt;?&gt; k = ConcurrentHashMap.class; SIZECTL = U.objectFieldOffset (k.getDeclaredField(&quot;sizeCtl&quot;)); TRANSFERINDEX = U.objectFieldOffset (k.getDeclaredField(&quot;transferIndex&quot;)); BASECOUNT = U.objectFieldOffset (k.getDeclaredField(&quot;baseCount&quot;)); CELLSBUSY = U.objectFieldOffset (k.getDeclaredField(&quot;cellsBusy&quot;)); Class&lt;?&gt; ck = CounterCell.class; CELLVALUE = U.objectFieldOffset (ck.getDeclaredField(&quot;value&quot;)); Class&lt;?&gt; ak = Node[].class; ABASE = U.arrayBaseOffset(ak); //表示数组单元所占用空间大小,scale 表示Node[]数组中每一个单元所占用空间大小 int scale = U.arrayIndexScale(ak); //1 0000 &amp; 0 1111 = 0 if ((scale &amp; (scale - 1)) != 0) throw new Error(&quot;data type scale not a power of two&quot;); //numberOfLeadingZeros() 这个方法是返回当前数值转换为二进制后，从高位到低位开始统计，看有多少个0连续在一块。 //8 =&gt; 1000 numberOfLeadingZeros(8) = 28 //4 =&gt; 100 numberOfLeadingZeros(4) = 29 //ASHIFT = 31 - 29 = 2 ？？ //ABASE + （5 &lt;&lt; ASHIFT） ASHIFT = 31 - Integer.numberOfLeadingZeros(scale); &#125; catch (Exception e) &#123; throw new Error(e); &#125; &#125; #2.基础方法##2.1 spread高位运算 123static final int spread(int h) &#123; return (h ^ (h &gt;&gt;&gt; 16)) &amp; HASH_BITS;&#125; ##2.2 tabAt该方法获取对象中offset偏移地址对应的对象field的值。实际上这段代码的含义等价于tab[i],但是为什么不直接使用 tab[i]来计算呢？ getObjectVolatile，一旦看到 volatile 关键字，就表示可见性。因为对 volatile 写操作 happen-before 于 volatile 读操作，因此其他线程对 table 的修改均对 get 读取可见； 虽然 table 数组本身是增加了 volatile 属性，但是“volatile 的数组只针对数组的引用具有volatile 的语义，而不是它的元素”。 所以如果有其他线程对这个数组的元素进行写操作，那么当前线程来读的时候不一定能读到最新的值。出于性能考虑，Doug Lea 直接通过 Unsafe 类来对 table 进行操作。 123static final &lt;K,V&gt; Node&lt;K,V&gt; tabAt(Node&lt;K,V&gt;[] tab, int i) &#123; return (Node&lt;K,V&gt;)U.getObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE);&#125; ##2.3 casTabAtcas设置当前节点为桶位的头节点 1234static final &lt;K,V&gt; boolean casTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; c, Node&lt;K,V&gt; v) &#123; return U.compareAndSwapObject(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, c, v);&#125; ##2.4 setTabAt 123static final &lt;K,V&gt; void setTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; v) &#123; U.putObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, v);&#125; ##2.5 resizeStampresizeStamp 用来生成一个和扩容有关的扩容戳，具体有什么作用呢？ 123static final int resizeStamp(int n) &#123; return Integer.numberOfLeadingZeros(n) | (1 &lt;&lt; (RESIZE_STAMP_BITS - 1));&#125; Integer.numberOfLeadingZeros 这个方法是返回无符号整数 n 最高位非 0 位前面的 0 的个数。 比如 10 的二进制是 0000 0000 0000 0000 0000 0000 0000 1010，那么这个方法返回的值就是 28。 根据 resizeStamp 的运算逻辑，我们来推演一下，假如 n=16，那么 resizeStamp(16)=32796转化为二进制是[0000 0000 0000 0000 1000 0000 0001 1100] 接着再来看,当第一个线程尝试进行扩容的时候，会执行下面这段代码： U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)rs 左移 16 位，相当于原本的二进制低位变成了高位 1000 0000 0001 1100 0000 0000 00000000 然后再+2 =1000 0000 0001 1100 0000 0000 0000 0000+10=1000 0000 0001 1100 0000 00000000 0010 高 16 位代表扩容的标记、低 16 位代表并行扩容的线程数 这样来存储有什么好处呢？ 1，首先在 CHM 中是支持并发扩容的，也就是说如果当前的数组需要进行扩容操作，可以由多个线程来共同负责 2，可以保证每次扩容都生成唯一的生成戳，每次新的扩容，都有一个不同的 n，这个生成戳就是根据 n 来计算出来的一个数字，n 不同，这个数字也不同 第一个线程尝试扩容的时候，为什么是+2 因为 1 表示初始化，2 表示一个线程在执行扩容，而且对 sizeCtl 的操作都是基于位运算的，所以不会关心它本身的数值是多少，只关心它在二进制上的数值，而 sc + 1 会在低 16 位上加 1。##2.6 tableSizeFor经过多次位移返回大于等于c的最小的二次方数 1234567891011121314151617181920 /** * Returns a power of two table size for the given desired capacity. * See Hackers Delight, sec 3.2 * 返回&gt;=c的最小的2的次方数 * c=28 * n=27 =&gt; 0b 11011 * 11011 | 01101 =&gt; 11111 * 11111 | 00111 =&gt; 11111 * .... * =&gt; 11111 + 1 =100000 = 32 */private static final int tableSizeFor(int c) &#123; int n = c - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125; #3. 构造方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public ConcurrentHashMap() &#123;&#125;public ConcurrentHashMap(int initialCapacity) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException(); //如果指定的容量超过允许的最大值，设置为最大值 int cap = ((initialCapacity &gt;= (MAXIMUM_CAPACITY &gt;&gt;&gt; 1)) ? MAXIMUM_CAPACITY : tableSizeFor(initialCapacity + (initialCapacity &gt;&gt;&gt; 1) + 1)); /** * sizeCtl &gt; 0 * 当目前table未初始化时，sizeCtl表示初始化容量 */ this.sizeCtl = cap;&#125;public ConcurrentHashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; this.sizeCtl = DEFAULT_CAPACITY; putAll(m);&#125;public ConcurrentHashMap(int initialCapacity, float loadFactor) &#123; this(initialCapacity, loadFactor, 1);&#125;public ConcurrentHashMap(int initialCapacity, float loadFactor, int concurrencyLevel) &#123; //参数校验 if (!(loadFactor &gt; 0.0f) || initialCapacity &lt; 0 || concurrencyLevel &lt;= 0) throw new IllegalArgumentException(); //如果初始容量小于并发级别，那就设置初始容量为并发级别 if (initialCapacity &lt; concurrencyLevel) initialCapacity = concurrencyLevel; //16/0.75 +1 = 22 long size = (long)(1.0 + (long)initialCapacity / loadFactor); // 22 - &gt; 32 int cap = (size &gt;= (long)MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : tableSizeFor((int)size); /** * sizeCtl &gt; 0 * 当目前table未初始化时，sizeCtl表示初始化容量 */ this.sizeCtl = cap;&#125; #4.put 1234public V put(K key, V value) &#123; //如果key已经存在，是否覆盖，默认是false return putVal(key, value, false);&#125; #5 putVal 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129final V putVal(K key, V value, boolean onlyIfAbsent) &#123; //控制k 和 v 不能为null if (key == null || value == null) throw new NullPointerException(); //通过spread方法，可以让高位也能参与进寻址运算。 int hash = spread(key.hashCode()); //binCount表示当前k-v 封装成node后插入到指定桶位后，在桶位中的所属链表的下标位置 //0 表示当前桶位为null，node可以直接放着 //2 表示当前桶位已经可能是红黑树 int binCount = 0; //tab 引用map对象的table //自旋 for (Node&lt;K,V&gt;[] tab = table;;) &#123; //f 表示桶位的头结点 //n 表示散列表数组的长度 //i 表示key通过寻址计算后，得到的桶位下标 //fh 表示桶位头结点的hash值 Node&lt;K,V&gt; f; int n, i, fh; //CASE1：成立，表示当前map中的table尚未初始化.. if (tab == null || (n = tab.length) == 0) //最终当前线程都会获取到最新的map.table引用。 tab = initTable(); //CASE2：i 表示key使用路由寻址算法得到 key对应 table数组的下标位置，tabAt 获取指定桶位的头结点 f else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123; //进入到CASE2代码块 前置条件 当前table数组i桶位是Null时。 //使用CAS方式 设置 指定数组i桶位 为 new Node&lt;K,V&gt;(hash, key, value, null),并且期望值是null //cas操作成功 表示ok，直接break for循环即可 //cas操作失败，表示在当前线程之前，有其它线程先你一步向指定i桶位设置值了。 //当前线程只能再次自旋，去走其它逻辑。 if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin &#125; //CASE3：前置条件，桶位的头结点一定不是null。 //条件成立表示当前桶位的头结点 为 FWD结点，表示目前map正处于扩容过程中.. else if ((fh = f.hash) == MOVED) //看到fwd节点后，当前节点有义务帮助当前map对象完成迁移数据的工作 //帮助扩容 tab = helpTransfer(tab, f); //CASE4：当前桶位 可能是 链表 也可能是 红黑树代理结点TreeBin else &#123; //当插入key存在时，会将旧值赋值给oldVal，返回给put方法调用处.. V oldVal = null; //使用sync 加锁“头节点”，理论上是“头结点” synchronized (f) &#123; //为什么又要对比一下，看看当前桶位的头节点 是否为 之前获取的头结点？ //为了避免其它线程将该桶位的头结点修改掉，导致当前线程从sync 加锁 就有问题了。之后所有操作都不用在做了。 if (tabAt(tab, i) == f) &#123;//条件成立，说明咱们 加锁 的对象没有问题，可以进来造了！ //条件成立，说明当前桶位就是普通链表桶位。 if (fh &gt;= 0) &#123; //1.当前插入key与链表当中所有元素的key都不一致时，当前的插入操作是追加到链表的末尾，binCount表示链表长度 //2.当前插入key与链表当中的某个元素的key一致时，当前插入操作可能就是替换了。binCount表示冲突位置（binCount - 1） binCount = 1; //迭代循环当前桶位的链表，e是每次循环处理节点。 for (Node&lt;K,V&gt; e = f;; ++binCount) &#123; //当前循环节点 key K ek; //条件一：e.hash == hash 成立 表示循环的当前元素的hash值与插入节点的hash值一致，需要进一步判断 //条件二：((ek = e.key) == key ||(ek != null &amp;&amp; key.equals(ek))) // 成立：说明循环的当前节点与插入节点的key一致，发生冲突了 if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; //将当前循环的元素的 值 赋值给oldVal oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; &#125; //当前元素 与 插入元素的key不一致 时，会走下面程序。 //1.更新循环处理节点为 当前节点的下一个节点 //2.判断下一个节点是否为null，如果是null，说明当前节点已经是队尾了，插入数据需要追加到队尾节点的后面。 Node&lt;K,V&gt; pred = e; if ((e = e.next) == null) &#123; pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; &#125; &#125; &#125; //前置条件，该桶位一定不是链表 //条件成立，表示当前桶位是 红黑树代理结点TreeBin else if (f instanceof TreeBin) &#123; //p 表示红黑树中如果与你插入节点的key 有冲突节点的话 ，则putTreeVal 方法 会返回冲突节点的引用。 Node&lt;K,V&gt; p; //强制设置binCount为2，因为binCount &lt;= 1 时有其它含义，所以这里设置为了2 binCount = 2; //条件一：成立，说明当前插入节点的key与红黑树中的某个节点的key一致，冲突了 if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) &#123; //将冲突节点的值 赋值给 oldVal oldVal = p.val; if (!onlyIfAbsent) p.val = value; &#125; &#125; &#125; &#125; //说明当前桶位不为null，可能是红黑树 也可能是链表 if (binCount != 0) &#123; //如果binCount&gt;=8 表示处理的桶位一定是链表 if (binCount &gt;= TREEIFY_THRESHOLD) //调用转化链表为红黑树的方法 treeifyBin(tab, i); //说明当前线程插入的数据key，与原有k-v发生冲突，需要将原数据v返回给调用者。 if (oldVal != null) return oldVal; break; &#125; &#125; &#125; //1.统计当前table一共有多少数据 //2.判断是否达到扩容阈值标准，触发扩容。 addCount(1L, binCount); return null;&#125; #6 initTable数组初始化方法，这个方法比较简单，就是初始化一个合适大小的数组。 sizeCtl ：这个标志是在 Node 数组初始化或者扩容的时候的一个控制位标识，负数代表正在进行初始化或者扩容操作。 -1 代表正在初始化 -N 代表有 N-1 个线程正在进行扩容操作，这里不是简单的理解成 n 个线程，sizeCtl 就是-N 0 标识 Node 数组还没有被初始化，正数代表初始化或者下一次扩容的大小 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/** * Initializes table, using the size recorded in sizeCtl. * * sizeCtl &lt; 0 * * 1. -1 表示当前table正在初始化（有线程在创建table数组），当前线程需要自旋等待.. * * 2.表示当前table数组正在进行扩容 ,高16位表示：扩容的标识戳 低16位表示：（1 + nThread） 当前参与并发扩容的线程数量 * * * * sizeCtl = 0，表示创建table数组时 使用DEFAULT_CAPACITY为大小 * * * * sizeCtl &gt; 0 * * * * 1. 如果table未初始化，表示初始化大小 * * 2. 如果table已经初始化，表示下次扩容时的 触发条件（阈值） */private final Node&lt;K,V&gt;[] initTable() &#123; //tab 引用map.table //sc sizeCtl的临时值 Node&lt;K,V&gt;[] tab; int sc; //自旋 条件：map.table 尚未初始化 while ((tab = table) == null || tab.length == 0) &#123; if ((sc = sizeCtl) &lt; 0) //大概率就是-1，表示其它线程正在进行创建table的过程，当前线程没有竞争到初始化table的锁。 Thread.yield(); // lost initialization race; just spin //1.sizeCtl = 0，表示创建table数组时 使用DEFAULT_CAPACITY为大小 //2.如果table未初始化，表示初始化大小 //3.如果table已经初始化，表示下次扩容时的 触发条件（阈值） else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) &#123; try &#123; //这里为什么又要判断呢？ 防止其它线程已经初始化完毕了，然后当前线程再次初始化..导致丢失数据。 //条件成立，说明其它线程都没有进入过这个if块，当前线程就是具备初始化table权利了。 if ((tab = table) == null || tab.length == 0) &#123; //sc大于0 创建table时 使用 sc为指定大小，否则使用 16 默认值. int n = (sc &gt; 0) ? sc : DEFAULT_CAPACITY; @SuppressWarnings(&quot;unchecked&quot;) Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; //最终赋值给 map.table table = tab = nt; //n &gt;&gt;&gt; 2 =&gt; 等于 1/4 n n - (1/4)n = 3/4 n =&gt; 0.75 * n //sc 0.75 n 表示下一次扩容时的触发条件。 sc = n - (n &gt;&gt;&gt; 2); &#125; &#125; finally &#123; //1.如果当前线程是第一次创建map.table的线程话，sc表示的是 下一次扩容的阈值 //2.表示当前线程 并不是第一次创建map.table的线程，当前线程进入到else if 块 时，将 //sizeCtl 设置为了-1 ，那么这时需要将其修改为 进入时的值。 sizeCtl = sc; &#125; break; &#125; &#125; return tab;&#125; #7 addCount 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124private final void addCount(long x, int check) &#123; //as 表示 LongAdder.cells //b 表示LongAdder.base //s 表示当前map.table中元素的数量 CounterCell[] as; long b, s; //条件一：true-&gt;表示cells已经初始化了，当前线程应该去使用hash寻址找到合适的cell 去累加数据 // false-&gt;表示当前线程应该将数据累加到 base //条件二：false-&gt;表示写base成功，数据累加到base中了，当前竞争不激烈，不需要创建cells // true-&gt;表示写base失败，与其他线程在base上发生了竞争，当前线程应该去尝试创建cells。 if ((as = counterCells) != null || !U.compareAndSwapLong(this, BASECOUNT, b = baseCount, s = b + x)) &#123; //有几种情况进入到if块中？ //1.true-&gt;表示cells已经初始化了，当前线程应该去使用hash寻址找到合适的cell 去累加数据 //2.true-&gt;表示写base失败，与其他线程在base上发生了竞争，当前线程应该去尝试创建cells。 //a 表示当前线程hash寻址命中的cell CounterCell a; //v 表示当前线程写cell时的期望值 long v; //m 表示当前cells数组的长度 int m; //true -&gt; 未竞争 false-&gt;发生竞争 boolean uncontended = true; //条件一：as == null || (m = as.length - 1) &lt; 0 //true-&gt; 表示当前线程是通过 写base竞争失败 然后进入的if块，就需要调用fullAddCount方法去扩容 或者 重试.. LongAdder.longAccumulate //条件二：a = as[ThreadLocalRandom.getProbe() &amp; m]) == null 前置条件：cells已经初始化了 //true-&gt;表示当前线程命中的cell表格是个空，需要当前线程进入fullAddCount方法去初始化 cell，放入当前位置. //条件三：!(uncontended = U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x) // false-&gt;取反得到false，表示当前线程使用cas方式更新当前命中的cell成功 // true-&gt;取反得到true,表示当前线程使用cas方式更新当前命中的cell失败，需要进入fullAddCount进行重试 或者 扩容 cells。 if (as == null || (m = as.length - 1) &lt; 0 || (a = as[ThreadLocalRandom.getProbe() &amp; m]) == null || !(uncontended = U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x)) ) &#123; fullAddCount(x, uncontended); //考虑到fullAddCount里面的事情比较累，就让当前线程 不参与到 扩容相关的逻辑了，直接返回到调用点。 return; &#125; if (check &lt;= 1) return; //获取当前散列表元素个数，这是一个期望值 s = sumCount(); &#125; //表示一定是一个put操作调用的addCount if (check &gt;= 0) &#123; //tab 表示map.table //nt 表示map.nextTable //n 表示map.table数组的长度 //sc 表示sizeCtl的临时值 Node&lt;K,V&gt;[] tab, nt; int n, sc; /** * sizeCtl &lt; 0 * 1. -1 表示当前table正在初始化（有线程在创建table数组），当前线程需要自旋等待.. * 2.表示当前table数组正在进行扩容 ,高16位表示：扩容的标识戳 低16位表示：（1 + nThread） 当前参与并发扩容的线程数量 * * sizeCtl = 0，表示创建table数组时 使用DEFAULT_CAPACITY为大小 * * sizeCtl &gt; 0 * * 1. 如果table未初始化，表示初始化大小 * 2. 如果table已经初始化，表示下次扩容时的 触发条件（阈值） */ //自旋 //条件一：s &gt;= (long)(sc = sizeCtl) // true-&gt; 1.当前sizeCtl为一个负数 表示正在扩容中.. // 2.当前sizeCtl是一个正数，表示扩容阈值 // false-&gt; 表示当前table尚未达到扩容条件 //条件二：(tab = table) != null // 恒成立 true //条件三：(n = tab.length) &lt; MAXIMUM_CAPACITY // true-&gt;当前table长度小于最大值限制，则可以进行扩容。 while (s &gt;= (long)(sc = sizeCtl) &amp;&amp; (tab = table) != null &amp;&amp; (n = tab.length) &lt; MAXIMUM_CAPACITY) &#123; //扩容批次唯一标识戳 //16 -&gt; 32 扩容 标识为：1000 0000 0001 1011 int rs = resizeStamp(n); //条件成立：表示当前table正在扩容 // 当前线程理论上应该协助table完成扩容 if (sc &lt; 0) &#123; //条件一：(sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs // true-&gt;说明当前线程获取到的扩容唯一标识戳 非 本批次扩容 // false-&gt;说明当前线程获取到的扩容唯一标识戳 是 本批次扩容 //条件二： JDK1.8 中有bug jira已经提出来了 其实想表达的是 = sc == (rs &lt;&lt; 16 ) + 1 // true-&gt; 表示扩容完毕，当前线程不需要再参与进来了 // false-&gt;扩容还在进行中，当前线程可以参与 //条件三：JDK1.8 中有bug jira已经提出来了 其实想表达的是 = sc == (rs&lt;&lt;16) + MAX_RESIZERS // true-&gt; 表示当前参与并发扩容的线程达到了最大值 65535 - 1 // false-&gt;表示当前线程可以参与进来 //条件四：(nt = nextTable) == null // true-&gt;表示本次扩容结束 // false-&gt;扩容正在进行中 if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex &lt;= 0) break; //前置条件：当前table正在执行扩容中.. 当前线程有机会参与进扩容。 //条件成立：说明当前线程成功参与到扩容任务中，并且将sc低16位值加1，表示多了一个线程参与工作 //条件失败：1.当前有很多线程都在此处尝试修改sizeCtl，有其它一个线程修改成功了，导致你的sc期望值与内存中的值不一致 修改失败 // 2.transfer 任务内部的线程也修改了sizeCtl。 if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) //协助扩容线程，持有nextTable参数 transfer(tab, nt); &#125; //1000 0000 0001 1011 0000 0000 0000 0000 +2 =&gt; 1000 0000 0001 1011 0000 0000 0000 0010 //条件成立，说明当前线程是触发扩容的第一个线程，在transfer方法需要做一些扩容准备工作 else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) //触发扩容条件的线程 不持有nextTable transfer(tab, null); s = sumCount(); &#125; &#125;&#125; #8. transferConcurrentHashMap 支持并发扩容，实现方式是，把 Node 数组进行拆分，让每个线程处理自己的区域，假设 table 数组总长度是 64，默认情况下，那么每个线程可以分到 16 个 bucket。然后每个线程处理的范围，按照倒序来做迁移。 通过 for 自循环处理每个槽位中的链表元素，默认 advace 为真，通过 CAS 设置 transferIndex属性值，并初始化 i 和 bound 值，i 指当前处理的槽位序号，bound 指需要处理的槽位边界，先处理槽位 31 的节点； （bound,i） =(16,31) 从 31 的位置往前推动。 每存在一个线程执行完扩容操作，就通过 cas 执行 sc-1。 接着判断(sc-2) !=resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT ; 如果相等，表示当前为整个扩容操作的 最后一个线程，那么意味着整个扩容操作就结束了；如果不相等，说明还得继续。 这么做的目的，一方面是防止不同扩容之间出现相同的 sizeCtl，另外一方面，还可以避免sizeCtl 的 ABA 问题导致的扩容重叠的情况。 扩容图解判断是否需要扩容，也就是当更新后的键值对总数 baseCount &gt;= 阈值 sizeCtl 时，进行rehash，这里面会有两个逻辑。 如果当前正在处于扩容阶段，则当前线程会加入并且协助扩容。 如果当前没有在扩容，则直接触发扩容操作。 扩容操作的核心在于数据的转移，在单线程环境下数据的转移很简单，无非就是把旧数组中的数据迁移到新的数组。但是这在多线程环境下，在扩容的时候其他线程也可能正在添加元素，这时又触发了扩容怎么办？可能大家想到的第一个解决方案是加互斥锁，把转移过程锁住，虽然是可行的解决方案，但是会带来较大的性能开销。因为互斥锁会导致所有访问临界区的线程陷入到阻塞状态，持有锁的线程耗时越长，其他竞争线程就会一直被阻塞，导致吞吐量较低。而且还可能导致死锁。 而 ConcurrentHashMap 并没有直接加锁，而是采用 CAS 实现无锁的并发同步策略，最精华的部分是它可以利用多线程来进行协同扩容。 它把 Node 数组当作多个线程之间共享的任务队列，然后通过维护一个指针来划分每个线程锁负责的区间，每个线程通过区间逆向遍历来实现扩容，一个已经迁移完的bucket会被替换为一个ForwardingNode节点，标记当前bucket已经被其他线程迁移完了。接下来分析一下它的源码实现。 fwd:这个类是个标识类，用于指向新表用的，其他线程遇到这个类会主动跳过这个类，因为这个类要么就是扩容迁移正在进行，要么就是已经完成扩容迁移，也就是这个类要保证线程安全，再进行操作。 advance:这个变量是用于提示代码是否进行推进处理，也就是当前桶处理完，处理下一个桶的标识。 finishing:这个变量用于提示扩容是否结束用的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237private final void transfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab) &#123; //n 表示扩容之前table数组的长度 //stride 表示分配给线程任务的步长 int n = tab.length, stride; // stride 固定为 16 if ((stride = (NCPU &gt; 1) ? (n &gt;&gt;&gt; 3) / NCPU : n) &lt; MIN_TRANSFER_STRIDE) stride = MIN_TRANSFER_STRIDE; // subdivide range //条件成立：表示当前线程为触发本次扩容的线程，需要做一些扩容准备工作 //条件不成立：表示当前线程是协助扩容的线程.. if (nextTab == null) &#123; // initiating try &#123; //创建了一个比扩容之前大一倍的table @SuppressWarnings(&quot;unchecked&quot;) Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n &lt;&lt; 1]; nextTab = nt; &#125; catch (Throwable ex) &#123; // try to cope with OOME sizeCtl = Integer.MAX_VALUE; return; &#125; //赋值给对象属性 nextTable ，方便协助扩容线程 拿到新表 nextTable = nextTab; //记录迁移数据整体位置的一个标记。index计数是从1开始计算的。 transferIndex = n; &#125; //表示新数组的长度 int nextn = nextTab.length; //fwd 节点，当某个桶位数据处理完毕后，将此桶位设置为fwd节点，其它写线程 或读线程看到后，会有不同逻辑。 ForwardingNode&lt;K,V&gt; fwd = new ForwardingNode&lt;K,V&gt;(nextTab); //推进标记 boolean advance = true; //完成标记 boolean finishing = false; // to ensure sweep before committing nextTab //i 表示分配给当前线程任务，执行到的桶位 //bound 表示分配给当前线程任务的下界限制 int i = 0, bound = 0; //自旋 for (;;) &#123; //f 桶位的头结点 //fh 头结点的hash Node&lt;K,V&gt; f; int fh; /** * 1.给当前线程分配任务区间 * 2.维护当前线程任务进度（i 表示当前处理的桶位） * 3.维护map对象全局范围内的进度 */ while (advance) &#123; //分配任务的开始下标 //分配任务的结束下标 int nextIndex, nextBound; //CASE1: //条件一：--i &gt;= bound //成立：表示当前线程的任务尚未完成，还有相应的区间的桶位要处理，--i 就让当前线程处理下一个 桶位. //不成立：表示当前线程任务已完成 或 者未分配 if (--i &gt;= bound || finishing) advance = false; //CASE2: //前置条件：当前线程任务已完成 或 者未分配 //条件成立：表示对象全局范围内的桶位都分配完毕了，没有区间可分配了，设置当前线程的i变量为-1 跳出循环后，执行退出迁移任务相关的程序 //条件不成立：表示对象全局范围内的桶位尚未分配完毕，还有区间可分配 else if ((nextIndex = transferIndex) &lt;= 0) &#123; i = -1; advance = false; &#125; //CASE3: //前置条件：1、当前线程需要分配任务区间 2.全局范围内还有桶位尚未迁移 //条件成立：说明给当前线程分配任务成功 //条件失败：说明分配给当前线程失败，应该是和其它线程发生了竞争吧 else if (U.compareAndSwapInt (this, TRANSFERINDEX, nextIndex, nextBound = (nextIndex &gt; stride ? nextIndex - stride : 0))) &#123; bound = nextBound; i = nextIndex - 1; advance = false; &#125; &#125; //CASE1： //条件一：i &lt; 0 //成立：表示当前线程未分配到任务 if (i &lt; 0 || i &gt;= n || i + n &gt;= nextn) &#123; //保存sizeCtl 的变量 int sc; if (finishing) &#123; nextTable = null; table = nextTab; sizeCtl = (n &lt;&lt; 1) - (n &gt;&gt;&gt; 1); return; &#125; //条件成立：说明设置sizeCtl 低16位 -1 成功，当前线程可以正常退出 if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) &#123; //1000 0000 0001 1011 0000 0000 0000 0000 //条件成立：说明当前线程不是最后一个退出transfer任务的线程 if ((sc - 2) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) //正常退出 return; finishing = advance = true; i = n; // recheck before commit &#125; &#125; //前置条件：【CASE2~CASE4】 当前线程任务尚未处理完，正在进行中 //CASE2: //条件成立：说明当前桶位未存放数据，只需要将此处设置为fwd节点即可。 else if ((f = tabAt(tab, i)) == null) advance = casTabAt(tab, i, null, fwd); //CASE3: //条件成立：说明当前桶位已经迁移过了，当前线程不用再处理了，直接再次更新当前线程任务索引，再次处理下一个桶位 或者 其它操作 else if ((fh = f.hash) == MOVED) advance = true; // already processed //CASE4: //前置条件：当前桶位有数据，而且node节点 不是 fwd节点，说明这些数据需要迁移。 else &#123; //sync 加锁当前桶位的头结点 synchronized (f) &#123; //防止在你加锁头对象之前，当前桶位的头对象被其它写线程修改过，导致你目前加锁对象错误... if (tabAt(tab, i) == f) &#123; //ln 表示低位链表引用 //hn 表示高位链表引用 Node&lt;K,V&gt; ln, hn; //条件成立：表示当前桶位是链表桶位 if (fh &gt;= 0) &#123; //lastRun //可以获取出 当前链表 末尾连续高位不变的 node int runBit = fh &amp; n; Node&lt;K,V&gt; lastRun = f; for (Node&lt;K,V&gt; p = f.next; p != null; p = p.next) &#123; int b = p.hash &amp; n; if (b != runBit) &#123; runBit = b; lastRun = p; &#125; &#125; //条件成立：说明lastRun引用的链表为 低位链表，那么就让 ln 指向 低位链表 if (runBit == 0) &#123; ln = lastRun; hn = null; &#125; //否则，说明lastRun引用的链表为 高位链表，就让 hn 指向 高位链表 else &#123; hn = lastRun; ln = null; &#125; for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) &#123; int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); else hn = new Node&lt;K,V&gt;(ph, pk, pv, hn); &#125; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; &#125; //条件成立：表示当前桶位是 红黑树 代理结点TreeBin else if (f instanceof TreeBin) &#123; //转换头结点为 treeBin引用 t TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; //低位双向链表 lo 指向低位链表的头 loTail 指向低位链表的尾巴 TreeNode&lt;K,V&gt; lo = null, loTail = null; //高位双向链表 lo 指向高位链表的头 loTail 指向高位链表的尾巴 TreeNode&lt;K,V&gt; hi = null, hiTail = null; //lc 表示低位链表元素数量 //hc 表示高位链表元素数量 int lc = 0, hc = 0; //迭代TreeBin中的双向链表，从头结点 至 尾节点 for (Node&lt;K,V&gt; e = t.first; e != null; e = e.next) &#123; // h 表示循环处理当前元素的 hash int h = e.hash; //使用当前节点 构建出来的 新的 TreeNode TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt; (h, e.key, e.val, null, null); //条件成立：表示当前循环节点 属于低位链 节点 if ((h &amp; n) == 0) &#123; //条件成立：说明当前低位链表 还没有数据 if ((p.prev = loTail) == null) lo = p; //说明 低位链表已经有数据了，此时当前元素 追加到 低位链表的末尾就行了 else loTail.next = p; //将低位链表尾指针指向 p 节点 loTail = p; ++lc; &#125; //当前节点 属于 高位链 节点 else &#123; if ((p.prev = hiTail) == null) hi = p; else hiTail.next = p; hiTail = p; ++hc; &#125; &#125; ln = (lc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(lo) : (hc != 0) ? new TreeBin&lt;K,V&gt;(lo) : t; hn = (hc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(hi) : (lc != 0) ? new TreeBin&lt;K,V&gt;(hi) : t; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; &#125; &#125; &#125; &#125; &#125;&#125; 链表迁移原理 1）高低位原理分析 ConcurrentHashMap 在做链表迁移时，会用高低位来实现，这里有两个问题要分析一下 1，如何实现高低位链表的区分 假如有这样一个队列第 14 个槽位插入新节点之后，链表元素个数已经达到了 8，且数组长度为 16，优先通过扩容来缓解链表过长的问题 假如当前线程正在处理槽位为 14 的节点，它是一个链表结构，在代码中，首先定义两个变量节点 ln 和 hn，实际就是 lowNode 和 HighNode，分别保存 hash 值的第 x 位为 0 和不等于0 的节点 通过 fn&amp;n 可以把这个链表中的元素分为两类，A 类是 hash 值的第 X 位为 0，B 类是 hash 值的第 x 位为不等于 0（至于为什么要这么区分，稍后分析），并且通过 lastRun 记录最后要处理的节点。最终要达到的目的是，A 类的链表保持位置不动，B 类的链表为 14+16(扩容增加的长度)=30 把 14 槽位的链表单独伶出来，用蓝色表示 fn&amp;n=0 的节点，假如链表的分类是这样 1234567for (Node&lt;K,V&gt; p = f.next; p != null; p = p.next) &#123; int b = p.hash &amp; n; if (b != runBit) &#123; runBit = b; lastRun = p; &#125;&#125; 通过上面这段代码遍历，会记录 runBit 以及 lastRun，按照上面这个结构，那么 runBit 应该是蓝色节点，lastRun 应该是第 6 个节点接着，再通过这段代码进行遍历，生成 ln 链以及 hn 链 1234567for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) &#123; int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); else hn = new Node&lt;K,V&gt;(ph, pk, pv, hn);&#125; 接着，通过 CAS 操作，把 hn 链放在 i+n 也就是 14+16 的位置，ln 链保持原来的位置不动。并且设置当前节点为 fwd，表示已经被当前线程迁移完了。 123setTabAt(nextTab, i, ln);setTabAt(nextTab, i + n, hn);setTabAt(tab, i, fwd); 迁移完成以后的数据分布如下 2）为什么要做高低位的划分 要想了解这么设计的目的，我们需要从 ConcurrentHashMap 的根据下标获取对象的算法来看，在 putVal 方法中 1018 行： (f = tabAt(tab, i = (n - 1) &amp; hash)) == null 通过(n-1) &amp; hash 来获得在 table 中的数组下标来获取节点数据，【&amp;运算是二进制运算符，1&amp; 1=1，其他都为 0】。 #9.helpTransfer如果对应的节点存在，判断这个节点的 hash 是不是等于 MOVED(-1)，说明当前节点是ForwardingNode 节点，意味着有其他线程正在进行扩容，那么当前现在直接帮助它进行扩容，因此调用 helpTransfer方法。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455final Node&lt;K,V&gt;[] helpTransfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt; f) &#123; //nextTab 引用的是 fwd.nextTable == map.nextTable 理论上是这样。 //sc 保存map.sizeCtl Node&lt;K,V&gt;[] nextTab; int sc; //条件一：tab != null 恒成立 true //条件二：(f instanceof ForwardingNode) 恒成立 true //条件三：((ForwardingNode&lt;K,V&gt;)f).nextTable) != null 恒成立 true if (tab != null &amp;&amp; (f instanceof ForwardingNode) &amp;&amp; (nextTab = ((ForwardingNode&lt;K,V&gt;)f).nextTable) != null) &#123; //拿当前标的长度 获取 扩容标识戳 假设 16 -&gt; 32 扩容：1000 0000 0001 1011 int rs = resizeStamp(tab.length); //条件一：nextTab == nextTable //成立：表示当前扩容正在进行中 //不成立：1.nextTable被设置为Null 了，扩容完毕后，会被设为Null // 2.再次出发扩容了...咱们拿到的nextTab 也已经过期了... //条件二：table == tab //成立：说明 扩容正在进行中，还未完成 //不成立：说明扩容已经结束了，扩容结束之后，最后退出的线程 会设置 nextTable 为 table //条件三：(sc = sizeCtl) &lt; 0 //成立：说明扩容正在进行中 //不成立：说明sizeCtl当前是一个大于0的数，此时代表下次扩容的阈值，当前扩容已经结束。 while (nextTab == nextTable &amp;&amp; table == tab &amp;&amp; (sc = sizeCtl) &lt; 0) &#123; //条件一：(sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs // true-&gt;说明当前线程获取到的扩容唯一标识戳 非 本批次扩容 // false-&gt;说明当前线程获取到的扩容唯一标识戳 是 本批次扩容 //条件二： JDK1.8 中有bug jira已经提出来了 其实想表达的是 = sc == (rs &lt;&lt; 16 ) + 1 // true-&gt; 表示扩容完毕，当前线程不需要再参与进来了 // false-&gt;扩容还在进行中，当前线程可以参与 //条件三：JDK1.8 中有bug jira已经提出来了 其实想表达的是 = sc == (rs&lt;&lt;16) + MAX_RESIZERS // true-&gt; 表示当前参与并发扩容的线程达到了最大值 65535 - 1 // false-&gt;表示当前线程可以参与进来 //条件四：transferIndex &lt;= 0 // true-&gt;说明map对象全局范围内的任务已经分配完了，当前线程进去也没活干.. // false-&gt;还有任务可以分配。 if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || transferIndex &lt;= 0) break; if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) &#123; transfer(tab, nextTab); break; &#125; &#125; return nextTab; &#125; return table;&#125; #10.get 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public V get(Object key) &#123; //tab 引用map.table //e 当前元素 //p 目标节点 //n table数组长度 //eh 当前元素hash //ek 当前元素key Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; e, p; int n, eh; K ek; //扰动运算后得到 更散列的hash值 int h = spread(key.hashCode()); //条件一：(tab = table) != null //true-&gt;表示已经put过数据，并且map内部的table也已经初始化完毕 //false-&gt;表示创建完map后，并没有put过数据，map内部的table是延迟初始化的，只有第一次写数据时会触发创建逻辑。 //条件二：(n = tab.length) &gt; 0 true-&gt;表示table已经初始化 //条件三：(e = tabAt(tab, (n - 1) &amp; h)) != null //true-&gt;当前key寻址的桶位 有值 //false-&gt;当前key寻址的桶位中是null，是null直接返回null if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (e = tabAt(tab, (n - 1) &amp; h)) != null) &#123; //前置条件：当前桶位有数据 //对比头结点hash与查询key的hash是否一致 //条件成立：说明头结点与查询Key的hash值 完全一致 if ((eh = e.hash) == h) &#123; //完全比对 查询key 和 头结点的key //条件成立：说明头结点就是查询数据 if ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) return e.val; &#125; //条件成立： //1.-1 fwd 说明当前table正在扩容，且当前查询的这个桶位的数据 已经被迁移走了 //2.-2 TreeBin节点，需要使用TreeBin 提供的find 方法查询。 else if (eh &lt; 0) return (p = e.find(h, key)) != null ? p.val : null; //当前桶位已经形成链表的这种情况 while ((e = e.next) != null) &#123; if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) return e.val; &#125; &#125; return null;&#125; #11.remove 123public V remove(Object key) &#123; return replaceNode(key, null, null);&#125; #12.replaceNode 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143final V replaceNode(Object key, V value, Object cv) &#123; //计算key经过扰动运算后的hash int hash = spread(key.hashCode()); //自旋 for (Node&lt;K,V&gt;[] tab = table;;) &#123; //f表示桶位头结点 //n表示当前table数组长度 //i表示hash命中桶位下标 //fh表示桶位头结点 hash Node&lt;K,V&gt; f; int n, i, fh; //CASE1： //条件一：tab == null true-&gt;表示当前map.table尚未初始化.. false-&gt;已经初始化 //条件二：(n = tab.length) == 0 true-&gt;表示当前map.table尚未初始化.. false-&gt;已经初始化 //条件三：(f = tabAt(tab, i = (n - 1) &amp; hash)) == null true -&gt; 表示命中桶位中为null，直接break， 会返回 if (tab == null || (n = tab.length) == 0 || (f = tabAt(tab, i = (n - 1) &amp; hash)) == null) break; //CASE2： //前置条件CASE2 ~ CASE3：当前桶位不是null //条件成立：说明当前table正在扩容中，当前是个写操作，所以当前线程需要协助table完成扩容。 else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); //CASE3: //前置条件CASE2 ~ CASE3：当前桶位不是null //当前桶位 可能是 &quot;链表&quot; 也可能 是 &quot;红黑树&quot; TreeBin else &#123; //保留替换之前的数据引用 V oldVal = null; //校验标记 boolean validated = false; //加锁当前桶位 头结点，加锁成功之后会进入 代码块。 synchronized (f) &#123; //判断sync加锁是否为当前桶位 头节点，防止其它线程，在当前线程加锁成功之前，修改过 桶位 的头结点。 //条件成立：当前桶位头结点 仍然为f，其它线程没修改过。 if (tabAt(tab, i) == f) &#123; //条件成立：说明桶位 为 链表 或者 单个 node if (fh &gt;= 0) &#123; validated = true; //e 表示当前循环处理元素 //pred 表示当前循环节点的上一个节点 Node&lt;K,V&gt; e = f, pred = null; for (;;) &#123; //当前节点key K ek; //条件一：e.hash == hash true-&gt;说明当前节点的hash与查找节点hash一致 //条件二：((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) //if 条件成立，说明key 与查询的key完全一致。 if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; //当前节点的value V ev = e.val; //条件一：cv == null true-&gt;替换的值为null 那么就是一个删除操作 //条件二：cv == ev || (ev != null &amp;&amp; cv.equals(ev)) 那么是一个替换操作 if (cv == null || cv == ev || (ev != null &amp;&amp; cv.equals(ev))) &#123; //删除 或者 替换 //将当前节点的值 赋值给 oldVal 后续返回会用到 oldVal = ev; //条件成立：说明当前是一个替换操作 if (value != null) //直接替换 e.val = value; //条件成立：说明当前节点非头结点 else if (pred != null) //当前节点的上一个节点，指向当前节点的下一个节点。 pred.next = e.next; else //说明当前节点即为 头结点，只需要将 桶位设置为头结点的下一个节点。 setTabAt(tab, i, e.next); &#125; break; &#125; pred = e; if ((e = e.next) == null) break; &#125; &#125; //条件成立：TreeBin节点。 else if (f instanceof TreeBin) &#123; validated = true; //转换为实际类型 TreeBin t TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; //r 表示 红黑树 根节点 //p 表示 红黑树中查找到对应key 一致的node TreeNode&lt;K,V&gt; r, p; //条件一：(r = t.root) != null 理论上是成立 //条件二：TreeNode.findTreeNode 以当前节点为入口，向下查找key（包括本身节点） // true-&gt;说明查找到相应key 对应的node节点。会赋值给p if ((r = t.root) != null &amp;&amp; (p = r.findTreeNode(hash, key, null)) != null) &#123; //保存p.val 到pv V pv = p.val; //条件一：cv == null 成立：不必对value，就做替换或者删除操作 //条件二：cv == pv ||(pv != null &amp;&amp; cv.equals(pv)) 成立：说明“对比值”与当前p节点的值 一致 if (cv == null || cv == pv || (pv != null &amp;&amp; cv.equals(pv))) &#123; //替换或者删除操作 oldVal = pv; //条件成立：替换操作 if (value != null) p.val = value; //删除操作 else if (t.removeTreeNode(p)) //这里没做判断，直接搞了...很疑惑 setTabAt(tab, i, untreeify(t.first)); &#125; &#125; &#125; &#125; &#125; //当其他线程修改过桶位 头结点时，当前线程 sync 头结点 锁错对象时，validated 为false，会进入下次for 自旋 if (validated) &#123; if (oldVal != null) &#123; //替换的值 为null，说明当前是一次删除操作，oldVal ！=null 成立，说明删除成功，更新当前元素个数计数器。 if (value == null) addCount(-1L, -1); return oldVal; &#125; break; &#125; &#125; &#125; return null;&#125; #13.TreeBin##13.1 属性 1234567891011121314151617//红黑树 根节点 TreeNode&lt;K,V&gt; root;//链表的头节点volatile TreeNode&lt;K,V&gt; first;//等待者线程（当前lockState是读锁状态）volatile Thread waiter;/** * 1.写锁状态 写是独占状态，以散列表来看，真正进入到TreeBin中的写线程 同一时刻 只有一个线程。 1 * 2.读锁状态 读锁是共享，同一时刻可以有多个线程 同时进入到 TreeBin对象中获取数据。 每一个线程 都会给 lockStat + 4 * 3.等待者状态（写线程在等待），当TreeBin中有读线程目前正在读取数据时，写线程无法修改数据，那么就将lockState的最低2位 设置为 0b 10 */volatile int lockState;// values for lockStatestatic final int WRITER = 1; // set while holding write lockstatic final int WAITER = 2; // set when waiting for write lockstatic final int READER = 4; // increment value for setting read lock ##13.2 构造器 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586TreeBin(TreeNode&lt;K,V&gt; b) &#123; //设置节点hash为-2 表示此节点是TreeBin节点 super(TREEBIN, null, null, null); //使用first 引用 treeNode链表 this.first = b; //r 红黑树的根节点引用 TreeNode&lt;K,V&gt; r = null; //x表示遍历的当前节点 for (TreeNode&lt;K,V&gt; x = b, next; x != null; x = next) &#123; next = (TreeNode&lt;K,V&gt;)x.next; //强制设置当前插入节点的左右子树为null x.left = x.right = null; //条件成立：说明当前红黑树 是一个空树，那么设置插入元素 为根节点 if (r == null) &#123; //根节点的父节点 一定为 null x.parent = null; //颜色改为黑色 x.red = false; //让r引用x所指向的对象。 r = x; &#125; else &#123; //非第一次循环，都会来带else分支，此时红黑树已经有数据了 //k 表示 插入节点的key K k = x.key; //h 表示 插入节点的hash int h = x.hash; //kc 表示 插入节点key的class类型 Class&lt;?&gt; kc = null; //p 表示 为查找插入节点的父节点的一个临时节点 TreeNode&lt;K,V&gt; p = r; for (;;) &#123; //dir (-1, 1) //-1 表示插入节点的hash值大于 当前p节点的hash //1 表示插入节点的hash值 小于 当前p节点的hash //ph p表示 为查找插入节点的父节点的一个临时节点的hash int dir, ph; //临时节点 key K pk = p.key; //插入节点的hash值 小于 当前节点 if ((ph = p.hash) &gt; h) //插入节点可能需要插入到当前节点的左子节点 或者 继续在左子树上查找 dir = -1; //插入节点的hash值 大于 当前节点 else if (ph &lt; h) //插入节点可能需要插入到当前节点的右子节点 或者 继续在右子树上查找 dir = 1; //如果执行到 CASE3，说明当前插入节点的hash 与 当前节点的hash一致，会在case3 做出最终排序。最终 //拿到的dir 一定不是0，（-1， 1） else if ((kc == null &amp;&amp; (kc = comparableClassFor(k)) == null) || (dir = compareComparables(kc, k, pk)) == 0) dir = tieBreakOrder(k, pk); //xp 想要表示的是 插入节点的 父节点 TreeNode&lt;K,V&gt; xp = p; //条件成立：说明当前p节点 即为插入节点的父节点 //条件不成立：说明p节点 底下还有层次，需要将p指向 p的左子节点 或者 右子节点，表示继续向下搜索。 if ((p = (dir &lt;= 0) ? p.left : p.right) == null) &#123; //设置插入节点的父节点 为 当前节点 x.parent = xp; //小于P节点，需要插入到P节点的左子节点 if (dir &lt;= 0) xp.left = x; //大于P节点，需要插入到P节点的右子节点 else xp.right = x; //插入节点后，红黑树性质 可能会被破坏，所以需要调用 平衡方法 r = balanceInsertion(r, x); break; &#125; &#125; &#125; &#125; //将r 赋值给 TreeBin对象的 root引用。 this.root = r; assert checkInvariants(root);&#125; ##13.3 putTreeVal 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970final TreeNode&lt;K,V&gt; putTreeVal(int h, K k, V v) &#123; Class&lt;?&gt; kc = null; boolean searched = false; for (TreeNode&lt;K,V&gt; p = root;;) &#123; int dir, ph; K pk; if (p == null) &#123; first = root = new TreeNode&lt;K,V&gt;(h, k, v, null, null); break; &#125; else if ((ph = p.hash) &gt; h) dir = -1; else if (ph &lt; h) dir = 1; else if ((pk = p.key) == k || (pk != null &amp;&amp; k.equals(pk))) return p; else if ((kc == null &amp;&amp; (kc = comparableClassFor(k)) == null) || (dir = compareComparables(kc, k, pk)) == 0) &#123; if (!searched) &#123; TreeNode&lt;K,V&gt; q, ch; searched = true; if (((ch = p.left) != null &amp;&amp; (q = ch.findTreeNode(h, k, kc)) != null) || ((ch = p.right) != null &amp;&amp; (q = ch.findTreeNode(h, k, kc)) != null)) return q; &#125; dir = tieBreakOrder(k, pk); &#125; TreeNode&lt;K,V&gt; xp = p; if ((p = (dir &lt;= 0) ? p.left : p.right) == null) &#123; //当前循环节点xp 即为 x 节点的爸爸 //x 表示插入节点 //f 老的头结点 TreeNode&lt;K,V&gt; x, f = first; first = x = new TreeNode&lt;K,V&gt;(h, k, v, f, xp); //条件成立：说明链表有数据 if (f != null) //设置老的头结点的前置引用为 当前的头结点。 f.prev = x; if (dir &lt;= 0) xp.left = x; else xp.right = x; if (!xp.red) x.red = true; else &#123; //表示 当前新插入节点后，新插入节点 与 父节点 形成 “红红相连” lockRoot(); try &#123; //平衡红黑树，使其再次符合规范。 root = balanceInsertion(root, x); &#125; finally &#123; unlockRoot(); &#125; &#125; break; &#125; &#125; assert checkInvariants(root); return null;&#125; ##13.4 find 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748final Node&lt;K,V&gt; find(int h, Object k) &#123; if (k != null) &#123; //e 表示循环迭代的当前节点 迭代的是first引用的链表 for (Node&lt;K,V&gt; e = first; e != null; ) &#123; //s 保存的是lock临时状态 //ek 链表当前节点 的key int s; K ek; //(WAITER|WRITER) =&gt; 0010 | 0001 =&gt; 0011 //lockState &amp; 0011 != 0 条件成立：说明当前TreeBin 有等待者线程 或者 目前有写操作线程正在加锁 if (((s = lockState) &amp; (WAITER|WRITER)) != 0) &#123; if (e.hash == h &amp;&amp; ((ek = e.key) == k || (ek != null &amp;&amp; k.equals(ek)))) return e; e = e.next; &#125; //前置条件：当前TreeBin中 等待者线程 或者 写线程 都没有 //条件成立：说明添加读锁成功 else if (U.compareAndSwapInt(this, LOCKSTATE, s, s + READER)) &#123; TreeNode&lt;K,V&gt; r, p; try &#123; //查询操作 p = ((r = root) == null ? null : r.findTreeNode(h, k, null)); &#125; finally &#123; //w 表示等待者线程 Thread w; //U.getAndAddInt(this, LOCKSTATE, -READER) == (READER|WAITER) //1.当前线程查询红黑树结束，释放当前线程的读锁 就是让 lockstate 值 - 4 //(READER|WAITER) = 0110 =&gt; 表示当前只有一个线程在读，且“有一个线程在等待” //当前读线程为 TreeBin中的最后一个读线程。 //2.(w = waiter) != null 说明有一个写线程在等待读操作全部结束。 if (U.getAndAddInt(this, LOCKSTATE, -READER) == (READER|WAITER) &amp;&amp; (w = waiter) != null) //使用unpark 让 写线程 恢复运行状态。 LockSupport.unpark(w); &#125; return p; &#125; &#125; &#125; return null;&#125; #总结在java8中，ConcurrentHashMap使用数组+链表+红黑树的组合方式，利用cas和synchronized保证并发写的安全。 引入红黑树的原因：链表查询的时间复杂度为On，但是红黑树的查询时间复杂度为O(log(n)),所以在节点比较多的情况下，使用红黑树可以大大提升性能。 链式桶是一个由node节点组成的链表。树状桶是一颗由TreeNode节点组成的红黑树。输的根节点为TreeBin类型。 当链表长度大于8整个hash表长度大于64的时候，就会转化为TreeBin。TreeBin作为根节点，其实就是红黑树对象。在ConcurrentHashMap的table数组中，存放的就是TreeBin对象，而不是TreeNoe对象。 数组table是懒加载的，只有第一次添加元素的时候才会初始化，所以initTable()存在线程安全问题。 重要的属性就是sizeCtl，用来控制table的初始化和扩容操作的过程： ● -1代表table正在初始化，其他线程直接join等待。 ● -N代表有N-1个线程正在进行扩容操作，严格来说，当其为负数的时候，只用到了低16位，如果低16位为M，此时有M-1个线程进行扩容。 ● 大于0有两种情况：如果table没有初始化，她就表示table初始化的大小，如果table初始化完了，就表示table的容量，默认是table大小的四分之三。 Transfer()扩容 table数据转移到nextTable。扩容操作的核心在于数据的转移，把旧数组中的数据迁移到新的数组。ConcurrentHashMap精华的部分是它可以利用多线程来进行协同扩容，简单来说，它把table数组当作多个线程之间共享的任务队列，然后通过维护一个指针来划分每个线程所负责的区间，每个线程通过区间逆向遍历来实现扩容，一个已经迁移完的 Bucket会被替换为一个Forwarding节点，标记当前Bucket已经被其他线程迁移完了。 helpTransfer()帮助扩容 ConcurrentHashMap并发添加元素时，如果正在扩容，其他线程会帮助扩容，也就是多线程扩容。 第一次添加元素时，默认初始长度为16，当往table中继续添加元素时，通过Hash值跟数组长度取余来决定放在数组的哪个Bucket位置，如果出现放在同一个位置，就优先以链表的形式存放，在同一个位置的个数达到了8个以上，如果数组的长度还小于64，就会扩容数组。如果数组的长度大于等于64，就会将该节点的链表转换成树。 通过扩容数组的方式来把这些节点分散开。然后将这些元素复制到扩容后的新数组中，同一个Bucket中的元素通过Hash值的数组长度位来重新确定位置，可能还是放在原来的位置，也可能放到新的位置。而且，在扩容完成之后，如果之前某个节点是树，但是现在该节点的“Key-Value对”数又小于等于6个，就会将该树转为链表。 put() JDK1.8在使用CAS自旋完成桶的设置时，使用synchronized内置锁保证桶内并发操作的线程安全。尽管对同一个Map操作的线程争用会非常激烈，但是在同一个桶内的线程争用通常不会很激烈，所以使用CAS自旋、synchronized不会降低ConcurrentHashMap的性能。为什么不用ReentrantLock显式锁呢?如果为每 个桶都创建一个ReentrantLock实 例，就会带来大量的内存消耗，反过来，使用CAS自旋、synchronized，内存消耗的增加更小。 get() get()通过UnSafe的getObjectVolatile()来读取数组中的元素。为什么要这样做?虽然HashEntry数组的引用是volatile类型，但是数组内元素的 用不是volatile类型，因此多线程对 数组元素的修改是不安全的，可能会在数组中读取到尚未构造完成的元素对象。get()方法通过UnSafe的getObjectVolatile方法来保证元素的读取安全，调用getObjectVolatile()去读取数组元素需要先获得元素在数组中的偏移量，在这里，get()方法根据哈希码计算出偏移量为u，然后通过偏移量u来尝试读取数值。","categories":[{"name":"1.基础知识","slug":"1-基础知识","permalink":"https://yinhuidong.github.io/categories/1-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"集合源码分析","slug":"集合源码分析","permalink":"https://yinhuidong.github.io/tags/%E9%9B%86%E5%90%88%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"}]}],"categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"ClickHouse","slug":"ClickHouse","permalink":"https://yinhuidong.github.io/categories/ClickHouse/"},{"name":"JUC","slug":"JUC","permalink":"https://yinhuidong.github.io/categories/JUC/"},{"name":"消息队列","slug":"消息队列","permalink":"https://yinhuidong.github.io/categories/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"name":"MyBatis","slug":"MyBatis","permalink":"https://yinhuidong.github.io/categories/MyBatis/"},{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/categories/Spring/"},{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/categories/MySQL/"},{"name":"1.基础知识","slug":"1-基础知识","permalink":"https://yinhuidong.github.io/categories/1-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"软件架构","slug":"软件架构","permalink":"https://yinhuidong.github.io/tags/%E8%BD%AF%E4%BB%B6%E6%9E%B6%E6%9E%84/"},{"name":"设计模式","slug":"设计模式","permalink":"https://yinhuidong.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"ClickHouse","slug":"ClickHouse","permalink":"https://yinhuidong.github.io/tags/ClickHouse/"},{"name":"并发编程","slug":"并发编程","permalink":"https://yinhuidong.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"},{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"https://yinhuidong.github.io/tags/RabbitMQ/"},{"name":"MyBatis","slug":"MyBatis","permalink":"https://yinhuidong.github.io/tags/MyBatis/"},{"name":"Spring","slug":"Spring","permalink":"https://yinhuidong.github.io/tags/Spring/"},{"name":"MySQL","slug":"MySQL","permalink":"https://yinhuidong.github.io/tags/MySQL/"},{"name":"集合源码分析","slug":"集合源码分析","permalink":"https://yinhuidong.github.io/tags/%E9%9B%86%E5%90%88%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"}]}